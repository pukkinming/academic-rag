{
  "paper_id": "2101.10808v3",
  "title": "Fast Facial Landmark Detection And Applications: A Survey",
  "published": "2021-01-12T09:40:40Z",
  "authors": [
    "Kostiantyn Khabarlak",
    "Larysa Koriashkina"
  ],
  "keywords": [
    "Computer Vision",
    "Edge Computing",
    "Facial Landmarks",
    "Neural Networks",
    "Mobile Applications",
    "Literature Overview"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dense facial landmark detection is one of the key elements of face processing pipeline. It is used in virtual face reenactment, emotion recognition, driver status tracking, etc. Early approaches were suitable for facial landmark detection in controlled environments only, which is clearly insufficient. Neural networks have shown an astonishing qualitative improvement for in-the-wild face landmark detection problem, and are now being studied by many researchers in the field. Numerous bright ideas are proposed, often complimentary to each other. However, exploration of the whole volume of novel approaches is quite challenging. Therefore, we present this survey, where we summarize state-of-the-art algorithms into categories, provide a comparison of recently introduced in-the-wild datasets (e.g., 300W, AFLW, COFW, WFLW) that contain images with large pose, face occlusion, taken in unconstrained conditions. In addition to quality, applications require fast inference, and preferably on mobile devices. Hence, we include information about algorithm inference speed both on desktop and mobile hardware, which is rarely studied. Importantly, we highlight problems of algorithms, their applications, vulnerabilities, and briefly touch on established methods. We hope that the reader will find many novel ideas, will see how the algorithms are used in applications, which will enable further research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Dense facial landmark detection is one of the key elements of face processing pipeline. Applications include virtual face animation, emotion recognition, driver status tracking, etc. Early attempts to solve the problem were based on deformable face model, where statistical algorithms predicted face model deformation coefficients. These approaches were unsuitable for landmark annotation with large pose, face occlusion or unusual illumination. Later, attention has been driven to neural networks, that show high quality in solving tasks, in which we, humans, are good at, such as image classification or natural language processing. Neural networks have also shown an astonishing qualitative improvement for in-the-wild face landmark detection problem, and are now being actively studied by many researchers in the field. Primarily, neural networks were designed to be executed on servers with many GPUs and a stable power supply. However, the development of Internet of Things and mobile devices makes client-server applications sometimes impractical or even unacceptable. For example, when Internet connectivity is poor, low latency data processing is required, if the amounts of raw data generated are too large to be sent over to a server. Finally, when no data can leave the user's device for security reasons. In many of these cases use of neural networks is desirable, and processing should be done directly on a mobile device. Thus, on-device machine learning has become one of the most prominent machine learning research directions  [1] ,  [2] .\n\nIn this paper we present a description of recently introduced neural-network-based facial landmark detection algorithms. Existing surveys are quite old and mostly cover either statistical algorithms or the ones based on ensembles of regression trees  [3] ,  [4] . These algorithms show poor facial landmark detection quality for in-the-wild pictures (i.e., taken in unconstrained environments). Recently, numerous bright neural-network-based approaches were proposed, that show substantially better quality. However, ex-1 arXiv:2101.10808v3 [cs.CV] 25 Apr 2022 ploration of the whole volume of novel approaches is quite challenging. Therefore, we present this work. The primary focus of this survey is on recently introduced algorithms, covering years 2018 -2021. We include some important older algorithm for completeness as well.\n\nWe start our survey by defining facial landmark detection problem, algorithm quality assessment metrics. Next, we describe common in-the-wild datasets (e.g., 300W, AFLW, COFW, WFLW) with dense landmark annotation (from 21 to 98 landmarks). These datasets contain images taken in unconstrained conditions with large pose, face occlusion, different emotions, etc. The following section describes ideas of facial landmark algorithms, that have led to accuracy improvement or have proposed a novel way to solve the problem. This section is key for this survey. To make algorithm ideas clear, we start by explaining common neural network backbones used for facial landmark detection. Based on these materials, we follow with an explanation of facial landmark detection algorithms ordered by years. Finally, we summarize state-of-the-art algorithms into categories, provide accuracy comparison on recently introduced in-the-wild datasets. In addition to quality, applications require fast inference, possibly on mobile devices. Hence, we include information about algorithm inference speed both on desktop and mobile hardware, which is rarely studied in literature. Where available, inference time is shown for desktop CPU and GPU, as well as mobile phone. Also, we provide estimated number of neural network parameters and floating-point operations. These are the metrics, that influence memory consumption and inference time correspondingly. Importantly, we highlight problems of algorithms, their applications and vulnerabilities. Overall, we note that algorithm accuracy needs to be improved by the next generation of algorithms. Also, state-of-the-art algorithms have inference times that are quite high for practical applications. We hope that in this survey the reader will find many novel ideas, will see how the algorithms are used in applications, which will enable new research in the field.\n\nThe paper is structured as follows: Section 2 covers facial landmark detection problem. Section 3 describes datasets used to train and evaluate models. Section 4.1 gives a brief introduction of historical landmark detection methods. The main Sections 4.2 and 4.3 cover common neural network backbones and landmark detection algorithms correspondingly. Analysis of algorithm accuracy, inference speed, and summary of novel ideas is presented in Section 4.4. Section 5 is focused on real-world use of face landmark detection methods. We show several possible approaches of joint face and landmark detection algorithms in Section 5.1. Applications of dense facial landmark detection are shown in animation in Section 5.2, driver status tracking in Section 5.3, face and emotion recognition in Section 5.4. Finally, adversarial attack vulnerability is dis-cussed in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Facial Landmark Detection Problem Statement",
      "text": "Let I be an input image, which is represented in a form of 3-dimensional tensor of size W × H × C, where W , H, C are the width, height, and number of image color channels correspondingly. Note, that typically color images are used with 3 channels, one for red, green and blue colors. Then facial landmark detection problem is to find such function Φ : I → Y , that from the input image I predicts a landmark matrix Ŷ ∈ R N L ×2 , where N L is the number of facial landmarks, Ŷi 1 ∈ [0; W ] represents X coordinate and Ŷi 2 ∈ [0; H] represents Y coordinate of i th landmark. Number of facial landmarks N L and exact mapping between i th facial landmark and its location on the face (the so-called annotation scheme) are defined at dataset level. Examples of face landmark annotations are present in Fig.  1 . Also, dataset defines which images are used to train function Φ (train set) and which to evaluate (test set).\n\n(a) 300W  1 (b) WFLW 2  [5]  Figure  1 . Examples of faces annotated with facial landmarks from several of the commonly used datasets: 300W and AFLW. In both cases landmarks cover areas of jaw, eyes, eyebrows, nose, lips. However, annotation schemes differ. For instance, WFLW annotates lower and upper boundary of eyebrows, whereas 300W has a single central line. Also, WFLW has the densest landmark annotation.\n\nNext, we present commonly used metrics to report algorithms' quality on facial landmark detection datasets. Note, that each dataset has a special protocol, which defines train/test split, metrics for algorithm comparison, etc. The main metrics include  [6] ,  [7] :\n\n1. Normalized Mean Error (NME, %):\n\nwhere Y is the matrix of true landmark locations, Ŷ is the matrix of predicted landmark locations, d is the normalization coefficient (different for each dataset), N L is the number of facial landmarks per face in the dataset, K is the number of images in the test set. Lower metric values are better.\n\n2. Failure Rate (FR, %):\n\ndenotes number of images with Normalized Mean Error above 10 % threshold. Lower metric values are better.\n\n3. Cumulative Error Distribution -Area Under Curve (CED-AUC). First, fraction of images whose NME is less than or equal to the NME value on X axis is plotted. Area under curve is then computed.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Common Face Landmark Datasets",
      "text": "There are several open datasets available to train and evaluate quality of face landmark detection algorithms. Each of the datasets includes image of a person and corresponding face landmark annotations. Landmarks are provided in a separate file. The datasets can include photos of the following kinds:\n\n• in controlled environment (e.g., studio) or in-the-wild;\n\n• with different shooting conditions, such as presence of face occlusion, large pose, make-up, etc.;\n\n• real images or synthetic (when faces are generated with an algorithm);\n\n• 2D or 3D face landmarks.\n\nNext, we describe typical datasets used to train and evaluate facial landmark detection models. The datasets were selected from the following sources: 1) introduced jointly with a novel facial landmark detection algorithm; 2) presented separately, but at least one of the algorithms from Section 4.3 uses the dataset for training or evaluation. While we discuss all such datasets, the focus of this survey is on in-the-wild 2D face landmark datasets with non-synthetic images. Note, that in-the-wild datasets also include images in controlled environments, which makes them applicable to a wide range of practical use-cases. 300 Faces in-the-Wild (300W)  [8]  dataset contains a collection of different datasets: LFPW  [9] , AFW  [10] , HE-LEN  [11] , XM2VTS  [12]  and IBUG, that were relabeled with 68 facial landmarks. The protocol defines which images should be used for training and which for testing. The testing subset is split into common, challenge and full. Normalized Mean Error for each of the splits is usually presented for comparison. The NME is normalized (d in Eq. (  1 )) by inter-pupil or inter-ocular (outer eye corner) distance. This is done, so that faces of different sizes make an equal contribution to the resulting error. Note, that images in the 300W dataset have different shooting conditions (lighting, color gamut), emotions and faces at an angle. There have been multiple extensions to the 300W datasets presented: 300W-LP-2D  [13] , where the original 300W dataset has been expanded with synthetically generated images with large pose; Masked-300W  [14]  has synthetically added medical mask to the 300W dataset images. However, the same blue medical mask model has been used for all images, which is a disadvantage. Annotated Facial Landmarks in-the-Wild (AFLW)  [15]  contains a larger number of images, yet they are labeled with only 21 facial landmarks. In comparison to 300W, this dataset has face photographs taken at a larger angle in range of ±120 • yaw and ±90 • pitch. The authors propose splitting the dataset into AFLW-Frontal (with face photos that are close to frontal) and AFLW-Full (all images). Also, there is a relabeled version with 68 facial landmarks, named AFLW-68  [16] , yet in practice it is used less often. MERL-RAV dataset presented in  [6]  has AFLW relabeled with 68 landmarks with an extra visibility label, such as: 1) visible; 2) self-occluded (for instance, due to large pose); 3) occluded by other object (hand, etc.). NME metric, normalized by face bounding box size (diagonal), is used for comparisons.\n\nCaltech Occluded Faces in-the-Wild (COFW)  [17]  focuses on face images, that are partially occluded by realworld objects (microphone, etc.) or by the person itself (hair, hand, etc.). In addition to the NME metric, Failure Rate (FR, Eq. (  2 )) is used. The dataset has 29 landmark annotations. NME is normalized by either inter-pupil or interocular distance. The COFW test set has also been relabeled to 68 landmarks in COFW-68  [18] , but no COFW training set with 68 landmarks is available. COFW-68 can be used to assess landmark detection quality, when the network has been trained on a different dataset.\n\nWider Facial Landmarks in-the-Wild (WFLW)  [5]  is the dataset with the largest number of facial landmarks (98 landmarks). It is also the most recently introduced. In comparison to the datasets covered so far, WFLW has more images taken under unusual conditions, e.g., with makeup, wide range of emotions, poses, in various lighting conditions, etc. All three above-mentioned metrics are used to present the results: NME, Failure Rate and CED-AUC. NME is normalized by inter-ocular distance. The results are reported for each subset of unusual images, as well as for all images available in the dataset. This makes it possible to analyse, which conditions are the most challenging to the algorithms. The following subsets are available in WFLW dataset: Pose, Expression, Illumination, Make-Up, Occlusion, Blur. Information about image scene type is included in the dataset and can also be used during training.\n\nMenpo-2D  [19] ,  [20]  is a collection of frontal and profile faces. However, annotation schemes and number of landmarks are different between types of faces. The dataset is less used in practice.\n\nIn addition, there are many datasets that provide 3D annotations of facial landmarks (either synthetically generated or manually), such as 300W-LP  [13] , AFLW2000-3D  [13] , LS3D-W  [21] , Menpo-3D  [20] ,  [22] . Also, landmark annotated video datasets exist, e.g., 300 Videos in the Wild (300VW)  [23] -  [25] .\n\nTable  1  summarizes information about common datasets. We include information about number of labeled images for algorithm training and testing, as well as number of facial landmarks the dataset has been labeled with. The most widely used datasets are shown in bold. 7,500 2,500 98 300W-LP-2D  [13]  61,225 -68 AFLW-68  [16]  20,000 4,386 68 COFW-68  [18]  -507 68 Menpo-2D  [19] ,  [20]  7,564 7,281 68/39 MERL-RAV  [6]  15,449 3,865 68 Masked-300W  [14]  3,837 600 68",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Facial Landmark Detection Algorithms",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Early Landmark Detection Algorithms",
      "text": "First algorithms were mainly based on fitting a deformable face mesh. The most prominent algorithms include Active Shape Model (ASM), Active Appearance Model (AAM) and Constrained Local Model (CLM)  [3] ,  [4] . Based on the obtained mesh, each of the landmark locations are computed. In many cases such algorithms utilize statistical methods as a base. They have good enough prediction accuracy in controlled environments (with proper lighting and frontal face). However, such approaches offer underwhelming performance for most types of in-thewild images. Next wave of methods was based on Random Forests and Gradient Boosting, such as ERT  [26]  algorithm, which we describe below. Such methods have better accuracy, but performance for occluded faces, faces shot under large angle or unusual illumination is still insufficient. As will be shown later in this work, many practical applications require accurate in-the-wild facial landmark detection.\n\nDlib  [27]  is an open-source machine learning library. Among others, it has Ensemble of Regression Trees (ERT)  [26]  facial landmark detection algorithm, which is a cascade, based on gradient boosting. The authors use a \"mean\" face template as an initial approximation, then the template is refined over several iterations. The algorithm requires the face to be first detected in the frame (Viola-Jones  [28]  face detector is used). Note, that most facial landmark detection algorithms require face to be first detected. High speed is the main advantage of ERT (according to the authors, around 1 millisecond per face). The library contains ERT implementation, trained on 300W dataset. The algorithm is still actively used in the modern research thanks to an open implementation and speed. However, not so long ago it has been shown that neural networks are preferred in terms of quality for faces with large pose  [29] . Mobile-friendly implementations of ERT are available.\n\nAn overview of early neural-network-based facial landmark detection algorithms can be found in  [4] ,  [30] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Face Landmark Detection Network Backbones",
      "text": "Modern in-the-wild face landmark detection algorithms are based on neural networks. They are divided into 2 main categories: direct (or coordinate) regression methods, when the model predicts x, y coordinates directly for each landmark; heatmap-based regression methods, where a 2D heatmap is built for each landmark. The values in the heatmap can be interpreted as probabilities of landmark location at a certain image location. Typically, argmax or its modification is used to acquire exact landmark coordinates from the heatmap. Fig.  2  illustrates the two approaches. As neural network architectures have become more complex, algorithms typically base on a pre-defined network architecture (called backbone). Facial landmark detection algorithms, described in the following subsection, propose modifications to training, inference procedure or the backbone itself. Here we introduce main backbones for landmark detection problem. Note, that in many cases backbones for face landmark and human pose (whole body) landmark detection are the same. Direct regression methods typically use widely known backbones from ImageNet challenge  [31] , such as ResNet  [32] , MobileNetV2  [2] , Mo-bileNetV3  [33] , ShuffleNet-V2  [34] . Heatmap-based methods commonly use Hourglass  [35]  network architecture, but also HRNet  [36]  and CU-Net  [37] . Such backbones are less known. Thus, we describe them here.\n\nHourglass  [35]  architecture has been initially designed for human pose estimation. The network takes a 256 × 256 image as an input. The authors note that processing the image at full resolution would require too much computation and memory. This is why a convolutional block is used to quickly process the image to obtain feature map of resolution 64 × 64, which remains the maximum feature map resolution till the end of the network. The feature map is then processed via Hourglass modules. An illustration of Hourglass network is shown in Fig.  3 . Note, that architecture allows stacking, i.e., Hourglass modules can be repeated sequentially multiple times. Typically stacks of 1, 2 or 4 Hourglass modules are used. The network outputs heatmaps at a resolution of 64 × 64, a single heatmap is produced for each of the landmarks.\n\nHourglass module follows encoder-decoder architecture. Input image is processed via convolutional blocks at different feature map resolutions. First, feature map resolution is decreased after each convolutional block (encoder part), then feature map resolution is restored (increased) after each block (decoder part). Accuracy of human pose estimation, facial landmark detection and several other tasks is improved by processing image at multiple resolutions.\n\nOverall, stacked Hourglass architecture becomes quite deep, which slows down training. The authors propose two ideas to solve the problem: skip connections inside Hourglass module and intermediate supervision. Firstly, as is clearly seen from Fig.  3 , after each convolutional block the output is split into two parts, one part is downscaled and fed into next convolutional block, another is skipped from encoder to decoder. The latter concept is then referred to as \"skip connection\". This improves gradient propagation. Secondly, intermediate supervision is applied to each Hourglass module (as previously said, stacks of multiple Hourglass modules can be constructed). Prediction heatmaps are always constructed after each Hourglass module (shown in green in Fig.  3 ). The training loss includes weighted sum of losses for each of the heatmap predictions.\n\nCU-Net  [37]  tries to improve Hourglass architecture not only in quality, but also in memory footprint and inference time. The authors note an importance of efficient architecture for use on mobile devices. Similarly to Hourglass, the network takes 256 × 256 image as an input and resizes it in preamble to 64 × 64, which remains the maximum resolution at which features are processed till the end of the network. To improve training and enable deeper CU-Net stacks, the authors propose to add skip connections not only between features of the same module, but also between different modules. To avoid excessive number of skip connections, a concept of Order-K coupling has been introduced in the paper. Order-K coupling denotes that skip connec-tions will be added only K modules forward. In most cases, adding skip connections to one module forward (K = 1) seems sufficient. The authors decrease memory consumption and improve inference speed by avoiding unnecessary features copies, sharing memory, and quantizing both features and parameters. In addition, blocks with smaller number of features are used to decrease overall number of parameters. All these improvements have allowed to achieve similar to Hourglass accuracy on human pose estimation with only a fraction of parameters and higher inference speed. Exact number of parameters and inference times are shown in Table  2 . However, despite the improvements, most recently introduced approaches prefer Hourglass architecture over CU-Net as will be shown later. HRNet  [36]  has also been initially proposed for human pose estimation and then adapted to face landmark prediction in  [38]  and other works. This architecture significantly differs from the previous two. HRNet doesn't follow encoder-decoder architecture and doesn't use multiple stacks. Instead, parallel branches with different feature resolutions are maintained throughout the network. An illustration of HRNet architecture is shown in Fig.  4 . Similarly to previous works, the network takes an image of size 256 × 256, which is then resized to 64 × 64 feature map in preamble. Next, the image is processed via convolutional blocks. Then another branch of resolution 32 × 32 is added. Note, that in contrast to previous works, 64 × 64 branch continues to be processed in parallel. The authors propose to exchange features between parallel branches. However, these feature maps are of different resolutions. To downscale feature map, strided convolution is used. To upscale feature map, nearest neighbor upsampling is used. Till the end of the network 4 parallel branches with different feature map resolutions are created. The final heatmap is generated at resolution of 64 × 64. At a similar number of parameters to a stack of 8 Hourglass modules, HRNet uses nearly twice fewer floating-point operations. Additionally, HRNet network width (number of convolutional channels) can be configured to change overall number of parameters and resulting inference speed.\n\nWe summarize backbone performance in Fig.  5 , where  we show number of floating-point operations in Fig.  5a  (the greater is the number, the more time it takes to infer the network) and number of parameters in Fig.  5b  (more parameters take more memory). Note, that Hourglass, CU-Net, HRNet require much more computation than other backbones, but have relatively small number of parameters. This is because these networks consider input at multiple resolutions and have to produce large heatmaps for each   9 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Facial Landmark Detection: Novel Algorithms And Ideas",
      "text": "In this section we present a description of recently introduced facial landmark detection algorithms. Each description is structured as follows: backbone and algorithm type are mentioned first, followed by explanation of novel ideas and approaches. The primary focus of this paper is on modern algorithms, covering years 2018 -2021. We include some important older algorithm for completeness as well. The facial landmark algorithms covered in this section have been selected if: 1) the algorithm improves state-of-the art score established in the previous year; 2) ideas presented in the algorithm are then used in several following papers; 3) algorithm expands applicability of facial landmark detection or presents distinctive novel idea not discussed before. If only a slight modification is presented, that doesn't improve inference speed, quality or applicability of the algorithm, such algorithm is not included in this section. The algorithms are collected from various sections, including, but not limited to, top worldwide computer vision conferences. Overall 22 algorithm discussions are presented in this section.\n\nDense Face Alignment (DeFA)  [39]  is a shape-modelbased approach. It uses custom-built convolutional neural network as a backbone. It is the only algorithm described in this section, where a neural network is used for facial landmark prediction through a deformable 3D face mesh. Algorithm is interesting in the following: 1) it allows to build a dense 3D face mesh using only a single 2D image. Mesh can be built for a wide range of poses and emotions (Fig.  6 ); 2) DeFA can be trained jointly on datasets with different number of landmarks, as landmarks are hooked as mesh constraints.\n\nStyle Aggregated Network (SAN)  [40]  is a heatmapbased approach, which is based on a modified ResNet-152 backbone. The authors have noticed style variability of photographs in 300W and AFLW datasets, which can be dark or light, colored or black & white. Existing to date algorithms were not accounting for that information. Furthermore, the authors have noticed that depending on style, prior algorithms were predicting facial landmark locations in slightly different places, with higher error on photographs with harsh lighting conditions. As a solution they have proposed: 1) to train Generative Adversarial Network (GAN)  [41] , namely CycleGAN  [42]  to transform images of different styles into neutral; 2) to train another neural network to predict landmarks from two inputs: style-neural and the original image. CycleGAN colorizes grayscale images and tones down bright colors. This makes all input images have a similar color distribution, which simplifies learning of face features by a neural network. Note, that style-neutral images produced by the proposed network are not always properly colorized and might containing artifacts, becauseof that the authors propose to predict landmark jointly on the original and style-neutral images.\n\nLook at Boundary (LAB)  [5]  is a combined heatmap and direct regression method. A stack of 4 Hourglass modules is used to predict boundary heatmap, from which another neural network predicts landmark matrix. The key advancement of this architecture is that the authors introduce 3 Images are included under MIT license. Source: DeFA face feature boundary heatmap, which is built as an intermediate representation between original image and predicted landmarks (Fig.  7 ). Such a trick improves facial landmark prediction quality. Furthermore, it allows to train boundary estimation module on several datasets with different annotation schemes at once. After boundary module, another network predicts facial landmark coordinates. It should be noted, that only boundary submodule can be trained on datasets with different annotation schemes, while the landmark regression is trained for each dataset separately. Face structural information is modeled with the use of message passing  [43] ,  [44] , that is, a graph-based way to model relationships. Boundary module is trained in adversarial (GAN) fashion. As the authors have shown, pretraining the boundary module on 300W improves prediction quality on AFLW and COFW datasets. Also, a novel facial landmark dataset was introduced in the work, namely WFLW.\n\nWing Loss  [29]  is a direct regression approach. Several backbones were considered: custom-built CNN-6; twostage approach, when CNN-6 produces coarse landmarks, and CNN-7 then refines them; ResNet-50 backbone. The authors note, that the field of loss functions for facial landmark detection problem is barely studied. Most researchers use L2 = x 2 /2 as a loss function for direct regression, which is known to be sensitive to outliers. For that reason, some of prior works have used smoothL1  [45]  loss instead. The authors make a comparison of L2 against other loss functions, such as L1(x) = |x| and smoothL1, which is defined as:\n\nand note that the latter two give better results. The main paper contribution is in introduction of a new loss, named Wing loss, which combines L1 for large landmark deviations and ln(•) for medium and small:\n\nwhere C = w -w ln(1 + w/ ), w and are hyperparameters (w = 15, = 3 in paper). Visual comparison of loss functions is presented in Fig.  8 . Also, to train more on hard examples the authors introduce PDB algorithm, which works as follows: 1) face rotation angle histogram is built; 2) rare examples (determined via the histogram) are duplicated with augmentations. As can be seen from Table  3 , using CNN-6/7 cascade with wing(•) loss in combination with PDB substantially lowers the NME.  ). Note, that quadratic growth of L2 loss makes it sensitive to outliers. Thus, forcing the network to learn annotation errors. L2, L1 and smoothL1 yield very small values for small landmark location differences. This hinders network training, when network predictions are almost correct. In contrast, Wing is less sensitive to outliers and is much sensitive to mediumto-small errors, which improves training overall. their extended training set, which resulted in better performance.\n\nPractical Facial Landmark Detector (PFLD)  [46]  is a direct approach. MobileNetV2 backbone with full (1X) and quarter (0.25X) width has been considered. PFLD enables fast facial landmark detection directly on a mobile device. This is, to the best of our knowledge, the only modern neural-network-based algorithm, whose authors have shown that their algorithm can work efficiently on a mobile device. MobileNetV2  [2]  is used as feature extractor in PFLD. Two heads are attached to it (Fig.  9 ): 1) facial landmark regression, where multi-scale fully-connected layer in the end of the head is used (lower branch); 2) 3D face model rotation angle estimator (yaw, pitch and roll), shown in upper branch of the figure. The second head contains a set of convolutional layers and is only used during training.\n\nThe most common datasets do not have information about 3D landmark coordinates. To get them the authors propose to 1) build a \"mean\" face representation containing 11 facial landmarks, based on the data in the training set; 2) estimate rotation matrix for each face between its and \"mean\" landmarks; 3) compute yaw, pitch, roll angles from the rotation matrix. According to the authors, such an approach is not very accurate for estimating angles, yet improves network training. Furthermore, during training, the data is weighted based on image difficulty using a special loss function:\n\nwhere N is the number of facial landmarks, M is the number of training examples, K = 3, θ 1 , θ 2 , θ 3 are yaw, pitch, roll rotation angles of the above-described 3D face model, d m n represents difference vector between n th predicted and ground true facial landmarks for m th image; C is the number of complexity classes for face images (such as profile or frontal face, face-up, face-down, emotions or occlusion), ω c n is fraction of images in the corresponding complexity class to their total number M .\n\nFAN  [21]  is a heatmap-based approach. A stack of 4 Hourglass modules is used. The authors modify Hourglass architecture by substituting Bottleneck block with hierarchical, parallel and multi-scale block with binary convolutions from  [47] . 3 methods have been trained in the work: for 2D, 3D landmark detection, and 2D-to-3D model. 2Dto-3D model serves to transform 2D landmark representation into 3D. Interestingly, the inputs to the 2D-to-3D network are image and landmark heatmaps (one for each of the input 2D landmarks). The algorithm has not been tested on conventional 2D face landmark datasets. Thus, is missing from summary in Section 4.4. While binarized convolutions are stated to be faster, than conventional  [47] , no testing results have been presented. Architecture has been applied to landmark prediction in further works. Also, LS3D-W 3D face landmark dataset has been presented in this work.\n\nAWing  [7]  is a heatmap-based algorithm, Hourglass is used as a backbone. Stacks from 1 to 4 modules have been considered. The algorithm is based on Wing loss, FAN, Figure  10 . AWing surface plot. AWing accepts true y and predicted ŷ heatmap probabilities. The function behaves as L2 loss for background pixels (when y, ŷ are close to zero), and as Wing loss for foreground (when y, ŷ are close to one), while preserving continuity. Thus, a greater weight is assigned to foreground pixels, resulting in sharper heatmaps and more accurate prediction.\n\nLAB papers, and CoordConv  [48] . The authors have noticed, that L2 loss function does not produce sharp-enough heatmaps on difficult face images, because it is insensitive to small errors. In the meantime, the original Wing loss is inappropriate for heatmap-based detection as its gradient is discontinuous at the point of zero. In addition, each heatmap has a class imbalance. Only a few pixels on the map relate to the foreground class (meaning that landmark is likely to be at this point), while most parts are labeled as background class. The class imbalance is also not considered in the original Wing loss implementation. To solve all these issues, Adaptive Wing loss (Fig.  10 ) is introduced, which is 1) differentiable around zero; 2) accents small errors around foreground pixels, but not around background. Here we do not give the function formula due to its complexity. To predict foreground pixels even more precisely, the authors introduce a special weighted loss map, which additionally enhances sharpness of the facial landmark heatmap.\n\nMobileFAN  [49]  is a heatmap-based approach, based on modified MobileNetV2 backbone with 1X or 0.5X width. The authors examine network distillation approaches in order to reduce the number of model parameters and increase inference speed for heatmap-based methods. Note, that despite name similarity the approach is not based on FAN  [21] , discussed earlier.\n\nGeometry Aggregated Network (GEAN)  [50]  is a heatmap-based approach, based on a stack of 4 Hourglass modules. The authors propose train-and test-time augmen-tation using Adversarial Attacks. Face adversarial attacks add noise or deformation to an image, so that face will not be recognized by face recognition system. To do that, face adversarial attack  [51]  warps the image to shift facial landmarks. The resulting face has slightly different shape, eye distance, etc. Hourglass is used to detect landmark locations on such deformed image. The detected landmarks now correspond to the warped face. However, we need to form a prediction for the original face. To do that, we shift landmark coordinates with warp deformation, that is opposite to the one introduced by the adversarial attack. Now the predicted landmarks correspond to the original face. Next, we follow this procedure for K random adversarial attacks. It turns out, that the predicted facial landmarks for each of the K images will be slightly different. Averaging such landmarks over all K images, results in accuracy improvement.\n\nAccording to the authors, with respect to performance/quality ratio, it is the most beneficial to generate K = 5 adversarial examples for both training and testing. It is possible to use different number of adversarial images for training and testing. The authors have explored several modifications to the adversarial attack algorithm, and the best results are obtained when attack scale is set individually for each of the landmarks' semantical groups. The groups are assigned based on face region, such as nose, eyes, eye-brows, etc.\n\nWe have a deeper look at the concept of Adversarial Attacks in Section 6 of this paper.\n\nHRNetV2  [38]  is a heatmap-based approach. In this work the original architecture of HRNet  [36]  has been improved and applied to the task of facial landmark prediction.\n\nLUVLi  [6]  is a heatmap-based approach. This is the only algorithm with CU-Net backbone. A stack of 8 modules is used. The authors state, that facial landmark detection is used in many critically important applications. Thus, they propose a method to predict facial landmark visibility and algorithm confidence for each landmark. Cholesky Estimator Network (CEN) and the Visibility Estimator Network (VEN) are introduced for landmark and visibility predictions correspondingly. To increase heatmap precision, the authors use weighted spatial mean of heatmap's positive elements, instead of simple argmax. Also, a relabeled AFLW dataset with 68 landmarks and landmark visibility labels is presented.\n\nDeep Adaptive Graph (DAG)  [52]  is a direct regression approach. Note, that landmarks here are predicted through a graph. Multiple backbones have been considered: VGG16  [53] , ResNet50, 4×Hourglass, HRNet-18. HRNet-18 has shown the best results.\n\nFace landmark prediction accuracy can be improved by taking into account structural information about human face. The authors propose a topology-adapting graph learning in a form of Graph Convolutional Network (GCN) cas-cade for facial and medical (e.g., hand, pelvis) landmark detection. In DAG algorithm graph G = (V, E, F ) is constructed, where V is a set of vertices, E is a set of edges, F = f 1 , f 2 , . . . , f |V | is the so-called graph signal or graph features. Each vertex v corresponds to a single landmark. Each pair of vertices (v i , v j ) i =j is connected via a weighted edge e ij . The weights e ij are learned during training process, they determine how information is propagated in a graph convolution. Larger weights denote stronger semantical connection between corresponding vertices. Graph convolution is defined as follows:\n\nwhere W 1 and W 2 are learnable parameter matrices. f i k is the feature computed for i th vertex and k th graph convolution.\n\nFeatures F contain visual p i and shape q i features. Visual features are acquired from feature map H, that is produced by processing the whole image via convolutional neural network. Feature p i , that corresponds to i th vertex, is then acquired from H at a location near the landmark. To obtain shape features q i the authors compute displacement vectors for each pair of landmarks. Displacement information improves the algorithm performance, when face is partially occluded. In such cases, landmark locations can then be predicted from neighboring landmarks.\n\nThe landmark prediction process is conducted as follows: initial graph is constructed with mean weights computed over training set. Two separate GCNs are used for iterative graph transformation. GCN-Global is used to predict perspective transformation of the initial graph. GCN-Local is then applied multiple times to predict offsets for each of the landmarks for precise graph refinement.\n\nThe authors show, that in case of significant face occlusion their algorithm is better than the competition. In addition, the learned graph is good at capturing semantical information about human face, greater weights e ij are learned for landmarks that appear physically closer to each other. For instance, edges between eyebrows have greater weight than edges between eyebrow and chin landmarks.\n\nPropagationNet  [54]  is a heatmap-based algorithm, which uses a stack of 4 Hourglass modules as a backbone. The authors note importance of face boundary information for landmark prediction. Previous LAB approach used heavy generative adversarial network for boundary estimation. The authors of PropagationNet propose much simpler and faster approach: several convolutional blocks transform landmark heatmaps into boundary heatmaps after each Hourglass module. Boundary heatmaps serve as attention mask for intermediate predictions in Hourglass module to improve the final prediction accuracy. In addition, Hour-glass module has modifications from FAN, and is extended with CoordConv  [48]  and Anti-aliased CNN  [55] .\n\nAlso, the authors introduce Focal Wing Loss, an extension of Adaptive Wing loss, that assigns greater weights to image scenes less presented in the current training batch. The examples of image scenes are large head pose, exaggerated expression, etc. The focus function σ (c) n for class c and sample n is defined as:\n\nwhere s SAAT  [14]  is a heatmap-based approach, which uses a stack of 2 Hourglass modules as a backbone. The authors propose augmenting the training set with adversarial images. The network architecture is left unchanged. In contrast to GEAN, only training procedure is modified, no artificially changed images are generated at test-time. Conditional GAN is used to perform the adversarial attack.\n\nLDDMM-Face  [56]  is a shape model approach, which uses HRNet-18 as a backbone. The primary focus of the work is on cross-dataset and sparse-to-dense annotation. Sparse-to-dense means, that the network can be trained on sparse face landmarks, and then it predicts dense landmark annotations. The authors estimate shape model deformation via large deformation diffeomorphic metric mapping (LD-DMM)  [57] ,  [58]  method. While cross-dataset landmark annotation is out of scope of this survey, this method is also capable of achieving good results for classical face landmark datasets.\n\nAnchorFace  [59]  is a direct regression method. Two modified backbones have been considered: ShuffleNet-V2 (with faster inference), HRNet-18 (with better accuracy). HRNet results are present only for WFLW dataset. The authors tackle the problem of landmark prediction for images with large pose. For that they propose to configure a set of anchor templates for faces with different poses. Anchor templates are configured either manually or via KMeans clusterization on the dataset. Then the templates are refined with a network that predicts offsets and confidence of each of the anchor templates.\n\nPIPNet  [60]  is a combined heatmap and direct regression approach. Several backbones have been considered: MobileNetV2, MobileNetV3, ResNet-18, ResNet-101, etc. In PIPNet it has been noted, that heatmap-based methods have high computational cost, but good accuracy. To alleviate the cost, the authors propose 3-head network. First head predicts coarse landmark heatmaps at lower than usual resolution. Second head predicts regression offsets. Thus, fine-tuning heatmap-based predictions. Third fine-tunes landmark predictions further by regressing offsets relative to the neighboring landmarks. All the heads share the same backbone and are computed in parallel. Additionally, \"self-learning with curriculum\" method has been introduced, where the authors try to learn on 300-W and then self-learn on other facial landmark datasets.\n\nADNet  [61]  is a heatmap-based approach. It uses a stack of 4 Hourglass modules as a backbone. The work is based on LAB and PropagationNet. Facial landmark datasets are annotated by humans. Thus, there exists some annotation error. It turns out, that error in tangent direction (relative to face boundary) is much higher, than in normal direction. Loss functions of existing algorithms do not account for such difference in annotation error. To mitigate the problem, anisotropic direction loss (ADL) is introduced, where higher weight is assigned to normal error. Also, Point-Edge heatmaps are presented, that are used as attention mask. Point-Edge heatmaps are predicted after each Hourglass module and have greater than zero values around face boundary corresponding to a landmark. This is shown in Fig.  11a . Note landmark center accentuation shown in red. Sum of all Point-Edge heatmaps forms face boundary information (Fig.  11b ). Landmark locations are obtained through soft-argmax. The final loss function that is used to train the model consists of a sum of AWing loss for Point and Edge heatmaps, as well as ADL loss for landmark heatmaps. HIH  [62]  and SubpixelHeatmap  [63]  are heatmapbased approaches. Both use a stack of 2 Hourglass modules as backbones. The algorithms focus on reducing heatmap quantization error. Input image and landmark annotations are of resolution 256 × 256. However, the heatmap is typically of size 64 × 64 per landmark, which is 1/16 th of the source image resolution. The landmark location is then found using argmax. The process of mapping floating point landmark location to a discrete grid is called quantization.\n\nHIH. The authors tackle the problem by splitting heatmaps into integer and decimal. Integer heatmap is predicted via the usual heatmap-based facial landmark prediction pipeline. Then another decimal heatmap block predicts a precise offset to the quantized landmark locations. Two ways to predict the offset have been considered: based on Convolutional Neural Networks and Transformers  [64] , denoted as HIH C and HIH T correspondingly.\n\nSubpixelHeatmap. To alleviate the above-described quantization problem, the authors propose a different approach, namely local soft-argmax computation. For a given heatmap H k , k th landmark location is first found as usual via: (ŷ (∆ŷ  (8)  where τ = 10 is the temperature, d = 5 is the suggested region size. Then the final landmark location is found using: (ŷ\n\nk denote landmark position on X and Y axes correspondingly, and l = d/2. The authors compare this approach to global soft-argmax (i.e., computed over the whole image) applied to pose estimation in  [65] , and state that local soft-argmax yields much better results.\n\nAlso, the authors apply test-time augmentation to improve network performance. They feed 2 images with different random augmentations (T 0 , T 1 ) through the network Φ aggregating the final heatmap as follows:\n\n, where θ is the network Φ parameter matrix. In addition, Hourglass architecture has been modified following FAN algorithm.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Facial Landmark Detection Algorithms: Summary",
      "text": "In this section we present and discuss facial landmark detection algorithm performance on the most widely used datasets: 300W, AFLW, COFW and WFLW. We summarize backbones used, and inference times on desktop and mobile devices. We present brief summary of contributions of each facial landmark detection method. We conclude this section with analysis of algorithm performance by years and per algorithm type.\n\nTables 4 to 8 present facial landmark detection method metrics on the most common datasets. The the best result is shown in red, second best is shown in blue. Metrics in the tables include NME (%), Failure Rate (FR, %) and CED-AUC. Table  9  has method backbones and inference times listed. Different hardware was used for algorithm inference speed measurements. So instead of defining first and second fastest, we show all algorithms that perform faster than 60 frames per second (17 ms) in green. Note, that in addition to face detection, other algorithms will need to be executed, that is why the threshold is so strict. The tables are filled based on the results presented in the corresponding papers. If the result was published later, the metric's source is shown in square brackets. Table  10  has a brief summary of algorithm novelties proposed in each paper.\n\nWe present 300W dataset results normalized by both inter-ocular distance in Table  4  and inter-pupil in Table  5 . Metrics are split into common, challenge, and full as per protocol. Can be noticed, most novel algorithms use interocular distance normalization. As is shown in Table  4 , error on challenging subset is still quite high (3.99 %) and is significantly higher than the best common subset error (2.53 %). From Table  5  we note that Wing neural-networkbased algorithm with ResNet-50 backbone is 1.7 times better, than regression-tree-based ERT.\n\nAFLW results are shown in Table  6 . NME (%) normalized by face bounding box diagonal is used to present the results. Errors are on average smaller than on 300W possibly as AFLW has fewer landmark to be annotated (21 vs 68 in 300W), and due to different normalization. Face diagonal is larger than inter-ocular distance.\n\nCOFW results are presented in Table  7 . The results in papers are presented either normalized by inter-pupil or inter-ocular (majority) distance, but not both, which makes direct comparison more difficult. NME (%) and Failure Rate (FR, %) are used to present the results. Interestingly, novel approaches have FR equal to 0.0, which means that no images in the test set have NME above 10% as follows from Eq. 2.\n\nWFLW is the most recent and interesting dataset in this survey. Results are presented in Table  8 . We show NME (%), failure rate (FR, %) and CED-AUC (denoted as AUC in the table) for the whole test set. Note, that lower values of NME and FR are better. In contrast, higher AUC values are better. We also present errors on all types of challenging image categories present in WFLW dataset: Pose, Expression (Expr.), Illumination (Ill.), Make-Up (M.U.), Occlusion (Occ.) and Blur. In Fig.  12  box plots for different image categories are shown. The plot is based on NME for all algorithms present in Table  8 . We note significant difference in NME for different subsets. The most significant challenge to the landmark detection datasets comes from large pose (best error is still at 6.56 %), followed by occlusion (4.36 %) and blur (4.21 %). In contrast, from make-up (3.62 %), illumination (3.87 %) and expression (3.87 %) comes the least challenge. Unlike COFW, failure rate is still quite high (1.55 %) on the test set. While a factor of 1.6 improvement has been achieved on large pose subset over the past 3 years, further improvement is welcome. We see this dataset as the one posing the most interest for novel research. Due to complexity of manual dense facial landmark annotation, the datasets are quite small. Thus, additional training data has a significant impact on model accuracy. We group additional data used into 3 main groups: 1) backbone pretraining on ImageNet; 2) usage of extra image labels (such as image scene); 3) pretraining on other face-related datasets. SAN, DAG, PIPNet, Wing (ResNet-50 only) state that they use ImageNet-pretrained backbones. Hence, they are related to the first group. Second group with PFLD, PropagationNet, annotates images manually with categories (i.e., significant pose, emotion) to assign higher weights to rare categories. Also, PropagationNet and ADNet (focal loss modification only) use weighted loss based on image classes directly available from WFLW dataset. The final third group of algorithms uses extra face-related data during training. GEAN uses pretrained face recognizer to perform an adversarial attack on; certain modifications of LAB use pretrained boundary module on 300W dataset and report results on COFW and AFLW; PFLD (1X+ modification only) is pretrained on WFLW and then reported on 300W. We denote face-data-based pretraining (3 rd category) with extra data label. We do not highlight such results as the best result; however, the data is still present in Tables  4 to 8 . Note the significant positive impact of LAB boundary module pretraining for AFLW and COFW dataset performance in Tables  6  and 7  correspondingly.\n\nIn Table  9  we present algorithm backbones, number of network parameters, floating-point operations, and inference times on desktop computers (CPU, GPU) and mobile phones. Hourglass and CU-Net backbones are typically stacked. We use N ×Hourglass to denote a stack of N Hourglass modules. Number of parameters translates to device memory consumption, which is especially important for mobile and edge devices. Gigaflops (GFlops) is   The final Table  10  presents a summary of facial landmark detection methods, where we show algorithm type, main contribution and notes on algorithm applicability and performance. We use the following abbreviations for algorithm type: D is direct regression, H is heatmap-based regression, SM is shape model, H + D indicates combined methods that use both heatmap and direct regression at different stages. Note that all of the recent neural-network-based facial landmark detection algorithms clearly show, that information explicitly present in the dataset is insufficient. To solve this problem several approaches are proposed:\n\n• use of an auxiliary representation, which contains structural information about the face, such as: 3D face mesh (DeFA); deformable shape model (LDDMM-Face); graph-based message passing (LAB); yaw, pitch, roll rotation angles (PFLD); landmark visibility (LUVLi); face representation as a graph model (DAG); offsets to anchors defined for faces with different poses (AnchorFace) or offset regression from neighboring landmarks (PIP);\n\n• boundary representation either explicitly (LAB) or via attention module (PropagationNet, ADNet);\n\n• hard example mining during training. Different variations on the theme have been presented in Wing, PFLD and PropagationNet papers;\n\n• aggregating predictions for multiple input images: with style modification (SAN); after adversarial attack (GEAN); or with several different augmentations (SubpixelHeatmap). As it has been noted, minor changes in image might result in major shift in landmark prediction, averaging such predictions results in improved accuracy;\n\n• train set augmentation using style (AVS) or adversarial attacks (SAAT);\n\n• reduction of very large errors (outliers) and increased contribution of small to medium-sized errors (to better refine predictions): Wing, AWing and derivate works; NME of 4.21 % is achieved by DAG algorithm (direct prediction based on graphs), the best heatmap-based NME is much lower at 3.72 %. This is also state-of-the-art result, that is achieved by SubpixelHeatmap. Note, significantly higher COFW error for direct approaches. This happens because only a single direct regression algorithm has been tested on COFW, that is Wing algorithm. The approach is quite old. Hence, this error spike should be considered as an outlier. Next, we take a look at combined heatmap-direct approaches. There are only two of them, LAB (which is quite old) and PIPNet. PIPNet proposed to predict coarse heatmap and then refine it via direct regression. While the idea is quite promising, the algorithm offers worse performance than direct or heatmap-based approaches. Finally, neural-network-based shape model algorithms offer the worst performance. Such approaches are quite useful, when network training is performed on images with different annotation schemes, or when 3D face mesh is required. Note, in this survey only DeFa algorithm produces 3D mesh. However, for most other use cases such approaches should not be considered.\n\nIn Fig.  14  we show the best inference time achieved by algorithms of each type. We do not visualize results for shape-model-based approaches as no timings have been presented in the corresponding papers. As expected, the fastest approaches use direct regression. They typically have more lightweight backbones. Also, they do not need to predict large heatmaps for each of the landmarks, which saves computation. Heatmap-based approaches have the best GPU inference time at 4.0 ms (achieved by MobileFan (0.5)), which is worse than 1.2 ms achieved by direct approach (PFLD 0.25X). While the timings seem to be quite small, note that both approaches offer significantly lower accuracy, than state-of-the-art algorithms. State-of-the-art algorithms still execute around 100 ms on GPU. Only GPU timings are available for heatmap-based approaches. Finally, we show timings for combined heatmap and direct regression methods. The best results achieved by PIPNet with ResNet-18 backbone. While the backbone is quite lightweight, inference time of 28.0 ms on CPU and 5.0 ms on GPU for this model is quite high. We would have expected the timings to be lower here.\n\nIn Fig.  15  we show the best algorithm performance grouped by dataset and year. To begin with, we discuss 300W dataset results, which are presented for Full and Challenge sets. We note that no progress has been made in 2021 in comparison to 2020. We expect error of all algorithms to stop decreasing at some point, as both training and test sets contain annotation errors. Thus, it would be interesting to see, whether any progress is made in 2022, or the remaining algorithm error is due to incorrect annotation. On COFW significant progress has been made over years. And WFLW dataset is still the most challenging. WFLW dataset has been introduced in 2018, and the largest improvement has been achieved in the following year, which is especially obvious on Pose subset, that has the highest normalized mean error. As discussed before, annotation of faces with unusual Make-Up has the least challenge to facial landmark algorithms, but NME has decreased even on this subset. Moving over to AFLW, in Fig.  16  we show AFLW performance over years. The performance is still being im- proved. We have deliberately left NME for algorithm from 2014 to note the significant progress made in 7 years.\n\nTo conclude this section, we would like to note the best algorithms on different datasets: SubpixelHeatmap, AD-Net, PropagationNet. All of these algorithms are heatmapbased. The key ideas proposed in them are complimentary, for instance, SubpixelHeatmap has offered a way to decrease heatmap quantization error. Recall, that input image and landmark annotations are of resolution 256 × 256, while the heatmap is only 64 × 64. The authors of ADNet have presented Point-Edge heatmaps as attention masks, and PropagationNet has presented Focal Wing Loss modification. However, inference time of ADNet is still quite high at 95.29 ms on GPU. We expect PropagationNet to be even slower, based on presented in Table  9  number of floating point operations. SubpixelHeatmap neural network is inferred for each image several times, which will also be slow. Thus, we would like to see faster algorithms in future, and those that are easily applicable to mobile devices.",
      "page_start": 13,
      "page_end": 20
    },
    {
      "section_name": "Facial Landmark Detection: Applications",
      "text": "",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Mobile-Friendly Joint Face And Landmark Detection",
      "text": "As we have noted previously, face landmark detection is one face processing pipeline steps. To actually get a dense landmark annotation, face has to be detected first and cropped based on its bounding box. In this section we present some of mobile-friendly face detection methods. Interestingly, these methods also predict coarse (5 or 6) face landmarks, such as eyes, mouth, nose.\n\nMulti-task Cascaded Convolutional Networks (MTCNN)  [66] . The neural network is trained jointly to detect faces and landmark locations (five of them, to be precise: eyes, tip of the nose, mouth corners), which improves quality on both tasks. The network is built in a form of a three-network cascade: Proposal Network (P-Net), Refine Network (R-Net), Output Network (O-Net). Each network predicts face bounding rectangle, probability that a particular rectangle contains a face, and five landmarks. P-Net is a fast fully convolutional network, which processes the original image in multiple resolutions (the so-called image pyramid). This network outputs a lot of coarse face rectangle predictions, which are then filtered out by the Non-Maximum Suppression (NMS) algorithm. Subsequently, R-Net refines the predicted rectangles. It does so without reprocessing the whole image, which saves computation time. NMS is then applied again. Last, O-Net makes the final refinement. This is the slowest network in the cascade, but it processes a small number of face rectangles. According to the authors, to improve quality it is important to solve the following tasks at the same time: 1) classify bounding rectangle as a face or not a face; 2) perform regression over bounding rectangle coordinates; 3) localize face landmarks. Each task has a weight α assigned: for P-Net and R-Net BlazeFace  [67]  is a novel approach to joint face and landmark detection. 6 landmarks are predicted: eye center, ear tragions, mouth center, and nose tip). The algorithm was specifically designed for inference on mobile devices. The authors claim sub-second detection time on mobile for Tensorflow  [68]  GPU implementation. The approach is based on Single Shot Detector (SSD)  [69]  with MobileNetV2 backbone. The authors propose to modify MobileNetV2 to improve performance to accuracy ratio. For that they increase complexity of Bottleneck block (main building block of MobileNetV2 architecture) and decrease the number of such blocks at the same time. Also, they have optimized SSD architecture for face detection by removing the ability to predict wide or tall bounding boxes, that are not common for faces. In addition, intra-frame jitter produced by the NMS algorithm has been reduced via a separate bounding box regression module. The network is proposed in 2 configurations: one for pictures taken on back camera (typically smaller faces) and another for frontal camera photos (typically larger faces). While the network has good accuracy, the input resolutions are fixed to 128 × 128 or 256 × 256, which is a disadvantage of the method. MTCNN, for exam-Figure  15 . State-of-the-art normalized mean error (NME, %) on 300W (Full, Challenge), COFW, WFLW (All, Make-Up, Pose) by years. Score on 300W dataset doesn't improve in 2021. Error on WFLW dataset, in contrast, significantly decreases with the time. WFLW error is higher (especially on images with large pose), than that of 300W and COFW, which makes it the most challenging dataset. ple, can take images of arbitrary resolution as input. Note, that training has been performed on a closed dataset. Thus, it is not possible to reproduce the results.\n\nA comprehensive review of other modern face detection methods is presented in  [70] . We see the following prospects for further research in the field of joint face and landmark detection:\n\n• inference speed to accuracy ratio requires improve-ment. Faster approaches often have lower quality;\n\n• large annotated dataset is required to train the model.\n\nIf the dataset is biased (unbalanced) in race or gender, face detection accuracy of underrepresented groups will typically suffer.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Face Animation And Reenactment",
      "text": "Facial landmark detection is used in human or imaginary character face animation algorithms. Applications include actor animation in movies, creation of TV or game virtual newscasters (as a 3D model or directly via GAN image generation). Recent landmark detection algorithms enable this without costly equipment by using a simple RGB camera.\n\nAccording to research presented in a series of papers, movie dubbing process from foreign languages is expensive and time-consuming. This is because lip movement for the original and dubbed audio tracks should match. Furthermore, the movement discrepancy leads to discomfort when watching movies, especially for hearing-impaired people. As a solution, authors of  [71]  propose to change lip movement during the dubbing process. Their algorithm detects facial landmarks and substitutes mouth region with a 3D model, adapted for the speaker. However, at this stage the substitution is still visible. Besides that, DeFA algorithm can build a 3D whole-face mesh for varied poses and emotions, as has been said previously.\n\nMany of the recent neural-network-based algorithms do not use an intermediate 3D face model for realistic image generation, but generate images directly from facial land-mark locations via Generative Adversarial Network (GAN). For instance, the authors of  [72]  by using MAML  [73]  meta-learning approach, GAN and the so-called perceptual loss  [74] , obtain high face reenactment quality (Fig.  17 ). Landmark information extracted from an image is one of the neural network inputs. FAN algorithm is used to extract the landmarks. The algorithm has some disadvantages though. For instance, when actor, that drives the animation, has significantly different face shape from animated face, the resulting animation is unrealistic and contains artifacts. According to the authors' report, this method outperforms the competition for face emotion transfer task in fewor one-shot problem statement. The authors note, that an improvement of facial extraction algorithm and addition of gaze direction might have improved the reenactment quality. In  [75]  authors are using Pix2PixHD  [76]  neural network to accomplish lip sync task. It has been proposed to synthesize the intermediate face representation using its boundaries, face landmarks (using Dlib library) and sound-trackbased representation.\n\nIn another work FReeNet  [77]  algorithm is presented for reenactment between different (unknown during training) people. For that a special Unified Landmark Converter module has been introduced, which adapts facial landmark coordinates between different people. Landmarks for the source and target people are extracted via PFLD algorithm. Then images are generated via Cycle-GAN  [42]  and a special loss function. The use of landmark converter module has given the largest performance increase on the test sets.\n\nA survey of emotion transfer and face reenactment methods can be found in  [78]  Section \"Expression Swap\". Most recent algorithms, that focus on face animation of real people use generative adversarial neural networks (GANs). Currently, these approaches have the following limitations, that require a solution in future, for instance:\n\n• videos produced by neural networks lack temporal stability. For instance, face animation might jitter, artifacts may appear on the screen;\n\n• face animation under large pose might cause unrealistic face deformation;\n\n• animation quality significantly suffers, if source character and animation driver have different face shape.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Driver Status Tracking",
      "text": "A large number of car accidents happens because-of sleepy or tired drivers. Expensive cars offer capabilities of emergency stopping when an obstacle is detected, and linekeep assist. Mainstream cars do not have such features. In both cases, it is better to track driver status and stop the car early, than to apply emergency brakes. Most of the research in the field is focused on implementing status tracking in an autonomous way (without Internet connection). Driver's smartphone or low-power portative device (such as Raspberry Pi) is used to process video signal from a camera placed in a car's cabin. Neural-network-based algorithms are among the most widely used approaches here.\n\nIn  [79]  the authors estimate driver tiredness via a neural network that takes facial landmarks as an input. Driver's face and landmarks are detected with existing methods. In contrast, in  [80]  a MobileNetV2-based architecture is presented to estimate driver's sleepiness directly from the video stream (without an intermediate step of landmark detection), yet total training time is quite high. In  [81]  neuralnetwork-based landmark detection is utilized to simplify dataset labelling, then a different network is trained to recognize driver's status. In addition to fatigue, the authors also estimate driver's distraction by tracking whether he looks in safe zones (such road, rear-view mirror, dashboard, etc.) or not. In  [82]  a system that tracks driver's ability to take over the driving from level 2 autonomous cars (partial driving automatization) is studied. The authors acquire driver's video via an infrared camera. Decision whether the driver is distracted is based on the detected landmarks. These and similar algorithms are developed to make the roads safer.\n\nSpecial hardware can also be used to track driver status  [79] ,  [80] , for instance, tracking of driving wheel movement; wearable devices that perform Electrocardiography (ECG) and heartbeat measurements. However, both of these approaches are more expensive and cannot track driver's distraction from the road.\n\nNeural-network-based driver status tracking algorithms have the following limitations, for instance:\n\n• achieving sufficiently fast inference on a mobile device is a challenge;\n\n• driver status tracking is often performed at night-time when driver is poorly illuminated. Facial landmark detection accuracy suffers in such conditions, especially given limited computation power. This is why development of mobile networks and landmark detection algorithms will definitely enhance the quality of driver status tracking systems.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Face Recognition And Emotion Classification",
      "text": "To begin with, we briefly talk about algorithms that perform one of the following tasks (often, the same algorithm can perform all of them): 1) face verification, when 2 pictures are given and the task is to say whether they contain the same person; 2) face recognition: given a photo and a known person database, algorithm should say who is on the photo or that the person is unknown; 3) clusterization, where the task is to group similar faces. The most efficient algorithms use face preprocessing, that is face detection and tight crop. Often for improving recognition quality the so-called \"face alignment\" should additionally be performed, that is a geometrical image transformation, when facial landmarks are moved to the canonical locations. Many of the modern algorithms use MTCNN for joint face detection and localization of 5 landmarks. The topic of face recognition is well-described in, for example,  [83] . We note in particular, high interest to face recognition directly on mobile devices  [84] ,  [85] . Also, we discuss emotion recognition. Our emotions mostly consist of lip, eyes, eyebrows or mouth movements. That is why in certain cases it is fruitful not to force the neural network to learn face parts during emotion recognition on its own, but to feed this information detected by another algorithm together with the original image  [86] ,  [87] .\n\nThe field of face recognition has several problems that require a solution in future, for example:\n\n• face recognition when photos represent a person of different ages;\n\n• when face occlusion is significant, which is especially important when medical masks have become common;\n\n• faces with large pose and emotion;\n\n• also, some of the backbones discussed here in a context of facial landmark detection are used for face recognition and emotion classification. Thus, improving neural network backbones is important as well.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Facial Landmark Detection: Vulnerabilities",
      "text": "Modern computer vision algorithms (including neural networks) are amenable to the so-called \"adversarial attacks\", first reported in the field of computer vision in  [88] . The authors were able to drastically change neural network prediction in classification task by adding especially crafted noise (invisible to human eye) to an image. The attack has been conducted by maximizing network error on the target image via L-BFGS method. Images with adversarial noise are almost always misclassified on MNIST dataset. It should be stressed that during the adversarial attack the network itself is not modified, only the images fed to it. Moreover, adversarial examples often remain malicious to networks different from the one they were crafted for, given that another network was trained on the same or similar dataset. It should be noted, that adding random noise has much lower negative effect on the network's classification accuracy, than adversarial attack noise. In  [89]  it has been shown that for a successful adversarial attack on the MNIST dataset, model as simple as logistic regression can be used to generate adversarial examples. The attack remains efficiently transferrable to architectures, that are more complicated.\n\nIf previous algorithms have attacked a digital image (stored in computer memory), in  [90]  it has been shown that attacks can be performed through a smartphone camera. In  [91]  binary importance maps have been introduced, which hint where adversarial marks should be placed on a piece of paper to fool the network trained to classify handwritten digits. The first adversarial attacks were white-box, i.e., the network architecture and trained weights are known to the attacker. Follow-up works similar to  [92]  and others have shown that it is possible to perform black-box attacks without such knowledge. Despite the fact that numerous works are devoted to detecting or preventing attacks from happening, new more advanced algorithms bypass all of the defense methods  [93] . A survey of adversarial attack methods can be found in  [94] . All of them are applicable to algorithms of face or facial landmark detection.\n\nIn the meantime, there exist special methods that can prevent the face from being found or correctly detected by using stickers or accessories in real world. In  [95]  it has been shown, that in a controllable environment it is possible to fool face recognition algorithm or Viola-Jones face detector. The authors used special eyeglasses with a print on a frame. In  [96]  it has been proposed to fool MTCNN face detection algorithm with the use of stickers on cheeks or medical mask. In cases when the face cannot be detected, landmark localization cannot be performed either. Face recognition adversarial attack based on facial landmarks is presented in  [51] .",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Conclusion",
      "text": "From a detailed survey, we see the following facial landmark detection algorithm problems, that require a solution in future research: 1) despite a significant growth of methods' quality, few of them focus on the real-world applicability in resource-constrained environments, such as mobile or edge devices; 2) many applications require high performance on mobile or portable devices, yet to the best of our knowledge, authors of only a single algorithm have targeted a mobile application directly in the original paper. Note that state-of-the-art algorithms have slow inference speed; 3) while modern research already focuses on datasets in uncontrollable environments, a promising research direction is to enhance algorithms in even harsher conditions, for images with large pose and significant face occlusion, while still maintaining high landmark density. Error of current generation of algorithms in these conditions is quite high. We see WFLW dataset as the one posing the most interest for further research. Also, it would be desirable to see more of the novel facial landmark detection algorithms to report their inference speed on desktop GPU, and if possible, on mobile devices.\n\nWe hope, that the described modern developments in all of the sections will lead the reader to new ideas of practical use and further research directions.",
      "page_start": 23,
      "page_end": 23
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Also, dataset deﬁnes which images are used to train",
      "page": 2
    },
    {
      "caption": "Figure 1: Examples of faces annotated with facial landmarks from",
      "page": 2
    },
    {
      "caption": "Figure 2: illustrates the two approaches.",
      "page": 4
    },
    {
      "caption": "Figure 3: Note, that archi-",
      "page": 5
    },
    {
      "caption": "Figure 3: , after each convolutional block the",
      "page": 5
    },
    {
      "caption": "Figure 3: ). The training loss includes weighted sum of",
      "page": 5
    },
    {
      "caption": "Figure 2: Direct landmark regression (upper row). The problem is solved in a form of regression, where actual landmark coordinates",
      "page": 6
    },
    {
      "caption": "Figure 3: Hourglass architecture. Input image of size 256 × 256 is rescaled to 64 × 64 in Preamble (orange). Hourglass encoder-decoder",
      "page": 6
    },
    {
      "caption": "Figure 5: b (more pa-",
      "page": 6
    },
    {
      "caption": "Figure 4: HRNet architecture. Input image of size 256 × 256 is rescaled to 64 × 64 in Preamble (orange). Next, input is processed",
      "page": 7
    },
    {
      "caption": "Figure 6: ); 2) DeFA can be trained jointly on datasets with",
      "page": 7
    },
    {
      "caption": "Figure 5: Comparison of different backbones by the number of ﬂoating-point operations (a) and the number of parameters (b). Note that",
      "page": 8
    },
    {
      "caption": "Figure 6: (a) DeFA dense 3D face mesh produced for different",
      "page": 8
    },
    {
      "caption": "Figure 7: ). Such a trick improves facial landmark",
      "page": 8
    },
    {
      "caption": "Figure 7: LAB: (a) image to be labeled; (b) ﬁrst module predicts",
      "page": 9
    },
    {
      "caption": "Figure 8: Also, to train more on hard examples the authors intro-",
      "page": 9
    },
    {
      "caption": "Figure 8: Loss function comparison: L2, L1, smoothL1, Wing",
      "page": 9
    },
    {
      "caption": "Figure 9: ): 1) facial land-",
      "page": 9
    },
    {
      "caption": "Figure 9: PFLD architecture. MobileNetV2 is used as feature ex-",
      "page": 10
    },
    {
      "caption": "Figure 10: AWing surface plot. AWing accepts true y and pre-",
      "page": 10
    },
    {
      "caption": "Figure 11: a. Note landmark center accentuation shown in red.",
      "page": 12
    },
    {
      "caption": "Figure 11: b). Landmark locations are obtained through",
      "page": 12
    },
    {
      "caption": "Figure 11: ADNet learns additional semantical information about",
      "page": 12
    },
    {
      "caption": "Figure 12: box plots for different",
      "page": 13
    },
    {
      "caption": "Figure 13: we show the best normalized mean error",
      "page": 17
    },
    {
      "caption": "Figure 12: Box plots that show Normalized Mean Error (NME,",
      "page": 19
    },
    {
      "caption": "Figure 14: we show the best inference time achieved",
      "page": 19
    },
    {
      "caption": "Figure 13: The best normalized mean error for each algorithm",
      "page": 19
    },
    {
      "caption": "Figure 15: we show the best algorithm performance",
      "page": 19
    },
    {
      "caption": "Figure 16: we show AFLW",
      "page": 19
    },
    {
      "caption": "Figure 14: The best inference time for each algorithm type: di-",
      "page": 20
    },
    {
      "caption": "Figure 15: State-of-the-art normalized mean error (NME, %) on 300W (Full, Challenge), COFW, WFLW (All, Make-Up, Pose) by years.",
      "page": 21
    },
    {
      "caption": "Figure 16: Algorithm Normalized Mean Error (NME, %) on",
      "page": 21
    },
    {
      "caption": "Figure 17: Reenactment scheme. (a) source character image (the",
      "page": 22
    }
  ],
  "tables": [
    {
      "caption": "Table 1: summarizes information about common datasets.",
      "page": 4
    },
    {
      "caption": "Table 1: Information about facial landmark datasets: number of",
      "page": 4
    },
    {
      "caption": "Table 2: However, despite the improvements, most",
      "page": 5
    },
    {
      "caption": "Table 2: Comparison of number of parameters and inference time",
      "page": 5
    },
    {
      "caption": "Table 9: 4.3. Facial Landmark Detection: Novel Algorithms",
      "page": 7
    },
    {
      "caption": "Table 3: , using CNN-6/7 cascade with",
      "page": 9
    },
    {
      "caption": "Table 3: NME comparison of different loss functions on AFLW",
      "page": 9
    },
    {
      "caption": "Table 9: has method backbones and inference times",
      "page": 13
    },
    {
      "caption": "Table 10: has a brief summary",
      "page": 13
    },
    {
      "caption": "Table 4: and inter-pupil in Table 5.",
      "page": 13
    },
    {
      "caption": "Table 5: we note that Wing neural-network-",
      "page": 13
    },
    {
      "caption": "Table 6: NME (%) normal-",
      "page": 13
    },
    {
      "caption": "Table 7: The results",
      "page": 13
    },
    {
      "caption": "Table 8: We note signiﬁcant dif-",
      "page": 13
    },
    {
      "caption": "Table 4: Face landmark detection normalized mean error (NME) on 300-W dataset. Inter-ocular normalization is used. The best result",
      "page": 14
    },
    {
      "caption": "Table 9: we present algorithm backbones, number of",
      "page": 14
    },
    {
      "caption": "Table 5: Face landmark detection normalized mean error (NME) on 300-W dataset. Inter-pupil normalization is used. The best result is",
      "page": 15
    },
    {
      "caption": "Table 6: Face landmark detection normalized mean error (NME)",
      "page": 15
    },
    {
      "caption": "Table 7: Face landmark detection normalized mean error (NME)",
      "page": 15
    },
    {
      "caption": "Table 8: Face landmark detection normalized mean error (NME), failure rate (FR), CED-AUC on WFLW. Normalization by inter-ocular",
      "page": 16
    },
    {
      "caption": "Table 10: presents a summary of facial landmark",
      "page": 16
    },
    {
      "caption": "Table 9: Comparison of neural network backbones, computational complexity and inference speed of facial landmark detection algorithms.",
      "page": 17
    },
    {
      "caption": "Table 10: Face landmark detection method brief summary. The following abbreviations are used for algorithm type: D for direct regression,",
      "page": 18
    },
    {
      "caption": "Table 8: The results are",
      "page": 19
    },
    {
      "caption": "Table 9: number of ﬂoat-",
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices",
      "authors": [
        "X Zhang",
        "X Zhou",
        "M Lin",
        "J Sun"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00716"
    },
    {
      "citation_id": "2",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L.-C Chen"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00474"
    },
    {
      "citation_id": "3",
      "title": "Facial feature point detection: A comprehensive survey",
      "authors": [
        "N Wang",
        "X Gao",
        "D Tao",
        "H Yang",
        "X Li"
      ],
      "year": "2018",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2017.05.013"
    },
    {
      "citation_id": "4",
      "title": "Facial landmark detection: A literature survey",
      "authors": [
        "Y Wu",
        "Q Ji"
      ],
      "year": "2019",
      "venue": "Int. J. Comput. Vision",
      "doi": "10.1007/s11263-018-1097-z"
    },
    {
      "citation_id": "5",
      "title": "Look at boundary: A boundary-aware face alignment algorithm",
      "authors": [
        "W Wu",
        "C Qian",
        "S Yang",
        "Q Wang",
        "Y Cai",
        "Q Zhou"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00227"
    },
    {
      "citation_id": "6",
      "title": "Luvli face alignment: Estimating landmarks' location, uncertainty, and visibility likelihood",
      "authors": [
        "A Kumar",
        "T Marks",
        "W Mou",
        "Y Wang",
        "M Jones",
        "A Cherian",
        "T Koike-Akino",
        "X Liu",
        "C Feng"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR42600.2020.00826"
    },
    {
      "citation_id": "7",
      "title": "Adaptive wing loss for robust face alignment via heatmap regression",
      "authors": [
        "X Wang",
        "L Bo",
        "L Fuxin"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.00707"
    },
    {
      "citation_id": "8",
      "title": "300 faces in-the-wild challenge: The first facial landmark localization challenge",
      "authors": [
        "C Sagonas",
        "G Tzimiropoulos",
        "S Zafeiriou",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision Workshops",
      "doi": "10.1109/ICCVW.2013.59"
    },
    {
      "citation_id": "9",
      "title": "Localizing parts of faces using a consensus of exemplars",
      "authors": [
        "P Belhumeur",
        "D Jacobs",
        "D Kriegman",
        "N Kumar"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2013.23"
    },
    {
      "citation_id": "10",
      "title": "Face detection, pose estimation, and landmark localization in the wild",
      "authors": [
        "X Zhu",
        "D Ramanan"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2012.6248014"
    },
    {
      "citation_id": "11",
      "title": "Interactive facial feature localization",
      "authors": [
        "V Le",
        "J Brandt",
        "Z Lin",
        "L Bourdev",
        "T Huang"
      ],
      "year": "2012",
      "venue": "Computer Vision -ECCV 2012 -12th European Conference on Computer Vision",
      "doi": "10.1007/978-3-642-33712-3\\_49"
    },
    {
      "citation_id": "12",
      "title": "Xm2vtsdb: The extended m2vts database",
      "authors": [
        "K Messer",
        "J Matas",
        "J Kittler",
        "J Luettin",
        "G Maître"
      ],
      "year": "1999",
      "venue": "Xm2vtsdb: The extended m2vts database"
    },
    {
      "citation_id": "13",
      "title": "Face alignment across large poses: A 3d solution",
      "authors": [
        "X Zhu",
        "Z Lei",
        "X Liu",
        "H Shi",
        "S Li"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2016.23"
    },
    {
      "citation_id": "14",
      "title": "Improving robustness of facial landmark detection by defending against adversarial attacks",
      "authors": [
        "C Zhu",
        "X Li",
        "J Li",
        "S Dai"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "15",
      "title": "Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization",
      "authors": [
        "M Köstinger",
        "P Wohlhart",
        "P Roth",
        "H Bischof"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)",
      "doi": "10.1109/ICCVW.2011.6130513"
    },
    {
      "citation_id": "16",
      "title": "Aggregation via separation: Boosting facial landmark detector with semi-supervised style translation",
      "authors": [
        "S Qian",
        "K Sun",
        "W Wu",
        "C Qian",
        "J Jia"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.01025"
    },
    {
      "citation_id": "17",
      "title": "Robust face landmark estimation under occlusion",
      "authors": [
        "X Burgos-Artizzu",
        "P Perona",
        "P Dollár"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Computer Vision",
      "doi": "10.1109/ICCV.2013.191"
    },
    {
      "citation_id": "18",
      "title": "Occlusion coherence: Detecting and localizing occluded faces",
      "authors": [
        "G Ghiasi",
        "C Fowlkes"
      ],
      "year": "2015",
      "venue": "CoRR",
      "arxiv": "arXiv:1506.08347"
    },
    {
      "citation_id": "19",
      "title": "The menpo facial landmark localisation challenge: A step towards the solution",
      "authors": [
        "S Zafeiriou",
        "G Trigeorgis",
        "G Chrysos",
        "J Deng",
        "J Shen"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "20",
      "title": "The menpo benchmark for multi-pose 2d and 3d facial landmark localisation and tracking",
      "authors": [
        "J Deng",
        "A Roussos",
        "G Chrysos",
        "E Ververas",
        "I Kotsia",
        "J Shen",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks)",
      "authors": [
        "A Bulat",
        "G Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2017.116"
    },
    {
      "citation_id": "22",
      "title": "International Conference on Computer Vision (ICCV) Workshops",
      "authors": [
        "S Zafeiriou",
        "G Chrysos",
        "A Roussos",
        "E Ververas",
        "J Deng",
        "G Trigeorgis"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision (ICCV) Workshops"
    },
    {
      "citation_id": "23",
      "title": "Offline deformable face tracking in arbitrary videos",
      "authors": [
        "G Chrysos",
        "E Antonakos",
        "S Zafeiriou",
        "P Snape"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision Workshop, ICCV Workshops",
      "doi": "10.1109/ICCVW.2015.126"
    },
    {
      "citation_id": "24",
      "title": "The first facial landmark tracking in-the-wild challenge: Benchmark and results",
      "authors": [
        "J Shen",
        "S Zafeiriou",
        "G Chrysos",
        "J Kossaifi",
        "G Tzimiropoulos",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision Workshop, ICCV Workshops 2015",
      "doi": "10.1109/ICCVW.2015.132"
    },
    {
      "citation_id": "25",
      "title": "Project-out cascaded regression with an application to face alignment",
      "authors": [
        "G Tzimiropoulos"
      ],
      "year": "2015",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015",
      "doi": "10.1109/CVPR.2015.7298989"
    },
    {
      "citation_id": "26",
      "title": "One millisecond face alignment with an ensemble of regression trees",
      "authors": [
        "V Kazemi",
        "J Sullivan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, ser. CVPR '14",
      "doi": "10.1109/CVPR.2014.241"
    },
    {
      "citation_id": "27",
      "title": "Dlib-ml: A machine learning toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "28",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2001",
      "venue": "2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2001.990517"
    },
    {
      "citation_id": "29",
      "title": "Wing loss for robust facial landmark localisation with convolutional neural networks",
      "authors": [
        "Z Feng",
        "J Kittler",
        "M Awais",
        "P Huber",
        "X Wu"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2018.00238"
    },
    {
      "citation_id": "30",
      "title": "A survey of deep facial landmark detection",
      "authors": [
        "Y Yan",
        "X Naturel",
        "T Chateau",
        "S Duffner",
        "C Garcia",
        "C Blanc"
      ],
      "year": "2018",
      "venue": "RFIAP"
    },
    {
      "citation_id": "31",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "32",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016",
      "doi": "10.1109/CVPR.2016.90"
    },
    {
      "citation_id": "33",
      "title": "Searching for mobilenetv3",
      "authors": [
        "A Howard",
        "R Pang",
        "H Adam",
        "Q Le",
        "M Sandler",
        "B Chen",
        "W Wang",
        "L Chen",
        "M Tan",
        "G Chu",
        "V Vasudevan",
        "Y Zhu"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South)",
      "doi": "10.1109/ICCV.2019.00140"
    },
    {
      "citation_id": "34",
      "title": "Shufflenet V2: practical guidelines for efficient CNN architecture design",
      "authors": [
        "N Ma",
        "X Zhang",
        "H Zheng",
        "J Sun"
      ],
      "year": "2018",
      "venue": "Computer Vision -ECCV 2018 -15th European Conference",
      "doi": "10.1007/978-3-030-01264-9\\_8"
    },
    {
      "citation_id": "35",
      "title": "Stacked hourglass networks for human pose estimation",
      "authors": [
        "A Newell",
        "K Yang",
        "J Deng"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 -14th European Conference",
      "doi": "10.1007/978-3-319-46484-8\\_29"
    },
    {
      "citation_id": "36",
      "title": "Deep highresolution representation learning for human pose estimation",
      "authors": [
        "K Sun",
        "B Xiao",
        "D Liu",
        "J Wang"
      ],
      "year": "2019",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2019.00584"
    },
    {
      "citation_id": "37",
      "title": "Towards efficient u-nets: A coupled and quantized approach",
      "authors": [
        "Z Tang",
        "X Peng",
        "K Li",
        "D Metaxas"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2019.2907634"
    },
    {
      "citation_id": "38",
      "title": "Deep high-resolution representation learning for visual recognition",
      "authors": [
        "J Wang",
        "K Sun",
        "T Cheng",
        "B Jiang",
        "C Deng",
        "Y Zhao",
        "D Liu",
        "Y Mu",
        "M Tan",
        "X Wang",
        "W Liu",
        "B Xiao"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/TPAMI.2020.2983686"
    },
    {
      "citation_id": "39",
      "title": "Dense face alignment",
      "authors": [
        "Y Liu",
        "A Jourabloo",
        "W Ren",
        "X Liu"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision Workshops (ICCVW)",
      "doi": "10.1109/ICCVW.2017.190"
    },
    {
      "citation_id": "40",
      "title": "Style aggregated network for facial landmark detection",
      "authors": [
        "X Dong",
        "Y Yan",
        "W Ouyang",
        "Y Yang"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00047"
    },
    {
      "citation_id": "41",
      "title": "Generative adversarial networks",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "CoRR",
      "arxiv": "arXiv:1406.2661"
    },
    {
      "citation_id": "42",
      "title": "Unpaired image-to-image translation using cycleconsistent adversarial networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2017.244"
    },
    {
      "citation_id": "43",
      "title": "Structured feature learning for pose estimation",
      "authors": [
        "X Chu",
        "W Ouyang",
        "H Li",
        "X Wang"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016",
      "doi": "10.1109/CVPR.2016.510"
    },
    {
      "citation_id": "44",
      "title": "Endto-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation",
      "authors": [
        "W Yang",
        "W Ouyang",
        "H Li",
        "X Wang"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2016.335"
    },
    {
      "citation_id": "45",
      "title": "Fast R-CNN",
      "authors": [
        "R Girshick"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Computer Vision, ICCV 2015",
      "doi": "10.1109/ICCV.2015.169"
    },
    {
      "citation_id": "46",
      "title": "PFLD: A practical facial landmark detector",
      "authors": [
        "X Guo",
        "S Li",
        "J Zhang",
        "J Ma",
        "L Ma",
        "W Liu",
        "H Ling"
      ],
      "year": "1902",
      "venue": "CoRR",
      "arxiv": "arXiv:1902.10859"
    },
    {
      "citation_id": "47",
      "title": "Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources",
      "authors": [
        "A Bulat",
        "G Tzimiropoulos"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Computer Vision, ICCV 2017",
      "doi": "10.1109/ICCV.2017.400"
    },
    {
      "citation_id": "48",
      "title": "An intriguing failing of convolutional neural networks and the coordconv solution",
      "authors": [
        "R Liu",
        "J Lehman",
        "P Molino",
        "F Such",
        "E Frank",
        "A Sergeev",
        "J Yosinski"
      ],
      "year": "2018",
      "venue": "Proceedings of the 32nd International Conference on Neural Information Processing Systems, ser. NIPS'18"
    },
    {
      "citation_id": "49",
      "title": "Mobilefan: Transferring deep hidden representation for face alignment",
      "authors": [
        "Y Zhao",
        "Y Liu",
        "C Shen",
        "Y Gao",
        "S Xiong"
      ],
      "year": "2020",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2019.107114"
    },
    {
      "citation_id": "50",
      "title": "Robust facial landmark detection via aggregation on geometrically manipulated faces",
      "authors": [
        "S Iranmanesh",
        "A Dabouei",
        "S Soleymani",
        "H Kazemi",
        "N Nasrabadi"
      ],
      "year": "2020",
      "venue": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV45572.2020.9093508"
    },
    {
      "citation_id": "51",
      "title": "Fast geometrically-perturbed adversarial faces",
      "authors": [
        "A Dabouei",
        "S Soleymani",
        "J Dawson",
        "N Nasrabadi"
      ],
      "year": "2019",
      "venue": "2019 IEEE Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV.2019.00215"
    },
    {
      "citation_id": "52",
      "title": "Structured landmark detection via topologyadapting deep graph learning",
      "authors": [
        "W Li",
        "Y Lu",
        "K Zheng",
        "H Liao",
        "C Lin",
        "J Luo",
        "C.-T Cheng",
        "J Xiao",
        "L Lu",
        "C.-F Kuo",
        "S Miao"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020"
    },
    {
      "citation_id": "53",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "54",
      "title": "Propagationnet: Propagate points to curve to learn structure information",
      "authors": [
        "X Huang",
        "W Deng",
        "H Shen",
        "X Zhang",
        "J Ye"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020",
      "doi": "10.1109/CVPR42600.2020.00729"
    },
    {
      "citation_id": "55",
      "title": "Making convolutional networks shiftinvariant again",
      "authors": [
        "R Zhang"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "56",
      "title": "Lddmmface: Large deformation diffeomorphic metric learning for flexible and consistent face alignment",
      "authors": [
        "H Yang",
        "J Lyu",
        "P Cheng",
        "X Tang"
      ],
      "year": "2021",
      "venue": "CoRR",
      "arxiv": "arXiv:2108.00690"
    },
    {
      "citation_id": "57",
      "title": "Large deformation diffeomorphic metric curve mapping",
      "authors": [
        "J Glaunès",
        "A Qiu",
        "M Miller",
        "L Younes"
      ],
      "year": "2008",
      "venue": "Int. J. Comput. Vis",
      "doi": "10.1007/s11263-008-0141-9"
    },
    {
      "citation_id": "58",
      "title": "Landmark matching via large deformation diffeomorphisms",
      "authors": [
        "S Joshi",
        "M Miller"
      ],
      "year": "2000",
      "venue": "IEEE Trans. Image Process",
      "doi": "10.1109/83.855431"
    },
    {
      "citation_id": "59",
      "title": "Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021",
      "authors": [
        "Z Xu",
        "B Li",
        "Y Yuan",
        "M Geng"
      ],
      "year": "2021",
      "venue": "The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event"
    },
    {
      "citation_id": "60",
      "title": "Pixel-in-pixel net: Towards efficient facial landmark detection in the wild",
      "authors": [
        "H Jin",
        "S Liao",
        "L Shao"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision",
      "doi": "10.1007/s11263-021-01521-4"
    },
    {
      "citation_id": "61",
      "title": "Adnet: Leveraging error-bias towards normal direction in face alignment",
      "authors": [
        "Y Huang",
        "H Yang",
        "C Li",
        "J Kim",
        "F Wei"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "62",
      "title": "Revisting quantization error in face alignment",
      "authors": [
        "X Lan",
        "Q Hu",
        "J Cheng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops"
    },
    {
      "citation_id": "63",
      "title": "Subpixel heatmap regression for facial landmark localization",
      "authors": [
        "A Bulat",
        "E Sanchez",
        "G Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "CoRR",
      "arxiv": "arXiv:2111.02360"
    },
    {
      "citation_id": "64",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin",
        "I Guyon",
        "U Luxburg",
        "S Bengio",
        "H Wallach",
        "R Fergus",
        "S Vishwanathan",
        "R Garnett"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "65",
      "title": "2d/3d pose estimation and action recognition using multitask deep learning",
      "authors": [
        "D Luvizon",
        "D Picard",
        "H Tabia"
      ],
      "year": "2018",
      "venue": "2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018",
      "doi": "10.1109/CVPR.2018.00539"
    },
    {
      "citation_id": "66",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters",
      "doi": "10.1109/LSP.2016.2603342"
    },
    {
      "citation_id": "67",
      "title": "Blazeface: Submillisecond neural face detection on mobile gpus",
      "authors": [
        "V Bazarevsky",
        "Y Kartynnik",
        "A Vakunov",
        "K Raveendran",
        "M Grundmann"
      ],
      "year": "2019",
      "venue": "CoRR",
      "arxiv": "arXiv:1907.05047"
    },
    {
      "citation_id": "68",
      "title": "TensorFlow: Large-scale machine learning on heterogeneous systems",
      "authors": [
        "Martin Abadi",
        "Ashish Agarwal",
        "Paul Barham",
        "Eugene Brevdo",
        "Zhifeng Chen",
        "Craig Citro",
        "Greg Corrado",
        "Andy Davis",
        "Jeffrey Dean",
        "Matthieu Devin",
        "Sanjay Ghemawat",
        "Ian Goodfellow",
        "Andrew Harp",
        "Geoffrey Irving",
        "Michael Isard",
        "Y Jia",
        "Rafal Jozefowicz",
        "Lukasz Kaiser",
        "Manjunath Kudlur",
        "Josh Levenberg",
        "Dandelion Mane",
        "Rajat Monga",
        "Sherry Moore",
        "Derek Murray",
        "Chris Olah",
        "Mike Schuster",
        "Jonathon Shlens",
        "Benoit Steiner",
        "Ilya Sutskever",
        "Kunal Talwar",
        "Paul Tucker",
        "Vincent Vanhoucke",
        "Vijay Vasudevan",
        "Fernanda Viegas",
        "Oriol Vinyals",
        "Pete Warden",
        "Martin Wattenberg",
        "Martin Wicke",
        "Yuan Yu",
        "Xiaoqiang Zheng"
      ],
      "year": "2015",
      "venue": "TensorFlow: Large-scale machine learning on heterogeneous systems"
    },
    {
      "citation_id": "69",
      "title": "SSD: single shot multibox detector",
      "authors": [
        "W Liu",
        "D Anguelov",
        "D Erhan",
        "C Szegedy",
        "S Reed",
        "C Fu",
        "A Berg"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 -14th European Conference",
      "doi": "10.1007/978-3-319-46448-0\\_2"
    },
    {
      "citation_id": "70",
      "title": "Going deeper into face detection: A survey",
      "authors": [
        "S Minaee",
        "P Luo",
        "Z Lin",
        "K Bowyer"
      ],
      "year": "2021",
      "venue": "CoRR",
      "arxiv": "arXiv:2103.14983"
    },
    {
      "citation_id": "71",
      "title": "Vdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track",
      "authors": [
        "P Garrido",
        "L Valgaerts",
        "H Sarmadi",
        "I Steiner",
        "K Varanasi",
        "P Pérez",
        "C Theobalt"
      ],
      "year": "2015",
      "venue": "Comput. Graph. Forum",
      "doi": "10.1111/cgf.12552"
    },
    {
      "citation_id": "72",
      "title": "Few-shot adversarial learning of realistic neural talking head models",
      "authors": [
        "E Zakharov",
        "A Shysheya",
        "E Burkov",
        "V Lempitsky"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2019.00955"
    },
    {
      "citation_id": "73",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "authors": [
        "C Finn",
        "P Abbeel",
        "S Levine"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning"
    },
    {
      "citation_id": "74",
      "title": "Perceptual losses for real-time style transfer and superresolution",
      "authors": [
        "J Johnson",
        "A Alahi",
        "L Fei-Fei"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016"
    },
    {
      "citation_id": "75",
      "title": "A neural lipsync framework for synthesizing photorealistic virtual news anchors",
      "authors": [
        "R Zheng",
        "Z Zhu",
        "B Song",
        "C Ji"
      ],
      "year": "2020",
      "venue": "25th International Conference on Pattern Recognition, ICPR 2020",
      "doi": "10.1109/ICPR48806.2021.9412187"
    },
    {
      "citation_id": "76",
      "title": "High-resolution image synthesis and semantic manipulation with conditional gans",
      "authors": [
        "T Wang",
        "M Liu",
        "J Zhu",
        "A Tao",
        "J Kautz",
        "B Catanzaro"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2018.00917"
    },
    {
      "citation_id": "77",
      "title": "Freenet: Multiidentity face reenactment",
      "authors": [
        "J Zhang",
        "X Zeng",
        "M Wang",
        "Y Pan",
        "L Liu",
        "Y Liu",
        "Y Ding",
        "C Fan"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR42600.2020.00537"
    },
    {
      "citation_id": "78",
      "title": "Deepfakes and beyond: A survey of face manipulation and fake detection",
      "authors": [
        "R Tolosana",
        "R Vera-Rodriguez",
        "J Fierrez",
        "A Morales",
        "J Ortega-Garcia"
      ],
      "year": "2020",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2020.06.014"
    },
    {
      "citation_id": "79",
      "title": "Driver drowsiness detection model using convolutional neural networks techniques for android application",
      "authors": [
        "R Jabbar",
        "M Shinoy",
        "M Kharbeche",
        "K Al-Khalifa",
        "M Krichen",
        "K Barkaoui"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Informatics, IoT, and Enabling Technologies (ICIoT)",
      "doi": "10.1109/ICIoT48696.2020.9089484"
    },
    {
      "citation_id": "80",
      "title": "Real-time monitoring of driver drowsiness on mobile platforms using 3d neural networks",
      "authors": [
        "J Wijnands",
        "J Thompson",
        "K Nice",
        "G Aschwanden",
        "M Stevenson"
      ],
      "year": "2019",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "81",
      "title": "Lightweight driver monitoring system based on multi-task mobilenets",
      "authors": [
        "W Kim",
        "W.-S Jung",
        "H Choi"
      ],
      "year": "2019",
      "venue": "Sensors",
      "doi": "10.3390/s19143200"
    },
    {
      "citation_id": "82",
      "title": "Driver status monitoring system in autonomous driving era",
      "authors": [
        "T Hyuga",
        "K Kinoshita",
        "K Nishiyuki",
        "Y Hasegawa"
      ],
      "year": "2019",
      "venue": "OMRON TECHNICS"
    },
    {
      "citation_id": "83",
      "title": "Deep face recognition: A survey",
      "authors": [
        "I Masi",
        "Y Wu",
        "T Hassner",
        "P Natarajan"
      ],
      "year": "2018",
      "venue": "2018 31st SIBGRAPI Conference on Graphics, Patterns and Images (SIB-GRAPI)",
      "doi": "10.1109/SIBGRAPI.2018.00067"
    },
    {
      "citation_id": "84",
      "title": "Mobiface: A lightweight deep learning face recognition on mobile devices",
      "authors": [
        "C Duong",
        "K Quach",
        "I Jalata",
        "N Le",
        "K Luu"
      ],
      "year": "2019",
      "venue": "2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS)",
      "doi": "10.1109/BTAS46853.2019.9185981"
    },
    {
      "citation_id": "85",
      "title": "Mobilefacenets: Efficient cnns for accurate real-time face verification on mobile devices",
      "authors": [
        "S Chen",
        "Y Liu",
        "X Gao",
        "Z Han"
      ],
      "year": "2018",
      "venue": "Biometric Recognition"
    },
    {
      "citation_id": "86",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "Sensors",
      "doi": "10.3390/s18020401"
    },
    {
      "citation_id": "87",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2020.2981446"
    },
    {
      "citation_id": "88",
      "title": "Intriguing properties of neural networks",
      "authors": [
        "C Szegedy",
        "W Zaremba",
        "I Sutskever",
        "J Bruna",
        "D Erhan",
        "I Goodfellow",
        "R Fergus"
      ],
      "year": "2014",
      "venue": "2nd International Conference on Learning Representations"
    },
    {
      "citation_id": "89",
      "title": "Scoping adversarial attack for improving its quality",
      "authors": [
        "K Khabarlak",
        "L Koriashkina"
      ],
      "year": "2019",
      "venue": "Computer Science, Control",
      "doi": "10.15588/1607-3274-2019-2-12"
    },
    {
      "citation_id": "90",
      "title": "Adversarial examples in the physical world",
      "authors": [
        "A Kurakin",
        "I Goodfellow",
        "S Bengio"
      ],
      "year": "2017",
      "venue": "5th International Conference on Learning Representations"
    },
    {
      "citation_id": "91",
      "title": "Minimizing perceived image quality loss through adversarial attack scoping",
      "authors": [
        "K Khabarlak",
        "L Koriashkina"
      ],
      "year": "1904",
      "venue": "CoRR",
      "arxiv": "arXiv:1904.10390"
    },
    {
      "citation_id": "92",
      "title": "Zoo: Zeroth order optimization based blackbox attacks to deep neural networks without training substitute models",
      "authors": [
        "P.-Y Chen",
        "H Zhang",
        "Y Sharma",
        "J Yi",
        "C.-J Hsieh"
      ],
      "year": "2017",
      "venue": "Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security",
      "doi": "10.1145/3128572.3140448"
    },
    {
      "citation_id": "93",
      "title": "Adversarial examples are not easily detected: Bypassing ten detection methods",
      "authors": [
        "N Carlini",
        "D Wagner"
      ],
      "year": "2017",
      "venue": "Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, ser. AISec '17",
      "doi": "10.1145/3128572.3140444"
    },
    {
      "citation_id": "94",
      "title": "Threat of adversarial attacks on deep learning in computer vision: A survey",
      "authors": [
        "N Akhtar",
        "A Mian"
      ],
      "year": "2018",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2018.2807385"
    },
    {
      "citation_id": "95",
      "title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition",
      "authors": [
        "M Sharif",
        "S Bhagavatula",
        "L Bauer",
        "M Reiter"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, ser. CCS '16",
      "doi": "10.1145/2976749.2978392"
    },
    {
      "citation_id": "96",
      "title": "Real-world attack on mtcnn face detection system",
      "authors": [
        "E Kaziakhmedov",
        "K Kireev",
        "G Melnikov",
        "M Pautov",
        "A Petiushko"
      ],
      "year": "2019",
      "venue": "2019 International Multi-Conference on Engineering, Computer and Information Sciences (SIBIRCON)",
      "doi": "10.1109/SIBIRCON48586.2019.8958122"
    }
  ]
}