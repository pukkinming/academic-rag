{
  "paper_id": "2407.14314v1",
  "title": "Emocam: Toward Understanding What Drives Cnn-Based Emotion Recognition",
  "published": "2024-07-19T13:47:02Z",
  "authors": [
    "Youssef Doulfoukar",
    "Laurent Mertens",
    "Joost Vennekens"
  ],
  "keywords": [
    "Artificial Neural Networks",
    "Convolutional Neural Networks",
    "Explainable AI",
    "Emotion Recognition",
    "Computer Vision"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Convolutional Neural Networks are particularly suited for image analysis tasks, such as Image Classification, Object Recognition or Image Segmentation. Like all Artificial Neural Networks, however, they are \"black box\" models, and suffer from poor explainability. This work is concerned with the specific downstream task of Emotion Recognition from images, and proposes a framework that combines CAM-based techniques with Object Detection on a corpus level to better understand on which image cues a particular model, in our case EmoNet, relies to assign a specific emotion to an image. We demonstrate that the model mostly focuses on human characteristics, but also explore the pronounced effect of specific image modifications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Thanks to recent progress, image analysis problems such as Object Detection using Artifical Neural Networks (ANN) can be more or less considered to be solved  [8, 19] . However, higher-order tasks, such as identifying the emotion content of an entire image, remain more challenging. Convolutional Neural Network (CNN) models such as EmoNet  [9]  present a promising approach in this area, but its results are not yet completely convincing. This raises the question to which extent this network is actually picking up meaningful cues in the images, and to what extent it is learning spurious correlations that may be present in the private dataset on which it was trained.\n\nANNs are still considered \"black box\" models, and the domain that attempts to untangle how they make the predictions they make, i.e., to improve their explainability, is a very active one  [2, 16, 18] . One of the techniques for this is Class Activation Maps  [20] , or CAM, which allows to highlight those parts of the image that contributed most to a model's (say, a CNN image classifier) output. This technique allows to visually inspect individual images or videos, but does not immediately allow for an automated global analysis on a corpus level.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "To Answer The Earlier Question Of What Image Cues A Cnn-Based Emotion",
      "text": "Recognition network, in casu EmoNet, most relies on, we propose EmoCAM. Our framework combines two information streams, namely CAM and Object Detection, to build a pipeline that allows to determine those object classes that most contributed to the model's decision making on a corpus level. Besides better understanding what object classes the model relies most upon, we also want to explore the potential of applying minor changes to the input images that steer the model towards a specific emotion by leveraging the obtained information from our EmoCAM analysis. Our source code can be found at https://gitlab.com/ EAVISE/lme/emocam.\n\nThe remainder of this paper is organized as follows: in Sec. 2 we describe our proposed framework in detail, followed by Sec. 3 where we look at a concrete case using the EmoNet network and FindingEmo  [11]  image dataset; limitations and roads for future work are explored in Sec. 4 and we present concluding remarks in Sec. 5.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "We start our analysis by applying, for a given CNN model M and corpus D, the following steps to each image I ∈ D, schematically illustrated in Fig.  1 .\n\nFirst, we process I with the object detection network of our choice, in casu, YOLOv3  [14]  trained on the Open Images dataset.  4  We opted for this particular pretrained network as other popular Object Detection dataset choices such as PASCAL VOC (20 classes) and MS-COCO (80 classes) are too restricted in the classes they propose. By contrast, Open Images, which contains 601 classes, presents a nice balance between human-related classes (e.g., \"human face\", \"mouth\", etc.), and more general classes representing contextual elements (e.g., \"car\", \"tree\", etc.). The result of this operation is a list of detected objects and their corresponding bounding boxes B. We filter the YOLOv3 output by keeping only bounding boxes with an IoU score > 0.005.\n\nSecond, we process I with M , and apply a CAM-based technique C to the last convolutional layer of M .  5  This gives us an activation map that we overlay on top of I to obtain a new image A.\n\nFinally, we lay the bounding boxes B on top of A, and look for those boxes b ∈ B for which the average CAM activation, or importance, C Act > 0.3, with C Act defined as the sum of the CAM activations within the box divided by the area of the box.  6  The threshold was heuristically determined by visually inspecting a limited set of images. We refer to these boxes as the set B * , and interpret these as those objects that most contributed to the model's decision.\n\nOnce we have found B * for every image in our corpus, we then analyse these data to find associations between object classes and output labels by constructing an association matrix M A , where a ij ∈ M A represents the number of images labeled with the j th EmoNet emotion label in which the i th object class has been detected at least once. By dividing each column j through by the total number of images labeled with the j th emotion such as to obtain percentages (after doing ×100), we obtain M ′ A which allows to ignore imbalances in the prediction rates of the different emotions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Results",
      "text": "We tested our proposed approach using the EmoNet model and the FindingEmo dataset. EmoNet is a model obtained through replacing the last layer of an AlexNet model pretrained on the ImageNet  [4]  corpus. This last layer was then trained on a private dataset of 137,482 images annotated for the emotion they evoke in the observer with one of 26 custom emotion labels. We use the Python port by L. Mertens  [10]  of the original Matlab release.\n\nFindingEmo is an image dataset consisting of 25,869 images annotated for, a.o., the dominant emotion in the picture, using one of the 24 emotion labels in Plutchik's Wheel of Emotions  [13] . All images represent multiple people in various natural settings and with varying degrees of interaction among them.\n\nWe first present detailed results for Grad-CAM  [17]  in Sec. 3.1, and follow this up with an exploration of the effect of using other CAM-based methods in Sec. 3.2.  7  Finally, we briefly explore the effect on the predicted label of artificially adding certain objects to images, attempting to answer the question whether the presence of certain objects can cause a specific label to be predicted.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results For Grad-Cam",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A Heatmap Depicting M ′",
      "text": "A as obtained using Grad-CAM together with EmoNet applied on the FindingEmo corpus can be found in Fig.  3 . We limit ourselves to the 25 most prominent Open Images classes (as determined by the average of the corresponding row in M ′ A ). A clear conclusion to be drawn from this graph is that human features do indeed contribute the most to the decision making, most particularly the human face which, except for \"Clothing\", represents the most important class for each EmoNet label.\n\nAdditionally, some more specific associations do manifest themselves. Clear examples are the association between \"Sports equipment\" and \"Excitement\", and \"Food\" and \"Craving\", both of which seem logical. Less clear is, e.g., the association between \"Furniture\" and \"Interest\", or \"Plant\" and \"Surprise\", which hint of spurious associations resulting from biases in either or both the EmoNet training dataset and FindingEmo.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Comparison Of Cam Methods",
      "text": "To answer the question to what extent different CAM methods yield different results, we performed a Representational Similarity Analysis  [1]  as follows. For each CAM method C ∈ {Grad-CAM, Ablation-CAM  [5] , LIME  [15] , LRP  [3] , LIFT-CAM  [7] }, we determine the association matrix M A with all Open Images classes as described in Sec. 2, keeping the same emotion and class ordering for each C 8  . We then flatten each matrix by concatenating all rows, turning it in to a 1D vector V M C . Finally, we construct a matrix R where each entry R CC ′ represents the Spearman Correlation rank between V M C and V M C ′ . The resulting matrix is shown in Fig.  3 . All related p-values were < < 0.05, indicating statistical significance.\n\nThe consistently high correlation values between all pairs indicate that variations in results obtained through different CAM methods can be expected to be minimal. We did observe both LIFT-CAM and LRP resulting in a notable association between \"Pillow\" and \"Sexual Desire\". Other than this, the differences between the methods appear to lie within the relative strengths of the associations observed, rather than the assocations themselves.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Prediction Stability",
      "text": "To illustrate how the obtained knowledge can be applied to fool the network by creating an adversarial attack, consider the image shown in Fig.  4 . We know from Sec. 3.1 that there is a high association between the object category \"Sports equipment\" and EmoNet label \"Excitement\". This inspired us to take an image labeled with high probabilty as \"Joy\" (92.9%; Excitement: 1.8%). After altering this image by pasting a rugby ball on top of the head of one of the two main subjects, the prediction changes to 66.1% \"Excitement\" (\"Joy\": 30.2%), demonstrating the dramatic effect the presence of a particular object can have on the model's output.\n\nNote that the position of the pasted object greatly influences the effect it has. Moving the rugby ball to the immediate right of the subject's face alters the predictions to 43.9% \"Joy\" and 42.1% \"Excitement\", while moving it to the   immediate left only alters the predictions by 4% in the same directions (\"Joy\": 34.2%; \"Excitement\": 62.1%). Covering the other subject's head instead, we obtain 52.0% \"Joy\" and 30.7% Excitement.\n\nWe further investigate this effect by performing the following experiment. For each I in D, we paste a given object O ∈ {Rugby ball, Soccer ball, Lotus flower}, resized such that its height equals 0.2 × height(I), in I centered at each one of a set of predefined relative positions P within the image, resulting in size(P ) alterations to I. The positions and objects considered are illustred in Fig.  5 . We then send the altered images through EmoNet, and observe how the prediction was affected. The results are shown in Fig.  6 .\n\nThe results confirm that the model shows high sensitivity to certain objects. Although differences of up to more than 10% can be observed in Fig.  6 , specifically for the rugby ball, no real tendencies reveal themselves. In combination with the example in Fig.  4 , we hypothesize the differences are not so much due to the absolute position of the object, but to what it occludes. For the rugby ball, 4 out of 17 positions resulted most often in a label switch to \"Excitement\". For the soccer ball, the number increases to 7. The Lotus flower clearly results in much less label shifts overall, with not a single position favoring \"Excitement\", confirming the importance of the object class in effecting a label switch.",
      "page_start": 5,
      "page_end": 7
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "Although the currently described approach already provides valuable insights, some limitations are to be noted.\n\nFirst, the approach is, by definition, heavily dependent on the choice of Object Detection network and its corresponding classes and performance. The upside is that, as a plug-and-play component, different Object Detection networks can be chosen for different tasks, allowing to pick object classes tailored to the task at hand. Second, our current implementation does not take into account the size of the bounding boxes, which can result in suboptimal results. Consider, e.g., the example shown in Fig.  7 . Although the subject's ear is clearly not the most important contributing element in the picture, because of the small size of the \"Human ear\" bounding box the average CAM activation is nonetheless the highest, spuriously pushing this object class to the top. Two main paths could be explored to counter this issue. The most straightforward would be to develop a scoring function that does take into account the bounding box size, or the activation distribution within it. Alternatively, segmentation models could be used instead of bounding box detection models, so as to obtain clearly delineated zones representing the different objects. Then of course, our first limitation still applies, i.e., the segmentation classes need to be relevant to the task at hand.\n\nThird, with regard to our experiment described in Sec. 3.3, a more interesting approach might be to, instead of, or along with, considering only a fixed set of positions, paste the object at the center of bounding boxes relating to specific features such as human heads, thus investigating the effect of masking specific objects. We also intend to apply EmoCAM to the modified images to explore if the shift in label is reflected in a shift in focus in the modified image.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "We propose the novel EmoCAM approach to explaining CNN decisions, specifically with the downstream task of Emotion Recognition from images in mind. Our objective is threefold: 1) better understanding what parts of the input image the model uses to make its decision, 2) allowing to check whether or not the information used by the model aligns with expectations from a human perspective, and 3) uncovering potential model biases. We have demonstrated our approach using the EmoNet model, FindingEmo dataset and multiple CAM techniques. Using our approach, we found that EmoNet indeed shows a strong focus on human elements, most notably (parts of) the human face, which is encouraging as it aligns with our understanding of human emotion recognition from Psychology. Nevertheless, we also found the model output to be quite unstable, in that adding specific objects (e.g., a rugby ball) to an image can dramatically alter its output and steer it towards a specific target emotion (e.g., \"Excitement\").",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic illustrating pipeline combining CAM with Object Detection. Photo",
      "page": 2
    },
    {
      "caption": "Figure 1: First, we process I with the object detection network of our choice, in casu,",
      "page": 2
    },
    {
      "caption": "Figure 2: Association between Open Images classes and predicted EmoNet label.",
      "page": 4
    },
    {
      "caption": "Figure 3: We limit ourselves to",
      "page": 4
    },
    {
      "caption": "Figure 3: RSA analysis of different CAM methods.",
      "page": 5
    },
    {
      "caption": "Figure 3: All related p-values were << 0.05, indicating statistical",
      "page": 5
    },
    {
      "caption": "Figure 4: Adversarial example. Original image on the left, labeled by EmoNet as 92.9%",
      "page": 6
    },
    {
      "caption": "Figure 5: Schematic illustration of “paste object in image” experiment. The grid on the left",
      "page": 6
    },
    {
      "caption": "Figure 5: We then send the altered images through EmoNet, and observe how the",
      "page": 6
    },
    {
      "caption": "Figure 6: The results confirm that the model shows high sensitivity to certain objects.",
      "page": 6
    },
    {
      "caption": "Figure 4: , we hypothesize the differences are not so much due",
      "page": 6
    },
    {
      "caption": "Figure 6: Percentage of images in the corpus whose predicted label changed when a",
      "page": 7
    },
    {
      "caption": "Figure 5: for the positions and",
      "page": 7
    },
    {
      "caption": "Figure 7: Although the subject’s ear is clearly not the most",
      "page": 7
    },
    {
      "caption": "Figure 7: An instance were the current EmoCAM approach fails to detect the most im-",
      "page": 8
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Chapter 27 -representational similarity analyses: A practical guide for functional mri applications",
      "year": "2018",
      "venue": "Handbook of in Vivo Neural Plasticity Techniques, Handbook of Behavioral Neuroscience",
      "doi": "10.1016/B978-0-12-812028-6.00027-6"
    },
    {
      "citation_id": "2",
      "title": "Explainable artificial intelligence (xai): What we know and what is left to attain trustworthy artificial intelligence",
      "authors": [
        "S Ali",
        "T Abuhmed",
        "S El-Sappagh",
        "K Muhammad",
        "J Alonso-Moral",
        "R Confalonieri",
        "R Guidotti",
        "J Del Ser",
        "N Díaz-Rodríguez",
        "F Herrera"
      ],
      "year": "2023",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2023.101805"
    },
    {
      "citation_id": "3",
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "authors": [
        "S Bach",
        "A Binder",
        "G Montavon",
        "F Klauschen",
        "K Müller",
        "W Samek"
      ],
      "year": "2015",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "4",
      "title": "Imagenet: A largescale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A largescale hierarchical image database"
    },
    {
      "citation_id": "5",
      "title": "Ablation-cam: Visual explanations for deep convolutional network via gradient-free localization",
      "authors": [
        "S Desai",
        "H Ramaswamy"
      ],
      "year": "2020",
      "venue": "2020 IEEE Winter Conference on Applications of Computer Vision (WACV)",
      "doi": "10.1109/WACV45572.2020.9093360"
    },
    {
      "citation_id": "6",
      "title": "contributors: Pytorch library for cam methods",
      "authors": [
        "J Gildenblat"
      ],
      "year": "2021",
      "venue": "contributors: Pytorch library for cam methods"
    },
    {
      "citation_id": "7",
      "title": "Towards better explanations of class activation mapping",
      "authors": [
        "H Jung",
        "Y Oh"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV48922.2021.00137"
    },
    {
      "citation_id": "8",
      "title": "A comprehensive review of object detection with deep learning",
      "authors": [
        "R Kaur",
        "S Singh"
      ],
      "year": "2023",
      "venue": "Digital Signal Processing",
      "doi": "10.1016/j.dsp.2022.103812"
    },
    {
      "citation_id": "9",
      "title": "Emotion schemas are embedded in the human visual system",
      "authors": [
        "P Kragel",
        "M Reddan",
        "K Labar",
        "T Wager"
      ],
      "year": "2019",
      "venue": "Science Advances"
    },
    {
      "citation_id": "10",
      "title": "Emonet: A pytorch port",
      "authors": [
        "L Mertens"
      ],
      "year": "2022",
      "venue": "Emonet: A pytorch port"
    },
    {
      "citation_id": "11",
      "title": "Findingemo: An image dataset for emotion recognition in the wild",
      "authors": [
        "L Mertens",
        "E Yargholi",
        "H De Beeck",
        "J Stock",
        "J Vennekens"
      ],
      "year": "2024",
      "venue": "Findingemo: An image dataset for emotion recognition in the wild"
    },
    {
      "citation_id": "12",
      "title": "Lightnet: Building blocks to recreate darknet networks in pytorch",
      "authors": [
        "T Ophoff"
      ],
      "year": "2018",
      "venue": "Lightnet: Building blocks to recreate darknet networks in pytorch"
    },
    {
      "citation_id": "13",
      "title": "Chapter 1 -a general psychoevolutionary theory of emotion",
      "authors": [
        "R Plutchik"
      ],
      "year": "1980",
      "venue": "Theories of Emotion",
      "doi": "10.1016/B978-0-12-558701-3.50007-7"
    },
    {
      "citation_id": "14",
      "title": "Yolov3: An incremental improvement",
      "authors": [
        "J Redmon",
        "A Farhadi"
      ],
      "year": "2018",
      "venue": "Yolov3: An incremental improvement"
    },
    {
      "citation_id": "15",
      "title": "why should i trust you?\": Explaining the predictions of any classifier",
      "authors": [
        "M Ribeiro",
        "S Singh",
        "C Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "doi": "10.1145/2939672.2939778"
    },
    {
      "citation_id": "16",
      "title": "Explainable ai (xai): A systematic meta-survey of current challenges and future opportunities",
      "authors": [
        "W Saeed",
        "C Omlin"
      ],
      "year": "2023",
      "venue": "Knowledge-Based Systems",
      "doi": "10.1016/j.knosys.2023.110273"
    },
    {
      "citation_id": "17",
      "title": "Gradcam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "R Selvaraju",
        "M Cogswell",
        "A Das",
        "R Vedantam",
        "D Parikh",
        "D Batra"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)",
      "doi": "10.1109/ICCV.2017.74"
    },
    {
      "citation_id": "18",
      "title": "Survey on explainable ai: From approaches, limitations and applications aspects",
      "authors": [
        "W Yang",
        "Y Wei",
        "H Wei",
        "Y Chen",
        "G Huang",
        "X Li",
        "R Li",
        "N Yao",
        "X Wang",
        "X Gu",
        "M Amin",
        "B Kang"
      ],
      "year": "2023",
      "venue": "Human-Centric Intelligent Systems"
    },
    {
      "citation_id": "19",
      "title": "Object detection with deep learning: A review",
      "authors": [
        "Z Zhao",
        "P Zheng",
        "S Xu",
        "X Wu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "doi": "10.1109/TNNLS.2018.2876865"
    },
    {
      "citation_id": "20",
      "title": "Learning deep features for discriminative localization",
      "authors": [
        "B Zhou",
        "A Khosla",
        "A Lapedriza",
        "A Oliva",
        "A Torralba"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR.2016.319"
    }
  ]
}