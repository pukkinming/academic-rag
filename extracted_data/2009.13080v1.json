{
  "paper_id": "2009.13080v1",
  "title": "Reactive Supervision: A New Method For Collecting Sarcasm Data",
  "published": "2020-09-28T05:04:22Z",
  "authors": [
    "Boaz Shmueli",
    "Lun-Wei Ku",
    "Soumya Ray"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Sarcasm detection is an important task in affective computing, requiring large amounts of labeled data. We introduce reactive supervision, a novel data collection method that utilizes the dynamics of online conversations to overcome the limitations of existing data collection techniques. We use the new method to create and release a first-of-its-kind large dataset of tweets with sarcasm perspective labels and new contextual features. The dataset is expected to advance sarcasm detection research. Our method can be adapted to other affective computing domains, thus opening up new research opportunities.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Sarcasm is ubiquitous in human conversations. As a form of insincere speech, the intent behind a sarcastic utterance is integral to its meaning. Perceiving a sarcastic utterance as genuine will often result in a complete reversal of the intended meaning, and vice versa  (Gibbs, 1986) . It is therefore crucial for affective computing systems and tasks, such as sentiment analysis and dialogue systems, to automatically detect sarcasm from the perspective of the author as well as the reader in order to avoid misunderstandings.  Oprea and Magdy (2019)  recently pioneered the study of intended sarcasm (by the author) vs. perceived sarcasm (by the reader) in the context of sarcasm detection tasks. The training of models for these tasks requires large amounts of labeled sarcasm data, with Twitter becoming a major source due to its popularity as a social network as well as the huge amounts of conversational text its users generate. Previous works describe three methods for collecting sarcasm data: distant supervision, manual annotation, and manual collection.\n\nDistant supervision automatically collects \"inthe-wild\" sarcastic tweets by leveraging author-generated labels such as the #sarcasm hashtag  (Davidov et al., 2010; Ptáček et al., 2014) . This method generates large amounts of data at low cost, but labels are often noisy and biased  (Bamman and Smith, 2015) .\n\nTo improve quality, manual annotation asks humans to label given tweets as sarcastic or not. Since finding sarcasm in a large corpus is \"a needle-in-ahaystack problem\"  (Liebrecht et al., 2013) , manual annotation can be combined with distant supervision  (Riloff et al., 2013) . Still, low inter-annotator reliability is often reported  (Swanson et al., 2014) , resulting not only from the subjective nature of sarcasm but also the lack of cultural context  (Joshi et al., 2016) . Moreover, neither method collects both sarcasm perspectives: distant supervision collects intended sarcasm, while manual annotation can only collect perceived sarcasm.\n\nLastly, in manual collection, humans are asked to gather and report sarcastic texts, either their own  (Oprea and Magdy, 2020)  or by others  (Filatova, 2012) . However, both manual methods are slower and more expensive than distant supervision, resulting in smaller datasets.\n\nTo overcome the above limitations, we propose reactive supervision, a novel conversation-based method that offers automated, high-volume, \"inthe-wild\" collection of high-quality intended and perceived sarcasm data. We use our method to create and release the SPIRS sarcasm dataset 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Reactive Supervision",
      "text": "Reactive supervision exploits the frequent use in online conversations of a cue tweet -a reply that highlights sarcasm in a prior tweet. Figure  1     alerts B by replying with a cue tweet (She was just being sarcastic!). Since A replies to B but refers to the sarcastic author in the 3rd person (She), C is necessarily the author of the perceived sarcastic tweet. Similarly, Figure  1  (right panel) shows how a 1st person cue (I was just being sarcastic!) can be used to unequivocally label intended sarcasm.\n\nTo capture sarcastic tweets, we thus first search for cue tweets (using the query phrase \"being sarcastic\", often used in responses to sarcastic tweets), then carefully examine each cue tweet to identify the corresponding sarcastic tweet.\n\nThe following formalizes our method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "Definitions We define a thread to be a sequence of tweets {t n , t n-1 , . . . , t 1 }, where t i+1 is a reply to t i , i = 1, . . . , n -1. Tweets are listed in reverse chronological order, with t 1 being the root tweet. The corresponding author sequence is a n a n-1 . . . a 1 , were we replace the original author names with consecutive capital letters (A, B, C, ...), starting with a n = A. For example, Figure  1  (right panel) depicts a thread of length n = 4 with author sequence ABAC. Here a 4 = a 2 = A, a 3 = B, and a 1 = C is the author of the root tweet.\n\nAlgorithm Given a thread {t n , t n-1 , . . . , t 1 } with cue tweet t n by a n = A, our aim is to identify the sarcastic tweet among {t n-1 , . . . , t 1 }. We first examine the personal subject pronoun used in the cue (I, you, s/he) and map it to a grammatical person class  (1st, 2nd, 3rd) . This informs us whether the sarcastic author is also the author of the cue (1st), its addressee (2nd), or another party (3rd).\n\nFor each person class we then apply a heuristic to identify the sarcastic tweet.\n\nFor example, for a 1st-person cue tweet (e.g., I was just being sarcastic!), the sarcastic tweet must also be authored by A. If the earlier tweets in T contain exactly one tweet from A, it is unambiguously the sarcastic tweet. Otherwise, if there are two or more earlier tweets from A (or none), the sarcastic tweet cannot be unambiguously pinpointed and the entire thread is discarded. We formalize this rule by requiring the author sequence to match the regular expression /ˆA[ˆA] * (A)[ˆA] * $/, where the capturing group (A) corresponds to the sarcastic tweet 2  . We are able to use regular expressions because we use a string of letters to represent the author sequence. 2nd-and 3rd-person cues produce corresponding rules and patterns. Table  1  lists the three person classes, corresponding regular expressions, and example author sequences.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Advantages",
      "text": "Additional Tweet Types Along with each sarcastic tweet, we collect the oblivious tweet (the unsuspecting reply to the sarcastic tweet) when available. As far as we know, this is the first work that identifies and collects oblivious texts, a new type of data that can improve research on the (mis)understanding of sarcasm, with applications such as automated assistive systems for people with emotional or cognitive disabilities. If the sarcastic tweet is a reply, we also capture the eliciting tweet, which is the tweet that evoked the sarcastic reply. We provide more details in Appendix A.\n\nExtraction of Semantic Relations Being able to identify the various tweets types (cue, oblivious, sarcastic, eliciting), reactive supervision can be understood more abstractly as capturing semantic dependency relations between utterances 3  . Reactive supervision can thus be useful in the context of discourse analysis.\n\nContext-Aware Annotation Our method uses cues from thread participants, who therefore serve as de facto annotators. As participants are familiar with the conversation's context, we overcome some quality issues of using external annotators, who are often unfamiliar with the conversation context due to cultural and social gaps  (Joshi et al., 2016) .\n\nSarcasm Perspective Previous datasets contain either intended or perceived sarcasm, but not both  (Oprea and Magdy, 2019) . Our method identifies and labels both intended and perceived sarcasm within the same data context: by their essence, 1stperson cue tweets capture intended sarcasm, while 2nd-and 3rd-person cues capture perceived sarcasm. We label a tweet as perceived sarcasm when at least one reader perceives the tweet as sarcastic and posts a cue tweet. Detecting perceived sarcasm is useful, for example, for training algorithms that flag sensitive texts which might be (mis)perceived as sarcastic (even by a single reader).\n\nFaster Data Collection We tested González-Ibáñez et al. (  2011 )'s distant supervision method of collecting tweets ending with #sarcasm and related hashtags, fetching 171 tweets/day on average. During the same period, our method collected 312 tweets/day on average, an 82% rate improvement.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Summary Of Advantages",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Spirs Dataset",
      "text": "We implemented reactive supervision using a 4step pipeline (see Algorithm 1):\n\n1. Fetch calls the Twitter Search API to collect cue tweets, using \"being sarcastic\" as the query.\n\n2. Classify is a rule-based, precision-oriented classifier that classifies cues as 1st-, 2nd-, or 3rdperson according to the referred pronoun (I, you, s/he). If the cue cannot be accurately classified (e.g., a pronoun cannot be found, the cue contains multiple pronouns, negation words are present), the cue is classified as unknown and discarded.\n\n3. Traverse calls the Twitter Lookup API to retrieve the thread by starting from the cue tweet and repeatedly fetching the parent tweet up to the root tweet.\n\n4. Finally, Match matches the thread's author sequence with the corresponding regular expression. Unmatched sequences are discarded. Otherwise, the sarcastic tweet is identified and saved along with the cue tweet, as well as the eliciting and oblivious tweets when available.\n\nThe pipeline collected 65K cue tweets containing the phrase \"being sarcastic\" and corresponding threads during 48 days in October and November 2019. 77% of the cues were classified as unknown and discarded, ending with 15 000 English sarcastic tweets. In addition, 10 648 oblivious and 9 156 eliciting tweets were automatically captured. Table  3  summarizes the SPIRS dataset. We added 15 000 negative instances by sampling random English tweets captured during the same period, discarding tweets with sarcasm-related words or hashtags.  Sarcastic tweets can be either root tweets or replies. We found that the majority of intended sarcasm tweets are replies (78.4%), while the majority of perceived sarcasm tweets are root tweets (77.0%). Further dataset statistics on author sequence and tweet position distributions are available in Appendices B and C.\n\nReliability To assess our method's reliability in capturing sarcastic tweets, we manually inspected 200 random sarcastic tweets, along with their cue tweets, from each person class. The accuracy of sarcastic tweet labeling was high: 98.5%, 98%, and 97% for 1st-, 2nd-, and 3rd-person cue tweets, respectively. Table  4  shows samples of correct and incorrect cue tweet classifications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Cue Tweet",
      "text": "Pers. Correct?",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Analysis",
      "text": "We present dataset baselines for three tasks: sarcasm detection, sarcasm detection with conversation context, and sarcasm perspective classification, a new task enabled by our dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Sarcasm Detection",
      "text": "The first experiment is sarcasm detection. We trained a total of three models: CNN (100 filters with a kernel size 3) and BiLSTM (100 units), both max-pooled and Adam-optimized with a learning rate of 0.0005; data was preprocessed as described in  Tay et al. (2018) ; the embedding layer was preloaded with GloVe embeddings (Twitter data, 100 dimensions)  (Pennington et al., 2014) . We also fine-tuned a pre-trained base uncased BERT model  (Devlin et al., 2019) . For all three models, we used 5-fold cross-validation for training, holding out 20% of the data for testing.\n\nResults are shown in Table  5  (top panel). BERT is the best performing model, with 70.3% accuracy. We compared SPIRS's classification results to the  Ptáček et al. (2014)  dataset, commonly used in sarcasm benchmarks. We found that Ptáček's accuracy is significantly higher (86.6%). We posit that it is because sarcasm is confounded with locale in the Ptáček (sarcastic tweets are from worldwide users; non-sarcastic tweets are from users near Prague), and thus classifiers learn features correlated to locale. We tested our hypothesis by replacing our negative samples with Ptáček's, which indeed resulted in boosting the accuracy by 19.1%.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Detection With Conversation Context",
      "text": "Our second sarcasm classification experiment uses conversation context by adding eliciting and oblivious tweets to the model. As far as we know, this is the first sarcasm-related task that uses oblivious texts. Our model concatenated the outputs of three identical 100-unit BiLSTMs (one per tweet: sarcastic, oblivious, eliciting) before feeding it into dense layers for classification. Tweets without surrounding context were not used in this task. Results are shown in Table  5  (middle panel). Accuracy for the full-context model was 74.7% (MCC 0.398).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Study",
      "text": "We conducted context ablation experiments to identify the contribution of each tweet type. We found that removing the eliciting tweets reduces accuracy by 0.5% and MCC by 0.026. Removing the oblivious tweets, however, lowered accuracy by 3.4% to 71.4%, and the MCC dropped significantly by 31%, from 0.398 to 0.275. This illustrates the importance of the new oblivious text data provided in the dataset and suggests its usefulness in sarcasm-related tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Perspective Classification",
      "text": "Taking advantage of the new labels in our dataset, we propose a new task to classify a sarcastic text's perspective: intended vs. perceived. Our results are displayed in Table  5  (bottom panel), demonstrating the superiority of BERT over the other models, with an accuracy of 68.2% and MCC of 0.366.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Error Analysis",
      "text": "We carefully examined the errors to analyze the causes of perspective misclassification. We observed that misclassified-as-intended tweets (e.g., \"You're lost!\", \"Omg that was so   funny\") had, on average, almost half the word count of misclassified-as-perceived tweets (17.2 vs. 27.8). We posit that longer, more informative texts make sarcasm easier to perceive; hence, short perceived sarcasm or long intended sarcasm might introduce errors. Analysis of the dataset's word count distribution supports our hypothesis (see Figure  2 ).\n\nLooking for further error sources, we inspected short intended tweets that were misclassified, for example \"great friends i have!\" and \"My mom is so beautiful\". These tweets can be read as root tweets and not as replies, yet most intended sarcasm tweets are replies while most perceived sarcasm tweets are root tweets (see Section 3). We hypothesize that the classifier learns discourse-related features (original tweet vs. reply tweet), which can lead to these errors. Further analysis of sarcasm perspective and its interplay with sarcasm pragmatics is a promising avenue for future research.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We present an innovative method for collecting sarcasm data that exploits the natural dynamics of online conversations. Our approach has multiple advantages over all existing methods. We used it to create and release SPIRS, a large sarcasm dataset with multiple novel features. These new features, including labels for sarcasm perspective and unique context (e.g., oblivious texts), offer opportunities for advances in sarcasm detection.\n\nReactive supervision is generalizable. By modifying the cue tweet selection criteria, our method can be adapted to related domains such as sentiment analysis and emotion detection, thereby advancing the quality and quantity of data collection and offering new research directions in affective computing.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A Search Pattern Production",
      "text": "We construct the regular expression for capturing all tweet types -sarcastic, oblivious, and eliciting -given a 3rd-person cue tweet. Similar logic produces the patterns for 1st-and 2nd-person cues.\n\nThe cue tweet author (A) refers to the sarcastic tweet author in the 3rd person (e.g., She was being sarcastic!); we thus assume that A's tweet is a response to a second author B, but refers to a third author C (the sarcastic author). To unambiguously pinpoint the sarcastic tweet, C can only appear once in the author sequence. Moreover, only A, B, and C can participate in the thread. Table  6  lists the search patterns for the three person classes. Note that the 2nd-person pattern does not include an oblivious tweet because A's cue tweet is a response to a sarcastic tweet from B, i.e., it is not triggered by an oblivious tweet.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Person Regular Expression 1St",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B Author Sequence Distribution",
      "text": "Table  7  shows the most common author sequences in SPIRS. The different colors correspond to the different tweet types. The most common pattern for 1st-person cues is ABAC (as in Figure  1 , right panel). AB is the most common pattern for 2ndperson cues, which denote a sarcastic root tweet followed immediately by a cue tweet (e.g., Why are you being sarcastic?). For 3rd-person cues, the most common pattern is ABC (as in Figure  1 , left panel). Note that some patterns appear in more than one person class. For example, ABA appears in both 1st-and 2nd-person classes, while ABAC appears in both 1st-and 3rd-person.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C Tweet Position Distribution",
      "text": "Reactive supervision enables the measurement of conversation position statistics for sarcastic tweets on Twitter. Given a thread {t n , . . . , t i = s, . . . , t 1 } with cue tweet t n , sarcastic tweet t i = s, and root tweet t 1 , we define the position of the sarcastic tweet as the distance i -1 between the sarcastic tweet and the root. Furthermore, the cue lag is the distance n -i between the cue and the sarcastic tweet. Table  8  shows the distribution of sarcastic tweets by position and cue lag in the SPIRS dataset. Root tweets (position = 0) account for 39% of sarcastic tweets. A further 39% of sarcastic tweets are direct replies to root tweets (position = 1). Interestingly, only 25% of cue tweets are direct replies to their sarcastic targets (lag = 1), while an overwhelming 71% have a lag of 2, mostly reflecting a response to an intermediate oblivious tweet. We further find that the average thread length is 3.9 tweets, while the average lag is 1.8 tweets.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Conversation threads. Left panel: 3rd-person",
      "page": 2
    },
    {
      "caption": "Figure 1: (right panel) shows how",
      "page": 2
    },
    {
      "caption": "Figure 1: (right panel) depicts a thread of",
      "page": 2
    },
    {
      "caption": "Figure 2: Word count distribution in SPIRS",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Intended sarcasm": "Perceived sarcasm"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Contextualized Sarcasm Detection on Twitter",
      "authors": [
        "David Bamman",
        "Noah Smith"
      ],
      "year": "2015",
      "venue": "Ninth International AAAI Conference on Web and Social Media"
    },
    {
      "citation_id": "2",
      "title": "Semi-supervised Recognition of Sarcastic Sentences in Twitter and Amazon",
      "authors": [
        "Dmitry Davidov",
        "Oren Tsur",
        "Ari Rappoport"
      ],
      "year": "2010",
      "venue": "Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL '10"
    },
    {
      "citation_id": "3",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "4",
      "title": "Irony and Sarcasm: Corpus Generation and Analysis Using Crowdsourcing",
      "authors": [
        "Elena Filatova"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)"
    },
    {
      "citation_id": "5",
      "title": "On the psycholinguistics of sarcasm",
      "authors": [
        "Raymond Gibbs"
      ],
      "year": "1986",
      "venue": "Journal of Experimental Psychology: General"
    },
    {
      "citation_id": "6",
      "title": "Identifying Sarcasm in Twitter: A Closer Look",
      "authors": [
        "Roberto González-Ibáñez",
        "Smaranda Muresan",
        "Nina Wacholder"
      ],
      "year": "2011",
      "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers"
    },
    {
      "citation_id": "7",
      "title": "Automatic acquisition of hyponyms from large text corpora",
      "authors": [
        "Marti Hearst"
      ],
      "year": "1992",
      "venue": "The 15th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "How Do Cultural Differences Impact the Quality of Sarcasm Annotation?: A Case Study of Indian Annotators and American Text",
      "authors": [
        "Aditya Joshi",
        "Pushpak Bhattacharyya",
        "Mark Carman",
        "Jaya Saraswati",
        "Rajita Shukla"
      ],
      "year": "2016",
      "venue": "Proceedings of the 10th SIGHUM Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities",
      "doi": "10.18653/v1/W16-2111"
    },
    {
      "citation_id": "9",
      "title": "The perfect solution for detecting sarcasm in tweets #not",
      "authors": [
        "Christine Liebrecht",
        "Florian Kunneman",
        "Antal Van Den",
        "Bosch"
      ],
      "year": "2013",
      "venue": "Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "10",
      "title": "Exploring author context for detecting intended vs perceived sarcasm",
      "authors": [
        "Silviu Oprea",
        "Walid Magdy"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1275"
    },
    {
      "citation_id": "11",
      "title": "iSarcasm: A Dataset of Intended Sarcasm",
      "authors": [
        "Silviu Oprea",
        "Walid Magdy"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      "doi": "10.3115/v1/D14-1162"
    },
    {
      "citation_id": "13",
      "title": "Sarcasm Detection on Czech and English Twitter",
      "authors": [
        "Tomáš Ptáček",
        "Ivan Habernal",
        "Jun Hong"
      ],
      "year": "2014",
      "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers"
    },
    {
      "citation_id": "14",
      "title": "Sarcasm as Contrast between a Positive Sentiment and Negative Situation",
      "authors": [
        "Ellen Riloff",
        "Ashequl Qadir",
        "Prafulla Surve",
        "Lalindra Silva",
        "Nathan Gilbert",
        "Ruihong Huang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Getting reliable annotations for sarcasm in online dialogues",
      "authors": [
        "Reid Swanson",
        "Stephanie Lukin",
        "Luke Eisenberg",
        "Thomas Corcoran",
        "Marilyn Walker"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14)"
    },
    {
      "citation_id": "16",
      "title": "Reasoning with Sarcasm by Reading In-Between",
      "authors": [
        "Yi Tay",
        "Anh Luu",
        "Siu Hui",
        "Jian Su"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P18-1093"
    }
  ]
}