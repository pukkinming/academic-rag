{
  "paper_id": "2210.05419v1",
  "title": "Exploring Interactions And Regulations In Collaborative Learning: An Interdisciplinary Multimodal Dataset",
  "published": "2022-10-11T12:56:36Z",
  "authors": [
    "Yante Li",
    "Yang Liu",
    "Kh√Ånh Nguyen",
    "Henglin Shi",
    "Eija Vuorenmaa",
    "Sanna Jarvela",
    "Guoying Zhao"
  ],
  "keywords": [
    "multimodal dataset",
    "Collaborative learning",
    "Facial expression",
    "Gesture",
    "Physiological signal"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Collaborative learning is an educational approach that enhances learning through shared goals and working together. Interaction and regulation are two essential factors related to the success of collaborative learning. Since the information from various modalities can reflect the quality of collaboration, a new multimodal dataset with cognitive and emotional triggers is introduced in this paper to explore how regulations affect interactions during the collaborative process. Specifically, a learning task with intentional interventions is designed and assigned to high school students aged 15 years old (N=81) in average. Multimodal signals, including video, Kinect, audio, and physiological data, are collected and exploited to study regulations in collaborative learning in terms of individual-participantsingle-modality, individual-participant-multiple-modality, and multiple-participant-multiple-modality. Analysis of annotated emotions, body gestures, and their interactions indicates that our multimodal dataset with designed treatments could effectively examine moments of regulation in collaborative learning. In addition, preliminary experiments based on baseline models suggest that the dataset provides a challenging in-the-wild scenario, which could further contribute to the fields of education and affective computing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Collaborative learning is a social system in which groups of learners solve problems or construct knowledge by working together  [4] . Recent findings demonstrate that collaborative learning can promote higher-level thinking, oral communication, leadership skills, student-faculty interaction, and student responsibility  [40] . Although many factors can affect collaborative learning, social interaction has been considered one of the most important  [31, 38] . To succeed in collaboration, learners should actively exchange their ideas, experience, resources, skills, and feelings within a team  [35, 36] . According to the research on promises of interactivity  [36] , interactions enable collaborators to learn and encourage them to be focused, participative and dedicated to interchange ideas with each other. To this end, studying and promoting the interactions in a collaborative setting will provide valuable insight into the quality of collaboration and be significant and helpful in various fields, especially education research  [7] .\n\nThere is a growing interest in studying interactions in a collaborative learning context by utilizing emotional and physiological measures in recent years  [3, 4] . Thanks to the development of the hardware and AI technologies [  14-16, 20, 21] , it is convenient to unobtrusively and automatically capture the physiological and visual signals of team members, which makes it possible to study the correspondence between multimodal signals for observing interactive processes during collaborative learning.\n\nA variety of research has studied emotional interactions among learners  [3, 17] . The results have revealed that positive emotional interactions are related to better collaboration. However, previous studies only focus on the facial expressions and the physiological signals in interactions independently. Many studies illustrate that the body gesture can also provide emotional clues  [18, 22] , and body gestures in interactions can contribute to solving cooperation and collaborative problem  [5] . In this paper, we design a collaborative learning task and collect a multimodel dataset, including video, Kinect video, audio, and physiological data, to analyse collaborative learning interactions, as shown in Fig.  1 .\n\nAnother critical factor for successful learning is regulation  [9] . Socially shared regulation can promote productive collaborative learning. Thus, we introduce regulation processes in collaborative learning by designing interventions, i.e., the cognitive trigger and the emotional trigger in our task setting. Through analyzing different features such as emotional reflection during triggers, researchers can inspect whether and how those external events influence interactions of group members.\n\nIn this paper, a new dataset is collected in terms of multiple modalities to comprehensively explore the regulation of learning and the aroused emotional interactions in collaborative learning. As far as we know, this is the first multimodal dataset for studying regulation in collaborative learning with regulatory triggers. The main contributions are as follows:\n\n‚Ä¢ We design a collaborative learning task with two kinds of triggers, i.e., cognitive and emotional triggers, to study the regulation inference in collaborative learning. ‚Ä¢ We collect a multimodal dataset consisting of video, Kinect, audio, and physiological data to analyse collaborative learning interctions.\n\n‚Ä¢ Statistical analysis and baseline experiments demonstrate that the regulation significantly impacts interactions during the collaborative process.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Manuscript Submitted To Acm",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Prior Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Collaborative Learning",
      "text": "Collaborative learning is an educational approach for enhanced learning  [7]  where two or more learners work together to solve problems, complete tasks, or learn new concepts. Learners work as a group rather than individually to obtain a complete understanding by interchanging their ideas, processing and synthesizing information instead of using rote memorization of texts  [35] . According to recent studies, collaborative learning can boost higher-level thinking, oral communications, leadership skills, student-faculty interactions, self-esteem and responsibility of students  [13] .\n\nVarious interactions emerging in the collaborative learning process are essential features of effective learning. Many researchers have studied emotional interactions in collaborative learning. Webb et al.  [42]  found that positive emotional interactions, like support and respect, are related to better collaboration. Dindar et al.  [3]  revealed that video-based facial emotion recognition helped explain social and affective dynamics in collaborative learning research. Besides emotions, physiological signals also serve crucial functions in studying collaborative processes  [4] . Several studies have investigated the interaction in collaborative learning by identifying the synchronizing extent of physiological signals  [3] . Additional work verified that gestures served a variety of signalling functions in collaborative problem-solving communication and had a diagnostic role for team members  [5, 34] .\n\nDue to the contribution of different modalities, existing research has focused on studying the collaborative learning process by exploring multimodal signals  [25, 33] . Nguyen et al.  [25]  developed a deep learning model to automatically detect interaction types for regulations in collaborative learning by analyzing electrodermal activities (EDA), video, and audio data. Reilly et al.  [33]  studied the Kinect and speech data and demonstrated how specific movements and gestures positively correlate with collaboration and learning gains. Unlike previous methods introducing only two or three modalities, in this paper, we collected a multimodal dataset including facial video, audio, gesture, EDA, heart rate (HR), and accelerometer to explore the collaborative process comprehensively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Regulation In Collaborative Learning",
      "text": "Recent research has highlighted the importance of co-regulation and socially shared regulation of learning to the group's collaborative learning success  [23] . While self-regulated learning depicts the individual process of monitoring, reflecting, and correcting one's emotion, motivation, and cognition towards attaining learning goals, co-regulation and socially shared regulation refer to this process in collaborative learning at the group level  [47] . Co-regulation of learning relates to the co-operation of regulation in which self-regulated learning occurs with support from another learner. However, socially shared regulation of learning involves learners in a group interdependently regulating the group's collaborative learning process and jointly regulating individual learning processes through social interactions  [6] . Therefore, besides multiple modalities, we also study regulation in collaborative learning tasks by introducing designed interventions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Relevant Datasets",
      "text": "There are currently various multimodal datasets used for studying emotion and gesture in collaborative learning. CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset  [43]  contains more than 23,500 sentence utterance videos from online YouTube speakers. It has three modalities: language, visual, and acoustic, annotated for six basic emotions and five sentiments. The EmoReact Dataset  [27]  is a multimodal emotion dataset of children which contains 1102 audio-visual clips of 17 different emotional states: six basic emotions, neutral, valence, and nine complex emotions, including uncertainty, curiosity, and frustration. Moreover, the Persuasive Opinion Multimedia (POM) corpus  [29]  consists of 1,000 movie review videos obtained from a social multimedia website. This dataset includes three modalities, video, text, and acoustic, and annotates for multiple speaker traits. Although the above multimodal datasets are compatible for analyzing emotions in collaborative learning, they only consider individuals instead of interactions among multiple members in a group. Zhang et al.  [45]  proposed a dataset studying the social relation between two or more people in one image or video. Alternatively, Kosti et al.  [12]  presented the EMOTIC dataset that involved scene context in addition to facial expression and body pose for extra information on emotion perception. However, these datasets only consider the visual modality and aim to study social or semantic relations.\n\nBy contrast, our work establishes a multimodal dataset in collaborative learning scenario. It explores the interactions among group members and introduces regulations by designed interventions, which provides an interdisciplinary platform for studying emotion regulations and their impacts in collaborative learning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset Collection",
      "text": "To systematically and comprehensively study the process of collaborative learning, we collect a multimodal dataset that contains facial videos, audio, physiological signals (including EDA, heart rate, and accelerometer), and Kinect data. This multimodal dataset is valuable for exploring:\n\n‚Ä¢ Whether different modalities have underlying correlations in the collaborative process.\n\n‚Ä¢ Whether the fusion of various data sources could facilitate the task of collaborative learning.\n\n‚Ä¢ Whether the regulation has impacts on multiple modalities during the collaborative process.\n\nDetails of the data collection and annotation are explained in the following subsections.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Equipment Setup And Data Synchronization",
      "text": "Our data recording was held in a laboratory studio, and the setup is shown in Fig.  1  (a) and (b). Specifically, three participants sit in front of laptops. Two-meter COVID social distance was kept between participants during data collection.\n\nA 360 ‚Ä¢ camera (Insta360 Pro) that contains six camera spots and a microphone was placed in the center. The six cameras are hardware synchronized, and the grabbed frames from the six channels are used for building the whole environment in 360 degrees. During the collection, each participant was facing one camera directly. This way, we could have a compact frontal face for every participant, as shown in Fig.  2 . Resolutions of individual video and reconstructed video are 3840 x 2160 and 1920 x 960, respectively, with an average recording rate of 30 fps. Furthermore, a surveillance camera was applied to monitor and recall. Three individual microphones were employed to record the audio data.\n\nTwo Kinect cameras (Azure Kinect DK) were utilized to capture the gesture of the three participants. The two devices were denoted as 'Master Kinect' and 'Salve Kinect' and synchronized by a cable automatically. Its average fps was around 30. Five sensor streams are aggregated in the Kinect camera, including a depth camera, a color camera, an infrared camera, IMU (Inertial Measurement Unit) and microphones. The Azure Kinect Viewer can visualize all the streams, as shown in Fig.  3 (a) .\n\nPhysiological data, including EDA, HR, and accelerometer, were captured by Physiological sensors (Shimmer GSR3+) as shown in Fig.  3 (b) . All the signals were collected at the sampling rate of 128 Hz, which could be used to reveal new insights into the emotional and cognitive processes in collaborative learning regulation  [4] .   Although the above multimodal data offers promising capabilities for analysis, the synchronization of multiple modalities collected from different channels is usually challenging in both methodological and theoretical aspects.\n\nIn order to reach the finest granularity synchronization possible, the data synchronization was planned before the official collection in which each data collection device clock was synchronized to record Unix timestamp. The real-time timestamps were then used for data synchronization. The audio and video metadata were tracked with device-recorded Unix timestamps, while every record of physiological data was also associated with a specific Unix timestamp. Finally, the Kinect data was synchronized with physiological data by the frame change of the video played during the introduction of the collaborative tasks.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Participants",
      "text": "The study involved small groups of three high school students aged 15 years old in average (N=81, Male = 45, Female =36) who worked on a collaborative task. The participants were recruited from high school classes through collaboration with the local teacher training school. As a group, they worked on a shared Google document to design a healthy diet for a customer based on described nutritional needs.\n\nSpecifically, the students were divided into 28 groups. Twenty-five groups were with three students. Three groups were only with two students. In order to study the external events' influence on collaborative learning, we designed the cognitive and emotional triggers in the middle of the learning procedure. During the entire collaborative process, the groups were divided into three types to evaluate the impacts of different triggers: Group A: without control (9 groups);\n\nGroup B: one cognitive trigger (9 groups); Group C: one cognitive trigger and three emotional triggers (10 groups).\n\nThe purpose and procedure of this research were explained to the students before the recording started. All students were aware that they could withdraw at any time of the collection. All students were asked to sign the consent form when he/she understood the contents and agreed to participate in the study. Special questions were included in the consent form concerning data sharing related issues. Moreover, their guardians were informed about the study and also received GDPR document before the data collection.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Learning Procedure",
      "text": "During the collection, participants would act as nutrition specialists for a smoothie caf√©. Their task was to plan a recipe for customers that supported the immune system during the pandemic. As shown in Fig.  4 , the learning procedure was divided into several phases in the following order:\n\ni INTRO (5 mins): An introduction video about general and practical issues was first played to help participants become familiar with the experimental setup and learning target.\n\nii 1st SELF-REPORT (5 mins): Participants were asked to fill out pre-situated self-reports about their study habits, thoughts, and feelings.\n\niii 1st FREE-DISCUSSION (15/25 mins): Participants worked on the meal plan via a shared Google Sheet document (see Fig.  5 ). Detailed task descriptions and all the needed materials could also be found in the document. When the cognitive trigger applies, the customer says that she has an allergy to latex protein and dairy products. After that, the group will also be presented several emotional triggers at a specific time interval. Zoom in for a better view. v 2nd SELF-REPORT (5-15 mins): All trigger moments were displayed to participants after they submitted the result. They were asked to recall their emotions during the triggers and fill out the post-situated self-reports. One researcher was in the room with each group the whole time to ensure the smooth procedure of the experiment but would not be involved in the collaborative learning and answer any task-related questions. Smoothie vouchers were promised to motivate participants to engage in the learning task.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Amount",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Annotation",
      "text": "Based on the video, the data 30 seconds before and after every trigger were annotated with three emotion categories, including negative, positive, and neutral. The annotation process was conducted in three steps. First, we extracted the frames around the cognitive and emotional triggers and roughly cropped the facial regions to make the facial expression easy to be followed. Second, ten annotators worked independently after a preparatory course. Each annotator was required to annotate three trigger clips. Labels were assigned to every trigger clip in seconds instead of frames because the emotion changes in an evolutionary manner. A tool was developed to play the frames in seconds for annotation continuously. Finally, we carried out fine-grained annotations by a 3-fold validation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Data Statistics And Samples",
      "text": "Due to an unexpected hardware failure, the physiological data of nine participants and 360 ‚Ä¢ videos of three participants were lost. Eventually, the rest 78 participants' data are complete and have been processed for analysis. Around 2730 minutes of frontal facial videos and audio data were recorded from 78 participants. Around 630 minutes of Kinect videos were collected from 30 participants of Group C with both cognitive and emotional triggers. Moreover, around 2040 minutes of physiological data were collected, including HR, EDA, and accelerometer. The data statics of multiple modalities are presented in Tab. 1.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Preprocessing",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion Extraction From Video Data",
      "text": "Two state-of-the-art emotion recognition methods proposed in 2021 are employed to fully use our 2D frontal video data, i.e., EmoNet  [41]  and Emotion-GCN  [1] , which can extract discrete and continuous emotion simultaneously. Before feeding data into networks, face detection was firstly conducted with Dlib, a toolkit containing machine learning algorithms, to crop the facial region from collected videos. The emotion of each frame in all the videos were extracted by EmoNet and Emotion-GCN methods with cropped faces as inputs. 4.1.2 Approach 2 -Emotion-GCN. Emotion-GCN  [1]  is a multi-task learning framework that exploits dependencies between seven discrete emotional labels (Neutral, Happy, Sad, Surprise, Fear, and Anger) and two continuous affective dimensions (Valence and Arousal) using stacked Graph Convolutional Networks (GCNs)  [10] , which guide the representation learning of a backbone network for facial expression recognition in-the-wild. The Emotion-GCN was trained and evaluated on AffectNet and Aff-Wild2  [11]  datasets. The accuracy of the categorical model is 66.46% and 48.92%, respectively. The CCC performance of valence and arousal on the dimensional model are 0.767 and 0.649, and 0.457 and 0.514, respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Skeleton Extraction From Kinect Data",
      "text": "The skeleton was extracted using the Kinect Azure devices, as shown in Fig.  6 . The engagement level of each subject is measured using the Euclidean distance of all joints moved within every second, which can be referred to as the summed movement speed of all joints. We calculated the joint movement speed during the extraction using three sets of joints, i.e., full-body joints with the head, upper body joints including the head, and upper body joints without the head. Besides, we also computed the leaning angle of the spine (to the forward direction) per second, which indicates how the subject is focusing.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Physiological Data",
      "text": "The collected physiological data from Shimmer devices were extracted and then matched with associated learners in every group. Different data types were separated from the raw physiological data, including HR, EDA, and accelerometer.\n\nWe conducted the decomposition to extract phasic and tonic features for EDA data. Furthermore, the trough-to-peak analysis using the Neuro2kit package was applied to detect distinct skin conductance responses that reflected emotional arousal moments.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Data Analysis And Results",
      "text": "This section evaluates the effectiveness of our designed triggers and their impacts on interactions in terms of individualparticipant-single-modality, individual-participant-multiple-modality, and multiple-participant-multiple-modality. Note that changes of physiological signals referring to triggers are easily overwhelmed by motion noises  [44, 46] , so we exclude them in this work.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Emotion Evaluation On The Video Data",
      "text": "As introduced in Sec. 4.1, we evaluate the three pre-trained models, EmoNet5, EmoNet8, and Emotion-GCN, using the annotated facial expressions from video data around triggers and providing baselines for the task of emotion recognition.\n\nFor discrete categories, since three emotion categories are considered during annotation, the emotion extracted by pre-trained models are mapped to Positive, Negative, and Neutral. The Happy is mapped to Positive; the Sad, Surprise, Fear, Anger; and Contempt are mapped to Negative. In this case, the EmoNet5, EmoNet8, and Emotion-GCN achieve an accuracy of 51%, 48%, and 35%, respectively. The EmoNet outperforms Emotion-GCN, and the EmoNet5 achieves the best performance. One possible reason is that our dataset is in-the-wild and has various head poses and occlusions.\n\nThe EmoNet aggregated face-alignment task can alleviate head poses and occlusions to some extent. Among the three emotions, we observe that Negative gets the best performance, as shown in the confusion matrix in Fig.  7 , while Neutral is easy to misclassify to Negative. This phenomenon might be caused by significant face deformation because participants always look down at the laptop screen during the learning procedure. In general, the results suggest that the collected in-the-wild dataset of collaborative learning has head pose and face occlusion factors, which could be challenging in the facial expression recognition task.\n\nIn addition, We compare the valence with the emotion value reported in the self-reports to validate the continuous emotion. At the end of data collection, the participants were asked to watch the recorded video during the trigger clips and report their emotions when the trigger happened. Since they tend to report the emotions at the highest intensity after hearing the trigger, the predicted valence at the highest arousal after the trigger in 10s was compared with the self-reported valence. The average L1 norm distance between predicted and self-reported valence is 0.44, 0.44, and 0.46 on EmoNet5, EmoNet8, and Emotion-GCN. The results further indicate that the EmoNet based on face-alignment task have strength with the in-the-wild situation with various head poses and occlusions.\n\nManuscript submitted to ACM Fig.  8 . Emotion distribution before and after triggers. '-' and '+' represent before and after trigger, respectively.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Effect Of Triggers On Emotion",
      "text": "The distribution of facial expressions before and after triggers in the 30 seconds is calculated in order to study the effect of the cognitive trigger (denoted as 'CT') and emotional triggers (denoted as 'ET') on emotion changes. From Fig.  8 , it can be seen that the percentage of positive emotion increases when triggers activate. In particular, for the first emotional trigger 'ET1', the percentage of positive emotion is raised by 25% compared with the emotion before the trigger, which is consistent with what was observed during the experiment. It means that the special request of the customer in CT and 'hurry up' in ETs did not arouse the participants' pressure or nervousness. One possible reason is that the participants understood it was just an experiment instead of a real scene of a smoothie store. In addition, they are teenagers in high school who are lively and laughing  [39] . Nevertheless, the results still illustrate that triggers impact emotions during collaborative learning. Chi-square results also confirmed the significant difference in the emotion distribution around different types of triggers, ùúí 2 (6, ùëÅ = 5940) = 27.52, ùúå < .001.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Emotion Correlation Between The Participants",
      "text": "This subsection investigates whether there is an association between the emotions of two participants during the trigger moments. Considering that the correlation between the paired emotions is not a linear relationship, we exploit Spearman's correlation to assess the emotional relationship between the participants  [30] . Spearman's rank-order correlation measures the strength and direction of the association/relationship between two continuous or ordinal variables  [32] . The correlation coefficient (ùëü ùë† ) can take values from +1 to -1, which indicates a perfect positive (+1) or negative (-1) association of ranks.\n\nTab. 2 shows the correlation coefficient (ùëü ùë† ) and two-tailed significance value (ùúå-value) of Spearman's rank-order correlation between paired emotions during different triggers. The duration of each trigger is 60 seconds. As there are three participants in every group, the emotions of three participants are concatenated to  and ùúå < 0.0005. There was a statistically median positive correlation between the emotions in these groups. Especially for the ET2 of G7, there was a statistically significant, strong positive correlation between emotions during ET2 of G7, ùëü ùë† (178) = .728, ùúå < .0005. When we investigate the original emotions in these groups, it can be observed that the positive emotion occupied a more significant proportion compared with the other groups without statistically significant emotion correlation. The results suggested that positive emotion has a relationship with positive empathy, as described in the study of positive socioemotional interactions in collaborative learning  [17] . In other words, positive socioemotional interactions lead to higher social regulation and better collaboration.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Skeleton Analysis",
      "text": "To study the association of gestures between participants, we exploit Spearman's correlation to assess the emotional relationship between skeleton moving speed extracted from the videos of master and slave Kinects. As shown in Tab. 3, there are more groups with a statistically significant, median positive correlation between the gesture speeds during 'CT' compared with other triggers. One possible reason is that 'CT' is the first trigger proposing the diet request leading to a stronger impression on the participants. There is no statistically significant, strong positive correlation in gesture speed between the skeleton in master and slave Kinects. A possible explanation for this might be the dynamic nature of regulation and interactions in collaborative learning  [9] .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Multimodal Analysis",
      "text": "Apart from the single-modal analysis above, exploring whether there is an association between different modalities is exciting and necessary. To obtain a holistic view of all the trigger moments, we exploit continuous emotional states and body moving speeds around each trigger time-point and use data of G6 as an example for visualization. Fig.  9  shows valence, arousal, and skeleton movement trends of G6. For the valence, significant changes appear after applying every trigger, which confirms the result of Sec.  shows that the effect of the same trigger decreases as the number of applications increases. In general, regardless of the modality or trigger, their trends correspond to previous correlations of emotion and skeleton. Similar consistency exists for other groups of this dataset, which demonstrates the potential value of our dataset in analyzing collaborative learning.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Discussion",
      "text": "This paper introduces a novel multimodal dataset specifically designed to study regulation in collaborative learning.\n\nThe dataset of collaborative learning groups contains 81 video clips from individual learners, which are annotated for three emotion labels around the intervention events, 28 360-stitched videos for learning groups, 18 Kinect depth sensor videos, and 66 128 Hz physiological signals. We respond to the recent calls to utilize multimodal data and advanced machine learning technologies to reveal the \"unobservable\" emotional and cognitive processes in collaborative learning and the induced regulation  [8, 26] . This paper also demonstrates a interdisciplinary approach with multimodal data to examine interactions for regulation in collaborative learning.\n\nConsistent with the literature  [25, 26] , according to the data annotation and emotion evaluation on the video data, we found that facial expression recognition with in-the-wild interaction situations is challenging due to the head poses, occlusions, and non-emotional facial movements like talking  [19] . As a result, novel methods such as more robust facial expression recognition algorithms are needed to be developed in the future. However, since learning regulation may rarely occur through interactions in collaborative learning  [28] , existing datasets on collaborative learning would not be sufficient to develop such methods specifically for examining the regulation of learning. Accordingly, the contribution of this study has been to provide a multimodal dataset with designed interventions for regulation in collaborative learning. This dataset has significant implications for further methodological development and theoretical advancement in researching, understanding, and supporting regulation in collaborative learning. Another substantial contribution of this study relates to the interdisciplinary approach with preliminary results to examine the utility of the proposed dataset. Our results reported a significant difference in emotion distribution among different interventions to trigger regulation in collaborative learning. Our findings support previous learning sciences research of regulation in collaborative learning  [37] . It demonstrates that the learning regulation is a temporal, cyclical, and dynamic process, and such external events would dynamically influence students' learning interactions. Our results also indicate that positive emotions are associated with positive empathy and better interaction. However, the gesture interaction has dynamic performance during different regulatory moments. In line with previous studies  [4] , our results reveal that emotions aroused by the regulation of learning are displayed in multimodal emotional behaviors.\n\nOur interdisciplinary approach in this presented study also responds to the recent calls for interdisciplinary effort bridging learning sciences, machine learning, and affective computing to maximize the impact of multimodal data and advanced techniques in examining and supporting regulation in collaborative learning (  [8, 26] . This paper contributes to both the field of computer sciences by offering a novel dataset for multimodal model development and to the field of learning sciences by providing new insights into the trigger moments for regulation in collaborative learning.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "Limitation And Future Work",
      "text": "Since this is a preliminary study of multimodal analysis for collaborative learning, several limitations should be addressed in our future work. One thing is the insufficient annotated data. We only annotated the clips around triggers as a test set of the collected dataset for experiments in this paper. More labeled clips with discrete and dimensional emotions should be considered for a systematic analysis. Besides, instead of directly using pre-trained models, new methods need to be designed for robust emotion recognition in collaborative learning.\n\nOn the other hand, the interaction analysis is mainly based on the correlation among individual facial expressions and gestures. High-level interaction annotations, such as eye contact, should be further considered. Another limitation is that this work only analyzes interactions influenced by triggers. The relationship between emotions and self-regulated learning should be further explored. Finally, multiple modalities, including facial behavior, physiological signal, and gestures, will be integrated and studied for a comprehensive understanding of collaborative learning.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The data collection setup. (a) An illustration of the seating plan and the location of the devices; (b) The real environment of",
      "page": 2
    },
    {
      "caption": "Figure 1: Another critical factor for successful learning is regulation [9]. Socially shared regulation can promote productive",
      "page": 2
    },
    {
      "caption": "Figure 1: (a) and (b). Specifically, three",
      "page": 4
    },
    {
      "caption": "Figure 2: Resolutions of individual video and reconstructed",
      "page": 4
    },
    {
      "caption": "Figure 3: (b). All the signals were collected at the sampling rate of 128 Hz, which could be used to reveal new",
      "page": 4
    },
    {
      "caption": "Figure 2: Example of view of six cameras in 360‚ó¶camera and the synthesized whole 360‚ó¶view. Zoom in for better view.",
      "page": 5
    },
    {
      "caption": "Figure 3: Example of Kinect and physiological data with recording software. Zoom in for better view.",
      "page": 5
    },
    {
      "caption": "Figure 4: , the learning procedure was",
      "page": 6
    },
    {
      "caption": "Figure 5: ). Detailed task descriptions and all the needed materials could also be found in the document.",
      "page": 6
    },
    {
      "caption": "Figure 4: An illustration of the collaborative learning task. Group members work together to prepare a healthy smoothie for a customer.",
      "page": 7
    },
    {
      "caption": "Figure 5: Example of participants‚Äô screen interfaces. The participants are required to make a healthy smoothie caf√©.",
      "page": 7
    },
    {
      "caption": "Figure 6: Examples of the skeleton extracted from master and slave Kinect, respectively.",
      "page": 9
    },
    {
      "caption": "Figure 6: The engagement level of each subject",
      "page": 9
    },
    {
      "caption": "Figure 7: The confusion matrices of the EmoNet5, EmoNet8, and Emotion-GCN on the annotated trigger clips.",
      "page": 10
    },
    {
      "caption": "Figure 8: Emotion distribution before and after triggers. ‚Äò-‚Äô and ‚Äò+‚Äô represent before and after trigger, respectively.",
      "page": 11
    },
    {
      "caption": "Figure 9: shows valence, arousal, and skeleton movement trends of G6. For the valence, significant changes appear after applying",
      "page": 12
    },
    {
      "caption": "Figure 9: Trend visualization of valence, arousal, and skeleton movement around cognitive and emotional triggers. Zoom in for a better",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "AB-yogurt",
          "Amount\n(g)": "250",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": "162,5 675 10 7,5 10 5 0,25 0 0Low"
        },
        {
          "Column_1": "",
          "Column_2": "Curd",
          "Amount\n(g)": "100",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": "70 295 4 12 0,5 0,5 0 0 0Low"
        },
        {
          "Column_1": "Base",
          "Column_2": "",
          "Amount\n(g)": "",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": ""
        },
        {
          "Column_1": "Kiwi",
          "Column_2": "",
          "Amount\n(g)": "165",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": "99 420,8 24,75 1,65 0,83 0,0495 4,95 0,0165 511,5High"
        },
        {
          "Column_1": "",
          "Column_2": "Strawberry",
          "Amount\n(g)": "125",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": "37,5 156,3 10 0,875 0,38 0,1875 2,5 0 187,5Low"
        },
        {
          "Column_1": "Apple",
          "Column_2": "",
          "Amount\n(g)": "10",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": "5 21,5 1,5 0,025 0,02 0,003 0,25 0 10,5Moderate"
        },
        {
          "Column_1": "Fruits and vegetables",
          "Column_2": "",
          "Amount\n(g)": "",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": ""
        },
        {
          "Column_1": "Oatmeal",
          "Column_2": "",
          "Amount\n(g)": "",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": "0 0 0 0 0 0 0 0 0Low"
        },
        {
          "Column_1": "",
          "Column_2": "Whey Protein Powder",
          "Amount\n(g)": "4",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": "16,4 68,6 0,24 3,2 0,24 0 0,08 0 0Low"
        },
        {
          "Column_1": "Other Flaxseed oil",
          "Column_2": "",
          "Amount\n(g)": "13",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": "117 489,5 0 0 13 0,975 0 0 0Low"
        },
        {
          "Column_1": "In total",
          "Column_2": "",
          "Amount\n(g)": "667",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": "507,4 2127 50,49 25,25 25 6,715 8,03 0,0165 709,5"
        },
        {
          "Column_1": "201,96 101 225\nCarbohydProteinFat\nCarbohydrate\nProtein\nFat",
          "Column_2": "",
          "Amount\n(g)": "",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": ""
        },
        {
          "Column_1": "ig.5. Exampleofparticipants‚Äôscreeninterfaces.Theparticipantsarerequiredtomakeahealthysmoothiecaf√©",
          "Column_2": "",
          "Amount\n(g)": "",
          "Energy Energy Carbohy Protein Of which Dietary Sodium Potassium\n(kcal) (kJ) drate (g) (g) Fat (g)saturated (g) fiber (g) (g) (mg) Natural rubber": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Modality": "Video",
          "Column_2": "Face",
          "Group": "23",
          "Sample": "78",
          "length/min": "2730"
        },
        {
          "Modality": "",
          "Column_2": "Kinect",
          "Group": "10",
          "Sample": "18",
          "length/min": "630"
        },
        {
          "Modality": "Audio",
          "Column_2": "",
          "Group": "23",
          "Sample": "78",
          "length/min": "2730"
        },
        {
          "Modality": "Physiologicalsignal",
          "Column_2": "HR",
          "Group": "19",
          "Sample": "66",
          "length/min": "2310"
        },
        {
          "Modality": "",
          "Column_2": "EDA",
          "Group": "",
          "Sample": "",
          "length/min": ""
        },
        {
          "Modality": "",
          "Column_2": "Accelerometer",
          "Group": "",
          "Sample": "",
          "length/min": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Exploiting emotional dependencies with graph convolutional networks for facial expression recognition",
      "authors": [
        "Panagiotis Antoniadis"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "2",
      "title": "Computer programs for the concordance correlation coefficient",
      "authors": [
        "Andrzej Sara B Crawford",
        "Hung-Mo Kosinski",
        "John Lin",
        "Huiman X Williamson",
        "Barnhart"
      ],
      "year": "2007",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "3",
      "title": "Leaders and followers identified by emotional mimicry during collaborative learning: A facial expression recognition study on emotional valence",
      "authors": [
        "Muhterem Dindar",
        "Sanna Jarvela",
        "Sara Ahola",
        "Xiaohua Huang",
        "Guoying Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Detecting shared physiological arousal events in collaborative problem solving",
      "authors": [
        "Muhterem Dindar",
        "Sanna J√§rvel√§",
        "Andy Nguyen",
        "Eetu Haataja",
        "Ahsen √áini"
      ],
      "year": "2022",
      "venue": "Contemporary Educational Psychology"
    },
    {
      "citation_id": "5",
      "title": "Gesture interaction in cooperation scenarios",
      "authors": [
        "Carlos Duarte",
        "Ant√≥nio Neto"
      ],
      "year": "2009",
      "venue": "International Conference on Collaboration and Technology"
    },
    {
      "citation_id": "6",
      "title": "Self-regulation, co-regulation, and shared regulation in collaborative learning environments",
      "authors": [
        "Allyson Hadwin",
        "Sanna J√§rvel√§",
        "Mariel Miller"
      ],
      "year": "2018",
      "venue": "Self-regulation, co-regulation, and shared regulation in collaborative learning environments"
    },
    {
      "citation_id": "7",
      "title": "Collaborative learning for virtual higher education",
      "year": "2021",
      "venue": "Learning, Culture and Social Interaction"
    },
    {
      "citation_id": "8",
      "title": "Bridging learning sciences, machine learning and affective computing for understanding cognition and affect in collaborative learning",
      "authors": [
        "Sanna J√§rvel√§",
        "Dragan Ga≈°eviƒá",
        "Tapio Sepp√§nen",
        "Mykola Pechenizkiy",
        "Paul Kirschner"
      ],
      "year": "2020",
      "venue": "British Journal of Educational Technology"
    },
    {
      "citation_id": "9",
      "title": "Capturing the dynamic and cyclical nature of regulation: Methodological Progress in understanding socially shared regulation in learning",
      "authors": [
        "Sanna J√§rvel√§",
        "Hanna J√§rvenoja",
        "Jonna Malmberg"
      ],
      "year": "2019",
      "venue": "International Journal of Computer-Supported Collaborative Learning"
    },
    {
      "citation_id": "10",
      "title": "Semi-Supervised Classification with Graph Convolutional Networks",
      "authors": [
        "Thomas Kipf",
        "Max Welling"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "11",
      "title": "AFEW-VA database for valence and arousal estimation in-the-wild",
      "authors": [
        "Jean Kossaifi",
        "Georgios Tzimiropoulos",
        "Sinisa Todorovic",
        "Maja Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "12",
      "title": "Context based emotion recognition using emotic dataset",
      "authors": [
        "Ronak Kosti",
        "Jose Alvarez",
        "Adria Recasens",
        "Agata Lapedriza"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "13",
      "title": "Benefits of collaborative learning",
      "authors": [
        "Marjan Laal",
        "Seyed Mohammad"
      ],
      "year": "2012",
      "venue": "Procedia-social and behavioral sciences"
    },
    {
      "citation_id": "14",
      "title": "Deep Learning for Micro-expression Recognition: A Survey",
      "authors": [
        "Yante Li",
        "Jinsheng Wei",
        "Yang Liu",
        "Janne Kauttonen",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Deep Learning for Micro-expression Recognition: A Survey",
      "arxiv": "arXiv:2107.02823"
    },
    {
      "citation_id": "15",
      "title": "Fast cross-scenario clothing retrieval based on indexing deep features",
      "authors": [
        "Zongmin Li",
        "Yante Li",
        "Yongbiao Gao",
        "Yujie Liu"
      ],
      "year": "2016",
      "venue": "Pacific Rim Conference on Multimedia"
    },
    {
      "citation_id": "16",
      "title": "A more effective method for image representation: Topic model based on latent dirichlet allocation",
      "authors": [
        "Zongmin Li",
        "Weiwei Tian",
        "Yante Li",
        "Zhenzhong Kuang",
        "Yujie Liu"
      ],
      "year": "2015",
      "venue": "2015 14th International Conference on Computer-Aided Design and Computer Graphics"
    },
    {
      "citation_id": "17",
      "title": "Affect and engagement during small group instruction",
      "authors": [
        "Lisa Linnenbrink-Garcia",
        "Toni Kempler Rogat",
        "Kristin Koskey"
      ],
      "year": "2011",
      "venue": "Contemporary Educational Psychology"
    },
    {
      "citation_id": "18",
      "title": "iMiGUE: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "Xin Liu",
        "Henglin Shi",
        "Haoyu Chen",
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Visual tracking via salient feature extraction and sparse collaborative model",
      "authors": [
        "Yang Liu",
        "Feng Yang",
        "Cheng Zhong",
        "Ying Tao",
        "Bing Dai",
        "Mengxiao Yin"
      ],
      "year": "2018",
      "venue": "AEU-International Journal of Electronics and Communications"
    },
    {
      "citation_id": "20",
      "title": "Uncertain Label Correction via Auxiliary Action Unit Graphs for Facial Expression Recognition",
      "authors": [
        "Yang Liu",
        "Xingming Zhang",
        "Janne Kauttonen",
        "Guoying Zhao"
      ],
      "year": "2022",
      "venue": "Uncertain Label Correction via Auxiliary Action Unit Graphs for Facial Expression Recognition",
      "arxiv": "arXiv:2204.11053"
    },
    {
      "citation_id": "21",
      "title": "SG-DSN: A Semantic Graph-based Dual-Stream Network for facial expression recognition",
      "authors": [
        "Yang Liu",
        "Xingming Zhang",
        "Jinzhao Zhou",
        "Lunkai Fu"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "22",
      "title": "Graph-based Facial Affect Analysis: A Review of Methods, Applications and Challenges",
      "authors": [
        "Yang Liu",
        "Xingming Zhang",
        "Jinzhao Zhou",
        "Xin Li",
        "Yante Li",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Graph-based Facial Affect Analysis: A Review of Methods, Applications and Challenges",
      "arxiv": "arXiv:2103.15599"
    },
    {
      "citation_id": "23",
      "title": "Capturing temporal and sequential patterns of self-, co-, and socially shared regulation in the context of collaborative learning",
      "authors": [
        "Jonna Malmberg",
        "Sanna J√§rvel√§",
        "Hanna J√§rvenoja"
      ],
      "year": "2017",
      "venue": "Contemporary Educational Psychology"
    },
    {
      "citation_id": "24",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "25",
      "title": "Multimodal Deep Learning Model for Detecting Types of Interactions for Regulation in Collaborative Learning",
      "authors": [
        "Andy Nguyen",
        "Sanna J√§rvel√§",
        "Yansen Wang",
        "Carolyn Ros√©",
        "Jonna Malmberg",
        "Hanna J√§rvenoja"
      ],
      "year": "2021",
      "venue": "Proceedings of the 15th International Conference of the Learning Sciences-ICLS 2021. International Society of the Learning Sciences"
    },
    {
      "citation_id": "26",
      "title": "Exploring Socially Shared Regulation with an AI Deep Learning Approach Using Multimodal Data",
      "authors": [
        "Andy Nguyen",
        "Sanna J√§rvel√§",
        "Yang Wang",
        "Carolyn R√≥se"
      ],
      "year": "2022",
      "venue": "Proceedings of International Conferences of Learning Sciences"
    },
    {
      "citation_id": "27",
      "title": "Emoreact: a multimodal approach and dataset for recognizing emotional responses in children",
      "authors": [
        "Behnaz Nojavanasghari",
        "Tadas Baltru≈°aitis",
        "Charles Hughes",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th acm international conference on multimodal interaction"
    },
    {
      "citation_id": "28",
      "title": "Multimodal data indicators for capturing cognitive, motivational, and emotional learning processes: A systematic literature review",
      "authors": [
        "Omid Noroozi",
        "J H√©ctor",
        "Marta Pijeira-D√≠az",
        "Muhterem Sobocinski",
        "Sanna Dindar",
        "Paul J√§rvel√§",
        "Kirschner"
      ],
      "year": "2020",
      "venue": "Education and Information Technologies"
    },
    {
      "citation_id": "29",
      "title": "Computational analysis of persuasiveness in social multimedia: A novel dataset and multimodal prediction approach",
      "authors": [
        "Sunghyun Park",
        "Suk Han",
        "Moitreya Shim",
        "Kenji Chatterjee",
        "Louis-Philippe Sagae",
        "Morency"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "30",
      "title": "Effective use of Spearman's and Kendall's correlation coefficients for association between two measured traits",
      "authors": [
        "Marie-Therese Puth",
        "Markus Neuh√§user",
        "Graeme Ruxton"
      ],
      "year": "2015",
      "venue": "Animal Behaviour"
    },
    {
      "citation_id": "31",
      "title": "Factors affecting students' learning performance through collaborative learning and engagement",
      "authors": [
        "Muhammad Asif Qureshi",
        "Asadullah Khaskheli",
        "Ahmed Qureshi",
        "Syed Ali Raza",
        "Sara Yousufi"
      ],
      "year": "2021",
      "venue": "Interactive Learning Environments"
    },
    {
      "citation_id": "32",
      "title": "Critical values for Spearman's rank order correlation",
      "authors": [
        "H Philip",
        "Ramsey"
      ],
      "year": "1989",
      "venue": "Journal of educational statistics"
    },
    {
      "citation_id": "33",
      "title": "Exploring Collaboration Using Motion Sensors and Multi-Modal Learning Analytics",
      "authors": [
        "Milan Joseph M Reilly",
        "Bertrand Ravenell",
        "Schneider"
      ],
      "year": "2018",
      "venue": "International Educational Data Mining Society"
    },
    {
      "citation_id": "34",
      "title": "Gesture in collaborative mathematics problem-solving",
      "authors": [
        "J Fiona",
        "Robert Reynolds",
        "Reeve"
      ],
      "year": "2001",
      "venue": "The Journal of Mathematical Behavior"
    },
    {
      "citation_id": "35",
      "title": "Complexity of social interactions in collaborative learning: The case of online database environment",
      "authors": [
        "Rikki Rimor",
        "Yigal Rosen",
        "Kefaya Naser"
      ],
      "year": "2010",
      "venue": "Interdisciplinary Journal of E-Learning and Learning Objects"
    },
    {
      "citation_id": "36",
      "title": "Promises of interactivity: Aligning learner perceptions and expectations with strategies for flexible and online learning",
      "authors": [
        "Rod Sims"
      ],
      "year": "2003",
      "venue": "Distance Education"
    },
    {
      "citation_id": "37",
      "title": "Exploring Adaptation in Socially-Shared Regulation of Learning Using Video and Heart Rate Data",
      "authors": [
        "M√°rta Sobocinski",
        "Jonna Malmberg",
        "Sanna J√§rvel√§"
      ],
      "year": "2021",
      "venue": "Technology, Knowledge and Learning"
    },
    {
      "citation_id": "38",
      "title": "Supporting social interaction in an intelligent collaborative learning system",
      "authors": [
        "Amy Soller"
      ],
      "year": "2001",
      "venue": "International journal of artificial intelligence in education"
    },
    {
      "citation_id": "39",
      "title": "Age of opportunity: Lessons from the new science of adolescence",
      "authors": [
        "D Laurence",
        "Steinberg"
      ],
      "year": "2014",
      "venue": "Age of opportunity: Lessons from the new science of adolescence"
    },
    {
      "citation_id": "40",
      "title": "Collaborative learning at engineering universities: Benefits and challenges",
      "authors": [
        "Olga Sumtsova",
        "Tatiana Aikina",
        "Liudmila Bolsunovskaya",
        "Chris Phillips",
        "Olga Zubkova",
        "Peter Mitchell"
      ],
      "year": "2018",
      "venue": "International Journal of Emerging Technologies in Learning (iJET)"
    },
    {
      "citation_id": "41",
      "title": "Georgios Tzimiropoulos, and Maja Pantic. 2021. Estimation of continuous valence and arousal levels from faces in naturalistic conditions",
      "authors": [
        "Antoine Toisoul",
        "Jean Kossaifi",
        "Adrian Bulat"
      ],
      "year": "2021",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "42",
      "title": "Help seeking in cooperative learning groups",
      "authors": [
        "Noreen M Webb",
        "Marsha Ing",
        "Nicole Kersting",
        "Kariane Mari"
      ],
      "year": "2013",
      "venue": "Help seeking in academic settings. Routledge"
    },
    {
      "citation_id": "43",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "44",
      "title": "Photoplethysmography-based heart rate monitoring in physical activities via joint sparse spectrum reconstruction",
      "authors": [
        "Zhilin Zhang"
      ],
      "year": "2015",
      "venue": "IEEE transactions on biomedical engineering"
    },
    {
      "citation_id": "45",
      "title": "Learning social relation traits from face images",
      "authors": [
        "Zhanpeng Zhang",
        "Ping Luo"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision"
    },
    {
      "citation_id": "46",
      "title": "TROIKA: A general framework for heart rate monitoring using wrist-type photoplethysmographic signals during intensive physical exercise",
      "authors": [
        "Zhilin Zhang",
        "Zhouyue Pi",
        "Benyuan Liu"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on biomedical engineering"
    },
    {
      "citation_id": "47",
      "title": "Handbook of self-regulation of learning and performance",
      "authors": [
        "J Barry",
        "Dale Zimmerman",
        "Schunk"
      ],
      "year": "2011",
      "venue": "Handbook of self-regulation of learning and performance"
    }
  ]
}