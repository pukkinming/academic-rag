{
  "paper_id": "2309.04849v2",
  "title": "Speech Emotion Recognition With Distilled Prosodic And Linguistic Affect Representations",
  "published": "2023-09-09T17:30:35Z",
  "authors": [
    "Debaditya Shome",
    "Ali Etemad"
  ],
  "keywords": [
    "Speech emotion recognition",
    "knowledge distillation",
    "prosodic features",
    "linguistic features"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose EmoDistill, a novel speech emotion recognition (SER) framework that leverages cross-modal knowledge distillation during training to learn strong linguistic and prosodic representations of emotion from speech. During inference, our method only uses a stream of speech signals to perform unimodal SER thus reducing computation overhead and avoiding run-time transcription and prosodic feature extraction errors. During training, our method distills information at both embedding and logit levels from a pair of pre-trained Prosodic and Linguistic teachers that are fine-tuned for SER. Experiments on the IEMOCAP benchmark demonstrate that our method outperforms other unimodal and multimodal techniques by a considerable margin, and achieves state-of-the-art performance of 77.49% unweighted accuracy and 78.91% weighted accuracy. Detailed ablation studies demonstrate the impact of each component of our method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) is a challenging yet crucial task, with applications spanning a broad spectrum from human-computer interaction to mental health diagnostics. The inherent ambiguity in perceiving emotions and the variability across speakers and languages further amplifies the complexity of SER.\n\nSpeech emotion information is present in and can be extracted from two different domains, linguistic and prosodic. The linguistic information includes the semantic aspects of emotion at the word level, while prosodic information includes the melodic aspects such as rhythm, tone, pitch, pauses, etc. Most existing solutions attempt to implicitly learn a combination of the two domains directly from raw speech signals. However, we identify four key problems in this category of approaches as follows: (i) Implicitly learning prosodic information from audio is often less than optimal because the discretization of audio signals during training of leading speech models like Hu-BERT  [1]  and Wav2Vec2  [2]  can lead to the weakening of important prosodic features.\n\n(ii) Direct fine-tuning of existing speech models which were originally trained for Automatic Speech Recognition (ASR), on SER tasks, may not always yield strong performances  [3] . (iii) Direct use of speech transcripts at run-time can lead to low performances due to transcription errors  [4] . (iv) Lastly, the use of both audio and linguistic information at run-time requires a multimodal system which can increase computational overhead.\n\nTo tackle the problems stated above, we propose EmoDistill, an SER method that learns both prosodic and linguistic information during training, but requires only input speech at run-time. Our method distills information from both logits and embeddings through a pre-trained prosodic teacher alongside a pre-trained linguistic teacher to learn unimodal representations for downstream SER. Experiments demonstrate that our method significantly outperforms prior solutions on the IEMOCAP  [5]  dataset to achieve state-of-the-art results. Additionally, ablation studies demonstrate the importance of each component of EmoDistill.\n\nIn summary, we make the following contributions: (1) We introduce EmoDistill, a novel cross-modal Knowledge Distillation (KD) framework for learning unimodal representations from speech that explicitly capture both the linguistic and prosodic aspects of emotions. Unlike multimodal models combining audio and text modalities, EmoDistill doesn't require explicitly transcribed text during inference, thereby reducing the computational overhead and errors that arise from transcription and prosodic feature extraction.  (2)  We empirically evaluate the importance of the ability to capture and distinguish linguistic and prosodic components of emotion in speech through detailed ablation studies. (3) Our rigorous evaluation on the IEMOCAP benchmark in a subject-independent setup demonstrates that EmoDistill outperforms previous state-of-the-art methods and achieves 77.49% unweighted accuracy (UA) and 78.91% weighted accuracy (WA).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "The recent progress of deep learning has had a considerable impact on the field of SER. Mao et al.  [6]    Multimodal methods that incorporate both speech and text in training and run-time have also been explored for SER. Sun et al.  [10]  utilized CNN and CNN-LSTM networks for multimodal SER from speech and text data on the IEMO-CAP corpus. Heusser et al.  [11]  explored multimodal fusion from BiLSTM-based speech features with text features from a pre-trained XLNet language model. Triantafyllopoulos et al.  [12]  studied various combinations of speech features from Multi-stage CNNs and text features from BERT for SER and demonstrated improved performance. Deschamps et al.  [13]  analyzed several multimodal fusion strategies us-ing Wav2Vec2-based speech features and FlauBERT-based text features for SER on an emergency call-center recordings corpus. Ho et al.  [14]  proposed an SER method with a multi-level multi-head attention mechanism for the fusion of MFCC-based audio features and BERT-based text features.\n\nAs discussed in the previous section, fusion-based methods have multiple disadvantages like transcription errors, due to which cross-modal KD is being explored. KD was introduced by Hinton et al.  [15]  for model compression, where they utilized only logit-level information. Subsequently, KD was adapted to transfer cross-modal information in lowresource tasks such as SER. Hajavi et al.  [16]  used video as privileged information for distilling feature-level knowledge into a unimodal student on speech data, and demonstrated improved performance on speaker recognition and SER. Ren et al.  [17]  developed a self-distillation framework for SER aimed at model compression and demonstrated improvements over layer-wise KD.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "The objective of our framework is to train an unimodal speech student model using KD from pre-trained prosodic and linguistic teacher models. The overview of our method is presented in Figure  1 . The details of each component are described as follows.\n\nLinguistic teacher. We consider a teacher model f L T with strong language representations and refer to it as Linguistic teacher. We adopt the pre-trained BERT-base  [18]  model as the backbone for f L T , and perform supervised fine-tuning on the training set of our emotion classification corpus. Prosodic teacher. We consider a teacher model f P T that takes explicit prosodic features as input, and refer to it as Prosodic teacher. We use eGeMAPs Low-Level Descriptors (LLDs)  [19]  as prosodic features, which are commonly used in SER literature. We perform supervised fine-tuning of f P T on the training set of our emotion classification corpus. We adopt a 2D ResNet-based  [20]  backbone for f P T which consists of 4 residual blocks. Student KD. To facilitate knowledge transfer from our Linguistic and Prosodic teacher models, we follow a teacherstudent KD setup and keep the weights of the teachers frozen. We consider a uni-modal speech model f S as the student, which consists of a pre-trained transformer encoder followed by 2 GELU-activated feedforward projection layers for disjoint linguistic and prosodic embeddings. We keep these disjoint to allow optimal embedding-level KD from each teacher without interference. These two embeddings are concatenated and passed on to a feed-forward network (FFN) for final output predictions. First, we transfer the logit-level knowledge using traditional KD with temperature-scaled labels  [15] . Specifically, we minimize the KL-Divergence L KL between the predicted logit distributions of teacher and student models, where the objective becomes:\n\nHere, y S refers to the predictions of the student, while y L and y P represent the predictions of Linguistic and Prosodic teacher models, respectively. In all cases, the predicted logits y are obtained using temperature parameter τ in the output softmax activation function. In practice, we use different values of τ for KD from f L T and f P T . Let z c be the output logits for class c, among a total of N classes. The temperaturescaled logits y c are obtained as:\n\nNext, we use embedding-level KD to transfer knowledge to the student model from the latent space of Linguistic and Prosodic teacher models. Let z L and z P denote the embeddings of Linguistic and Prosodic teachers, while z ′ L and z ′ P denote the embeddings of the student model from linguistic and prosodic projection layers respectively. We minimize the negative cosine similarity L cos among the teacher and student embeddings as follows:\n\nGiven two embeddings a and b, L cos can be defined as:\n\nwhere ∥•∥ 2 represents ℓ 2 -norm. Finally, the total training loss of EmoDistill becomes:\n\nwhere L CE refers to the standard cross-entropy loss, and α, β, γ are loss coefficients.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "We use the Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset for our experiments  [5] . IEMOCAP is the most widely used benchmark for SER. The dataset encompasses roughly 12 hours of audio-visual content, with an average duration of 4.5 seconds for each vocal segment. We only use the audio and text transcriptions in this work.\n\nFollowing prior works, we use 4 categories of emotions: 'neutral', 'angry', 'sad', and 'happy' (merged with 'excited' class).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "We train all models on 4×NVIDIA A100 GPUs, using a batch size of 128, except for EmoDistill w/HuBERT-large for which we use batch size of 64 due to computational limitations. We use AdamW optimizer with CosineAnnealingWarmup learning rate (LR) scheduler starting with a base LR of 1 × 10 -4 . For logit-level KD from the Prosodic teacher, a temperature τ P = 0.5 is chosen, while for the Linguistic teacher temperature τ L = 5 (see ablation experiments in Section 4.3). α = 1, β = 10, γ = 2 are used as loss coefficients. The pretrained weights of HuBERT-base and HuBERT-large were obtained from TorchAudio. For BERT-base, we use the 'bertbase-uncased' checkpoint from HuggingFace. For extracting eGeMAPs LLDs, we use the opensmile-python toolkit  [21] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Discussion",
      "text": "Performance. Following prior works, we evaluate EmoDistill on the IEMOCAP benchmark using 10-fold cross-validation in the leave-one-speaker-out scheme. The results are shown in Table  1 . It can be clearly seen that EmoDistill significantly outperforms prior works in terms of both WA and UA metrics, with improvements of up to 7.26% in UA and 4.99% in WA over the best previous method  [22] . Furthermore, we observe that while our method is technically not multi-modal as it only uses a single modality during inference, it still outperforms prior works that have dedicated components for different text and audio modalities in the literature  [11, 10, 14, 12] . Ablation studies. To understand the impact of each component of EmoDistill, we conduct a systematic ablation study and present the results in Table  2 . First, we individually remove the L logits as well as L embedding and observe between 1% to 2% drop in performance in each case. Next, we ablate the model by individually removing the entire Prosodic and Linguistic teachers (f P T and f L T ). In this experiment, we observe that while the removal of either component degrades performance, the ablation of the Linguistic teacher has a more significant negative impact. We then ablate both teachers together (f P T and f L T ), essentially only using the HuBERT-base backbone with fine-tuning for SER, and observe a considerable drop in performance. Finally we remove the student network along with either of the teachers, essentially only using the remaining teacher for inference. We observe here that while both tests result in a considerable drop in performance, the removal of f S and f L T together has the highest negative impact, indicating that linguistic information is crucial, and prosodic information can serve as complementary knowledge to improve SER but can't replace linguistic information.\n\nNext, we aim to analyze the impact of the temperature parameter τ on the performance. To this end, remove f L T and set the prosodic temperature parameter τ P to 0.1, 0.5, 2, and 4. Similarly, we remove f P T and set the linguistic temperature parameter τ L to 0.5, 2, 4, and 10. As shown in Figure  2  (Left), τ P = 0.5 (hard-logits) works best and increasing τ P shows strong decline in performance. In the second case, as shown in Figure  2  (Right), we observe that τ L = 4 (softlogits) works best and decreasing τ L leads to a strong decline in performance. Although standard logit-level KD methods use soft logits (τ > 1), we observe that soft logits don't work well for the Prosodic teacher f P T . Our intuition is that since f P T is a weak teacher (see Table  2 ), smaller temperature values result in hard logits as per Eq. 2, and therefore improve performance by providing stronger supervision signals through distillation. Finally, we observe that for both teachers, too high or low temperatures lead to a drop in performance.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We present EmoDistill, a novel cross-modal knowledge distillation framework for learning emotion representations from speech. EmoDistill explicitly captures linguistic and prosodic aspects of emotions in a unimodal inference setup, reducing computational overhead and limitations like transcription and prosodic feature extraction errors. For training our framework, EmoDistill extracts information from both the embedding and logit levels through a pair of pre-trained Prosodic and Linguistic teacher models that have been fine-tuned for SER. Experiments on the commonly used SER benchmark IEMOCAP demonstrates that our method considerably outperforms other state-of-the-art methods by achieving 77.49% (7.26% improvement) and 78.91% (4.99% improvement) weighted and unweighted accuracies. We demonstrate the importance each component of our method through detailed ablation experiments. Practical applications of EmoDistill may include scenarios with low compute resources, and where emotions are expressed not only through language and semantics but also through prosody.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: EmoDistill Framework. Our student network is trained using a distillation of logit-level and embedding-level knowledge",
      "page": 2
    },
    {
      "caption": "Figure 1: The details of each component are de-",
      "page": 2
    },
    {
      "caption": "Figure 2: Left: We remove f L",
      "page": 4
    },
    {
      "caption": "Figure 2: (Left), τP = 0.5 (hard-logits) works best and increasing",
      "page": 4
    },
    {
      "caption": "Figure 2: (Right), we observe that τL = 4 (soft-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: Ablation study demonstrating the impact of key",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "WA",
          "UA": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: SER results on IEMOCAP. Bold denotes the best",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "UA\nWA",
          "Column_5": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "5",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "7",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE Trans. on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "D Luo",
        "Y Zou",
        "D Huang"
      ],
      "year": "2018",
      "venue": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition"
    },
    {
      "citation_id": "9",
      "title": "A fine-tuned Wav2Vec 2.0/HuBERT benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned Wav2Vec 2.0/HuBERT benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "10",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Multimodal cross-and selfattention network for speech emotion recognition",
      "authors": [
        "L Sun",
        "B Liu",
        "J Tao",
        "Z Lian"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Bimodal speech emotion recognition using pre-trained language models",
      "authors": [
        "V Heusser",
        "N Freymuth",
        "S Constantin",
        "A Waibel"
      ],
      "year": "2019",
      "venue": "Bimodal speech emotion recognition using pre-trained language models",
      "arxiv": "arXiv:1912.02610"
    },
    {
      "citation_id": "13",
      "title": "Multistage linguistic conditioning of convolutional layers for speech emotion recognition",
      "authors": [
        "A Triantafyllopoulos",
        "U Reichel",
        "S Liu",
        "S Huber",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "14",
      "title": "Exploring attention mechanisms for multimodal emotion recognition in an emergency call center corpus",
      "authors": [
        "T Deschamps-Berger",
        "L Lamel",
        "L Devillers"
      ],
      "year": "2023",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network",
      "authors": [
        "N Ho",
        "H Yang",
        "S Kim",
        "G Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "17",
      "title": "Audio representation learning by distilling video as privileged information",
      "authors": [
        "A Hajavi",
        "A Etemad"
      ],
      "year": "2023",
      "venue": "IEEE Trans. on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Fast yet effective speech emotion recognition with self-distillation",
      "authors": [
        "Z Ren",
        "T Nguyen",
        "Y Chang",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pretraining of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "20",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "IEEE/CVF Conf. on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "ACM Int. Conf. on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X Wen",
        "Y Wei",
        "Y Xu",
        "K Liu",
        "H Shan"
      ],
      "year": "2023",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Light-sernet: A lightweight fully convolutional neural network for speech emotion recognition",
      "authors": [
        "A Aftab",
        "A Morsali",
        "S Ghaemmaghami",
        "B Champagne"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition with local-global aware deep representation learning",
      "authors": [
        "J Liu",
        "Z Liu",
        "L Wang",
        "L Guo",
        "J Dang"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Hierarchical network based on the fusion of static and dynamic features for speech emotion recognition",
      "authors": [
        "Q Cao",
        "M Hou",
        "B Chen",
        "Z Zhang",
        "G Lu"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Speech sentiment analysis via pre-trained features from end-to-end asr models",
      "authors": [
        "Z Lu",
        "L Cao",
        "Y Zhang",
        "C Chiu",
        "J Fan"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition using sequential capsule networks",
      "authors": [
        "X Wu",
        "Y Cao",
        "H Lu",
        "S Liu",
        "D Wang",
        "Z Wu",
        "X Liu",
        "H Meng"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing"
    }
  ]
}