{
  "paper_id": "2310.01430v1",
  "title": "Sarcasm In Sight And Sound: Benchmarking And Expansion To Improve Multimodal Sarcasm Detection",
  "published": "2023-09-29T07:00:41Z",
  "authors": [
    "Swapnil Bhosale",
    "Abhra Chaudhuri",
    "Alex Lee Robert Williams",
    "Divyank Tiwari",
    "Anjan Dutta",
    "Xiatian Zhu",
    "Pushpak Bhattacharyya",
    "Diptesh Kanojia"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The introduction of the MUStARD dataset, and its emotion recognition extension MUStARD++, have identified sarcasm to be a multi-modal phenomenon -expressed not only in natural language text, but also through manners of speech (like tonality and intonation) and visual cues (facial expression). With this work, we aim to perform a rigorous benchmarking of the MUStARD++ dataset by considering state-of-the-art language, speech, and visual encoders, for fully utilizing the totality of the multi-modal richness that it has to offer, achieving a 2% improvement in macro-F1 over the existing benchmark. Additionally, to cure the imbalance in the 'sarcasm type' category in MUStARD++, we propose an extension, which we call MUStARD++ Balanced, benchmarking the same with instances from the extension split across both train and test sets, achieving a further 2.4% macro-F1 boost. The new clips were taken from a novel source -the TV show, House MD, which adds to the diversity of the dataset, and were manually annotated by multiple annotators with substantial inter-annotator agreement in terms of Cohen's kappa and Krippendorf's alpha. Our code, extended data, and SOTA benchmark models are made public. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Sarcasm is a manner of speech that conveys a sense of contempt or mockery through the use of irony. For instance, the remark: \"The tropical summers are so pleasant!\", although it refers to tropical summers as being pleasant, it is, in fact, expressing contempt for the extreme temperatures experienced in the tropics during summers. However, not all sarcastic remarks are expressed solely through natural language. For instance, the statement: \"The cake is so delicious!\", may or may not be sarcastic. If it were sarcastic, there would be no way to say so unless one looked at the speaker's facial expression, and paid attention to the tonality of the remark. Such instances pose a challenge for the automatic detection of sarcasm using only text as input. Further, it is revealed that sarcasm detection is an inherently multi-modal problem involving natural language, speech, and vision  [1, 2] . Recently, many other natural language processing (NLP) tasks under the sentiment umbrella have been investigated through the lens of multimodality  [3] [4] [5] [6] . Sarcasm detection is usually 1 Contact: s.bhosale@surrey.ac.uk, d.kanojia@surrey.ac.uk arXiv:2310.01430v1 [cs.CL] 29 Sep 2023 performed as a supervised learning-based classification task, and hence, requires a significant number of hard-to-obtain labelled multimodal data. These challenges motivate our investigation into the study of multi-modal sarcasm detection, and further the efforts with the help of additional data and rigorous benchmarking.\n\nWith this motivation, MUStARD  [1] , a multi-modal sarcasm detection dataset, was introduced; and it provides information from all three associated modalities, i.e., text, speech, and video. Additionally, Ray et al.  [2]  recognized underlying emotions to be a crucial determinant of the sarcastic nature of remarks, and introduced MUStARD++, an extension of MUStARD with emotion annotations, and an expansion on the number of instances. The associated benchmarking performed in the above works leverage the multi-modal nature of their datasets by encoding the individual modalities through independent text, speech, and vision modules, and combining their respective representations via a fusion layer  [7] . We identify two limitations in the existing multi-modal sarcasm detection benchmarking literature. First, all benchmarks on the task have so far only relied on classical encoding approaches of the individual modalities. For instance, MUStARD++, which is one of the most recent benchmarks in the literature, used Mel Frequency Cepstral Coefficients (MFCC), Mel spectrogram (using the Librosa library  [8] ), prosodic features from OpenSMILE  [9]  for speech, and ResNet-152  [10]  for videos, which cannot fully leverage the complementarity in the multi-modal information available in the datasets. Second, we also observe that the 'sarcasm type' category in MUStARD++ exhibits an imbalance. Through this work, we aim to bridge these gaps by rigorous benchmarking based on SOTA vision, speech, and language encoders. We also aim to expand the MUStARD++ dataset by adding new instances that alleviate this imbalance to a certain degree.\n\nRecent literature on self-supervised learning has shown that large-scale self-supervised pre-training can often generalize much better than even supervised pre-training. The introduction of CLIP  [11]  enabled this for multi-modal vision-language tasks as well. Through this work, we propose to perform a benchmarking for the task of multi-modal sarcasm detection that is more in tune with the current state-of-the-art (SOTA) in multi-modal learning. We leverage ViFi-CLIP  [12] , a SOTA CLIP-based model for encoding videos in a common video-text representation space. We use Wav2vec 2.0, a transformer-based self-supervised speech representation learning model trained using a masked learning objective for audio. With our proposed benchmarking, we were able to achieve > 2% gain in macro-F1 over the existing benchmark.\n\nTo cure the 'sarcasm type' category imbalance in MUStARD++, we propose an extension, we call, MUStARD++ Balanced. The publicly available clips in our extension are taken from the TV show, House MD, which, apart from curing the imbalance, also adds diversity to the dataset by virtue of being a novel source that had not been considered in any form either in MUStARD or MUStARD++. Our dataset extension has been annotated by three independent annotators who show a substantial inter-annotator agreement in terms of Cohen's kappa and Krippendorf's alpha. We split the clips in our extension across both train and test sets, and perform further benchmarking using our proposed setup. Our experiments revealed a further 2.4% boost to macro-F1 from using our extension, and show that this balance, as well as, the expansion in terms of the number of data points, could be effectively leveraged by our benchmark models to achieve notable improvements in sarcasm detection performance. We perform additional experiments that quantify the distribution shift introduced by our extension, demonstrating the added diversity.\n\nTo summarize, we make the following contributions: (1) rigorous benchmarking for the task of multi-modal sarcasm detection on the MUStARD++ dataset using SOTA vision, speech, and language models; (2) an extension to the MUStARD++ dataset, that we call MUStARD++ Balanced, to cure the 'sarcasm type' imbalance in the former, and to add data diversity; and (3) extensive experiments that show the significant gains in macro-F1 obtained by using our proposed benchmarking strategy, as well as from using our novel, balanced extension.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "We discuss the literature related to sarcasm detection and multimodal learning in the subsections below.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sarcasm Detection",
      "text": "Transformers  [13]  architecture-based approaches have increased in prevalence within NLP and also within sarcasm detection literature. This is most notably due to their ability to pick up semantic and syntactic relationships within text. Various rule-based and machine learning-based approaches to sarcasm detection have been discussed in  [14] , and they also present a linguistic perspective to sarcasm detection.  [15]  used a pattern-based approach for the task, while emphasizing the role of four sets of features obtained based on different sarcasm types. This pattern-based study achieved 83.1% accuracy and 91.1% precision on the task of sarcasm detection. Since the advent of Transformers, usage of machine learning approaches has seen a steep decline. Some studies include  [16]  and  [17] , which used a Naive Bayes and Decision Tree model, respectively, in order to identify sarcasm where both achieve the best F1 scores over 70 on their chosen datasets.\n\nOn the dataset released with the SemEval 2018 Shared Task 3  [18] ,  [19]  offered an RCNN-RoBERTa methodology, where a RoBERTa transformer was used with BiLSTM to enhance task performance. Further, they also report that the RCVV-RoBERTa approach achieved an F1-score of 90.0 on the Riloff dataset  [20] . Several recent methods for sarcasm detection using text are discussed by  [21] . A BERT  [22]  model without concatenated layers, BERT encodings with a Logistic Regression model, and other language models like IAN  [23]  that are trained and assessed on a Twitter-based sarcastic dataset are among these more recent efforts. With an F1-score of 73.4 in those evaluations, the BERT language model, without any additional layers, performs the best. Some existing literature investigates methods for performing sarcasm detection in Arabic  [24] , where an extensive set of experiments are performed on different transformer architectures, that include mBERT, XLM-RoBERTa  [25]  and language-specific models like MARBERT  [26] . In this low-resource setting for Arabic, the most effective model in this study achieves an F1-score of 58.4, which shows the need to investigate sarcasm in a multilingual low-resource setting. A weighted average Ensemble of a CNN, LSTM, and Gated Recurrent Unit (GRU) based architectures is trained with GloVe  [27]  word embeddings to identify sarcasm, as demonstrated in  [28] . This Ensemble outperformed comparative studies by up to 8% on SARC  [29] , a Reddit comments dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Modal Learning",
      "text": "Existing literature on multimodal sentiment classification refers to the MOUD  [30]  and MOSI  [31]  datasets, while the IEMOCAP dataset  [32]  for the task of multimodal emotion recognition. Poria et. al.  [33]  propose the use of a bidirectional contextual long short-term memory (bc-LSTM) architecture for both tasks and show improvements over baseline on all three datasets. However, Majumder et. al.  [34]  later propose context modelling with a hierarchical fusion of multimodal features and achieve improved performance in a monologue setting. In the conversation setting, Hazarika et. al.  [35]  propose using a Conversational Memory Network (CMN) to leverage contextual information from the conversation history and achieve improved performance. Novel multimodal neural architectures  [36, 37]  and multimodal fusion approaches  [38, 39]  have propelled the deployment of computational models in this domain, while more efficient multimodal fusion approaches have also been discussed in  [40] [41] [42] .\n\nFor multimodal sarcasm detection, a recent survey discusses the datasets and approaches in detail  [43] . The MUStARD dataset  [44]  provides clips compiled from popular TV shows, including Friends, The Golden Girls, The Big Bang Theory, and Sarcasmaholics Anonymous, annotated with sarcasm labels. Ray et. al.  [45]  extend upon this dataset by adding emotion labels and additional clips while also benchmarking for the multimodal sarcasm detection task. They call this extended dataset MUStARD++ and utilise feature fusion  [7]  and a feed-forward network to predict the sarcasm label. The authors show an F1-score of 70.2% points using audio, text and video modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Details",
      "text": "The MUStARD  [1]  dataset contains a total of 6, 365 videos, collected from TV shows like Friends, The Golden Girls, The Big Bang Theory, and Sarcasmaholics, of which 345 are sarcastic, and 6, 020 are non-sarcastic. Although some works in the literature have performed benchmarking on the MUStARD dataset  [46, 1, 2] , they do not consider the latest developments in multi-modal visionlanguage fusion  [11, 12]  and speech processing  [47] . Chauhan et al.  [48]  annotated MUStARD with 9 emotion labels, which was refined and extended by Ray et al.  [2]  in MUStARD++ by adding valence and arousal information, which are important indicators for recognizing the emotion behind a remark, as well as adding further data points. Table  1  summarizes the statistics of the above datasets, and compares it with our extension. MUStARD++ extended MUStARD (345 sarcastic video samples) with 256 sarcastic video samples (216 clips from The Big Bang Theory and 40 from Silicon Valley), and they label 601 non-sarcastic instances out of 6, 020 in MUStARD to balance the annotation. Along with refining the emotion labels in  [48] , they annotated all their data points with information about valence, the positivity/negativity of emotion, and arousal, a measure of the associated emotional intensity  [49] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Sarcastic Non-Sarcastic Total",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset Extension",
      "text": "MUStARD++ categorised their data points by adding the 'sarcasm type' information. Specifically, they included the following sarcasm-type annotations to their dataset:\n\n• Propositional (PRO): Remarks that need additional context to be classified as being sarcastic. For example: \"That's very kind of you.\"\n\n• Illocutionary (ILL): The type of sarcasm where the irony is expressed through non-textual cues, like voice or gesture. For example, while tasting a poorly cooked food, one may remark, \"How delicious!\", while expressing their contempt through facial expressions.\n\n• Embedded (EMB): When the sarcastic incongruity is clearly embedded in the text. For example, \"What a pleasant orchestra of honking cars!\"\n\n• Like-Prefixed (LIKE): Prefixes the statement with a \"like\" to stress the irony. For example, \"Like it means the end of the world!\".\n\nA lack of balance in a sarcasm detection dataset among the above types can make a model develop a bias in favour or against certain types of sarcasm, which would make the downstream system unfair. Thus, ensuring that the training dataset contains fair proportions of all four types of sarcasm is the first step towards building a trustworthy sarcasm detection system. Table  2  shows our attempt to balance the various sarcasm types in MUStARD++. It can be seen that MUStARD++ has a clear under representation of samples in the EMB and the LIKE types, when compared with the PRO and ILL types. As can be seen from the % Change column, we curate our extension such that the above imbalance can be somewhat mitigated.\n\nData Validation & Analysis Per our annotators, sarcasm annotation and its type are challenging to identify and cognitively cumbersome; thus, our annotation task spanned longer than anticipated. Multimodal data annotation is further challenging as it requires annotators to watch lengthy videos when compared to textual data annotation. Hence, our data annotation instances are also limited in number. However, with three annotators, our annotation shows a substantial agreement (0.743) using Fleiss' Kappa (K), and (0.798) Krippendorf's alpha (α). All three annotators were graduate students who were paid an agreed-upon fixed compensation for the annotation task, and consent was obtained to use the collected data for research and public release. We extend MUStARD++ by a total of 164 instances, out of which 91 are sarcastic. For final instance labels, we choose the majority label among the three for sarcastic/non-sarcastic. There was no difference in the sarcasm-type annotation among the majority of annotators for the final label.\n\nLicense & Potential Negative Impact We license our data extension 2  , raw annotations, and code under the CC-BY-NC-4.0 3  license, the same as MUStARD++.\n\nMultimodal datasets are often collected using content produced for television, live shows, viz., standup comedy, podcasts, etc. However, such content can often contain material which is offensive in nature, for the purpose of creating humour. Similarly, sarcasm is also used to generate humour emanating offence at times. We acknowledge that our data and instances from MUStARD++ possibly contain such material. We release this data for ease of reproducibility and for propagating further research in the area. For our work, we obtain data from publicly available sources material on Youtube and release it under the aforementioned license.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "We first describe the individual backbones proposed for experimentation with each modality, i.e.ViFi-CLIP for video and text encoders, wav2vec 2.0 for audio, and BART for text, respectively. Particularly, in the case of the audio backbone, we present our results when extracting features pre-and post-finetuning the entire backbone on an auxiliary speech emotion recognition (SER) task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Video & Text Backbone: Vifi-Clip",
      "text": "Rasheed et al.  [12]  propose a new approach for video learning that uses fine-tuned CLIP models. CLIP is a large-scale pre-trained model which learns to map images and text to a shared embedding spaceallowing it to learn visual and semantic features from images. ViFi-CLIP shows that fine-tuning CLIP models on a small dataset of videos can improve the performance of video learning models on a variety of tasks; particularly because fine-tuning allows the CLIP models to learn temporal information which is not the case with the original CLIP model. The authors evaluated their approach on a variety of video learning benchmarks, and it outperformed the existing state-of-the-art.\n\nGiven a video sample V i ∈ R T ×H×W ×C with T frames, and corresponding text label Y , ViFi-CLIP's video encoder utilizes a CLIP image encoder to encode the T frames independently as a batch of images and produce frame level embeddings x i ∈ R T ×D . In order to incorporate temporal learning the frame-level embeddings are aggregated using an average-pooling operation to obtain a video-level representation v i ∈ R D . During training, the authors maximize the cosine similarity between video embeddings from the video encoder and text embeddings obtained from CLIP's text encoder, for text corresponding to each video. We utilize ViFi-CLIP's video encoder (V1), and the text encoder (T1) together in combination and also separately for experimentation and reporting results.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Audio Backbone: Wav2Vec2",
      "text": "Self-supervised models exploit large scale speech corpus without explicit labels. Wav2vec2 is one such self-supervised model based on transformers; that adopts a masked learning objective to predict missing frames from the remaining context. Intermediate features from Wav2vec2 have been widely explored in the literature to extract contextualized features for related downstream tasks or even fine-tune the entire model on auxiliary tasks. It consists of three sub-modules, feature encoder, transformer module, and quantization module. Feature encoder is a multi-layer CNN that processes the input signal into low-level features. Based on this representation, the transformer module is further applied to produce contextualized representation. The quantization module discretizes the low-level features into a trainable codebook. To train the model, part of the low-level features are masked from the transformer module, and the objective is to identify the quantized version of the masked features based on its context.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fine-Tuning Wav2Vec2 On The Ser Task",
      "text": "Speech emotion recognition and sarcasm recognition are both tasks related to understanding and interpreting the emotions and intentions behind the human speech. While they are not directly correlated, there can be some overlapping aspects in terms of feature extraction and contextual understanding. To infer the underlying emotion, SER involves analyzing various acoustic and linguistic cues, such as pitch, intensity, speech rate, prosody, and word choice. Similarly, sarcasm detection requires understanding the context, the speaker's intention, and subtle linguistic cues, such as intonation, emphasis, and contradictory statements, to recognize sarcastic remarks.\n\nWe hypothesize that fine-tuning on speech emotion recognition can help the model better understand contextual cues, speaker intentions, and subtle linguistic nuances, which are crucial for detecting sarcasm. Additionally, given that the SER is a more widely explored task with a comparatively larger availability of datasets, fine-tuning can help leverage knowledge and representations learned during pre-training providing improved generalization that can benefit sarcasm detection, especially when dealing with diverse and varied sarcastic speech patterns.\n\nOne of the challenges in fine-tuning pre-trained models is the discrepancy or mismatch between the domain in which the model was pre-trained and the target domain to which it is being applied. Task Adaptive Pre-Training [TAPT]  [50] , is an existing NLP fine-tuning strategy that was proposed to resolve the domain shift by continuing the pre-training process on the target dataset.  [47]  introduced a novel fine-tuning method termed Psuedolabel-TAPT, which modifies the TAPT objective to learn contextualized emotion representations. Experiments show that P-TAPT performs better than TAPT, especially under low-resource settings. We adopt this strategy to fine-tune the existing wav2vec2 model on the IEMOCAP dataset for the SER task. The embeddings to be extracted for sarcasm detection are kept the same as the ones extracted above. We later provide an empirical analysis of both the pre-and post-fine-tuning features, as a part of the training for the sarcasm recognition task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Text Backbone: Bart",
      "text": "Bidirectional and Autoregressive Transformers (BART)  [51]  is a denoising autoencoder designed to reconstruct the original document from a corrupted version. It employs a sequence-to-sequence model with a bidirectional encoder to process the corrupted text and a left-to-right autoregressive decoder. During pre-training, BART optimizes the negative log-likelihood of the uncorrupted document. Unlike other denoising autoencoders that are specifically designed for particular noise patterns, BART can handle various types of document corruption. In the most extreme scenario, where all source information is lost, BART functions as a language model. Additionally, as a sequence-to-sequence model, BART can be utilized for tasks such as text summarization, question answering, and translation. BART generates a feature vector x t ∈ R dt for each instance x. To transform the text, we utilize the BART Large model with d t set to 1024. By taking the average of the representations from the last four transformer layers, we obtain a distinctive embedding representation for both the utterance and the context.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Benchmarking & Data Distribution",
      "text": "We perform multi-class classification experiments by utilizing the features extracted as described above. Since we have three modalities, with-and without contextual information, we perform several ablation studies to understand the impact of the presence or absence of each of these aspects. To maintain a fair comparison with the prior literature, we adopt the same multimodal fusion mechanism used in  [2] , i.e.a collaborative gated attention architecture  [7] . Further, we perform experiments only for the speaker-independent setting, as we evaluate the performance of our models in a general setting, devoid of speaker names in the data.\n\nTraining Details We perform our experiments on a single NVIDIA RTX A5500 GPU where a single modality ablation takes ∼ 0.6 hours, resulting in a total computation time of 20 hours for the complete benchmarking reported in all tables. For each modality configuration, we report the average of 5-fold cross-validation, where each fold is created using stratified K-fold separation, ensuring minimal skewness across the validation set in terms of the type of sarcasm. Each experiment is run for a maximum of 50 epochs while deploying an early stopping mechanism with a patience of 5 consecutive epochs unless the validation F1 score starts showing a non-increasing trend. We use 42, 5141, 1516, 12667 and 2238 as random seeds for each training fold. We use a learning rate of 1e -3, and a batch size of 256 with the Adam optimizer with linear weight decay of 1e -2.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": "We perform experiments separately on the MUStARD++ and MUStARD++ with our extension separately, and report the average of 5 folds in Table  3  and Table  4  respectively. We contrast all the modality configurations in terms of mean Precision (P), mean Recall (R), and mean macro-F1, for both settings -with and without contextual information from the video clip. The labels in the first column depict the backbone architecture that was used to extract the features, more specifically, T -BART  [51] , T1 -Text encoder from ViFiCLIP, V -Average pooled intermediate ResNet  [2] , V1 -Video encoder from ViFiCLIP, A -Average pooled spectral features (MFCC)  [2] , A1 -wav2vec2, A2 -wav2vec2 fine-tuned on SER. Values in bold show the best-performing modality combination, whereas the underlined values highlight the cases where task performance was observed to be better without contextual information. For all other cases, models with contextual information outperform the models without contextual information.\n\nFor the MUStARD++ dataset, it is clearly evident that the proposed benchmarking is superior and more robust compared to the prior work  [2] , i.e.T , A and V backbones, where we surpass each unimodality with 1.59%, 2.19% and 1.44% across text, audio and video respectively. Moreover, upon fusing all modalities, we achieve a 3.89% and 3.39% improvement against the SOTA -T + A + V , when training utterances without context and with context, respectively. With the new encoders from pre-trained models, we report SOTA scores on MUStARD++.\n\nWhen comparing the individual audio backbones, we find that the embeddings derived from the wav2vec2 model fine-tuned on the SER task (A2), surpass both the spectral features (A) and the ones derived from the wav2vec2 model trained on an ASR task (A1), by a substantial margin. Likewise, the video encoder (V 1) and the text encoder (T 1) from the pre-trained ViFiCLIP model outperform both their corresponding unimodal prior arts (V and T , respectively). We hypothesize this is primarily because of the contrastive objective utilized in the training of ViFiCLIP and the large-scale training data that has been exposed to the model as a part of the CLIP pre-training.\n\nAmong all three modalities, the textual embeddings are more indicative of the sarcasm detection task than the audio and video embeddings. For audio, it is mainly owed to the mismatch between clean speech that was used to pre-train the wav2vec2 model (and even after fine-tuning on the SER task) and the noisy speech often superimposed with the background laughter, which is in a clear mismatch with the training data used for training the wav2vec2 model, or its fine-tuned model on the SER task.\n\nA similar trend is observed when evaluating our approach on the MUStARD++ dataset with our proposed extension, wherein the highest macro F1-scores of 0.732 and 0.736 are obtained by combining all three modalities, when training utterances without context and with context, respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Modal",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Exploring The Distribution Shift",
      "text": "To quantify the actual distribution shift introduced as a result of our extension, we compare the V 1 + T 1 + A2 configured model, trained with and without the extended annotations on a common test set. Formally, we segment out two sets, of 50 and 200 samples from the original MUStARD++ dataset, ensuring an equal distribution amongst all sarcasm types, and use it as our validation set and test set, respectively.\n\nWe showcase this in Table  5  where we first, train a model on the remainder of the 952 samples from the MUStARD++ set, and report class-wise accuracies on the hold-out test set. Next, we replace randomly chosen samples from the training set of 952 samples with samples from our proposed extension. We repeat this for 5 random runs and report class-wise accuracies on the held-out test set while utilizing majority voting from each of the 5 models for inference. Please note that the number of training samples is the same in both training setups.\n\nIt is clearly evident that merging the samples with our proposed extension helps improve the individual class-wise accuracies, and, eventually, an absolute improvement in the mean macro-F1 score of 0.95. Interestingly, apart from the improvement in the accuracies of three prominent types of sarcasm, namely, Propositional, Illocutionary and Embedded, we also find an improvement in the accuracy of non-sarcastic utterances. We hypothesize this is particularly because (a) intra-cluster variances among sarcastic types are now more defined, and (b) inter-cluster deviation between sarcastic and non-sarcastic is more well separated, owing to well-defined intra-clusters in the shared latent space of our model, and the addition of new non-sarcastic instances.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "Among the different types of sarcasm discussed above, it has been argued in the literature that Illocutionary sarcasm is the most challenging since it involves non-speech like body language and gestures. Additionally, we observe that the majority of the samples mis-classified by our model are the ones with short duration (< 2 seconds), and hence do not contain enough context, to be processed by any of our backbones. In the captions for Figure  1 , we show the utterances which are very limited in length and on the basis of which our models tried to predict sarcasm.\n\nFurther, we do not perform the distribution shift experiment for Like-prefixed sarcasm we did not have enough samples for a test set. However, our experiment does reveal that non-sarcastic instances also benefit from instance addition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "We perform comprehensive benchmarking on a multimodal dataset for the task of sarcasm detection. Our benchmarking reveals that the use of better encoders for both audio and video signals, improves the task performance by 3.39, and 3.89 percent points, with and without contextual information, respectively. We utilize pre-trained models by extracting features and training classification models in a supervised setting, with an existing feature fusion methodology. Additionally, we perform a  quantitative analysis of the results obtained from various modality combinations, achieve SOTA performance on the dataset, and observe that Illocutionary sarcasm is more challenging to predict.\n\nOur work also analyzes the model for sarcasm type imbalance and slightly alleviates it by adding new instances to the dataset. We call this proposed extension, MUStARD++ Balanced. We validate this dataset extenuation with a substantial agreement based on Fleiss' Kappa and Krippendorf's alpha.\n\nOur recorded annotation experience and observations will help future extensions for this dataset, with a focus on finding instances of Like-prefixed sarcasm. Further, we choose the best modality combinations from unimodal, bimodal, and trimodal settings, and perform similar experiments to observe further improvement in the task performance. Our experiments on investigating the distribution shift for sarcasm types reveal that each sarcasm type observes an improvement in task performance by significant margins. Further, we perform a qualitative analysis of erroneous results from our experiments with the best-performing approach. We discuss our analysis and try to interpret why they fail to predict sarcasm correctly.\n\nIn future, we plan to keep extending the dataset with new instances, especially for Illocutionary and Like-prefixed sarcasm types, and experiment with more feature fusion approaches. In the short term, we plan to identify instances which are < 2 seconds in length and prune them from the data as they do not have sufficient context. We would like to further investigate linguistic phenomena like sarcasm, humour, and emotion in a more coherent setting, i.e.multi-task learning-based setup where a joint model trains on three or more tasks under this sentiment umbrella.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Limitations",
      "text": "We acknowledge that our dataset extension is obtained from a publicly available source but we do release this data in the form of trimmed clips under a non-commercial license akin to existing datasets already existing in the same domain. Further, we would like to acknowledge that pre-trained models contain bias against certain members of society and can embed these biases in our models. We have already discussed the potential negative impacts of our models above and acknowledge that despite significantly improved task performance, they may not be able to identify such biases and wrongful predictions. We release the data, the code and the models only for the purpose of furthering research in this domain.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , we show the utterances which are",
      "page": 9
    },
    {
      "caption": "Figure 1: Mis-classified samples. Ground truth: Sarcastic; Type: Illocutionary.",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: summarizes the statistics of the above",
      "page": 4
    },
    {
      "caption": "Table 1: Comparison of statistics of existing sarcasm detection dataset and our proposed extension.",
      "page": 4
    },
    {
      "caption": "Table 2: shows our attempt to",
      "page": 4
    },
    {
      "caption": "Table 2: Statistics of our MUStARD++ Balanced extension, and its comparison with MUStARD++.",
      "page": 5
    },
    {
      "caption": "Table 3: and Table 4 respectively. We contrast",
      "page": 7
    },
    {
      "caption": "Table 3: On MUStARD++ Dataset. (Precision (P), mean Recall (R), and mean macro-F1 (primary",
      "page": 8
    },
    {
      "caption": "Table 4: On MUStARD++ Balanced Dataset",
      "page": 8
    },
    {
      "caption": "Table 5: Distribution Shift helps task performance for sarcasm types and non-sarcastic (NON)",
      "page": 9
    },
    {
      "caption": "Table 5: where we first, train a model on the remainder of the 952 samples from",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Towards multimodal sarcasm detection (an _Obviously_ perfect paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "2",
      "title": "A multimodal corpus for emotion recognition in sarcasm",
      "authors": [
        "A Ray",
        "S Mishra",
        "A Nunna",
        "P Bhattacharyya"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "3",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S.-F Chang",
        "M Pantic"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "4",
      "title": "The hateful memes challenge: Detecting hate speech in multimodal memes",
      "authors": [
        "D Kiela",
        "H Firooz",
        "A Mohan",
        "V Goswami",
        "A Singh",
        "P Ringshia",
        "D Testuggine"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Multimodal meme dataset (multioff) for identifying offensive content in image and text",
      "authors": [
        "S Suryawanshi",
        "B Chakravarthi",
        "M Arcan",
        "P Buitelaar"
      ],
      "year": "2020",
      "venue": "Proceedings of the second workshop on trolling, aggression and cyberbullying"
    },
    {
      "citation_id": "6",
      "title": "Audio-visual fusion network based on conformer for multimodal emotion recognition",
      "authors": [
        "P Guo",
        "Z Chen",
        "Y Li",
        "H Liu"
      ],
      "year": "2022",
      "venue": "Artificial Intelligence: Second CAAI International Conference"
    },
    {
      "citation_id": "7",
      "title": "Use what you have: Video retrieval using representations from collaborative experts",
      "authors": [
        "Y Liu",
        "S Albanie",
        "A Nagrani",
        "A Zisserman"
      ],
      "year": "2019",
      "venue": "Use what you have: Video retrieval using representations from collaborative experts",
      "arxiv": "arXiv:1907.13487"
    },
    {
      "citation_id": "8",
      "title": "librosa: Audio and music signal analysis in python",
      "authors": [
        "B Mcfee",
        "C Raffel",
        "D Liang",
        "D Ellis",
        "M Mcvicar",
        "E Battenberg",
        "O Nieto"
      ],
      "year": "2015",
      "venue": "Proceedings of the 14th python in science conference"
    },
    {
      "citation_id": "9",
      "title": "Opensmile",
      "venue": "Opensmile"
    },
    {
      "citation_id": "10",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "11",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "12",
      "title": "Fine-tuned clip models are efficient video learners",
      "authors": [
        "H Rasheed",
        "M Khattak",
        "M Maaz",
        "S Khan",
        "F Khan"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin ; I. Guyon",
        "U Luxburg",
        "S Bengio",
        "H Wallach",
        "R Fergus",
        "S Vishwanathan",
        "R Garnett"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "Automatic sarcasm detection: A survey",
      "authors": [
        "A Joshi",
        "P Bhattacharyya",
        "M Carman"
      ],
      "year": "2017",
      "venue": "ACM Comput. Surv",
      "doi": "10.1145/3124420"
    },
    {
      "citation_id": "15",
      "title": "A pattern-based approach for sarcasm detection on twitter",
      "authors": [
        "M Bouazizi",
        "T Ohtsuki"
      ],
      "year": "2016",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Mining subjective knowledge from customer reviews: A specific case of irony detection",
      "authors": [
        "A Reyes",
        "P Rosso"
      ],
      "year": "2011",
      "venue": "Proceedings of the 2nd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA 2.011)"
    },
    {
      "citation_id": "17",
      "title": "Modelling sarcasm in Twitter, a novel approach",
      "authors": [
        "F Barbieri",
        "H Saggion",
        "F Ronzano"
      ],
      "year": "2014",
      "venue": "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis"
    },
    {
      "citation_id": "18",
      "title": "SemEval-2018 task 3: Irony detection in English tweets",
      "authors": [
        "C Van Hee",
        "E Lefever",
        "V Hoste"
      ],
      "year": "2018",
      "venue": "Proceedings of the 12th International Workshop on Semantic Evaluation"
    },
    {
      "citation_id": "19",
      "title": "A transformer-based approach to irony and sarcasm detection",
      "authors": [
        "R Potamias",
        "G Siolas",
        "A Stafylopatis"
      ],
      "year": "2020",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "20",
      "title": "Sarcasm as contrast between a positive sentiment and negative situation",
      "authors": [
        "E Riloff",
        "A Qadir",
        "P Surve",
        "L Silva",
        "N Gilbert",
        "R Huang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Applying transformers and aspect-based sentiment analysis approaches on sarcasm detection",
      "authors": [
        "T Shangipour Ataei",
        "S Javdan",
        "B Minaei-Bidgoli"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Figurative Language Processing"
    },
    {
      "citation_id": "22",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "23",
      "title": "Interactive attention networks for aspectlevel sentiment classification",
      "authors": [
        "D Ma",
        "S Li",
        "X Zhang",
        "H Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17",
      "doi": "10.24963/ijcai.2017/568"
    },
    {
      "citation_id": "24",
      "title": "Benchmarking transformer-based language models for Arabic sentiment and sarcasm detection",
      "authors": [
        "I Farha",
        "W Magdy"
      ],
      "year": "2021",
      "venue": "Proceedings of the Sixth Arabic Natural Language Processing Workshop"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "A Conneau",
        "K Khandelwal",
        "N Goyal",
        "V Chaudhary",
        "G Wenzek",
        "F Guzmán",
        "E Grave",
        "M Ott",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Arbert & marbert: Deep bidirectional transformers for arabic",
      "authors": [
        "M Abdul-Mageed",
        "A Elmadany",
        "E Nagoudi"
      ],
      "year": "2021",
      "venue": "Arbert & marbert: Deep bidirectional transformers for arabic"
    },
    {
      "citation_id": "27",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "28",
      "title": "Sarcasm detection using deep learning and ensemble learning",
      "authors": [
        "P Goel",
        "R Jain",
        "A Nayyar",
        "S Singhal",
        "M Srivastava"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "29",
      "title": "A large self-annotated corpus for sarcasm",
      "authors": [
        "M Khodak",
        "N Saunshi",
        "K Vodrahalli"
      ],
      "year": "2018",
      "venue": "A large self-annotated corpus for sarcasm"
    },
    {
      "citation_id": "30",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "V Pérez-Rosas",
        "R Mihalcea",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "31",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "32",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "33",
      "title": "Contextdependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "34",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria"
      ],
      "year": "2018",
      "venue": "Knowledge-based systems"
    },
    {
      "citation_id": "35",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "36",
      "title": "Words can shift: Dynamically adjusting word representations using nonverbal behaviors",
      "authors": [
        "Y Wang",
        "Y Shen",
        "Z Liu",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "37",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "H Pham",
        "P Liang",
        "T Manzini",
        "L.-P Morency",
        "B Póczos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "P Liang",
        "Z Liu",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Learning factorized multimodal representations",
      "authors": [
        "Y.-H Tsai",
        "P Liang",
        "A Zadeh",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2018",
      "venue": "Learning factorized multimodal representations",
      "arxiv": "arXiv:1806.06176"
    },
    {
      "citation_id": "40",
      "title": "Low rank fusion based transformers for multimodal sequences",
      "authors": [
        "S Sahay",
        "E Okur",
        "S Kumar",
        "L Nachman"
      ],
      "year": "2020",
      "venue": "Second Grand-Challenge and Workshop on Multimodal Language (Challenge-HML)"
    },
    {
      "citation_id": "41",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "42",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Bagher",
        "L.-P Zadeh",
        "Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "43",
      "title": "Multimodal sarcasm detection: A survey",
      "authors": [
        "A Bhat",
        "A Chauhan"
      ],
      "year": "2022",
      "venue": "2022 IEEE Delhi Section Conference (DELCON)"
    },
    {
      "citation_id": "44",
      "title": "Towards multimodal sarcasm detection (an _obviously_ perfect paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "45",
      "title": "A multimodal corpus for emotion recognition in sarcasm",
      "authors": [
        "A Ray",
        "S Mishra",
        "A Nunna",
        "P Bhattacharyya"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "46",
      "title": "Multibench: Multiscale benchmarks for multimodal representation learning",
      "authors": [
        "P Liang",
        "Y Lyu",
        "X Fan",
        "Z Wu",
        "Y Cheng",
        "J Wu",
        "L Chen",
        "P Wu",
        "M Lee",
        "Y Zhu"
      ],
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "47",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Sentiment and emotion help sarcasm? a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis",
      "authors": [
        "D Chauhan",
        "D S R",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "49",
      "title": "",
      "authors": [
        "R Picard"
      ],
      "year": "1997",
      "venue": ""
    },
    {
      "citation_id": "50",
      "title": "Don't stop pretraining: Adapt language models to domains and tasks",
      "authors": [
        "S Gururangan",
        "A Marasović",
        "S Swayamdipta",
        "K Lo",
        "I Beltagy",
        "D Downey",
        "N Smith"
      ],
      "year": "2020",
      "venue": "Don't stop pretraining: Adapt language models to domains and tasks",
      "arxiv": "arXiv:2004.10964"
    },
    {
      "citation_id": "51",
      "title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2019",
      "venue": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",
      "arxiv": "arXiv:1910.13461"
    }
  ]
}