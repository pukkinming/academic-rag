{
  "paper_id": "2309.07085v2",
  "title": "Mitigating Group Bias In Federated Learning For Heterogeneous Devices",
  "published": "2023-09-13T16:53:48Z",
  "authors": [
    "Khotso Selialia",
    "Yasra Chandio",
    "Fatima M. Anwar"
  ],
  "keywords": [
    "Federated Learning",
    "Algorithmic Fairness",
    "Group Fairness"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Federated learning is emerging as a privacy-preserving model training approach in distributed edge applications. As such, most edge deployments are heterogeneous in nature, i.e., their sensing capabilities and environments vary across deployments. This edge heterogeneity violates the independence and identical distribution (IID) property of local data across clients. It produces biased global models, i.e., models that contribute to unfair decision-making and discrimination against a particular community or a group. Existing bias mitigation techniques only focus on bias generated from label heterogeneity in non-IID data without accounting for domain variations due to feature heterogeneity. Our work proposes a group-fair FL framework that minimizes group-bias while preserving privacy. Our main idea is to leverage average conditional probabilities to compute a cross-domain group importance weights derived from heterogeneous training data to optimize the performance of the worst-performing group using a modified multiplicative weights update method. Additionally, we propose regularization techniques to minimize the difference between the worst and best-performing groups while ensuring through our thresholding mechanism to strike a balance between bias reduction and group performance degradation. Our evaluation of image classification benchmarks assesses the fair decision-making of our framework in real-world settings. CCS Concepts: â€¢ Computing methodologies â†’ Machine learning; Distributed computing methodologies.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Federated learning (FL) is a privacy-preserving machine learning (ML) technique wherein local models are trained on decentralized edge devices (clients) and subsequently aggregated at the server to form a global model. This approach alleviates the need for raw data transfers and ensures data privacy, making it particularly well-suited for applications with privacy sensitivities, such as medical diagnosis  [21, 38, 59] , next-character prediction  [67] , activity recognition  [17, 56, 66] , and human emotion recognition  [14, 46, 72] , where preserving data security is imperative. Despite its merits, there is a growing concern regarding FL models, as they exhibit exceptional performance for certain groups while simultaneously underperforming for others (e.g., providing accurate image captioning for pristine group images than noisy group images as shown in Figure  1 ). A group categorizes data based on attributes such as race, gender, class, or label  [7] .\n\nGroup biases and discriminatory practices threaten societal well-being, undermining public confidence in ML models and their applications  [7] . Research shows racial bias in electronic health records, especially in medical analysis, potentially causing treatment disparities for minority groups  [68] . Biased models often result from label heterogeneity in non-IID data across clients, as discussed in works like  [52, 57] , arising from diverse label distributions tied to data collection device environments. For example, certain geo-regions may have varying label distributions, reducing training data volume for specific groups  [8, 30] .\n\nOur work highlights feature noise heterogeneity as a significant source of group bias in FL models, stemming from varied noise-influenced features due to domain differences, especially in heterogeneous devices  [48] . Heterogeneity leads to distinct feature distributions in local client data. For example, low-quality sensors on some devices introduce distortion like Gaussian noise, resulting in different feature distributions compared to high-quality sensor devices  [47] . This inherent feature noise causes shifts in group data moments, which are statistical properties such as mean and variance within a group in a dataset  [35] , influencing biased model outcomes. female dressed as a princess in a white dress male wearing a half sleeve t-shirt and a chef apron female dressed as a bride in a white dress male wearing a half sleeve t-shirt and a vest\n\nFeature heterogeneity is due to environmental or device-specific factors such as low resources.\n\nFigure  1 . Illustrating the adverse effects of feature heterogeneity (noise) and its bias impact on image classification data  [42]  on an example language model (LM) in FL settings. The global LM, engaging in image captioning based on features from multiple clients, shows higher performance for images without distortions compared to those with a shift in feature distributions. This emphasizes the intricate interplay of feature heterogeneity and bias in FL, highlighting the influence of heterogeneous client datasets on the model's outcome.\n\nPrevious FL research introduces Disparate Learning Processes (DLPs) to tackle bias and fairness issues. Examples of DLPs include in-processing methods like  [9, 11, 12, 15, 16, 18, 23, 31, 43, 44, 52, 57, 61, 73, 74, 76]  and Robustness and generalization strategies such as  [34, 41] . In-processing techniques modify learning to include group fairness constraints, while robustness and generalization enhance model resilience in diverse data settings. However, DLPs don't ensure fairness in settings with feature heterogeneity, especially due to feature noise, as they don't address misaligned moments in feature distributions  [35] . For DLPs that use \"reweighting\" with importance weights to adjust the model's objective function, their effectiveness relies on suitable importance weight selection  [6] . Importance weights prioritize specific groups or features during training to mitigate biases and enhance fairness  [6] . If not chosen carefully or not aligned with genuine sources of bias, these weights can lead to continued unfairness  [6] . We propose using weights derived from noisy feature data for more efficient debiasing in FL models affected by feature noise. This work introduces learnable importance weights from heterogeneous data features to enhance fairness in training, utilizing the multiplicative weight update (MW) method  [3]  for better fairness based on feature characteristics, especially considering data characteristics with feature noise. Our approach is inspired by insights from social science, particularly addressing discrimination as a health disparity determinant  [36] . By incorporating learnable importance weights, we aim to mitigate biases across demographic groups, contributing to a more equitable FL framework.\n\nThe efficacy of importance weighting diminishes due to exploding weight norms from the empirical risk scaling with importance weights, especially in large models, risking overfitting  [6] . To tackle this, we propose using neural network regularization techniques  [55]  in Multiplicative Weight update with Regularization (MWR) to mitigate group bias. Additionally, methods using importance weighting may introduce unfairness by overly emphasizing poorly-performing groups, potentially reducing the performance of better-performing groups to minimize overall variability  [13] . To address this issue, we present a heuristic approach for deriving importance weights that mitigate group bias while maintaining a performance threshold for better-performing groups, preventing their performance from dropping below a desirable level. We summarize our contributions below: We ensure that MWR optimizes the performance of the worstperforming group while also keeping the performance of the best-performing group above a desirable threshold. â€¢ Implementation and Evaluation: We implement and evaluate the MWR method against existing bias-mitigation techniques on commonly used state-of-the-art image classification FL benchmark datasets (CIFAR10  [37] , MNIST  [40] ,\n\nFashionMNIST  [71] , USPS  [32] , SynthDigits  [24] , and MNIST-M  [24] ). Our findings show that MWR outperforms baseline methods, boosting the accuracy of the worst group's performance up to 41% without substantially degrading the best group's performance.\n\n2 Background and Related Work 2.1 Bias in Machine Learning. Bias in ML refers to a model favoring specific individuals or groups, leading to unfair outcomes  [51] . Common sources of bias in centralized learning include prejudice, underestimation, and negative legacy  [1, 8, 49] . Techniques such as preprocessing, in-processing, and post-processing  [22, 26, 33]  have effectively mitigated centralized learning bias. However, applying centralized learning techniques in FL is challenging due to privacy concerns, requiring access to features across clients and risking data privacy compromise.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Bias Metrics",
      "text": "In FL, group bias is assessed through three dimensions: 1) aiming for equal opportunities by evaluating the performance discrepancy in True Positive Rates (TPR) between groups  [58, 69] ; 2) optimizing the Worst-case TPR (WTPR) for each group  [50, 58] ; 3) minimizing the standard deviation of TPR (TPSD) to ensure fairness across groups  [58, 73] . The choice of TPR as a performance metric of in assessing group group fairness aligns our approach with recent advancements in bias mitigation literature  [58] . This decision stems from recognizing the critical importance of fairly detecting true positives, which cannot be addressed solely by relying on accuracy.\n\nWhile our primary focus is on achieving fairness with a minimax property (optimizing WTPR outcome within each group), we evaluate using various fairness metrics to ensure versatility and broad support.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Bias Mitigation",
      "text": "The bias mitigation work falls mainly into four categories, including: 1) Client-fairness techniques  [12, 31, 43, 44, 52, 61] ,\n\n2) Group-fairness techniques  [9, 11, 15, 16, 18, 23, 57, 73, 74, 76] , 3) Collaborative Fairness techniques  [19, 48, 54, 75] , and 4) Robustness and Generalization techniques  [34, 41, 60] .\n\nClient fairness targets the development of algorithms leading to models that exhibit similar performance across different clients  [44] . On the other hand, group fairness requires the model to perform similarly on different demographic groups  [73] . Many state-of-the-art fairness techniques in FL, focusing on client fairness and group fairness, use inprocessing methods to modify the learning process or objective function by incorporating fairness constraints  [73] .\n\nIn-processing involves assigning weights to the objective function from different clients or groups during training to balance the influence of the model on different groups or clients. For instance, AFL  [52]  optimizes the combination of worst-weighted losses from local clients, proving resilient to data with an unknown distribution. q-FFL  [44]  reweights loss functions to give higher weights to devices with poorer performance, addressing challenges in fair resource allocation in computer networks. TERM handles outliers and class imbalance by tilting the loss function with a designated tilting factor  [43] . GIFAIR-FL  [73]     [48] . It is important to note that while we discuss Collaborative Fairness, here does not specifically address mitigating group bias in FL, as these techniques do not inherently focus on improving group performances.\n\nRobustness and Generalization techniques address distributional shifts in user data. For instance, FedRobust  [60]  trains a model to handle worst-case affine shifts, assuming that each client can express its data distribution as an affine transformation of a global distribution, focusing on group fairness. However, FedRobust requires sufficient data for each client to estimate the local worst-case shift, impacting global model performance when this condition is unmet. Fed-NTD tackles catastrophic forgetting distillation  [29]  but may not fully handle bias from feature noise. SCAFFOLD  [34]  addresses client drift in heterogeneous data by estimating update directions. However, SCAFFOLD may not correct moments in noisy feature distributions. In contrast, we use importance weights from noisy features to prioritize disadvantaged groups during training, enhancing fairness by indirectly correcting misaligned moments.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Preliminary Study",
      "text": "This section analyzes group-bias arising from heterogeneous feature distributions within local data across clients. The study utilizes Federated Averaging (FedAvg  [45] ), a widely adopted aggregation method for training global models in FL.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "Applications and Datasets. Our study analyzes group-bias across ğ¾ âˆˆ {4, 5} clients (computers that simulate the FL  environment, mirroring real-world heterogeneous data collection devices following recent works in FL  [30, 52, 73] ) using two deep learning models and two datasets. We employ the ResNet model  [28]  for CIFAR10  [37]  image classification and a Convolutional Neural Network (CNN) on the DIGITS classification dataset, which comprises data from diverse sources with feature shifts. The goal is to replicate real-world FL scenarios with varied client data. We construct the DIGITS dataset by combining data from SynthDigits  [24] , MNIST-M  [24] , and MNIST  [4] .\n\nWe select these datasets to compare group-bias with existing bias mitigation techniques in FL. Each dataset is evenly distributed among ğ¾ clients in the FL framework, ensuring equal allocation of group data points. Clients utilize replicated versions of the original benchmark test set, aligning noise feature distributions between training and test data.\n\nWe set all model parameters to match FL parameters for global model convergence under IID data settings, including label and feature noise homogeneity. Client settings include a mini-batch size of 128, a learning rate of 0.01, and 40 (for CIFAR10) and 12 (for DIGITS) training rounds. Heterogeneous Feature Distributions. We add noise to mimic real-world distorted images that fail to share the same feature distribution with the pristine training images  [25, 62, 64] . In particular, we add Gaussian noise with a variance greater than or equal to 0.03, consistent with the real-world deployments  [48] . We create two different distortion levels in each dataset across ğ¾ clients. For the CIFAR10, three advantaged clients (A, B, C) lack distortions, while the other two disadvantaged clients (D, E) host data with Gaussian noise of variance ğ‘£ğ‘ğ‘Ÿ âˆˆ {0.03, 0.07, 0.11, 0.3, 0.4, 0.8, 1.0}. For the DIGITS dataset, two advantaged clients (C, D) lack distortions, while the other two disadvantaged clients (A, B) host data with Gaussian noise.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Key Findings",
      "text": "Non-IID Study. We study the FL model's unfairness by examining how the biased global model treats local groups differently for each client. We measure the TPR performance gap between the best and worst groups using each client's local test data (with a similar distortion level as the training data). Figure  2a  shows group-bias in CIFAR10, while Figure  2b  illustrates this in DIGITS. The global model's recognition of local groups varies per client, as seen in the discrepancy between their performances. Increasing Gaussian noise on a client amplifies this difference, indicating that heterogeneous local features across clients contribute to group bias. Limitation of Federated Averaging. We empirically investigate how heterogeneous local data distributions affect local model gradients. Post-convergence, we extract gradients from the last linear layer of each local model across two clients. Figure  3  shows histograms of these gradients, highlighting variations across clients with heterogeneous features (3b) compared to more consistent distributions in clients with homogeneous features (3a). In 3a, a Spearman correlation  [53]   Our non-IID study underscores the challenges in conventional FedAvg schemes, revealing consistently unfair model behavior across distinct applications and datasets. This problem emphasizes the need for bias mitigation methods to alleviate adverse outcomes, including performance degradation in critical applications like medical contexts and the inability to adapt to dynamic heterogeneous environments.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "The primary objective of our work is to address group bias resulting from feature heterogeneity across clients, all while preventing the leakage of sensitive data. In this section, we formally define our problem and then present our approach to mitigate group bias without substantially degrading the best group performance.\n\nSend â„ ğ‘ ğ‘˜ (ğœ½ ğ‘ ğ‘˜ ) to the server.\n\ndrawn from distribution ğ‘ƒ (X, Y, G, C). Here, x ğ‘– âˆˆ X represents training images from a total of |X| images, ğ‘¦ ğ‘– âˆˆ Y corresponds to |Y| targets, ğ‘” ğ‘— âˆˆ G denotes group membership (from |G| groups) of x ğ‘– , and ğ‘ ğ‘˜ is the client on which (x ğ‘– , ğ‘¦ ğ‘– ) resides out of |C| clients. Our primary goal is to derive a global model â„ ğœ½ (with parameters ğœ½ ) that mitigates group bias for each client, with following objective:\n\nIn   [27]  is used to train a local model, â„ ğœƒ ğ‘ ğ‘˜ , minimizing the empirical risk of the worst-performing group. On the server side, â„ ğœ½ ğ‘ ğ‘˜ from all clients is received and aggregated into a global model â„ ğœ½ . Workflow. We illustrate the end-to-end workflow for training with the proposed approach in Figure  4 .\n\nâ¶ In our setup, the server selects all the available clients in each round to avoid the effect of client sampling bias  [10, 70, 77] . Then, the server distributes copies of the global model to the clients.\n\nâ·-â¹ Each client computes the mixture of group likelihoods, denoted as ğ‘ (ğ‘” ğ‘— |x ğ‘– ) (specifically, ğ‘ (ğ‘” ğ‘— |x ğ‘–,ğ‘˜ )). In Â§ 4.2, we outline the privacy-preserving computation details of this denominator, occurring once at the beginning of FL. After each round, clients communicate the local model and local ğ‘ (ğ‘” ğ‘— |x ğ‘–,ğ‘˜ ) for all groups (only in the first round) to the server.\n\nâº After clients submit their local models and local ğ‘ (ğ‘” ğ‘— |x ğ‘–,ğ‘˜ ), the server uses FedAvg to aggregate the local models and generate an updated global model. Additionally, the server computes a mixture of group likelihoods for all groups using local likelihoods (emphasizing that this computation occurs once at the beginning of FL).\n\nâ» Each client performs local training after distributing updated global model copies and a mixture of likelihoods for all groups. The training involves using our approach MWR to adjust group importance weights based on the mixture of likelihoods for all groups ( Â§ 4.3).\n\nâ¼ Each client computes the performance threshold for the best group and compares it with the best group performance to evaluate MWR's effectiveness in mitigating group bias without compromising the best group performance ( Â§ 4.5).",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Enabling Privacy-Preserving Group Fairness",
      "text": "Our approach centers on weighting empirical risks with group importance weights, ğ‘¤ ğ‘” ğ‘— , as shown in Equation  1 . Calculating these weights is straightforward in centralized learning  [20] , where a global data view is available. However, In FL, lacking this global view is not trivial. We must estimate ğ‘¤ ğ‘” ğ‘— while safeguarding client data privacy. Our solution addresses this by approximating the denominator of ğ‘¤ ğ‘” ğ‘— (ğ‘ (G = ğ‘” ğ‘— |X)) through a process involving a mixture of group likelihoods across clients. Suppose G = 1, ..., ğ‘— represents groups across clients in FL. Each client ğ‘ ğ‘˜ employs a multiclass logistic linear regression probabilistic model  [2]  to predict the likelihood of an input sample x ğ‘–,ğ‘˜ belonging to a specific group ğ‘” ğ‘— . The model is defined as ] , where ğ‘“ ğœ½,ğ‘— (x ğ‘–,ğ‘˜ ) [ğ‘” ğ‘— =ğ‘— ] is a multinomial probability mass function  [39] . Each client uses the softmax function ğ‘“ ğœ½,ğ‘— (x ğ‘–,ğ‘˜ )\n\nto obtain group membership probabilities ensuring that these probabilities are positive and sum up to one. Clients share their group likelihood estimates with the server. The server then computes each group's global average likelihood using per-client group average likelihood estimates and the law of total probability. For an event space {ğ‘ 1 , ğ‘ 2 , ..., ğ‘ |C| } with\n\n(\n\nHere To solve the group bias problem, we modify the MW algorithm and transform it into a constrained optimization problem to improve the performance of the the worst-performing group. Algorithm 1 details the workings of the MW algorithm. We assign each client with groups and a set of |G| classes for the underlying application during the local learning process. The optimization constraints comprise decisions made by both the local and global models for groups assigned to clients, ensuring fairness in group classification. Using image features in the training dataset, we validate constraint satisfaction in each local training iteration and identify suitable groups. We then associate decisions made by each local model with a group empirical risk that quantifies how well a decision made by the local model satisfies the constraints. Over time, we minimize the overall risk of the global model by ensuring that each local model incurs a low per-group risk. This involves tracking the global weight for each group and randomly selecting groups with a probability proportional to their importance weights ğ‘¤ ğ‘” ğ‘— . In each iteration, we update ğ‘¤ ğ‘” ğ‘— using the MW algorithm, multiplying their numerator ğ‘(G = ğ‘” ğ‘— |G) with factors dependent on the risk of the associated group decision. This update is performed while maintaining the denominator ğ‘ (G = ğ‘” ğ‘— |G) fixed as in\n\nğ‘ (G=ğ‘” ğ‘— |X) , which penalizes costly group decisions.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ensuring Optimality Through Regularization",
      "text": "The MW algorithm maximizes worst-group performance by scaling the empirical risk and deep neural network weights. However, the weight magnitude does not ensure optimal risk function convergence  [6] . In our setup, model parameters ğœƒ are trained with cross-entropy loss and stochastic gradient descent (SGD)  [5]  optimization, converging toward the solution of the hard-margin support vector machine 1  in the direction ğœ½ ğ‘¡ | |ğœ½ ğ‘¡ | |  [65] . Introducing weight to the loss function may introduce inconsistencies in the margin. Instead of directly applying importance weighting to the empirical risk, we aim to minimize the following objective for each client ğ‘˜:\n\nSince the optimization problem with importance weighting is vulnerable to scaling weights and biases, we introduce regularization to the norm of ğœƒ ğ‘ ğ‘˜ to increase the margin and mitigate the risk of its enlargement due to scaling, forming the basis of our Multiplicative Weight update with Regularization (MWR) algorithm.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Bias Mitigation Without Degrading",
      "text": "High-Performing Groups While MWR ensures group fairness, importance weighting approaches may exbibit unfairness by disproportionately focusing on the worst-performing groups, potentially degrading the performance of the best-performing groups in an attempt to reduce the variance in estimating their contributions to the overall performance  [13] . Practically, an algorithm for bias mitigation should achieve fairness without significantly degrading the performance of best-performing groups. To address this, we propose a heuristic approach to reweighing the likelihood (group importance weights) associated with each data point belonging to group G = ğ‘” ğ‘— in the dataset. Suppose we have a set of unnormalized importance weights ğ‘¤ 1 , ğ‘¤ 2 , ..., ğ‘¤ ğ‘› corresponding to ğ‘› data points in a dataset, where each data point has an associated importance weight, we normalize these weights for each group by computing Åµ1 , Åµ2 , ..., Åµ |G| using:\n\nThe rationale behind Equation3 is to distribute emphasis evenly among different groups, preventing a scenario where a single group dominates the estimation due to an excessively high importance weight. Through weight normalization, we ensure that each group's contribution aligns more closely with its true importance or representation within the dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Satisfying Performance Thresholds",
      "text": "Finally, we establish a performance threshold for the best true positive rate (BTPR) to mitigate group bias without significantly compromising the BTPR . We denote BTPR for a client ğ‘ ğ‘– as ğ‘‡ ğ‘ƒğ‘… ğ‘ğ‘’ğ‘ ğ‘¡,ğ‘ ğ‘– and WTPR as ğ‘‡ ğ‘ƒğ‘… ğ‘¤ğ‘œğ‘Ÿğ‘ ğ‘¡,ğ‘ ğ‘– . We define the threshold for the best TPR as ğ‘‡ ğ‘ƒğ‘… ğ‘¡â„ğ‘Ÿğ‘’ğ‘ â„ğ‘œğ‘™ğ‘‘ . Our fairness enforcement objective aims to minimize the gap between the best and worst-performing groups while maintaining a specified level of TPR performance, as follows:\n\nHere ğœ‚ ğœ‡ is a parameter governing the trade-off between group fairness and performance. Inequality in 4 scales the difference between BTPR and WTPR by ğœ‚ ğœ‡ and compares it to the difference between the BTPR and the threshold. For each client, we rearrange the inequality in 4 to obtain the minimum BTPR threshold as expressed in equation 5.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Evaluation",
      "text": "This section evaluates our MWR group-bias mitigation technique on four image classification datasets (CIFAR10, DIGITS, MNIST, and FashionMNIST). We benchmark our approach against standard bias mitigation techniques in FL.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment Testbed",
      "text": "Our evaluation setup uses the same number of clients, data partitioning scheme, and other learning components (such as learning rate, train/test split, batch size, epochs, rounds) described in Â§3.1 unless stated otherwise.\n\nBaseline. We evaluate our approach across four key categories, scrutinizing both bias reduction and overall model performance. The FL baseline category (FedAvg) represents a conventional learning scheme in FL. In the FL bias-reduction category, we include methods such as AFL  [52] , TERM  [43] , and GIFAIR-FL  [73] . These methods employ empirical risk reweighting to mitigate bias and adapt the global model to diverse local data distributions. The FL heterogeneity category (FedNTD  [41] ) specifically addresses performance loss in FL models arising from data heterogeneity by managing global model memory loss. In the FL robustness category (SCAFFOLD  [34] ), the focus is on enhancing the resilience of FL models against outliers and noisy data, thereby mitigating the impact of irregularities in specific device local datasets. To ensure a fair evaluation across all baselines, we meticulously calibrate hyperparameters across datasets, guaranteeing the convergence of the global model.\n\nHyperparameter Tuning for MWR. We use the same experimental setup as FedAvg, AFL, FedNTD, TERM, GIFAIR-FL, and SCAFFOLD. However, to apply MWR update algorithm per-group loss, we set the value of ğœ‚ ğœ‡ (see Algorithm 1) to different values in the set {0.01, 0.02, 0.001, 0.009, 0.0001} based on the level of Gaussian noise in data partitions. Finally, MWR uses an ğ¿1 regularization parameter of 0.00001 for all datasets.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Efficacy And Robustness Analysis",
      "text": "We now assess the efficacy and robustness of our MWR group-bias mitigation technique with the baselines.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effect On Group",
      "text": "Bias. We assess the efficacy of MWR's group-bias mitigation through: (i) evaluating the best-and worst-group performance (TPR), (ii) analyzing the TPR group variance per client, and (iii) examining the TPR discrepancy per client. This evaluation is conducted on four datasets, incorporating low-grade distortion to simulate prevalent real-world heterogeneity  [30] . Table1 presents the TPR, TPRSD, WTPR, and BTPR perfromance scores across various bias mitigation techniques and datasets. Notably, among these techniques, MWR stands out by achieving a significantly fairer outcomes for groups. We can see that our algorithm substantially decreases TPRSD across most clients while maintaining a consistently high TPR. Importance weighting, especially when derived from features characteristics, is powerful in mitigating biases caused by feature noise. If the bias is primarily driven by certain features, assigning appropriate weights to these features can help the model focus on relevant information and reduce the impact of noisy features, resulting in more consistent and equitable predictions.\n\nAlthough AFL and FedNTD occasionally outperform MWR in some instances concerning the TPRSD metric as can be seen in DIGITS dataset's client4 and MNIST dataset's clients4 and 5, the differences between the results are marginal. Importance weighting is sensitive to distribution shifts in the feature space. If there are instances where the distribution shifts significantly, the importance weights may not be as effective. On the other hand, techniques such as FedNTD, through knowledge distillation, seem to be more robust to feature noise as it involves transferring knowledge from a more complex model (teacher) to a simpler one (student), potentially leading to better generalization and lower standard deviation in true positive rates across groups. Additionally, it becomes evident from Table  1  that MWR results in an increased WTPR for the group with the smallest TPR, accompanied by the smallest TPRD among the evaluated bias mitigation techniques.\n\nImportance weights derived from image features captures the distinctive characteristics of different groups more effectively than other methods. This adaptability is crucial in mitigating bias since it tailors the mitigation strategy to the specific features and challenges present in each group. Despite TERM appearing to outperform our proposed method for the minimax group fairness metric (WTPR) in CIFAR10 dataset's clients 1, 2, and 3, this can be understood as a consequence of the reduction in TPR among privileged clients lacking local data with distortions. This reduction elevates the lower TPR among disadvantaged clients affected by distortions Importantly, the differences between the results are marginal, indicating a closely competitive performance between the methods despite this disparity while elevating the group-fairness among clients. Takeaway: MWR ensures fairness across groups and maintains predictive accuracy by using importance weights that prioritize the worst-performing groups. Its key strength lies in maintaining fairness without sacrificing performance, achieved through even distribution of importance weights among different groups.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Robustness Of Bias Mitigation.",
      "text": "In our previous analysis, we added low-grade Gaussian noise to mimic noise in edge device images  [47] . To further test MWR's resilience against increased feature heterogeneity, we raised noise levels in segmented datasets like CIFAR10, MNIST, DIGITS, and Fashion-MNIST to variances of 0.11, 1.10, 1.00, and 0.4, respectively. Model performance evaluation used the same fairness metrics as before. Table  2  displays TPR, TPRSD, WTPR, and BTPR scores across various bias mitigation techniques and datasets, exploring high-grade distortion scenarios in local data. Consistent with our earlier findings, MWR delivers significantly fairer outcomes across diverse groups. The table shows MWR reduces TPRSD across most devices while maintaining high TPR. Compared with Table  1 , MWR increases WTPR for the lowest TPR group, resulting in minimal TPRD among bias mitigation techniques. This enhancement in WTPR for disadvantaged groups minimally affects high-performing groups' performance.\n\nAlthough some bias mitigation techniques may slightly outperform in TPRSD and WTPR fairness metrics, this often occurs at the expense of decreased TPR in privileged clients not affected by distortions. However, this decrease compensates for an increase in lower TPR among disadvantaged clients. Despite these differences, the results remain closely competitive among methods, indicating similar performance despite disparity, while simultaneously improving group fairness among clients.\n\nTakeaway. our robustness analysis suggests that MWR stands out as a robust and fair approach even in scenarios with highgrade heterogeneity, showcasing its effectiveness in mitigating bias across diverse datasets and client groups.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Privacy Analysis",
      "text": "This section explores how differential privacy affects group fairness and performance in MWR, particularly in scenarios where local group probability distributions ğ‘ (G = ğ‘” ğ‘– |x ğ‘–,ğ‘˜ ) are shared with the server to compute importance weights. Differential privacy is crucial for preserving privacy in client metadata, preventing disclosure of sensitive details like group selection probabilities.  We use the MNIST and FashionMNIST datasets for our privacy budget analysis, maintaining consistency in experimental setups and various learning components as detailed in Â§3.1. We introduce different levels of Laplace noise, denoted by ğœ–, to local probability distributions. An ğœ– value of 0.00 represents perfect differential privacy in the implementation of MWR.\n\nFigures 5 to 8 show the impact of varying levels of Laplace noise (ğœ–) on group-fairness metrics (WTPR, TPRSD, and TPRD) and group performance (TPR) in MWR, addressing bias in local data with different levels of feature noise. In Figures  5a to 7b , we see that using a privacy budget (ğœ– âˆˆ 0.0, 0.4, 0.8) for metadata exchange maintains fairness metrics similar to deploying MWR without privacy (ğœ– -â†’ âˆ) on MNIST and FashionMNIST. This is evident from minimal variations in WTPR, TPRSD, and TPRD across all clients (with high and low feature heterogeneity) under all privacy budgets. Moreover, the privacy budget ensures fairness while preserving the best and worst TPR performance. This aligns with the fairness guarantee of MWR, as the privacy budget values (ğœ– âˆˆ 0.0, 0.4, 0.8) fall within a range that provides algorithmic fairness, as noted in  [1] . Our privacy analysis underscores that our method ensures client privacy through differential privacy on shared metadata without significantly affecting bias or accuracy. Takeaway. MWR demonstrates the feasibility of preserving sensitive information while effectively reducing group bias.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Fairness Budget Analysis",
      "text": "MWR incorporates a fairness budget, denoted as ğœ‚ ğœ‡ , to regulate importance weight adjustments for fairness. This control mechanism in MWR adjusts importance weights based on past group performance (group loss) for fairness metrics. We assess the impact of ğœ‚ ğœ‡ on group fairness metrics (WTPR, TPRSD, TPRD) using MNIST and FashionMNIST datasets, setting ğœ‚ ğœ‡ to different values (-0.009, -0.003, -0.001, -0.0002). Tables  3  and 4  show how the fairness budget ğœ‚ ğœ‡ affects both group fairness and group performance (TPR) with MWR.\n\nIncreasing ğœ‚ ğœ‡ values improve fairness guarantees, leading to better WTPR, TPRSD, and TPRD due to faster convergence and adaptation to fairness issues. Conversely, lower ğœ‚ ğœ‡ values result in more gradual adjustments, slowing down the algorithm's fairness improvements. This experiment is",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). A group catego-",
      "page": 1
    },
    {
      "caption": "Figure 1: Illustrating the adverse effects of feature heterogeneity (noise) and its bias impact on image classification data [42] on",
      "page": 2
    },
    {
      "caption": "Figure 2: Varied noise levels in CIFAR10 and DIGITS datasets. The notation \"Noise = ğ‘¥\" denotes the introduction of Gaussian",
      "page": 4
    },
    {
      "caption": "Figure 2: a shows group-bias in CIFAR10, while Figure 2b",
      "page": 4
    },
    {
      "caption": "Figure 3: shows histograms of these gradients,",
      "page": 4
    },
    {
      "caption": "Figure 3: Gradient distribution in a fully connected layer on the CIFAR10 dataset. The red and blue bars depict the local",
      "page": 5
    },
    {
      "caption": "Figure 4: â¶In our setup, the server selects all the available clients in",
      "page": 5
    },
    {
      "caption": "Figure 4: Overview of the proposed approach.",
      "page": 6
    },
    {
      "caption": "Figure 5: Examining the performance trade-off in ğ‘€ğ‘Šğ‘…concerning privacy and accuracy across various levels of differential",
      "page": 10
    },
    {
      "caption": "Figure 6: Examining the performance trade-off in ğ‘€ğ‘Šğ‘…concerning privacy and accuracy across various levels of differential",
      "page": 10
    },
    {
      "caption": "Figure 7: Analyzing the privacy-bias trade-off in ğ‘€ğ‘Šğ‘…across differential privacy (DP) noise levels on FashionMNIST. (a)",
      "page": 11
    },
    {
      "caption": "Figure 8: Analyzing the privacy-bias trade-off in ğ‘€ğ‘Šğ‘…across differential privacy (DP) noise levels on MNIST. (a) introduces a",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "92\n92\n92\n92\n92": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithms": "",
          "Datasets": "CIFAR10"
        },
        {
          "Algorithms": "FedAvg [45]",
          "Datasets": "Client #"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "AFL [52]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "FedNTD [41]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "TERM [43]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "GIFAIR-FL [73]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "SCAFFOLD [34]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "MWR",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR-threshold"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithms": "",
          "Datasets": "CIFAR10"
        },
        {
          "Algorithms": "FedAvg [45]",
          "Datasets": "Client #"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "AFL [52]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "FedNTD [41]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "TERM [43]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "GIFAIR-FL [73]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "SCAFFOLD [34]",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        },
        {
          "Algorithms": "MWR",
          "Datasets": "TPRD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "TPRSD â†“"
        },
        {
          "Algorithms": "",
          "Datasets": "WTPR â†‘"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR-threshold"
        },
        {
          "Algorithms": "",
          "Datasets": "BTPR â†‘"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Client #": "TPRD â†“",
          "1": "2",
          "2": "2",
          "3": "2",
          "4": "20",
          "5": "20"
        },
        {
          "Client #": "TPRSD â†“",
          "1": "0.63",
          "2": "0.63",
          "3": "0.63",
          "4": "6.45",
          "5": "6.45"
        },
        {
          "Client #": "WTPR â†‘",
          "1": "98",
          "2": "98",
          "3": "98",
          "4": "60",
          "5": "60"
        },
        {
          "Client #": "BTPR â†‘",
          "1": "100",
          "2": "100",
          "3": "100",
          "4": "80",
          "5": "80"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Mitigating bias in federated learning",
      "authors": [
        "Annie Abay",
        "Yi Zhou",
        "Nathalie Baracaldo",
        "Shashank Rajamoni",
        "Ebube Chuba",
        "Heiko Ludwig"
      ],
      "year": "2020",
      "venue": "Mitigating bias in federated learning",
      "arxiv": "arXiv:2012.02447"
    },
    {
      "citation_id": "2",
      "title": "Multiclass classification by sparse multinomial logistic regression",
      "authors": [
        "Felix Abramovich",
        "Tomer Vadim Grinshtein",
        "Levy"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "3",
      "title": "The multiplicative weights update method: a meta-algorithm and applications",
      "authors": [
        "Sanjeev Arora",
        "Elad Hazan",
        "Satyen Kale"
      ],
      "year": "2012",
      "venue": "Theory of computing"
    },
    {
      "citation_id": "4",
      "title": "A survey of handwritten character recognition with mnist and emnist",
      "authors": [
        "Alejandro Baldominos",
        "Yago Saez",
        "Pedro Isasi"
      ],
      "year": "2019",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "5",
      "title": "Large-scale machine learning with stochastic gradient descent",
      "authors": [
        "LÃ©on Bottou"
      ],
      "year": "2010",
      "venue": "Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France"
    },
    {
      "citation_id": "6",
      "title": "Invited and Contributed Papers",
      "authors": [
        "Keynote"
      ],
      "venue": "Invited and Contributed Papers"
    },
    {
      "citation_id": "7",
      "title": "What is the effect of importance weighting in deep learning",
      "authors": [
        "Jonathon Byrd",
        "Zachary Lipton"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "8",
      "title": "On Fair Classification with Mostly Private Sensitive Attributes",
      "authors": [
        "Canyu Chen",
        "Yueqing Liang",
        "Xiongxiao Xu",
        "Shangyu Xie",
        "Yuan Hong",
        "Kai Shu"
      ],
      "year": "2022",
      "venue": "On Fair Classification with Mostly Private Sensitive Attributes",
      "arxiv": "arXiv:2207.08336"
    },
    {
      "citation_id": "9",
      "title": "Why is my classifier discriminatory? Advances in neural information processing systems",
      "authors": [
        "Irene Chen",
        "David Fredrik D Johansson",
        "Sontag"
      ],
      "year": "2018",
      "venue": "Why is my classifier discriminatory? Advances in neural information processing systems"
    },
    {
      "citation_id": "10",
      "title": "Fedfair: Training fair models in cross-silo federated learning",
      "authors": [
        "Lingyang Chu",
        "Lanjun Wang",
        "Yanjie Dong",
        "Jian Pei",
        "Zirui Zhou",
        "Yong Zhang"
      ],
      "year": "2021",
      "venue": "Fedfair: Training fair models in cross-silo federated learning",
      "arxiv": "arXiv:2109.05662"
    },
    {
      "citation_id": "11",
      "title": "Iterative parameter mixing for distributed large-margin training of structured predictors for natural language processing",
      "authors": [
        "Gregory Francis"
      ],
      "year": "2015",
      "venue": "Iterative parameter mixing for distributed large-margin training of structured predictors for natural language processing"
    },
    {
      "citation_id": "12",
      "title": "Addressing algorithmic disparity and performance inconsistency in federated learning",
      "authors": [
        "Sen Cui",
        "Weishen Pan",
        "Jian Liang",
        "Changshui Zhang",
        "Fei Wang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "13",
      "title": "Distributionally robust federated averaging",
      "authors": [
        "Yuyang Deng",
        "Mohammad Mahdi Kamani",
        "Mehrdad Mahdavi"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Minimax group fairness: Algorithms and experiments",
      "authors": [
        "Emily Diana",
        "Wesley Gill",
        "Michael Kearns",
        "Krishnaram Kenthapadi",
        "Aaron Roth"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "15",
      "title": "Toward an Affect-Sensitive AutoTutor",
      "authors": [
        "D' Sidney",
        "Rosalind Mello",
        "Arthur Picard",
        "Graesser"
      ],
      "year": "2007",
      "venue": "IEEE Intelligent Systems",
      "doi": "10.1109/MIS.2007.79"
    },
    {
      "citation_id": "16",
      "title": "Robust fairness-aware learning under sample selection bias",
      "authors": [
        "Wei Du",
        "Xintao Wu"
      ],
      "year": "2021",
      "venue": "Robust fairness-aware learning under sample selection bias",
      "arxiv": "arXiv:2105.11570"
    },
    {
      "citation_id": "17",
      "title": "Fairnessaware agnostic federated learning",
      "authors": [
        "Wei Du",
        "Depeng Xu",
        "Xintao Wu",
        "Hanghang Tong"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 SIAM International Conference on Data Mining (SDM)"
    },
    {
      "citation_id": "18",
      "title": "Evaluation of federated learning aggregation algorithms: application to human activity recognition",
      "authors": [
        "Sannara Ek",
        "FranÃ§ois Portet",
        "Philippe Lalanda",
        "German Vega"
      ],
      "year": "2020",
      "venue": "Adjunct proceedings of the 2020 ACM international joint conference on pervasive and ubiquitous computing and proceedings of the 2020 ACM international symposium on wearable computers"
    },
    {
      "citation_id": "19",
      "title": "Fairfed: Enabling group fairness in federated learning",
      "authors": [
        "Shen Yahya H Ezzeldin",
        "Chaoyang Yan",
        "Emilio He",
        "Salman Ferrara",
        "Avestimehr"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Improving fairness for data valuation in horizontal federated learning",
      "authors": [
        "Zhenan Fan",
        "Huang Fang",
        "Zirui Zhou",
        "Jian Pei",
        "Changxin Michael P Friedlander",
        "Yong Liu",
        "Zhang"
      ],
      "year": "2022",
      "venue": "2022 IEEE 38th International Conference on Data Engineering (ICDE)"
    },
    {
      "citation_id": "21",
      "title": "Learning bounds for open-set learning",
      "authors": [
        "Zhen Fang",
        "Jie Lu",
        "Anjin Liu",
        "Feng Liu",
        "Guangquan Zhang"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "22",
      "title": "Federated learning for COVID-19 screening from Chest X-ray images",
      "authors": [
        "Ines Feki",
        "Yousri Sourour Ammar",
        "Khan Kessentini",
        "Muhammad"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "23",
      "title": "Certifying and removing disparate impact",
      "authors": [
        "Michael Feldman",
        "A Sorelle",
        "John Friedler",
        "Carlos Moeller",
        "Suresh Scheidegger",
        "Venkatasubramanian"
      ],
      "year": "2015",
      "venue": "proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "24",
      "title": "Enforcing fairness in private federated learning via the modified method of differential multipliers",
      "authors": [
        "RodrÃ­guez Borja",
        "Filip GÃ¡lvez",
        "Rogier Granqvist",
        "Matt Van Dalen",
        "Seigel"
      ],
      "year": "2021",
      "venue": "NeurIPS 2021 Workshop Privacy in Machine Learning"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised domain adaptation by backpropagation",
      "authors": [
        "Yaroslav Ganin",
        "Victor Lempitsky"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "26",
      "title": "Robustness of deep convolutional neural networks for image degradations",
      "authors": [
        "Sanjukta Ghosh",
        "Rohan Shet",
        "Peter Amon",
        "Andreas Hutter",
        "AndrÃ© Kaup"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning",
      "authors": [
        "Nina GrgiÄ‡-HlaÄa",
        "Muhammad Bilal Zafar",
        "Krishna Gummadi",
        "Adrian Weller"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "28",
      "title": "The elements of statistical learning: data mining, inference, and prediction",
      "authors": [
        "Trevor Hastie",
        "Robert Tibshirani",
        "Jerome Friedman",
        "Jerome Friedman"
      ],
      "year": "2009",
      "venue": "The elements of statistical learning: data mining, inference, and prediction"
    },
    {
      "citation_id": "29",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "30",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "31",
      "title": "The non-iid data quagmire of decentralized machine learning",
      "authors": [
        "Kevin Hsieh",
        "Amar Phanishayee",
        "Onur Mutlu",
        "Phillip Gibbons"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "32",
      "title": "Fedmgda+: Federated learning meets multi-objective optimization",
      "authors": [
        "Zeou Hu",
        "Kiarash Shaloudegi",
        "Guojun Zhang",
        "Yaoliang Yu"
      ],
      "year": "2020",
      "venue": "Fedmgda+: Federated learning meets multi-objective optimization",
      "arxiv": "arXiv:2006.11489"
    },
    {
      "citation_id": "33",
      "title": "A database for handwritten text recognition research",
      "authors": [
        "Jonathan Hull"
      ],
      "year": "1994",
      "venue": "IEEE Transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "34",
      "title": "Fairness-aware classifier with prejudice remover regularizer",
      "authors": [
        "Toshihiro Kamishima",
        "Shotaro Akaho",
        "Hideki Asoh",
        "Jun Sakuma"
      ],
      "year": "2012",
      "venue": "Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2012"
    },
    {
      "citation_id": "35",
      "title": "Scaffold: Stochastic controlled averaging for federated learning",
      "authors": [
        "Praneeth Sai",
        "Satyen Karimireddy",
        "Mehryar Kale",
        "Sashank Mohri",
        "Sebastian Reddi",
        "Ananda Stich",
        "Suresh Theertha"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "36",
      "title": "Feature noise induces loss discrepancy across groups",
      "authors": [
        "Fereshte Khani",
        "Percy Liang"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "37",
      "title": "Methods for the scientific study of discrimination and health: an ecosocial approach",
      "authors": [
        "Nancy Krieger"
      ],
      "year": "2012",
      "venue": "American journal of public health"
    },
    {
      "citation_id": "38",
      "title": "Learning multiple layers of features from tiny images",
      "authors": [
        "Alex Krizhevsky",
        "Geoffrey Hinton"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images"
    },
    {
      "citation_id": "39",
      "title": "Privacy-Preserving federated learning in medical diagnosis with homomorphic re-Encryption",
      "authors": [
        "Hanchao Ku",
        "Willy Susilo",
        "Yudi Zhang",
        "Wenfen Liu",
        "Mingwu Zhang"
      ],
      "year": "2022",
      "venue": "Computer Standards & Interfaces"
    },
    {
      "citation_id": "40",
      "title": "Multi-objective tabu search using a multinomial probability mass function",
      "authors": [
        "Sadan Kulturel-Konak",
        "Alice Smith",
        "Bryan Norman"
      ],
      "year": "2006",
      "venue": "European Journal of Operational Research"
    },
    {
      "citation_id": "41",
      "title": "Gradient-based learning applied to document recognition",
      "authors": [
        "Yann Lecun",
        "LÃ©on Bottou",
        "Yoshua Bengio",
        "Patrick Haffner"
      ],
      "year": "1998",
      "venue": "Proc. IEEE"
    },
    {
      "citation_id": "42",
      "title": "Preservation of the global knowledge by not-true distillation in federated learning",
      "authors": [
        "Gihun Lee",
        "Minchan Jeong",
        "Yongjin Shin",
        "Sangmin Bae",
        "Se-Young Yun"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "44",
      "title": "Tilted empirical risk minimization",
      "authors": [
        "Tian Li",
        "Ahmad Beirami",
        "Maziar Sanjabi",
        "Virginia Smith"
      ],
      "year": "2020",
      "venue": "Tilted empirical risk minimization",
      "arxiv": "arXiv:2007.01162"
    },
    {
      "citation_id": "45",
      "title": "Fair resource allocation in federated learning",
      "authors": [
        "Tian Li",
        "Maziar Sanjabi",
        "Ahmad Beirami",
        "Virginia Smith"
      ],
      "year": "2019",
      "venue": "Fair resource allocation in federated learning",
      "arxiv": "arXiv:1905.10497"
    },
    {
      "citation_id": "46",
      "title": "On the convergence of fedavg on non-iid data",
      "authors": [
        "Xiang Li",
        "Kaixuan Huang",
        "Wenhao Yang",
        "Shusen Wang",
        "Zhihua Zhang"
      ],
      "year": "2019",
      "venue": "On the convergence of fedavg on non-iid data",
      "arxiv": "arXiv:1907.02189"
    },
    {
      "citation_id": "47",
      "title": "Developing multimodal intelligent affective interfaces for tele-home health care",
      "authors": [
        "Christine Lisetti",
        "Fatma Nasoz",
        "Cynthia Lerouge",
        "Onur Ozyer",
        "Kaye Alvarez"
      ],
      "year": "2003",
      "venue": "Int. J. Hum.-Comput. Stud",
      "doi": "10.1016/S1071-5819(03)00051-X"
    },
    {
      "citation_id": "48",
      "title": "CollabAR: Edge-assisted collaborative image recognition for mobile augmented reality",
      "authors": [
        "Zida Liu",
        "Guohao Lan",
        "Jovan Stojkovic",
        "Yunfan Zhang",
        "Carlee Joe-Wong",
        "Maria Gorlatova"
      ],
      "year": "2020",
      "venue": "2020 19th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)"
    },
    {
      "citation_id": "49",
      "title": "Collaborative fairness in federated learning",
      "authors": [
        "Lingjuan Lyu",
        "Xinyi Xu",
        "Qian Wang",
        "Han Yu"
      ],
      "year": "2020",
      "venue": "Federated Learning: Privacy and Incentive"
    },
    {
      "citation_id": "50",
      "title": "Fairness through causal awareness: Learning causal latent-variable models for biased data",
      "authors": [
        "David Madras",
        "Elliot Creager",
        "Toniann Pitassi",
        "Richard Zemel"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference on fairness, accountability, and transparency"
    },
    {
      "citation_id": "51",
      "title": "Minimax pareto fairness: A multi objective perspective",
      "authors": [
        "Natalia Martinez",
        "Martin Bertran",
        "Guillermo Sapiro"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "52",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "Ninareh Mehrabi",
        "Fred Morstatter",
        "Nripsuta Saxena",
        "Kristina Lerman",
        "Aram Galstyan"
      ],
      "year": "2021",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "53",
      "title": "Agnostic federated learning",
      "authors": [
        "Mehryar Mohri",
        "Gary Sivek",
        "Ananda Theertha"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "54",
      "title": "Spearman correlation coefficients, differences between",
      "authors": [
        "Leann Myers",
        "Maria Sirois"
      ],
      "year": "2004",
      "venue": "Encyclopedia of statistical sciences"
    },
    {
      "citation_id": "55",
      "title": "Game of gradients: Mitigating irrelevant clients in federated learning",
      "authors": [
        "Lokesh Nagalapatti",
        "Ramasuri Narayanam"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "56",
      "title": "A comparison of regularization techniques in deep neural networks",
      "authors": [
        "Ismoilov Nusrat",
        "Sung-Bong Jang"
      ],
      "year": "2018",
      "venue": "Symmetry"
    },
    {
      "citation_id": "57",
      "title": "Clusterfl: a similarity-aware federated learning system for human activity recognition",
      "authors": [
        "Xiaomin Ouyang",
        "Zhiyuan Xie",
        "Jiayu Zhou",
        "Jianwei Huang",
        "Guoliang Xing"
      ],
      "year": "2021",
      "venue": "Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services"
    },
    {
      "citation_id": "58",
      "title": "Minimax demographic group fairness in federated learning",
      "authors": [
        "Afroditi Papadaki",
        "Natalia Martinez",
        "Martin Bertran",
        "Guillermo Sapiro",
        "Miguel Rodrigues"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "59",
      "title": "Improving Fairness in AI Models on Electronic Health Records: The Case for Federated Learning Methods",
      "authors": [
        "Raphael Poulain",
        "Mirza Farhan Bin",
        "Rahmatollah Tarek",
        "Beheshti"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "60",
      "title": "Collaborative federated learning for healthcare: Multi-modal covid-19 diagnosis at the edge",
      "authors": [
        "Adnan Qayyum",
        "Kashif Ahmad",
        "Muhammad Ahtazaz Ahsan",
        "Ala Al-Fuqaha",
        "Junaid Qadir"
      ],
      "year": "2022",
      "venue": "IEEE Open Journal of the Computer Society"
    },
    {
      "citation_id": "61",
      "title": "Robust federated learning: The case of affine distribution shifts",
      "authors": [
        "Amirhossein Reisizadeh",
        "Farzan Farnia"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "62",
      "title": "Mehryar Mohri, and Ananda Theertha Suresh. 2021. Communication-efficient agnostic federated averaging",
      "authors": [
        "Jae Ro",
        "Mingqing Chen",
        "Rajiv Mathews"
      ],
      "year": "2021",
      "venue": "Mehryar Mohri, and Ananda Theertha Suresh. 2021. Communication-efficient agnostic federated averaging",
      "arxiv": "arXiv:2104.02748"
    },
    {
      "citation_id": "63",
      "title": "Adapting visual category models to new domains",
      "authors": [
        "Kate Saenko",
        "Brian Kulis",
        "Mario Fritz",
        "Trevor Darrell"
      ],
      "year": "2010",
      "venue": "Computer Vision-ECCV 2010: 11th European Conference on Computer Vision"
    },
    {
      "citation_id": "64",
      "title": "Phase transition in the hard-margin support vector machines",
      "authors": [
        "Houssem Sifaou",
        "Abla Kammoun",
        "Mohamed-Slim Alouini"
      ],
      "year": "2019",
      "venue": "2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing"
    },
    {
      "citation_id": "65",
      "title": "FLAIR: Federated Learning Annotated Image Repository",
      "authors": [
        "Congzheng Song",
        "Filip Granqvist",
        "Kunal Talwar"
      ],
      "year": "2022",
      "venue": "FLAIR: Federated Learning Annotated Image Repository",
      "arxiv": "arXiv:2207.08869"
    },
    {
      "citation_id": "66",
      "title": "Mor Shpigel Nacson, Suriya Gunasekar, and Nathan Srebro",
      "authors": [
        "Daniel Soudry",
        "Elad Hoffer"
      ],
      "year": "2018",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "67",
      "title": "Human activity recognition using federated learning",
      "authors": [
        "Konstantin Sozinov",
        "Vladimir Vlassov",
        "Sarunas Girdzijauskas"
      ],
      "year": "2018",
      "venue": "2018 IEEE Intl Conf on Parallel & Distributed Processing with Applications"
    },
    {
      "citation_id": "68",
      "title": "Sustainable Computing & Communications",
      "authors": [
        "Networking Computing"
      ],
      "venue": "Sustainable Computing & Communications"
    },
    {
      "citation_id": "69",
      "title": "FedSEA: A Semi-Asynchronous Federated Learning Framework for Extremely Heterogeneous Devices",
      "authors": [
        "Jingwei Sun",
        "Ang Li",
        "Lin Duan",
        "Samiul Alam",
        "Xuliang Deng",
        "Xin Guo",
        "Haiming Wang",
        "Maria Gorlatova",
        "Mi Zhang",
        "Hai Li"
      ],
      "year": "2022",
      "venue": "FedSEA: A Semi-Asynchronous Federated Learning Framework for Extremely Heterogeneous Devices"
    },
    {
      "citation_id": "70",
      "title": "Negative Patient Descriptors: Documenting Racial Bias In The Electronic Health Record: Study examines racial bias in the patient descriptors used in the electronic health record",
      "authors": [
        "Michael Sun",
        "Tomasz Oliwa",
        "Monica Peek",
        "Elizabeth Tung"
      ],
      "year": "2022",
      "venue": "Health Affairs"
    },
    {
      "citation_id": "71",
      "title": "Modeling techniques for machine learning fairness: A survey",
      "authors": [
        "Mingyang Wan",
        "Daochen Zha",
        "Ninghao Liu",
        "Na Zou"
      ],
      "year": "2021",
      "venue": "Modeling techniques for machine learning fairness: A survey",
      "arxiv": "arXiv:2111.03015"
    },
    {
      "citation_id": "72",
      "title": "Adaptive federated learning in resource constrained edge computing systems",
      "authors": [
        "Shiqiang Wang",
        "Tiffany Tuor",
        "Theodoros Salonidis",
        "Kin Leung",
        "Christian Makaya",
        "Ting He",
        "Kevin Chan"
      ],
      "year": "2019",
      "venue": "IEEE journal on selected areas in communications"
    },
    {
      "citation_id": "73",
      "title": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
      "authors": [
        "Han Xiao",
        "Kashif Rasul",
        "Roland Vollgraf"
      ],
      "year": "2017",
      "venue": "Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms",
      "arxiv": "arXiv:1708.07747"
    },
    {
      "citation_id": "74",
      "title": "Experience-Driven Procedural Content Generation",
      "authors": [
        "Georgios Yannakakis",
        "Julian Togelius"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.6"
    },
    {
      "citation_id": "75",
      "title": "Gifair-fl: An approach for group and individual fairness in federated learning",
      "authors": [
        "Xubo Yue",
        "Maher Nouiehed",
        "Kontar"
      ],
      "year": "2021",
      "venue": "Gifair-fl: An approach for group and individual fairness in federated learning",
      "arxiv": "arXiv:2108.02741"
    },
    {
      "citation_id": "76",
      "title": "Improving fairness via federated learning",
      "authors": [
        "Yuchen Zeng",
        "Hongxu Chen",
        "Kangwook Lee"
      ],
      "year": "2021",
      "venue": "Improving fairness via federated learning",
      "arxiv": "arXiv:2110.15545"
    },
    {
      "citation_id": "77",
      "title": "Fairfl: A fair federated learning approach to reducing demographic bias in privacysensitive classification models",
      "authors": [
        "Ziyi Daniel Yue Zhang",
        "Dong Kou",
        "Wang"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Big Data (Big Data)"
    },
    {
      "citation_id": "78",
      "title": "Unified group fairness on federated learning",
      "authors": [
        "Fengda Zhang",
        "Kun Kuang",
        "Yuxuan Liu",
        "Long Chen",
        "Chao Wu",
        "Fei Wu",
        "Jiaxun Lu",
        "Yunfeng Shao"
      ],
      "year": "2021",
      "venue": "Unified group fairness on federated learning",
      "arxiv": "arXiv:2111.04986"
    },
    {
      "citation_id": "79",
      "title": "On the convergence properties of a ğ¾-step averaging stochastic gradient descent algorithm for nonconvex optimization",
      "authors": [
        "Fan Zhou",
        "Guojing Cong"
      ],
      "year": "2017",
      "venue": "On the convergence properties of a ğ¾-step averaging stochastic gradient descent algorithm for nonconvex optimization",
      "arxiv": "arXiv:1708.01012"
    }
  ]
}