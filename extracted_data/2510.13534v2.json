{
  "paper_id": "2510.13534v2",
  "title": "High Semantic Features For The Continual Learning Of Complex Emotions: A Lightweight Solution",
  "published": "2025-10-15T13:27:41Z",
  "authors": [
    "Thibault Geoffroy",
    "Gauthier Gerspacher",
    "Lionel Prevost"
  ],
  "keywords": [
    "Complex Emotion Recognition",
    "Incremental Learning",
    "Action Units",
    "Gaussian Mixture Models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Incremental learning is a complex process due to potential catastrophic forgetting of old tasks when learning new ones. This is mainly due to transient features that do not fit from task to task. In this paper, we focus on complex emotion recognition. First, we learn basic emotions and then, incrementally, like humans, complex emotions. We show that Action Units, describing facial muscle movements, are non-transient, highly semantical features that outperform those extracted by both shallow and deep convolutional neural networks. Thanks to this ability, our approach achieves interesting results when learning incrementally complex, compound emotions with an accuracy of 0.75 on the CFEE dataset and can be favorably compared to state-of-the-art results. Moreover, it results in a lightweight model with a small memory footprint.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the fourth industrial age, artificial intelligence (AI) plays a crucial role in many of our activities and endeavors. AI is able to match or even outperform humans in cognitive tasks such as image recognition and natural language processing. But in the uniquely human domains where communication, empathy, and compassion are needed, AI is still limited in handling those more subtle tasks.\n\nCommunication between humans is fundamental to our ability to learn, work, and interact. Among communication channels, facial expressions are one of the most efficient vectors to express emotional states and intentions. In fact, more than 55% of emotional communication (also called non-verbal communication) is conveyed through facial expressions  (Mehrabian, 2017) . As a result, the automatic recognition of facial emotions is becoming a crucial component, as recognizing emotional states forms the foundation for a computer's understanding of emotions and its subsequent responses.\n\nIn recent decades, numerous Facial Expression Recognition (FER) approaches have been investigated in the fields of computer vision and machine learning to encode expression information from face representations. Unfortunately, most of these studies focus on basic emotion recognition such as joy, fear, and anger  (Ekman, 1992) . But in real day-to-day situations, recognizing only these basic universal emo-tions is no longer enough, and recognizing complex emotions becomes a necessity. For example, in the area of education, when students face problems during pedagogical activities and exhibit complex emotional states like misunderstanding, stress or pride.\n\nCurrent state of the art in basic emotion recognition primarily relies on deep learning techniques (e.g.,  Deep Convolutional Neural Networks and, recently, Transformers) . In recent years, these methods have enabled computer vision systems to achieve highly efficient results, outperforming classical methods that rely on handcrafted static features. Surprisingly, these methods exhibit poor performance on complex emotion recognition. On the contrary, continual learning methods perform better.\n\nIn this study, we decided to address the problem of complex emotion recognition by incrementally learning these latter. In fact, our aim was to mimic humans. As humans, studies show that we learn to express and recognize basic emotions during our early childhood  (Denham, 1998)    (Pons and de Rosnay, 2004)    (Widen and Russell, 2008) , while complex and subtle emotions are learned continuously throughout our life. The other aim of this study is to promote a lightweight sustainable model, with a low memory and carbon footprint.\n\nThe study is divided into two parts. First, we compare the behavior of two network architectures using the same incremental learning strategy found in the literature. The first one is shallow and hand-crafted, while the second is deeper and already pre-trained. We use both as feature extractors from images, make copies of them and train these copies independently, task by task, to build an incremental, multiple expert recognizer (the process is detailed in section 3.2). That way, we extract low or mid-level features that are transient since they can change from one task to another. In the second part, we extract highly semantic features (facial muscle activation also known as Facial Action Units) directly from images, using an open-source research tool. Contrary to the previous features, these Action Units are non-transient and stay the same during all the learning process, whatever the emotion and its complexity. All these experts also use class-conditional Gaussian models to model class distributions and perform classification. so, our main contributions in the FER domain are the following:\n\n• Features extracted by CNNs, whatever the complexity of the latter and even if it is trained continuously, have a low to mid semantic level. Thus, they cannot discriminate the subtle differences that may appear between complex emotions. • Action Units, on the other hand, have a higher semantic level and offer strong discrimination capabilities for complex emotions. Moreover, since they are non-transient features, they do not need continual learning. • Gaussian models constitute a lightweight solution as density estimators and powerful emotion predictors when used on a smart feature space. Moreover, the memory footprint of an ensemble of Gaussian models is really low, compared to neural networks.\n\nThe paper is organized as follows. Section 2 presents in detail the different ways of describing a facial emotion and several solutions to train a classifier incrementally. In section 3, we describe the methodology, including pre-processing and emotion recognition using different feature extractors: low-level CNN features and high-level Action Units. Section 4 is devoted to a thorough analysis of the experimental results. Finally, the last section analyzes the findings and concludes this work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "State Of The Art",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Basic Emotions, Complex Emotions And Action Units",
      "text": "Basic emotions are generally recognized throughout cultures and are commonly linked to specific physical reactions and facial expressions. Fear, anger, disgust, sadness, joy, and surprise are among the most frequently acknowledged basic emotions  (Ekman, 1992) .\n\nComplex emotions are intricate and multifaceted emotional experiences that encompass a blend of basic emotions, cognitive processes, and social influences. Contrary to basic emotions that are short-lived, complex emotions tend to persist for extended periods. Furthermore, they frequently result from combinations or interactions of basic emotions  (Maiden, 2023)  and are more likely to involve mixed sentiments or uncertainty. That is why they are usually called compound emotions or expressions. In  (Du et al., 2014) , authors define 21 categories of compound emotions that humans express in-the-wild. For example, jealousy could blend fear (of losing a relationship) with anger (toward a perceived rival). Thus, the corresponding compound emotion would be \"fearfully angry\". Most of the time, compound expressions are more than the sum of their basic parts, as shown in figure  1 .\n\nFigure  1 : Basic and compound emotions  (Maiden, 2023) .\n\nThe previous methods of expression categorization are highly subjective. In contrast, the Facial Action Coding System (FACS)  (Friesen and Ekman, 1983 ) offers an objective way to describe expressions. FACS encodes the activation of facial muscles as known as Action Units (AUs). For example, AU1 encodes the raising of the inner brow and AU20, the lip stretcher. Expression of emotion involves several AUs and their intensity indicates the degree of expression of this emotion (see figure  2 . It is important to note that a single expression can be produced by various sets of AUs, depending on the subject. Nevertheless, AUs are highly semantic features that could be used to predict emotions with a high accuracy, as demonstrated in  (Senechal et al., 2014) .\n\nAction Units detection has been extensively studied by the affective computing community over two decades. In summary, methods have evolved (as in many domains in pattern recognition) from handcrafted features (such as Haar or Gabor filters) used to feed a SVM  (Senechal et al., 2011) , to deep CNNs  (Akay and Arica, 2018) .  (Zhi and Zhang, 2020)  presents a detailed study of these methods. Recently, vision transformers have been proposed  (Yuan et al., 2024) . But most of these methods cannot easily han- dle incremental learning and suffer from catastrophic forgetting.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Incremental Learning",
      "text": "Also called Continual Learning or Lifelong Learning, Incremental Learning is a domain of machine learning where the data does not arrive in one go but in several phases (also called tasks) and where access to previous data is not allowed. Throughout the Incremental Learning process, the distribution of data points within a class or the number of labels can change. This creates a scenario where machine learning models need to be able to acquire knowledge from new data (i.e. plasticity) while preserving previously gained knowledge (i.e. stability). One of the main pitfalls of Incremental Learning is the tendency of machine learning models to overwrite old knowledge with new, more recent information (lack of stability). This tendency can greatly reduce the performance of the model on previous tasks and is referred to in the literature as catastrophic forgetting  (Zhou et al., 2024) .\n\nClass-Incremental Learning (CIL) represents one of the most challenging scenarios in the field of Incremental Learning. In this scenario, data arrive in batches that contain a number of data points and labels. The model has access to the data of the most recent task for the training. During inference, Task-Incremental Learning can be done using two different scenarios. The task-agnostic scenario supposes that the task-id is unknown and the classifier has to predict the right label through all the already seen labels. The task-aware scenario supposes that the task id is known during inference.\n\nThe easiest solution for CIL is to store exemplars along tasks  (Wang et al., 2022) . But this replay method is not always an option due to memory space and other constraints. Therefore, exemplar-free CIL has been thoroughly studied  (Smith et al., 2021) ,  (Zhu et al., 2022) . Several methods exist to avoid replay, such as pseudo-replay of synthetic samples generated by a generative model  (Shin et al., 2017) . Knowledge distillation of older models during the training of new ones to retain knowledge on old tasks (  (Maiden, 2023) ,  (Thuseethan et al., 2020) ) is also usual. Finally, architecture-based methods for CIL can dynamically adjust the network's parameters while learning new tasks. In the extreme case, each task can have a dedicated network as an expert  (Rypeść et al., 2024) . That greatly improves plasticity but also requires more resources as the number of parameters increases. To limit the latter, it is possible to freeze a part of the feature extractors and share it between all experts.\n\nA few years ago, some exemplar-free methods employed nearest mean classifiers with stored class centroids  (Rebuffi et al., 2017) . Recently, in  (Yang et al., 2023) , a fixed, pre-trained feature extractor and class-conditional Gaussian models with diagonal covariance matrices were used to solve CIL problems. In  (Rypeść et al., 2024) , this solution has been extended by building an \"expert\" which combines a deep network and an ensemble of class-conditional Gaussian models to deal with each task. During CIL, when a new task appears, a new expert inherits the network parameters learned on the previous task. New Gaussian models, conditional to new task classes, are trained. During inference, the loglikelihoods of the sample are computed by all the Gaussian models, and the Bayes rule is used to predict the class. More information on this method will be given in section 3.2.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Proposed Methodology",
      "text": "We first describe the image pre-processing (face alignment and AUs extraction). Then, we detail the class-incremental learning algorithm that adds one dedicated expert (shallow or deep CNN) per task before combining them. Finally, we use the same process using AUs and conditional Gaussian Mixture Models (GMM).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pre-Processing",
      "text": "This work mainly uses OpenFace, a tool designed for the affective computing community and researchers interested in building interactive applications based on facial behavior analysis. It is the first non-profit toolkit capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye gaze estimation with available source code 1 . The computer vision algorithms that represent the core of OpenFace 2.0 demonstrate state-of-the-art results in all of the above-mentioned tasks  (Baltrusaitis et al., 2018) . In detail, landmark detection uses a Constrained Local Model (CLM). Then, face is aligned and encoded by using PCA-reduced Histogram of Oriented Gradient. This vector is concatenated with shape features extracted by the CLM. Finally, a Support Vector Regressor uses this mixed information to predict the intensity of AUs activated in the face.\n\nFor training and inference, each image is processed by OpenFace to get a cropped face image, facial landmarks, and a vector containing 17 AUs intensities (on a 0 to 5 scale). Then, the image is aligned using the position of the center of the pupils.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Incremental Learning With Neural Networks",
      "text": "We propose here an architecture-based incremental learning method based on  (Rypeść et al., 2024)  that gets state-of-the-art results on the benchmark dataset CIFAR-100. The core idea of this approach is to directly diversify experts by training them sequentially on different tasks t and then combining their knowledge during the inference. Each expert contains two components:\n\n• A convolutional network that serves as feature extractor and generates a unique feature space.\n\n• An ensemble of class-conditional Gaussian models (one per class).\n\nAlgorithm The problem is divided into two tasks, each corresponding to a set of classes to be learned. As shown in figure  3 , an expert is trained on task 1 data. A CNN is learned to discriminate task classes. Then, it is used as an extractor to project images in its own feature space, by deleting dense classification layers. Finally, class-conditional Gaussian models (µ c , Σ c ) are trained on each class c of task 1, to model class-conditional distributions in the feature space. Then, since task 1 has been learned, the corresponding data are erased. When task 2 begins, a second CNN is instantiated and inherits the parameters of the first CNN. Transfer learning and fine-tuning are performed, using task 2 data only. Then, Gaussian models are trained on task 2 data, in the new feature space, one for each class. The process is repeated as long as there are tasks left. Considering T tasks, we will finally have T different experts, each specialized 1 https://github.com/TadasBaltrusaitis/OpenFace on \"their\" set of classes, composed of a feature extractor and several Gaussian models. This incremental learning algorithm is clearly based on architecture growing and not on replays or regularization methods. Some tricks are proposed in  (Rypeść et al., 2024)  for a large number of classes to constrain growing that are not reported here since the number of classes/tasks stays low. Inference To infer, all the experts are activated simultaneously. Each CNN generates a feature representation r i (the feature vector for task i) of the input image x and each Gaussian model calculates its log-likelihoods L c , using the equation below, given the distribution (µ c , Σ c ) of each class (for each expert separately).\n\n(1) where S is the size of the feature space. Then, they softmax those log-likelihoods and compute their average over all experts. Finally, Bayes posterior decision rule selects the class with the highest average softmax as the prediction, as shown in figure  4 . For task agnostic, all the classes are considered. For task aware, this procedure is limited to classes from the considered task. CNN During incremental learning, a new network N t is created for each new task t. If the task objective is to discriminate k classes, N t it will have k outputs. For task t > 1, N t it inherits the parameters of N t-1 and is fine-tuned on task t data.\n\nFor feature extraction, we used the last dense layer, which size is 64. In other words, for both training and inference, the input image is processed by the network that outputs a 64-dimensional feature vector.\n\nGMM They are used to estimate the underlying distribution of each class given the feature vectors. We use Gaussian Mixture Models instead of simple Gaussian models. We employ diagonal covariance matrices since we do not have enough samples per class to estimate full matrices. Models are trained using the Expectation-Maximization algorithm. The optimal number of components of the mixture is found by using the Akaike Information Criterion. It may vary depending on the heterogeneity of the class.\n\nEach class distribution is modeled by one GMM, trained only on data of this class. Therefore, for k classes and whatever the number of tasks, we will train k GMMs that compute, during inference, loglikelihoods for the input image x. We then use the equation below (a variant of Bayes rule) to predict the class k * of the image x.\n\nThis method is really interesting, as it transforms a set of k GMMs, used as generative models into a discriminative model that separates k classes.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Incremental Learning With Action Units",
      "text": "In this section, we use OpenFace to extract 17 AU activations (from a 0 to 5 scale). That way, we no longer need a CNN feature extractor. The new feature vector has a dimension of 17 that is much smaller than before. That is why we can use full covariance matrices in GMMs, hoping for better estimation and higher performance.\n\nGMMs are learned as previously, from task to task (see figure  5 ). Now, contrary to incremental learning with CNN, there is no longer a need to transfer knowledge from task to task, since the AU detector is already trained efficiently. During inference, the GMM that outputs the highest log-likelihood gives the predicted class.\n\nWe also evaluate here Bayesian Gaussian Mixture Models (BGMMs) based on variational inference  (Corduneanu and Bishop, 2001)  mainly for two reasons. Firstly, they have the ability to avoid singular data point solutions, and, secondly, they can directly determine the optimal number of components without resorting to methods such as cross-validation. 4 Experimental results",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset And Experimental Process",
      "text": "Most of FER databases only include basic emotion images. The Compound Facial Expression of Emotions (CFEE) dataset  (Du et al., 2014)  is one of the few public datasets whose purpose is complex emotion recognition. Unfortunately, as far as we know, CASME database  (Sun et al., 2022)  is no longer available. The CMED dataset  (Matharaarachchi et al., 2025)  focuses on child micro-expressions, which are not the aim of our study. CFEE contains 5,060 images of 230 subjects, divided into 22 classes: 7 basic emotion classes (including neutral) and 15 compound emotion classes. This distinction between basic and compound emotions leads to structuring the learning process: like humans, machine begins to learn basic emotions and then compound emotions are learned incrementally. Thus, a possible learning scenario is described below:\n\n• Task 1: 6 basic emotions and neutral • Task 2: compound joy (happily surprised, happily disgusted, awed)\n\n• Task 3: compound sadness (sadly fearful, sadly angry, sadly surprised, sadly disgusted)\n\n• Task 4 compound fear (fearfully angry, fearfully surprised, fearfully disgusted)\n\n• Task 5: compound anger (angrily surprised, angrily disgusted, hatred)\n\n• Task 6: compound disgust (disgustedly surprised, appalled)\n\nTherefore, we have a total of T = 6 tasks to learn the whole set of compound emotions. Task t 1 is devoted to learning basic emotions, while the following tasks t 2 to t 6 aim to learn a set of compound emotions linked by their primary emotion, like \"angrily surprised\" and \"angrily disgusted\". It is important to notice that, due to the incremental learning process, tasks t 2 to t 6 can be interchanged randomly, without impacting the general performance.\n\nAs a performance metric, we use accuracy (ACC), which is the ratio of the correctly classified samples on t tasks (from task 1 to task t) to the total effective of these tasks n t . Given y i the label and ŷi the prediction, accuracy is given by:\n\nRegarding state-of-the-art results on the CFEE dataset, we will first detail recent experiments in offline (batch) learning. These are gathered in table 1. As far as we know, only three methods have been proposed to learn compound emotions incrementally. Their protocol is not far from ours: they initially learn basic emotions and then, sequentially, compound emotions. Table  2  presents their core methods and performances (KD stands for Knowledge Distillation). When comparing both tables, it is clear that incremental learning performs better than offline methods. This is proof that our proposal (learning incrementally complex emotions) is meaningful.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Methods Accuracy",
      "text": "Highway CNN  (Slimani et al., 2021)  0.52 Tranfer learning VGG19  (Xie et al., 2021)  0.57 CNN-Transformer  (Ullah et al., 2023)  0.66   (Thuseethan et al., 2020)  0.85 Deep KD of basic features  (Maiden, 2023)  0.74\n\nTable  2 : State-of-Art in continuous learning.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results Of Cil With Neural Networks",
      "text": "In this study, we used two different convolutional networks. The first one (CNN1) is handcrafted and shallow. The second one is MobileNet (MBN), a pretrained deep network. We wanted to compare their behaviors in the context of incremental learning of complex emotions. We detail below the architectures of both networks.\n\nwhere DSi stands for Depthwise Separable convolutions. We also used Batch Normalization and DropOut when needed. Therefore, the total number of parameters is 166K (0.65MB) for CNN1 and 2,431K (9.3MB) for MBN.\n\nFor training, we made data augmentation and we used the Adam optimizer with an initial learning rate of 10 -3 , a categorical cross-entropy loss and early stopping on the cross-validation set. Each experiment was repeated ten times and results display the mean accuracy.\n\nIn the following, we compare, for both CNNs:\n\n• The T-SNE mapping of extracted features on 2D to see whether classes are easily separable or not.\n\n• The performance of each expert in the task-aware scenario.\n\n• The performance of the ensemble of experts in the task-agnostic scenario.\n\nT-SNE mapping: Figure  6  shows the T-SNE mapping of features when MBN is used as a features extractor. Since it is deeper than CNN1, it should extract better features. We can see that some classes have a low within-class variance. Unfortunately, between-class variance is too low and most of the classes are overlapping. Task-aware scenario: Here, we report the performance of each expert on its own task. The classification of an input of task t is done between the classes of this task only. We see in figures 7 and 8 that taskaware accuracies vary between 0.6 and 0.95 for both networks. It seems that, whatever the complexity of the network, for this specific problem of facial emotion recognition, performances are more or less the same. We can make the assumption that our convolutional networks have difficulties in finding optimal features for this problem. This poor performance is either due to a too simple architecture or a too small training set.  Task-agnostic scenario: In this experiment, the task corresponding to the input is not known. Therefore, the classification must be made between all the classes learned. Figures 9 and 10 show the cumulated task accuracy when t task has been learned. We can see that both networks have the same behavior again. MBN is just slightly more accurate than CNN1 with an accuracy of 0.74 against 0.72 on task 1. But they perform equally with an accuracy of 0.27 on all tasks.\n\nWe make the assumption that accuracy decreases from task to task because task-dedicated networks learn features able to discriminate within task classes. On the contrary, they do not learn features able to discriminate between tasks. Since compound emotions are only slightly different from one to another, experts are not able to differentiate them. This may explain the difference observed between state-of-theart results on CIFAR-100 detailed in  (Rypeść et al., 2024)  and those obtained in this work.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Results Of Cil With Action Units",
      "text": "Now, we use directly AUs as feature vectors. Therefore, GMMs estimate the distribution in the AU space and compute a likelihood. The higher likelihood gives the predicted class. We make the same experiments (mean accuracy on 10 runs) and visualization as in section 4.2.   Task-aware scenario: We can see in figure  12  that task accuracies now vary between 0.88 and 1.00. This result is much more satisfactory, and mainly due to the use of Action Units as features since the rest of the ensemble has remained unchanged. These features are non-transient, so they do not vary when tasks change. Moreover, they provide rich semantic information about facial muscle activities that is quite sufficient to manage subtle differences between compound emotions. Task-agnostic scenario: Results, presented in figure  13  are again much more convincing since accuracy evolves, from 0.92 (resp.0.82) for task 1 only to 0.75 (resp. 0.57) for all tasks learned incrementally with BGMMs (resp. GMMs). Notice that an accuracy of 0.75 is the state-of-the-art (see table  1 ). Again, AU features clearly help to differentiate compound emotions. The main drawback is that our classifier is purely generative since we only model underlying class distributions with class-conditional GMMs trained on the data of \"their\" classes. There is clearly a lack of a discriminative mechanism to effectively separate complex between-class boundaries. The confusion matrix, displayed in figure  14 , confirms that most confusions appear between classes of different tasks (bordered by black squares). We can also compute the number of parameters of this model. Given K the number of classes and C, the number of components (µ c , Σ c ) per class, we have:\n\nwhere S is the dimension of the AU space. Therefore, given C = 10, for example, the number of parameters is about 0.03M to be added to the 6M parameters of OpenFace, which is quite low compared to vision transformers like ViT-B/16 which count 86M parameters. Thus, this sustainable solution, thanks to its low memory and carbon footprint, is very satisfactory.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusions",
      "text": "We have compared in this paper several feature extractors in order to learn incrementally complex, compound facial emotions. We have shown experimentally that deep features, extracted by convolutional networks, were semantically too low to complete this task. We make the assumption that features extracted are not subtle enough to differentiate compound emotions. On the contrary, using facial muscle activities leads to much better results, near state-of-the-art. This can be explained as these activities are different from one emotion to another and, above all, non-transient. In other words, they do not need to be continually learned.\n\nIn fact, these results are not that surprising. Psychological research shows it is easier for a human to recognize basic emotions or more complex states like depression  (Rosenberg and Ekman, 2005)  or pain  (Williams, 2003)  when AUs are primarily identified. In a seminal work  (Senechal et al., 2014) ,  Senechal et al.  demonstrate that recognizing emotions in the Action Unit space led to higher performances than recognizing emotions in a feature space extracted from images. This work is a new proof of this assumption.\n\nWe have also shown that complex Gaussian models provide an effective way to model class distributions, and that these distributions are sufficiently separated to allow successful emotion prediction, provided that the feature space is carefully chosen. This solution is not only efficient but also lightweight, with a minimal memory footprint, which makes it particularly suitable for embedded applications.\n\nIn the future, we will try to improve the system's accuracy by adding a second stage to the classifier, which aims at discriminating conflict between blended emotion classes. We will also apply this system in an unsupervised incremental way, to discover new emotion classes when they appear, annotate them using Active Learning or Vision Language Model and learn them.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Figure 1: Basic and compound emotions (Maiden, 2023).",
      "page": 2
    },
    {
      "caption": "Figure 2: It is important to",
      "page": 2
    },
    {
      "caption": "Figure 2: Facial Action Coding (Du et al., 2014).",
      "page": 3
    },
    {
      "caption": "Figure 3: , an expert is trained on task 1",
      "page": 4
    },
    {
      "caption": "Figure 3: Incremental training of an ensemble of experts",
      "page": 4
    },
    {
      "caption": "Figure 4: For task ag-",
      "page": 4
    },
    {
      "caption": "Figure 4: Inference of the ensemble of experts (for T=2).",
      "page": 4
    },
    {
      "caption": "Figure 5: ). Now, contrary to incremental learning",
      "page": 5
    },
    {
      "caption": "Figure 5: Training an ensemble of AUs based classifiers (for",
      "page": 5
    },
    {
      "caption": "Figure 6: shows the T-SNE",
      "page": 6
    },
    {
      "caption": "Figure 6: T-SNE projection with MBN as feature extractor.",
      "page": 6
    },
    {
      "caption": "Figure 7: Task-aware performance using CNN1.",
      "page": 7
    },
    {
      "caption": "Figure 8: Task-aware performance using MBN.",
      "page": 7
    },
    {
      "caption": "Figure 9: Incremental learning: Task-agnostic performance",
      "page": 7
    },
    {
      "caption": "Figure 10: Incremental learning:",
      "page": 7
    },
    {
      "caption": "Figure 11: is better now. We clearly see more",
      "page": 7
    },
    {
      "caption": "Figure 11: T-SNE projection with AUs.",
      "page": 7
    },
    {
      "caption": "Figure 12: that task accuracies now vary between 0.88 and 1.00.",
      "page": 7
    },
    {
      "caption": "Figure 12: Task-aware performance using Action Units.",
      "page": 8
    },
    {
      "caption": "Figure 13: are again much more convincing since ac-",
      "page": 8
    },
    {
      "caption": "Figure 14: , confirms that",
      "page": 8
    },
    {
      "caption": "Figure 13: Task-agnostic performance using Action Units.",
      "page": 8
    },
    {
      "caption": "Figure 14: Task-agnostic confusion matrix.",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: As far as we know, only three methods have been",
      "page": 6
    },
    {
      "caption": "Table 2: presents their core methods",
      "page": 6
    },
    {
      "caption": "Table 1: State-of-Art in offline learning.",
      "page": 6
    },
    {
      "caption": "Table 2: State-of-Art in continuous learning.",
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial action unit detection using deep neural networks in videos",
      "authors": [
        "S Akay",
        "N Arica"
      ],
      "year": "2018",
      "venue": "Signal Processing and Communications Applications Conference"
    },
    {
      "citation_id": "2",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L Morency"
      ],
      "year": "2018",
      "venue": "International Conference on Face and Gesture"
    },
    {
      "citation_id": "3",
      "title": "Variational bayesian model selection for mixture distribution",
      "authors": [
        "A Corduneanu",
        "C Bishop"
      ],
      "year": "2001",
      "venue": "Artificial Intelligence & Statistics"
    },
    {
      "citation_id": "4",
      "title": "Emotional development in young children",
      "authors": [
        "S Denham"
      ],
      "year": "1998",
      "venue": "Emotional development in young children"
    },
    {
      "citation_id": "5",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "6",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "7",
      "title": "EMFACS-7: Emotional facial action coding system",
      "authors": [
        "W Friesen",
        "P Ekman"
      ],
      "year": "1983",
      "venue": "EMFACS-7: Emotional facial action coding system"
    },
    {
      "citation_id": "8",
      "title": "Complex facial expression recognition using deep knowledge distillation of basic features",
      "authors": [
        "A Maiden"
      ],
      "year": "2023",
      "venue": "Complex facial expression recognition using deep knowledge distillation of basic features"
    },
    {
      "citation_id": "9",
      "title": "Cmed: A child micro-expression dataset",
      "authors": [
        "N Matharaarachchi",
        "M Pasha",
        "S Coleman",
        "K Peng-Wong"
      ],
      "year": "2025",
      "venue": "Cmed: A child micro-expression dataset"
    },
    {
      "citation_id": "10",
      "title": "Communication without words",
      "authors": [
        "A Mehrabian"
      ],
      "year": "2017",
      "venue": "Communication Theory"
    },
    {
      "citation_id": "11",
      "title": "Emotion comprehension between 3 and 11 years: Developmental cascades and cultural comparisons",
      "authors": [
        "F Pons",
        "P Harris",
        "R De Rosnay"
      ],
      "year": "2004",
      "venue": "Emotion"
    },
    {
      "citation_id": "12",
      "title": "icarl: Incremental classifier and representation learning",
      "authors": [
        "S Rebuffi",
        "A Kolesnikov",
        "G Sperl",
        "C Lampert"
      ],
      "year": "2017",
      "venue": "International Conference on Computer Vision and Pattern Recogniion"
    },
    {
      "citation_id": "13",
      "title": "What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS)",
      "authors": [
        "E Rosenberg",
        "P Ekman"
      ],
      "year": "2005",
      "venue": "What the Face Reveals: Basic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "14",
      "title": "Divide and not forget: Ensemble of selectively trained experts in continual learning",
      "authors": [
        "G Rypeść",
        "S Cygert",
        "V Khan",
        "T Trzciński",
        "B Zieliński",
        "B Twardowski"
      ],
      "year": "2024",
      "venue": "Divide and not forget: Ensemble of selectively trained experts in continual learning"
    },
    {
      "citation_id": "15",
      "title": "Impact of action unit detection in automatic emotion recognition",
      "authors": [
        "T Senechal",
        "K Bailly",
        "L Prevost"
      ],
      "year": "2014",
      "venue": "Pattern Anaysis and Applications"
    },
    {
      "citation_id": "16",
      "title": "Facial feature tracking for emotional dynamic analysiss",
      "authors": [
        "T Senechal",
        "V Rapp",
        "L Prevost"
      ],
      "year": "2011",
      "venue": "International Conference on Advanced Concepts for Intelligent Vision Systems"
    },
    {
      "citation_id": "17",
      "title": "Continual learning with deep generative replay",
      "authors": [
        "H Shin",
        "J Lee",
        "J Kim",
        "J Kim"
      ],
      "year": "2017",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Compound facial expression recognition in the wild based on a hybrid cnn and lstm model",
      "authors": [
        "K Slimani",
        "D Alghazzawi",
        "M Ali"
      ],
      "year": "2021",
      "venue": "International Conference on Applied and Computational Mathematics"
    },
    {
      "citation_id": "19",
      "title": "Always be dreaming: A new approach for data-free class-incremental learning",
      "authors": [
        "J Smith",
        "Y Hsu",
        "J Balloch",
        "Y Shen",
        "H Jin",
        "Z Kira"
      ],
      "year": "2021",
      "venue": "International Conference on Coputer Vision"
    },
    {
      "citation_id": "20",
      "title": "Dynamic micro-expression recognition using knowledge distillation",
      "authors": [
        "B Sun",
        "S Cao",
        "D Li",
        "J He",
        "L Yu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Complex emotion profiling: An incremental active learning based approach with sparse annotations",
      "authors": [
        "S Thuseethan",
        "S Rajasegarar",
        "J Yearwood"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "22",
      "title": "Deep continual learning for emerging emotion recognition",
      "authors": [
        "S Thuseethan",
        "S Rajasegarar",
        "J Yearwood"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Cnntransformer architecture solution for compound facial expression recognition",
      "authors": [
        "S Ullah",
        "Y Xie",
        "J Ou",
        "W Tian"
      ],
      "year": "2023",
      "venue": "International Conference on Computer and Communications"
    },
    {
      "citation_id": "24",
      "title": "Foster: Feature boosting and compression for class-incremental learning",
      "authors": [
        "F.-Y Wang",
        "D.-W Zhou",
        "H.-J Ye",
        "D.-C Zhan"
      ],
      "year": "2022",
      "venue": "Foster: Feature boosting and compression for class-incremental learning"
    },
    {
      "citation_id": "25",
      "title": "A developmental study of children's and adults' descriptions of facial expressions of emotion",
      "authors": [
        "S Widen",
        "J Russell"
      ],
      "year": "2008",
      "venue": "Journal of Genetic Psychology"
    },
    {
      "citation_id": "26",
      "title": "Facial expression of pain: An evolutionary account",
      "authors": [
        "A Williams"
      ],
      "year": "2003",
      "venue": "Behavioral and Brain Sciences"
    },
    {
      "citation_id": "27",
      "title": "A transfer learning approach to compound facial expression recognition",
      "authors": [
        "Y Xie",
        "W Tian",
        "T Ma"
      ],
      "year": "2021",
      "venue": "International Conference on Advances in Image Processing"
    },
    {
      "citation_id": "28",
      "title": "Continual learning with bayesian model based on a fixed pre-trained feature extractor",
      "authors": [
        "Y Yang",
        "Z Cui",
        "J Xu"
      ],
      "year": "2023",
      "venue": "Visual Intelligence"
    },
    {
      "citation_id": "29",
      "title": "Auformer: Vision transformers are parameter-efficient facial action unit detectors",
      "authors": [
        "K Yuan",
        "Z Yu",
        "X Liu",
        "W Xie",
        "H Yue",
        "J Yang"
      ],
      "year": "2024",
      "venue": "Auformer: Vision transformers are parameter-efficient facial action unit detectors"
    },
    {
      "citation_id": "30",
      "title": "A comprehensive survey on automatic facial action unit analysis",
      "authors": [
        "R Zhi",
        "M Liu",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "A comprehensive survey on automatic facial action unit analysis"
    },
    {
      "citation_id": "31",
      "title": "Class-incremental learning: A survey",
      "authors": [
        "D.-W Zhou",
        "Q.-W Wang",
        "Z.-H Qi",
        "H.-J Ye",
        "D.-C Zhan",
        "Z Liu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "32",
      "title": "Selfsustaining representation expansion for non-exemplar class-incremental learning",
      "authors": [
        "K Zhu",
        "W Zhai",
        "Y Cao",
        "J Luo",
        "Z Zha"
      ],
      "year": "2022",
      "venue": "International Conference on Computer Vision and Pattern Recognition"
    }
  ]
}