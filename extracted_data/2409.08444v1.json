{
  "paper_id": "2409.08444v1",
  "title": "Towards Unified Facial Action Unit Recognition Framework By Large Language Models",
  "published": "2024-09-13T00:26:09Z",
  "authors": [
    "Guohong Hu",
    "Xing Lan",
    "Hanyu Jiang",
    "Jiayi Lyu",
    "Jian Xue"
  ],
  "keywords": [
    "AU recognition",
    "Large Language Model",
    "LoRA"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Action Units (AUs) are of great significance in the realm of affective computing. In this paper, we propose AU-LLaVA, the first unified AU recognition framework based on the Large Language Model (LLM). AU-LLaVA consists of a visual encoder, a linear projector layer, and a pre-trained LLM. We meticulously craft the text descriptions and fine-tune the model on various AU datasets, allowing it to generate different formats of AU recognition results for the same input image. On the BP4D and DISFA datasets, AU-LLaVA delivers the most accurate recognition results for nearly half of the AUs. Our model achieves improvements of F1-score up to 11.4% in specific AU recognition compared to previous benchmark results. On the FEAFA dataset, our method achieves significant improvements over all 24 AUs compared to previous benchmark results. AU-LLaVA demonstrates exceptional performance and versatility in AU recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial expressions convey crucial nonverbal information. They reveal emotions, intentions, and mental states  [1] , playing a vital role in human communication and social interaction  [2] . To systematically study facial expressions, Ekman and Friesen developed the Facial Action Coding System (FACS)  [3]  in 1978. Expression categories and facial Action Units (AUs) are closely related  [4] -  [6] . A reliable AU recognition system is essential for accurate AU detection and intensity estimation. Most of the existing methods rely on Convolutional Neural Network  [7] , Graph Neural Network  [8] , or Transformer  [9] . Those methods learn AU information encoded in the training set into the backbone, which limits the generalization capability to recognize AU across different datasets. Moreover, they rely solely on visual information and focus exclusively on either detecting the presence of AUs or estimating AU's intensity. Meanwhile, Large Language Models (LLMs) have demonstrated remarkable capabilities in language tasks and reasoning  [10] -  [13] . Inspired by recent successes in applying LLMs to visual tasks  [14] -  [17] , we propose AU-LLaVA, a novel approach that leverages LLaVA  [18]  for unified AU recognition. The proposed method aims to combine the strengths of LLMs with the precision required for facial expression analysis, potentially advancing the field of automated facial action unit recognition. † Corresponding Author Under Review. The AU-LLaVA, fine-tuned on diverse AU datasets, demonstrates versatility in generating various AU recognition results for a single input image. AU-LLaVA's query-based approach allows for flexible output formats, as illustrated in Fig.  1 . This architecture leverages the strengths of language models in visual tasks, potentially enhancing the accuracy and adaptability of AU recognition systems. The primary contributions of this study are as follows:\n\n• This paper presents AU-LLaVA, the first known unified AU recognition framework based on LLMs. This novel approach integrates visual encoding with the reasoning capabilities of LLMs for facial expression analysis. classifier is trained on the extracted features from images. For example, Baltrusaitis et al.  [19]  presented a facial AU intensity estimation and occurrence detection system based on histograms of oriented gradients and geometry features, including landmark locations, which were used to train a Support Vector Machine classifier. DRML  [20]  addresses region-specific and multi-label learning jointly, while EAC-Net  [21]  employs a single attention map per image, combining all regions associated with action units. Yanan Chang et al.  [22]  proposed a novel knowledge-driven self-supervised representation learning framework for AU recognition. AU labeling rules are summarized and leveraged to guide the framework's design. However, few works leverage the powerful reasoning and generalization capabilities of large language models.\n\nThey rely entirely on the model's internal architecture for encoding and reasoning AUs. AUs are learned purely based on visual information.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "B. Large Language Model On Vison Tasks",
      "text": "Researchers are integrating LLMs into visual domains to enhance model performance by leveraging additional modalities and contextual information. Instruction tuning  [23]  is widely used to align vision and language modalities, enhancing the capabilities of multimodal LLMs (MLLMs). In this vein, MiniGPT-4  [24]  aligns a visual encoder with an advanced LLM, achieving detailed image description generation and other multi-modal capabilities. Similarly, LLaVA  [18]  leverages GPT-4 generated instruction-following data to create a large multimodal model, demonstrating impressive visual-language understanding and chat abilities. Although MLLMs have made great advances, most approaches still concentrate on vision-language tasks like Visual Question-Answering (VQA), leaving LLMs' potential in classical vision tasks largely untapped. In the work related to facial expressions based on large models, EmoLA  [25]  addresses the challenges in facial affective behavior analysis by introducing a comprehensive instruction-following dataset and benchmark. It enhances MLLM's performance by incorporating a facial prior expert module with face structure knowledge and employing a low-rank adaptation module for efficient fine-tuning. EMO-LLaMA  [26]  incorporates facial priors through a Face Info Mining module and leverages handcrafted prompts with age-gender-race attributes, enhancing the model's ability to interpret facial expressions across diverse human groups.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. The Proposed Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Overview",
      "text": "This paper leverages the powerful capabilities of a LLM for reasoning, employing the text description and AU datasets with various formats of AU labels to fine-tune LLM. We define LLM-based AU recognition as a VQA task and propose AU-LLaVA. As shown in (1), given an input facial image I and the text description T, AU-LLaVA aims to output an array A where each element corresponds to a type of AU. where i represents the index of three types of AU recognition tasks with different output formats:\n\n1) Integers in {0, 1} to indicate the AU's presence or absence. 2) Integers in [0, 5] to represent the level of an AU.\n\n3) Floating-point numbers in [0, 1] to represent the intensity of an AU. AU-LLaVA adopts a similar architecture to LLaVA  [18] . As shown in Fig.  2 , AU-LLaVA consists three main components: a visual encoder Φ V , a linear projector Φ P and a large language model Φ L . Φ V takes facial image I as input and outputs a series of image features, which are then passed to Φ P to generate a sequence of image tokens. The text description T is transformed into a sequence of text tokens through a tokenizer. Subsequently, the text tokens and image tokens are combined and fed into the Φ L . This allows process Eq. 1 to be further specified as Eq. 2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Text Description",
      "text": "It is shown that the textual instruction T plays a crucial role in manifesting the capabilities of LLMs  [27] . When designing T, similar to previous VQA tasks, we assign tailored questions to each task. Additionally, we meticulously construct descriptions comprising three essential elements: Part 1 is the purpose of the current AU recognition task, which aims to inform the model of what specific task it needs to perform. For instance, when the model is instructed in an AU detection task, Part 1 of T directs the model's focus solely on the presence or absence of AUs without considering the intensity of each AU, providing a clear goal-oriented orientation and improving efficiency. Part 2 introduces the AUs, specifying that each AU represents a specific facial muscle action. Such an introduction helps the model establish a fundamental understanding and cognitive framework for AUs. Additionally, it indicates which facial regions the LLM should focus on when recognizing a specific AU. Part 3 is the description of the expected output format from the model. For example, in an AU detection task, the model is expected to output either 0 or 1 for each AU, indicating whether the AU is present or not. It clarifies the model's output requirements, ensuring that the results align with the task's objectives. A sample of a detailed description of textual description T can be found in Fig.  2 . After being processed by a tokenizer, the entire textual description T is converted into text tokens, which are then fed into a LLM Φ L for further processing and analyzing.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Lora Modules",
      "text": "To uncover and enhance the performance of LLM on specific recognition tasks, it is necessary to fine-tune the pretrained LLM using AU datasets. However, given the immense number of parameters in LLM, it is impractical to train the entire model's parameters from scratch with limited GPU and a finite dataset size. Inspired by the work of Hu et al.  [28] , the LoRA technique is employed to the training of AU-LLaVA. LoRA is a parameter-efficient fine-tuning technique that specializes pre-trained models by introducing trainable low-rank matrices at each layer, significantly reducing the number of trainable parameters. The entire fine-tuning process in LoRA can be mathematically expressed as follows:\n\nW 0 represents a pre-trained weight matrix, whereas W denotes the fine-tuned weight matrix. U ∈ R d×r and V ∈ R r×k signify two low-rank matrices that satisfy the condition r ≪ k.\n\nDuring the training procedure, freeze W 0 , and only U and V are updated. Consequently, only the the parameters within U and V need to be trained, significantly reducing the number of trainable parameters compared to fine-tuning the entire model. Specifically, with regard to our AU-LLaVA, we employ LoRA modules into both visual encoder Φ V and LLM Φ L , and finetune them with the linear projector.\n\nIV. EXPERIMENTS A. Datasets BP4D  [29]  dataset is widely used in AU detection. It contains 41 subjects with 23 females and 18 males, each of which is involved in 8 different tasks. It includes approximately 140,000 face images with binary AU labels (1 for presence or 0 for absence). DISFA  [30]  dataset contains 27 subjects with 12 females and 15 males. It includes approximately 138,000 face images annotated with AU intensities (discrete level 0 to 5). FEAFA  [31] ,  [32]  dataset features 122 participants in naturalistic settings. In addition, 99,356 frames are manually labeled using continuous AU intensities (in the continuous range [0, 1]).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Implementation Details",
      "text": "Face detection and alignment were performed using SCRFD  [33]  on each dataset. Before feeding the facial images into AU-LLaVA, they were cropped to a size of 224×224. Adhering to the protocol established in previous works  [34] ,  [35] , subjectindependent 3-fold cross-validation was conducted on BP4D and DISFA. The average results across these 3 folds were then reported. For FEAFA dataset, we randomly split it into training and validation subsets with a ratio of 4:1. We utilize ViT-L/14 as the visual encoder Φ V , which is pre-trained with DINOv2  [36]  weights. We employ an instruction-tuned Vicuna7B  [37]  as the pre-trained large language model Φ L . The projector Φ P consists of a single fully connected layer.\n\nDuring training, data augmentation techniques were applied. The AdamW optimizer was utilized with a batch size of 1 per GPU, and gradient accumulation was performed over 16 steps. The learning rate was initially set to 5e-4 and followed a cosine decay schedule, with a weight decay of 0.05 and a warm-up ratio of 0.03. In the first stage, the model was trained for 5 epochs, with LoRA fine-tuning applied to each self-attention layer of the visual encoder and the LLM, using a hidden dimension r of 8. AU-LLaVA was then trained for approximately 4 days on each of the three datasets using four V100 GPUs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Ablation Study",
      "text": "Ablation experiments were conducted on the BP4D dataset, following the experimental setup outlined in Section IV-B. The fine-tuning procedure was modified to fine-tune exclusively on either the LLM Φ L or the visual encoder Φ V . The results are shown in Table  I , where Φ L & Φ V represents the full finetuning strategy of AU-LLaVA. Four AUs were selected for comparison, as they correspond to the eye and mouth regions of the face, respectively. The ablation study confirms that the designed training strategies lead to enhanced performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Comparison With State-Of-The-Art Methods",
      "text": "We compared our method with current AU recognition work. The results for other methods are sourced directly from their respective publications. The F1-score is calculated for 12 AUs in BP4D and 8 AUs in DISFA across the three cross-validation folds. The proposed method is compared with existing AU recognition approaches based on F1-score.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "AU1 AU3 AU5 AU7 AU9 AU11 AU13 AU15 AU17 AU19 AU21 AU23 Avg. DRML  [20]  . Notably, when training on the DISFA dataset, AU-LLaVA is expected to output levels of AUs (from 0 to 5), thus we used the original dataset labels for training. During validation, levels 2 and below were treated as 0, and levels above 2 as 1, which led to a slight decrease in performance compared to the BP4D dataset. Additionally, the previous methods compared with AU-LLaVA rely solely on visual information, whereas AU-LLaVA integrates both visual and textual modalities and captures correlations through cross-modality attention. Moreover, we do not provide a predefined relationship between AUs, even though the occurrence of one AU is often linked to others. If these relationships were explicitly defined before training, as done in SRERL  [40] , further improvements in performance could be anticipated.\n\nFor the FEAFA dataset, since the AU labels are continuous intensity values between 0 and 1, the Mean Absolute Error (MAE) is computed for the 24 AUs. Table  IV  presents the comparison for 13 AUs along with the average performance.\n\nIn fact, AU-LLaVA achieved significant improvements over the previous best results across all 24 AUs. Note that both JAA  [41]  and DRML  [20]  are proposed for AU detection rather than AU intensity estimation. When adapting them for intensity estimation, their performance is not as strong on the BP4D and DISFA datasets. The results demonstrate the strong generalization ability of our model. When adapting AU-LLaVA to different tasks, the only change required is the text description.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "This paper introduces AU-LLaVA, a pioneering unified framework for facial Action Unit (AU) recognition leveraging large language models (LLMs). By harnessing the advanced reasoning capabilities of LLMs, AU-LLaVA effectively recognizes AUs across diverse tasks. This approach not only demonstrates the potential of integrating textual and visual modalities but also sets a new standard for facial AU recognition. Potential future research directions include investigating the impact of integrating AU relationships into the text descriptions, as well as applying AU-LLaVA to other visual tasks such as recognizing facial micro-expressions and detecting facial landmarks.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Versatility of AU-LLaVA results. For a single input image, the model",
      "page": 1
    },
    {
      "caption": "Figure 2: Architectural framework of AU-LLaVA, which comprises three",
      "page": 2
    },
    {
      "caption": "Figure 2: , AU-LLaVA consists three main components: a",
      "page": 2
    },
    {
      "caption": "Figure 2: After being",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "{huguogong22,lanxing19,jianghanyu231,lyujiayi21}@mails.ucas.ac.cn; xuejian@ucas.ac.cn"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Abstract—Facial Action Units\n(AUs) are of great\nsignificance"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "[0,1,1,…]\nBinary\nin the\nrealm of affective\ncomputing.\nIn this paper, we propose"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Question type 1"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Face \nAU-LLaVA,\nthe first unified AU recognition framework based"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "[0,3,4,…]\nLevel\nAU-LLaVA"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Question type 2\nImage"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "on the Large Language Model\n(LLM). AU-LLaVA consists of a\n+"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "[0.1,0.6,0.7,…]\nIntensity\nvisual encoder, a linear projector layer, and a pre-trained LLM."
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Question type 3"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "We meticulously\ncraft\nthe\ntext descriptions\nand fine-tune\nthe"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "model on various AU datasets, allowing it\nto generate different"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "formats of AU recognition results for the same input\nimage. On\nFig. 1. Versatility of AU-LLaVA results. For a single input\nimage,\nthe model"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "the BP4D and DISFA datasets, AU-LLaVA delivers\nthe most\ndemonstrates multi-modal capabilities:\n(a) binary AU detection (0 or 1),\n(b)"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "discrete AU intensity levels\n(0-5),\n(c) continuous AU intensity values\n(0-1).\naccurate recognition results for nearly half of the AUs. Our model"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "AU-LLaVA is a unified AU recognition framework based on LLM.\nachieves\nimprovements of F1-score up to 11.4% in specific AU"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "recognition compared to previous benchmark results. On the"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "FEAFA dataset, our method achieves\nsignificant\nimprovements"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "over all 24 AUs compared to previous benchmark results. AU-\nAU-LLaVA integrates a visual encoder, a linear projector"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "LLaVA demonstrates exceptional performance and versatility in"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "layer,\nand a pre-trained LLaVA model. The\nsystem utilizes"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "AU recognition."
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "meticulously crafted text descriptions,\nincorporating task ob-"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Index Terms—AU recognition, Large Language Model, LoRA."
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "jectives, AU locations\nand definitions,\nand expected output"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "formats. Such text provides the model with a clear problem-"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "oriented focus. When recognizing a specific AU,\nthe model"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "I.\nINTRODUCTION"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "relies\non\nthe\nlocation\ndescription\ncorresponding\nto which,"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "allowing it\nto concentrate on a\nspecific\nregion of\nthe\nfacial"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Facial\nexpressions\nconvey\ncrucial\nnonverbal\ninformation."
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "image, thereby enhancing the model’s performance. Moreover,"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "They reveal emotions, intentions, and mental states [1], playing"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "the cues are provided explicitly for\nthe large language model"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "a vital\nrole\nin human communication and social\ninteraction"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "and remain constant during the training and validation."
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "[2]. To systematically study facial\nexpressions, Ekman and"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "The AU-LLaVA, fine-tuned on diverse AU datasets, demon-"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Friesen developed the Facial Action Coding System (FACS)"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "strates versatility in generating various AU recognition results"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "[3]\nin 1978. Expression categories\nand facial Action Units"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "for a single input\nimage. AU-LLaVA’s query-based approach"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "(AUs) are closely related [4]–[6]. A reliable AU recognition"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "allows for flexible output formats, as illustrated in Fig. 1. This"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "system is\nessential\nfor\naccurate AU detection and intensity"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "architecture leverages the strengths of\nlanguage models in vi-"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "estimation. Most\nof\nthe\nexisting methods\nrely\non Convo-"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "sual\ntasks, potentially enhancing the accuracy and adaptability"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "lutional Neural Network\n[7], Graph Neural Network\n[8],"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "of AU recognition systems. The primary contributions of\nthis"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "or Transformer\n[9]. Those methods\nlearn AU information"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "study are as follows:"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "encoded in the\ntraining set\ninto the backbone, which limits"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "• This paper presents AU-LLaVA,\nthe first known unified"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "the generalization capability to recognize AU across different"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "AU recognition framework based on LLMs. This novel"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "datasets. Moreover,\nthey rely solely on visual\ninformation and"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "approach integrates visual\nencoding with the\nreasoning"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "focus\nexclusively\non\neither\ndetecting\nthe\npresence\nof AUs"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "capabilities of LLMs for\nfacial expression analysis."
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "or\nestimating AU’s\nintensity. Meanwhile, Large Language"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "• AU-LLaVA demonstrates exceptional efficacy on differ-"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Models (LLMs) have demonstrated remarkable capabilities in"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "ent\ntasks, achieving superior F1-score for approximately"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "language\ntasks\nand reasoning [10]–[13].\nInspired by recent"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "half of\nthe AUs\nin BP4D and DISFA datasets. For\nthe"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "successes\nin\napplying LLMs\nto\nvisual\ntasks\n[14]–[17], we"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "FEAFA dataset, AU-LLaVA achieves significant improve-"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "propose AU-LLaVA, a novel approach that\nleverages LLaVA"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "ments over all 24 AUs compared to previous benchmarks."
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "[18]\nfor unified AU recognition. The proposed method aims"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "to combine the strengths of LLMs with the precision required"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "II. RELATED WORK"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "for\nfacial expression analysis, potentially advancing the field"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "A. AU Recognition"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "of automated facial action unit\nrecognition."
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Facial Action Unit\n(AU)\nrecognition has been extensively"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "researched\nover\nthe\ndecades,\nleading\nto\nthe\ndevelopment"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "†Corresponding Author"
        },
        {
          "School of Engineering Science, University of Chinese Academy of Sciences, Beijing, China\n商 业和 技术 评 估产 品": "Under Review.\nof\nvarious\ninfluential methods.\nIn\nprevious work,\nthe AU"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Output": ""
        },
        {
          "Output": "AUs:[1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 1 , 0 , 0]"
        },
        {
          "Output": ""
        },
        {
          "Output": ""
        },
        {
          "Output": "Fig.\n2.\nArchitectural\nframework\nof AU-LLaVA, which"
        },
        {
          "Output": ""
        },
        {
          "Output": "primary components:\na visual\nencode,\na\nlinear projector,"
        },
        {
          "Output": "LLM. AU-LLaVA processes facial"
        },
        {
          "Output": "generating\nan\narray where\neach\nelement\ncorresponds\nto\na"
        },
        {
          "Output": ""
        },
        {
          "Output": ""
        },
        {
          "Output": ""
        },
        {
          "Output": "integrated into both the visual encoder and the LLM to enhance efficiency."
        },
        {
          "Output": ""
        },
        {
          "Output": ""
        },
        {
          "Output": ""
        },
        {
          "Output": "tasks with different output\nformats:"
        },
        {
          "Output": ""
        },
        {
          "Output": "{0, 1}\n1)\nIntegers\nin\nto\nindicate\nthe AU’s"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "advanced LLM, achieving detailed image description gener-": "ation\nand\nother multi-modal\ncapabilities. Similarly, LLaVA"
        },
        {
          "advanced LLM, achieving detailed image description gener-": "[18]\nleverages GPT-4 generated instruction-following data to"
        },
        {
          "advanced LLM, achieving detailed image description gener-": "create\na\nlarge multimodal model,\ndemonstrating\nimpressive"
        },
        {
          "advanced LLM, achieving detailed image description gener-": "visual-language\nunderstanding\nand\nchat\nabilities. Although"
        },
        {
          "advanced LLM, achieving detailed image description gener-": ""
        },
        {
          "advanced LLM, achieving detailed image description gener-": "MLLMs\nhave made\ngreat\nadvances, most\napproaches\nstill"
        },
        {
          "advanced LLM, achieving detailed image description gener-": ""
        },
        {
          "advanced LLM, achieving detailed image description gener-": "concentrate\non\nvision-language\ntasks\nlike Visual Question-"
        },
        {
          "advanced LLM, achieving detailed image description gener-": ""
        },
        {
          "advanced LLM, achieving detailed image description gener-": "Answering (VQA),\nleaving LLMs’ potential\nin classical vi-"
        },
        {
          "advanced LLM, achieving detailed image description gener-": ""
        },
        {
          "advanced LLM, achieving detailed image description gener-": "sion\ntasks\nlargely\nuntapped.\nIn\nthe work\nrelated\nto\nfacial"
        },
        {
          "advanced LLM, achieving detailed image description gener-": ""
        },
        {
          "advanced LLM, achieving detailed image description gener-": "expressions based on large models, EmoLA [25] addresses the"
        },
        {
          "advanced LLM, achieving detailed image description gener-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Description"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "For\nexample, Baltrusaitis\net\nal.\n[19] presented a\nfacial AU",
          "Image\nText Description": "P"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "art l AUs detection aims to identify the"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "intensity\nestimation\nand\noccurrence\ndetection\nsystem based",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "presence of specific facial muscle"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "on histograms of oriented gradients\nand geometry features,",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "movements, which are encoded as AUs."
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "including\nlandmark\nlocations, which were\nused\nto\ntrain\na",
          "Image\nText Description": "Part 2 AU1:inner brow raiser,"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "AU2:outer brow raiser......"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "Support Vector Machine\nclassifier. DRML\n[20]\naddresses",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Part 3 0 or 1 for each AU, indicating"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "region-specific and multi-label learning jointly, while EAC-Net",
          "Image\nText Description": "Visual"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "the absence or presence of that muscle"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "ΦV"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "[21] employs a single attention map per image, combining all",
          "Image\nText Description": "Encoder\nmovement."
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Question"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "regions associated with action units. Yanan Chang et al.\n[22]",
          "Image\nText Description": "LoRA"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Which AUs are present in the image?"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "proposed a novel knowledge-driven self-supervised represen-",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "tation learning framework for AU recognition. AU labeling",
          "Image\nText Description": "Tokenizer"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Projector\nΦP"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "rules are summarized and leveraged to guide the framework’s",
          "Image\nText Description": "···\n ···"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "design. However,\nfew works leverage the powerful\nreasoning",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Tokens"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "and generalization capabilities of\nlarge language models.",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "They rely entirely on the model’s\ninternal architecture for",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Large Language Model\nΦL\nLoRA"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "encoding and reasoning AUs. AUs are learned purely based",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "on visual\ninformation.",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Output"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "B. Large Language Model on Vison Tasks",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "AUs:[1 , 0 , 1 , 1 , 1 , 1 , 0 , 1 , 0 , 1 , 0 , 0]"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "Researchers\nare\nintegrating LLMs\ninto visual domains\nto",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "enhance model performance by leveraging additional modal-",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Fig.\n2.\nArchitectural\nframework\nof AU-LLaVA, which\ncomprises\nthree"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "ities\nand\ncontextual\ninformation.\nInstruction\ntuning\n[23]\nis",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "primary components:\na visual\nencode,\na\nlinear projector,\nand a pretrained"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "widely\nused\nto\nalign\nvision\nand\nlanguage modalities,\nen-",
          "Image\nText Description": "LLM. AU-LLaVA processes facial\nimages and textual descriptions as inputs,"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "generating\nan\narray where\neach\nelement\ncorresponds\nto\na\nspecific Action"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "hancing the\ncapabilities of multimodal LLMs\n(MLLMs).\nIn",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Unit. During the training phase, Low-Rank Adaptation (LoRA) modules are"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "this vein, MiniGPT-4 [24]\naligns\na visual\nencoder with an",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "integrated into both the visual encoder and the LLM to enhance efficiency."
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "advanced LLM, achieving detailed image description gener-",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "ation\nand\nother multi-modal\ncapabilities. Similarly, LLaVA",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "[18]\nleverages GPT-4 generated instruction-following data to",
          "Image\nText Description": "where i represents the index of three types of AU recognition"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "create\na\nlarge multimodal model,\ndemonstrating\nimpressive",
          "Image\nText Description": "tasks with different output\nformats:"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "visual-language\nunderstanding\nand\nchat\nabilities. Although",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "{0, 1}\n1)\nIntegers\nin\nto\nindicate\nthe AU’s\npresence\nor"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "MLLMs\nhave made\ngreat\nadvances, most\napproaches\nstill",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "absence."
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "concentrate\non\nvision-language\ntasks\nlike Visual Question-",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "2)\nIntegers in [0, 5]\nto represent\nthe level of an AU."
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "Answering (VQA),\nleaving LLMs’ potential\nin classical vi-",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "3)\nFloating-point numbers in [0, 1] to represent the intensity"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "sion\ntasks\nlargely\nuntapped.\nIn\nthe work\nrelated\nto\nfacial",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "of an AU."
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "expressions based on large models, EmoLA [25] addresses the",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "AU-LLaVA adopts a similar architecture to LLaVA [18]. As"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "challenges in facial affective behavior analysis by introducing",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "shown in Fig. 2, AU-LLaVA consists three main components: a"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "a comprehensive instruction-following dataset and benchmark.",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "visual encoder ΦV , a linear projector ΦP and a large language"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "It\nenhances MLLM’s performance by incorporating a\nfacial",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "takes\nfacial\nimage I as\ninput and outputs a\nmodel ΦL. ΦV"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "prior expert module with face structure knowledge and em-",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "series\nof\nimage\nfeatures, which\nare\nthen\npassed\nto\nto ΦP"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "ploying a low-rank adaptation module for efficient fine-tuning.",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "generate a sequence of image tokens. The text description T is"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "EMO-LLaMA [26]\nincorporates\nfacial priors\nthrough a Face",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "transformed into a sequence of text tokens through a tokenizer."
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "Info Mining module and leverages handcrafted prompts with",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "Subsequently,\nthe text\ntokens and image tokens are combined"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "age-gender-race\nattributes,\nenhancing\nthe model’s\nability to",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "and fed into the ΦL. This allows process Eq. 1 to be further"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "interpret\nfacial expressions across diverse human groups.",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "specified as Eq. 2."
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "III. THE PROPOSED METHOD",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "(2)\nAi = ΦL\n(cid:0)ΦP (ΦV (I)) , Ti(cid:1) ."
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "A. Overview",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "B. Text Description"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "This paper\nleverages\nthe powerful\ncapabilities of\na LLM",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "It\nis\nshown that\nthe\ntextual\ninstruction T plays\na\ncrucial"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "for reasoning, employing the text description and AU datasets",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "role\nin manifesting\nthe\ncapabilities\nof LLMs\n[27]. When"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "with various formats of AU labels to fine-tune LLM. We define",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "designing T, similar to previous VQA tasks, we assign tailored"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "LLM-based AU recognition as a VQA task and propose AU-",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "questions to each task. Additionally, we meticulously construct"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "LLaVA. As\nshown in (1), given an input\nfacial\nimage I and",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "descriptions comprising three essential elements: Part 1 is the"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "the text description T, AU-LLaVA aims to output an array A",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "purpose of\nthe\ncurrent AU recognition task, which aims\nto"
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "where each element corresponds to a type of AU.",
          "Image\nText Description": ""
        },
        {
          "classifier\nis\ntrained\non\nthe\nextracted\nfeatures\nfrom images.": "",
          "Image\nText Description": "inform the model of what specific task it needs to perform. For"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Part 1 of T directs the model’s focus solely on the presence": "or absence of AUs without considering the intensity of each",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "range [0, 1])."
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "AU, providing a clear goal-oriented orientation and improving",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "B.\nImplementation Details"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "efficiency. Part 2 introduces the AUs, specifying that each AU",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "represents a specific facial muscle action. Such an introduction",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "Face detection and alignment were performed using SCRFD"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "helps\nthe model\nestablish a\nfundamental understanding and",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "[33] on each dataset. Before feeding the facial images into AU-"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "cognitive framework for AUs. Additionally,\nit\nindicates which",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "LLaVA,\nthey were cropped to a size of 224×224. Adhering to"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "facial\nregions\nthe LLM should focus on when recognizing a",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "the protocol established in previous works [34], [35], subject-"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "specific AU. Part 3 is\nthe description of\nthe expected output",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "independent 3-fold cross-validation was conducted on BP4D"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "format from the model. For example,\nin an AU detection task,",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "and DISFA. The average results across these 3 folds were then"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "the model\nis expected to output either 0 or 1 for each AU,",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "reported. For FEAFA dataset, we randomly split it into training"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "indicating whether\nthe AU is present or not.\nIt\nclarifies\nthe",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "and validation subsets with a ratio of 4:1. We utilize ViT-L/14"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "model’s output\nrequirements,\nensuring that\nthe\nresults\nalign",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "as the visual encoder ΦV , which is pre-trained with DINOv2"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "with the task’s objectives. A sample of a detailed description",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "[36] weights. We employ an instruction-tuned Vicuna7B [37]"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "of\ntextual description T can be found in Fig. 2. After being",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "as the pre-trained large language model ΦL. The projector ΦP"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "processed by a tokenizer,\nthe entire textual description T is",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "consists of a single fully connected layer."
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "During training, data augmentation techniques were applied."
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "converted into text\ntokens, which are then fed into a LLM ΦL",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "for\nfurther processing and analyzing.",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "The AdamW optimizer was utilized with a batch size of 1"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "per GPU, and gradient accumulation was performed over 16"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "C. LoRA Modules",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "steps. The learning rate was initially set\nto 5e-4 and followed"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "To\nuncover\nand\nenhance\nthe\nperformance\nof\nLLM on",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "a\ncosine decay schedule, with a weight decay of 0.05 and"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "specific recognition tasks,\nit\nis necessary to fine-tune the pre-",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "a warm-up ratio of 0.03.\nIn the first\nstage,\nthe model was"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "trained LLM using AU datasets. However, given the immense",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "trained for 5 epochs, with LoRA fine-tuning applied to each"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "number of parameters\nin LLM,\nit\nis\nimpractical\nto train the",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "self-attention layer of\nthe visual encoder and the LLM, using"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "entire model’s\nparameters\nfrom scratch with\nlimited GPU",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "a hidden dimension r of 8. AU-LLaVA was\nthen trained for"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "and a finite dataset\nsize.\nInspired by the work of Hu et\nal.",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "approximately 4 days on each of the three datasets using four"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "[28],\nthe LoRA technique is employed to the training of AU-",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "V100 GPUs."
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "LLaVA. LoRA is a parameter-efficient fine-tuning technique",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "C. Ablation Study"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "that\nspecializes pre-trained models by introducing trainable",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "low-rank matrices\nat\neach\nlayer,\nsignificantly\nreducing\nthe",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "Ablation experiments were conducted on the BP4D dataset,"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "number of trainable parameters. The entire fine-tuning process",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "following the experimental setup outlined in Section IV-B. The"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "in LoRA can be mathematically expressed as follows:",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "fine-tuning procedure was modified to fine-tune exclusively on"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "either the LLM ΦL or the visual encoder ΦV . The results are"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "(3)\nW = W0 + U × V",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "represents\nthe full fine-\nshown in Table I, where ΦL & ΦV"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "tuning strategy of AU-LLaVA. Four AUs were\nselected for"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "W0",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "notes the fine-tuned weight matrix. U ∈ Rd×r and V ∈ Rr×k",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "comparison, as they correspond to the eye and mouth regions"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "signify two low-rank matrices that satisfy the condition r ≪ k.",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "of\nthe face,\nrespectively. The ablation study confirms that\nthe"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "designed training strategies lead to enhanced performance."
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "During the training procedure, freeze W0, and only U and V",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "are updated. Consequently, only the the parameters within U",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "and V need to be trained, significantly reducing the number of",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "TABLE I"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "ABLATION STUDY ON FINE-TUNING EACH COMPONENT OF AU-LLAVA ON"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "trainable parameters compared to fine-tuning the entire model.",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "BP4D, USING THE F1-SCORE METRIC(IN %)."
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "Specifically, with regard to our AU-LLaVA, we employ LoRA",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "modules into both visual encoder ΦV\nand LLM ΦL, and fine-",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "FT Comp.\nAU2\nAU4\nAU12\nAU17\nAvg."
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "56.0\n57.1\n89.3\n61.8\n58.4\nΦV"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "tune them with the linear projector.",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "90.5\n56.7\n53.7\n52.1\n43.6\nΦL"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "58.2\n61.9\n90.5\n62.5\n60.3\nΦL & ΦV"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "IV. EXPERIMENTS",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "A. Datasets",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "D. Comparison with state-of-the-art Methods"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "BP4D [29] dataset\nis widely used in AU detection.\nIt con-",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": ""
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "tains 41 subjects with 23 females and 18 males, each of which",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "We\ncompared\nour method with\ncurrent AU recognition"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "is\ninvolved\nin\n8\ndifferent\ntasks.\nIt\nincludes\napproximately",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "work. The results for other methods are sourced directly from"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "140,000 face images with binary AU labels (1 for presence or",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "their\nrespective publications. The F1-score\nis\ncalculated for"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "0 for absence). DISFA [30] dataset contains 27 subjects with",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "12 AUs\nin BP4D and\n8 AUs\nin DISFA across\nthe\nthree"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "12 females and 15 males.\nIt\nincludes approximately 138,000",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "cross-validation folds. The proposed method is compared with"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "face\nimages\nannotated with AU intensities\n(discrete\nlevel 0",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "existing AU recognition approaches based on F1-score. Table"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "to 5). FEAFA [31],\n[32] dataset\nfeatures 122 participants\nin",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "II\nshows\nthe\nperformance\ncomparison\nof AU-LLaVA with"
        },
        {
          "Part 1 of T directs the model’s focus solely on the presence": "naturalistic settings.\nIn addition, 99,356 frames are manually",
          "labeled\nusing\ncontinuous AU intensities\n(in\nthe\ncontinuous": "other AU recognition methods on BP4D. Our method achieves"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "AU10"
        },
        {
          "TABLE II": "77.2"
        },
        {
          "TABLE II": "66.3"
        },
        {
          "TABLE II": "79.9"
        },
        {
          "TABLE II": "81.9"
        },
        {
          "TABLE II": "83.5"
        },
        {
          "TABLE II": "82.7"
        },
        {
          "TABLE II": "87.8"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "AU6"
        },
        {
          "TABLE III": "15.7"
        },
        {
          "TABLE III": "29.0"
        },
        {
          "TABLE III": "28.6"
        },
        {
          "TABLE III": "50.7"
        },
        {
          "TABLE III": "47.1"
        },
        {
          "TABLE III": "41.4"
        },
        {
          "TABLE III": "30.8"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "and AU12. Table III\nshows\nthe comparison on DISFA. Our",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "(MAE)\nis\ncomputed for\nthe 24 AUs. Table\nIV presents\nthe"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "method achieves the best accuracy in recognizing AU1, AU2,",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "comparison for 13 AUs along with the average performance."
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "and AU25. The F1-score is\nimproved for nearly half of\nthe",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "In fact, AU-LLaVA achieved significant\nimprovements over"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "AUs\nacross\nboth\ndatasets. Moreover, AU-LLaVA achieves",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "the previous best\nresults\nacross\nall 24 AUs. Note\nthat both"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "significant\nimprovements\nin the\nrecognition of\ncertain AUs.",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "JAA [41]\nand DRML [20]\nare\nproposed\nfor AU detection"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "For\ninstance, on the BP4D dataset, AU1, AU4,\nand AU10",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "rather\nthan AU intensity estimation. When adapting them for"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "exhibit\nan\naverage\nimprovement\nof\n4.13\npercentage\npoints.",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "intensity\nestimation,\ntheir\nperformance\nis\nnot\nas\nstrong\non"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "Meanwhile, On the DISFA dataset, AU-LLaVA significantly",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "the BP4D and DISFA datasets. The\nresults demonstrate\nthe"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "surpasses the previous best method in recognizing AU2, with",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "strong generalization ability of our model. When adapting AU-"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "an improvement of 11.4 percentage points.",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "LLaVA to different\ntasks,\nthe only change required is the text"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "description."
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "Notably, when training on the DISFA dataset, AU-LLaVA",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "is expected to output\nlevels of AUs\n(from 0 to 5),\nthus we",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "V. CONCLUSION"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "used the original dataset\nlabels for training. During validation,",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "levels 2 and below were treated as 0, and levels above 2 as 1,",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "This\npaper\nintroduces AU-LLaVA,\na\npioneering\nunified"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "which led to a slight decrease in performance compared to the",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "framework for facial Action Unit (AU) recognition leveraging"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "BP4D dataset. Additionally,\nthe previous methods compared",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "large language models\n(LLMs). By harnessing the advanced"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "with AU-LLaVA rely solely on visual\ninformation, whereas",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "reasoning capabilities of LLMs, AU-LLaVA effectively rec-"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "AU-LLaVA integrates both visual and textual modalities and",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "ognizes AUs\nacross\ndiverse\ntasks. This\napproach\nnot\nonly"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "captures correlations through cross-modality attention. More-",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "demonstrates\nthe\npotential\nof\nintegrating\ntextual\nand\nvisual"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "over, we\ndo\nnot\nprovide\na\npredefined\nrelationship\nbetween",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "modalities but also sets a new standard for facial AU recogni-"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "AUs, even though the occurrence of one AU is often linked",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "tion. Potential\nfuture research directions include investigating"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "to others.\nIf\nthese relationships were explicitly defined before",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "the impact of integrating AU relationships into the text descrip-"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "training,\nas\ndone\nin SRERL [40],\nfurther\nimprovements\nin",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "tions, as well as applying AU-LLaVA to other visual tasks such"
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "performance could be anticipated.",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": ""
        },
        {
          "the best accuracy in recognizing AU1, AU2, AU4, AU6, AU10,": "",
          "intensity values between 0 and 1,\nthe Mean Absolute Error": "as\nrecognizing facial micro-expressions\nand detecting facial"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "A. M. Dai,\nand Q. V. Le,\n“Finetuned language models\nare\nzero-shot"
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "learners,” arXiv preprint arXiv:2109.01652, 2021."
        },
        {
          "REFERENCES": "[1] C. Bisogni, A. Castiglione, S. Hossain, F. Narducci,\nand S. Umer,",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[24] D. Zhu,\nJ. Chen, X.\nShen, X. Li,\nand M. Elhoseiny,\n“Minigpt-4:"
        },
        {
          "REFERENCES": "“Impact of deep learning approaches on facial expression recognition",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Enhancing vision-language understanding with advanced large language"
        },
        {
          "REFERENCES": "in healthcare industries,” IEEE Transactions on Industrial\nInformatics,",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "models,” arXiv preprint arXiv:2304.10592, 2023."
        },
        {
          "REFERENCES": "vol. 18, no. 8, pp. 5619–5627, 2022.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[25] Y. Li, A. Dao, W. Bao, Z. Tan, T. Chen, H. Liu, and Y. Kong, “Facial"
        },
        {
          "REFERENCES": "[2] C. DARWIN, “The expression of\nthe emotions\nin man and animals,”",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "arXiv\npreprint\naffective\nbehavior\nanalysis with\ninstruction\ntuning,”"
        },
        {
          "REFERENCES": "The\nAmerican\nJournal\nof\nthe Medical\nSciences,\np.\n477.\n[Online].",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "arXiv:2404.05052, 2024."
        },
        {
          "REFERENCES": "Available: http://dx.doi.org/10.1097/00000441-195610000-00024",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[26] B. Xing, Z. Yu, X. Liu, K. Yuan, Q. Ye, W. Xie, H. Yue, J. Yang, and"
        },
        {
          "REFERENCES": "[3]\nP. Ekman and W. V. Friesen,\n“Facial\naction coding system (facs):\na",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "H. K¨alvi¨ainen,\n“Emo-llama: Enhancing\nfacial\nemotion\nunderstanding"
        },
        {
          "REFERENCES": "technique for the measurement of facial actions,” Rivista Di Psichiatria,",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "with instruction tuning,” arXiv preprint arXiv:2408.11424, 2024."
        },
        {
          "REFERENCES": "vol. 47, no. 2, pp. 126–38, 1978.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[27]\nS. Xuan, Q. Guo, M. Yang, and S. Zhang, “Pink: Unveiling the power"
        },
        {
          "REFERENCES": "[4]\nS. Du, Y. Tao, and A. M. Martinez, “Compound facial expressions of",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "of\nreferential comprehension for multi-modal\nllms,” Oct 2023."
        },
        {
          "REFERENCES": "emotion.” Proceedings of\nthe National Academy of Sciences, Apr 2014.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[28] H. J., Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, and W. Chen,"
        },
        {
          "REFERENCES": "[Online]. Available: http://dx.doi.org/10.1073/pnas.1322355111",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "“Lora: Low-rank adaptation of\nlarge language models.” arXiv: Compu-"
        },
        {
          "REFERENCES": "[5] W. Friesen and P. Ekman, “Emfacs-7: Emotional\nfacial action coding",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "tation and Language,arXiv: Computation and Language, Jun 2021."
        },
        {
          "REFERENCES": "system,” Jan 1983.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[29] X. Zhang, L. Yin,\nJ. F. Cohn, S. Canavan, M. Reale, A. Horowitz,"
        },
        {
          "REFERENCES": "[6] K. M.\nPrkachin,\n“The\nconsistency\nof\nfacial\nexpressions\nof\npain:\na",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "P.\nLiu,\nand\nJ. M.\nGirard,\n“Bp4d-spontaneous:\na\nhigh-resolution"
        },
        {
          "REFERENCES": "comparison\nacross modalities,”\nPain,\nvol.\n51,\nno.\n3,\np.\n297–306,",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Image\nand\nspontaneous\n3d\ndynamic\nfacial\nexpression\ndatabase,”"
        },
        {
          "REFERENCES": "Dec 1992. [Online]. Available: http://dx.doi.org/10.1016/0304-3959(92)",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Vision\nComputing,\np.\n692–706,\nOct\n2014.\n[Online].\nAvailable:"
        },
        {
          "REFERENCES": "90213-u",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "http://dx.doi.org/10.1016/j.imavis.2014.06.002"
        },
        {
          "REFERENCES": "[7] W. Li,\nF. Abitahi,\nand Z. Zhu,\n“Action\nunit\ndetection with\nregion",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[30]\nS. M. Mavadati, M. H. Mahoor, K. Bartlett,\nP.\nTrinh,\nand\nJ.\nF."
        },
        {
          "REFERENCES": "adaptation, multi-labeling learning and optimal temporal fusing,” Cornell",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "IEEE\nCohn,\n“Disfa: A spontaneous\nfacial\naction intensity database,”"
        },
        {
          "REFERENCES": "University - arXiv,Cornell University - arXiv, Apr 2017.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Transactions on Affective Computing, p. 151–160, Apr 2013.\n[Online]."
        },
        {
          "REFERENCES": "[8]\nZ.-M. Chen, X.-S. Wei,\nP. Wang,\nand Y. Guo,\n“Multi-label\nimage",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Available: http://dx.doi.org/10.1109/t-affc.2013.4"
        },
        {
          "REFERENCES": "2019\nIEEE/CVF\nrecognition with\ngraph\nconvolutional\nnetworks,”\nin",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[31] Y. Yan, K. Lu,\nJ. Xue, P. Gao, and J. Lyu, “Feafa: A well-annotated"
        },
        {
          "REFERENCES": "Conference on Computer Vision and Pattern Recognition (CVPR), Jun",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "dataset\nfor\nfacial expression analysis and 3d facial animation,” Cornell"
        },
        {
          "REFERENCES": "2019.\n[Online]. Available: http://dx.doi.org/10.1109/cvpr.2019.00532",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "University - arXiv,Cornell University - arXiv, Apr 2019."
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[32] W. Gan,\nJ. Xue, K. Lu, Y. Yan, P. Gao,\nand\nJ. Lyu,\n“Feafa+:\nan"
        },
        {
          "REFERENCES": "[9] K. Yuan, Z. Yu, X. Liu, W. Xie, H. Yue, and J. Yang, “Auformer: Vision",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "extended well-annotated dataset\nfor\nfacial\nexpression analysis\nand 3d"
        },
        {
          "REFERENCES": "transformers are parameter-efficient\nfacial action unit detectors,” arXiv",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "International Conference\non Digital\nfacial\nanimation,”\nin Fourteenth"
        },
        {
          "REFERENCES": "preprint arXiv:2403.04697, 2024.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Image Processing (ICDIP 2022), vol. 12342.\nSPIE, 2022, pp. 307–"
        },
        {
          "REFERENCES": "[10]\nL. Ouyang, J. Wu, X. Jiang et al., “Training language models to follow",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "316."
        },
        {
          "REFERENCES": "instructions with human feedback.”",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[33]\nJ. Guo, J. Deng, A. Lattas, and S. Zafeiriou, “Sample and computation"
        },
        {
          "REFERENCES": "[11]\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya et al., “Training",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "redistribution\nfor\nefficient\nface\ndetection,”\n2021.\n[Online]. Available:"
        },
        {
          "REFERENCES": "compute-optimal\nlarge language models.”",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "https://arxiv.org/abs/2105.04714"
        },
        {
          "REFERENCES": "[12]\nT. Brown, B. Mann, N. Ryder, M. Subbiah,\nJ. Kaplan, P. Dhariwal,",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[34] W. Li,\nF. Abitahi,\nand Z. Zhu,\n“Action\nunit\ndetection with\nregion"
        },
        {
          "REFERENCES": "et al.,\n“Language models\nare\nfew-shot\nlearners,” arXiv: Computation",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "adaptation, multi-labeling learning and optimal\ntemporal fusing,” IEEE,"
        },
        {
          "REFERENCES": "and Language,arXiv: Computation and Language, May 2020.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "2017."
        },
        {
          "REFERENCES": "[13]\nJ. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[35] X. Niu, H. Han, S. Yang, Y. Huang, and S. Shan, “Local\nrelationship"
        },
        {
          "REFERENCES": "image\npre-training with\nfrozen\nimage\nencoders\nand\nlarge\nlanguage",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "learning with person-specific shape regularization for\nfacial action unit"
        },
        {
          "REFERENCES": "conference on machine\nmodels,”\nin International\nlearning.\nPMLR,",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "detection,” IEEE, 2019."
        },
        {
          "REFERENCES": "2023, pp. 19 730–19 742.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[36] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov,"
        },
        {
          "REFERENCES": "[14]\nZ. Zhao, Y. Cao, S. Gong,\nand\nI. Patras,\n“Enhancing\nzero-shot\nfa-",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "et\nP.\nFernandez, D. Haziza,\nF. Massa, A. El-Nouby\nal.,\n“Dinov2:"
        },
        {
          "REFERENCES": "cial expression recognition by llm knowledge transfer,” arXiv preprint",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "arXiv\npreprint\nLearning\nrobust\nvisual\nfeatures without\nsupervision,”"
        },
        {
          "REFERENCES": "arXiv:2405.19100, 2024.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "arXiv:2304.07193, 2023."
        },
        {
          "REFERENCES": "[15] C. O. Kumar, N. Gowtham, M. Zakariah, and A. Almazyad, “Multimodal",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[37] W.-L. Chiang, Z. Li, Z. Lin et al.,\n“Vicuna: An open-source\nchatbot"
        },
        {
          "REFERENCES": "emotion recognition using feature fusion: An llm-based approach,” IEEE",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "impressing gpt-4 with 90%* chatgpt quality,” March 2023.\n[Online]."
        },
        {
          "REFERENCES": "Access, 2024.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Available: https://lmsys.org/blog/2023-03-30-vicuna/"
        },
        {
          "REFERENCES": "[16] X. Lan,\nJ. Xue,\nJ. Qi, D.\nJiang, K. Lu,\nand T.-S. Chua,\n“Expllm:",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[38]\nF.-E. FanRong-En, C.-W. ChangKai-Wei, H.-J. HsiehCho-Jui, W.-R."
        },
        {
          "REFERENCES": "arXiv\nTowards\nchain\nof\nthought\nfor\nfacial\nexpression\nrecognition,”",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "WangXiang-Rui, and L.-J. LinChih-Jen, “Liblinear: A library for\nlarge"
        },
        {
          "REFERENCES": "preprint arXiv:2409.02828, 2024.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "linear classification,” Journal of Machine Learning Research,Journal of"
        },
        {
          "REFERENCES": "[17] D. Wang, S. Xuan, and S. Zhang, “Locllm: Exploiting generalizable hu-",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Machine Learning Research, Jun 2008."
        },
        {
          "REFERENCES": "man keypoint\nlocalization via large language model,” in Proceedings of",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[39] C. Corneanu, M. Madadi,\nand\nS.\nEscalera,\n“Deep\nstructure\ninfer-"
        },
        {
          "REFERENCES": "the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "-\nence network for\nfacial\naction unit\nrecognition,” Cornell University"
        },
        {
          "REFERENCES": "2024, pp. 614–623.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "arXiv,Cornell University - arXiv, Mar 2018."
        },
        {
          "REFERENCES": "[18] H. Liu, C. Li, Q. Wu,\nand Y.\nJ. Lee,\n“Visual\ninstruction\ntuning,”",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[40] G.\nLi,\nX.\nZhu,\nY\n.\nZeng,\nQ. Wang,\nand\nL.\nLin,\n“Semantic"
        },
        {
          "REFERENCES": "Advances in neural\ninformation processing systems, vol. 36, 2024.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "relationships\nguided\nrepresentation\nlearning\nfor\nfacial\naction\nunit"
        },
        {
          "REFERENCES": "[19]\nT. Baltrusaitis, M. Mahmoud, and P. Robinson, “Cross-dataset\nlearning",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Proceedings\nof\nthe\nAAAI\nConference\non\nArtificial\nrecognition,”"
        },
        {
          "REFERENCES": "and person-specific normalisation for automatic action unit detection,”",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "Intelligence,\np.\n8594–8601,\nAug\n2019.\n[Online].\nAvailable:\nhttp:"
        },
        {
          "REFERENCES": "2015\n11th\nIEEE\nInternational\nConference\nand Workshops\non\nin",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "//dx.doi.org/10.1609/aaai.v33i01.33018594"
        },
        {
          "REFERENCES": "Automatic Face and Gesture Recognition (FG), May 2015.\n[Online].",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[41]\nZ. Shao, Z. Liu, J. Cai, and L. Ma, “Deep adaptive attention for\njoint"
        },
        {
          "REFERENCES": "Available: http://dx.doi.org/10.1109/fg.2015.7284869",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "facial action unit detection and face alignment,” 2018."
        },
        {
          "REFERENCES": "[20] K. Zhao, W.-S. Chu,\nand H. Zhang,\n“Deep\nregion\nand multi-label",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "[42] C. Wei, K. Lu, W. Gan, and J. Xue, “Spatiotemporal\nfeatures and local"
        },
        {
          "REFERENCES": "learning for\nfacial action unit detection,” in 2016 IEEE Conference on",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "relationship learning for facial action unit\nintensity regression,” in 2021"
        },
        {
          "REFERENCES": "Computer Vision and Pattern Recognition (CVPR), Jun 2016.\n[Online].",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "IEEE International Conference on Image Processing (ICIP), 2021, pp."
        },
        {
          "REFERENCES": "Available: http://dx.doi.org/10.1109/cvpr.2016.369",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": "1109–1113."
        },
        {
          "REFERENCES": "[21] W. Li, F. Abtahi, Z. Zhu, and L. Yin, “Eac-net: Deep nets with enhancing",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "IEEE Transactions on\nand cropping for\nfacial\naction unit detection,”",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "Pattern Analysis and Machine Intelligence, p. 2583–2596, Nov 2018.",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "[Online]. Available: http://dx.doi.org/10.1109/tpami.2018.2791608",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "[22] Y. Chang and S. Wang, “Knowledge-driven self-supervised representa-",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        },
        {
          "REFERENCES": "tion learning for\nfacial action unit\nrecognition.”",
          "[23]\nJ. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "TABLE II COMPARISON WITH THE RELATED METHODS ON BP4D DATASET USING THE F1 SCORE METRIC (IN %). THE NUMBERS BOLDED AND UNDERLINED REPRESENT THE BEST PERFORMANCE",
      "venue": "TABLE II COMPARISON WITH THE RELATED METHODS ON BP4D DATASET USING THE F1 SCORE METRIC (IN %). THE NUMBERS BOLDED AND UNDERLINED REPRESENT THE BEST PERFORMANCE"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Lsvm"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "Impact of deep learning approaches on facial expression recognition in healthcare industries",
      "authors": [
        "C Bisogni",
        "A Castiglione",
        "S Hossain",
        "F Narducci",
        "S Umer"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Industrial Informatics"
    },
    {
      "citation_id": "4",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "C Darwin"
      ],
      "venue": "The American Journal of the Medical Sciences",
      "doi": "10.1097/00000441-195610000-00024"
    },
    {
      "citation_id": "5",
      "title": "Facial action coding system (facs): a technique for the measurement of facial actions",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Rivista Di Psichiatria"
    },
    {
      "citation_id": "6",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.1322355111"
    },
    {
      "citation_id": "7",
      "title": "Emfacs-7: Emotional facial action coding system",
      "authors": [
        "W Friesen",
        "P Ekman"
      ],
      "year": "1983",
      "venue": "Emfacs-7: Emotional facial action coding system"
    },
    {
      "citation_id": "8",
      "title": "The consistency of facial expressions of pain: a comparison across modalities",
      "authors": [
        "K Prkachin"
      ],
      "year": "1992",
      "venue": "Pain",
      "doi": "10.1016/0304-3959(92)90213-u"
    },
    {
      "citation_id": "9",
      "title": "Action unit detection with region adaptation, multi-labeling learning and optimal temporal",
      "authors": [
        "W Li",
        "F Abitahi",
        "Z Zhu"
      ],
      "year": "2017",
      "venue": "Action unit detection with region adaptation, multi-labeling learning and optimal temporal"
    },
    {
      "citation_id": "10",
      "title": "Multi-label image recognition with graph convolutional networks",
      "authors": [
        "Z.-M Chen",
        "X.-S Wei",
        "P Wang",
        "Y Guo"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/cvpr.2019.00532"
    },
    {
      "citation_id": "11",
      "title": "Auformer: Vision transformers are parameter-efficient facial action unit detectors",
      "authors": [
        "K Yuan",
        "Z Yu",
        "X Liu",
        "W Xie",
        "H Yue",
        "J Yang"
      ],
      "year": "2024",
      "venue": "Auformer: Vision transformers are parameter-efficient facial action unit detectors",
      "arxiv": "arXiv:2403.04697"
    },
    {
      "citation_id": "12",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "L Ouyang",
        "J Wu",
        "X Jiang"
      ],
      "venue": "Training language models to follow instructions with human feedback"
    },
    {
      "citation_id": "13",
      "title": "Training compute-optimal large language models",
      "authors": [
        "J Hoffmann",
        "S Borgeaud",
        "A Mensch",
        "E Buchatskaya"
      ],
      "venue": "Training compute-optimal large language models"
    },
    {
      "citation_id": "14",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal"
      ],
      "year": "2020",
      "venue": "arXiv: Computation and Language,arXiv: Computation and Language"
    },
    {
      "citation_id": "15",
      "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "16",
      "title": "Enhancing zero-shot facial expression recognition by llm knowledge transfer",
      "authors": [
        "Z Zhao",
        "Y Cao",
        "S Gong",
        "I Patras"
      ],
      "year": "2024",
      "venue": "Enhancing zero-shot facial expression recognition by llm knowledge transfer",
      "arxiv": "arXiv:2405.19100"
    },
    {
      "citation_id": "17",
      "title": "Multimodal emotion recognition using feature fusion: An llm-based approach",
      "authors": [
        "C Kumar",
        "N Gowtham",
        "M Zakariah",
        "A Almazyad"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Expllm: Towards chain of thought for facial expression recognition",
      "authors": [
        "X Lan",
        "J Xue",
        "J Qi",
        "D Jiang",
        "K Lu",
        "T.-S Chua"
      ],
      "year": "2024",
      "venue": "Expllm: Towards chain of thought for facial expression recognition",
      "arxiv": "arXiv:2409.02828"
    },
    {
      "citation_id": "19",
      "title": "Locllm: Exploiting generalizable human keypoint localization via large language model",
      "authors": [
        "D Wang",
        "S Xuan",
        "S Zhang"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "21",
      "title": "Cross-dataset learning and person-specific normalisation for automatic action unit detection",
      "authors": [
        "T Baltrusaitis",
        "M Mahmoud",
        "P Robinson"
      ],
      "year": "2015",
      "venue": "2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "23",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "K Zhao",
        "W.-S Chu",
        "H Zhang"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "25",
      "title": "Eac-net: Deep nets with enhancing and cropping for facial action unit detection",
      "authors": [
        "W Li",
        "F Abtahi",
        "Z Zhu",
        "L Yin"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/tpami.2018.2791608"
    },
    {
      "citation_id": "26",
      "title": "Knowledge-driven self-supervised representation learning for facial action unit recognition",
      "authors": [
        "Y Chang",
        "S Wang"
      ],
      "venue": "Knowledge-driven self-supervised representation learning for facial action unit recognition"
    },
    {
      "citation_id": "27",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "J Wei",
        "M Bosma",
        "V Zhao",
        "K Guu",
        "A Yu",
        "B Lester",
        "N Du",
        "A Dai",
        "Q Le"
      ],
      "year": "2021",
      "venue": "Finetuned language models are zero-shot learners",
      "arxiv": "arXiv:2109.01652"
    },
    {
      "citation_id": "28",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "D Zhu",
        "J Chen",
        "X Shen",
        "X Li",
        "M Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    },
    {
      "citation_id": "29",
      "title": "Facial affective behavior analysis with instruction tuning",
      "authors": [
        "Y Li",
        "A Dao",
        "W Bao",
        "Z Tan",
        "T Chen",
        "H Liu",
        "Y Kong"
      ],
      "year": "2024",
      "venue": "Facial affective behavior analysis with instruction tuning",
      "arxiv": "arXiv:2404.05052"
    },
    {
      "citation_id": "30",
      "title": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "authors": [
        "B Xing",
        "Z Yu",
        "X Liu",
        "K Yuan",
        "Q Ye",
        "W Xie",
        "H Yue",
        "J Yang",
        "H Kälviäinen"
      ],
      "year": "2024",
      "venue": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "arxiv": "arXiv:2408.11424"
    },
    {
      "citation_id": "31",
      "title": "Pink: Unveiling the power of referential comprehension for multi-modal llms",
      "authors": [
        "S Xuan",
        "Q Guo",
        "M Yang",
        "S Zhang"
      ],
      "year": "2023",
      "venue": "Pink: Unveiling the power of referential comprehension for multi-modal llms"
    },
    {
      "citation_id": "32",
      "title": "Lora: Low-rank adaptation of large language models.\" arXiv: Computation and Language",
      "authors": [
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models.\" arXiv: Computation and Language"
    },
    {
      "citation_id": "33",
      "title": "Bp4d-spontaneous: a high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "X Zhang",
        "L Yin",
        "J Cohn",
        "S Canavan",
        "M Reale",
        "A Horowitz",
        "P Liu",
        "J Girard"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing",
      "doi": "10.1016/j.imavis.2014.06.002"
    },
    {
      "citation_id": "34",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "S Mavadati",
        "M Mahoor",
        "K Bartlett",
        "P Trinh",
        "J Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Feafa: A well-annotated dataset for facial expression analysis and 3d facial animation",
      "authors": [
        "Y Yan",
        "K Lu",
        "J Xue",
        "P Gao",
        "J Lyu"
      ],
      "year": "2019",
      "venue": "Feafa: A well-annotated dataset for facial expression analysis and 3d facial animation"
    },
    {
      "citation_id": "37",
      "title": "Feafa+: an extended well-annotated dataset for facial expression analysis and 3d facial animation",
      "authors": [
        "W Gan",
        "J Xue",
        "K Lu",
        "Y Yan",
        "P Gao",
        "J Lyu"
      ],
      "year": "2022",
      "venue": "Fourteenth International Conference on Digital Image Processing"
    },
    {
      "citation_id": "38",
      "title": "Sample and computation redistribution for efficient face detection",
      "authors": [
        "J Guo",
        "J Deng",
        "A Lattas",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Sample and computation redistribution for efficient face detection"
    },
    {
      "citation_id": "39",
      "title": "Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing",
      "authors": [
        "W Li",
        "F Abitahi",
        "Z Zhu"
      ],
      "year": "2017",
      "venue": "Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing"
    },
    {
      "citation_id": "40",
      "title": "Local relationship learning with person-specific shape regularization for facial action unit detection",
      "authors": [
        "X Niu",
        "H Han",
        "S Yang",
        "Y Huang",
        "S Shan"
      ],
      "year": "2019",
      "venue": "Local relationship learning with person-specific shape regularization for facial action unit detection"
    },
    {
      "citation_id": "41",
      "title": "Learning robust visual features without supervision",
      "authors": [
        "M Oquab",
        "T Darcet",
        "T Moutakanni",
        "H Vo",
        "M Szafraniec",
        "V Khalidov",
        "P Fernandez",
        "D Haziza",
        "F Massa",
        "A El-Nouby"
      ],
      "year": "2023",
      "venue": "Learning robust visual features without supervision",
      "arxiv": "arXiv:2304.07193"
    },
    {
      "citation_id": "42",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "W.-L Chiang",
        "Z Li",
        "Z Lin"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
    },
    {
      "citation_id": "43",
      "title": "Liblinear: A library for large linear classification",
      "authors": [
        "F.-E Fanrong-En",
        "C.-W Changkai-Wei",
        "H.-J Hsiehcho-Jui",
        "W -R. Wangxiang-Rui",
        "L.-J Linchih-Jen"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research,Journal of Machine Learning Research"
    },
    {
      "citation_id": "44",
      "title": "Deep structure inference network for facial action unit recognition",
      "authors": [
        "C Corneanu",
        "M Madadi",
        "S Escalera"
      ],
      "year": "2018",
      "venue": "Deep structure inference network for facial action unit recognition"
    },
    {
      "citation_id": "45",
      "title": "Semantic relationships guided representation learning for facial action unit recognition",
      "authors": [
        "G Li",
        "X Zhu",
        "Y Zeng",
        "Q Wang",
        "L Lin"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v33i01.33018594"
    },
    {
      "citation_id": "46",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "authors": [
        "Z Shao",
        "Z Liu",
        "J Cai",
        "L Ma"
      ],
      "year": "2018",
      "venue": "Deep adaptive attention for joint facial action unit detection and face alignment"
    },
    {
      "citation_id": "47",
      "title": "Spatiotemporal features and local relationship learning for facial action unit intensity regression",
      "authors": [
        "C Wei",
        "K Lu",
        "W Gan",
        "J Xue"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Image Processing"
    }
  ]
}