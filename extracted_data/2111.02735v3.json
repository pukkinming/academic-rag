{
  "paper_id": "2111.02735v3",
  "title": "A Fine-Tuned Wav2Vec 2.0/Hubert Benchmark For Speech Emotion Recognition, Speaker Verification And Spoken Language Understanding",
  "published": "2021-11-04T10:39:06Z",
  "authors": [
    "Yingzhi Wang",
    "Abdelmoumene Boumadane",
    "Abdelwahab Heba"
  ],
  "keywords": [
    "wav2vec 2.0",
    "HuBERT",
    "speech emotion recognition",
    "speaker verification",
    "spoken language understanding"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech self-supervised models such as wav2vec 2.0 and HuBERT are making revolutionary progress in Automatic Speech Recognition (ASR). However, they have not been totally proven to produce better performance on tasks other than ASR. In this work, we explored partial fine-tuning and entire fine-tuning on wav2vec 2.0 and HuBERT pre-trained models for three non-ASR speech tasks: Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding. With simple proposed downstream frameworks, the best scores reached 79.58% weighted accuracy on speaker-dependent setting and 73.01% weighted accuracy on speaker-independent setting for Speech Emotion Recognition on IEMOCAP, 2.36% equal error rate for Speaker Verification on VoxCeleb1, 89.38% accuracy for Intent Classification and 78.92% F1 for Slot Filling on SLURP, showing the strength of fine-tuned wav2vec 2.0 and HuBERT on learning prosodic, voice-print and semantic representations.",
      "page_start": 1,
      "page_end": 5
    },
    {
      "section_name": "Introduction",
      "text": "Nowadays, people are expecting less labeled data to train wellgeneralized models for supervised tasks, since data labeling is a very time-and money-consuming process. Furthermore, people have been attempting to find a powerful feature embedding that can assist the fine-tuning and multi-task training for downstream tasks. The appearance of self-supervised learning meets the above two requirements exactly. In speech domain, excellent self-supervised models are emerging  [1, 2, 3, 4, 5, 6, 7, 8, 9] , among which the most high-performing and the most widely used are wav2vec 2.0  [10]  and HuBERT  [11] . Many wav2vec 2.0/HuBERT pretrained models have also been published and this has greatly promoted their applications in the field of speech. Therefore, we chose wav2vec2.0 and HuBERT as our research objects in this work.\n\nThe wav2vec 2.0 model architecture contains mainly three modules. A convolutional neural network (CNN) feature encoder encodes the raw waveform inputs into latent speech representations. Mask operations are applied before they are fed to the Transformerbased contextualized encoder. A quantization module is used to quantize the latent speech representations from the CNN encoder into a discretized embedding which is then used as the target. The objective is to optimize the contrastive loss, which enforces the model to identify the true quantized latent speech representations.\n\nHuBERT shares the same architecture as wav2vec 2.0. Instead of constructing a contrastive loss, HuBERT uses an offline clustering step to generate noisy labels for Masked Language Model pretraining. Specifically, HuBERT consumes masked continuous speech features to predict predetermined cluster assignments. The predictive loss is applied over the masked regions, forcing the model to learn good high-level representations of unmasked inputs in order to infer the targets of masked ones correctly.\n\nWav2vec 2.0 and HuBERT outperformed all existing ASR models at that time, proving that they can construct a better verbal embedding. However, speech also contains other important information such as emotion, speaker and semantics, for which the industry also has high expectations. In the field of Speech Emotion Recognition (SER), Speaker Verification (SV) and Spoken Language Understanding (SLU), it is still vague whether self-supervised models can produce better performance compared with traditional supervised models (spectral features + CNN-based feature extraction + RNN/Transformer based time series modeling)  [12, 13, 14, 15, 16] . However, meaningful attempts have been made in some previous works, which we will introduce below.\n\nIn SUPERB  [17] , the performance of different frozen selfsupervised encoders were benchmarked across a wide range of speech tasks. For SER, the HuBERT large model stood out from other self-supervised encoders with 67.62% accuracy (ACC) on IEMOCAP  [18] . For SV, the HuBERT base model obtained the best Equal Error Rate (EER) 5.11% on VoxCeleb1  [19] . SLU contains two separate subtasks: Intent Classification (IC) and Slot Filling (SF). The HuBERT large model achieved the best results on both IC and SF tasks with 98.76% ACC on Fluent Speech Commands dataset  [20]  and 89.81% F1 score on SNIPS  [21]  respectively.\n\nFor SER,  [22]  combined the features from frozen wav2vec2.0 with other hand-crafted prosodic features and then fed them into a 1d-CNN for a deeper extraction.  [23]  explored wav2vec fine-tuning strategies and 65.4% WA on IEMOCAP was achieved. For SV,  [24, 25]  both explored fine-tuned wav2vec 2.0,  [24]  obtained 3.61% EER on VoxCeleb1, while  [25]  also obtained 1.91% EER on VoxCeleb1 by adding VoxCeleb2 into the training set.\n\nWe notice that self-supervised models were only used as frozen feature extractors in SUPERB and some other works. Believing that only by fine-tuning can we show the real power of self-supervised models, we explored the fine-tuning of wav2vec2.0/HuBERT on three speech tasks and provided full fine-tuning experiment details. Taking inspiration from  [10]  and  [11] , we added another fine-tuning method by splitting a pre-trained wav2vec 2.0/HuBERT model into two parts: the CNN feature encoder and the Transformer contextualized encoder. We froze the CNN feature encoder and only fine-tuned the Transformer contextualized encoder. We then tested partially fine-tuned wav2vec2.0/HuBERT pre-trained models together with the entirely fine-tuned ones with the following tasks below:\n\n• Speech Emotion Recognition on IEMOCAP • Speaker Verification on VoxCeleb1\n\n• Spoken Language Understanding on SLURP  [26]  The results show that our fine-tuned models achieved excellent results on the three tasks, which further proves their strong capac-arXiv:2111.02735v3 [cs.CL] 3 Oct 2022",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "In this section, we will first introduce the pre-training of wav2vec 2.0/HuBERT model, then we will show our fine-tuning methods and downstream models for each task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Pretrained Wav2Vec 2.0",
      "text": "The wav2vec 2.0 pre-training is similar to the masked language modelling in BERT  [28]  and is carried out under a self-supervised setting. Contiguous time steps from the CNN encoder representations are randomly masked, and the model is trained to reproduce the quantized local encoder representations for masked frames at the output of the contextualized encoder.\n\nThe training objective is illustrated in Eq.1, where sim(ct, qt) is the cosine similarity between the contextualized encoder outputs ct and the quantized CNN encoder representations qt, t is the masked time step, Qt is the union of candidate representations q which includes qt and K = 100 distractors, κ is the temperature which is set to 0.1. The distractors are outputs of the local encoder sampled from masked frames belonging to the same utterance as qt. The contrastive loss is then given by Lm summed over all masked frames. At the end, an L2 regularization is added to the contrastive loss, as well as a diversity loss to increase the use of the quantized codebook representations.\n\nThe pre-training process is optimized with Adam  [29]  and the learning rate decays linearly after a waming up. In  [10] , wav2vec 2.0 is also fine-tuned on ASR aiming to improve ASR performance. For ASR fine-tuning, a randomly initialized linear projection is added to the output of the contextual encoder and the CTC (Connectionist Temporal Classification  [30] ) loss is minimized. For more details of 1 https://github.com/speechbrain/speechbrain/tree/develop/recipes the pre-training and ASR fine-tuning of wav2vec 2.0, please refer to  [10] .\n\nIn this work, we compare four released wav2vec 2.0 pre-trained models: the wav2vec 2.0 base model (12 transformer blocks and 768 embedding dimension) and its ASR fine-tuned version, the wav2vec 2.0 large model (24 transformer blocks and 1024 embedding dimension) and its ASR fine-tuned version. Both base and large models are pre-trained on 960h LibriSpeech  [31]  data, which is also used for their ASR fine-tuning. ASR fine-tuned models for both wav2vec 2.0 and HuBERT are taken into consideration because we assume that some tasks may benefit from the ASR fine-tuning.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Pretrained Hubert",
      "text": "In the same way as wav2vec 2.0, CNN-encoded audio features are randomly masked in HuBERT. To generate labels for the first iteration of HuBERT pre-training, a k-means clustering is applied on 39-dimensional MFCC features. To generate better targets for the subsequent iterations, k-means clustering then works on the latent features extracted from the HuBERT model pre-trained in the previous iteration. A projection layer is added over transformer blocks to predict cluster labels. Cross-entropy loss is computed over masked timestamps, which can be defined as:\n\nM ⊂ [T ] denotes the set of indices to be masked for a length-T sequence X, and X = r(X; M ) denotes a corrupted version of X where xt is replaced with a mask embedding x if t ∈ M . A masked prediction model f takes as input X and predicts a distribution over the target indices at each timestep p f (•| X; t). To improve target quality, cluster ensembles are utillized in case that an individual clustering model performs badly, Z (k) then denotes the target sequences generated by the k-th clustering model.\n\nHuBERT pre-training uses the same optimizer and learning rate scheduler as wav2vec 2.0. For ASR fine-tuning, the projection layer is removed and replaced by a randomly initialized softmax layer, then the CTC loss is optimized. For more details of the pre-training of HuBERT, please refer to  [11] . Fig.  2 . Simple downstream models for SER, SID and SLU. For SER and SID, an average time pooling and a linear classifier is built over wav2vec 2.0/HuBERT. For SLU, an attentional decoder decodes intents and slots directly from the fine-tuned wav2vec2.0/HuBERT embedding.\n\nLike wav2vec 2.0, we compare three released HuBERT pretrained models: the HuBERT base model (12 transformer blocks and 768 embedding dimension, of which no ASR fine-tuned version is released), the HuBERT large model (24 transformer blocks and 1024 embedding dimension) and its ASR fine-tuned version. The HuBERT base model is pre-trained on 960h LibriSpeech data, while the large model is pre-trained on 60k hours Libri-Light  [32]  data. The ASR fine-tuning is also based on 960h LibriSpeech data.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Fine-Tuning",
      "text": "As is shown in Figure  1  on the left for partial fine-tuning, the wav2vec 2.0/HuBERT model is divided into two parts: the CNNbased feature encoder and the transformer-based contextualized encoder. We froze the CNN-based feature encoder, fixing all the parameters of these CNN blocks, and only fine-tuned the parameters of the transformer blocks. Partial fine-tuning can be understood as a domain adaptation training for the top level, which aims to prevent interference and damage to the bottom CNN layers that already have an expressive ability.\n\nFor entire fine-tuning which is shown on the right in Figure  1 , the CNN and Transformer modules are both fine-tuned during the downstream training process. By training general features at the bottom level, entire fine-tuning allows higher-level expressions to be more complete and more targeted.\n\nThen, assuming that fine-tuned wav2vec 2.0/HuBERT are already powerful enough to capture information, we directly added simple downstream adaptors (classifier/decoder) to wav2vec 2.0/Hu-BERT without adding another heavy and redundant encoder. The downstream adaptors for each task are presented as below.\n\nFor SER, an average time pooling and one linear layer are added as a simple downstream classifier (Fig.  2 ). The average time pooling compresses variant time lengths into one, then the linear layer effectuates an utterance-level classification minimizing the cross-entropy loss.\n\nFor SV, a Speaker Identification (SID) task is first implemented using the same downstream framework as SER. Pairwise cosine-similarity scores are then produced for SV on the pre-trained SID embeddings before the linear classification layer.\n\nFor SLU (Fig.  2 ), another attentional GRU-based decoder is added to decode semantic information directly from the fine-tuned wav2vec2.0/HuBERT embedding. In our work, intents and slots are both treated as a sequence-to-sequence ASR task and are both decoded from the attentional decoder. The Negative Log-Likelihood (NLL) loss is then calculated over a character-level token generation. Following the observations of  [33] , we utilized a beam-search with a beam of 80 without coverage penalty to identify the optimum sequence for validation set and test set.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "In the experiment section, we will first introduce the datasets used for the three tasks, then we will list the models we compared and add details of our experiment settings. Finally, we will show the results for each task and compare with the existing state-of-the-art baselines.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "The three most widely used and most representative datasets were chosen in our experiments, which are IEMOCAP for SER, Vox-Celeb1 for SV and SLURP for SLU.\n\nThe Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset has approximately 12 hours of data and consists of scripted and improvised dialogues by 10 speakers. In order to form a contrast in this work, we used 4 emotional classes as in SUPERB: anger, happiness, sadness and neutral, following the work of  [34] . The evaluation metric is weighted accuracy (WA) and the experiments were carried out on two different split settings: Speaker-Dependent (SD) setting and Speaker-Independent (SI) setting. For SD, the results were averaged on 5 different random seeds for train-validation-test split. For SI, a 10-fold cross-validation was performed with a leavetwo-speaker-out strategy (one for validation and one for test).\n\nVoxCeleb1 contains over 100,000 utterances from 1,251 speakers, with approximately 351 hours of audio in total. In our work, a Speaker Identification task was first implemented, encouraging the model to learn to distinguish 1211 different voice-prints. A verification was then carried out on the vox1-o test set of 40 speakers by calculating cosine similarity on the embeddings from the pre-trained Speaker Identification model. VoxCeleb2 and noise augmentation were not used in our experiments. We used equal error rate (EER) as the evaluation metric and the results were averaged on 5 different seeds for train-validation split.\n\nThe Spoken Language Understanding Resource Package (SLURP) Dataset is a collection of 72K audio recordings of single turn user interactions with a home assistant, annotated with three levels of semantics: Scenario, Action and Entities. The training and evaluation are based on its official training, validation and test sets.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Fine-Tuning Settings",
      "text": "We rename the models we compare with a method as below.\n\n• EF/PF/Frozen: Entirely Fine-tuned/Partially Fine-tuned/Not fine-tuned\n\n• w2v/hbt: wav2vec 2.0/HuBERT based model\n\n• base/large: base/large pre-trained model\n\n• -/960h: with/without ASR fine-tuning using 960h Lib-riSpeech data For example, EF-w2v-base refers to an entirely fine-tuned wav2vec 2.0 base model, while PF-hbt-large-960h refers to a partially fine-tuned HuBERT large model with an ASR fine-tuning. For more detailed parameters of released pre-trained wav2vec 2.0/Hu-BERT models, please refer to  [10]  and  [11] .\n\nDuring the fine-tuning process, we applied two different schedulers to respectively adjust the fine-tuning learning rate of the wav2vec 2.0/HuBERT encoder and the learning rate of the downstream model. Both the schedulers use an Adam Optimizer and linearly anneal the learning rates according to the performance of validation stage. For SER and SV, the initialized fine-tuning learning rate and the downstream learning rate are set to 10 -5 and 10 -4 . For SLU, these values are set to 10 -5 and 3 × 10 -4 . Table  1 . Benchmark results for two utterance-level tasks: Speech Emotion Recognition (SER) on weighted accuracy (WA%) and Speaker Verification (SV) on Equal Error Rate (EER%). In relation to SER, it is divided into SER-SD (Speaker-Dependent settng) and SER-SI (Speaker-Independent setting).\n\nModel The results of SER and SV are shown in Table  1 . For comparison, we show SUPERB's results as a non-fine-tuned baseline (marked with  [17]  in Table  1 ). Furthermore, we took Head-Fusion ACNN  [35]  for SER-SD (Speaker-Dependent setting), Attention Pooling based representation  [36]  for SER-SI (Speaker-Independent setting) and Siamese Capsule network  [37]  for SV as state-of-the-art baselines respectively. Compared with other more recent works,  [36]  provides a comparable result by reporting a competitve Weighted Accuracy using only speech, and  [37]  also provides a comparable result using only Voxceleb1 as the training set (the same as SUPERB). First of all, from an overall perspective, we notice a significant improvement on the results of fine-tuned models over those of SUPERB's nonfine-tuned models. Then, for SER, we are surprised to find that all of our fine-tuned models performed well, where the partially fine-tuned HuBERT large model reached a best WA of 79.58% for SER-SD and a best WA of 73.01% for SER-SI, improving by 3.40% and 1.26% on the state-of-the-art baselines respectively. Moreover, we observe that partial fine-tuning appeared to be a better fine-tuning method than entire fine-tuning. We consider that IEMOCAP is a small dataset with only 12 hours of data and training too many parameters may easily cause an overfitting. Additionally, we noticed that the ASR fine-tuning was not helping the downstream SER task, suggesting a loss of prosodic information during the ASR fine-tuning.\n\nIn the case of SV, the entirely fine-tuned HuBERT with ASR fine-tuning reached a best 2.36% Equal Error Rate, surpassing the baseline by 0.78%. However, contrary to SER, entire fine-tuning outperforms partial fine-tuning as can be seen from the results. Due to the large amount of data (351 hours) of VoxCeleb1 that are also acoustically similar to the data used for pre-training, the pre-trained encoder parameters provide an ideal initialization for the downstream SV task, releasing all the layers and fine-tuning with a low learning rate can lead to a good result. Finally, we find that Hu-BERT turned out to be a better self-supervised encoder compared to wav2vec 2.0 for both SER and SV tasks. For SLU, the results of its two subtasks are shown in Table  2 . Likewise, we carried out experiments of frozen wav2vec 2.0/HuBERT models to form a contrast. However, the performance of frozen models on this task drops significantly, especially for wav2vec 2.0 large model the loss cannot even converge, demonstrating that the frozen wav2vec 2.0/HuBERT cannot hold complete semantic information. Continuous Token Interface  [38]  is chosen as the state-of-the-art baseline. The best ACC for IC is 89.38% with the entirely fine-tuned HuBERT large model, while the partially fine-tuned Hu-BERT large model reached the best F1 78.92% for SF. For SLU, the gap between the two fine-tuning methods is not obvious. A slight drop is observed on ASR fine-tuned models, which implies that ASR fine-tuning will also result in a loss of semantic information.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this work we explored different fine-tuning methods on two of the most powerful self-supervised models (wav2vec 2.0 and HuBERT), then benchmarked their performance on Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding tasks. State-of-the-art results were achieved for all the three tasks, proving the excellent generalizability of wav2vec 2.0/HuBERT on learning prosodic, voice-print and semantic representations. We hope to show the broad prospects of self-supervised learning and also provide some useful insights for its industrial applications.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.",
      "page": 2
    },
    {
      "caption": "Figure 2: Simple downstream models for SER, SID and SLU. For SER",
      "page": 3
    },
    {
      "caption": "Figure 1: on the left for partial ﬁne-tuning, the",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "learn good high-level representations of unmasked inputs in order to"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "infer the targets of masked ones correctly."
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "Wav2vec 2.0 and HuBERT outperformed all existing ASR mod-"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "els at\nthat\ntime, proving that\nthey can construct a better verbal em-"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "bedding. However, speech also contains other important information"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "such as emotion, speaker and semantics, for which the industry also"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "has high expectations.\nIn the ﬁeld of Speech Emotion Recogni-"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "tion (SER), Speaker Veriﬁcation (SV) and Spoken Language Un-"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "derstanding (SLU),\nit\nis still vague whether self-supervised mod-"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "els can produce better performance compared with traditional super-"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "vised models (spectral\nfeatures + CNN-based feature extraction +"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "RNN/Transformer based time series modeling) [12, 13, 14, 15, 16]."
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "However, meaningful attempts have been made in some previous"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "works, which we will introduce below."
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "In SUPERB [17],\nthe\nperformance\nof\ndifferent\nfrozen\nself-"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "supervised\nencoders were\nbenchmarked\nacross\na wide\nrange\nof"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "speech tasks.\nFor SER,\nthe HuBERT large model stood out\nfrom"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "other\nself-supervised encoders with 67.62% accuracy (ACC) on"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "IEMOCAP [18]. For SV, the HuBERT base model obtained the best"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "Equal Error Rate (EER) 5.11% on VoxCeleb1 [19]. SLU contains"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "two separate subtasks:\nIntent Classiﬁcation (IC) and Slot Filling"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "(SF). The HuBERT large model achieved the best\nresults on both"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "IC and SF tasks with 98.76% ACC on Fluent Speech Commands"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "dataset [20] and 89.81% F1 score on SNIPS [21] respectively."
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "For SER,\n[22] combined the features from frozen wav2vec2.0"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "with other hand-crafted prosodic features and then fed them into a"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "1d-CNN for a deeper extraction.\n[23] explored wav2vec ﬁne-tuning"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "strategies and 65.4% WA on IEMOCAP was achieved. For SV, [24,"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "25] both explored ﬁne-tuned wav2vec 2.0, [24] obtained 3.61% EER"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "on VoxCeleb1, while [25] also obtained 1.91% EER on VoxCeleb1"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "by adding VoxCeleb2 into the training set."
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "We notice that self-supervised models were only used as frozen"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "feature extractors in SUPERB and some other works. Believing that"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "only by ﬁne-tuning can we show the real power of self-supervised"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "models, we\nexplored the ﬁne-tuning of wav2vec2.0/HuBERT on"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "three\nspeech tasks\nand provided full ﬁne-tuning experiment de-"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "tails.\nTaking inspiration from [10]\nand [11], we\nadded another"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "ﬁne-tuning method by splitting a pre-trained wav2vec 2.0/HuBERT"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "model into two parts:\nthe CNN feature encoder and the Transformer"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "contextualized encoder. We froze the CNN feature encoder and"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "only ﬁne-tuned the Transformer contextualized encoder. We then"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "tested partially ﬁne-tuned wav2vec2.0/HuBERT pre-trained models"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "together with the entirely ﬁne-tuned ones with the following tasks"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "below:"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "•\nSpeech Emotion Recognition on IEMOCAP"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "•\nSpeaker Veriﬁcation on VoxCeleb1"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "•\nSpoken Language Understanding on SLURP [26]"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": ""
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "The results show that our ﬁne-tuned models achieved excellent"
        },
        {
          "(cid:63) Zaion Lab, Zaion, Paris, France": "results on the three tasks, which further proves their strong capac-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "the pre-training and ASR ﬁne-tuning of wav2vec 2.0, please refer to"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "[10]."
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "In this work, we compare four released wav2vec 2.0 pre-trained"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "models:\nthe wav2vec 2.0 base model (12 transformer blocks and 768"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "embedding dimension) and its ASR ﬁne-tuned version, the wav2vec"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "2.0 large model (24 transformer blocks and 1024 embedding dimen-"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "sion) and its ASR ﬁne-tuned version. Both base and large models"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "are pre-trained on 960h LibriSpeech [31] data, which is also used"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "for their ASR ﬁne-tuning. ASR ﬁne-tuned models for both wav2vec"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "2.0 and HuBERT are taken into consideration because we assume"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "that some tasks may beneﬁt from the ASR ﬁne-tuning."
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "2.2. Pretrained HuBERT"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "In the same way as wav2vec 2.0, CNN-encoded audio features are"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "randomly masked in HuBERT. To generate labels for the ﬁrst\niter-"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "ation of HuBERT pre-training, a k-means clustering is applied on"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "39-dimensional MFCC features. To generate better\ntargets for\nthe"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "subsequent\niterations, k-means clustering then works on the latent"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "features extracted from the HuBERT model pre-trained in the previ-"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "ous iteration. A projection layer is added over transformer blocks to"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "predict cluster labels. Cross-entropy loss is computed over masked"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": "timestamps, which can be deﬁned as:"
        },
        {
          "Fig. 1. Partial ﬁne-tuning (left) and entire ﬁne-tuning (right) of wav2vec 2.0/HuBERT.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "SID embeddings before the linear classiﬁcation layer."
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "For SLU (Fig.2),\nanother\nattentional GRU-based decoder\nis"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "added to decode semantic information directly from the ﬁne-tuned"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "wav2vec2.0/HuBERT embedding.\nIn our work,\nintents and slots"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "are both treated as a sequence-to-sequence ASR task and are both"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "decoded from the attentional decoder. The Negative Log-Likelihood"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "(NLL)\nloss is then calculated over a character-level\ntoken genera-"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "tion. Following the observations of [33], we utilized a beam-search"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "with a beam of 80 without coverage penalty to identify the optimum"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "sequence for validation set and test set."
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "3. EXPERIMENTS"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "In the experiment section, we will ﬁrst\nintroduce the datasets used"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "for\nthe three tasks,\nthen we will\nlist\nthe models we compared and"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "add details of our experiment settings.\nFinally, we will show the"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "results for each task and compare with the existing state-of-the-art"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "baselines."
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "3.1. Datasets"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "The three most widely used and most\nrepresentative datasets were"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "chosen in our experiments, which are IEMOCAP for SER, Vox-"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "Celeb1 for SV and SLURP for SLU."
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "The Interactive Emotional Dyadic Motion Capture (IEMOCAP)"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "dataset has approximately 12 hours of data and consists of scripted"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "and improvised dialogues by 10 speakers. In order to form a contrast"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "in this work, we used 4 emotional classes as in SUPERB: anger, hap-"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "piness, sadness and neutral, following the work of [34]. The evalu-"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "ation metric is weighted accuracy (WA) and the experiments were"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "carried out on two different split settings: Speaker-Dependent (SD)"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "setting and Speaker-Independent\n(SI) setting.\nFor SD,\nthe results"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "were averaged on 5 different random seeds for train-validation-test"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "split. For SI, a 10-fold cross-validation was performed with a leave-"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "two-speaker-out strategy (one for validation and one for test)."
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "VoxCeleb1 contains over 100,000 utterances from 1,251 speak-"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "ers, with approximately 351 hours of audio in total.\nIn our work, a"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "Speaker Identiﬁcation task was ﬁrst\nimplemented, encouraging the"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "model\nto learn to distinguish 1211 different voice-prints. A veriﬁ-"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "cation was then carried out on the vox1-o test set of 40 speakers by"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "calculating cosine similarity on the embeddings from the pre-trained"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "Speaker\nIdentiﬁcation model. VoxCeleb2 and noise augmentation"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "were not used in our experiments. We used equal error rate (EER)"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "as the evaluation metric and the results were averaged on 5 different"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "seeds for train-validation split."
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "The Spoken Language Understanding Resource Package (SLURP)"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "Dataset\nis a collection of 72K audio recordings of single turn user"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "interactions with a home assistant, annotated with three levels of se-"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "mantics: Scenario, Action and Entities. The training and evaluation"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "are based on its ofﬁcial training, validation and test sets."
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "3.2. Fine-tuning settings"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "We rename the models we compare with a method as below."
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "• EF/PF/Frozen: Entirely Fine-tuned/Partially Fine-tuned/Not"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "ﬁne-tuned"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "• w2v/hbt: wav2vec 2.0/HuBERT based model"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "•\nbase/large: base/large pre-trained model"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": ""
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "•\n-/960h:\nwith/without ASR ﬁne-tuning\nusing\n960h\nLib-"
        },
        {
          "cosine-similarity scores are then produced for SV on the pre-trained": "riSpeech data"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Benchmark results for two utterance-level tasks: Speech outperformspartialfine-tuningascanbeseenfromtheresults. Due",
      "data": [
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "wav2vec 2.0 base model, while PF-hbt-large-960h refers to a par-",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "ﬁne-tuned models. Then, for SER, we are surprised to ﬁnd that all of"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "tially ﬁne-tuned HuBERT large model with an ASR ﬁne-tuning. For",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "our ﬁne-tuned models performed well, where the partially ﬁne-tuned"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "more detailed parameters of\nreleased pre-trained wav2vec 2.0/Hu-",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "HuBERT large model reached a best WA of 79.58% for SER-SD and"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "BERT models, please refer to [10] and [11].",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "a best WA of 73.01% for SER-SI, improving by 3.40% and 1.26% on"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "During the ﬁne-tuning process, we applied two different sched-",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "the state-of-the-art baselines respectively. Moreover, we observe that"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "ulers\nto\nrespectively\nadjust\nthe ﬁne-tuning\nlearning\nrate\nof\nthe",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "partial ﬁne-tuning appeared to be a better ﬁne-tuning method than"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "wav2vec 2.0/HuBERT encoder and the learning rate of\nthe down-",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "entire ﬁne-tuning. We consider\nthat\nIEMOCAP is a small dataset"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "stream model.\nBoth the schedulers use an Adam Optimizer and",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "with only 12 hours of data and training too many parameters may"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "linearly anneal\nthe learning rates according to the performance of",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "easily cause an overﬁtting. Additionally, we noticed that\nthe ASR"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "validation stage. For SER and SV, the initialized ﬁne-tuning learning",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "ﬁne-tuning was not helping the downstream SER task, suggesting a"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "rate and the downstream learning rate are set to 10−5 and 10−4. For",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "loss of prosodic information during the ASR ﬁne-tuning."
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "SLU, these values are set to 10−5 and 3 × 10−4.",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "In the case of SV,\nthe entirely ﬁne-tuned HuBERT with ASR"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "ﬁne-tuning reached a best 2.36% Equal Error Rate, surpassing the"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "baseline by 0.78%. However, contrary to SER, entire ﬁne-tuning"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Table 1. Benchmark results for\ntwo utterance-level\ntasks: Speech",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "outperforms partial ﬁne-tuning as can be seen from the results. Due"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Emotion Recognition\n(SER)\non weighted\naccuracy\n(WA%)\nand",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "to the large amount of data (351 hours) of VoxCeleb1 that are also"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Speaker Veriﬁcation (SV) on Equal Error Rate (EER%).\nIn rela-",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "acoustically similar to the data used for pre-training, the pre-trained"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "tion to SER,\nit\nis divided into SER-SD (Speaker-Dependent settng)",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "encoder parameters provide\nan ideal\ninitialization for\nthe down-"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "and SER-SI (Speaker-Independent setting).",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "stream SV task,\nreleasing all\nthe layers and ﬁne-tuning with a low"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "learning rate can lead to a good result.\nFinally, we ﬁnd that Hu-"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Model\nSER-SD\nSER-SI\nSV",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "BERT turned out to be a better self-supervised encoder compared to"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "EF-w2v-base\n75.90\n70.75\n2.77",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "wav2vec 2.0 for both SER and SV tasks."
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "PF-w2v-base\n77.02\n70.21\n3.15",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "EF-w2v-base-960h\n73.64\n64.20\n4.46",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "PF-w2v-base-960h\n73.84\n68.34\n4.38",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "Table 2. Benchmark results for Spoken Language Understanding:"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "Intent Classiﬁcation (IC) on accuracy (ACC%) and Slot Filling (SF)"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "EF-w2v-large\n77.00\n70.96\n3.42",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "on F1 score (F1%)."
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "PF-w2v-large\n77.47\n70.99\n3.85",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "EF-w2v-large-960h\n73.00\n68.18\n4.27",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "Model\nIC (ACC%)\nSF (F1%)"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "PF-w2v-large-960h\n76.75\n69.08\n4.47",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "EF-w2v-base\n87.13\n74.32"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "EF-hbt-base\n76.53\n69.83\n2.84",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "PF-w2v-base\n86.58\n74.73"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "PF-hbt-base\n76.60\n69.68\n3.13",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "EF-w2v-base-960h\n85.89\n74.33"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "PF-w2v-base-960h\n86.13\n73.78"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "EF-hbt-large\n78.52\n72.31\n2.86",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "79.58\n73.01\nPF-hbt-large\n3.21",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "EF-w2v-large\n85.80\n72.45"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "2.36\nEF-hbt-large-960h\n78.78\n72.71",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "PF-w2v-large\n86.29\n73.16"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "PF-hbt-large-960h\n78.96\n72.98\n2.38",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "EF-w2v-large-960h\n86.10\n73.39"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "PF-w2v-large-960h\n86.35\n74.03"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Frozen-w2v-base[17]\n-\n63.43\n6.02",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Frozen-w2v-large[17]\n-\n65.64\n5.65",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "EF-hbt-base\n87.44\n75.06"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Frozen-hbt-base[17]\n-\n64.92\n5.11",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "PF-hbt-base\n87.51\n75.32"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Frozen-hbt-large[17]\n-\n67.62\n5.98",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "89.38\nEF-hbt-large\n78.43"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Head Fusion[35]\n76.18\n-\n-",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "78.92\nPF-hbt-large\n89.22"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Attention Pooling[36]\n-\n71.75\n-",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "EF-hbt-large-960h\n88.71\n78.89"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "Siamese Capsule[37]\n-\n-\n3.14",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "PF-hbt-large-960h\n88.32\n78.17"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "Frozen-w2v-base\n47.15\n37.66"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "Frozen-w2v-large\n3.88\n3.85"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "Frozen-hbt-base\n68.74\n57.06"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "3.3. Results and discussion",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": ""
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "Frozen-hbt-large\n74.42\n60.07"
        },
        {
          "EF-w2v-base\nFor\nexample,\nrefers\nto\nan\nentirely\nﬁne-tuned": "3.3.1.\nSpeech Emotion Recognition & Speaker Veriﬁcation",
          "on the results of ﬁne-tuned models over\nthose of SUPERB’s non-": "CTI[38]\n86.92\n74.66"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "ﬁne-tuned HuBERT large model, while the partially ﬁne-tuned Hu-"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "BERT large model reached the best F1 78.92% for SF. For SLU, the"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "gap between the two ﬁne-tuning methods is not obvious. A slight"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "drop is observed on ASR ﬁne-tuned models, which implies that ASR"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "ﬁne-tuning will also result in a loss of semantic information."
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "4. CONCLUSIONS"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "In this work we explored different ﬁne-tuning methods on two of the"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "most powerful self-supervised models (wav2vec 2.0 and HuBERT),"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "then benchmarked their performance on Speech Emotion Recog-"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "nition, Speaker Veriﬁcation and Spoken Language Understanding"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "tasks. State-of-the-art results were achieved for all\nthe three tasks,"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "proving the excellent generalizability of wav2vec 2.0/HuBERT on"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "learning prosodic,\nvoice-print\nand semantic\nrepresentations. We"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "hope to show the broad prospects of self-supervised learning and"
        },
        {
          "the-art baseline. The best ACC for\nIC is 89.38% with the entirely": "also provide some useful insights for its industrial applications."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "works,” in ICASSP 2019-2019 IEEE International Conference"
        },
        {
          "5. REFERENCES": "[1] Yu-An Chung, Wei-Ning Hsu, Hao Tang, and James Glass,",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,"
        },
        {
          "5. REFERENCES": "“An unsupervised autoregressive model\nfor speech represen-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "2019, pp. 6695–6699."
        },
        {
          "5. REFERENCES": "tation learning,” in Interspeech, 2019.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[14] Gautam Bhattacharya, Md Jahangir Alam, and Patrick Kenny,"
        },
        {
          "5. REFERENCES": "[2] Alexander H. Liu, Yu-An Chung, and James Glass,\n“Non-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "“Deep speaker embeddings for short-duration speaker veriﬁca-"
        },
        {
          "5. REFERENCES": "Autoregressive Predictive Coding for Learning Speech Rep-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "tion.,” in Interspeech, 2017, pp. 1517–1521."
        },
        {
          "5. REFERENCES": "resentations from Local Dependencies,”\nin Proc. Interspeech",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[15] Brecht Desplanques, Jenthe Thienpondt, and Kris Demuynck,"
        },
        {
          "5. REFERENCES": "2021, 2021, pp. 3730–3734.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "“ECAPA-TDNN: emphasized channel attention, propagation"
        },
        {
          "5. REFERENCES": "[3] Andy T Liu, Shu-wen Yang, Po-Han Chi, Po-chun Hsu, and",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "and aggregation in TDNN based speaker veriﬁcation,”\nin In-"
        },
        {
          "5. REFERENCES": "Hung-yi Lee,\n“Mockingjay: Unsupervised speech representa-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "terspeech. 2020, pp. 3830–3834, ISCA."
        },
        {
          "5. REFERENCES": "tion learning with deep bidirectional transformer encoders,” in",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[16] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj"
        },
        {
          "5. REFERENCES": "ICASSP 2020-2020 IEEE International Conference on Acous-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Kumar, Baiyang Liu, and Yoshua Bengio,\n“Towards end-to-"
        },
        {
          "5. REFERENCES": "tics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "end spoken language understanding,”\nin 2018 IEEE Interna-"
        },
        {
          "5. REFERENCES": "6419–6423.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "tional Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "5. REFERENCES": "[4] Andy T Liu, Shang-Wen Li, and Hung-yi Lee,\n“Tera: Self-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "(ICASSP). IEEE, 2018, pp. 5754–5758."
        },
        {
          "5. REFERENCES": "supervised learning of transformer encoder representation for",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[17]\nShu-wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Lai,"
        },
        {
          "5. REFERENCES": "speech,” IEEE/ACM Transactions on Audio, Speech, and Lan-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Kushal Lakhotia, Yist Lin, Andy Liu,\nJiatong Shi, Xuankai"
        },
        {
          "5. REFERENCES": "guage Processing, vol. 29, pp. 2351–2366, 2021.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng,"
        },
        {
          "5. REFERENCES": "[5] Mirco Ravanelli,\nJianyuan Zhong, Santiago Pascual, Pawel",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Ko-tik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-"
        },
        {
          "5. REFERENCES": "Swietojanski, Joao Monteiro, Jan Trmal, and Yoshua Bengio,",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Wen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung-"
        },
        {
          "5. REFERENCES": "“Multi-task self-supervised learning for\nrobust speech recog-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "yi Lee,\n“Superb:\nSpeech processing universal performance"
        },
        {
          "5. REFERENCES": "nition,” in ICASSP 2020-2020 IEEE International Conference",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "benchmark,”\nin Proc.\nInterspeech 2021, 08 2021, pp. 1194–"
        },
        {
          "5. REFERENCES": "on Acoustics, Speech and Signal Processing (ICASSP). IEEE,",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "1198."
        },
        {
          "5. REFERENCES": "2020, pp. 6989–6993.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[18] Carlos\nBusso,\nMurtaza\nBulut,\nChi-Chun\nLee,\nAbe"
        },
        {
          "5. REFERENCES": "[6]\nSteffen\nSchneider, Alexei Baevski, Ronan Collobert,\nand",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Kazemzadeh,\nEmily Mower,\nSamuel Kim,\nJeannette N"
        },
        {
          "5. REFERENCES": "Michael Auli,\n“wav2vec:\nUnsupervised Pre-Training\nfor",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Chang, Sungbok Lee, and Shrikanth S Narayanan,\n“Iemocap:"
        },
        {
          "5. REFERENCES": "Speech Recognition,”\nin Proc.\nInterspeech 2019, 2019, pp.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Lan-\nInteractive emotional dyadic motion capture database,”"
        },
        {
          "5. REFERENCES": "3465–3469.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "guage resources and evaluation, vol. 42, no. 4, pp. 335–359,"
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "2008."
        },
        {
          "5. REFERENCES": "[7] A. Baevski, S. Schneider, and M. Auli,\n“vq-wav2vec: Self-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "supervised learning of discrete speech representations,”\nin In-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[19] Arsha Nagrani,\nJoon Son Chung,\nand Andrew Zisserman,"
        },
        {
          "5. REFERENCES": "ternational Conference on Learning Representations (ICLR),",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "“Voxceleb: A large-scale speaker\nidentiﬁcation dataset,”\nin"
        },
        {
          "5. REFERENCES": "2020.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Interspeech. 2017, pp. 2616–2620, ISCA."
        },
        {
          "5. REFERENCES": "[8]\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shu-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[20] Natalia Tomashenko, Antoine Caubri`ere, Yannick Est`eve, An-"
        },
        {
          "5. REFERENCES": "jie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yosh-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "toine Laurent,\nand Emmanuel Morin,\n“Recent advances\nin"
        },
        {
          "5. REFERENCES": "ioka, Xiong Xiao, et al., “Wavlm: Large-scale self-supervised",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "end-to-end spoken language understanding,”\nin International"
        },
        {
          "5. REFERENCES": "pre-training for full stack speech processing,” IEEE Journal of",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Conference on Statistical Language and Speech Processing."
        },
        {
          "5. REFERENCES": "Selected Topics in Signal Processing, 2022.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Springer, 2019, pp. 44–55."
        },
        {
          "5. REFERENCES": "[9] Chengyi Wang, Yu Wu, Yao Qian, Kenichi Kumatani, Shujie",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[21] Alice Coucke, Alaa Saade, Adrien Ball, Th´eodore Bluche,"
        },
        {
          "5. REFERENCES": "Liu, Furu Wei, Michael Zeng, and Xuedong Huang,\n“Unis-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Alexandre Caulier, David Leroy, Cl´ement Doumouro, Thibault"
        },
        {
          "5. REFERENCES": "peech:\nUniﬁed speech representation learning with labeled",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Gisselbrecht, Francesco Caltagirone, Thibaut Lavril,\net\nal.,"
        },
        {
          "5. REFERENCES": "and unlabeled data,”\nin International Conference on Machine",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "“Snips voice platform:\nan embedded spoken language under-"
        },
        {
          "5. REFERENCES": "Learning. PMLR, 2021, pp. 10937–10947.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "standing system for private-by-design voice interfaces,” arXiv"
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "preprint arXiv:1805.10190, 2018."
        },
        {
          "5. REFERENCES": "[10] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "Michael Auli, “wav2vec 2.0: A framework for self-supervised",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[22] Leonardo Pepino, Pablo Riera, and Luciana Ferrer,\n“Emotion"
        },
        {
          "5. REFERENCES": "learning of speech representations,” in NeurIPS, 2020.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Recognition from Speech Using wav2vec 2.0 Embeddings,” in"
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Proc. Interspeech 2021, 2021, pp. 3400–3404."
        },
        {
          "5. REFERENCES": "[11] Wei-Ning Hsu,\nBenjamin Bolte,\nYao-Hung Hubert\nTsai,",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "Kushal Lakhotia, Ruslan Salakhutdinov,\nand Abdelrahman",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[23] Yangyang Xia,\nLi-Wei\nChen,\nAlexander\nRudnicky,\nand"
        },
        {
          "5. REFERENCES": "Mohamed,\n“Hubert:\nSelf-supervised speech representation",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "Richard M Stern, “Temporal context in speech emotion recog-"
        },
        {
          "5. REFERENCES": "learning by masked prediction of hidden units,” arXiv preprint",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "nition,” in Proc. Interspeech, 2021, vol. 2021, pp. 3370–3374."
        },
        {
          "5. REFERENCES": "arXiv:2106.07447, 2021.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[24] Zhiyun Fan, Meng Li, Shiyu Zhou, and Bo Xu,\n“Exploring"
        },
        {
          "5. REFERENCES": "[12]\nPengcheng Li, Yan Song, Ian Vince McLoughlin, Wu Guo, and",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "wav2vec 2.0 on Speaker Veriﬁcation and Language Identiﬁca-"
        },
        {
          "5. REFERENCES": "Lirong Dai,\n“An attention pooling based representation learn-",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "tion,” in Proc. Interspeech 2021, 2021, pp. 1509–1513."
        },
        {
          "5. REFERENCES": "ing method for speech emotion recognition,”\nin Interspeech.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "[25] Nik Vaessen\nand David A Van Leeuwen,\n“Fine-tuning"
        },
        {
          "5. REFERENCES": "2018, pp. 3087–3091, ISCA.",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": ""
        },
        {
          "5. REFERENCES": "",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "wav2vec2 for\nspeaker\nrecognition,”\nin ICASSP 2022-2022"
        },
        {
          "5. REFERENCES": "[13] Xixin Wu, Songxiang Liu, Yuewen Cao, Xu Li, Jianwei Yu,",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "IEEE International Conference on Acoustics, Speech and Sig-"
        },
        {
          "5. REFERENCES": "Dongyang Dai, Xi Ma, Shoukang Hu, Zhiyong Wu, Xunying",
          "Liu, et al.,\n“Speech emotion recognition using capsule net-": "nal Processing (ICASSP). IEEE, 2022, pp. 7967–7971."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Verena Rieser,\n“Slurp: A spoken language understanding re-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "source package,” arXiv preprint arXiv:2011.13205, 2020."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[27] Mirco Ravanelli,\nTitouan\nParcollet,\nPeter\nPlantinga, Aku"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nau-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "man Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, et al.,"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "arXiv\n“Speechbrain:\nA general-purpose\nspeech\ntoolkit,”"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "preprint arXiv:2106.04624, 2021."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[28]\nJacob Devlin, Ming-Wei Chang, Kenton Lee,\nand Kristina"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Toutanova,\n“BERT: pre-training of deep bidirectional\ntrans-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "formers\nfor\nlanguage understanding,”\nin NAACL-HLT (1)."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "2019, pp. 4171–4186, Association for Computational Linguis-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "tics."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[29] Diederik P. Kingma and Jimmy Ba,\n“Adam: A method for"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "stochastic optimization,” in ICLR (Poster), 2015."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[30] Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Schmidhuber,\n“Connectionist\ntemporal\nclassiﬁcation:\nla-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "belling unsegmented sequence data with recurrent neural net-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "the 23rd international conference\nworks,”\nin Proceedings of"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "on Machine learning, 2006, pp. 369–376."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[31] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Khudanpur,\n“Librispeech: An asr corpus based on public do-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "main audio books,” in 2015 IEEE International Conference on"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Acoustics, Speech and Signal Processing (ICASSP), 2015, pp."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "5206–5210."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[32]\nJacob\nKahn,\nMorgane\nRivi`ere,\nWeiyi\nZheng,\nEvgeny"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Kharitonov, Qiantong Xu, Pierre-Emmanuel Mazar´e,\nJulien"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Karadayi, Vitaliy Liptchinsky, Ronan Collobert, Christian Fue-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "gen, et al., “Libri-light: A benchmark for asr with limited or no"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "supervision,”\nin ICASSP 2020-2020 IEEE International Con-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "ference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "IEEE, 2020, pp. 7669–7673."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[33] Loren Lugosch, Piyush Papreja, Mirco Ravanelli, Abdelwahab"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Heba, and Titouan Parcollet,\n“Timers and Such: A Practical"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "Benchmark for Spoken Language Understanding with Num-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "bers,” NeurIPS Datasets and Benchmarks, 2021."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[34] Haytham M Fayek, Margaret Lech, and Lawrence Cavedon,"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "“Evaluating deep learning architectures\nfor\nspeech emotion"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "recognition,” Neural Networks, vol. 92, pp. 60–68, 2017."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[35] Mingke Xu, Fan Zhang, and Wei Zhang,\n“Head fusion:\nIm-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "proving the accuracy and robustness of speech emotion recog-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "nition on the iemocap and ravdess dataset,” IEEE Access, vol."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "9, pp. 74539–74549, 2021."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[36]\nPengcheng Li, Yan Song,\nIan Vince McLoughlin, Wu Guo,"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "and Li-Rong Dai,\n“An attention pooling based representation"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "learning method for speech emotion recognition,” Interspeech,"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "2018."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[37] Amirhossein Hajavi and Ali Etemad,\n“Siamese capsule net-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "work for\nend-to-end speaker\nrecognition in the wild,”\nin"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "ICASSP 2021-2021 IEEE International Conference on Acous-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "tics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "7203–7207."
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "[38]\nSeunghyun Seo, Donghyun Kwak, and Bowon Lee,\n“Integra-"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "tion of pre-trained networks with continuous token interface"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "for end-to-end spoken language understanding,” arXiv preprint"
        },
        {
          "[26] Emanuele Bastianelli, Andrea Vanzo, Pawel Swietojanski, and": "arXiv:2104.07253, 2021."
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Yu-An Chung",
        "Wei-Ning Hsu",
        "Hao Tang",
        "James Glass"
      ],
      "year": "2019",
      "venue": "An unsupervised autoregressive model for speech representation learning"
    },
    {
      "citation_id": "3",
      "title": "Non-Autoregressive Predictive Coding for Learning Speech Representations from Local Dependencies",
      "authors": [
        "Alexander Liu",
        "Yu-An Chung",
        "James Glass"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "4",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "Andy Liu",
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Po-Chun Hsu",
        "Hung-Yi Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Tera: Selfsupervised learning of transformer encoder representation for speech",
      "authors": [
        "Andy Liu",
        "Shang-Wen Li",
        "Hung-Yi Lee"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "6",
      "title": "Multi-task self-supervised learning for robust speech recognition",
      "authors": [
        "Mirco Ravanelli",
        "Jianyuan Zhong",
        "Santiago Pascual",
        "Pawel Swietojanski",
        "Joao Monteiro",
        "Jan Trmal",
        "Yoshua Bengio"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "vq-wav2vec: Selfsupervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Auli"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "9",
      "title": "Wavlm: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Unispeech: Unified speech representation learning with labeled and unlabeled data",
      "authors": [
        "Chengyi Wang",
        "Yu Wu",
        "Yao Qian",
        "Kenichi Kumatani",
        "Shujie Liu",
        "Furu Wei",
        "Michael Zeng",
        "Xuedong Huang"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "11",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A framework for self-supervised learning of speech representations"
    },
    {
      "citation_id": "12",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "arxiv": "arXiv:2106.07447"
    },
    {
      "citation_id": "13",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Vince Mcloughlin",
        "Wu Guo",
        "Lirong Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "Xixin Wu",
        "Songxiang Liu",
        "Yuewen Cao",
        "Xu Li",
        "Jianwei Yu",
        "Dongyang Dai",
        "Xi Ma",
        "Shoukang Hu",
        "Zhiyong Wu",
        "Xunying Liu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Deep speaker embeddings for short-duration speaker verification",
      "authors": [
        "Gautam Bhattacharya",
        "Md Jahangir Alam",
        "Patrick Kenny"
      ],
      "year": "2017",
      "venue": "Deep speaker embeddings for short-duration speaker verification"
    },
    {
      "citation_id": "16",
      "title": "ECAPA-TDNN: emphasized channel attention, propagation and aggregation in TDNN based speaker verification",
      "authors": [
        "Brecht Desplanques",
        "Jenthe Thienpondt",
        "Kris Demuynck"
      ],
      "venue": "ECAPA-TDNN: emphasized channel attention, propagation and aggregation in TDNN based speaker verification"
    },
    {
      "citation_id": "17",
      "title": "Towards end-toend spoken language understanding",
      "authors": [
        "Dmitriy Serdyuk",
        "Yongqiang Wang",
        "Christian Fuegen",
        "Anuj Kumar",
        "Baiyang Liu",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "18",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Lai",
        "Kushal Lakhotia",
        "Yist Lin",
        "Andy Liu",
        "Jiatong Shi",
        "Xuankai Chang",
        "Guan-Ting Lin",
        "Tzu-Hsien Huang",
        "Wei-Cheng Tseng",
        "Ko-Tik Lee",
        "Da-Rong Liu",
        "Zili Huang",
        "Shuyan Dong",
        "Shang-Wen Li",
        "Shinji Watanabe",
        "Abdelrahman Mohamed",
        "Hungyi Lee"
      ],
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "19",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "Voxceleb: A large-scale speaker identification dataset",
      "authors": [
        "Arsha Nagrani",
        "Son Chung",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "Voxceleb: A large-scale speaker identification dataset"
    },
    {
      "citation_id": "21",
      "title": "Recent advances in end-to-end spoken language understanding",
      "authors": [
        "Natalia Tomashenko",
        "Antoine Caubrière",
        "Yannick Estève",
        "Antoine Laurent",
        "Emmanuel Morin"
      ],
      "year": "2019",
      "venue": "International Conference on Statistical Language and Speech Processing"
    },
    {
      "citation_id": "22",
      "title": "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces",
      "authors": [
        "Alice Coucke",
        "Alaa Saade",
        "Adrien Ball",
        "Théodore Bluche",
        "Alexandre Caulier",
        "David Leroy",
        "Clément Doumouro",
        "Thibault Gisselbrecht",
        "Francesco Caltagirone",
        "Thibaut Lavril"
      ],
      "year": "2018",
      "venue": "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces",
      "arxiv": "arXiv:1805.10190"
    },
    {
      "citation_id": "23",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Temporal context in speech emotion recognition",
      "authors": [
        "Yangyang Xia",
        "Li-Wei Chen",
        "Alexander Rudnicky",
        "Richard Stern"
      ],
      "venue": "Proc. Interspeech, 2021"
    },
    {
      "citation_id": "25",
      "title": "Exploring wav2vec 2.0 on Speaker Verification and Language Identification",
      "authors": [
        "Zhiyun Fan",
        "Meng Li",
        "Shiyu Zhou",
        "Bo Xu"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Fine-tuning wav2vec2 for speaker recognition",
      "authors": [
        "Nik Vaessen",
        "David Van Leeuwen"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "Slurp: A spoken language understanding resource package",
      "authors": [
        "Emanuele Bastianelli",
        "Andrea Vanzo",
        "Pawel Swietojanski",
        "Verena Rieser"
      ],
      "year": "2020",
      "venue": "Slurp: A spoken language understanding resource package",
      "arxiv": "arXiv:2011.13205"
    },
    {
      "citation_id": "28",
      "title": "Speechbrain: A general-purpose speech toolkit",
      "authors": [
        "Mirco Ravanelli",
        "Titouan Parcollet",
        "Peter Plantinga",
        "Aku Rouhe",
        "Samuele Cornell",
        "Loren Lugosch",
        "Cem Subakan",
        "Nauman Dawalatabad",
        "Abdelwahab Heba",
        "Jianyuan Zhong"
      ],
      "year": "2021",
      "venue": "Speechbrain: A general-purpose speech toolkit",
      "arxiv": "arXiv:2106.04624"
    },
    {
      "citation_id": "29",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT (1)"
    },
    {
      "citation_id": "30",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "ICLR (Poster)"
    },
    {
      "citation_id": "31",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Faustino Gomez",
        "Jürgen Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "32",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "33",
      "title": "Libri-light: A benchmark for asr with limited or no supervision",
      "authors": [
        "Jacob Kahn",
        "Morgane Rivière",
        "Weiyi Zheng",
        "Evgeny Kharitonov",
        "Qiantong Xu",
        "Pierre-Emmanuel Mazaré",
        "Julien Karadayi",
        "Vitaliy Liptchinsky",
        "Ronan Collobert",
        "Christian Fuegen"
      ],
      "year": "2020",
      "venue": "Speech and Signal Processing"
    },
    {
      "citation_id": "34",
      "title": "Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers",
      "authors": [
        "Loren Lugosch",
        "Piyush Papreja",
        "Mirco Ravanelli",
        "Abdelwahab Heba",
        "Titouan Parcollet"
      ],
      "year": "2021",
      "venue": "Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers"
    },
    {
      "citation_id": "35",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "36",
      "title": "Head fusion: Improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Wei Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "37",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Vince Mcloughlin",
        "Wu Guo",
        "Li-Rong Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "38",
      "title": "Siamese capsule network for end-to-end speaker recognition in the wild",
      "authors": [
        "Amirhossein Hajavi",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "Integration of pre-trained networks with continuous token interface for end-to-end spoken language understanding",
      "authors": [
        "Seunghyun Seo",
        "Donghyun Kwak",
        "Bowon Lee"
      ],
      "year": "2021",
      "venue": "Integration of pre-trained networks with continuous token interface for end-to-end spoken language understanding",
      "arxiv": "arXiv:2104.07253"
    }
  ]
}