{
  "paper_id": "2312.01568v1",
  "title": "Multimodal Speech Emotion Recognition Using Modality-Specific Self-Supervised Frameworks",
  "published": "2023-12-04T01:49:24Z",
  "authors": [
    "Rutherford Agbeshi Patamia",
    "Paulo E. Santos",
    "Kingsley Nketia Acheampong",
    "Favour Ekong",
    "Kwabena Sarpong",
    "She Kun"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is a topic of significant interest in assistive robotics due to the need to equip robots with the ability to comprehend human behavior, facilitating their effective interaction in our society. Consequently, efficient and dependable emotion recognition systems supporting optimal human-machine communication are required. Multi-modality (including speech, audio, text, images, and videos) is typically exploited in emotion recognition tasks. Much relevant research is based on merging multiple data modalities and training deep learning models utilizing low-level data representations. However, most existing emotion databases are not large (or complex) enough to allow machine learning approaches to learn detailed representations. This paper explores modalityspecific pre-trained transformer frameworks for self-supervised learning of speech and text representations for data-efficient emotion recognition while achieving state-of-the-art performance in recognizing emotions. This model applies feature-level fusion using nonverbal cue data points from motion capture to provide multimodal speech emotion recognition. The model was trained using the publicly available IEMOCAP dataset, achieving an overall accuracy of 77.58% for four emotions, outperforming state-of-the-art approaches",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotions are a distinguishing feature of how humans interact with each other on a personal level. Emotions either introduce or eliminate ambiguity in communication, changing the meaning of what is being conveyed. Because emotions are essential in human-to-human communication, recent research has sought to replicate similar characteristics in machines, enabling them to be more effective in comprehending and communicating with people  [1] . Human emotion recognition by machines aids various research fields, such as virtual reality, gaming, robotics, and customer care operations. For instance, if an automated call center system is able to infer a customer's emotional state, the application may be able to provide more appropriate responses or send the call to a human operator straightaway  [2] . Emotion detection systems can be used in virtual reality and games to detect a player's emotional suffering, paving the way for more realistic, engaging, and immersive gaming experiences SA 5042, Australia. (Email: paulo.santos@flinders.edu.au)  [3] . Recognizing human emotion by a robot can also lead to more natural human-robot interactions  [4] .\n\nOver the years, research into emotion recognition has been performed by analyzing speech from low-level acoustic information. A machine-learning classifier is usually applied to map this input information to a specific emotion category. With the recent advancement of deep learning methods  [5] -  [7] , multiple efforts have been undertaken to learn emotion representations from audio signals using neural networks. However, the development of deep learning-based systems is frequently hampered by a lack of labeled data. In contrast to automated speech recognition (ASR) datasets, commonly used Speech Emotion Recognition datasets  [8] -  [10]  are restricted in complexity and small in size. Furthermore, systems trained on these datasets may not be generalizable to other domains, such as customer service centers  [2] . As a result, self-supervised pre-trained models such as wave-tovector (wav2vec)  [11]  and Bidirectional Encoder Representations from Transformers (BERT)  [12]  have been created to address the above-mentioned issue by learning from largescale audio and text datasets without the need of extensive labeling.\n\nTransformer-based models are recognized to solve the difficulties mentioned above. Since most pre-existing transformer-incorporated models utilized in this field are trained on large volumes of unlabeled data, and are only fine-tuned on labeled data for emotion recognition tasks. They do not require large datasets, with detailed labeling, during training in contrast to other deep learning models. Studies have also demonstrated that the features derived by these novel transformer-based algorithms outperform classic spectral-based features on Speech Emotion Recognition (SER)  [13] .\n\nThe following are the contributions of this work:\n\n1) The exploration of fine-tuning dual modality-specific transformer-based pre-trained architectures (using high-level speech audio and text representations as input) for multimodal emotion recognition; 2) The inclusion of nonverbal behavioral cues from the Motion Capture (MoCap) spectrum, which is universal and independent of cultural background, to create a versatile, robust, fair, and efficient multimodal emotion recognition system requiring low computational resources; 3) The investigation of how a simple fusion method for speech, text, and MoCap modalities may outperform more complex approaches when using Self Supervised\n\nLearning models with similar architectural properties. The proposed framework is a modular and extensible multimodal speech emotion recognition system. It consists of independently trainable sub-modules for speech, text, and motion capture emotion recognition. Each sub-module can be trained separately, and the absence of any modality or sub-module does not harm the overall framework. The prefinal layers must only be fine-tuned for combining the submodules, avoiding time-consuming complete model retraining. The framework leverages the advanced properties of pre-trained models to overcome accuracy loss and feature explosion during the feature extraction process.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Speech Emotion Recognition (SER) is a technique for recognizing and classifying a speaker's emotion from audio inputs. Neural network-based solutions have positively impacted and continue to influence contemporary advancements in SER studies. Many researchers have adopted deep learning algorithms for robust feature representation in different domains, and SER is no exception. This development can be attributed to the fast pace of solutions to visual task recognition  [14] . In  [15] , the authors introduced a comprehensive model incorporating convolutional layers and a multi-head selfattention mechanism, which utilized deep encoded linguistic information and audio spectrogram representation to perform emotion recognition in speech. Also, to address the class imbalance in their dataset, they carried out down-sampling and ensembling, further improving the SER accuracy. [16] presented a paradigm for essential sequence segment selection based on a Radial-Based Function Network (RBFN) with cluster similarities. These features were then sent into a deep Bidirectional Long Short-Term Memory (BLSTM), which learns the temporal information required to recognize the ultimate state of emotion. An RBF-based K-mean clustering algorithm was implemented in conjunction with the CNN model to extract more valuable features from spectrograms of the speech signal to achieve precise recognition performance. Consequently, a CNN & LSTM model with an attached attention model was used to investigate SER theories in  [17] . Additionally, the significance of acoustic context information was investigated, which provided the means to verify that even relatively insignificant acoustic cues encompass emotional information valuable to the overall project.\n\nLeonardo et al.  [18]  developed a shallow neural network using transfer learning to extract features from pre-trained wav2vec 2.0 models for the SER problem. By merging different layers in the wav2vec 2.0 model with the trainable weights learned concurrently with the downstream model, they found that integrating information from multiple levels resulted in better outcomes than using only the encoder output, as observed in earlier studies. In a subsequent study, the authors  [19]  presented two techniques, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT), that utilize wav2vec 2.0 for SER. They also introduced a pseudo-label task adaptive pretraining (P-TAPT) approach that addresses the mismatch challenge between pretraining and the target domain by continuing the relevant process on the target dataset and learning contextualized emotion representations, resulting in improved overall model performance.\n\nThe paper presents a new approach to multimodal speech emotion recognition that combines three modalities, including a nonverbal data point independent of cultural background, using feature concatenation. Unlike previous work, the modality components are treated independently, which means that the absence of one modality would not disrupt the entire system. In such a scenario, only the precursive layer of the system would need to be retrained, not the other modalities.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Methodology",
      "text": "We explored the use of modality-specific self-supervised pre-trained models plus the addition of a non-verbal data point (Motion Capture) for the task of multimodal speech emotion recognition. This consists of wav2vec2.0 (pretrained on speech corpus), BERT (pre-trained on text corpus), and CNN mechanism for the non-verbal data point. To guarantee a competently informed multimodal emotion fusion, the outcomes of all modalities were merged at the feature level using user-defined criteria. Fig.  1  illustrates the detailed implementation of our concept. It should be noted that the fused characteristics were subsequently routed via a fully connected layer for a decision or predicted emotional class.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Wav2Vec2.0 For Speech Modality",
      "text": "Wav2vec2.0 is a self-supervised pre-trained audio representation learning system. Its design involves a multi-level convolutional feature encoder f: X → Z that accepts raw audio X as input and generates latent speech embeddings Z 1 , ..., Z T for T time-steps. The encoder is made up of many blocks, the first of which is a temporal convolution succeeded by layer normalization  [20]  and the last of which is a Gaussian error linear units (GELU) activation function  [21] . The encoder's total stride specifies the number of time-steps T sent to the Transformer g: Z → C to construct representations C 1 , ..., C T aggregating data from the entire sequence  [12] . The output of the feature encoder is discretized into qt by applying a quantization module Z → Q to encode the targets, which involves distinguishing the genuine quantized latent speech representation for a masked time step from a set of distractors.\n\nWe adopted wav2vec2.0 as the acoustic encoder for speech emotion recognition by fine-tuning it with labeled emotiondependent data. A linear projection layer was added on top of contextual representations to translate them into emotiondependent classes. The model was improved by fine-tuning the CTC loss  [22] , and the identified emotion-dependent components were mapped to an emotion category using a majority vote for speech emotion recognition.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Bert For Text Modality",
      "text": "The text-related aspect of the study involved using deep contextual embeddings (BERT)  [12]  to calculate the vector representation of words. BERT is a pre-trained model with a Transformer encoder architecture  [12]  that uses a multi-layer Transformer encoder, with each layer consisting of a multihead self-attention sublayer accompanied by a position-wise fully linked feedforward network. The model employs an embedding table and a final fully connected layer to convert hidden vectors to an output softmax over the vocabulary. To achieve the desired results, the study fine-tuned a pre-trained BERT model with 12 transformer blocks, 12 attention heads, and 110 million parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Cnn-Lstm With Self Attention For Motion Capture Modality",
      "text": "The use of nonverbal data, specifically motion capture (MoCap), has been recognized as crucial for emotion analysis in human-centered cognitive computing. MoCap technology is capable of tracking and recording a person's movements in real time, making it a valuable source of information for emotion recognition. MoCap data from various submodes, such as facial, head rotation, and hand, are used in this multimodal task. Despite its relevance, nonverbal signals, including MoCap, have received relatively less attention compared to other modalities.\n\nWe collected data points for all feature values within the start and finish time values range and distributed them into 200 separate partitions. These arrays were averaged along the columns to yield 165 significant aspects of the face based on tracking the eyes, eyebrows, nose, mouth, and facial contour. The coordinate information of each point was collected by detecting essential points that express emotional information. The hand modality typically comprises a 2D index with 18 features and a 3D data set with 6 critical points for head rotation. Each sub-mode (facial, hand, and head rotation) was trained separately, and after combining them, a dimensional array of (200, 189) was incurred for each utterance.\n\nCNN-LSTM with Self-Attention architecture was used for emotion classification, where the incurred dimensional vector of (200, 189) was fed into the network. The CNN-LSTM-Attention network comprises 2D convolutional layers with activation, max-pooling, dense layers, and LSTM layers accompanied by a self-attention layer. The CNN layers extracted distinctive features from the motion time series. The convolution operation calculates the convoluted output z(i, j) from the input grid value x(i, j) and convolution kernel y(i, j). The convoluted output equation is given as:\n\nwhere a and b are the dimensions of the kernel filter and z(i, j) is an element of the two-dimensional convoluted output matrix Z. The structural architecture of our motion capture is further detailed here. The first convolution layer accepts a two-dimensional array (200, 189) as input. This layer comprises filters with kernel sizes and strides that use the same padding strategy for each convolution operation. The input matrix yields an output matrix of the same size. After the convolution procedure, there is an activation layer, followed by a max-pooling layer then a dropout layer with a rate of 0.2. The dropout layer output is then fed into subsequent convolution blocks of the same structure without the input shape. The procedure is repeated a few more times until the output from the last convolution layer is reshaped and fed into an LSTM block coupled with a self-attention layer.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Multimodal Fusion",
      "text": "The fusion process is critical in multimodal speech emotion recognition. A unique fusion process was considered to increase the model's performance in this study. The combination of three modalities (Speech → Sp, Text → Tx, and MoCap → MC) was used to transform the three unimodal systems into a single multimodal emotion system. Four alternative combinations were tested [Sp, Tx], [Sp, MC], [Tx, MC], and [Sp, Tx, MC], and the characteristics of each unimodal phase were extracted independently. The association rule was used to apply the fusion approach. The most powerful speech, text, and MoCap characteristics containing temporal and spatial information were concatenated. Each description can be represented in the following way: speech vector Sp = {sp 1 , sp 2 , ..., sp t }, textual vector T x = {tx 1 , tx 2 , ..., tx t }, and MoCap vector M C = {mc 1 , mc 2 , ..., mc t }, and the final multimodal emotion feature (MEF) vector description being:\n\nIn the feature-level fusion or Shallow-Fusion  [23]  technique used in the multimodal speech emotion recognition paradigm, the concatenated embeddings (MEF) from the three modalities (Speech, Text, and MoCap) were processed by a classification head consisting of a fully connected layer that outputs the relative probability between different emotion classes, followed by a softmax function to capture the relationship between features from different modalities. This fusion technique was chosen for its flexibility and the direct connection between emotional features from multiple modalities and the final decision, as well as for significantly retaining the necessary feature information for the final decision.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experiments A. Dataset",
      "text": "The entire set of experiments was carried out using the publicly available Interactive Emotional Dyadic Motion Capture (IEMOCAP) multimodal and multispeaker database  [8] , which contains five recorded sessions of conversations (Session 1-5). IEMOCAP is a 12-hour audiovisual dataset with video, speech, motion capture of the face, and text transcriptions rendered by ten professional actors. To follow existing literature, we chose utterances from only four emotional classes from the database, totaling 5531 utterances. 1708 neutral, 1636 excited (including happy), 1103 angry, and 1084 sad.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Setup And Evaluation Procedure",
      "text": "The experiments used k-fold leave-one-speaker-out crossvalidation, with the first four sessions containing 80% of the data for training and the fifth session containing 20% of the data for testing. This method ensured speaker independence in the prediction, and a total of 5531 conversations were used in the experiment.\n\nThe simulations were done using TensorFlow, and two optimizers, Adam & Stochastic Gradient Descent, were used for training with a learning rate of 1e-5 and default exponential decay rate of moment estimates. The batch size varied between 4 and 100 during training. The models were trained in a distributed mode using Tesla P100 52GB GPUs. Further details on this will be provided in the upcoming sections.\n\nOur findings are presented in the form of classification accuracy (Equation  3 ). Furthermore, confusion matrices aided in identifying the errors caused by our classification model during prediction, allowing us to assess the model's validity. The following are provided using the words true positive (TP), false positive (FP), true negative (TN), and falsenegative (FN).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Training",
      "text": "The experiment's training phase was supervised, and the pre-trained models were trained unsupervised. The model architecture has three modalities, and each was trained separately before combining them for the multimodal task.\n\n1) Speech Emotion Recognition Model: For speech modality, we experimented by fine-tuning the pre-trained wav2vec2.0-base model with 12 transformer blocks and 7 convolutional blocks, and each block has 512 channels. The model was the backbone of the speech phase and was based on the TensorFlow Hub repository  [11] . The fine-tuning ensured that the input was sampled at 16kHz, similar to the pre-trained model. Unlike previous work, which added a Language Modeling (LM) head, the model predicted emotional labels rather than transcribing speech into text. The model signature takes static sequence lengths of 246000, so constants and hyper-parameters were created with an audio max length of 246000. The pre-trained layer was wrapped in successive NN dense layers with ReLU activation, followed by a four-unit classification layer with Softmax activation. The stochastic gradient descent optimizer was utilized with a learning rate of 1e-5, and 1 GPU (Tesla P100) was used for each run with a batch size of 4.\n\n2) Text Emotion Recognition Model: Fine-tuning BERT for our text phase was simple since the Transformer's selfattention mechanism allowed BERT to simulate downstream tasks by switching out the relevant inputs and outputs. As the backbone for our text, we utilized the BERT base uncased, which contains 12 layers, 12 heads, and 110M parameters. Using the pre-trained BERT, a contextualized word embedding was constructed. Our present model was then given the embeddings. The data is shaped as a 768dimensional vector with a maximum sequence length of 124, followed by dense layers, and the last layer is a 4-unit output layer with softmax activation. Our model's learning rate was defined as 1e-5 and tuned with Adam optimizer.\n\n3) Motion Capture Emotion Recognition Model: For the MoCap modality, each sub-modality (facial, head, and hand) was trained separately with up to three deep learning models before being combined into a single motion time series. The head sub-mode included three architectures: model 1 (2D Conv), model 2 (2D Conv-LSTM), and model 3 (2D Conv-LSTM-Attention). All three comprised a stack of 2D convolutional layers, with subsequent maxpooling and dropout layers, followed by dense layers and a final output layer with softmax activation. Adam optimizer with a learning rate of 1e-5 was used in initialization. Model 2 and Model 3 included LSTM and LSTM-Attention blocks, which required reshaping the output from the last convolutional layer before feeding into the LSTM blocks. The output from the LSTM and LSTM-Attention blocks was flattened and passed through dense layers. These three models were used for feature learning with the Hand and Facial sub-modes before merging into a single motion time series.\n\nThe concatenated motion time series was trained using a 189-feature-length 2D Conv-LSTM-Attention architecture. The architecture had convolutional blocks with kernel sizes of 3 and comprised five layers with 128 filters, followed by ReLU activation, max-pooling, and a dropout layer. The output was reshaped and fed into a 128-unit LSTM block coupled with a self-attention layer. The output from the LSTM block was flattened and fed into two dense layers before eventually feeding into a four-neuron dense layer with softmax activation for classification.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "4) Multimodal Emotion Recognition Model:",
      "text": "The multimodal phase combined the three modalities assumed in this work. To get the predictions, we concatenated all three vectors at the feature level and sent the concatenated features through a dense layer. Finally, we sent the dense layer output through a classification head, which featured a fully connected layer followed by a softmax function. To comprehend our work better, we investigated additional comparable concatenations such as (Text + Speech, Text + MoCap, and Speech + MoCap). The loss function was the sparse categorical cross-entropy, and Adam was the optimizer. In every iteration, the model was changed in response to the output of the loss function. Finally, we sent the concatenated embedding through a classification head with a fully connected layer that outputs logits followed by a softmax function.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Results And Analysis",
      "text": "The results of our experiments are summarized in this section. To better understand how important each modality (speech, text, motion capture) was to our multimodal emotion experiment, we trained each modality classifier individually, pairwise, and finally merged all three modalities. Experiments were performed on the IEMOCAP dataset using the leave-one-speaker-out cross-validation, which trained the model during Sessions 1-4 and tested it in Session 5. Our system used emotional audio, text, and MoCap data from IEMOCAP, two task-specific transformer-based models, and a convolutional neural network. The tables below represent the findings and methods utilized in this study.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model",
      "text": "Validation Accuracy Speech-only LSTM + Attention  [24]  GRU  [25]  Bi-LSTM  [26]  GRU + Attention  [13]  Bi-LSTM + Attention  [",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Comparison Between Unimodal Inputs In Fine-Tune State",
      "text": "Table I illustrates two essential points. It compares the performance of unimodal inputs and fine-tuned approaches on the IEMOCAP dataset. In our experiments, we employed the high-level features of wav2vec and BERT as the sequence representation for our speech and text respectively. As indicated in Table  I , text-only outperformed speech and MoCap exclusively. A possible reason for this result could be the presence of strong emotional cues in the linguistic framework because there are not many intricate circumstances in this dataset where speech is more effective. However, when comparing text-only results to our best-performing multimodal model, we detected a considerable improvement, emphasizing the relevance of multimodality.\n\nWe produced the confusion matrices shown in Fig.  2  and 3 for our unimodal text and speech. In single mode, there was an apparent emotional disorientation. In both voice and text mode, the class excited was misread as neutral, with a more significant misjudged prediction percentage of 30 in text mode compared to 20% in speech. We also observed 70-80% accuracy prediction in three emotion categories (neutral, anger, and sad), with the excited category performing the worst in both the speech and text models. The high prediction accuracy observed in the emotional classes might be due to the adaptability of pre-trained architectures employed for feature extraction and prediction in text and speech. We also observed complementarity between speech and text. As seen in Fig.  5 , their combination improved the accuracy of most emotion categories while reducing misjudgments between emotions.\n\nWe also explored whether using wav2vec embeddings for speech emotion recognition had any advantages. We used hyperparameter tweaks to train our speech, such as an Adam optimizer with learning rates of 5e-5 & 1e-5 and an SGDoptimized architecture with a learning rate of 1e-5. However, we report the findings for the SGD setup, which produced an absolute accuracy of 63.99% when wav2vec features were  The MoCap-only section described concatenating three subsections into a single modality to generate our MoCap input. We trained and evaluated each subsection using several deep learning architectures, and their results are displayed in Table  II . It helped us comprehend the nature of our motion capture data. Since the separate subsections (facial, head, and hand) do not have enough data to operate as independent data points, the results obtained from training each subsection were relatively poor. The combined output exceeded the 2D Conv baseline models used in training the separate subsections, demonstrating the efficacy of the proposed 2D Conv-LSTM-Attention for the concatenated motion time series over the individual sub-modes of our motion capture data.\n\nIn our MoCap experiment's confusion matrix, Fig.  4 , we discovered that the neutral emotional class had the best-  The majority of the downsides experienced are linked to the dataset. We discovered that the Motion Capture markers could not pick up information at several data collecting points, mainly attributed to occlusions or ambiguity. When a significant fraction of markers are missing for lengthy periods, most approaches for filling missing markers may soon become inefficient and yield unacceptable results. The excited class had the lowest prediction accuracy of 50%.  speech and text findings, placing less emphasis on the speech model is acceptable because the text model compensates for the majority of emotional perplexity in the combined phase. We hypothesize that this is due to the certainty with which the text model received its scores. From Table  III , we can also see that all paired combinations containing the text modality obtained a result significantly higher than the pairwise version (speech + MoCap) that did not include the text. It is possible to conclude that merging distinct modalities contributes sensibly to the emotion recognition task because our pairwise findings outperformed our speechonly, text-only, and MoCap-only results. This section further evaluates the efficacy of the proposed approach, showing that combining the uni-modal models at the feature level can improve performance. Table IV compares the proposed approach with other state-of-the-art multimodal (speech-text-mocap) emotion recognition approaches. To ensure equitable comparison, all the approaches are based on the leave-one-speaker-out (LOSO) cross-validation configuration, using the mean accuracy as the metric and analogous preprocessing of the IEMOCAP data as executed in this work. These results show that the performance of our model, using speech, text, and MoCap features, was superior to that of most baseline approaches, suggesting that it is advantageous to combine wav2vec 2.0, BERT, and CNN-LSTM with a Self-Attention module. It is worth noting that the improvement of our mode is not limited to the effect of the fusion approach. Furthermore, an added contributor that influenced the result is the integration of prosodic data, which holds significance in the recognition of emotions.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "B. Comparison Of The Pairwise Fusions",
      "text": "In addition, our multimodal model was evaluated using a produced confusion matrix shown in Fig.  5 . The multimodal model has a more significant recognition accuracy for anger and excited and lower recognition accuracy for neutral and sad. Because the accuracy results obtained on our unimodal models in Table  I  above are marginal, we argue that our multimodal model compensates for and benefits from the deficiencies of the single modalities. Despite extensive emotion recognition capabilities, the multimodal model made a few mistakes, misclassifying 20% of the emotional excitement class as neutral and 30% as a split between anger, sadness, and excitement. This might be due to some disparity in our dataset since some specific emotional categories had more data than others, or it could have been due to the simple fusion type. We intend to investigate different fusion approaches in the future to see how they differ.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This paper presented a multimodal emotion recognition system that addresses the downstream issue of multimodal speech recognition using modality-specific pre-trained structures and nonverbal behavioral cue data points acquired through motion capture. We fine-tuned Wav2vec 2.0 and BERT architectures for speech and text modalities, respectively, and utilized three sub-modes for the MoCap modality. The resulting system significantly improves emotion recognition by fusing unimodal modality information.\n\nWe experimented with individual and paired modalities before concatenating the three modalities (speech, text, and MoCap) at the feature level, which improved the performance of their emotion recognition system and reduced data sparsity. Merging the modalities compensated for one another's flaws and enhanced the system's performance while reducing data sparsity. The best fusion configuration achieved a mean accuracy of 77.58% on the IEMOCAP dataset for leave-onespeaker-out (LOSO) cross-validation.\n\nIn the future, we intend to explore a pre-trained approach for the motion capture data point to avoid training from scratch, which requires excessive computational resources. We aim to use the pre-trained model's advantage to extract more valuable emotion features from MoCap, combined with the two architectures for speech and text used in the study, to improve the multimodal model's performance. Other fusion strategies that best suit features extracted from different pretrained architectures will also be considered. Additionally, cross-database validation will be used to assess the methods' resilience further.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the",
      "page": 3
    },
    {
      "caption": "Figure 1: The structure of our multi-modal model.",
      "page": 4
    },
    {
      "caption": "Figure 5: , their combination improved the accuracy of most",
      "page": 6
    },
    {
      "caption": "Figure 2: Text only normalized confusion matrix on model test accuracy",
      "page": 7
    },
    {
      "caption": "Figure 3: Speech only normalized confusion matrix on model test accuracy",
      "page": 7
    },
    {
      "caption": "Figure 4: MoCap only normalized confusion matrix on model test accuracy",
      "page": 7
    },
    {
      "caption": "Figure 5: Multimodal normalized confusion matrix on model test accuracy",
      "page": 8
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey on the development of intelligent robots in speech emotion recognition",
      "authors": [
        "Q Gao",
        "H Ning",
        "B Du"
      ],
      "year": "2021",
      "venue": "2021 International Wireless Communications and Mobile Computing (IWCMC)"
    },
    {
      "citation_id": "2",
      "title": "Mutual impact of acoustic and linguistic representations for continuous emotion recognition in call-center conversations",
      "authors": [
        "M Tahon",
        "M Macary",
        "Y Estève",
        "D Luzzati"
      ],
      "year": "2021",
      "venue": "Mutual impact of acoustic and linguistic representations for continuous emotion recognition in call-center conversations"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition based on speech segment using lstm with attention model",
      "authors": [
        "B Atmaja",
        "M Akagi"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Signals and Systems (ICSigSys)"
    },
    {
      "citation_id": "4",
      "title": "Comparative analytical survey on cognitive agents with emotional intelligence",
      "authors": [
        "R Zall",
        "M Kangavari"
      ],
      "year": "2022",
      "venue": "Cogn. Comput"
    },
    {
      "citation_id": "5",
      "title": "Deep learning in neural networks: An overview",
      "authors": [
        "J Schmidhuber"
      ],
      "year": "2015",
      "venue": "Neural networks : the official journal of the International Neural Network Society"
    },
    {
      "citation_id": "6",
      "title": "Deep learning",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville"
      ],
      "year": "2015",
      "venue": "Nature"
    },
    {
      "citation_id": "7",
      "title": "A review on speech emotion recognition using deep learning and attention mechanism",
      "authors": [
        "E Lieskovska",
        "M Jakubec",
        "R Jarina",
        "M Chmulik"
      ],
      "year": "2021",
      "venue": "Electronics"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "E Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "9",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "10",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "ArXiv"
    },
    {
      "citation_id": "11",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Rahman Mohamed",
        "M Auli"
      ],
      "year": "2006",
      "venue": "ArXiv"
    },
    {
      "citation_id": "12",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "ArXiv"
    },
    {
      "citation_id": "13",
      "title": "Transformer based multimodal speech emotion recognition with improved neural networks",
      "authors": [
        "R Patamia",
        "W Jin",
        "K Acheampong",
        "K Sarpong",
        "E Tenagyei"
      ],
      "year": "2021",
      "venue": "2021 IEEE 2nd International Conference on Pattern Recognition and Machine Learning (PRML)"
    },
    {
      "citation_id": "14",
      "title": "Recent advances in deep learning techniques and its applications: An overview",
      "authors": [
        "A Hazra",
        "P Choudhary",
        "M Singh"
      ],
      "year": "2020",
      "venue": "Recent advances in deep learning techniques and its applications: An overview"
    },
    {
      "citation_id": "15",
      "title": "Deep encoded linguistic and acoustic cues for attention based end to end speech emotion recognition",
      "authors": [
        "S Bhosale",
        "R Chakraborty",
        "S Kopparapu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Clustering-based speech emotion recognition by incorporating learned features and deep bilstm",
      "authors": [
        "M Mustaqeem",
        "S Sajjad",
        "Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition",
      "authors": [
        "M Jalal",
        "R Milner",
        "T Hain"
      ],
      "year": "2020",
      "venue": "Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "19",
      "title": "Exploring wav2vec 2.0 finetuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "20",
      "title": "Layer normalization",
      "authors": [
        "J Ba",
        "J Kiros",
        "G Hinton"
      ],
      "year": "2016",
      "venue": "ArXiv"
    },
    {
      "citation_id": "21",
      "title": "Gaussian error linear units (gelus),\" arXiv: Learning",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus),\" arXiv: Learning"
    },
    {
      "citation_id": "22",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "23",
      "title": "Jointly fine-tuning \"bert-like\" self supervised models to improve multimodal speech emotion recognition",
      "authors": [
        "S Siriwardhana",
        "A Reis",
        "R Weerasekera",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "24",
      "title": "Multi-modal emotion recognition on iemocap with neural networks",
      "authors": [
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multi-modal emotion recognition on iemocap with neural networks"
    },
    {
      "citation_id": "25",
      "title": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network",
      "authors": [
        "N.-H Ho",
        "H.-J Yang",
        "S Kim",
        "G Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "26",
      "title": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks",
      "authors": [
        "N Krishnad",
        "A Patil"
      ],
      "year": "2020",
      "venue": "Multimodal emotion recognition using cross-modal attention and 1d convolutional neural networks"
    },
    {
      "citation_id": "27",
      "title": "Multimodal speech emotion recognition based on aligned attention mechanism",
      "authors": [
        "M Liu",
        "N Xue",
        "M Huo"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Unmanned Systems (ICUS)"
    },
    {
      "citation_id": "28",
      "title": "Group gated fusion on attention-based bidirectional alignment for multimodal emotion recognition",
      "authors": [
        "P Liu",
        "K Li",
        "H Meng"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "29",
      "title": "A multi-scale fusion framework for bimodal speech emotion recognition",
      "authors": [
        "M Chen",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "A multi-scale fusion framework for bimodal speech emotion recognition"
    },
    {
      "citation_id": "30",
      "title": "Multimodal emotion recognition with high-level speech and text features",
      "authors": [
        "M Makiuchi",
        "K Uto",
        "K Shinoda"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    }
  ]
}