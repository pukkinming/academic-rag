{
  "paper_id": "2202.06509v3",
  "title": "Pr-Pl: A Novel Transfer Learning Framework With Prototypical Representation Based Pairwise Learning For Eeg-Based Emotion Recognition",
  "published": "2022-02-14T06:44:05Z",
  "authors": [
    "Rushuang Zhou",
    "Zhiguo Zhang",
    "Hong Fu",
    "Li Zhang",
    "Linling Li",
    "Gan Huang",
    "Yining Dong",
    "Fali Li",
    "Xin Yang",
    "Zhen Liang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "I. Introduction 1 Equal Contributions.",
      "text": "A FFECTIVE computing is a fast growing interdisciplinary research field and is attracting researchers' attention from different areas including computer science, neuroscience, psychology, and signal processing  [1] . Recently, electroencephalography (EEG) based emotion recognition has become an increasingly important topic for affective computing and human sentiment analysis  [2] ,  [3] . A proper design of EEGbased emotion recognition models is helpful for facilitating the data processing, benefiting discriminant feature characterization, and lightening the model performance. Currently, there exist two main critical issues in EEG-based emotion recognition. One is individual differences: how to build a generalized affective computing model which could tolerate the remarkable individual differences in the simultaneously collected EEG signals; and another is noisy label learning: how to train a reliable and stable affective computing model which is less reliant on the subjective feedback.\n\nIn recent years, more and more researchers have focused on applying transfer learning methods to alleviate the individual differences in EEG signals  [4] -  [9]  and improve feature invariant representation  [10] -  [12] . Considering the individuals with and without labels (termed as source domain and target domain), transfer learning tries to minimize the distribution difference between the source and target domains by approximately satisfying the assumption of independent and identical distribution and can consequently realize a higher recognition performance on the target domain. Through a domain-shifting strategy, the invariant feature representations across different domains are learned and the relationships among the learned features, data distribution, and labels are explored. For example, Li et al.  [5]  proposed a multisource transfer learning method with two transfer learning stages. In the first stage, appropriate samples were selected from the existing source domain. In the second stage, a style transfer mapping was implemented to alleviate the differences between the selected source samples and the unknown target samples. The results showed the proposed transfer method outperformed the nontransfer method with an improvement of 12.72% on the public SEED database  [13]  with a three-class classification problem (negative, neutral, and positive). Inspired by neuroscience findings that different emotions would lead to different brain reactions, Li et al.  [6]  proposed a novel R2G-STNN network to integrate the EEG spatial-temporal dynamics at the local and global brain areas and realize an efficient emotion recognition performance together with a domain shift learning. More details about current EEG-based emotion recognition models with the transfer learning algorithms are presented in Section II.\n\nFor video-evoking EEG-emotion experiments, subjects may not always be able to accurately react to the intended emotions, and at the same time may not be able to accurately describe and feedback on their emotional changes. This would bring label noise to the emotional information annotation of EEG samples and further lead to a negative impact on the model performance  [14] . To tackle this issue, Zhong et al.  [8]  developed an emotion-aware distribution learning method (RGNN), in which they blurred the label information by changing the one-hot label representation (1, 0, 0) to 1 -2 3 , 2 3 , 0 and trained the model to be less sensitive label noise. However, the model performance would greatly rely on the selection of value, and an optimal value selection could be different for different databases and different individuals. Current EEG-based emotion recognition models are mainly based on pointwise learning, which heavily relies on precisely labeled data. Contrarily, pairwise learning makes it possible to model the relative associations between pairs of instances and to efficiently encode the proximity among samples with less reliance on labeling. Thus, pointwise learning has achieved tremendous success in a number of real-world applications  [15] -  [18] .\n\nTo further improve the effectiveness and generalizability of EEG-based emotion recognition models and eliminate the negative effects from individual differences and label noises, in this paper, we formulate the emotion recognition tasks as a pairwise learning problem and propose a novel transfer learning framework with prototypical representation based pairwise learning (which is termed as PR-PL below). Here, we model the relative relationship between pairs of EEG samples in terms of prototypical representations, which is advantageous to pointwise learning when the labeling task is difficult and even the provided labels are wrong labels  [19] . The major novelties of the proposed PR-PL are summarized as follows.  (1)  We formulate emotion recognition as pairwise learning to replace the classifier and greatly alleviate the label dependence on emotion labels. The pairwise learning provides us an alternative way to measure whether two EEG signals belong to the same emotion category without the reliance on the precise labeling information. The extensive experimental results on two well-known emotional databases (SEED  [13]  and SEED-IV  [20] ) prove the proposed PR-PL is a more accurate model than the state-of-the-arts for solving the EEG-based emotion recognition tasks under different application environments (cross-subject cross-session, cross-subject single-session, within-subject cross-session, and within-subject single-session).  (2)  We propose a novel prototypical learningbased adversarial discriminative domain adaptation method to explore latent variables of emotion categories, encode the semantic structure of EEG data, and learn subject-generalized prototypical representations for emotion revealing across individuals. The characterized prototypical representations show a high feature concentration within one single emotion category and a high feature separability across different emotion categories. (3) Different from the existing transfer learning methods that only focus on feature separability in the source domain, we consider the feature separability of both source and target domains through the end-to-end domain adversarial training to further enhance the model effectiveness and generalizability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "The existing EEG-based emotion recognition models with the transfer learning algorithms can be generally categorized into two types.\n\n(a) Non-deep transfer learning models. Pan et al.  [21]  proposed a transfer component analysis (TCA) algorithm to reduce the marginal distribution difference between the source and target domains, in which the transfer information was learned in a reproducing kernel Hilbert space through maximizing mean discrepancy. Zheng and Lu  [22]  introduced two types of subject-to-subject transfer methods to deal with the challenge of the individual differences in EEG signal processing. One was to explore a shared common feature space underlying source and target domains using TCA and kernel principal analysis (KPCA), and another was to construct multiple personalized classifiers on the source domain and map the classifier parameters to the target domain using transductive parameter transfer (TPT). These non-deep transfer learning strategies show the possibility to bridge the discrepancy across two domains with improved performance on the target domain. However, due to the small capacity and low complexity, the model accuracy and stability are still limited, which fails to satisfy the requirements of affective brain-computer interfaces (aBCI) in practical applications.\n\n(b) Deep transfer learning models. Most of the existing affective models are based on deep transfer learning methods built with domain-adversarial neural network (DANN) proposed in  [23] . The main idea of DANN is to find a shared feature representation for source and target domains with indistinguishable distribution differences and also maintain the predictive ability of the estimated features on the source samples for a specific classification task. Li et al.  [24]  was the first to introduce DANN in aBCI. Benefiting from the powerful feature representation ability of deep networks and the high efficiency of adversarial learning in distributed adaptation, the results showed that DANN based aBCI system was superior to other methods. The following aBCI systems could be considered as a series of DANN-based models, which generally start from two directions to improve the DANN performance in solving EEG-based emotion recognition tasks. Although the above models have achieved higher accuracies compared to the original model with DANN in emotion recognition tasks, there still exist three major technical challenges. First, the learned feature representation is susceptible to noise interference from both source and target domains and would further affect the model generalizability  [29] ,  [30] . Second, the existing models only focus on the feature separability in the source domain but ignore the feature separability in the target domain. The current DANN-based models mainly concern the emotion classification loss in the source domain, which would lead to the over-fitting of source domain data and the decrease of classification ability on target domain data. Third, the existing algorithms largely rely on a large amount of labeled source domain data. However, in practical EEG\n\nHere, X s i and X t i are EEG samples, and Y s i and Y t i are the corresponding emotion labels. To make the narrative clearer, the frequently used notations are summarized in Table  I . As shown in Fig.  1 , the proposed PR-PL includes three losses (domain adversarial loss, pairwise learning loss on source domain, and pairwise learning loss on target domain) and two main parts (prototypical representation and pairwise learning). In the prototypical representation, three types of features are defined. The characterized features f (X s ) or f (X t ) from the EEG samples are termed as sample features, which are forced to be as indistinguishable as possible from the source and target domains. Under an assumption that any emotion could be represented by a prototype via prototypical learning, the prototype features of each emotion category are learned based on the sample features f (X s ) and Y s from the source domain. The interaction relationships between the sample features and prototype features are measured and the interaction features are characterized which will be used in the following pairwise learning. In the pairwise learning, the pair relationships on both source and target domains are explored. As the information about Y t are unknown during model training, an adaptive thresholding method is developed for valid pair selection and pseudo label generation.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "A. Sample Feature Extraction",
      "text": "To make both source and target data satisfy the assumption of independent and identical distribution and obey the same distribution, we characterize the sample features based on a domain adversarial training introduced in DANN. Here, the distribution difference between the source domain and the target domain is alleviated and the sample features with domain invariant properties are characterized. Through this process, the individual differences in EEG signals could be alleviated and the generalization ability of the models could be improved  [6] ,  [25] -  [27] . Specifically, the domain difference\n\n(1) In the training process, we adopt the end-to-end training method  [23] , and implement the domain adversarial training by introducing a gradient reversal layer. The feature extractor f (•) maximizes the classification ability to enhance emotion recognition performance and at the same time, the discriminator d(•) minimizes the domain discrimination to reduce the distribution difference between the source domain and target domain. The final domain adversarial training objective function is defined as\n\nwhich is termed as domain adversarial loss in Fig.  1 . Here, L s classifier (θ f ) is the classification loss to measure the classification ability in source domain. In this paper, L s classifier (θ f ) will be realized by pairwise learning on source and target domains introduced in Section III-D and III-E below. L disc (θ f , θ d ) is the adversarial loss for the discriminator to be trained to distinguish the sample features characterized from source and target domains. θ f and θ d are the parameters of f (•) and d (•). λ is a balanced hyperparameter for ensuring the stability of domain adversarial, which is given by a exponential growth method as\n\nHere, p is a factor related to the training round, given by a ratio of the current training round to the maximum training round.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Prototype Feature Extraction",
      "text": "We assume that there exists a prototype for each emotion category. Through prototypical learning, the prototype features are learned to indicate the representation property of every single emotion category. Based on the sample features extracted from different subjects under different emotions, we could consider these sample features are distributed around the prototype features. In other words, for each emotion category, the prototype features could be considered the \"center of mass\" of all the sample features. From the perspective of a probability distribution, the prototype feature of an emotion category can be regarded as the mean value of the sample feature distribution of the emotion, and the variance of the distribution is caused by the non-stationary EEG, including but not limited to individual differences. Assume that the sample features under an emotion category c obey the Gaussian distribution N µ c , σ 2 c . The prototype features of the emotion category c could be calculated as the mean vector µ c of the distribution. For the source domain data {X s , Y s } = {(x i , y i )} Ns i=0 , the corresponding sample features are characterized by the feature extractor f (•), given as f (x i ) (defined in Section III-A). The prototype feature vector of the emotion category c can be calculated by averaging all the sample features that belong to this category, given as\n\nwhere\n\nare a collection of source domain data belonging to the emotion category c. |X c s | are the corresponding sample size in this emotion category. In other words, µ c could be expressed as the centroid of the sample features of X c s . The mean value calculation is a widely used, simple, and effective noise reduction strategy, which can make the prototype features stronger than the sample features and help to alleviate the problem of the traditional DANN network being susceptible to related noise interference  [31] . It is worth noting that since the calculation of the prototype features needs to use emotional label information, we only use the source domain data here for prototype feature extraction.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Interaction Feature Extraction",
      "text": "The traditional DANN extracted domain-invariant shared feature representations could be easily contaminated by the shared and related noises in source and target domains. In this paper, we introduce a bilinear interaction to measure the interaction relationships between the sample features and prototype features and extract the interaction features for the following pairwise learning. For a given d-dimensional sample feature f (x i ), its interaction relationship to a certain prototype feature µ ( c) can be measured by a bilinear transformation h(•), defined as\n\nwhere S ∈ R d×d is a bilinear transformation matrix with trainable parameters and is not restricted by symmetry or positive definiteness. Suppose there are total n emotion categories, the interaction measurement between a certain sample feature f (x i )\n\nT and different prototype features can be represented as\n\nwhere {µ i } n i=1 are all the prototype features. An introduction of the activation function (softmax) here is to add nonlinear advantages to the interaction measurement, enhance the feature representation ability, and at the same time allow the feature vector l to have category prediction capabilities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Pairwise Learning On Source Domain",
      "text": "Traditional pointwise learning algorithms often regard the feature vector l as the predicted label of the sample feature x i and use the cross-entropy loss function to match the label y i of x i based on supervised training. Then, if the sample features of one EEG trial match the prototype features of jth emotion category the most, the EEG trial will be assigned to the jth emotion category. However, this type of pointwise learning only focuses on the relationship between sample features and prototype features and ignores the relationship between different sample features. For example, the sample features belonging to different emotion categories should be separated from each other, and the sample features belonging to the same emotion category should be gathered together. To tackle this issue, we introduce pairwise learning to capture the inherent relationship of samples.\n\nFor the source domain data {X s , Y s } = {x s i , y s i } Ns i=1 , the corresponding loss function for pairwise learning is defined as\n\nwhere g x s i , x s j ; θ is the similarity measurement of the samples x s i and x s j , with the parameter of θ. According to the assumption of pairwise learning, if y s i = y s j , then r s ij = 1; otherwise r s ij = 0. The loss function L (•) is a difference calculation of r ij and g x s i , x s j ; θ , given by a two-category cross-entropy loss as\n\nIn the training process, the label information of source domain data is used to define r in a supervised manner. In other words, based on the given information of Y s , if two samples belong to the same emotion category, then r = 1, otherwise r = 0. A supervised r can ensure the stability of the training process and the generalization ability of the model. The next key question is how to define a proper g x s i , x s j ; θ to compute the similarity between x s i and x s j in terms of the characterized interaction features (termed as l i and l j ). To make the similarity results locate in the range of [0, 1] and extract better and more robust feature representations for subsequent emotion recognition, we add a norm restriction on l as\n\nThe similarity of l norm i and l norm j is calculated as the cosine similarity, given as\n\nwhere • refers to inner product operation. As stated in Chang et al.  [32] , the above-mentioned norm restriction can make the vector l have a clustering function, and the elements in the vector represent the probability that the feature belongs to a certain category cluster. Overall, the objective function of pairwise learning on the source domain is defined as\n\nwhich is termed as supervised pairwise loss in Fig.  1 .\n\nHere, θ = {θ f , S}, θ f is the parameter of feature extractor f (•), and S is the defined bilinear transformation matrix for interaction feature extraction. Besides, to avoid redundant feature extraction, a soft regularization R is introduced with a weight parameter of β, which is defined as\n\nHere, each row of the matrix P refers to the prototype feature belonging to one emotion category, • F is a F norm of the matrix, and I is an identity matrix. The above loss function (Eq. 11) could be interpreted as a clustering loss, instead of emotion category classification loss. The main optimization goal is to gather the EEG samples that may belong to the same emotion category and separate the EEG samples that do not belong to the same category. The vector l is the characterized interaction feature for mapping to a non-linear informative feature space by measuring the interaction relationship between the sample features and all the available prototype features.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Pairwise Learning On Target Domain",
      "text": "In this paper, besides of source domain, we also introduce the pairwise learning on the target domain to improve the feature separability in the target domain, as\n\nwhich is termed as unsupervised pairwise loss in Fig.  1 .\n\nHere, l t is the interaction features of the target domain data characterized in Eq. 6. The scalar r t ij symbolizes the pairing relationship of the samples in the target domain. Since the label information of the target domain is completely missing in the training process, we cannot accurately obtain the pairing relationship as the source domain. To address this issue, we introduce an adaptive thresholding method to generate the valid pseudo labels and define the pairing relationship of the target domain data. Suppose that r t ij is defined as\n\nwhere τ u and τ l are the upper and lower bounds to select valid pairs with high confidence for unsupervised pairwise learning (valid pair selection). Here, if the calculated pairwise similarity is higher or equal to the defined upper bound (τ u ), then the corresponding pseudo label r t ij would be assigned to 1; while if the calculated pairwise similarity is lower than the defined lower bound (τ l ), then the corresponding pseudo label r t ij would be assigned to 0. For the other pairs that do not meet the threshold requirement, we would consider that the model is uncertain about whether the sample pair is paired or not and consider these pairs as invalid results. In order to prevent incorrect optimization, the invalid pairs would be temporarily excluded and will not participate in training and loss calculations at the current training round. In the early training stage, the classification performance in the target domain is not good enough. In order to ensure the training stability, we set a strict upper threshold (τ u ) and a lower threshold (τ l ) and exclude most of the pair results in the target domain. Along with the enhancement of model performance in the target domain, we can gradually lower the upper threshold and raise the lower threshold and allow more samples to participate in the training part. In other words, along with the increase in training steps, more pair samples in the target domain will be included for model learning. Here, we form a non-linear dynamic update for thresholding, as\n\nwhere τ t h represents the upper threshold of the current training round t, τ t l represents the current lower threshold, and maxepoch is the maximum training round. Based on the given initial values τ 0 h and τ 0 l , the calculations of τ t h and τ t l could be considered as non-linear changes with respect to the training rounds as\n\nIn all, combining the domain adversarial loss (Eq. 2), the pairwise learning loss in the source domain (Eq. 11, and the pairwise learning loss in the target domain (Eq. 13), the final objective function of PR-PL could be given as follows:\n\nwhere γ is a hyperparameter to control the importance of the pairing loss of the target domain, given as γ = δ × epoch maxepoch . epoch is the current training round, and maxepoch is the maximum training round. Empirically, δ is set to 2.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Emotional Databases And Data Preprocessing",
      "text": "To have a fair comparison with the state-of-the-art methods, we validate our proposed model on two well-known public databases: SEED  [13]  and SEED-IV  [20] . In SEED database  [13] , a total of 15 subjects were invited. Each subject performed three sessions on different days and each session contained 15 trials. A total of three emotions were elicited (negative, neutral, and positive). In SEED-IV database  [20] , a total of 15 subjects participated in the experiment. For each subject, a total of 3 sessions were performed on different days and each session contained 24 trials. A total of four emotions were elicited ( happiness, sadness, fear, and neutral). The EEG signals of both SEED and SEED-IV databases were simultaneously collected using the 62-channel ESI Neuroscan system.\n\nFor EEG preprocessing, the data sampling rate was first downsampled to 200Hz, and the contaminated noises (e.g. EMG and EOG) were manually removed. Then, the data were filtered by a band-pass filter of 0.3 Hz to 50Hz. For each trial, the data was divided into a number of segments with a length of 1s. Based on the pre-defined five frequency bands: Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-13 Hz), Beta (14-30 Hz), and Gamma (31-50 Hz), the corresponding differential entropy (DE) features were extracted to represent the logarithm energy spectrum in a specific frequency band and total 310 features (5 frequency band × 62 channels) were obtained for one EEG segment. Then, all the features were smoothed with the linear dynamic system (LDS) method, which can utilize the time dependency of emotion changes and filter out emotion unrelated and noisy EEG components  [33] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Implementation Results",
      "text": "In our experiments, the feature extractor f and discriminator d are both made up of multilayer perceptron (MLP) with the Relu activation function. All the parameters are randomly initialized from a uniform distribution. The bilinear operator matrix S is also randomly initialized. In the model architecture, the feature extractor structure is designed as 310 (input layer)-64 (hidden layer 1)-Relu activation-64 (hidden layer 2)-Relu activation-64 (output feature layer). The discriminator structure is designed as 64 (input layer)-64 (hidden layer 1)-Relu activation-dropout layer-64 (hidden layer 2)-1 (output layer)-Sigmoid activation. The size of matrix S given in Eq. 5 is 64 × 64. Besides, we adopt an RMSprop optimizer for network training, which shows a better performance than the other classic optimizers. The learning rate is set to 1e-3 and the mini-batch size for training is 96. To avoid overfitting problems, we use L2 regularizes (1e-5) in the networks. The regularization coefficient β in Eq. 11 is 0.01. The balance parameter γ for pairwise learning on the target domain in Eq. 18 is controlled by a constant factor δ of 2. The threshold τ 0 h and τ 0 l are given to 0.9 and 0.5 respectively. All the models are trained on an NVIDIA GeForce RTX 2080 GPU, with CUDA 10.0 using the Pytorch API.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Experiment Protocols",
      "text": "To fully evaluate the robustness and stability of the proposed model and compare it with the existing literature, we validate PR-PL using four different validation protocols.\n\n(1) Cross-subject cross-session leave-one-subject-out crossvalidation. We evaluate the model with a strict cross-subject cross-session leave-one-subject-out to fully estimate the model robustness on the unknown subject(s) and session(s). One subject's all sessions data are used as the target and the remaining subjects' all sessions are used as the source. We repeat the training validation until each subject's all sessions are treated as the target for once. Due to the variants in individuals and sessions, this evaluation protocol poses a great challenge to the model's effectiveness in the EEG-based emotion recognition tasks. (2) Cross-subject single-session leave-one-subject-out cross-validation. It is the most widely used validation scheme in the EEG-based emotion recognition tasks  [5] ,  [27] ,  [28] ,  [34] . One subject's one-session data is treated as the target and the other remaining subjects are used as the source. We repeat the training validation process until each subject is treated as the target for once. Same as the other studies, we only consider the first session in this type of cross-validation. (3) Within-subject cross-session leave-onesession-out cross-validation. Similar to the existing methods, a time-series cross-validation method is adopted here, where the past data is used to predict current or future data. For one subject, the first two sessions are used as the source, and the latter session is used as the target. The average accuracies and standard deviations across subjects are calculated as the final results. (4) Within-subject single-session cross-validation. Following the validation protocol presented in the existing studies  [13] ,  [20] , for each session of one subject, we use the first 9 (SEED) or 16 (SEED-IV) trials as the source and the rest 6 (SEED) or 8 (SEED-IV) trials as the target. The results are reported as the average performance across all the subjects. In the following performance comparison across four different validation protocols, the model results reproduced by us are indicated by '*'.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "D. Cross-Subject Cross-Session Leave-One-Subject-Out Crossvalidation Results",
      "text": "To verify the model efficiency and stability on both crosssubject and cross-session conditions, we verify the proposed PR-PL using cross-subject cross-session leave-one-subject-out cross-validation on both SEED and SEED-IV databases. As reported in Table  II  and Table  III , the results show our proposed model achieves the highest results, where the emotion recognition performance of PR-PL is 85.56%±4.78% for three-class classification task on SEED and 74.92%±7.92% for four-class classification task on SEED-IV. Compared to the existing studies, the proposed PR-PL increases the classification accuracy to 3.39% and 1.08% for SEED and SEED-IV, with smaller standard deviations. These results demonstrate the proposed PR-PL has better affective effectiveness with higher recognition accuracy and better generalizability.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "E. Cross-Subject Single-Session Leave-One-Subject-Out Crossvalidation Results",
      "text": "Table IV summarizes the model results in cross-subject single-session leave-one-subject-out recognition task and compare the performance with the literature. All the results are presented in terms mean±standard deviation. The results show our proposed model (PR-PL) achieves the best performance (93.06%), with a standard deviation of 5.12%. Our PR-PL leads 2.14% against the reported best results in the literature. Especially, compared to the latest proposed DANN based deep transfer learning networks (e.g. ATDD-DANN  [26] , R2G-STNN  [6] , BiHDM  [44] , BiDANN  [25] , and WGAN-GP  [27] ), the proposed PR-PL with pairwise learning can avoid the inherent defects of DANN design (e.g. only considers feature separability on source domain) and well address the individual differences and noisy labeling issues in aBCI applications.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Pacc Methods Pacc",
      "text": "Traditional machine learning methods TKL  [25]  63.54±15.47 T-SVM  [25]  72.53±14.00 TCA  [44]  63.64±14.88 TPT  [24]  75.17±12.83 KPCA  [24]  61.28±14.62 GFK  [44]  71.31±14.09 SA  [44]  69.00±10.89 DICA  [46]  69.40±07.80 DNN  [24]  61.01±12.38 SVM  [24]  58.18±13.85",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Deep Learning Methods",
      "text": "DGCNN  [45]  79.95±09.02 DAN  [24]  83.81±08.56 RGNN  [8]  85.30±06.72 BiHDM  [44]  85.40±07.53 WGAN-GP  [27]  87.10±07.10 MMD  [28]  80.88±10.10 ATDD-DANN  [26]  90.92±01.05 JDA-Net  [28]  88.28±11.44 R2G-STNN  [6]  84.16±07.63 SimNet*  [31]  81.58±05.11 BiDANN  [25]  83.28±09.60 DResNet  [46]  85.30±08.00 ADA  [28]  84.47±10.65 DANN  [28]  81.65±09.92 PR-PL 93.06±05.12",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "F. Within-Subject Cross-Session Cross-Validation Results",
      "text": "By calculating the average and standard deviation of the experimental results of each subject, the final within-subject cross-session cross-validation results are reported in Table  V  for the SEED database and Table VI for the SEED-IV database. For both databases, our proposed PR-PL achieves the highest recognition performance compared with the stateof-the-art methods (including both traditional machine learning methods and deep learning methods), where the results are 93.18%±6.55% and 74.62%±14.15% for SEED (threeclass emotion recognition) and SEED-IV (four-class emotion recognition), respectively.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "G. Within-Subject Single-Session Cross-Validation Results",
      "text": "Consistent with the evaluation method presented in the previous studies that only consider the first two sessions of    IV ) and within-subject singlesession (Table  VII ) emotion recognition tasks, the proposed PR-PL achieves the highest accuracies and at the same time perform the closest results on the two cross-validation methods (cross-subject single-session: 93.06±05.12; within-subject single-session: 94.84±09.16). For the other models, such as DGCNN  [45] , BiDANN  [25] , R2G-STNN  [6] , RGNN  [8] ,\n\nand BiHDM  [44] , there exists a significant difference between cross-subject and within-subject results (9.09% difference on average). This comparison demonstrates the efficiency and reliability of the proposed PR-PL in various emotion recognition applications.\n\nFor the SEED-IV database, we calculate the performance on all three sessions as reported in the other studies and decode emotions into four categories (happiness, sadness, fear, and neutral). Our proposed model outperforms the existing studies, with the highest accuracy of 83.33%, which leads to a 3.96% increase as compared to the SOTA (79.37%  [8] ).",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "V. Discussion And Conclusion",
      "text": "To fully study the model performance, we evaluate the effect of different settings in PR-PL. Note that all the results presented in this section are based on the SEED database using the cross-subject single-session cross-validation evaluation protocol.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Ablation Study",
      "text": "We conduct the ablation study to systematically explore the effectiveness of different components in the proposed model and examine the corresponding contributions to the overall performance. As shown in Table  IX , it is found that the introduction of domain adversarial training can greatly enhance the emotion recognition performance on the target domain. When the model is without discriminator and target domain information, the recognition accuracy reduces from 93.06% to 83.30%. Such a significant drop shows the significant impact of individual differences problem on model performance and highlights the great potential of transfer learning in aBCI applications. Besides, the results show a combination of pairwise learning on the source and target domain benefits to the model performance, where the recognition accuracy is increased by 6.35% (from 87.5% to 93.06%). For the pseudolabeling method, the corresponding accuracy increases from 89.92% to 92.46% when the pseudo-labeling method changes from fixed to linear dynamic. The accuracy further increases to 93.06%, when a nonlinear dynamic-based adaptive pseudolabeling method is adopted. The results show a non-linear dynamic pseudo-labeling could be helpful to screen out the valid paired samples and improve the model trainability. For the final loss function given in Eq. 18, instead of using a fixed weight for the pairwise loss on the target domain, we propose to update the weight gradually along with the training epochs to prevent model learning failures in the early training stage and balance the relationships among different losses. The benefit of a dynamic γ is also reflected in the ablation study, where the recognition accuracy increases from 89.47% and 93.06%.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Effect Of Noisy Labels",
      "text": "To further verify the model robustness during noisy label learning, we randomly contaminate the source labels with η% noises and test the corresponding model performance on unknown target data. Specifically, we replace η% real labels in Y s using randomly generated labels and train the model in supervised learning. Then, we test the trained model performance on the target domain. Note here that the noisy contamination is only conducted on the source domain, as the target domain needs to be used for model evaluation. In the implementation, η% value is adjusted to 10%, 20%, and 30%, respectively. The corresponding model accuracies with the standard deviations are 89.22%±6.05%, 88.39%±6.73%, and 87.71%±5.02%. It shows that, with an increase of label noise ratio from 10% to 30%, the model performance decreases slightly, with a decrease rate of 1.69%. These results demonstrate the proposed PR-PL is a reliable model which has a higher tolerance to noisy labels. In the future works, the recently proposed novel methods, such as  [52]  and  [53] , could be incorporated to further eliminate more general noises in EEG signals and improve the model stability in the crosscorpus applications.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Confusion Matrices",
      "text": "To qualitatively study the model performance in each emotion class, we visually analyze the confusion matrices and compare results with the latest models  [8] ,  [25] ,  [44] . As shown in Fig.  2 , it shows all the models are good at distinguishing positive emotions from other emotions (the recognition rates are all above 90%), but it is relatively poor at distinguishing negative and neutral emotions. For example, the recognition rate of neural in BiDANN  [25]  is even lower than 80% (76.72%). Compared to the existing methods ((a), (b), and (c)), our proposed model can enhance the model recognition ability, especially for distinguishing neutral and negative emotions. As shown in (d), the recognition rates for negative, neutral, and positive emotions are 92.10%, 90.39%, and 96.50%.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Visualization Of Learned Representation",
      "text": "To verify the effectiveness of the proposed model from a more intuitive perspective, we visualize the characterized sample and interaction features of source and target domains using T-SNE  [54]  in Fig.  3 . Here, we randomly select 500 samples from the source and 500 samples from the target for visualization of the learned feature representation. Compared to the representation learned by the other model settings (w/o pairwise learning on the source and target and w/o pairwise learning on the target), the representation learned by the proposed PR-PL forms more separated emotional clusters. Comparing the extracted sample features (c) and interaction features (f) by the proposed PR-PL, the separability of the extracted interaction features from different emotion classes is further enlarged and at the same time, the concentration of the feature distribution for each emotion is also improved.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Conclusion",
      "text": "The paper proposes a novel transfer learning framework with prototypical representation-based pairwise learning (PR-PL), that characterizes EEG data with prototypical representations and formulates the EEG-based emotion recognition task as pairwise learning. We evaluate our proposed model on two well-known emotional databases (SEED and SEED-IV) under four cross-validation protocols (cross-subject single-session, within-subject single-session, within-subject cross-session, and cross-subject cross-session) and compare it with the existing state-of-the-art methods. Our extensive experimental results show PR-PL achieves the best results on all four crossvalidation protocols and demonstrate the advantage of PR-PL in tackling individual differences and noisy labeling issues in aBCI systems.",
      "page_start": 10,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , the proposed PR-",
      "page": 3
    },
    {
      "caption": "Figure 1: The proposed PR-PL framework.",
      "page": 4
    },
    {
      "caption": "Figure 1: Here, θ = {θf, S}, θf is the parameter of feature extractor",
      "page": 5
    },
    {
      "caption": "Figure 1: Here, lt is the interaction features of the target domain data",
      "page": 6
    },
    {
      "caption": "Figure 3: Here, we randomly select 500",
      "page": 10
    },
    {
      "caption": "Figure 2: Confusion matrices of different models. (a) BiDANN [25], (b) BiHDM [44], (c) RGNN [8], and (d) PR-PL.",
      "page": 11
    },
    {
      "caption": "Figure 3: T-SNE visualization of the learned features from the",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "x": "(cid:107)huanggan@szu.edu.cn, ∗∗yinidong@cityu.edu.hk, ††fali.li@uestc.edu.cn, ‡‡yangxinknow@gmail.com,\njanezliang@szu.edu.cn"
        },
        {
          "x": "Abstract—Affective brain-computer\ninterfaces based on elec-"
        },
        {
          "x": "troencephalography (EEG)\nis an important branch in the ﬁeld\nA FFECTIVE computing is a fast growing interdisciplinary"
        },
        {
          "x": "of affective computing. However,\nindividual differences and noisy"
        },
        {
          "x": "from different areas including computer science, neuroscience,"
        },
        {
          "x": "labels\nseriously\nlimit\nthe\neffectiveness\nand\ngeneralizability\nof"
        },
        {
          "x": "psychology,\nand\nsignal\nprocessing\n[1]. Recently,\nelectroen-"
        },
        {
          "x": "EEG-based emotion recognition models. In this paper, we propose"
        },
        {
          "x": "cephalography (EEG) based emotion recognition has become\na novel\ntransfer\nlearning framework with Prototypical Repre-"
        },
        {
          "x": "sentation based Pairwise Learning\n(PR-PL)\nto\nlearn discrimi-\nan increasingly important\ntopic\nfor\naffective\ncomputing and"
        },
        {
          "x": "native and generalized prototypical representations\nfor emotion"
        },
        {
          "x": "human sentiment analysis [2],\n[3]. A proper design of EEG-"
        },
        {
          "x": "revealing across\nindividuals and formulate\nemotion recognition"
        },
        {
          "x": "based emotion recognition models\nis helpful\nfor\nfacilitating"
        },
        {
          "x": "as pairwise learning for alleviating the reliance on precise label"
        },
        {
          "x": "the\ndata\nprocessing,\nbeneﬁting\ndiscriminant\nfeature\ncharac-"
        },
        {
          "x": "information. More speciﬁcally, a prototypical\nlearning-based ad-"
        },
        {
          "x": "terization, and lightening the model performance. Currently,\nversarial discriminative domain adaptation method is developed"
        },
        {
          "x": "to\nencode\nthe\ninherent\nemotion-related\nsemantic\nstructure\nof\nthere\nexist\ntwo main\ncritical\nissues\nin EEG-based\nemotion"
        },
        {
          "x": "EEG data, while pairwise\nlearning with an adaptive pseudo-"
        },
        {
          "x": "recognition. One\nis\nindividual differences: how to build a"
        },
        {
          "x": "labeling method is developed to\nachieve\na\nreliable\nand stable"
        },
        {
          "x": "generalized affective\ncomputing model which could tolerate"
        },
        {
          "x": "model\nlearning with noisy labels. Through domain adaptation,"
        },
        {
          "x": "the\nremarkable\nindividual differences\nin the\nsimultaneously"
        },
        {
          "x": "feature representations of source and target domains are aligned"
        },
        {
          "x": "collected EEG signals; and another\nis noisy label\nlearning:\non a shared feature space, while the feature separability of both"
        },
        {
          "x": "source and target domains is also considered. The characterized\nhow to train a reliable and stable affective computing model"
        },
        {
          "x": "prototypical\nrepresentations\nare\nevident with\na\nhigh\nfeature"
        },
        {
          "x": "which is less reliant on the subjective feedback."
        },
        {
          "x": "concentration within one\nsingle\nemotion category\nand a high"
        },
        {
          "x": "In recent years, more\nand more\nresearchers have\nfocused"
        },
        {
          "x": "feature separability across different emotion categories. Extensive"
        },
        {
          "x": "on applying transfer\nlearning methods\nto alleviate\nthe\nindi-"
        },
        {
          "x": "experiments are conducted on two benchmark databases under"
        },
        {
          "x": "vidual differences in EEG signals [4]–[9] and improve feature\nfour\ncross-validation evaluation protocols\n(cross-subject\ncross-"
        },
        {
          "x": "session, cross-subject within-session, within-subject cross-session,\ninvariant representation [10]–[12]. Considering the individuals"
        },
        {
          "x": "and within-subject within-session).\nThe\nexperimental\nresults"
        },
        {
          "x": "with and without\nlabels (termed as source domain and target"
        },
        {
          "x": "demonstrate the superiority of\nthe proposed PR-PL against\nthe"
        },
        {
          "x": "domain),\ntransfer\nlearning tries\nto minimize the distribution"
        },
        {
          "x": "state-of-the-arts under all four evaluation protocols, which shows"
        },
        {
          "x": "difference between the source and target domains by approxi-"
        },
        {
          "x": "the effectiveness and generalizability of PR-PL in dealing with"
        },
        {
          "x": "mately satisfying the assumption of independent and identical\nthe ambiguity of EEG responses in affective studies. The source"
        },
        {
          "x": "code is available at https://github.com/KAZABANA/PR-PL.\ndistribution and can consequently realize a higher recognition"
        },
        {
          "x": "performance on the target domain. Through a domain-shifting"
        },
        {
          "x": "Index Terms—Electroencephalography; Emotion Recognition;"
        },
        {
          "x": "strategy,\nthe invariant\nfeature representations across different\nPrototypical Representation; Pairwise Learning; Transfer Learn-"
        },
        {
          "x": "ing.\ndomains are learned and the relationships among the learned"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "EEG-based emotion recognition tasks under different applica-"
        },
        {
          "2": "tion environments\n(cross-subject\ncross-session,\ncross-subject"
        },
        {
          "2": "single-session, within-subject cross-session, and within-subject"
        },
        {
          "2": "single-session). (2) We propose a novel prototypical\nlearning-"
        },
        {
          "2": "based\nadversarial\ndiscriminative\ndomain\nadaptation method"
        },
        {
          "2": "to explore latent variables of emotion categories, encode the"
        },
        {
          "2": "semantic structure of EEG data, and learn subject-generalized"
        },
        {
          "2": "prototypical\nrepresentations\nfor emotion revealing across\nin-"
        },
        {
          "2": "dividuals. The characterized prototypical representations show"
        },
        {
          "2": "a high feature concentration within one single emotion cate-"
        },
        {
          "2": "gory and a high feature separability across different emotion"
        },
        {
          "2": "categories.\n(3) Different\nfrom the\nexisting transfer\nlearning"
        },
        {
          "2": "methods that only focus on feature separability in the source"
        },
        {
          "2": "domain, we consider\nthe feature separability of both source"
        },
        {
          "2": "and target domains through the end-to-end domain adversarial"
        },
        {
          "2": "training to further enhance the model effectiveness and gener-"
        },
        {
          "2": "alizability."
        },
        {
          "2": ""
        },
        {
          "2": "II. RELATED WORK"
        },
        {
          "2": ""
        },
        {
          "2": "The existing EEG-based emotion recognition models with"
        },
        {
          "2": "the transfer\nlearning algorithms can be generally categorized"
        },
        {
          "2": "into two types."
        },
        {
          "2": "(a) Non-deep transfer\nlearning models. Pan et al.\n[21]"
        },
        {
          "2": "proposed\na\ntransfer\ncomponent\nanalysis\n(TCA)\nalgorithm"
        },
        {
          "2": ""
        },
        {
          "2": "to\nreduce\nthe marginal\ndistribution\ndifference\nbetween\nthe"
        },
        {
          "2": "source and target domains,\nin which the transfer\ninformation"
        },
        {
          "2": "was\nlearned\nin\na\nreproducing\nkernel Hilbert\nspace\nthrough"
        },
        {
          "2": "maximizing mean discrepancy. Zheng and Lu [22] introduced"
        },
        {
          "2": "two types of subject-to-subject\ntransfer methods to deal with"
        },
        {
          "2": "the\nchallenge\nof\nthe\nindividual\ndifferences\nin EEG signal"
        },
        {
          "2": "processing. One was to explore a shared common feature space"
        },
        {
          "2": "underlying source and target domains using TCA and kernel"
        },
        {
          "2": "principal analysis (KPCA), and another was to construct multi-"
        },
        {
          "2": "ple personalized classiﬁers on the source domain and map the"
        },
        {
          "2": "classiﬁer parameters\nto the target domain using transductive"
        },
        {
          "2": "parameter\ntransfer\n(TPT). These\nnon-deep\ntransfer\nlearning"
        },
        {
          "2": "strategies show the possibility to bridge the discrepancy across"
        },
        {
          "2": "two domains with improved performance on the target domain."
        },
        {
          "2": "However, due to the small capacity and low complexity,\nthe"
        },
        {
          "2": "model accuracy and stability are still\nlimited, which fails\nto"
        },
        {
          "2": "satisfy the requirements of affective brain-computer interfaces"
        },
        {
          "2": "(aBCI)\nin practical applications."
        },
        {
          "2": "(b) Deep transfer learning models. Most of\nthe existing"
        },
        {
          "2": "affective models are based on deep transfer\nlearning methods"
        },
        {
          "2": "built with domain-adversarial neural network (DANN) pro-"
        },
        {
          "2": "posed in [23]. The main idea of DANN is\nto ﬁnd a shared"
        },
        {
          "2": "feature\nrepresentation\nfor\nsource\nand\ntarget\ndomains with"
        },
        {
          "2": "indistinguishable\ndistribution\ndifferences\nand\nalso maintain"
        },
        {
          "2": "the predictive ability of\nthe estimated features on the source"
        },
        {
          "2": "samples for a speciﬁc classiﬁcation task. Li et al. [24] was the"
        },
        {
          "2": "ﬁrst to introduce DANN in aBCI. Beneﬁting from the powerful"
        },
        {
          "2": "feature representation ability of deep networks and the high"
        },
        {
          "2": "efﬁciency of adversarial\nlearning in distributed adaptation,\nthe"
        },
        {
          "2": "results\nshowed that DANN based aBCI\nsystem was\nsuperior"
        },
        {
          "2": "to other methods. The following aBCI systems could be con-"
        },
        {
          "2": "sidered as a series of DANN-based models, which generally"
        },
        {
          "2": "start\nfrom two directions to improve the DANN performance"
        },
        {
          "2": "in solving EEG-based emotion recognition tasks."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3": "TABLE I: Frequently used notations and descriptions."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "Notation\nDescription"
        },
        {
          "3": ""
        },
        {
          "3": "S\\T\nsource\\target domain"
        },
        {
          "3": ""
        },
        {
          "3": "xs\\xt\nsource\\target\nfeature"
        },
        {
          "3": "ys\\yt\nsource\\target\nlabel"
        },
        {
          "3": "N s\\N t\nnumber of source\\target samples"
        },
        {
          "3": ""
        },
        {
          "3": "Ds\\Dt\nthe source\\target dataset"
        },
        {
          "3": ""
        },
        {
          "3": "f (·)\nsample feature extractor"
        },
        {
          "3": "d (·)\ndomain discriminator"
        },
        {
          "3": "h (·)\nbi-linear operation"
        },
        {
          "3": ""
        },
        {
          "3": "the parameter of\nthe feature extractor f (·)\nθf"
        },
        {
          "3": ""
        },
        {
          "3": "the parameter of\nthe discriminator d (·)\nθd"
        },
        {
          "3": "S\nthe parameter of\nthe bi-linear operation h (·)"
        },
        {
          "3": "µ\ncentroid"
        },
        {
          "3": ""
        },
        {
          "3": "l\npredict\nlabel"
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "applications,\nit\nis difﬁcult\nto collect accurate labels\nfor each"
        },
        {
          "3": ""
        },
        {
          "3": "single EEG trial."
        },
        {
          "3": ""
        },
        {
          "3": ""
        },
        {
          "3": "III. METHODOLOGY"
        },
        {
          "3": ""
        },
        {
          "3": "S"
        },
        {
          "3": "Suppose\nthe\nEEG trials\nin\nthe\nsource\ndomain\nand"
        },
        {
          "3": "T"
        },
        {
          "3": "target\ndomain\nare\ngiven\nand\nas Ds\n{Xs, Ys}"
        },
        {
          "3": ""
        },
        {
          "3": "=\nand\nwhere\nDt\n{Xt, Yt},\n{Xs, Ys} = {(xs\ni , ys\ni )}Ns"
        },
        {
          "3": "i=0"
        },
        {
          "3": ""
        },
        {
          "3": "and X t\n{Xt, Yt} = {(xt\ni )}Nt"
        },
        {
          "3": "i are EEG samples,\ni, yt\ni=0. Here, X s"
        },
        {
          "3": ""
        },
        {
          "3": "and Y s\nand Y t\nare\nthe\ncorresponding\nemotion\nlabels. To\ni\ni"
        },
        {
          "3": ""
        },
        {
          "3": "make the narrative clearer,\nthe frequently used notations are"
        },
        {
          "3": ""
        },
        {
          "3": "summarized in Table I. As shown in Fig. 1,\nthe proposed PR-"
        },
        {
          "3": ""
        },
        {
          "3": "PL includes\nthree\nlosses\n(domain adversarial\nloss, pairwise"
        },
        {
          "3": ""
        },
        {
          "3": "learning loss on source domain, and pairwise learning loss on"
        },
        {
          "3": ""
        },
        {
          "3": "target domain) and two main parts (prototypical representation"
        },
        {
          "3": ""
        },
        {
          "3": "and pairwise learning). In the prototypical representation, three"
        },
        {
          "3": ""
        },
        {
          "3": "types of features are deﬁned. The characterized features f (Xs)"
        },
        {
          "3": ""
        },
        {
          "3": "sample fea-\nfrom the EEG samples are termed as\nor f (Xt)"
        },
        {
          "3": ""
        },
        {
          "3": "tures, which are forced to be as indistinguishable as possible"
        },
        {
          "3": ""
        },
        {
          "3": "from the source and target domains. Under an assumption that"
        },
        {
          "3": ""
        },
        {
          "3": "any emotion could be represented by a prototype via prototyp-"
        },
        {
          "3": ""
        },
        {
          "3": "ical learning, the prototype features of each emotion category"
        },
        {
          "3": ""
        },
        {
          "3": "from\nare learned based on the sample features f (Xs) and Ys"
        },
        {
          "3": ""
        },
        {
          "3": "the source domain. The interaction relationships between the"
        },
        {
          "3": ""
        },
        {
          "3": "sample features and prototype features are measured and the"
        },
        {
          "3": ""
        },
        {
          "3": "interaction\nfeatures\nare\ncharacterized which will\nbe\nused"
        },
        {
          "3": ""
        },
        {
          "3": "in the following pairwise learning.\nIn the pairwise learning,"
        },
        {
          "3": ""
        },
        {
          "3": "the pair\nrelationships on both source and target domains are"
        },
        {
          "3": ""
        },
        {
          "3": "explored. As\nthe\nare unknown during\ninformation about Yt"
        },
        {
          "3": "model\ntraining, an adaptive thresholding method is developed"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": ""
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "speciﬁc features (CDA)."
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": ""
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": ""
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "Although the above models have achieved higher accuracies"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": ""
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "compared to the original model with DANN in emotion recog-"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "nition tasks,\nthere still exist\nthree major\ntechnical challenges."
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": ""
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "First,\nthe learned feature representation is susceptible to noise"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "interference from both source and target domains and would"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "further\naffect\nthe model generalizability [29],\n[30]. Second,"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "the\nexisting models\nonly\nfocus\non\nthe\nfeature\nseparability"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "in the\nsource domain but\nignore\nthe\nfeature\nseparability in"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "the target domain. The current DANN-based models mainly"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "concern the emotion classiﬁcation loss in the source domain,"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "which would lead to the over-ﬁtting of\nsource domain data"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "and the decrease of classiﬁcation ability on target domain data."
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "Third,\nthe existing algorithms largely rely on a large amount"
        },
        {
          "framework\nof\ntask-invariant\nfeatures\n(MDA)\nand\ntask-": "of\nlabeled source domain data. However,\nin practical EEG"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Prototypical Representation": "",
          "Pairwise Learning": ""
        },
        {
          "Prototypical Representation": "",
          "Pairwise Learning": ""
        },
        {
          "Prototypical Representation": "",
          "Pairwise Learning": ""
        },
        {
          "Prototypical Representation": "",
          "Pairwise Learning": "Pairwise"
        },
        {
          "Prototypical Representation": "Subjective Feedbacks",
          "Pairwise Learning": ""
        },
        {
          "Prototypical Representation": "",
          "Pairwise Learning": "Learning"
        },
        {
          "Prototypical Representation": "",
          "Pairwise Learning": ""
        },
        {
          "Prototypical Representation": "",
          "Pairwise Learning": ""
        },
        {
          "Prototypical Representation": "",
          "Pairwise Learning": "Supervised Pairwise Loss"
        },
        {
          "Prototypical Representation": "Sample Features\nPrototype Features",
          "Pairwise Learning": "Pairwise"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0,  1,  0": "Prototype Features"
        },
        {
          "0,  1,  0": "Prototype \nInteraction Features"
        },
        {
          "0,  1,  0": "Features"
        },
        {
          "0,  1,  0": ""
        },
        {
          "0,  1,  0": "Bilinear Interaction"
        },
        {
          "0,  1,  0": "Sample"
        },
        {
          "0,  1,  0": "Prototype Features"
        },
        {
          "0,  1,  0": "Features\n   0,   0.95, 0.05"
        },
        {
          "0,  1,  0": "0.93, 0.04, 0.03"
        },
        {
          "0,  1,  0": "Sample\n0.87, 0.12, 0.01"
        },
        {
          "0,  1,  0": "Features\n0.01,    0,   0.99"
        },
        {
          "0,  1,  0": ".\n."
        },
        {
          "0,  1,  0": "."
        },
        {
          "0,  1,  0": "0.02, 0.96, 0.02"
        },
        {
          "0,  1,  0": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1: The proposed PR-PL framework.": "between\nthe\nsource\ndomain\nsample\nfeature\nand\nthe\nf (Xs)"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "target domain sample feature f (Xt) are minimized by adopt-"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": ""
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "ing domain adversarial\ntraining. Here, f (·)\nis\nthe designed"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": ""
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "feature extractor for extracting the sample features from EEG"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "signals. A discriminator network d (·) with the parameter θd"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": ""
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "is introduced to distinguish whether\nthe characterized sample"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": ""
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "features (f (Xs) or f (Xt)) come from the source domain (S)"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": ""
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "or\nthe target domain (T).\nIts loss function is a standard two-"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "category cross-entropy loss function, given as"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "Nt(cid:88)\nNs(cid:88)"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "log (cid:0)1 − d (cid:0)f (cid:0)xt\n(cid:1)(cid:1)(cid:1) .\nlog d (f (xs\nLdisc (θf , θd) = −\ni\ni )) −"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": ""
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "i=0\ni=0"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "(1)"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "In the\ntraining process, we\nadopt\nthe\nend-to-end training"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "method [23], and implement the domain adversarial training by"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "introducing a gradient reversal layer. The feature extractor f (·)"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "maximizes the classiﬁcation ability to enhance emotion recog-"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "nition performance and at the same time, the discriminator d(·)"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "minimizes the domain discrimination to reduce the distribution"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "difference between the source domain and target domain. The"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "ﬁnal domain adversarial\ntraining objective function is deﬁned"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "as"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": ""
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "min\nmax\nLs"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "(2)\nclassiﬁer(θf ) − λLdisc (θf , θd) ,"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "θf\nθd"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": ""
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "which is termed as domain adversarial\nloss in Fig. 1. Here,"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "Ls"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "classiﬁer(θf ) is the classiﬁcation loss to measure the classiﬁca-"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "tion ability in source domain.\nIn this paper, Ls"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "classiﬁer(θf ) will"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "be realized by pairwise learning on source and target domains"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": ""
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "is\nintroduced in Section III-D and III-E below. Ldisc(θf , θd)"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "the\nadversarial\nloss\nfor\nthe\ndiscriminator\nto\nbe\ntrained\nto"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "distinguish the sample features characterized from source and"
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "target domains. θf and θd are the parameters of f (·) and d (·)."
        },
        {
          "Fig. 1: The proposed PR-PL framework.": "λ is a balanced hyperparameter\nfor ensuring the stability of"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "tackle this issue, we introduce pairwise learning to capture the"
        },
        {
          "5": ""
        },
        {
          "5": "inherent\nrelationship of samples."
        },
        {
          "5": ""
        },
        {
          "5": ""
        },
        {
          "5": "the\nFor\nthe\nsource domain data {Xs, Ys} = {xs\ni , ys\ni }Ns\ni=1,"
        },
        {
          "5": ""
        },
        {
          "5": "corresponding loss\nfunction for pairwise\nlearning is deﬁned"
        },
        {
          "5": ""
        },
        {
          "5": "as"
        },
        {
          "5": ""
        },
        {
          "5": "(cid:88) i\nL (cid:0)rs\n(7)\nLclass (θ) =\nij, g (cid:0)xs\ni , xs\nj; θ(cid:1)(cid:1) ,"
        },
        {
          "5": ",j"
        },
        {
          "5": ""
        },
        {
          "5": "where g (cid:0)xs\nthe sam-\ni , xs\nj; θ(cid:1) is the similarity measurement of"
        },
        {
          "5": ""
        },
        {
          "5": "ples xs\nand xs\nj, with the parameter of θ. According to the"
        },
        {
          "5": "i"
        },
        {
          "5": "then rs\nassumption of pairwise\nlearning,\nif ys\nij = 1;\nj ,\ni = ys"
        },
        {
          "5": ""
        },
        {
          "5": "rs"
        },
        {
          "5": "otherwise\nloss\nis\na\ndifference\nij = 0. The"
        },
        {
          "5": "and g (cid:0)xs\ncalculation of rij\ni , xs\nj; θ(cid:1), given by a two-category"
        },
        {
          "5": ""
        },
        {
          "5": "cross-entropy loss as"
        },
        {
          "5": ""
        },
        {
          "5": "L (cid:0)rs\nij, g (cid:0)xs\ni , xs\nj; θ(cid:1)(cid:1) = −rs\nij log (cid:0)g (cid:0)xs\ni , xs\nj; θ(cid:1)(cid:1)"
        },
        {
          "5": "(8)"
        },
        {
          "5": "− (cid:0)1 − rs\n(cid:1) log (cid:0)1 − g (cid:0)xs\nij\ni , xs\nj; θ(cid:1)(cid:1) ."
        },
        {
          "5": "In the training process, the label information of source domain"
        },
        {
          "5": ""
        },
        {
          "5": "data is used to deﬁne r in a supervised manner. In other words,"
        },
        {
          "5": ""
        },
        {
          "5": "if\ntwo samples belong\nbased on the given information of Ys,"
        },
        {
          "5": ""
        },
        {
          "5": "to the same emotion category,\nthen r = 1, otherwise r = 0. A"
        },
        {
          "5": ""
        },
        {
          "5": "supervised r can ensure the stability of the training process and"
        },
        {
          "5": ""
        },
        {
          "5": "the generalization ability of the model. The next key question"
        },
        {
          "5": ""
        },
        {
          "5": "is how to deﬁne a proper g (cid:0)xs\ni , xs\nj; θ(cid:1) to compute the similarity"
        },
        {
          "5": ""
        },
        {
          "5": "between xs\nand xs\nin terms of\nthe characterized interaction\ni\nj"
        },
        {
          "5": ""
        },
        {
          "5": "features (termed as li and lj). To make the similarity results"
        },
        {
          "5": ""
        },
        {
          "5": "locate in the range of [0, 1] and extract better and more robust"
        },
        {
          "5": ""
        },
        {
          "5": "feature representations for subsequent emotion recognition, we"
        },
        {
          "5": ""
        },
        {
          "5": "add a norm restriction on l as"
        },
        {
          "5": ""
        },
        {
          "5": "l"
        },
        {
          "5": ".\nlnorm =\n(9)"
        },
        {
          "5": "(cid:107)l(cid:107)2"
        },
        {
          "5": ""
        },
        {
          "5": "lnorm\nThe similarity of\nand lnorm\nis calculated as the cosine"
        },
        {
          "5": "i\nj"
        },
        {
          "5": "similarity, given as"
        },
        {
          "5": ""
        },
        {
          "5": "ls\n· ls"
        },
        {
          "5": "i\nj\n,\ng (cid:0)xs\n· lnorm\n=\n(10)\ni , xs\nj; θ(cid:1) = lnorm\nj"
        },
        {
          "5": "(cid:13)(cid:13)\n(cid:13)(cid:13)\nls\n(cid:107)ls"
        },
        {
          "5": "j\ni (cid:107)2\n2"
        },
        {
          "5": "where ·\nrefers to inner product operation. As stated in Chang"
        },
        {
          "5": "et al.\n[32],\nthe\nabove-mentioned norm restriction can make"
        },
        {
          "5": "the vector\nl have a clustering function, and the elements\nin"
        },
        {
          "5": "the vector represent\nthe probability that\nthe feature belongs to"
        },
        {
          "5": "a certain category cluster. Overall,\nthe objective function of"
        },
        {
          "5": "pairwise learning on the source domain is deﬁned as"
        },
        {
          "5": ""
        },
        {
          "5": "(cid:32)\n(cid:33)"
        },
        {
          "5": "ls\n· ls"
        },
        {
          "5": "i\nj"
        },
        {
          "5": "Ls\nL\nrs\n+ βR,"
        },
        {
          "5": "(cid:88) i\n(11)\npairwise(θ) ="
        },
        {
          "5": "(cid:13)(cid:13)\n(cid:13)(cid:13)\n(cid:107)ls\nls"
        },
        {
          "5": "j\ni (cid:107)2\n,j\n2"
        },
        {
          "5": ""
        },
        {
          "5": "supervised\npairwise\nloss\nwhich\nis\ntermed\nas\nin\nFig.\n1."
        },
        {
          "5": ""
        },
        {
          "5": "is\nthe parameter of\nfeature extractor\nHere, θ = {θf , S}, θf"
        },
        {
          "5": ""
        },
        {
          "5": "f (·),\nand S\nis\nthe\ndeﬁned\nbilinear\ntransformation matrix"
        },
        {
          "5": ""
        },
        {
          "5": "for\ninteraction feature extraction. Besides,\nto avoid redundant"
        },
        {
          "5": ""
        },
        {
          "5": "feature extraction, a soft\nregularization R is\nintroduced with"
        },
        {
          "5": ""
        },
        {
          "5": "a weight parameter of β, which is deﬁned as"
        },
        {
          "5": ""
        },
        {
          "5": "R = (cid:13)\n(12)\n(cid:13)P T P − I(cid:13)"
        },
        {
          "5": "(cid:13)F ."
        },
        {
          "5": ""
        },
        {
          "5": "Here, each row of the matrix P refers to the prototype feature"
        },
        {
          "5": "is a F norm of"
        },
        {
          "5": "the\nbelonging to one emotion category, (cid:107)·(cid:107)F"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6": "τ t−1\n− τ t−1"
        },
        {
          "6": "h\nl"
        },
        {
          "6": "τ t\n,\n+\n(16)\nl = τ t−1"
        },
        {
          "6": "maxepoch"
        },
        {
          "6": ""
        },
        {
          "6": "where τ t"
        },
        {
          "6": "h represents the upper"
        },
        {
          "6": ""
        },
        {
          "6": "τ t\ning round t,\nrepresents\nthe\ncurrent\nlower\nthreshold,\nand"
        },
        {
          "6": "l"
        },
        {
          "6": ""
        },
        {
          "6": "maxepoch is the maximum training round. Based on the given"
        },
        {
          "6": ""
        },
        {
          "6": ",\ncould be\nh and τ 0\nh and τ t"
        },
        {
          "6": ""
        },
        {
          "6": "considered as non-linear changes with respect\nto the training"
        },
        {
          "6": ""
        },
        {
          "6": "rounds as"
        },
        {
          "6": ""
        },
        {
          "6": "(cid:17)t(cid:19)\n(cid:16)"
        },
        {
          "6": "2\nh−τ 0"
        },
        {
          "6": "×\n1 −\nτ t\nh = τ 0\nh − τ 0"
        },
        {
          "6": "2\nmaxepoch"
        },
        {
          "6": " \n(17)"
        },
        {
          "6": "(cid:16)\n(cid:17)t(cid:19)"
        },
        {
          "6": "2\nh−τ 0"
        },
        {
          "6": "×\n1 −\n.\nτ t\nl = τ 0\nl + τ 0\n2\nmaxepoch"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "In all, combining the domain adversarial\nloss\n(Eq. 2),\nthe"
        },
        {
          "6": ""
        },
        {
          "6": "pairwise learning loss in the source domain (Eq. 11, and the"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "pairwise learning loss in the target domain (Eq. 13),\nthe ﬁnal"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "objective function of PR-PL could be given as follows:"
        },
        {
          "6": ""
        },
        {
          "6": "min\nmax\nLs\n(θf , S)+γLt"
        },
        {
          "6": "pairwise\npairwise (θf , S)−λLdisc (θf , θd) ,"
        },
        {
          "6": "θf ,S\nθd"
        },
        {
          "6": ""
        },
        {
          "6": "(18)"
        },
        {
          "6": ""
        },
        {
          "6": "where γ is a hyperparameter\nto control\nthe importance of\nthe"
        },
        {
          "6": "epoch"
        },
        {
          "6": "pairing loss of the target domain, given as γ = δ ×"
        },
        {
          "6": "maxepoch ."
        },
        {
          "6": ""
        },
        {
          "6": "epoch is\nthe\ncurrent\ntraining\nround,\nand maxepoch is\nthe"
        },
        {
          "6": ""
        },
        {
          "6": "maximum training round. Empirically, δ is set\nto 2."
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "IV. EXPERIMENTAL RESULTS"
        },
        {
          "6": ""
        },
        {
          "6": "A. Emotional Databases and Data Preprocessing"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "To have\na\nfair\ncomparison with the\nstate-of-the-art meth-"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "ods, we\nvalidate\nour\nproposed model\non\ntwo well-known"
        },
        {
          "6": "public databases: SEED [13]\nand SEED-IV [20].\nIn SEED"
        },
        {
          "6": "database [13], a total of 15 subjects were invited. Each subject"
        },
        {
          "6": "performed three sessions on different days and each session"
        },
        {
          "6": "contained 15 trials. A total of\nthree\nemotions were\nelicited"
        },
        {
          "6": "(negative, neutral, and positive). In SEED-IV database [20], a"
        },
        {
          "6": "total of 15 subjects participated in the experiment. For each"
        },
        {
          "6": "subject,\na\ntotal\nof\n3\nsessions were\nperformed\non\ndifferent"
        },
        {
          "6": ""
        },
        {
          "6": "days\nand\neach\nsession\ncontained\n24\ntrials. A total\nof\nfour"
        },
        {
          "6": "emotions were elicited ( happiness, sadness, fear, and neutral)."
        },
        {
          "6": "The EEG signals of both SEED and SEED-IV databases were"
        },
        {
          "6": "simultaneously collected using the 62-channel ESI Neuroscan"
        },
        {
          "6": "system."
        },
        {
          "6": "For EEG preprocessing,\nthe data\nsampling rate was ﬁrst"
        },
        {
          "6": "downsampled\nto\n200Hz,\nand\nthe\ncontaminated\nnoises\n(e.g."
        },
        {
          "6": "EMG and EOG) were manually removed. Then,\nthe data were"
        },
        {
          "6": "ﬁltered by a band-pass ﬁlter of 0.3 Hz\nto 50Hz. For\neach"
        },
        {
          "6": "trial,\nthe data was divided into a number of segments with a"
        },
        {
          "6": "length of 1s. Based on the pre-deﬁned ﬁve frequency bands:"
        },
        {
          "6": "Delta (1-3 Hz), Theta (4-7 Hz), Alpha (8-13 Hz), Beta (14-"
        },
        {
          "6": "30 Hz), and Gamma (31-50 Hz), the corresponding differential"
        },
        {
          "6": "entropy (DE) features were extracted to represent the logarithm"
        },
        {
          "6": "energy spectrum in a speciﬁc frequency band and total 310"
        },
        {
          "6": "features (5 frequency band × 62 channels) were obtained for"
        },
        {
          "6": "one EEG segment. Then, all\nthe features were smoothed with"
        },
        {
          "6": "the linear dynamic system (LDS) method, which can utilize"
        },
        {
          "6": "the time dependency of emotion changes and ﬁlter out emotion"
        },
        {
          "6": ""
        },
        {
          "6": ""
        },
        {
          "6": "unrelated and noisy EEG components [33]."
        },
        {
          "6": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": "up"
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": "the Relu activation function. All"
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": "initialized from a uniform distribution. The bilinear operator"
        },
        {
          "In our experiments, the feature extractor f and discriminator": "matrix S is also randomly initialized."
        },
        {
          "In our experiments, the feature extractor f and discriminator": "the feature extractor structure is designed as 310 (input"
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": "activation-dropout"
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": "5 is 64 × 64. Besides, we adopt an RMSprop optimizer"
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": ""
        },
        {
          "In our experiments, the feature extractor f and discriminator": "size"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7": "TABLE II: The mean accuracies (%) and standard deviations"
        },
        {
          "7": ""
        },
        {
          "7": "(%) of emotion recognition on SEED database using cross-"
        },
        {
          "7": ""
        },
        {
          "7": "subject\ncross-session\nleave-one-subject-out\ncross-validation."
        },
        {
          "7": ""
        },
        {
          "7": "Here,\nthe model\nresults\nreproduced by us\nare\nindicated by"
        },
        {
          "7": ""
        },
        {
          "7": "‘*’."
        },
        {
          "7": ""
        },
        {
          "7": "Methods\nMethods\nPacc\nPacc"
        },
        {
          "7": ""
        },
        {
          "7": "Traditional machine learning methods"
        },
        {
          "7": ""
        },
        {
          "7": "RF* [35]\n69.60±07.64\nKNN* [36]\n60.66±07.93"
        },
        {
          "7": "SVM* [37]\n68.15±07.38\nAdaboost* [38]\n71.87±05.70"
        },
        {
          "7": "TCA* [21]\n64.02±07.96\nCORAL* [39]\n68.15±07.83"
        },
        {
          "7": "SA* [?]\n61.41±09.75\nGFK* [40]\n66.02±07.59"
        },
        {
          "7": ""
        },
        {
          "7": "Deep learning methods"
        },
        {
          "7": ""
        },
        {
          "7": "DCORAL* [41]\n81.97±05.16\nDAN* [42]\n81.04±05.32"
        },
        {
          "7": "DDC* [43]\n82.17±04.96\nDANN* [23]\n81.08±05.88"
        },
        {
          "7": ""
        },
        {
          "7": "PR-PL\n85.56±04.78"
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "the rest 6 (SEED) or 8 (SEED-IV)\ntrials as\nthe target. The"
        },
        {
          "7": ""
        },
        {
          "7": "results are reported as the average performance across all\nthe"
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "subjects. In the following performance comparison across four"
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "different validation protocols,\nthe model results reproduced by"
        },
        {
          "7": ""
        },
        {
          "7": "us are indicated by ‘*’."
        },
        {
          "7": ""
        },
        {
          "7": "D. Cross-subject\ncross-session\nleave-one-subject-out\ncross-"
        },
        {
          "7": ""
        },
        {
          "7": "validation results"
        },
        {
          "7": ""
        },
        {
          "7": "To verify the model efﬁciency and stability on both cross-"
        },
        {
          "7": ""
        },
        {
          "7": "subject and cross-session conditions, we verify the proposed"
        },
        {
          "7": ""
        },
        {
          "7": "PR-PL using cross-subject cross-session leave-one-subject-out"
        },
        {
          "7": ""
        },
        {
          "7": "cross-validation on both SEED and SEED-IV databases. As"
        },
        {
          "7": ""
        },
        {
          "7": "reported\nin Table\nII\nand Table\nIII,\nthe\nresults\nshow our"
        },
        {
          "7": ""
        },
        {
          "7": "proposed model achieves the highest\nresults, where the emo-"
        },
        {
          "7": ""
        },
        {
          "7": "tion recognition performance of PR-PL is 85.56%±4.78% for"
        },
        {
          "7": ""
        },
        {
          "7": "three-class\nclassiﬁcation task on SEED and 74.92%±7.92%"
        },
        {
          "7": ""
        },
        {
          "7": "for\nfour-class\nclassiﬁcation task on SEED-IV. Compared to"
        },
        {
          "7": ""
        },
        {
          "7": "the existing studies,\nthe proposed PR-PL increases the classi-"
        },
        {
          "7": ""
        },
        {
          "7": "ﬁcation accuracy to 3.39% and 1.08% for SEED and SEED-IV,"
        },
        {
          "7": ""
        },
        {
          "7": "with smaller standard deviations. These results demonstrate the"
        },
        {
          "7": ""
        },
        {
          "7": "proposed PR-PL has better affective effectiveness with higher"
        },
        {
          "7": ""
        },
        {
          "7": "recognition accuracy and better generalizability."
        },
        {
          "7": ""
        },
        {
          "7": ""
        },
        {
          "7": "E. Cross-subject\nsingle-session\nleave-one-subject-out\ncross-"
        },
        {
          "7": ""
        },
        {
          "7": "validation results"
        },
        {
          "7": ""
        },
        {
          "7": "Table\nIV summarizes\nthe model\nresults\nin\ncross-subject"
        },
        {
          "7": "single-session leave-one-subject-out recognition task and com-"
        },
        {
          "7": "pare\nthe performance with the\nliterature. All\nthe\nresults\nare"
        },
        {
          "7": "presented in terms mean±standard deviation. The results show"
        },
        {
          "7": "our proposed model\n(PR-PL) achieves\nthe best performance"
        },
        {
          "7": "(93.06%), with a\nstandard deviation of 5.12%. Our PR-PL"
        },
        {
          "7": "leads 2.14% against\nthe reported best\nresults in the literature."
        },
        {
          "7": "Especially, compared to the latest proposed DANN based deep"
        },
        {
          "7": "transfer\nlearning\nnetworks\n(e.g. ATDD-DANN [26], R2G-"
        },
        {
          "7": "STNN [6], BiHDM [44], BiDANN [25], and WGAN-GP [27]),"
        },
        {
          "7": "the\nproposed PR-PL with\npairwise\nlearning\ncan\navoid\nthe"
        },
        {
          "7": "inherent defects of DANN design (e.g. only considers feature"
        },
        {
          "7": "separability on source domain) and well address the individual"
        },
        {
          "7": "differences and noisy labeling issues in aBCI applications."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "subject",
          "(%) of emotion recognition on SEED database using within-": "subject cross-session cross-validation. Here,"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "Here,",
          "(%) of emotion recognition on SEED database using within-": "reproduced by us are indicated by ‘*’."
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "‘*’.",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": "Pacc"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": "Traditional machine learning methods"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": "76.42±11.15"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": "74.27±12.88"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": "69.84±09.46"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": "89.16±07.90"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": "91.14±05.61"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": "89.45±06.74"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": "91.17±08.11"
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        },
        {
          "(%) of emotion recognition on SEED-IV database using cross-": "",
          "(%) of emotion recognition on SEED database using within-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "(%) of emotion recognition on SEED database using cross-"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "subject\nsingle-session\nleave-one-subject-out"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "Here,\nthe model\nresults\nreproduced by us"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "‘*’."
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "Methods\nMethods\nPacc"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "Traditional machine learning methods"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "TKL [25]\n63.54±15.47\nT-SVM [25]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "TCA [44]\n63.64±14.88\nTPT [24]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "KPCA [24]\n61.28±14.62\nGFK [44]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "SA [44]\n69.00±10.89\nDICA [46]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "DNN [24]\n61.01±12.38\nSVM [24]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "Deep learning methods"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "DGCNN [45]\n79.95±09.02\nDAN [24]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "RGNN [8]\n85.30±06.72\nBiHDM [44]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "WGAN-GP [27]\n87.10±07.10\nMMD [28]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "ATDD-DANN [26]\n90.92±01.05\nJDA-Net\n[28]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "R2G-STNN [6]\n84.16±07.63\nSimNet* [31]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "BiDANN [25]\n83.28±09.60\nDResNet\n[46]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "ADA [28]\n84.47±10.65\nDANN [28]"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": "PR-PL"
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        },
        {
          "TABLE IV: The mean accuracies (%) and standard deviations": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": ""
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "labeling method"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": ""
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "dynamic pseudo-labeling could be helpful"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": ""
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "valid paired samples and improve the model"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "function given in Eq. 18,"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "for"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "propose to update the weight gradually along with the training"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": ""
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "epochs to prevent model"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": ""
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "stage and balance the relationships among different losses. The"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "beneﬁt of a dynamic γ is also reﬂected in the ablation study,"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "recognition accuracy increases"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": ""
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": ""
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": ""
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "B. Effect of Noisy Labels"
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": ""
        },
        {
          "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-": "To further verify the model"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": "TABLE IX: The ablation study of our proposed model."
        },
        {
          "9": ""
        },
        {
          "9": "Ablation study about\ntraining strategy\nPacc"
        },
        {
          "9": ""
        },
        {
          "9": "w/o discriminator and target\ninformation\n83.30±04.21"
        },
        {
          "9": ""
        },
        {
          "9": "w/o pairwise learning on the source and target\n87.50±06.64"
        },
        {
          "9": "w/o pairwise learning on the target\n88.81±06.63"
        },
        {
          "9": ""
        },
        {
          "9": "w/o prototypical\nrepresentation\n91.00±04.65"
        },
        {
          "9": "w/o thresholding for pseudo label generation\n92.13±05.90"
        },
        {
          "9": "About hyperparameter controlling strategy\nPacc"
        },
        {
          "9": ""
        },
        {
          "9": "w/ ﬁxed pseudo-labeling\n89.92±07.21"
        },
        {
          "9": ""
        },
        {
          "9": "w/\nlinear dynamic pseudo-labeling\n92.46±04.95"
        },
        {
          "9": "w/ ﬁxed γ for\ntarget pairwise loss\n89.47±10.22"
        },
        {
          "9": "PR-PL\n93.06±05.12"
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "increased by 6.35% (from 87.5% to 93.06%). For the pseudo-"
        },
        {
          "9": "labeling method,\nthe corresponding accuracy increases\nfrom"
        },
        {
          "9": "89.92% to 92.46% when the pseudo-labeling method changes"
        },
        {
          "9": "from ﬁxed to linear dynamic. The accuracy further\nincreases"
        },
        {
          "9": ""
        },
        {
          "9": "to 93.06%, when a nonlinear dynamic-based adaptive pseudo-"
        },
        {
          "9": ""
        },
        {
          "9": "labeling method\nis\nadopted. The\nresults\nshow a\nnon-linear"
        },
        {
          "9": ""
        },
        {
          "9": "dynamic pseudo-labeling could be helpful\nto screen out\nthe"
        },
        {
          "9": ""
        },
        {
          "9": "valid paired samples and improve the model\ntrainability. For"
        },
        {
          "9": "the ﬁnal\nloss\nfunction given in Eq. 18,\ninstead of using a"
        },
        {
          "9": "ﬁxed weight\nfor\nthe pairwise loss on the target domain, we"
        },
        {
          "9": "propose to update the weight gradually along with the training"
        },
        {
          "9": ""
        },
        {
          "9": "epochs to prevent model\nlearning failures in the early training"
        },
        {
          "9": ""
        },
        {
          "9": "stage and balance the relationships among different losses. The"
        },
        {
          "9": "beneﬁt of a dynamic γ is also reﬂected in the ablation study,"
        },
        {
          "9": "where\nthe\nrecognition accuracy increases\nfrom 89.47% and"
        },
        {
          "9": "93.06%."
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "B. Effect of Noisy Labels"
        },
        {
          "9": ""
        },
        {
          "9": "To further verify the model\nrobustness during noisy label"
        },
        {
          "9": "learning, we\nrandomly\ncontaminate\nthe\nsource\nlabels with"
        },
        {
          "9": "η% noises\nand\ntest\nthe\ncorresponding model\nperformance"
        },
        {
          "9": "on\nunknown\ntarget\ndata.\nSpeciﬁcally, we\nreplace\nη% real"
        },
        {
          "9": ""
        },
        {
          "9": "labels\nusing randomly generated labels\nand train the\nin Ys"
        },
        {
          "9": ""
        },
        {
          "9": "model\nin supervised learning. Then, we test\nthe trained model"
        },
        {
          "9": ""
        },
        {
          "9": "performance on the target domain. Note here that\nthe noisy"
        },
        {
          "9": ""
        },
        {
          "9": "contamination\nis\nonly\nconducted\non\nthe\nsource\ndomain,\nas"
        },
        {
          "9": ""
        },
        {
          "9": "the target domain needs\nto be used for model evaluation.\nIn"
        },
        {
          "9": "the implementation, η% value is adjusted to 10%, 20%, and"
        },
        {
          "9": "30%,\nrespectively. The corresponding model accuracies with"
        },
        {
          "9": ""
        },
        {
          "9": "the standard deviations are 89.22%±6.05%, 88.39%±6.73%,"
        },
        {
          "9": ""
        },
        {
          "9": "and 87.71%±5.02%.\nIt shows that, with an increase of\nlabel"
        },
        {
          "9": ""
        },
        {
          "9": "noise\nratio\nfrom 10% to\n30%,\nthe model\nperformance\nde-"
        },
        {
          "9": ""
        },
        {
          "9": "creases slightly, with a decrease rate of 1.69%. These results"
        },
        {
          "9": ""
        },
        {
          "9": "demonstrate\nthe proposed PR-PL is\na\nreliable model which"
        },
        {
          "9": ""
        },
        {
          "9": "has\na higher\ntolerance\nto noisy labels.\nIn the\nfuture works,"
        },
        {
          "9": ""
        },
        {
          "9": "the recently proposed novel methods,\nsuch as\n[52] and [53],"
        },
        {
          "9": ""
        },
        {
          "9": "could be incorporated to further eliminate more general noises"
        },
        {
          "9": ""
        },
        {
          "9": "in EEG signals and improve the model stability in the cross-"
        },
        {
          "9": ""
        },
        {
          "9": "corpus applications."
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "C. Confusion Matrices"
        },
        {
          "9": ""
        },
        {
          "9": "To\nqualitatively\nstudy\nthe model\nperformance\nin\neach"
        },
        {
          "9": "emotion\nclass, we\nvisually\nanalyze\nthe\nconfusion matrices"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "REFERENCES"
        },
        {
          "10": ""
        },
        {
          "10": "[1]\nS. Siddharth, T.-P. Jung, and T. J. Sejnowski, “Utilizing deep learning"
        },
        {
          "10": ""
        },
        {
          "10": "towards multi-modal bio-sensing and vision-based affective computing,”"
        },
        {
          "10": "IEEE Transactions on Affective Computing, 2019."
        },
        {
          "10": "[2] X. Hu, J. Chen, F. Wang, and D. Zhang, “Ten challenges for eeg-based"
        },
        {
          "10": ""
        },
        {
          "10": "affective computing,” Brain Science Advances, vol. 5, no. 1, pp. 1–20,"
        },
        {
          "10": ""
        },
        {
          "10": "2019."
        },
        {
          "10": "[3] W. Hu, G. Huang, L. Li, L. Zhang, Z. Zhang, and Z. Liang, “Video-"
        },
        {
          "10": "triggered eeg-emotion public databases and current methods: A survey,”"
        },
        {
          "10": "Brain Science Advances, vol. 6, no. 3, pp. 255–287, 2020."
        },
        {
          "10": ""
        },
        {
          "10": "[4] V.\nJayaram, M. Alamgir, Y. Altun, B.\nScholkopf,\nand M. Grosse-"
        },
        {
          "10": "Wentrup, “Transfer\nlearning in brain-computer\ninterfaces,” IEEE Com-"
        },
        {
          "10": "putational\nIntelligence Magazine, vol. 11, no. 1, pp. 20–31, 2016."
        },
        {
          "10": "[5]\nJ. Li, S. Qiu, Y.-Y. Shen, C.-L. Liu, and H. He, “Multisource transfer"
        },
        {
          "10": ""
        },
        {
          "10": "learning for cross-subject eeg emotion recognition,” IEEE transactions"
        },
        {
          "10": "on cybernetics, vol. 50, no. 7, pp. 3281–3293, 2019."
        },
        {
          "10": "[6] Y. Li, W. Zheng, L. Wang, Y. Zong, and Z. Cui, “From regional to global"
        },
        {
          "10": "brain: A novel hierarchical\nspatial-temporal neural network model\nfor"
        },
        {
          "10": ""
        },
        {
          "10": "eeg emotion recognition,” IEEE Transactions on Affective Computing,"
        },
        {
          "10": "2019."
        },
        {
          "10": "[7] H. Cui, A. Liu, X. Zhang, X. Chen, K. Wang,\nand X. Chen,\n“Eeg-"
        },
        {
          "10": "based\nemotion\nrecognition\nusing\nan\nend-to-end\nregional-asymmetric"
        },
        {
          "10": ""
        },
        {
          "10": "convolutional neural network,” Knowledge-Based Systems, vol. 205, p."
        },
        {
          "10": "106243, 2020."
        },
        {
          "10": "[8]\nP. Zhong, D. Wang, and C. Miao, “Eeg-based emotion recognition using"
        },
        {
          "10": "IEEE Transactions\non Affective\nregularized\ngraph\nneural\nnetworks,”"
        },
        {
          "10": ""
        },
        {
          "10": "Computing, 2020."
        },
        {
          "10": ""
        },
        {
          "10": "[9] X. Gu, Z. Cao, A. Jolfaei, P. Xu, D. Wu, T.-P. Jung, and C.-T. Lin, “Eeg-"
        },
        {
          "10": "based brain-computer\ninterfaces\n(bcis): A survey of\nrecent\nstudies on"
        },
        {
          "10": "signal\nsensing technologies and computational\nintelligence approaches"
        },
        {
          "10": ""
        },
        {
          "10": "and their applications,” IEEE/ACM transactions on computational biol-"
        },
        {
          "10": ""
        },
        {
          "10": "ogy and bioinformatics, 2021."
        },
        {
          "10": "¨\n[10] O.\nOzdenizci, Y. Wang, T. Koike-Akino, and D. Erdo˘gmus¸, “Adversarial"
        },
        {
          "10": "deep learning in eeg biometrics,” IEEE signal processing letters, vol. 26,"
        },
        {
          "10": ""
        },
        {
          "10": "no. 5, pp. 710–714, 2019."
        },
        {
          "10": ""
        },
        {
          "10": "[11] ——,\n“Learning\ninvariant\nrepresentations\nfrom eeg\nvia\nadversarial"
        },
        {
          "10": "inference,” IEEE access, vol. 8, pp. 27 074–27 085, 2020."
        },
        {
          "10": "[12] D. Bethge, P. Hallgarten, T. Grosse-Puppendahl, M. Kari, R. Mikut,"
        },
        {
          "10": "¨\nA. Schmidt, and O.\nOzdenizci, “Domain-invariant\nrepresentation learn-"
        },
        {
          "10": ""
        },
        {
          "10": "ing from eeg with private encoders,” arXiv preprint arXiv:2201.11613,"
        },
        {
          "10": "2022."
        },
        {
          "10": "[13] W.-L. Zheng and B.-L. Lu, “Investigating critical\nfrequency bands and"
        },
        {
          "10": ""
        },
        {
          "10": "channels for eeg-based emotion recognition with deep neural networks,”"
        },
        {
          "10": ""
        },
        {
          "10": "IEEE Transactions on Autonomous Mental Development, vol. 7, no. 3,"
        },
        {
          "10": "pp. 162–175, 2015."
        },
        {
          "10": "[14] Y.\nJia, M. Salzmann,\nand T. Darrell,\n“Factorized\nlatent\nspaces with"
        },
        {
          "10": ""
        },
        {
          "10": "in Neural\nInformation Processing\nstructured\nsparsity.”\nin Advances"
        },
        {
          "10": ""
        },
        {
          "10": "Systems 23: 24th Annual Conference on Neural Information Processing"
        },
        {
          "10": "Systems 2010, 01 2010, pp. 982–990."
        },
        {
          "10": "[15] H. Bao, G. Niu,\nand M.\nSugiyama,\n“Classiﬁcation\nfrom pairwise"
        },
        {
          "10": ""
        },
        {
          "10": "similarity and unlabeled data,” in International Conference on Machine"
        },
        {
          "10": ""
        },
        {
          "10": "Learning.\nPMLR, 2018, pp. 452–461."
        },
        {
          "10": "[16] H. Bao, T. Shimada, L. Xu, I. Sato, and M. Sugiyama, “Similarity-based"
        },
        {
          "10": "classiﬁcation: Connecting similarity learning to binary classiﬁcation,”"
        },
        {
          "10": "2020."
        },
        {
          "10": ""
        },
        {
          "10": "[17] C.-C. Hsu, Y.-X. Zhuang, and C.-Y. Lee, “Deep fake image detection"
        },
        {
          "10": "based on pairwise learning,” Applied Sciences, vol. 10, no. 1, p. 370,"
        },
        {
          "10": "2020."
        },
        {
          "10": "[18]\nP. Zhuang, Y. Wang,\nand Y. Qiao,\n“Learning\nattentive\npairwise\nin-"
        },
        {
          "10": "the AAAI\nteraction for ﬁne-grained classiﬁcation,”\nin Proceedings of"
        },
        {
          "10": ""
        },
        {
          "10": "Conference on Artiﬁcial Intelligence, vol. 34, no. 07, 2020, pp. 13 130–"
        },
        {
          "10": "13 137."
        },
        {
          "10": "[19]\nL. Yao, S. Li, Y. Li, M. Huai,\nJ. Gao, and A. Zhang, “Representation"
        },
        {
          "10": "learning for\ntreatment effect estimation from observational data,” Ad-"
        },
        {
          "10": "vances in Neural\nInformation Processing Systems, vol. 31, 2018."
        },
        {
          "10": "[20] W.-L. Zheng, W. Liu, Y. Lu, B.-L. Lu, and A. Cichocki, “Emotionmeter:"
        },
        {
          "10": "IEEE\nA multimodal\nframework\nfor\nrecognizing\nhuman\nemotions,”"
        },
        {
          "10": "transactions on cybernetics, vol. 49, no. 3, pp. 1110–1122, 2018."
        },
        {
          "10": ""
        },
        {
          "10": "[21]\nS. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, “Domain adaptation via"
        },
        {
          "10": ""
        },
        {
          "10": "transfer component analysis,” IEEE Transactions on Neural Networks,"
        },
        {
          "10": "vol. 22, no. 2, pp. 199–210, 2011."
        },
        {
          "10": "[22] W.-L. Zheng and B.-L. Lu, “Personalizing eeg-based affective models"
        },
        {
          "10": ""
        },
        {
          "10": "the Twenty-Fifth International\nwith transfer learning,” in Proceedings of"
        },
        {
          "10": ""
        },
        {
          "10": "Joint Conference on Artiﬁcial Intelligence, ser. IJCAI’16.\nAAAI Press,"
        },
        {
          "10": "2016, p. 2732–2738."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Recognition, 2018, pp. 8004–8013."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[32]\nJ. Chang, L. Wang, G. Meng, S. Xiang, and C. Pan, “Deep adaptive"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "image clustering,” in 2017 IEEE International Conference on Computer"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Vision (ICCV), 2017, pp. 5880–5888."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[33]\nL.-C. Shi and B.-L. Lu, “Off-line and on-line vigilance estimation based"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "on linear dynamical\nsystem and manifold learning,”\nin 2010 Annual"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "International Conference\nof\nthe\nIEEE Engineering\nin Medicine\nand"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "(a)\n(b)\n(c)",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Biology, 2010, pp. 6587–6590."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[34] W.-L. Zheng, Y.-Q. Zhang,\nJ.-Y. Zhu,\nand B.-L. Lu,\n“Transfer\ncom-"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "ponents between subjects\nfor eeg-based emotion recognition,” in 2015"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "International Conference on Affective Computing and Intelligent\nInter-"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "action (ACII), 2015, pp. 917–922."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[35] Breiman, “Random forests,” Machine Learning, vol. 45, no. 1, pp. 5–32,"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "2001."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[36]\n“Alternative k-nearest neighbour rules in supervised pattern recognition:"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "(d)\n(e)\n(f)",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Part\n1.\nk-nearest\nneighbour\nclassiﬁcation\nby\nusing\nalternative"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "voting\nrules,”\nAnalytica Chimica Acta,\nvol.\n136,\npp.\n15–27,\n1982."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[Online].\nAvailable:\nhttps://www.sciencedirect.com/science/article/pii/"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "S0003267001953590"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[37]\nJ. Suykens and J. Vandewalle, “Least\nsquares\nsupport vector machine"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "Fig. 3: T-SNE visualization of\nthe learned features\nfrom the",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "classiﬁers,” Neural Processing Letters, vol. 9, no. 3, pp. 293–300, 1999."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "source and the target domains using different model settings.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[38]\nJ. Zhu, A. Arbor, and T. Hastie, “Multi-class adaboost,” Statistics & Its"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "(a),\n(b)\nand\n(c)\nare\nthe\nsample\nfeatures\nextracted\nby w/o",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Interface, vol. 2, no. 3, pp. 349–360, 2006."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "pairwise\nlearning\non\nthe\nsource\nand\ntarget, w/o\npairwise",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[39] B. Sun,\nJ. Feng, and K. Saenko, “Return of\nfrustratingly easy domain"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "the Thirtieth AAAI Conference on Artiﬁ-\nadaptation,” in Proceedings of"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "learning on the\ntarget,\nand PR-PL.\n(d),\n(e)\nand (f)\nare\nthe",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "cial\nIntelligence, ser. AAAI’16.\nAAAI Press, 2016, p. 2058–2065."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "interaction features extracted by w/o pairwise learning on the",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[40] B. Gong, Y.\nShi,\nF. Sha,\nand K. Grauman,\n“Geodesic\nﬂow kernel"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "source\nand target, w/o pairwise\nlearning on the\ntarget,\nand",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "2012\nIEEE Conference\non\nfor\nunsupervised\ndomain\nadaptation,”\nin"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Computer Vision and Pattern Recognition, 2012, pp. 2066–2073."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "PR-PL.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[41] B. Sun and K. Saenko,\n“Deep coral: Correlation alignment\nfor deep"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "domain\nadaptation,”\nin Computer Vision\n– ECCV 2016 Workshops,"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "G. Hua and H. J´egou, Eds.\nCham: Springer\nInternational Publishing,"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "2016, pp. 443–450."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "[23] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavi-",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[42] M. Long, Y. Cao,\nJ. Wang,\nand M.\nJordan,\n“Learning\ntransferable"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "olette, M. Marchand, and V. Lempitsky, “Domain-adversarial\ntraining",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "features with deep adaptation networks,” in International conference on"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "of neural networks,” The journal of machine learning research, vol. 17,",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "machine learning.\nPMLR, 2015, pp. 97–105."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "no. 1, pp. 2096–2030, 2016.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[43]\nE. Tzeng,\nJ. Hoffman, N. Zhang, K. Saenko,\nand T. Darrell,\n“Deep"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "[24] H. Li, Y.-M.\nJin, W.-L. Zheng, and B.-L. Lu, “Cross-subject emotion",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "domain\nconfusion: Maximizing\nfor\ndomain\ninvariance,” CoRR,\nvol."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "Information\nrecognition\nusing\ndeep\nadaptation\nnetworks,”\nin Neural",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "abs/1412.3474, 2014. [Online]. Available: http://arxiv.org/abs/1412.3474"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "Processing, L. Cheng, A. C. S. Leung, and S. Ozawa, Eds.\nCham:",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[44] Y. Li, L. Wang, W. Zheng, Y. Zong, L. Qi, Z. Cui, T. Zhang,\nand"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "Springer\nInternational Publishing, 2018, pp. 403–413.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "T. Song, “A novel bi-hemispheric discrepancy model\nfor eeg emotion"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "[25] Y. Li, W. Zheng, Y. Zong, Z. Cui, T. Zhang,\nand X. Zhou,\n“A bi-",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "recognition,” IEEE Transactions on Cognitive and Developmental Sys-"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "hemisphere domain adversarial neural network model\nfor eeg emotion",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "tems, vol. 13, no. 2, pp. 354–367, 2020."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "recognition,” IEEE Transactions on Affective Computing, 2018.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[45]\nT. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "[26] X. Du, C. Ma, G. Zhang, J. Li, Y.-K. Lai, G. Zhao, X. Deng, Y.-J. Liu,",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "dynamical graph convolutional neural networks,” IEEE Transactions on"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "and H. Wang, “An efﬁcient\nlstm network for emotion recognition from",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2018."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "multichannel eeg signals,” IEEE Transactions on Affective Computing,",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "pp. 1–1, 2020.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[46] B.-Q. Ma, H. Li, W.-L. Zheng,\nand B.-L. Lu,\n“Reducing the\nsubject"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "variability of\neeg signals with adversarial domain generalization,”\nin"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "[27] Y. Luo, S. Y. Zhang, W. L. Zheng,\nand B. L. Lu,\n“Wgan\ndomain",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Neural\nInformation Processing, T. Gedeon, K. W. Wong, and M. Lee,"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "adaptation for eeg-based emotion recognition,” International Conference",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Eds.\nCham: Springer\nInternational Publishing, 2019, pp. 30–42."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "Information Processing, 2018.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": ""
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "[28]\nJ. Li, S. Qiu, C. Du, Y. Wang,\nand H. He,\n“Domain adaptation for",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[47] H. Chen, Z. Li, M.\nJin,\nand J. Li,\n“Meernet: Multi-source\neeg-based"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "eeg emotion recognition based on latent representation similarity,” IEEE",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "emotion\nrecognition\nnetwork\nfor\ngeneralization\nacross\nsubjects\nand"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "Transactions on Cognitive and Developmental Systems, vol. 12, no. 2,",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "International Conference of\nthe IEEE\nsessions,” in 2021 43rd Annual"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "pp. 344–353, 2020.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Engineering in Medicine & Biology Society (EMBC).\nIEEE, 2021, pp."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "[29] M.\nSalzmann, C. H.\nEk, R. Urtasun,\nand\nT. Darrell,\n“Factorized",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "6094–6097."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "orthogonal latent spaces,” Journal of Machine Learning Research, vol. 9,",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[48] Y. Li, W. Zheng, Z. Cui, Y. Zong, and S. Ge, “Eeg emotion recognition"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "pp. 701–708, 2010.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "based on graph regularized sparse linear regression,” Neural Processing"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "[30] K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan,",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "Letters, vol. 49, p. 1–17, 04 2019."
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "“Domain separation networks,” ser. NIPS’16, 2016, p. 343–351.",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "[49] W. Zheng,\n“Multichannel\neeg-based\nemotion\nrecognition\nvia\ngroup"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "[31]\nP. O. Pinheiro, “Unsupervised domain adaptation with similarity learn-",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "sparse canonical correlation analysis,” IEEE Transactions on Cognitive"
        },
        {
          "Fig. 2: Confusion matrices of different models.\n(a) BiDANN [25],": "ing,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern",
          "(b) BiHDM [44],\n(c) RGNN [8], and (d) PR-PL.": "and Developmental Systems, vol. 9, no. 3, pp. 281–290, 2017."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "dynamical graph convolutional neural networks,” IEEE Transactions on"
        },
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2020."
        },
        {
          "[50]": "[51]",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "T. Zhang, W. Zheng, Z. Cui, Y. Zong,\nand Y. Li,\n“Spatial–temporal"
        },
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "recurrent neural network for emotion recognition,” IEEE Transactions"
        },
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "on Cybernetics, vol. 49, no. 3, pp. 839–847, 2019."
        },
        {
          "[50]": "[52] X. Xiao, M. Xu,",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "J.\nJin, Y. Wang, T.-P.\nJung, and D. Ming, “Discrim-"
        },
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "inative canonical pattern matching for\nsingle-trial classiﬁcation of erp"
        },
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "components,” IEEE Transactions on Biomedical Engineering, vol. 67,"
        },
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "no. 8, pp. 2266–2275, 2019."
        },
        {
          "[50]": "[53]",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "J.\nJin, Z. Wang, R. Xu, C. Liu, X. Wang, and A. Cichocki, “Robust"
        },
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "similarity measurement based on a novel time ﬁlter for ssveps detection,”"
        },
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "IEEE Transactions on Neural Networks and Learning Systems, 2021."
        },
        {
          "[50]": "[54] V. D. M. Laurens and G. Hinton, “Visualizing data using t-sne,” Journal",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": ""
        },
        {
          "[50]": "",
          "T. Song, W. Zheng, P. Song, and Z. Cui, “Eeg emotion recognition using": "of Machine Learning Research, vol. 9, no. 2605, pp. 2579–2605, 2008."
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "S Siddharth",
        "T.-P Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Ten challenges for eeg-based affective computing",
      "authors": [
        "X Hu",
        "J Chen",
        "F Wang",
        "D Zhang"
      ],
      "year": "2019",
      "venue": "Brain Science Advances"
    },
    {
      "citation_id": "3",
      "title": "Videotriggered eeg-emotion public databases and current methods: A survey",
      "authors": [
        "W Hu",
        "G Huang",
        "L Li",
        "L Zhang",
        "Z Zhang",
        "Z Liang"
      ],
      "year": "2020",
      "venue": "Brain Science Advances"
    },
    {
      "citation_id": "4",
      "title": "Transfer learning in brain-computer interfaces",
      "authors": [
        "V Jayaram",
        "M Alamgir",
        "Y Altun",
        "B Scholkopf",
        "M Grosse-Wentrup"
      ],
      "year": "2016",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "5",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "6",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Eegbased emotion recognition using an end-to-end regional-asymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "8",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Eegbased brain-computer interfaces (bcis): A survey of recent studies on signal sensing technologies and computational intelligence approaches and their applications",
      "authors": [
        "X Gu",
        "Z Cao",
        "A Jolfaei",
        "P Xu",
        "D Wu",
        "T.-P Jung",
        "C.-T Lin"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on computational biology and bioinformatics"
    },
    {
      "citation_id": "10",
      "title": "Adversarial deep learning in eeg biometrics",
      "authors": [
        "O Özdenizci",
        "Y Wang",
        "T Koike-Akino",
        "D Erdogmus"
      ],
      "year": "2019",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "11",
      "title": "Learning invariant representations from eeg via adversarial inference",
      "year": "2020",
      "venue": "IEEE access"
    },
    {
      "citation_id": "12",
      "title": "Domain-invariant representation learning from eeg with private encoders",
      "authors": [
        "D Bethge",
        "P Hallgarten",
        "T Grosse-Puppendahl",
        "M Kari",
        "R Mikut",
        "A Schmidt",
        "O Özdenizci"
      ],
      "year": "2022",
      "venue": "Domain-invariant representation learning from eeg with private encoders",
      "arxiv": "arXiv:2201.11613"
    },
    {
      "citation_id": "13",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "14",
      "title": "Factorized latent spaces with structured sparsity",
      "authors": [
        "Y Jia",
        "M Salzmann",
        "T Darrell"
      ],
      "year": "2010",
      "venue": "Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Classification from pairwise similarity and unlabeled data",
      "authors": [
        "H Bao",
        "G Niu",
        "M Sugiyama"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Similarity-based classification: Connecting similarity learning to binary classification",
      "authors": [
        "H Bao",
        "T Shimada",
        "L Xu",
        "I Sato",
        "M Sugiyama"
      ],
      "year": "2020",
      "venue": "Similarity-based classification: Connecting similarity learning to binary classification"
    },
    {
      "citation_id": "17",
      "title": "Deep fake image detection based on pairwise learning",
      "authors": [
        "C.-C Hsu",
        "Y.-X Zhuang",
        "C.-Y Lee"
      ],
      "year": "2020",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "18",
      "title": "Learning attentive pairwise interaction for fine-grained classification",
      "authors": [
        "P Zhuang",
        "Y Wang",
        "Y Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Representation learning for treatment effect estimation from observational data",
      "authors": [
        "L Yao",
        "S Li",
        "Y Li",
        "M Huai",
        "J Gao",
        "A Zhang"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "21",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "22",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, ser. IJCAI'16"
    },
    {
      "citation_id": "23",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "24",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "25",
      "title": "A bihemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Wgan domain adaptation for eeg-based emotion recognition",
      "authors": [
        "Y Luo",
        "S Zhang",
        "W Zheng",
        "B Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "28",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "29",
      "title": "Factorized orthogonal latent spaces",
      "authors": [
        "M Salzmann",
        "C Ek",
        "R Urtasun",
        "T Darrell"
      ],
      "year": "2010",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "30",
      "title": "Domain separation networks",
      "authors": [
        "K Bousmalis",
        "G Trigeorgis",
        "N Silberman",
        "D Krishnan",
        "D Erhan"
      ],
      "year": "2016",
      "venue": "ser. NIPS'16"
    },
    {
      "citation_id": "31",
      "title": "Unsupervised domain adaptation with similarity learning",
      "authors": [
        "P Pinheiro"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "32",
      "title": "Deep adaptive image clustering",
      "authors": [
        "J Chang",
        "L Wang",
        "G Meng",
        "S Xiang",
        "C Pan"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "33",
      "title": "Off-line and on-line vigilance estimation based on linear dynamical system and manifold learning",
      "authors": [
        "L.-C Shi",
        "B.-L Lu"
      ],
      "year": "2010",
      "venue": "2010 Annual International Conference of the IEEE Engineering in Medicine and Biology"
    },
    {
      "citation_id": "34",
      "title": "Transfer components between subjects for eeg-based emotion recognition",
      "authors": [
        "W.-L Zheng",
        "Y.-Q Zhang",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "35",
      "title": "Random forests",
      "authors": [
        "Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "36",
      "title": "Alternative k-nearest neighbour rules in supervised pattern recognition: Part 1. k-nearest neighbour classification by using alternative voting rules",
      "year": "1982",
      "venue": "Analytica Chimica Acta"
    },
    {
      "citation_id": "37",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "38",
      "title": "Multi-class adaboost",
      "authors": [
        "J Zhu",
        "A Arbor",
        "T Hastie"
      ],
      "year": "2006",
      "venue": "Statistics & Its Interface"
    },
    {
      "citation_id": "39",
      "title": "Return of frustratingly easy domain adaptation",
      "authors": [
        "B Sun",
        "J Feng",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, ser. AAAI'16"
    },
    {
      "citation_id": "40",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Deep coral: Correlation alignment for deep domain adaptation",
      "authors": [
        "B Sun",
        "K Saenko"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 Workshops"
    },
    {
      "citation_id": "42",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "M Long",
        "Y Cao",
        "J Wang",
        "M Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "43",
      "title": "Deep domain confusion: Maximizing for domain invariance",
      "authors": [
        "E Tzeng",
        "J Hoffman",
        "N Zhang",
        "K Saenko",
        "T Darrell"
      ],
      "year": "2014",
      "venue": "CoRR"
    },
    {
      "citation_id": "44",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "45",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Reducing the subject variability of eeg signals with adversarial domain generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "47",
      "title": "Meernet: Multi-source eeg-based emotion recognition network for generalization across subjects and sessions",
      "authors": [
        "H Chen",
        "Z Li",
        "M Jin",
        "J Li"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "48",
      "title": "Eeg emotion recognition based on graph regularized sparse linear regression",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "S Ge"
      ],
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "49",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "W Zheng"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "50",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "52",
      "title": "Discriminative canonical pattern matching for single-trial classification of erp components",
      "authors": [
        "X Xiao",
        "M Xu",
        "J Jin",
        "Y Wang",
        "T.-P Jung",
        "D Ming"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "53",
      "title": "Robust similarity measurement based on a novel time filter for ssveps detection",
      "authors": [
        "J Jin",
        "Z Wang",
        "R Xu",
        "C Liu",
        "X Wang",
        "A Cichocki"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "54",
      "title": "Visualizing data using t-sne",
      "authors": [
        "V Laurens",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}