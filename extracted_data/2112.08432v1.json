{
  "paper_id": "2112.08432v1",
  "title": "Expert And Crowd-Guided Affect Annotation And Prediction",
  "published": "2021-12-15T19:20:04Z",
  "authors": [
    "Ramanathan Subramanian",
    "Yan Yan",
    "Nicu Sebe"
  ],
  "keywords": [
    "Index Terms-Crowdsourcing",
    "Affect annotation and Prediction",
    "Multi-task learning",
    "Expert-guided"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We employ crowdsourcing to acquire time-continuous affective annotations for movie clips, and refine noisy models trained from these crowd annotations incorporating expert information within a Multi-task Learning (MTL) framework. We propose a novel expert guided MTL (EG-MTL) algorithm, which minimizes the loss with respect to both crowd and expert labels to learn a set of weights corresponding to each movie clip for which crowd annotations are acquired. We employ EG-MTL to solve two problems, namely, P1: where dynamic annotations acquired from both experts and crowdworkers for the Validation set are used to train a regression model with audio-visual clip descriptors as features, and predict dynamic arousal and valence levels on 5-15 second snippets derived from the clips; and P2: where a classification model trained on the Validation set using dynamic crowd and expert annotations (as features) and static affective clip labels is used for binary emotion recognition on the Evaluation set for which only dynamic crowd annotations are available. Observed experimental results confirm the effectiveness of the EG-MTL algorithm, which is reflected via improved arousal and valence estimation for P1, and higher recognition accuracy for P2.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "A Ffective video tagging has been acknowledged as an important multimedia problem for long, given its utility for applications such as personalized media recommendation and video summarization. For over a decade, many affective tagging methodologies have been proposed in literature-these approaches can be broadly classified as content-centric  [12] ,  [41] , which analyze audio-visual content in the stimulus to predict the conveyed emotion, or usercentric  [1] ,  [3] ,  [15] ,  [17] ,  [18] ,  [31] ,  [36]  that infer the evoked emotion by examining the viewer's facial expressions and physiological responses (e.g., brain signals, eye and muscle movements). While human behavioral signals such as cognitive and emotional states are inherently dynamic and constantly evolve over time, most affective tagging approaches only associate a single, static emotional label to the stimulus.\n\nThe above limitation in affective computing can be attributed to a number of factors. Firstly, interpreting and measuring emotion in terms of arousal and valence 1 is an inherently difficult problem-emotion is a highly subjective feeling, and the discrepancy between the emotion envisioned by the content creator versus the actual emotion evoked in viewers has been highlighted by many works. This subjectivity results in noisy or 'ill-generalizable' machine learning models  [26] , and state-of-the-art static emotion tagging methods can only achieve less than 70% accuracy employing multimodal user signals recorded in controlled\n\n• Ramanathan Subramanian is with the University of Canberra, Bruce, ACT 2617. (Email: ramanathan.subramanian@ieee.org) • Yan Yan is with the Department of Computer Science, Illinois Institute of Technology, 60616 Illinois, USA. (Email: yyan34@iit.edu) • Nicu Sebe is with the Dept. of Information Engineering and Computer Science, University of Trento, 38123 Trento, Italy. (Email: sebe@disi.unitn.it)\n\n1. We will only discuss the dimensional or circumplex model of affect  [30]  in this work. lab conditions. Secondly, acquiring a sufficient number of time-continuous emotional annotations to model the dynamic 'ground-truth' is an extremely tedious and difficult task as humans are better at rating attributes (especially behavioral) in relative rather than absolute terms  [23] ,  [32] .\n\nOne attractive solution to acquire large amounts of dynamic annotations is crowdsourcing (CS). Recently, CS has become popular for performing tedious and large-scale annotation tasks via human collaboration via the Internet. When it is difficult to employ a sufficient number of experts for analyzing extensive data, CS is an efficient alternative as many individuals work on small data chunks to provide useful information in the form of tags. CS has been successfully employed to develop data-driven solutions for computationally difficult problems in multiple domains like natural language processing  [42]  and computer vision  [46] . Two reasons mainly contribute to the success of CS-  (1)  crowd workers are paid a fraction of the wages that experts are entitled to, thereby achieving cost efficiency, and (2) the experimenter's task becomes scalable when the original task is split into smaller and manageable microtasks and distributed among crowdworkers. Nevertheless, cost-effectiveness is achieved at the expense of expertise; crowdworkers may lack the motivation and/or technical and cognitive skills to effectively perform a given task  [29] . Therefore, efficient machine learning techniques robust to noisy data are crucial to the success of CS approaches.\n\nThis paper is an extension of the work presented in  [2] , where Multi-task learning (MTL) was employed to learn a crowd-based model for predicting dynamic arousal (A) and valence (V) levels in movie snippets. Given a set of related tasks, MTL simultaneously learns all tasks by modeling the similarities as well as differences among them to build task-specific classification or regression models. This joint learning accounting for task relationships is more efficient than learning each task independently. Given a set of movie clips whose emotional attributes are known, say via an oracle, one can expect clips of similar nature (e.g., high valence) to have some similarities in terms of content, and conversely, in terms of the responses they evoke from viewers. Therefore, MTL can be seen as a naturally useful tool for large-scale affective media tagging. We extend the MTL idea by incorporating expert knowledge in the learning process to refine crowd models.\n\nIn this work, we seek to improve the efficacy of crowdbased models by employing a small amount of expert data to guide the learning process-we employed 16 experts familiar with emotional attributes to provide dynamic A,V ratings for the clips annotated by crowdworkers  2  , and enhance crowd models employing expert knowledge via a novel expert-guided MTL (EG-MTL) algorithm. The EG-MTL algorithm seeks to simultaneously minimize the loss with respect to both crowd and expert labels in the optimization framework and learns a set of weights corresponding to each of the movie clips for which crowd annotations were sought. The learning framework is flexible, i.e., it is guided by both a-priori knowledge embedded in the form of a graph, as well as data descriptors.\n\nMore specifically, we employ EG-MTL to solve problems P1 and P2 described below: P1: For a dataset comprising both crowd and expert dynamic emotion annotations (termed Validation set), refine a regression model trained using crowd labels and audio-visual clip descriptors via expert knowledge  3 for dynamic A,V prediction on 5-15 second snippets (derived from the same clips). P2: Train a binary classification (emotion recognition) model on the Validation set using static 'ground-truth' A,V labels and the dynamic A,V crowd ratings as features. Incorporate expert knowledge to refine this model and enhance recognition on a second Evaluation set, for which only dynamic crowd ratings are available.\n\nWe show that the performance of crowd-based models trained for P1 and P2 can be significantly improved upon incorporating expert information via EG-MTL. Overall, this work makes the following research contributions:\n\n1. While other works such as  [26] ,  [28]  have focused on fusing information from multiple annotators to obtain a representative 'gold standard' annotation, this is the first affective computing work that focuses on enhancing noisy crowd-based models via expert knowledge. 2. This is also the first work to employ Multi-task Learning for continuous/static emotion prediction. MTL can effectively reveal latent relationships between related tasks, and we employ the same to investigate a) similarities among time-continuous annotations of multiple raters, and b) similarities among audio-visual features corresponding to high/low arousal and valence content. The proposed EG-MTL algorithm is flexible as the learning is guided by both a-priori knowledge defining task relationships in the form of a graph, as well as data descriptors employed for model training. 3. Different from prior works such as  [23] ,  [32]  which have cursorily examined the relationship between dynamic and static A,V labels, this work more thoroughly examines the influence of dynamic emotion changes on the overall emotion evoked by a movie scene. As typified by problem P2, we explicitly attempt static A,V prediction from dynamic A,V profiles. 4. Given that movie clips are unique stimuli in the sense that they convey emotions more dynamically and effectively than other media (e.g., music videos as shown in  [1] ), we believe that the compiled dynamic expert and crowd annotations will be of great value to the affective computing community. These annotations will be made publicly available for future research 4 . The paper is organized as follows: Section 2 overviews the literature. Description of the stimuli and protocol employed for acquiring affective annotations is presented in Section 3. A brief overview of various MTL baselines along with a description of the proposed EG-MTL algorithm is provided in Section 4. Annotation data analysis and emotion prediction experiments are detailed in Section 5. Conclusions are stated in Section 6.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "This section examines related work on (1) Crowdsourcing for media processing, (2) Affective analysis and tagging, (3) CS for affective Computing and (4) Multi-task learning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Crowdsourcing For Media Processing",
      "text": "Steiner et al.  [37]  defined three types of video events, namely, visual events, occurrence events and Internet-based events, and showed that these events can be detected from video sequences via crowdsourcing upon combining textual, visual and behavioral cues. Vondrick et al.  [40]  argued that frameby-frame video annotation is essential for a variety of tasks, as in the case of time-continuous emotion measurement, even if it is tedious to accomplish for human annotators. An online framework for collecting valid facial responses to media content was proposed by McDuff et al.  [22] , who found significant differences between subgroups who liked/disliked or were familiar/unfamilar with a particular commercial.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Affective Analysis And Tagging",
      "text": "A primary issue in affective multimedia analysis is the inherent difficulty in finding a sufficient number of knowledgable annotators so as to generate reliable ground-truth labels for model training. Consequently, only a few annotators are used in a majority of affective studies  [33] ,  [41] . Also, emotion perception varies with individual traits such as personality  [16] , and considerable differences may be observed in affective ratings compiled from different persons over a small population. To address this problem, a number of works have turned to crowdsourcing or large-scale user studies. In a seminal study affective movie study, Gross et al.  [11]  compiled a benchmark collection of movie clips for evoking eight emotional states such as anger, disgust, fear and neutral, based on emotion ratings compiled for 250 movie clips from 954 subjects.\n\nDEAP  [17] , MAHNOB  [36]  and DECAF  [1]  are three recent works that have attempted user-centric static affect recognition via physiological responses compiled from 27-40 users to music and movie stimuli. Given the aforementioned differences in emotion perception and the rather small number of participants involved, it is unsurprising that these state-of-the-art approaches do not achieve high recognition accuracies, even with user data compiled under controlled lab conditions. Challenges in acquiring timecontinuous emotion annotations on a large database are discussed in  [23] . This work highlights some inherent problems concerning dynamic annotation of emotional attributes such as (i) definition of fuzzy attributes such as A,V being lessintuitive to some annotators, (ii) humans being more adept at rating relative rather than absolute attributes, and (iii) the existence of subject-specific annotation time-delays between occurrence of an emotional event and its annotation for continuous ratings. Among the handful of works that have attempted continuous emotion prediction, Nicolau et al.  [25]  proposed a output-associative relevance vector machine based regression framework for A,V estimation from multiple non-verbal cues such as facial expressions, shoulder movements and audio cues.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Crowdsourcing For Affective Computing",
      "text": "Soleymani et al.  [34]  performed CS on a limited scale to collect 1300 affective annotations from 40 volunteers for 155 Hollywood movie clips. In another CS-based affective video annotation study, Soleymani et al.  [35]  compiled annotations for the MediaEval 2010 Affect Task Corpus on AMT, and asked workers to self-report their boredom levels. In a recent CS-based media tagging work, Soleymani et al.  [32]  presented a dataset of 1000 songs for music emotion analysis, each annotated continuously over time by at least 10 users. Nevertheless, movies best approximate the real world and are more effective in eliciting emotions from viewers as compared to musical content as shown in in  [1] , which is why we believe continuous emotion annotation and prediction in movie stimuli is valuable in the context of affective media representation and modeling. Also, fusing noisy annotations from multiple crowdworkers to obtain a representative ground-truth annotation becomes crucial in the context of CS-Raykar et al.  [28]  proposed an annotation fusion mechanism where the multiple annotations are assumed to be Gaussian distributed, whose mean represents the true label and whose variance is denoted by the annotation noise. Nicolaou et al.  [26]  proposed an improved annotation fusion methodology for continuous annotations employing probabilistic canonical correlation analysis (PCCA). Their approach also included a latent time warping process to account for annotator-specific temporal lags.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Task Learning",
      "text": "Recently, multi-task learning (MTL) has been employed in several computer vision applications such as image classification  [45] , head pose estimation  [43]  and multi-view action recognition  [44] . Given a set of related tasks, MTL  [7]  seeks to simultaneously learn a set of task-specific classification or regression models. The intuition behind MTL is simple: a joint learning procedure which accounts for task relationships is expected to lead to more accurate models as compared to learning each task separately. While MTL has been used previously for learning from noisy crowd annotations  [14] , MTL has not been hitherto used for affective media tagging.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Analysis Of Related Work",
      "text": "A careful examination of related literature suggests that (i) Due to the subjectivity in emotion perception, a vast majority of affective computing studies have involved small or mid-sized user populations, and only recently, has attention been devoted to compiling a large repository of affective annotations via crowdsourcing. (ii) Most affective studies have been restricted to predicting the overall emotion evoked by stimuli rather than the continuous emotion profile-this is mainly due to the fact that acquiring reliable dynamic affective annotations to serve as ground-truth is both difficult and tedious. Of late, there has been increasing interest in continuous emotion prediction. (iii) Fusion of multiple annotations to synthesize a representative annotation that can be used for training generalizable models is itself a nontrivial task.\n\nThis work represents one of the first attempts to take affective computing 'beyond the lab', and focuses on refining models trained from dynamic crowd annotations using a small amount of expert knowledge. Note that a similar framework can be employed to harness non-verbal behavioral cues as well. The following section details the stimuli used and protocol adopted for compiling affective crowd annotations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "For this study, we compiled the Validation (Val) and Evaluation (Eval) datasets containing arousal (A) and valence (V) annotations from crowdworkers and/or experts. Characteristics of the Val and Eval sets are summarized in Table  1 . For both the Val and Eval sets, time-continuous A,V annotations were acquired for 12 affective movie clips used in DECAF  [1] . The main differences between crowdworkers and experts are as follows: (1) Experts were individuals familiar with affective research and the definition of dimensional emotional attributes such as valence and arousal, but crowdworkers were not; (2) Experts were explicitly asked to view the movie clips as many times as required in order to familiarize themselves with the emotional dynamics prior to clip rating, but crowdworkers were given no such instruction, and (3) Experts were asked to provide ratings that 'reflected the level of A/V meant to be elicited in the viewer' during each time instant so as to mimic the movie director's intentions, while crowdworkers were asked to report 'how they felt' through the course of the scene.\n\nFor the Val set, dynamic emotional annotations were acquired from both crowdworkers (via the CrowdFlower platform as detailed below) and 16 experts (via the GTrace interface  [9] ) which records A,V ratings in the range [-1, 1]. For the purpose of sanity checking and to examine the relationship between dynamic and static emotional ratings 5 , crowdworkers were also asked to report their overall affective impression of the clip. For the Eval set, only continuous A,V annotations were obtained using GTrace from 35 subjects, mainly undergraduate and graduate students naive to the purpose of this study-they are also referred to as a 'crowd' in this paper since their characteristics and instructions provided were identical to those of the crowdworkers mentioned above. We now describe some salient aspects of the data acquisition process, focusing mainly on the framework employed for acquiring crowd annotations for the Val set.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Movie Clips",
      "text": "All movie clips used in this work were adopted from DECAF  [1] , and for each of the Val and Eval sets, 12 clips equally distributed among the four quadrants, namely, high A high V (HAHV). low A high V (LAHV), low A low V (LALV) and high A high V (HAHV) in the AV space were used as shown in Table  2 . The movie clips are about 1-1.5 minutes long, and the DECAF study also provides a characteristic emotion tag and a mean static A,V rating for each clip in the range [-2, 2], based on self-reports from 42 viewers-these ratings were used to determine static, binary 'ground-truth' labels for the Val and Eval movie clips.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Crowd Annotation Protocol (Val Set)",
      "text": "We posted the annotation task for Val clips on Amazon Mechanical Turk (AMT) and other CS channels via the Crowd-Flower (CF) platform. CF is an intermediate platform for posting the AMT task on our behalf. Moreover, CF provides a simple qualification mechanism to discard outliers. If workers passed the qualification test, they were considered qualified enough to perform a given task. However, predesigned tests are very generic and limited to simple tasks, which do not allow for trivially discarding bad-quality annotations. So, we performed PHP server-side scripting and redirection, collection and evaluation of all annotations real-time on our server via HTTP requests, before letting workers submit the task. The architecture of the designed CS platform is shown in Fig.  1 (a).\n\nTo ensure high-quality annotations, each crowdworker could only annotate 5 movie clips, and at least 15 dynamic A,V annotations were collected for each movie clip. We also recorded facial expressions of crowdworkers (not used in this work) as they performed the annotations. Informed consent was obtained from workers and, workers had to 5. This is left to future work and not investigated in this study.\n\nprovide their demographics (age, gender and location) prior to the task. Time-continuous A/V ratings were compiled from workers over separate sessions (a worker need not annotate for both valence and arousal for the same clip under this setting), and workers were also required to rate each clip for static/overall arousal or valence. Each worker was paid 10 cents/video upon successful task completion.\n\nWorkers did not get paid if their annotations and webcam facial videos were not recorded on our server. To evaluate the annotation quality, each video annotation was logged in XML format and analyzed. A continuous slider was used to record emotional rating, and if the slider had not moved for more than 80% of the clip duration, or if more than 20% of the data was lost, the annotation was automatically discarded. Also, files smaller than a threshold size were discarded. If the annotation task was left incomplete, a warning message notified the worker about the same. Workers could then re-annotate the video and get paid. Furthermore, some constraints were implemented to maintain annotation quality such as: (1) Workers could not play (or rate) multiple video clips simultaneously. (2) Workers could annotate a video as many times as they wanted to (the most recent annotation overrode previous ones in such cases). (  3 ) Workers were allowed to use only the Chrome browser for annotation due to unavailability of HTML5 technology support in other browsers. (4) Media player controllers were removed from the interface so that workers could not fast forward/rewind the movie clips, and finally, (5) If the annotation was stopped midway, it had to be redone from scratch.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Annotation Mechanism",
      "text": "A screen shot of the user interface for recording annotations on AMT is presented in Fig.  1(b ). The following components were part of the continuous annotation and facial expression recording process.\n\nVideo Player: To provide an uninterrupted video stream for workers with low bandwidth, we uploaded the Val movie clips onto YouTube. On the client-side, YouTube JavaScript player API was integrated and used in our web-based user interface.\n\nSlider: A slider was used to record time-continuous A,V ratings of workers watching movie clips. The slider values ranged from -2 to 2 for both factors (very unpleasant to very pleasant for valence, and calm to highly excited for arousal). In order to facilitate workers' decision making, a standard visual scale Self Assessment Manikin (SAM) image was displayed.\n\nWebcam Panel: To upload workers' facial expressions in real-time, we used HTML5 technology to buffer the worker's webcam recording on the client-side once the play button was pressed. The buffered video was compressed and uploaded on our server upon clip completion.\n\nQuestionnaires: To ensure that workers were fully engaged while performing the annotation task, they needed to report (1) their overall A/V rating for each viewed clip  on a scale of -2 to 2, and (2) their familiarity with the clip to examine if it biased their ratings.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Annotation Statistics And Pre-Processing",
      "text": "Overall, 1012 and 527 workers provided continuous valence and arousal ratings respectively for the Val clips. Their age, gender and locality distributions are shown in Fig.  1(c), (d)  and (e). As a preliminary step to eliminate bad-quality annotations, we discarded those annotations with (1) more than threshold missing values, (2) less than standard deviation threshold, and (3) missing or inconsistent static ratings, where sign of the overall rating was opposite to that of the maximum continuous annotation value.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Expert-Guided Multi-Task Learning (Eg-Mtl)",
      "text": "In this section, we first provide a brief description of multi-task learning (MTL) and the MTL baselines used for performance evaluation, before moving on to describe the proposed EG-MTL algorithm. Multi-task learning (MTL) exploits the relationships among a set of associated tasks to learn both inter-task similarities as well as task-specific differences, which is more beneficial as compared to learning task-specific models. Given a set of tasks t = 1...T , with X t denoting training data for the task t and Y t their corresponding labels, MTL seeks to jointly learn a set of weights W = [W 1 ..W T ], where W t models task t. In Section 5, we will examine the relationship between (i) continuous and static affective movie clip labels, and (ii) continuous A,V labels and audio-visual features via MTL variants available as part of the MALSAR library  [47] , which are described below:\n\nMulti-task Lasso (MT-Lasso): extends the Lasso algorithm  [38]  to MTL, and assumes that sparsity is shared among all tasks. It attempts to minimize the objective function\n\n, where . F and . 1 denote the Frobenius (L2) and L1 norm respectively. Regularization parameter α controls sparsity, while β controls the 2 -norm penalty.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Norm-Regularized Mtl ( -Mtl [4]): Minimizes The Objective Function",
      "text": "F , where . 2,1 denotes the matrix 21 norm. The underlying assumption in this model is that all tasks are related, which is not always true, and can negatively impact model performance. α, β denote regularization parameters controlling group and L2-norm sparsity respectively.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dirty Mtl [13]:",
      "text": "The key idea of dirty MTL is to decompose W into P and Q such that W = P + Q, where P and Q denote the group and task-wise sparse components. The algorithm attempts to minimize the objective function\n\nwhere ρ 1 controls the group sparsity on P , while ρ 2 controls sparsity on Q.\n\nRobust MTL  [10] : Robust MTL assumes that the model W can be decomposed into two components: a shared feature structure P that captures task-relatedness, and a groupsparse structure Q that detects outliers. If the task is not an outlier, then it falls into the joint feature structure P with its corresponding column in Q being a zero vector; if not, then the Q matrix has non-zero entries at the corresponding column. The algorithm minimizes the objective function\n\n, such that W = P + Q, where regularization parameter ρ 1 controls joint feature learning, while parameter ρ 2 controls the column-wise group sparsity on Q that detects outliers.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Sparse Graph Regularization (Sr-Mtl):",
      "text": "where a-priori knowledge concerning task-relatedness is modeled in terms of a graph R in the objective function. This way, similarity is only enforced between W t 's corresponding to related tasks. The minimized objective function in this case is\n\n, where R is the graph encoding task relationships, and α, β, γ denote regularization parameters as above.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Eg-Mtl Description",
      "text": "Crowdsourcing has recently become a popular and effective methodology for procuring large amounts of training data in supervised learning approaches. Nevertheless, crowdworkers come from varied backgrounds and may lack the motivation and/or technical and cognitive skills required to effectively accomplish a given task, resulting in noisy outputs. Especially since emotion is a highly subjective phenomenon and the time lag between emotion perception and annotation can vary among crowdworkers, one can expect very diverse dynamic A,V crowd annotations for the presented movie clips.\n\nIn order to improve the efficacy and generalizability of models trained using crowd A,V annotations, we leverage on a small number of annotations provided by more reliable experts 6 . Intuitively, combining expert labels with crowd labels can facilitate better model training than solely using 6. Considerably higher agreement for A,V is noted among experts as compared to crowdworkers for Val clips-see Section 5.\n\ncrowd labels; however, to our knowledge, none of the CS-based affect prediction works have attempted to clean crowd models via this perspective.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Problem Formulation",
      "text": "Notation: We denote with • F and • 1 the Frobenius and the 1 norms respectively. (•) indicates the transpose operator, while |.| denotes a set cardinality. For problems P1 and P2 described in Section 1, we model each movie clip as a task, and for each task (clip) t we denote the set of dynamic A,V crowdworker annotations using the matrix\n\n, where N t is the crowdworkers that have annotated clip t for A/V and D is the length of dynamic A,V annotations (we only analyze the final 50 seconds of clip time in this work). We also define the matrix\n\nwhere N = R t=1 N t denotes the total number of training examples from the R tasks. For each training sample, we construct a binary label indicator vector y t i ∈ IR RC as y t i = [0, 0, ..., 0\n\nT ask 1 , 0, 1, ..., 0\n\nT ask 2 , ..., 0, 0, ..., 0\n\nwhere C is the number of class labels (C = 2 for P2 and the number of discrete A/V levels in [-1, 1] for P1). The position of the non-zero element indicates the task and class membership of the corresponding training sample. A label matrix Y ∈ IR N ×RC is then obtained concatenating the y t i 's for all training samples. We also use some additional expertlabeled instances to improve model quality. If there are N e expert labels, then the expert training set can be defined as P ∈ IR Ne×D and a label matrix V ∈ IR Ne×RC , in a similar way as defined for the crowd.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Methodology:",
      "text": "We propose to solve the following optimization problem:\n\nwhere W ∈ D×RC is the learnt weight matrix, E ∈ |E|×RC is an edge-vertex incident matrix where\n\nis a diagonal matrix to control the reliability of crowd workers, while λ 1 , λ 2 , λ 3 are appropriate regularization parameters.\n\nThe proposed objective function has three effects. Task relatedness is defined via the graph regularization term (we assume all clips to be related in this work), and knowledge from one task can be utilized by the other related tasks. Prior knowledge regarding the required level of feature sharing is embedded in the learning framework through γ ij 's (set to 1 in this work). Sparsity is enforced in the learning process, which emphasizes the contribution of discriminative features and de-emphasizes the contribution of less discriminative features. Finally, by incorporating loss functions pertaining to both the crowd and expert labels in the optimization, we 'refine' the learned weights W so as to agree with the expert knowledge, which is closer to the ground-truth as compared to crowd labels as per our assumption. This model refinement reflects in the form of enhanced classification/regression performance as discussed in Section 5.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Optimization",
      "text": "To solve Eqn.(1), we propose to adopt the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)  [5] . The objective function is convex and is the sum of a smooth term f (•) and a non-smooth term g(•) if we define:\n\nwe calculate the derivative of f (W) with respect to W, we have gradient descent:\n\nwhich is used in the update equation for W in Algorithm 1 outlined below. For the non-smooth term g(W), we can adopt Soft Thresholding  [6]  to solve the non-smooth 1norm term. The optimization procedure is outlined in Algorithm 1.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Algorithm 1 Accelerated Gradient Descent For Solving (1)",
      "text": "INPUT: Crowd and Expert feature matrix X and P, Crowd and Expert label matrix Y and V, λ1, λ2, λ3, E. Initialize W0, α0 = 1, line search parameter L0 = 1.  [6] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Loop:",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Data Analysis And Experiments",
      "text": "In this section, we first examine the dynamic annotations acquired from crowdworkers and experts, before focusing our attention on the experiments concerning problems P1 and P2. We computed the Kendall's coefficient of concordance (Kendall's W) to determine inter-annotator agreement for the crowd and expert populations on the Val set, as well as for the crowd on the Eval set-the mean and standard deviation values for arousal and valence are presented in Tables  3  and 4 . Apart from computing W for A,V over the entire (or Full) clip, we also determined W for the first (1st) and second (2nd) halves of each clip. This is because all clips began with a relatively neutral segment as reported in  [1] . Furthermore, based on the static emotion labels available for the clips in  [1] , we repeated the above analyses for the HA, LA, HV and LV clips.\n\nTables  3  and 4  clearly reveal that the A,V agreement is higher among experts as compared to crowdworkers for the Val set-the fact that experts are likely to provide more consistent annotations forms the basis of our EG-MTL algorithm, and this result is also on expected lines since experts were conversant with emotional attributes, and familiarized themselves with the emotional scene dynamics by viewing the clips multiple times prior to annotating them (whereas crowdworkers could view the clips only while performing the annotation task). Nevertheless, only a moderate level of agreement is noted among experts for both A and V considering Ws computed over the entire clip duration. Deeper analysis reveals higher agreement on the A,V ratings among experts for the emotionally salient second half of the clip as compared to the first half. Also, both experts and crowdworkers agreed considerably more on the A ratings for HA clips as compared to LA clips (considering full clips); On the other hand, while very similar Ws are noted for expert V ratings in HV and LV clips, crowdworkers agreed considerably more on V ratings for HV clips.\n\nWe performed a 3-way ANOVA test on the concordance scores with the population type (expert/crowd), clip-half (1st/2nd) and attribute intensity (HA/LA or HV/LV) as factors. ANOVA on W scores for arousal revealed the main effect of population type (F (1,47) = 11.1, p < 0.05) and attribute intensity (F (1,47) = 21.25, p < 0.000001), while no other main or interaction effects were significant. ANOVA on valence W scores again revealed the main effect of population type (F (1,47) = 6.51, p < 0.05) and attribute intensity (F (1,47) = 7.37, p < 0.01), while also showing up the interaction effect of these two factors (F (1,47) = 6.58, p < 0.01). We also computed Pearson correlations between expert and crowd annotators for the Val set to investigate if there was higher agreement on the trend of the dynamic A,V profiles as compared to the actual A,V ratings. However, there was little difference between the Kendall Ws and Pearson coefficients for both experts and crowdworkers. Finally, much higher agreement was noted among crowd annotations for the Eval set as compared to the Val set. Whether this increased agreement can be attributed to the nature of the Eval clips is unclear due to the unavailability of expert annotations for Eval clips. However, the fact that a majority of the 35 Eval annotators performed annotations in a rather controlled academic environment could have contributed to greater rating consistency.\n\nGiven that the Multi-task learning framework requires dimensional consistency of the input data, and the timecontinuous annotations were of varying length (due to varying clip lengths), we used the A,V ratings provided over the last 50 seconds by experts/crowdworkers in all our experiments. This choice was made owing to two reasons (1) As reported in  [1] , the latter part of the clips was emotionally more salient, which reflects in higher agreement among even the expert raters for the second half of the movie clips and (2) As shown in Figure  2  where the median and mean of the crowd (in red) and expert (in green) A,V profiles are plotted for an exemplar Val clip corresponding to each of the four AV quadrants, the dynamic A/V profile 'tends to' the static emotional label typical within the final 50 second time frame. Before moving on to show how MTL captures latent relationships between tasks in the context of affect prediction, we first describe the audio-visual features extracted from the clips for analysis.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Audio-Visual Feature Extraction",
      "text": "For the purpose of problem P1 where an MTL model needs to be trained for continuous emotion prediction from the dynamic A/V ratings and audio-visual clip features, we extracted audio and visual features that have been found to correlate well with the A,V dimensions by prior affective studies  [12] ,  [17] ,  [27] ,  [41] . These features were extracted on a per-second basis for our regression experiments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Audio Features",
      "text": "Sound information in the form of loudness of speech (energy of sound) is related to arousal, while rhythm and average pitch in speech relates to valence  [27] . Mel-frequency cepstrum components (MFCCs)  [20]  are representative of the short-term sound power spectrum and have been commonly employed for emotion recognition. Commonly used features in audio and speech processing  [20]  were extracted from the audio channels. To extract MFCCs, we divided the audio segment into 20 divisions and then extracted the first 13 MFCC components from each division. Using the sequence of MFCC components over a segment, we computed 13 derivatives of MFCC, DMFCC, and mean auto correlation, AMFCC proposed in  [20] . Upon calculating MFCC, DMFCC and AFCC (13 values each), we used their means as features. To chracterize emotional speech, we extract formants up to 4400Hz over the audio segment, and formant means were used as features  [24] . Also, we used the ACA toolbox  [19]  to calculate mean and standard deviation (std) of (i) spectral flux, (ii) spectral centroid and (iii) time-domain zero crossing rate  [20]  over 20 audio segment divisions. We computed the power spectral density of the audio signal and the bandwidth, band energy ratio (BER) and density spectrum magnitude (DSM) according to  [20] . Finally, we computed the mean proportion of silence as defined in  [8] .\n\nOverall, 56 audio features listed in Table  5  were extracted.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Visual Features",
      "text": "Lighting key and color variance  [41]  are well-known video features known to evoke emotions. Therefore, we extracted lighting key from each frame in the HSV space by multiplying the mean by the standard deviation of V values.\n\nColor variance  [17]  is defined as the determinant of the covariance matrix of L, U, and V in the CIE LUV color space. Also, the amount of motion in a movie scene is indicative of its excitement level  [17] . Therefore, we computed the optical flow  [21]  in consecutive frames of a video segment to motion magnitude for each frame. As the proportions of colors are important elements for evoking emotions  [39] , A 20-bin color histogram of hue and lightness values in the HSV space was computed for each frame of a segment and averaged over all frames. The mean of the bins reflect the variation in the video content. For each frame in a segment, the median of the L and S values in HSL space were computed; their average for all the frames in a segment is an indication of the segment lightness and saturation. We also used the definitions in  [41]  to calculate shadow proportion, visual excitement, grayness and visual detail. Extracted video features are listed in Table  5 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Capturing Latent Task Relationships Via Mtl",
      "text": "We now illustrate the utility of MTL, and show how it can capture relationships between related tasks in the form of features shared by the various tasks. To this end, we use the dynamic expert ratings on the Val set since they are noted to be the most consistent from the concordance analyses presented earlier. As a starting point, we perform an emotion recognition experiment modeling each expert as a task, i.e., the dynamic ratings provided by each expert (over the last 50 sec) for all of the movie clips are fed as a feature matrix with the binary ground-truth A,V labels for each movie clip (as derived from  [1] ) denoting the data labels. This learning framework can answer the following question: Are there any similarities among the dynamic rating patterns of the experts, and which similarities (in terms of time points) most influence the static emotional labels of the movie clips? Figure  3  presents the learned weight (W ) matrices for the above problem using the Lasso, 2,1 , Dirty and Robust",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Audio Features Description Mfcc Features (39)",
      "text": "MFCC coefficients  [20] , Derivative of MFCC, MFCC Autocorrelation (AM-FCC)",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Energy (1) And Pitch (1)",
      "text": "Average energy of audio signal  [20]  and first pitch frequency Formants (4)\n\nFormants up to 4400Hz Time frequency  (8)  mean and std of: MSpectrum flux, Spectral centroid, Delta spectrum magnitude, Band energy ratio  [20]",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Zero Crossing Rate (1)",
      "text": "Average zero crossing rate of audio signal  [20]",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Silence Ratio (2)",
      "text": "Mean and std of proportion of silence in a time window  [8] ,  [20]",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Video Features Description Brightness (6)",
      "text": "Mean of: Lighting key, shadow proportion, visual details, grayness, median of Lightness for frames, mean of median saturation for frames Color Features  (41)  Color variance, 20-bin histograms for hue and lightness in HSV space VisualExcitement (1)\n\nFeatures as defined in  [41]",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Motion (1)",
      "text": "Mean inter-frame motion  [21]  MTL baselines detailed in Section 4. Apart from Lasso-MTL, all other methods attempt to learn a group component, and it is evident from figure rows 2-4 that there are greater similarities in the expert rating patterns for the latter half of the clips as also typified by the higher Kendall W values for the second halves. Also, maximum consistency among expert V ratings is noted at the very end of the movie clips (this also maximally influences the overall clip V label), while the influential time points corresponding to similar A ratings are more distributed.\n\nFor problems P1 and P2 which require prediction of dynamic or static V,A clip labels, it is more appropriate to model each clip as a task. As a preliminary step, we attempted static emotion recognition modeling each clip as a task, i.e., the MTL baselines are trained with a feature matrix containing dynamic A/V expert ratings and static Val clip A/V labels as data labels. The above learning framework reveals those time points in the continuous emotion profile that contribute the most to the static emotional clip labels. Figure  4  presents the learned W s using the 2,1 , Dirty, Robust and SR-MTL methods. Evidently, dynamic A,V ratings towards the end of the movie clip have the greatest impact on its static emotional label-this result reinforces the observation made from Figure  2 . The last two rows show the learned weights using graph-regularized MTL for HA, HV, LA and LV clips (clips with identical static A/V labels are defined as related tasks, and these task relationships are embedded in the form of a graph). Note that the most influential time-points corresponding to HA clips occur a few seconds before the end, while a larger sequence of time points are required to accurately predict the arousal label for LA clips (possibly due to the larger variance in A ratings for LA clips). Conversely, very few time points determine the static clip V for both HV and LV clips.\n\nAs a supplement to Figure  4 , Table  6  presents static A,V  recognition accuracies 7 when the different MTL algorithms are trained with the dynamic Val affective ratings provided by experts and crowdworkers. Sparsity levels of the learned W matrices are also specified in braces. Reflecting the higher consistency in dynamic V annotations and the greater agreement among expert ratings, superior recognition accuracy in general is achieved for V and with expert annotations with the learned W matrices of comparable sparsity. We also show how the movie clips are related in terms of the audio-visual features in Figure  5  as a final illustration. To this end, the various MTL baselines are trained with the extracted audio or visual features and the ground-truth A/V labels. The x-axes of the W matrices shown in Figure  5  depict audio or visual feature dimensions in the same order as listed in Table  5 . Formant features appear to be the most important audio descriptors for both static A,V prediction as seen from the W matrices. Looking at the SRMTL W 's for high and low V in the second column of the last two rows, it is interesting to note that zero crossing rate and silence ratio make a salient contribution to LV clips but not to HV clips. Concerning video features, brightness and color descriptors are indicative of both A and V as expected. From the SRMTL W 's in the third and fourth columns of the last two rows, visual excitement and motion descriptors are seen to have a greater role in characterizing HA and HV clips rather than in LA and LV clips.\n\nTable  7  supplements Figure  5  by tabulating the root mean square error (RMSE) in predicted A,V levels for MTL baseline models trained with dynamic expert rating labels and audio/visual descriptors as features 8 . Superior prediction of dynamic A levels is achieved with both audio and visual features. Also, with W matrices of comparable sparsity, audio features are seen to be better predictors of both A,V as compared to the considered visual descriptors.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Arousal Valence",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Experimental Results",
      "text": "We now evaluate the proposed EG-MTL algorithm against other MTL baselines for problems P1 and P2, and begin with P1.   8 . EG-MTL (7) corresponds to the condition where the rating data of only seven experts, which by itself is insufficient to train a model, is used to refine crowd annotations via the proposed approach. Clearly, all multi-task methods outperform single-task Lasso regressor confirming the utility of MTL. Also, incorporation of a small amount of expert knowledge to refine the crowd model is highly beneficial as exemplified by the superior performance of EG-MTL and EG-MTL  (7)  with respect to the other methods in a vast majority of conditions. The fact that there is a considerable difference in prediction performance of EG-MTL and other approaches also highlights the amount of noise in the dynamic affective crowd annotations.\n\nRMSE values increase for longer test snippets (implying lesser training data). Furthermore, RMSE values are higher for snippets extracted from the second half of each movie clip as compared to first half snippets, as the latter part of all clips is known to be emotionally salient. Finally, considering the performance of the best audio and video-based models, audio features produce better prediction performance in general as compared to video descriptors, consistent with the observations from Table  7 . Finally, the considered video descriptors predict arousal better than valence, and this result also holds good for Table  7 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Arousal-Audio",
      "text": "Valence-Audio Arousal-Video Valence-Video",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Refining A Classification Model Via Eg-Mtl",
      "text": "We now focus on problem P2 and discuss experimental results thereof.\n\nP2 problem description: P2 involves learning a binary classification model with the dynamic Val expert and crowd annotations as features, and corresponding static ground-truth A,V labels as data labels. This model is to be subsequently utilized for improving static A,V recognition performance on the Eval set for which only dynamic crowd annotations are available. In the optimization function (Eq.(  1 )), we therefore have X and P denoting the set of dynamic crowd and expert affective annotations repsectively, while Y and V are identically the set of static A,V labels for the Val movie clips.\n\nExperimental settings: All MTL models were trained using dynamic Val crowd A,V ratings over the final 50 second duration of the clip and the ground-truth labels available for the Val clips as part of  [1] . For the EG-MTL algorithm, dynamic expert A,V ratings were fed as expert data. As with P1, the best regularization parameter λ 1 controlling group sparsity was chosen from the set [0.1, 1, 10, 100] via five fold cross-validation on the training set for all methods. For the proposed EG-MTL algorithm, λ 2 , λ 3 were set to 1. The learned W s from each method were then applied on the dynamic Eval crowd to predict static A,V labels for the Eval clips.\n\nResults and Discussion: Table  9  presents A,V recognition accuracies obtained with the different MTL approaches on the Eval set. Evidently, all MTL methods outperform single-task Lasso for both A and V recognition, which reconfirms the benefit of employing multi-task learning for affect prediction. Among the various MTL baselines, SRMTL produces best recognition for both arousal and valence (along with 2,1 MTL) due to the fact that the weights are adapted based on the graph specifying related tasks (clips). Nevertheless, the proposed EG-MTL approach produces the best recognition performance with EG-MTL  (7) , which incorporates data from only seven experts in the learning process producing very competitive performance. For each of the considered approaches, higher recognition performance is obtained for arousal as compared to valence on the Eval set, which can possibly be attributed to the low agreement among crowdworkers for HV clips (see Table  4 ).",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "Given that emotion perception is a highly subjective phenomenon, the difficulty in finding adequate number of reliable annotators has forced most affect recognition approaches to involve only small or mid-sized user populations. Crowdsourcing, which has been successfully utilized in natural language processing and computer vision applications, presents an attractive proposition for the affective computing community in this regard, and can be employed to generate large amounts of training data in an inexpensive manner. Nevertheless, crowd generated data can be extremely noisy and requires robust and efficient machine learning techniques so as to be useful for learning generalizable models. This work, which proposes to use a small amount of expert data in order to improve/refine crowd models via the expert-guided Multi-task learning (EG-MTL) algorithm, is one of the first steps towards making crowdsourced data utilizable for affective computing applications.\n\nIn this paper, we have shown how EG-MTL can be effectively used to (i) refine a crowd-based regression model, and enhance prediction of arousal and valence levels for 5-15 second snippets, and (ii) effectively learn classification weights using crowd-plus-expert data on one (Val) dataset, and utilize this knowledge to improve binary emotion recognition performance on a second (Eval) set for which only crowd data is available. Apart from presenting benchmarking results, we also illustrate how MTL can be utilized to determine discriminative features for arousal and valence prediction, and also identify time points from the dynamic emotion profile that have the greatest contributions towards determining the static arousal and valence for a movie clip. We believe that multi-task learning which attempts to learn the relationships between multiple tasks is naturally suited to (especially dimensional) affect recognition, but has surprisingly not been utilized for the same.\n\nEven though EG-MTL achieves superior performance by incorporating some expert knowledge in the learning framework, its full potential is yet to fully realized. For example, we did not explicitly define the related tasks (clips) via the graph regularization term in Eq.(  1 ), and an automated framework for determining the optimal γ ij 's based on the input data needs to be developed. Future work will involve investigation of these aspects, and development of a MTLbased framework to fuse multiple dynamic annotations so as to compute a representative emotional profile for affective media from crowd-generated data.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (b). The following components",
      "page": 4
    },
    {
      "caption": "Figure 1: Acquiring crowd annotations for the Val dataset: (a) Architecture of the designed Crowdﬂower platform (Turkers refer to the AMT",
      "page": 5
    },
    {
      "caption": "Figure 2: where the median and mean",
      "page": 7
    },
    {
      "caption": "Figure 2: Dynamic arousal (top) and valence (bottom) ratings provided by",
      "page": 8
    },
    {
      "caption": "Figure 3: presents the learned weight (W) matrices for",
      "page": 8
    },
    {
      "caption": "Figure 4: presents the learned Ws using the ℓ2,1, Dirty, Robust and",
      "page": 9
    },
    {
      "caption": "Figure 2: The last two rows show the learned",
      "page": 9
    },
    {
      "caption": "Figure 4: , Table 6 presents static A,V",
      "page": 9
    },
    {
      "caption": "Figure 5: as a ﬁnal illustration.",
      "page": 9
    },
    {
      "caption": "Figure 5: depict audio or visual feature dimensions in the same order",
      "page": 9
    },
    {
      "caption": "Figure 5: by tabulating the root",
      "page": 9
    },
    {
      "caption": "Figure 3: MTL with experts as tasks and the arousal (left) and valence",
      "page": 10
    },
    {
      "caption": "Figure 4: MTL with Val movie clips as tasks and experts’ arousal (left),",
      "page": 10
    },
    {
      "caption": "Figure 5: MTL with Val movie clips as tasks and audio-visual descriptors as features. (From top to bottom) W matrices obtained with ℓ2,1, Dirty,",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The movie clips are about 1–",
      "data": [
        {
          "Attribute": "Crowd annotations\nExpert annotations\nNo. of clips\nNo. of crowd annotations/clip\nNo. of expert annotations/clip\nType of crowd annotations\nType of expert annotations",
          "Val": "yes\nyes\n12\n15+\n16\nDynamic, static A,V\nDynamic A,V",
          "Eval": "yes\nno\n12\n35\n-\nDynamic A,V\n-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Front": "5 s\n10 s\n15 s",
          "Back": "5 s\n10 s\n15 s"
        },
        {
          "Front": "0.880±0.170\n1.724±0.531\n2.111±0.522\n0.542±0.043\n0.988±0.129\n1.202±0.034\n0.526±0.032\n0.959±0.148\n1.183±0.089\n0.564±0.019\n0.889±0.076\n1.126±0.067\n0.494±0.040\n0.735±0.026\n0.904±0.043\n0.480±0.052\n0.720±0.015\n0.877±0.026\n0.484±0.043\n0.726±0.062\n0.893±0.069",
          "Back": "1.066±0.150\n1.677±0.219\n2.004±0.152\n0.685±0.078\n1.050±0.202\n1.331±0.094\n0.685±0.078\n1.053±0.219\n1.288±0.104\n0.671±0.071\n0.986±0.198\n1.221±0.130\n0.666±0.103\n1.078±0.189\n1.290±0.124\n0.585±0.093\n0.893±0.166\n1.047±0.097\n0.592±0.071\n0.919±0.062\n1.086±0.085"
        },
        {
          "Front": "0.819±0.083\n1.227±0.055\n1.437±0.077\n0.542±0.052\n0.800±0.053\n0.961±0.061\n0.580±0.058\n0.775±0.083\n0.894±0.115\n0.528±0.032\n0.758±0.024\n0.942±0.036\n0.482±0.044\n0.715±0.021\n0.829±0.027\n0.477±0.048\n0.710±0.023\n0.848±0.036\n0.478±0.052\n0.711±0.023\n0.859±0.046",
          "Back": "1.098±0.143\n1.541±0.133\n2.064±0.072\n0.731±0.115\n1.079±0.064\n1.489±0.099\n0.751±0.111\n1.097±0.097\n1.409±0.104\n0.689±0.102\n0.969±0.089\n1.313±0.031\n0.684±0.111\n0.976±0.106\n1.357±0.045\n0.680±0.117\n0.923±0.060\n1.234±0.038\n0.683±0.110\n0.972±0.038\n1.309±0.123"
        },
        {
          "Front": "0.879+-0.205\n1.661+-0.610\n2.526+-0.632\n0.482+-0.088\n0.745+-0.066\n1.055+-0.088\n0.465+-0.090\n0.729+-0.073\n1.068+-0.112\n0.391+-0.085\n0.683+-0.067\n0.913+-0.059\n0.365+-0.043\n0.575+-0.028\n0.761+-0.054\n0.298+-0.056\n0.479+-0.015\n0.608+-0.010\n0.302±0.052\n0.523±0.023\n0.665±0.062",
          "Back": "0.841+-0.047\n1.250+-0.075\n1.501+-0.015\n0.367+-0.060\n0.665+-0.088\n0.947+-0.039\n0.355+-0.051\n0.644+-0.075\n0.914+-0.066\n0.334+-0.024\n0.578+-0.098\n0.685+-0.045\n0.640+-0.070\n0.384+-0.036\n0.811+-0.042\n0.313+-0.025\n0.549+-0.024\n0.811+-0.042\n0.336±0.027\n0.792±0.038\n0.625±0.052"
        },
        {
          "Front": "2.146±0.122\n3.705±0.242\n4.474±0.699\n0.183±0.070\n0.307+-0.078\n0.508+-0.017\n0.183±0.070\n0.307±0.078\n0.508±0.017\n0.205±0.057\n0.350±0.078\n0.538±0.035\n0.174±0.075\n0.295±0.077\n0.505±0.018\n0.161±0.060\n0.252+-0.050\n0.359±0.017\n0.172±0.082\n0.290±0.067\n0.403±0.010",
          "Back": "1.673±0.241\n2.240±0.285\n2.412±0.548\n0.218+-0.013\n0.358+-0.040\n0.460+-0.014\n0.218±0.013\n0.358±0.040\n0.460±0.014\n0.235±0.017\n0.391±0.038\n0.493±0.017\n0.227±0.014\n0.366±0.038\n0.486±0.019\n0.349±0.028\n0.427±0.021\n0.223±0.014\n0.225±0.013\n0.355±0.035\n0.442±0.018"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "DECAF: MEG-based multimodal database for decoding affective physiological responses",
      "authors": [
        "M Abadi",
        "R Subramanian",
        "S Kia",
        "P Avesani",
        "I Patras",
        "N Sebe"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "A multi-task learning framework for time-continuous emotion estimation from crowd annotations",
      "authors": [
        "M Abadi",
        "A Abad",
        "R Subramanian",
        "N Rostamzadeh",
        "E Ricci",
        "J Varadarajan",
        "N Sebe"
      ],
      "year": "2014",
      "venue": "CrowdMM Workshop"
    },
    {
      "citation_id": "3",
      "title": "User-centric affective video tagging from MEG and peripheral physiological responses",
      "authors": [
        "M Abadi",
        "S Kia",
        "S Ramanathan",
        "P Avesani",
        "N Sebe"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction, ACII 2013"
    },
    {
      "citation_id": "4",
      "title": "Multi-task feature learning",
      "authors": [
        "A Argyriou",
        "T Evgeniou",
        "M Pontil"
      ],
      "year": "2007",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems",
      "authors": [
        "A Beck",
        "M Teboulle"
      ],
      "year": "2009",
      "venue": "SIAM Journal on Imaging Sciences"
    },
    {
      "citation_id": "6",
      "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "authors": [
        "S Boyd",
        "N Parikh",
        "E Chu",
        "B Peleato",
        "J Eckstein"
      ],
      "year": "2010",
      "venue": "Foundations and Trends in Machine Learning"
    },
    {
      "citation_id": "7",
      "title": "Multitask learning",
      "authors": [
        "R Caruana"
      ],
      "year": "1998",
      "venue": "Multitask learning"
    },
    {
      "citation_id": "8",
      "title": "Mixed type audio classification with support vector machine",
      "authors": [
        "L Chen",
        "S Gunduz",
        "M Ozsu"
      ],
      "year": "2006",
      "venue": "IEEE Int'l Conference on Multimedia and Expo"
    },
    {
      "citation_id": "9",
      "title": "Tracing emotion: an overview",
      "authors": [
        "R Cowie",
        "G Mckeown",
        "E Douglas-Cowie"
      ],
      "year": "2012",
      "venue": "Int'l Journal of Synthetic Emotions (IJSE)"
    },
    {
      "citation_id": "10",
      "title": "Robust multi-task feature learning",
      "authors": [
        "P Gong",
        "J Ye",
        "C Zhang"
      ],
      "year": "2012",
      "venue": "ACM Int'l conference on Knowledge discovery and data mining"
    },
    {
      "citation_id": "11",
      "title": "Emotion elicitation using films",
      "authors": [
        "J Gross",
        "R Levenson"
      ],
      "year": "1995",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "12",
      "title": "Affective video content representation and modeling",
      "authors": [
        "A Hanjalic",
        "L.-Q Xu"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "A dirty model for multi-task learning",
      "authors": [
        "A Jalali",
        "P Ravikumar",
        "S Sanghavi",
        "C Ruan"
      ],
      "year": "2010",
      "venue": "Neural Information Processing Systems"
    },
    {
      "citation_id": "14",
      "title": "A convex formulation for learning from crowds",
      "authors": [
        "H Kajino",
        "Y Tsuboi",
        "H Kashima"
      ],
      "year": "2012",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Making computers look the way we look: Exploiting visual attention for image understanding",
      "authors": [
        "H Katti",
        "R Subramanian",
        "M Kankanhalli",
        "N Sebe",
        "T.-S Chua",
        "K Ramakrishnan"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, MM '10"
    },
    {
      "citation_id": "16",
      "title": "Personality modulates the effects of emotional arousal and valence on brain activation",
      "authors": [
        "E Kehoe",
        "J Toomey",
        "J Balsters",
        "A Bokde"
      ],
      "year": "2012",
      "venue": "Social Cognitive & Affective Neuroscience"
    },
    {
      "citation_id": "17",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Fusion of facial expressions and eeg for implicit affective tagging",
      "authors": [
        "S Koelstra",
        "I Patras"
      ],
      "year": "2013",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "19",
      "title": "An introduction to audio content analysis: Applications in signal processing and music informatics",
      "authors": [
        "A Lerch"
      ],
      "year": "2012",
      "venue": "An introduction to audio content analysis: Applications in signal processing and music informatics"
    },
    {
      "citation_id": "20",
      "title": "Classification of general audio data for content-based retrieval",
      "authors": [
        "D Li",
        "I Sethi",
        "N Dimitrova",
        "T Mcgee"
      ],
      "year": "2001",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "21",
      "title": "An iterative image registration technique with an application to stereo vision",
      "authors": [
        "B Lucas",
        "T Kanade"
      ],
      "year": "1981",
      "venue": "Int'l Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "22",
      "title": "Crowdsourcing facial responses to online videos",
      "authors": [
        "D Mcduff",
        "R Kaliouby",
        "R Picard"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "EmoSPACE Workshop"
    },
    {
      "citation_id": "24",
      "title": "Robust formant tracking for continuous speech with speaker variability",
      "authors": [
        "K Mustafa",
        "I Bruce"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Output-associative RVM regression for dimensional and continuous emotion prediction",
      "authors": [
        "M Nicolaou",
        "H Gunes",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "26",
      "title": "Dynamic probabilistic cca for analysis of affective behaviour and fusion of continuous annotations",
      "authors": [
        "M Nicolaou",
        "V Pavlovic",
        "M Pantic"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "28",
      "title": "Learning from crowds",
      "authors": [
        "V Raykar",
        "S Yu",
        "L Zhao",
        "G Valadez",
        "C Florin",
        "L Bogoni",
        "L Moy"
      ],
      "year": "2010",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "29",
      "title": "Who are the crowdworkers?: shifting demographics in Mechanical Turk",
      "authors": [
        "J Ross",
        "L Irani",
        "M Silberman",
        "A Zaldivar",
        "B Tomlinson"
      ],
      "year": "2010",
      "venue": "Human Factors in Computing Systems"
    },
    {
      "citation_id": "30",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "31",
      "title": "Looking beyond a clever narrative: Visual context and attention are primary drivers of affect in video advertisements",
      "authors": [
        "A Shukla",
        "H Katti",
        "M Kankanhalli",
        "R Subramanian"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction, ICMI '18"
    },
    {
      "citation_id": "32",
      "title": "1000 songs for emotional analysis of music",
      "authors": [
        "M Soleymani",
        "M Caro",
        "E Schmidt",
        "C.-Y Sha",
        "Y.-H Yang"
      ],
      "year": "2013",
      "venue": "CrowdMM Workshop"
    },
    {
      "citation_id": "33",
      "title": "Affective characterization of movie scenes based on multimedia content analysis and user's physiological emotional responses",
      "authors": [
        "M Soleymani",
        "G Chanel",
        "J Kierkels",
        "T Pun"
      ],
      "year": "2008",
      "venue": "IEEE Int'l Symposium on Multimedia"
    },
    {
      "citation_id": "34",
      "title": "A collaborative personalized affective video retrieval system",
      "authors": [
        "M Soleymani",
        "J Davis",
        "T Pun"
      ],
      "year": "2009",
      "venue": "Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "35",
      "title": "Crowdsourcing for affective annotation of video: development of a viewer-reported boredom corpus",
      "authors": [
        "M Soleymani",
        "M Larson"
      ],
      "year": "2010",
      "venue": "Workshop on Crowdsourcing for Search Evaluation"
    },
    {
      "citation_id": "36",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Crowdsourcing event detection in youtube video",
      "authors": [
        "T Steiner",
        "R Verborgh",
        "R Van De Walle",
        "M Hausenblas",
        "J Vallés"
      ],
      "year": "2011",
      "venue": "Workshop on detection, representation, and exploitation of events in the semantic web"
    },
    {
      "citation_id": "38",
      "title": "Regression shrinkage and selection via the lasso",
      "authors": [
        "R Tibshirani"
      ],
      "year": "1996",
      "venue": "Journal of the Royal Statistical Society. Series B (Methodological)"
    },
    {
      "citation_id": "39",
      "title": "Effects of color on emotions",
      "authors": [
        "P Valdez",
        "A Mehrabian"
      ],
      "year": "1994",
      "venue": "Journal of Experimental Psychology: General"
    },
    {
      "citation_id": "40",
      "title": "Efficiently scaling up crowdsourced video annotation",
      "authors": [
        "C Vondrick",
        "D Patterson",
        "D Ramanan"
      ],
      "year": "2013",
      "venue": "Int'l Journal of Computer Vision"
    },
    {
      "citation_id": "41",
      "title": "Affective understanding in film",
      "authors": [
        "H Wang",
        "L.-F Cheong"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "42",
      "title": "Crowd-sourcing for difficult transcription of speech",
      "authors": [
        "J Williams",
        "I Melamed",
        "T Alonso",
        "B Hollister",
        "J Wilpon"
      ],
      "year": "2011",
      "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)"
    },
    {
      "citation_id": "43",
      "title": "No matter where you are: Flexible graph-guided multi-task learning for multi-view head pose classification under target motion",
      "authors": [
        "Y Yan",
        "E Ricci",
        "R Subramanian",
        "O Lanz",
        "N Sebe"
      ],
      "year": "2013",
      "venue": "International Conference in Computer Vision"
    },
    {
      "citation_id": "44",
      "title": "Multi-task linear discriminant analysis for view invariant action recognition",
      "authors": [
        "Y Yan",
        "E Ricci",
        "R Subramanian",
        "G Liu",
        "N Sebe"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "45",
      "title": "Visual classification with multi-task joint sparse representation",
      "authors": [
        "X Yuan",
        "S Yan"
      ],
      "year": "2010",
      "venue": "Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "Labelme video: Building a video database with human annotations",
      "authors": [
        "J Yuen",
        "B Russell",
        "C Liu",
        "A Torralba"
      ],
      "year": "2009",
      "venue": "Int'l Conference on Computer Vision"
    },
    {
      "citation_id": "47",
      "title": "Ramanathan Subramanian received his Ph.D. in Electrical and Computer Engg. from NUS in 2008",
      "authors": [
        "J Zhou",
        "J Chen",
        "J Ye"
      ],
      "year": "2011",
      "venue": "He is an Associate Professor in University of Canberra, Australia. His past affiliations include IHPC (Singapore), U Glasgow (Singapore), IIIT Hyderabad (India), IIT Ropar (India) and UIUC-ADSC"
    }
  ]
}