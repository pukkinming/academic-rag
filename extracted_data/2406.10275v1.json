{
  "paper_id": "2406.10275v1",
  "title": "Exhubert: Enhancing Hubert Through Block Extension And Fine-Tuning On 37 Emotion Datasets",
  "published": "2024-06-11T21:30:15Z",
  "authors": [
    "Shahin Amiriparian",
    "Filip Packań",
    "Maurice Gerczuk",
    "Björn W. Schuller"
  ],
  "keywords": [
    "affective computing",
    "speech emotion recognition",
    "transformers",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multicultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EMOSET++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EMOSET++: https://huggingface.co/amiriparian/ExHuBERT.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) has a rich research history going back to the 1970s (first patents)  [1]  and 1990s (first research papers)  [2]  and while it has reaped the benefits of deep learning, a core issue remains to this day: While there are many available databases of emotional speech, most of them only contain comparatively few samples or speakers, hindering effective single-corpus training of large neural networks  [3] . As a response to this circumstance, cross-and multi-corpus SER has established itself as a highly important research direction  [4] . Due to the fact that databases in the field often differ significantly in recording settings, nature of speech and emotion portrayal (acted, elicited, natural), language, and other factors, successful approaches have employed special strategies and architectural considerations such as domain adaptation  [5]  or adapter transfer learning  [6]  to achieve satisfactory performance. Other efforts have gone towards making deep learning models more robust against distortions, noise, and other variations in the speech signal  [7, 8] .\n\nHowever, the recent paradigm shift in general deep learning towards large, Transformer-based models has already impacted the field  [9] . The Transformer architecture's inherent capability of learning arbitrary structural information from high-dimensional data combined with the exploitation of huge amounts of unlabeled data through self-and unsupervised learning has significantly reduced the need for human-annotated corpora for training powerful and transferable models  [10, 11] . Specifically for the fields of speech recognition and analysis, pre-trained Transformers such as Wav2Vec2.0 (W2V2)  [12]  or HuBERT  [13]  have shown considerable generalization capabilities. By exploiting self-supervision on large amounts of unlabeled speech data, these models learn to effectively capture the structure of spoken language. For a wide range of downstream tasks, pre-trained transformers provide competitive performance as feature extractors  [14, 15]  or through finetuning, e. g., for SER and speaker identification  [16] . Wagner et al.  [9]  finetune wav2vec and HuBERT models on MSP-Podcast  [17]  and show that their best models provide state-of-the-art performance on a number of SER corpora. They further trace the models' efficacy to a number of beneficial characteristics induced by both pre-training and the transformer architecture itself, such as implicit modeling of linguistic information and a general resilience against speaker, gender, or domain variations.\n\nWhile these and other works have convincingly made the argument for large, pre-trained Transformer models in SER, none have investigated whether their generalisability and robustness can enable effective learning of a single, transferable model on a heterogenous set of databases without the need for domain or corpus adaptation strategies. In the present study, we aim to fill this gap by evaluating the capability of large audio transformers to learn salient features for SER by multi-corpus finetuning. For this purpose, we build on the work of  [6] , introducing EmoSet++, integrating 37 SER corpora spanning 15 languages. We then fine-tune HuBERT on the assembled corpus and compare its transfer learning performance to other large pre-trained models on 6 additional SER databases. Finally, we introduce ExHuBERT, which integrates EMOSET++ finetuning with Backbone Block Expansion (BBE) -a technique recently introduced in LLAMA Pro  [18]  -to deliver a state-ofthe-art model and training strategy for emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emoset++",
      "text": "We introduce EMOSET++, a comprehensive multi-lingual, multi-cultural speech emotion corpus. It extends EmoSet  [6]  and integrates 37 unique emotion datasets with 150,907 speech recordings and a cumulative length of 119.5 hours. Most of the datasets that we have included comprise common languages such as English, German, or Mandarin, alongside rarer ones like Persian or Urdu. For all corpora, we create speaker-independent splits. To facilitate training, all distinct dataset labels (comprising 106 emotion classes) are mapped to six classes, representing combinations of low/high arousal and negative/neutral/positive valence. The mapping is based on Russel's Circumplex of Affect  [19] . The dataset splits were derived through three methods: adopting from the collected dataset, manual construction for speaker independence, or simply dividing it into 10 % partitions for both testing and validation in cases where speaker information was not explicitly provided. We do not conduct any re-weighting or re-balancing of speech samples for our machine learning experiments. Figure  1  displays sample durations ranging from 0 to 6 seconds, with longer samples excluded for readability. Most samples fall in [0 -5] second range.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Approach",
      "text": "We propose an enhanced version of HuBERT  [13]  through finetuning on EMOSET++ and incorporating Backbone Block Expansion (BBE), denoted as ExHuBERT. Specifically, we finetune the encoder part of the HuBERT architecture on 37 diverse emotion datasets, which span various languages and cultural backgrounds (cf. Section 3.1). After the weights are updated, we conduct backbone extension (cf. Section 3.2), and in the final step, we evaluate the performance of the backbone extended HuBERT (ExHuBERT) on 6 unseen test emotion datasets (cf. Section 4).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Fine-Tuning Hubert On Emoset++",
      "text": "Our system is built upon the Transformer architecture Hu-BERT  [13] , which has shown promising results in SER  [9, 28, 29] . A simple linear layer is added on top for the classification of the 6 mapped arousal valence classes. Fine-tuning is conducted in a round-robin fashion, ensuring each dataset contributes equally to the model. Upon achieving a state where our model spans multiple domains and languages, we utilize its transfer learning and generalization capabilities within Ex-HuBERT.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Backbone Block Expansion",
      "text": "After the fine-tuning process, we duplicate each encoder layer along with its weights. This augmentation results in an expanded version of HuBERT (ExHuBERT), featuring a total of 48 layers. The newly added layers are inserted after the original ones, accompanied by a skip connection to maintain the original layer's behavior. To stabilize the training process after layer duplication, we add a Zero Linear Layer (ZLL) at the end of each duplicated layer. The ZLL comprises initialized zero weights, ensuring that the output of the copied layers initiates from zero. This technique plays a crucial role in training by preventing unknown outputs from destabilizing the training process  [18] . Furthermore, we freeze the original layers to preserve their encoded knowledge, permitting only the copied layers to undergo training. These steps guarantee that the HuBERT model with BBE behaves identically to the HuBERT model without BBE during the initial stages. A high-level overview of ExHuBERT is depicted in Figure  2 .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments And Results",
      "text": "We split the experiments into two main parts: i) selection of the suitable audio Transformer for the BBE and fine-tuning of the chosen Transformer on EMOSET++ (cf. Section 4.1), and ii) conducting BBE on the fine-tuned Transformer and evaluating its performance on unseen emotion datasets (cf. Section 4.2).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Selection Of The Suitable Audio Transformer For Bbe",
      "text": "To choose the optimal architecture for BBE, we evaluate 6 stateof-the-art Transformers, including W2V2 XLS-R (300 million 1  and 1 billion 2  ), Whisper (Medium 3  and Large v3  4  ), and Hu-BERT (Large 5  and XLarge  6  ) on all 26 emotion corpora of EMOSET  [6] . We selected two variants of each architecture to compare their parameter impact, ensuring that variants of the same size had approximately equal parameter counts. Each Transformer is initialized with pre-trained weights obtained from huggingface.co. Additionally, we add a simple linear layer on top of each Transformer, with an output size of 6, corresponding to the mapped classes. For the evaluation metric, we use Unweighted Average Recall (UAR) due to its effectiveness in assessing the overall classification performance across all classes without bias towards any under-or oversampled class. We fine-tune all Transformer models in a roundrobin fashion, sequentially passing each dataset forward and backward through the model with one batch in each step. For W2V2 and HuBERT variants, we use raw audio waveforms resampled to 16 kHz as inputs, while we feed Whisper with log Mel spectrograms (with either 80 or 128 bins) obtained from waveforms. We freeze the CNN encoder during the entire training process for W2V2 and HuBERT. This step is unnecessary for Whisper. We conclude the experimentation phase after 3k steps using AdamW optimisation with β1 = 0.9, β2 = 0.999, ϵ = 1e -08, and a learning rate of 1e -05. The performance of these audio Transformers on the unseen test partition of eight commonly used emotion datasets is provided in Table 1  7  .\n\nOur results demonstrate that the HuBERT Large model outperforms others, achieving an average UAR of 62.7 % over all eight datasets, followed by W2V2 XLS-R 300 M with 57.1 % UAR. The worst-performing models are both variants of Whisper, each achieving 42.0 % and 41.1 % UAR, respectively. We consequently settle on HuBERT Large as the base model for fine-tuning on the extended EMOSET++ and transfer learning through BBE.\n\nSubsequently, we test the impact of using both 83.7 % of EMOSET++ and the full EMOSET++ for fine-tuning the selected HuBERT model for speech emotion recognition, aiming to evaluate the impact of dataset size on model performance and generalization. Training on EMOSET++ leads to substantial performance gains for all databases, compared to the original EMOSET, raising the average UAR over the 8 databases from  62.7 % to 64.9 % when only adding 5 training corpora, and to 69.7 % with the full set. All of the evaluated databases benefit from adding more corpora to EMOSET. However, EmoFilm and Mandarin Emotional Speech stop seeing gains after adding the first 5 new datasets.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Exhubert Comparison",
      "text": "We compare the transfer learning capabilities of our enhanced version of HuBERT (ExHuBERT Large EMOSET++) against other state-of-the-art Transformer models on a set of six unseen SER corpora, including Athens Emotional States Inventory (AESI)  [30] , Audio, Speech, and Vision Processing Lab Emotional Sound database (ASVP-ESD)  [31] , JL-Corpus  [32] , MLEnd 8 , Synthesized Database of Basic Emotions (SyntAct)  [33] , and Variably Intense Vocalizations of Affect and Emotion Corpus (VIVAE)  [34] . Specifically, we chose two variants of W2V2 -W2V2 XLS-R and the emotion finetuned W2V2 model by Wagner et al.  [9]  -and HuBERT Large LS960 to evaluate the impact of model architectures and the efficacy of EMOSET++ fine-tuning. Furthermore, we ablate the performance gains achieved through EMOSET++ fine-tuning from those due to block expansion by additionally expanding the LibriSpeech pre-trained HuBERT model (ExHuBERT Large LS960). Finally, we increase the number of train-8 https://mlenddatasets.github.io/spoken numerals/ able parameters of our ExHuBERT model by  (1)  unfreezing the original HuBERT layers (ExHuBERT Large Non-Frozen EMOSET++) and (2) tripling each original layer during block expansion (ExHuBERT XLarge EMOSET++). Table  2  shows the results and the number of trainable parameters for each of the evaluated models on six databases external to EMOSET++. For all but one of the six external databases, starting from a model that has been pre-trained on SER data, be it MSP-Podcast for W2V2 Emotion or EMOSET++ for HuBERT Large, leads to substantially improved performance, compared to the corresponding models trained on general speech data. The noteworthy outlier is found with SyntAct, which is a database of synthesized emotional speech. Here, performance has degraded from the respective base models of W2V2 and HuBERT large, indicating that fine-tuning on emotional speech data might have diminished generalization capabilities to non-and atypical SER tasks. However, BBE closes this performance gap, enabling efficient transfer from human-recorded to synthetic emotional speech. We hypothesize that without BBE, important SER knowledge acquired during pre-training gets overwritten early in the fine-tuning process due to the domain gap between synthetic and natural voices. Looking at the remaining databases, BBE leads to performance gains for HuBERT fine-tuned on EMOSET++, raising the average UAR from 71.1 % to 74.2 %. On the other hand, applying BBE to HuBERT pre-trained on LibriSpeech (ExHuBERT Large LS960) does not lead to con-   sistent improvements across the six SER corpora, with a slightly lower average UAR of 61.3 % compared to fine-tuning without BBE. Overall, BBE seems to have a positive effect on accuracy when the domain shift between source and target databases is rather small, e. g., from one SER corpus to another.\n\nWe further analyze how fast the different models converge during training on an unseen corpus in Figure  3 , which shows the validation UARs over 10,000 training steps. Analogous to the test set results, EMOSET++ pre-training increases overall performance and further achieves faster convergence on all databases compared to models without SER pre-training and the MSP-Podcast fine-tuned W2V2. For AESI, BBE helps the ExHuBERT model to converge even earlier.\n\nTo conclude, we look at the results achieved with the larger versions of ExHuBERT, which double the amount of trainable parameters. Unfreezing the original HuBERT layers after BBE degrades performance even from simple fine-tuning of HuBERT on each of the target corpora, highlighting the importance of keeping the weights of the original model fixed for transfer learning. Expanding each block in ExHuBERT by adding a second copy of the respective layer suffers from substantial overfitting, achieving the worst overall UAR of all evaluated Transformer models.\n\nIn summary, the validation of our approach on both databases contained within EMOSET++ and external corpora shows that (1) multi-corpus pre-training on EMOSET++ leads to substantially increased SER performance on seen and unseen corpora, and (2) the addition BBE in ExHuBERT further helps with generalization to new datasets.\n\nFor running all of our machine learning experiments, we utilized one RTX-3090 GPU with 24 GB memory, and needed a total time of 313 hours: 121 h for stage 1 (pre-selection of Transformers), 87 h for EMOSET++ fine-tuning, 17 h for Ex-HuBERT EMOSET++ testing, and 88 h for the second stage.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "We have proposed a novel twofold approach for SER by (i) collecting EMOSET++, a comprehensive multi-cultural and multilingual corpus comprising 37 emotion datasets, and (ii) introducing an enhanced version of HuBERT, denoted as Ex-HuBERT, achieved through fine-tuning on EMOSET++ and incorporating backbone extension. To find the suitable Transformer for BBE, we first selected six versions of state-ofthe-art audio Transformers and analyzed their performance on EMOSET  [6]  and then fine-tuned the best-performing Transformer (which was HuBERT Large) on EMOSET++. In the subsequent phase, we applied BBE to the fine-tuned HuBERT Large model and compared its performance with other Transformers on six previously unseen emotion datasets. The experimental results underscore the effectiveness of our proposed approach, demonstrating its capacity to generalize across diverse datasets and establish new benchmarks for a variety of emotion recognition tasks. Lastly, we have uploaded ExHuBERT on huggingface.co 9  . Fine-tuning and deploying ExHuBERT may be computationally demanding, potentially limiting its use in resource-constrained environments.\n\nFor future work, we aim to include MSP-Podcast dataset  [17]  in EMOSET++ and enhance ExHuBERT for continuous recognition of arousal, valence, and dominance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgements",
      "text": "This work was supported by MDSI -Munich Data Science Institute as well as MCML -Munich Center of Machine Learning.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Distribution of speech sample durations within",
      "page": 2
    },
    {
      "caption": "Figure 1: displays sample durations",
      "page": 2
    },
    {
      "caption": "Figure 2: 4. Experiments and Results",
      "page": 2
    },
    {
      "caption": "Figure 2: Outline of the proposed ExHuBERT architecture, including skip connections and zero linear layers. The CNN encoder and",
      "page": 3
    },
    {
      "caption": "Figure 3: Training curves on the six external SER corpora, displaying validation UAR.",
      "page": 4
    },
    {
      "caption": "Figure 3: , which shows",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "88.1\n70.8\n58.9": "40.2",
          "99.1\n90.7\n60.5": "39.1"
        },
        {
          "88.1\n70.8\n58.9": "92.9\n63.9\n34.1",
          "99.1\n90.7\n60.5": "94.6\n67.8\n38.5"
        },
        {
          "88.1\n70.8\n58.9": "70.0",
          "99.1\n90.7\n60.5": "67.5"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech analyzer for analyzing pitch or frequency perturbations in individual speech pattern to determine the emotional state of the person, US Patent 4,093",
      "authors": [
        "J Williamson"
      ],
      "year": "1978",
      "venue": "Speech analyzer for analyzing pitch or frequency perturbations in individual speech pattern to determine the emotional state of the person, US Patent 4,093"
    },
    {
      "citation_id": "3",
      "title": "Recognizing emotion in speech",
      "authors": [
        "F Dellaert",
        "T Polzin",
        "A Waibel"
      ],
      "year": "1996",
      "venue": "Proc. ICSLP, IEEE"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "5",
      "title": "Deep cross-corpus speech emotion recognition: Recent advances and perspectives",
      "authors": [
        "S Zhang",
        "R Liu",
        "X Tao",
        "X Zhao"
      ],
      "year": "2021",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "6",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and cross-language speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "EmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion Recognition",
      "authors": [
        "M Gerczuk",
        "S Amiriparian",
        "S Ottl",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Robust speech emotion recognition under different encoding conditions",
      "authors": [
        "C Oates",
        "A Triantafyllopoulos",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. INTERSPEECH, ISCA"
    },
    {
      "citation_id": "9",
      "title": "Towards robust speech emotion recognition using deep residual networks for speech enhancement",
      "authors": [
        "A Triantafyllopoulos",
        "G Keren",
        "J Wagner",
        "I Steiner",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "Proc. IN-TERSPEECH, ISCA"
    },
    {
      "citation_id": "10",
      "title": "Dawn of the transformer era in speech emotion recognition: Closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proc. NeurIPS, I. Guyon, U. V. Luxburg"
    },
    {
      "citation_id": "12",
      "title": "Selfsupervised representation learning: Introduction, advances, and challenges",
      "authors": [
        "L Ericsson",
        "H Gouk",
        "C Loy",
        "T Hospedales"
      ],
      "year": "2022",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "13",
      "title": "Wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "14",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "15",
      "title": "SUPERB: Speech processing universal PERformance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang"
      ],
      "year": "2021",
      "venue": "Proc. IN-TERSPEECH, ISCA"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "17",
      "title": "A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2022",
      "venue": "A Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "18",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "LLaMA pro: Progressive LLaMA with block expansion",
      "authors": [
        "C Wu",
        "Y Gan",
        "Y Ge",
        "Z Lu",
        "J Wang",
        "Y Feng",
        "P Luo",
        "Y Shan"
      ],
      "year": "2024",
      "venue": "LLaMA pro: Progressive LLaMA with block expansion",
      "arxiv": "arXiv:2401.02415"
    },
    {
      "citation_id": "20",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "21",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. INTERSPEECH, ISCA"
    },
    {
      "citation_id": "22",
      "title": "DEMoS: An Italian emotional speech corpus",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "23",
      "title": "Categorical vs Dimensional Perception of Italian Emotional Speech",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "A Baird",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH, ISCA"
    },
    {
      "citation_id": "24",
      "title": "Emotion Recognition In The Wild Challenge 2014: Baseline, Data and Protocol",
      "authors": [
        "A Dhall",
        "R Goecke",
        "J Joshi",
        "K Sikka",
        "T Gedeon"
      ],
      "year": "2014",
      "venue": "Proc. ICMI"
    },
    {
      "citation_id": "25",
      "title": "The eNTERFACE' 05 Audio-Visual Emotion Database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proc. ICDEW"
    },
    {
      "citation_id": "26",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "27",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "28",
      "title": "Speech emotion recognition using hidden Markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "29",
      "title": "Cross-corpus speech emotion recognition with hubert self-supervised representation",
      "authors": [
        "M Pastor",
        "D Ribas",
        "A Ortega",
        "A Miguel",
        "E Lleida"
      ],
      "year": "2022",
      "venue": "IberSPEECH, ISCA"
    },
    {
      "citation_id": "30",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Y Wang",
        "A Boumadane",
        "A Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "arxiv": "arXiv:2111.02735"
    },
    {
      "citation_id": "31",
      "title": "The development of the Athens Emotional States Inventory (AESI): Collection, validation and automatic processing of emotionally loaded sentences",
      "authors": [
        "T Chaspari",
        "C Soldatos",
        "P Maragos"
      ],
      "year": "2015",
      "venue": "The World Journal of Biological Psychiatry: The Official Journal of the World Federation of Societies of Biological Psychiatry"
    },
    {
      "citation_id": "32",
      "title": "Audio,Speech and Vision Processing Lab Emotional Sound database",
      "authors": [
        "T Dejoli",
        "Q He",
        "W Xie"
      ],
      "year": "2021",
      "venue": "Audio,Speech and Vision Processing Lab Emotional Sound database"
    },
    {
      "citation_id": "33",
      "title": "An Open Source Emotional Speech Corpus for Human Robot Interaction Applications",
      "authors": [
        "J James",
        "L Tian",
        "C Watson"
      ],
      "year": "2018",
      "venue": "Proc. INTERSPEECH"
    },
    {
      "citation_id": "34",
      "title": "SyntAct: A Synthesized Database of Basic Emotions",
      "authors": [
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "SyntAct: A Synthesized Database of Basic Emotions"
    },
    {
      "citation_id": "35",
      "title": "The variably intense vocalizations of affect and emotion (VIVAE) corpus prompts new perspective on nonspeech perception",
      "authors": [
        "N Holz",
        "P Larrouy-Maestri",
        "D Poeppel"
      ],
      "year": "2022",
      "venue": "Emotion"
    }
  ]
}