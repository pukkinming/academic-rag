{
  "paper_id": "2211.09172v4",
  "title": "Deep Emotion Recognition In Textual Conversations: A Survey",
  "published": "2022-11-16T19:42:31Z",
  "authors": [
    "Patrícia Pereira",
    "Helena Moniz",
    "Joao Paulo Carvalho"
  ],
  "keywords": [
    "Emotion Recognition",
    "Conversations",
    "Deep Learning",
    "Sentiment Analysis",
    "Dialogue"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) is a key step towards successful human-machine interaction. While the field has seen tremendous advancement in the last few years, new applications and implementation scenarios present novel challenges and opportunities. These range from leveraging the conversational context, speaker, and emotion dynamics modelling, to interpreting common sense expressions, informal language, and sarcasm, addressing challenges of real-time ERC, recognizing emotion causes, different taxonomies across datasets, multilingual ERC, and interpretability. This survey starts by introducing ERC, elaborating on the challenges and opportunities of this task. It proceeds with a description of the emotion taxonomies and a variety of ERC benchmark datasets employing such taxonomies. This is followed by descriptions comparing the most prominent works in ERC with explanations of the neural architectures employed. Then, it provides advisable ERC practices towards better frameworks, elaborating on methods to deal with subjectivity in annotations and modelling and methods to deal with the typically unbalanced ERC datasets. Finally, it presents systematic review tables comparing several works regarding the methods used and their performance. Benchmarking these works highlights resorting to pre-trained Transformer Language Models to extract utterance representations, using Gated and Graph Neural Networks to model the interactions between these utterances, and leveraging Generative Large Language Models to tackle ERC within a generative framework. This survey emphasizes the advantage of leveraging techniques to address unbalanced data, the exploration of mixed emotions, and the benefits of incorporating annotation subjectivity in the learning phase.",
      "page_start": 1,
      "page_end": 21
    },
    {
      "section_name": "Task Definition",
      "text": "The task of ERC consists of determining the emotion of each utterance in a textual conversation, for which speaker information is provided for each utterance. Formally, given a sequence of N utterances [(u 1 , p 1 ), (u 2 , p 2 ), . . . , (u N , p N )], where each utterance u i is spoken by participant p i , the goal is to predict the emotion label e i for each utterance u i .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Survey Contributions",
      "text": "To the best of our knowledge, the existent surveys on Emotion Recognition in Conversations are by Poria et al  [90] , focusing on the challenges and advances in text ERC, pointing out conversational context modelling, speaker-specific modelling, the presence of emotion shift, multiparty conversations and the presence of sarcasm, and a more recent survey by Fu et al  [23] , on multimodal ERC, that covers context modeling in conversations, speaker dependency, methods for fusing multimodal information and presents challenges such as real-world ERC and classification on the typical ERC imbalanced datasets.\n\nRelated relevant surveys covered sentiment analysis research challenges and directions  [87]  and textual emotion recognition and its challenges  [14] , the latter including a section on textual emotion recognition in dialogue, elaborating on utterance context modelling and dynamic emotion modelling. However, these are not specific to conversations, not Research in ERC has come a long way and several challenges have been addressed, as described in this section. From context, to speaker and emotion shift modelling to interpreting common sense, informal language and sarcasm, Deep Learning architectures have demonstrated their potential to address these aspects, although there is room for improvement. Application scenarios such as real-time ERC pose additional challenges. In this section, we describe such challenges and point to the sections that are connected with each challenge, and in subsection 6.1 we wrap up the challenges, elaborating on how they are currently being addressed. In Section 7, we suggest future work directions concerning partly unaddressed challenges.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Context, Speaker And Emotion Dynamics Modelling",
      "text": "While early works  [53, 120]  have tackled challenges from conversational context modelling, to speaker-specific modelling and emotion dynamics, resorting to, for example, gated networks, recent ERC approaches have leveraged the novel developments in Deep Learning, such as Transformers and Transformer-based Pre-trained Models to address these aspects.\n\nContext modelling can be interpreted as leveraging information from previous or surrounding words or utterances. Speaker-specific modelling leverages information about each speaker's utterances, capitalizing on the fact that an utterance belongs to a given speaker, being extremely important in multiparty conversations. Emotion dynamics concerns the variations of the emotions of the participants in the conversation, being more informative but also harder to model than emotion shift. Emotions concern not only who express them but also the receiver. For example, more conscious and reflective people tend to be more sensitive regarding other's emotions.\n\nConcerning these challenges, gated and graph-based neural networks and transformer-based architectures are successful at modelling long range dependencies between words in utterances and relations between utterances. Key ERC works in Section 4 are described with regards to how these Deep Learning architectures were employed to address these challenges.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Common Sense, Informal Language And Sarcasm",
      "text": "Contextual embeddings from language models, such as BERT  [15]  and RoBERTA  [65] , are pre-trained with large amounts of data, which makes them suitable to deal with informal language, sarcasm, and use of common sense to express emotions, although there is room for improvement. Most of the ERC works featured in this survey make use of these contextual embeddings.\n\nRegarding common sense, commonsense knowledge bases such as COMET  [4]  were developed to aid in leveraging the meaning behind phrases employing such knowledge. Three key ERC works described in Section 4 make use of knowledge bases. These knowledge bases, however, are not specific to emotions, which could be an extension to consider.\n\nConcerning sarcasm, speaker-specific modelling can aid in identifying the sarcastic participants in the conversation, to more accurately classify their utterances  [90] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Real-Time Erc",
      "text": "Systems to be deployed in real-life conversational scenarios may require real-time emotion recognition  [20] , which is challenging not only for modelling and recognition but also in annotation because the boundaries of each expression of emotion have to be simultaneously recognized as the emotion category  [84] . Datasets such as the ones described in subsubsection 3.2.5 feature annotations in continuous time, being more suitable for real-time ERC.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Recognizing Emotion Causes",
      "text": "Recognizing emotion causes, or what triggered the emotions in the conversation, is also an only recently addressed challenge  [89, 113, 43, 38, 40, 68] . It involves resorting to datasets enriched with emotion causes, guiding classifiers to identify not only the emotion at stake but also what caused that emotion to arise. Addressing this challenge can greatly contribute towards a more complete emotional understanding.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Different Taxonomies",
      "text": "With the use of different taxonomies and taxonomy types, as described in subsection 3.1 and reflected in the different emotion representations in the datasets in subsection 3.2, it is not straightforward to compare and establish connections between the different datasets, making it difficult, for example, to fine-tune the same ERC model with data from more than a few datasets in which adjustments can be made to their respective taxonomies.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multilingual Erc",
      "text": "Most of the benchmark datasets used to evaluate models and develop ERC applications are in English, as it can be observed in subsection 3.2. Translating the English conversations to the target application languages is not ideal since some emotion can be lost in translation. The collection and annotation of multilingual datasets for ERC would benefit research in this direction. Some dialogues involve code-switching, i.e., featuring several languages in the same dialogue or even utterance  [49] . The cultural aspect of emotion expression/interpretation also poses a challenge, since emotion is not universally expressed or understood in the same way across different cultures.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Interpretability",
      "text": "A property lacking in most of the works resorting to deep learning architectures is interpretability. It is difficult to interpret the reasoning behind these models' decisions and identify what are the characteristic features of each class. Examining attention weights is a way of determining to which features the models were attending to while computing classifications, which can aid interpretability, although there are some issues raised in the community  [42, 118] . Other methods can be proposed to provide more interpretability to the ERC pipeline, such as Fuzzy Fingerprinting Transformer Embedding Representations  [82] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Erc Taxonomies And Benchmark Datasets",
      "text": "In this section, a description of different taxonomies, i.e., ways of representing emotions, is provided, followed by the presentation of various datasets that resort to such taxonomies. The \"dialogues in the datasets reflect our daily communication way\"  [62] , therefore encompassing several emotions across different taxonomies.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Taxonomies",
      "text": "One of the most important issues in the study of emotion is whether there are basic types of affect or whether affective states should be modelled as combinations of locations on shared dimensions  [134] . These two dominant views corresponded to the categorical approach, defended by Panksepp  [78] , and to the dimensional approach, proposed by Russell  [97] , respectively. This is reflected in two main taxonomy approaches, that are described and compared in this section. The Plutchik wheel of emotions  [85]  is also described.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dimensional Vs Categorical Approach",
      "text": "The description of emotions can follow a dimensional approach, a continuous model that allows for an unlimited states' description. Following this approach, emotion is described by a range of values for valence, arousal, and dominance, the so-called VAD model  [73] . Valence is a measure of pleasure or displeasure of emotion: happiness has a positive valence, fear has a negative valence; arousal ranges from excitement to calmness: anger is high in arousal, sadness is low in arousal; dominance measures how much choice one has over an emotion: fear is low dominance, admiration is high dominance. The distribution of common emotions over the VAD 3-dimensional space can be observed in Figure  2 .\n\nA motivation for the use of the dimensional description could be the existence of mixed emotions, elaborated in subsubsection 3.1.3.\n\nThe description can also follow a categorical approach, in which an emotion is assigned to one or more labels from a set of classes. Compared to the dimensional approach, this categorical discrete framework makes it easier to annotate emotions but is more limited. Several scholars proposed to identify a set of primary emotions. The basic emotions proposed by Ekman  [18]  constitute a standard for this emotion description, categorised into joy, anger, sadness, surprise, fear, and disgust.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Plutchik Wheel Of Emotions",
      "text": "The Plutchik wheel of emotions, depicted in Figure  3 , identifies eight core emotions, organised in opposite pairs of a wheel: sadness and joy, anger and fear, expectation and surprise, and trust and disgust. These have associated emotions according to their intensities and also combine to form new emotional states  [85] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Mixed Emotions",
      "text": "A person can experience simultaneously several emotions. For example, one can experience joy and surprise at the same time and sometimes even opposite emotions simultaneously  [3] . While the VAD model allows for an unlimited description of emotions so that a mixture of emotions can be expressed in a single VAD description, the single-label categorical approach does not account for mixed emotions. This motivates the need to classify emotions with multiple labels when using the categorical approach. This need becomes more obvious with the decreasing number of labels in a dataset, but there are also multi-label datasets with a high number of emotion labels, such as the GoEmotions dataset  [13]  with 28 emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Erc Taxonomy Considerations",
      "text": "The existent taxonomies are usually transversally applied along all Emotion Recognition tasks. However, for ERC, it can be observed that there are coincident predominant classes in several benchmark datasets, such as Neutral and Happiness/Joy, as can be seen in the class distribution of the datasets on subsection 3.2. Within different ERC domains such as chit-chat vs task-oriented, e.g. mental health  [16]  and customer support  [32] , there are specific classes of emotions that will be more represented than others in each domain. With such a rich diversity of possible classes for this task, as it can be observed in the 28 classes of the GoEmotions dataset, the dimensional approach can encompass all classes but might make it harder to distinguish between differentiated emotional displays.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Erc Benchmark Datasets",
      "text": "In this subsection, different categorical, single-label datasets are presented, that were used by the works benchmarked in this survey. These works are organised in Table  3 . Other relevant datasets are also presented. Overall, their class imbalance is significant suggesting the use of balancing techniques that will be elaborated on in subsection 5.2. All datasets are in English, which is a limitation concerning multilingual ERC, as pointed out in section 2.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iemocap [6]",
      "text": "The IEMOCAP dataset consists of 151 videos of two speakers' dialogues, comprising 7.433 utterances with an average length of 11,6 words. Each utterrance is labelled with one of 8 emotions: happy, sad, neutral, angry, excited, frustrated fear, and disgust. Most of the works, however, benchmark their performance on the first 6 classes. The maximum class imbalance ratio is 1:3 (happy: frustrated).\n\nSample: \"Is that, is that -is that just foam? I can't even tell. Although, if you can't tell, it's probably isn't them. It'll probably be unmistakable, don't you think?\" Label: Excited",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Meld [88]",
      "text": "The Multimodal EmotionLines Dataset (MELD) contains 13.708 utterances from 1.433 dialogues from the TV series Friends. Each utterance encompasses audio, visual, and textual modalities, with an average length of 8,0 words. Emotion categories are Ekman's basic emotions and neutral. The maximum class imbalance ratio is 1:18 (fear: neutral).\n\nSample: \"Are we okay now?\" Label: Fear",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Emorynlp [135]",
      "text": "EmoryNLP is a text dataset extracted from transcripts of the TV show Friends. It contains 12.606 utterances, labelled with one of the six primary emotions in Willcox's feeling wheel  [119]  and a neutral label. The maximum class imbalance ratio is 1:4 (sad: neutral).\n\nSample: \"What a coincidence, I listen in my sleep.\" Label: Peaceful",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dailydialog [62]",
      "text": "DailyDialog is a large text dataset built from websites used to practise English dialogue in daily life, containing 13.118 multi-turn dialogues, comprising 102.879 utterances with an average length of 14,6 words. Each utterance is labelled with one of the six Ekman's basic emotions or other. The maximum class imbalance ratio is 1:1156 (fear:other).\n\nSample: \"I was scared stiff of giving my first performance\" Label: Disgust Table  1  shows a comparison of the label distributions of the aforementioned datasets and Table  2  shows a comparison of the corresponding percentages.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Other Datasets",
      "text": "The SEMAINE dataset  [72]  consists in audiovisual recordings of interactions between a human and an operator undertaking the role of an avatar with four personalities. Its five core emotion dimensions are valence, activation, power, anticipation/expectation, and intensity.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Deep Learning For Emotion Recognition In Conversations",
      "text": "In this section, key ERC works are described, focusing on the challenges and opportunities of the conversational scenario, presented in Section 2, and on the diverse Deep Learning architectures that can be employed for the task. Descriptions of key Deep Learning models are provided along with descriptions of works that resorted to those models. The works on Emotion Recognition in Conversations using Deep Learning described in this survey were chosen to be representative of the different techniques that are employed, covering gated Neural Networks with and without bidirectionality, Memory Networks, Graph Neural Networks, Attention Mechanisms, and Transformers. It is remarkable to observe how in four years research evolved from a simple Long Short-Term Memory Network (LSTM), taking into account the context of the conversation, to complex gated and graph neural network architectures and how the invention of transformers was reflected in not only word embeddings, such as BERT, but also in new classifier architectures.\n\nComparisons between these works are performed based on the reported weighted-F1-score without the majority class on the IEMOCAP dataset, the choice of most authors including the prominent works of Poria et al  [86] , Hazarika et al  [31] , Majumder et al  [69] , Ghosal et al  [26] , Zhong et al  [144] , Shen et al  [98] , Ghosal et al  [25] , Wang et al  [114] , Li et al  [58] , Shen et al  [99]  and Lei et al  [54] , since the dataset is not too imbalanced.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Convolutional Neural Networks For Feature Extraction",
      "text": "Convolutional Neural Networks (CNNs) are a type of feedforward neural network inspired by the biology of the visual cortex that mimics the local filtering over the input space performed by the receptive fields. In the case of text, a linear layer is applied to a sequence of words, each represented by its embeddings, separately and in the same way for each sequence. The convolutional layer can have many filters, each extracting a different feature, and is usually followed by a pooling operation. In the case of Emotion Recognition in Conversations, convolutional layers act as feature extractors, useful since strong predictors of the label can appear in different places in the input. With the advent of Pre-Trained Language Models for utterance representation, elaborated on subsection 4.10, convolutional neural networks have been less used.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "The Multi-Layer Perceptron",
      "text": "Emotion Recognition is a classification task. The most simple neural network architecture that can be employed for such a task is the Multi-Layer Perceptron.\n\nThe Multi-Layer Perceptron (MLP) is a non-linear function approximator that can be used for classification and regression. It is a class of feed-forward artificial neural networks that is composed of at least three layers: an input layer, a hidden layer, and an output layer. Its complexity and representation power increases with the number of layers. A MLP is mathematically described by the following equation:\n\nIn which L is the number of layers, W l ∈ R l-1 × R l and b l ∈ R l are the weight matrix and bias vector corresponding to layer l, and f l is a non-linear activation function corresponding to layer l. Except for the input layer, all layers' units are followed by a non-linear activation function, such as the sigmoid, the hyperbolic tangent, or the rectifier linear unit. When performing classification, the last layer is followed by the sigmoid activation function for binary classification or the softmax activation function for multiclass classification. A multiclass MLP with one hidden layer can is depicted in Figure  4 .\n\n. . . . . . . . . The MLP is commonly trained with the Backpropagation algorithm  [96] . The idea is to update the model parameters in the opposite direction of the gradient, such that a loss computed from the prediction error is minimised. The algorithm is composed of a forward pass and a backward pass. In the forward pass, the input is passed through all the layers in the network, yielding a prediction. In the backward pass, the gradients of the model's parameters are computed with respect to the loss function, making use of the chain rule. These are computed one layer at a time, iterating backward from the last layer. The computation of the gradients along with weight update are done following an optimization method, commonly using Stochastic Gradient Descent (SGD) algorithms.\n\nWhen used for Emotion Recognition, the input to the network is a set of extracted features representing each utterance obtained for example with the Bag of Words approach for text, which does not account for the order of the words, fitting this type of network but not being the ideal method for the task. That is the main reason why state-of-the-art works do not resort to this architecture.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Elman Recurrent Neural Network",
      "text": "Feed-forward neural networks, such as the above, assume data has no sequential dependencies which is not the case in Emotion Recognition in which an utterance is a sequence of words that can also comprise sequential data from other modalities. Recurrent Neural Networks (RNNs) introduce memory into the network, being able to learn sequences. They process one input, such as a word, at a time, producing a hidden state which is a summary of the sequence of inputs up to the current timestep. The most simple one, the Elman RNN  [19] , updates the hidden state as in the following equation:\n\nwhere h t ∈ R h is the hidden state at time t, x t ∈ R N is the input at time t, W ∈ R h×N , U ∈ R h×h and b ∈ R h are parameters to be learned and f is a non-linear activation function such as the sigmoid or the hyperbolic tangent. At each timestep the RNN can produce an output:\n\nwhere y t ∈ R y is the output at time t, V ∈ R y×h and c ∈ R y are parameters to be learned and g is a non linear activation function.\n\nA visualization of the RNN update through time-steps is provided in Figure  5 . RNNs are trained with a modification of the Backpropagation algorithm, the Backpropagation Through Time (BPTT) algorithm  [117] . All the parameters are shared between all time steps and it can be observed that an unrolled RNN is comparable to a feed-forward neural network with the same parameters shared over layers.\n\nIn the case of Emotion Recognition, the input to the network is a sequential utterance, for example, the combination of text, audio, and visuals corresponding to one word being fed per timestep, and the output is the predicted emotion.\n\nIn some cases, particularly when dealing with long sequences such as when considering several utterances to model context in Emotion Recognition in Conversations, the backpropagation algorithm yields vanishingly small gradients, preventing the parameters from changing their values and eventually stopping training.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Long Short-Term Memory Networks",
      "text": "To overcome this problem, Long Short-Term Memory Networks (LSTMs)  [33]  were proposed. LSTMs are composed of a memory cell and three gates. The memory cell stores the information about the input sequence across timesteps while the gates control the flow of information across the memory cell: the input gate controls the proportion of the current input to include in the memory cell; the forget gate the proportion of the previous memory cell to forget; the information gate the information to output from the current memory cell. This can be described by the following equations: where i t is the input gate, f t the forget gate, o t the output gate, c t the memory cell and sigm and tahn the element-wise sigmoid and hyperbolic tangent activation functions respectively. The several W, U and b are parameters to be learned.\n\nA visual depiction of an LSTM cell is provided in Figure  6 .\n\nThe vanishing gradient problem is overcome by the gates, specially the forget gate.\n\nAs in the Elman RNN, in the case of Emotion Recognition the input to the network can be a sequential utterance and the output is the predicted emotion. LSTMs are usually leveraged to model dependencies between several sequential utterances. In this case, the input to the network is a sequence of utterances.\n\nThe LSTM described in the equations above does not capture information from future time steps, which is useful for example in text applications when the context of a word is made up of both previous and subsequent words in the phrase. Therefore, Bidirectional Long Short-Term Memory Networks (Bi-LSTM)  [27]  were proposed. Bi-LSTMs consist of two LSTMs: one executing a forward pass and the other a backward pass. A sequence of hidden states is produced for each direction that are usually joined into a unique sequence of hidden states through concatenation.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Context-Dependent Emotion Recognition [86]",
      "text": "There is a strong correlation and influence in emotion distribution between sequential utterances in a conversation.\n\nLeveraging those contextual dependencies is key to the performance of a classifier. An utterance is a unit of speech which is bound by breathes or pauses  [77] . In textual data, it is usually delimited by punctuation such as full stops, but can also be delimited by question or exclamation marks. Along these lines, the work of  [86]  considers interdependences among utterances. The approach consists of an architecture made of LSTMs to extract contextual features from the utterances. The model enables consecutive utterances to share information while preserving their order. The authors propose the contextual LSTM, consisting of unidirectional LSTM cells, the hidden LSTM, in which the dense layer after the LSTM cell is omitted, and the bi-directional contextual LSTM that considers information from before and after the targeted utterance. It achieved an F1-score of 54.95% on the IEMOCAP dataset.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Gated Reccurent Units",
      "text": "Another RNN, which structure is somehow similar but with fewer parameters than the LSTM, is the Gated Recurrent Unit (GRU)  [10]  that can be described by the following equations:\n\nin which the r t is the reset gate that leads the candidate hidden state ĥt , to ignore or not the previous hidden state, and z t is the update gate that controls the proportion of the information from the previous hidden state to carry over to the current hidden state.\n\nGRUs can work better than LSTMs when there is less available data since they have fewer parameters, and are less prone to overfitting.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Dialoguernn [69]",
      "text": "DialogueRNN combines GRUs in a sophisticated way, in which each GRU plays a specific role in modelling the conversation.\n\nDialogueRNN predicts the emotion of an utterance based on the speaker, the context of preceding utterances, and the emotions from preceding utterances. The model has three components all modeled by GRUs. The party-state models the parties' emotion dynamics throughout the conversation. The global state has the encoding of preceding utterances and the party state, and models the context of the utterance. Finally, the emotion representation is based on the party state and the global state and is used to perform the final classification.\n\nSeveral variants of the method are proposed. DialogueRNN l considers an additional listener state while a speaker utters, BiDialogueRNN uses a Bi-directional RNN, DialogueRNN+Att uses attention over all surrounding emotion representations and BiDialogueRNN+Att combines the latter approaches.\n\nIt yielded an F1-score of 62.75% on the IEMOCAP dataset. The authors argue DialogueRNN variants outperform the contextual LSTM due to better context representation.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Attention Mechanisms",
      "text": "The aforementioned work mentions the use of attention over surrounding emotion representations. To motivate Attention Mechanisms (AM)  [1]  we now introduce Sequence to Sequence (Seq2Seq) models. Seq2Seq models are composed of an RNN encoder that takes the input sequence and outputs a context vector, the last encoder hidden state, that is fed into an RNN decoder that generates an output sequence from this vector. When the input sequence is very long the context vector does not have enough capacity to represent the information from the entire sequence. This is where AMs are useful. Taking inspiration from the human visual system that attends to different parts of the space, while building its representation of the scene, the AM allows the decoder to attend to the relevant encoder hidden states. At each time step, a context vector is obtained by a weighted sum of these hidden states, where the weights of the sum are proportional to the similarity between the current decoded hidden state and the encoded hidden states, as described in the following equations:\n\nwhere h d t is the decoder hidden state and h e i and h e j are the encoder hidden states. The most common similarity score function is the dot product. By introducing AM in an RNN the performance of a classifier usually increases.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Memory Networks",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conversational Memory Network [31]",
      "text": "The recurrent networks described before are limited in terms of long-range summarization since they rely on a sequential processing approach with implicit memory. The Conversational Memory Network (CMN) accounts for this factor through the use of memory networks, which can better capture long-term dependencies and have attention models that can summarize specific details using explicit memory structures.\n\nThe work is based on the notion that emotional dynamics involve self and inter-speaker emotional influence. Separate histories for each speaker, consisting of the speaker's previous utterances, are modeled into memory cells of the Conversational Memory Network using GRUs. Memory networks provide a memory component that can be read from and written to and also perform inference. The CMN then employs an attention mechanism over historical utterances from each speaker to filter out relevant content for the current utterance. This mechanism is repeated for multiple hops of the network to subsequently classify the utterance. CMN achieves an F1-score of 56.13% on the IEMOCAP dataset.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Interactive Conversational Memory Network [30]",
      "text": "The Interactive Conversational Memory Network (ICON) is an improvement upon CMN that has an additional dynamic global influence module to model inter-personal emotional influence, a module that maintains a global representation of the conversation, a global state that is updated using a GRU operation on the previous state and current speaker's history. ICON yields an F1-score of 58.54% on the IEMOCAP dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Graph Neural Networks",
      "text": "An alternative to gated neural networks that is also useful to capture long term dependencies is using graph neural networks. Graph neural networks are based on graphs. The latter model a set of objects, the nodes, and their relationships, the edges. While standard neural networks such as CNNs and RNNs cannot handle the graph input properly since they need a specific order for the nodes, the output of GNNs is invariant for the input order of nodes. Furthermore, while in standard neural networks, dependencies between nodes are a node feature, in GNNs there are edges to represent these dependencies. GNNs can propagate information by the graph structure instead of using it as part of features  [145] .\n\nTwo common types of GNNs are recurrent GNNs (RecGNNs) and convolutional GNNs (ConvGNNs). RecGNNs learn node representations with recurrent neural architectures, assuming that a node in a graph constantly exchanges information with its neighbors until a stable equilibrium is reached. ConvGNNs generalize the convolution operation from grid structure to graph structure, generating a node representation by aggregating its features with its neighbours' features. For classification tasks, such as Emotion Recognition, an end-to-end framework can be constructed for example by stacking graph convolutional layers followed by a softmax layer  [122] .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Dialoguegcn [26]",
      "text": "Dialogue Graph Convolutional Network (GCN) takes into account intra and inter-speaker dependency and background information, having a sequential and a speaker-level encoder. The sequential context encoder encodes the utterances using a bidirectional GRU and is speaker-agnostic. The speaker-level context encoder creates a directed edge-labelled graph in which each utterance, enriched with context, is a node in the graph. The information from neighbor nodes is then aggregated and passed to a neural network in each node, resulting in an updated representation of each node that takes into account its neighbors. In DialogueGCN, node features are initialized with sequentially encoded feature vectors from the sequential context encoders, and edge weights are set using a similarity-based attention module. The utterance level emotion classification is turned into a problem of node classification in the graph. DialogueGCN achieves an F1-score of 64.18% on the IEMOCAP dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Transformer",
      "text": "Despite the success of recurrent and graph neural networks in a wide variety of tasks including Emotion Recognition, a more powerful network model exists, constituting the current state-of-the-art: the Transformer  [112] . The Transformer is also better at capturing long-term dependencies than gated RNNs, due to its shorter path of information flow, which is useful when modelling several utterances in Emotion Recognition in Conversations. It does not resort to recurrence or convolutions. It is also more parallelizable and needs less training time. It is composed of an Encoder and Decoder, which can be visualized in Figure  7 .\n\nEach Encoder layer is composed of two sub-layers. The first is a so called self-attention layer for building the context of the sequence that calculates the relevance of the tokens in the sequence for each given token.\n\nThe self-attention mechanism takes as input three matrices, the Query, Key, and Value matrices. These are computed from the dot product between the input sequence matrix and weight matrices learned in the training phase. It then computes a vector of attention scores, as described in the following equation:  helps stabilizing the gradients.\n\nThe self-attention layer uses a multi-head attention mechanism so that the attention score vectors do not exclusively give a high probability score for its corresponding token, defined by the following equations:\n\nwhere\n\nin which",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Being H The Number Of Heads And K=V=M/H.",
      "text": "The output of the self-attention layer is fed to the second layer, a fully connected feed-forward network, with two linear transformations with a ReLU activation in between, applied separately and in the same way to each position, allowing for parallelization.\n\nA residual connection is applied around each of the two sub-layers, followed by layer normalization.\n\nEach Decoder layer is composed of three sublayers. The first is a masked self-attention layer similar to the self-attention layer described mathematically in Equations 16 and 17, but in which each position in the decoder attends to all positions in the decoder up to and including that position, which is attained by masking out values in the softmax. The second is an \"encoder-decoder attention\" layer, described mathematically in the same equations, where K and V come from the output of the encoder and Q comes from the output of the decoder's masked self-attention layer, mimicking the typical encoder-decoder attention mechanisms in Seq2Seq models. The last layer is a feed-forward neural network, similar to the Encoder layer. Similar to the Encoder, a residual connection is applied around each of the two sub-layers, followed by layer normalization.\n\nPositional encodings are added to the embeddings that are fed to the Encoder and Decoder to introduce information about the position of the tokens in the sequence.\n\nThe most common use of the Transformer is as the backbone for a fine-tuned pre-trained transformer-based language model, such as BERT  [15] , described in subsection 4.10. These language models are trained with large amounts of data to perform many tasks and only need to be fine-tuned for the specific task at hand. In the case of a classification task such as Emotion Recognition, one just needs to add a linear layer or a more complex neural network architecture on top of the language model. The training is performed with supervised learning, resorting to backpropagation algorithms. 4.9.1 Knowledge-Enriched Transformer  [144]  The invention of the Transformer  [112]  led to new state-of-the-art results in several Natural Language Processing tasks. The Knowledge-Enriched Transformer (KET) uses self-attention to model context and response using the Transformer, that a has shorter path of information flow than gated RNNs, overcoming the difficulty in capturing long-term dependencies. Conversations are modeled in the entire Transformer as a single input.\n\nFirst, concepts are retrieved for each word in the conversation using an external knowledge base, a graph of concepts, then the concept representation is computed using a dynamic context-aware graph attention mechanism over the concepts that are then combined with the input sentence embeddings. Self-attention is used to learn utterance representations individually and hierarchical self-attention is applied to the context to learn the context representation. Finally, the encoder and decoder attention mechanism is applied, followed by max pooling and a linear layer. KET achieves an F1-score of 59.56% on the IEMOCAP dataset.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Cesta [114]",
      "text": "The CESTa or Contextualized Emotion Sequence Tagging approaches ERC as a task of sequence tagging, choosing the set of tags with the highest likelihood for the whole utterance sequence at once, by using a Conditional Random Field (CRF)  [50] . In order to capture long-range global context, utterance representations are generated by a multi-layer Transformer encoder. These are then fed to a bi-LSTM encoder that captures self and inter-speaker dependencies, resulting in contextualized representations of utterances that are then passed to the CRF layer. It achieves an F1-score of 67.10% in the IEMOCAP dataset.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Dialogxl [98]",
      "text": "DialogXL is an adaptation of the pre-trained language model XLNet  [130] , which is based on the Transformer-XL  [11] , for Emotion Recognition in Conversations. It replaces XLNet's segment recurrence with a memory utterance recurrence to leverage historical utterances. These utterances' hidden states are stored in a memory bank to reuse them while identifying a query utterance. The approach also replaces XLNet's vanilla self-attention proposing a dialog-aware self-attention to grasp useful intra-and inter-speaker dependencies. This is composed of four types of self-attention: global and local self-attention for different sizes of receptive fields, and speaker and listener self-attention for intra-and inter-speaker dependencies. DialogXL achieves an F1-score of 65.94% on the IEMOCAP dataset.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Embeddings From Pre-Trained Transformer Language Models",
      "text": "Text features must represent words in a way that is useful for the models, in this case, numeric vectors referred to as word embeddings. These embeddings yield similar representations for similar words.\n\nStatic embeddings  [74, 80, 75]  are obtained based on the co-occurrence of adjacent words and have a fixed representation for the words, not taking into account their context.\n\nContrarily, contextual embeddings include information from the context in the word representation, a representation that differs according to the occurrence of the word. The only disadvantage of this kind of representation is that we cannot use pre-trained embeddings as in static embeddings so there is an inference time for generating the embeddings. The models are pre-trained with large amounts of data, in order to enable generating good embedings. A first approach, Embeddings from Language Models (ELMo)  [83]  is based on a stack of two Bi-LSTMs, leveraging the full context of a word. It provides a context-free representation of the word along with context information of the sense of the word and its syntax. The second approach, Bidirectional Encoder Representations from Transformers (BERT)  [15] , built from a Transformer encoder stack, leads to state-of-the-art results in multiple Natural Language Processing tasks. The input given to BERT is the sum of three embeddings, the token embedding corresponding to the sequence of words, the segment embedding containing information on which sentence each token belongs to, and the position embedding, defining the position of each token in the sequence and distance to other tokens taking into account the order of the sequence. An improvement upon BERT, RoBERTa  [65]  is trained with more data and for a longer period of time. Given that context is key for text Emotion Recognition in Conversations, contextual word embeddings are more adequate for the task than static embeddings, as can be seen in the higher performance of works that resort to these embeddings, namely resultant from Pre-trained language models, such as BERT-based embeddings. The latter are also more tailored to deal with common sense, informal language, and sarcasm, ERC challenges described in Section 2, since they are pre-trained on large amounts of text.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Cosmic [25]",
      "text": "Besides choosing appropriate utterance representations and classifier architectures, there are other factors that can be leveraged for the task of ERC. One such factor is the commonsense knowledge of the interlocutors that plays a central role in inferring the latent variables of a conversation, such as speaker state and intent.\n\nCOSMIC extracts commonsense features resorting to COMET  [4] , a commonsense transformer encoder-decoder model trained on the task of generative commonsense knowledge construction that uses GPT  [93]  as its generative model. The approach uses RoBERTa for context-independent feature extraction, passes each utterance through COMET's encoder, and extracts the activations from the final time step. COSMIC maintains a context state and attention vector which are always shared between the participants of the conversation. Five GRUs are used to model context state, internal state, external state, intent state, and emotion state, being the latter used for emotion classification. This work achieves an F1-score of 65.28% in the IEMOCAP dataset.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Psychological [58]",
      "text": "Psychological also leverages COMET as the commonsense knowledge base and resorts to RoBERTa as the utterancelevel encoder. It proposes SKAIG as a conversation-level encoder, a locally connected graph, where the targeted utterance receives information from that past and future context and is also self-connected.\n\nAssuming that the influence of an utterance on contextual utterances is locally effective, the targeted node is connected with contextual nodes in a window of a given size of utterances in the past and future. Knowledge from COMET is introduced to enrich the edges with different relations. To propagate information through SKAIG a Graph Transformer  [101]  is used. Finally, a linear unit predicts the emotion distributions. This work achieves an F1-score of 66.96% in the IEMOCAP dataset.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Dag-Erc [99]",
      "text": "Graph-based methods tend to neglect distant utterances and sequential information. On the other hand, recurrencebased methods leverage those but tend to update the query utterance's state with limited information from the nearest utterances. Combining both graph and recurrence structures explores their advantages and mitigates their drawbacks. This is the idea behind DAG-ERC, which regards each conversation as a directed acyclic graph (DAG), a combination of both structures. DAG-ERC is based on the DAGNN  [105]  architecture, with two improvements: a relation-aware feature transformation that gathers information based on speaker identity and a contextual information unit that enhances the information of historical context. RoBERTa-Large is used as the feature extractor. It achieves an F1-score of 68.03% on the IEMOCAP dataset, outperforming the previous approaches.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Erc Within A Generative Framework",
      "text": "With the advent of Generative Large Language Models (LLMs), such as the GPT series  [92, 93, 5]  and Llama  [106] , classification tasks started to be reformulated within generative frameworks. The idea is to prompt the LLM with the utterance to be classified and other relevant additional input and request an emotion label for the utterance. Open-source LLMs can also be fine-tuned and even pre-trained.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Instructerc [54]",
      "text": "In InstructERC the prompt consists of the utterance to be classified, the ERC instruction, the conversational context, the set of possible classification labels, and similar utterances together with its emotion labels. The LLM is pre-trained with a speaker identification task and fine-tuned both with the main ERC task and an emotion influence prediction task to improve overall performance. Experiments were made with several LLMs, and the best results were obtained with LLama2. InstructERC achieves 71.39% in the IEMOCAP dataset, outperforming the previous approaches.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Advisable Erc Practices",
      "text": "This section provides useful methods for better ERC frameworks, from different ways of dealing with subjectivity in emotion annotation and modelling, to several ways of dealing with the typically unbalanced ERC datasets.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Learning Subjectivity",
      "text": "All human annotation tasks involve some degree of uncertainty, that can stem amongst other factors from the subjectivity inherent to the annotator. Since Emotion Recognition is amongst the most subjective tasks  [111]  and that subjectivity is the main source of uncertainty in Emotion Recognition annotation, we address uncertainty as subjectivity in this survey  [95] . A good annotation practice is to resort to several annotators to avoid biasing the results towards a single annotator's subjectivity. This will naturally yield different labels from each annotator, the so called annotation disagreement. The more appropriately one deals with the disagreement in the annotated labels the more reliable will the data and classifiers be. Several techniques for dealing with disagreement are described in this section. They are divided into techniques that address subjectivity by adapting the labels, towards an estimate of a gold label, and techniques that embrace subjectivity by adapting the classifier architectures to deal with the subjectivity of the labels, leveraging such property.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Addressing Subjectivity",
      "text": "For categorical labels, considering that a gold or true label exists for each sample, the simple way to obtain it would be through majority voting, and choosing the label with the most annotations. However, this does not account for the different annotator skill levels or sample inherent difficulty. Thus, a common process is to calculate an inter-annotator agreement score to find low agreement data. A very simplistic way of calculating inter-annotator agreement for categorical labels would be to use the percent agreement, dividing the number of agreement classifications by the total number of classifications. However, non-expert annotators do not always know which label to use so they sometimes just guess, resulting in a random label. Cohen pointed out that there is some level of agreement in these random labels and developed Cohen's Kappa to take that factor into account  [71] . The related Fleiss' Kappa  [22]  is the adaptation of Cohen's Kappa for 3 or more annotators.\n\nThe low agreement data can then be discarded or trained on and evaluated separately from the high agreement data. It can be considered random noise if it does not reveal the existence of annotator bias  [111] .\n\nFor interval labels, weighted fusion of data can be performed with approaches such as the Evaluator Weighted Estimator (EWE)  [28] , which weights each rater's annotations based on its rater-specific inter-reliability scores, or the Weighted Trustability Evaluator (WTE)  [29] , which is instead based on each rater's performance consistency.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Embracing Subjectivity [95]",
      "text": "The annotator (dis)agreement can be used as privileged information for master-class learning, being viewed as samples' additional information  [21]  to facilitate learning in the training phase, that might or might not be required for making predictions during testing, for example, to weigh positively the loss of the corresponding samples. In this setting, it is useful to learn the meaning of high-rater disagreement for the particular dataset in use.\n\nInstead of considering the existence of a hard label, as in subsubsection 5.1.1, one can calculate the distribution of label annotations to define a soft label distribution per sample, which encodes the subjectivity for each sample and allows for the classifier to learn label correlations. Another approach is to model the annotations of each particular rater, through an ensemble of models. It allows to give more importance to expert raters rather than novices or spammers. The addition of an \"unsure\" label would facilitate the annotator's workload and also make the model learn the ambiguity ground-truth, although not having to predict \"unsure\" labels during testing. Furthermore, methods to understand whether a sample is mislabelled by certain raters or inherently ambiguous could inform the model and annotators during active learning processes.\n\nIn ERC, the context of the interactions is not always explicit and the dialogues contain a variety of topics that can exacerbate subjectivity even more, making it much more advisable to embrace subjectivity in this domain.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Dealing With Unbalanced Data",
      "text": "All the benchmark datasets used for ERC are unbalanced, some of them are severely unbalanced: the unbalance ratio varies from 1:3 up to 1:1156. In the presence of these unbalances, most classifiers tend to favor the majority classes which, in these datasets, corresponds to the neutral emotion class. Hence, unless the unbalance issue is addressed, most models will be trained and evaluated based on the neutral class results, which is far from intended. All classes should be equally relevant in ERC, and even when Accuracy values look acceptable, a more detailed analysis usually reveals a very poor performance in minority classes.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Metrics For Unbalanced Data",
      "text": "The performance metrics used for Emotion Recognition in Conversations are Accuracy, Precision, Recall, and F-score or F1, the harmonic mean of Precision and Recall. These are commonly used metrics in many classification applications. One can consider the weighted version of these metrics, that take into account the relative frequencies of each class, or micro-averaging, in which all samples equally contribute to the final averaged metric. Since both weighted versions and micro-averaging depend more on the classes with a bigger number of items, those are not good indicators of the performance of the minority classes. Therefore, those should not be used when the classes have the same importance and unequal frequencies. In unweighted macro-averaging, the metric is computed for each class, and the results are averaged over all the classes. By maximizing an unweighted metric, one is maximizing the performance of the model in correctly classifying all classes regardless of the number of items they have.\n\nERC is clearly a task that deals with real-world unbalanced data. As such, a proper evaluation should always be performed on a test set that maintains the real-world unbalance of the data and using unweighted metrics such as macro F-score. Ideally, performance results on each of the individual classes should also be presented. Accuracy, micro-averaging, and weighted metrics are mostly irrelevant in what concerns the performance of the models since they are mostly dependent on the majority class, usually neutral. Unweighted metrics that are not affected by dataset unbalance, such as Recall, should also be avoided, unless used in conjunction with those that are, such as Precision.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Balancing Techniques",
      "text": "Besides the use of appropriate metrics, another way of dealing with dataset unbalance is to apply balancing techniques to the training set. Common such techniques are random under-sampling, removing observations from the majority class, and random over-sampling, adding copies to the minority class. More elaborate techniques, such as Smote -Synthetic Minority Oversampling Technique  [8] , usually improve the results.\n\nBalancing a training set has its advantages, but it is of utmost importance that the test set maintains the original set balance. Otherwise, metrics such as Precision and F-score are artificially improved and the model is unlikely to be effective in an unbalanced real-world dataset. It should be noted that Recall is not affected when training and testing on an artificially balanced dataset so ignoring Precision and looking only at Recall as an evaluation metric will not be a good indicator for real-world performance of a model when there is a real-world imbalance among classes.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Few-Shot Learning",
      "text": "Few-Shot Learning  [115]  is also an efficient method for dealing with unbalanced data. Contrary to supervised learning, it does not require a high number of training examples. Instead of learning to generalize class identities from a training set to a test set, the Few-Shot Learning models learn to discriminate the similarities and differences between classes, by training a function that predicts similarity.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Systematic Review Of Erc Works",
      "text": "In Table  3 , a plethora of works on ERC, since 2017, are displayed along with the methods they use and their reported performance. there is still much room for improvement, as it can be seen by the performance of the classifiers on the benchmark datasets and the several unexplored directions put forward in this section. While we described how current work has addressed several challenges, we also pointed out the opportunities that partly unaddressed challenges constitute.\n\nAs main contributions of this survey, we presented partly addressed challenges for ERC, such as recognizing emotion causes, dealing with different taxonomies across datasets, multilingual ERC, and interpretability with associated future work directions. We compiled an extensive list of Deep Learning works in ERC, being the first to simultaneously report their methods, modalities, and performance across various datasets. Finally, our descriptions of Deep Learning methods in the context of ERC provided insights into the suitability of each method for this task, which were not given in previous surveys.\n\nThe survey highlights the advantage of leveraging techniques to address unbalanced data, the exploration of mixed emotions, and the benefits of incorporating annotation subjectivity in the learning phase.\n\nOur survey relies on established benchmark datasets. While this aids in comparing different works, it also constitutes a limitation since benchmarking on these datasets may not generalize well to domain-specific applications such as different industries or languages. We also just briefly mention some challenges of real-world applications, not elaborating on the scalability or performance of models in such settings. So our findings may be highly relevant to academic benchmarks but less relevant to domain-specific applications.\n\nFinally, important ethical aspects pertaining to Emotion Recognition are now presented. These aspects are, for example, and not limited to, whether an Emotion Recognition module should be developed or used for a certain purpose, which data to collect and the subjects behind the data, diversity and inclusiveness, privacy and control, and possible biases and misuses of the application  [76] . Research in these directions will benefit the community with better Emotion Recognition in Conversation modules for current and novel applications.",
      "page_start": 20,
      "page_end": 21
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: depicts the evolution of the number of ERC publications across the years,",
      "page": 2
    },
    {
      "caption": "Figure 1: Evolution of the number of ERC publications across the years",
      "page": 2
    },
    {
      "caption": "Figure 2: A motivation for the use of the dimensional description could be the existence of mixed emotions, elaborated in",
      "page": 5
    },
    {
      "caption": "Figure 2: Distribution of common emotions over the VAD 3-dimensional space [2]",
      "page": 6
    },
    {
      "caption": "Figure 3: , identifies eight core emotions, organised in opposite pairs of a",
      "page": 6
    },
    {
      "caption": "Figure 3: The Plutchik wheel of emotions. Image retrieved from Wikipedia.",
      "page": 6
    },
    {
      "caption": "Figure 4: A Multy Layer Perceptron with one hidden layer, input layer with feature dimension N and output layer with",
      "page": 9
    },
    {
      "caption": "Figure 5: Two visual descriptions of the Recurrent Neural Network. The description on the left is unrolled, highlighting",
      "page": 10
    },
    {
      "caption": "Figure 6: Long-Short Term Memory Network cell",
      "page": 11
    },
    {
      "caption": "Figure 6: The vanishing gradient problem is overcome by the gates, specially the forget gate.",
      "page": 11
    },
    {
      "caption": "Figure 7: Each Encoder layer is composed of two sub-layers. The first is a so called self-attention layer for building the context of",
      "page": 13
    },
    {
      "caption": "Figure 7: The Transformer. The Encoder on the left and Decoder on the right display some similar components.",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label": "Neutral\nHappiness/Joy\nSurprise\nSadness\nAnger\nDisgust\nFear\nFrustration\nExcitement\nPeace\nPowerful",
          "IEMOCAP": "1708\n648\n-\n1084\n1103\n-\n-\n1849\n1041\n-\n-",
          "MELD": "6436\n2308\n1636\n1002\n1607\n361\n358\n-\n-\n-\n-",
          "Emory NLP": "3776\n2755\n-\n844\n1332\n-\n1646\n-\n-\n1190\n1063",
          "Daily Dialog": "85572\n12885\n1823\n1150\n1022\n353\n74\n-\n-\n-\n-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label": "Neutral\nHappiness/Joy\nSurprise\nSadness\nAnger\nDisgust\nFear\nFrustration\nExcitement\nPeace\nPowerful",
          "IEMOCAP": "23.0\n8.7\n-\n14.6\n14.8\n-\n-\n24.9\n14.0\n-\n-",
          "MELD": "47.0\n16.8\n11.9\n7.3\n11.7\n2.6\n2.6\n-\n-\n-\n-",
          "Emory NLP": "30.0\n21.9\n-\n6.7\n10.6\n-\n13.1\n-\n-\n9.4\n8.4",
          "Daily Dialog": "83.2\n12.5\n1.8\n1.1\n1.0\n0.3\n0.1\n-\n-\n-\n-"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "GRU\nw-F1(6)\nw-F1\nw-F1\nm-F1"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Jiao et al [46]*\nBi-GRU, Attention\n62.7\n58.1\nMajumder et al [69]*\nBi-GRU, Attention\n62.75\nLu et al [67]\nBi-GRU, Attention\n64.37\n60.72"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "LSTM"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Partaourides et al [79]\nBi-LSTM, GRU, Attention\n53.0\nPoria et al [86]*\nLSTM\n54.95\nXing et al [124]*\nBi-LSTM, GRU, Attention\n64.30\n60.45"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Hu et al [37]\nBi-LSTM, Attention\n66.20\n58.39"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Memory Network"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Hazarika et al [31]*\nMemory Network, GRU, Attention\n56.13\nHazarika et al [30]*\nMemory Network, GRU, Attention\n58.54\nLai et al [51]*\nMemory Network, GRU, Attention\n62.43"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Graph Neural Network (GNN)"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Ghosal et al [26]\nGraph Neural Network, GRU\n64.18\n58.10\nIshiwatari et al [41]\nGraph Neural Network, Attention\n65.22\n60.91\n34.42\n54.31\nHu et al [39]*\nGraph Neural Network\n66.22\n58.65\nSheng et al [100]\nGraph Neural Network, Bi-LSTM, Attention\n66.61\n58.45\nHu et al [36]*\nGraph Neural Network, Bi-GRU\n68.18\n59.46"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Quantum-inspired Neural Network\nLi et al [60]*\n59.88\n58.00"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Transformer"
        },
        {
          "Author, Year\nMethods\nIEMO\nMELD\nENLP\nDD": "Zhong et al [144]\nTransformer, Knowledge Base (KB)\n59.56\n58.18\n34.39\n53.37\nZhang et al [138]\nTransformer\n61.43\n58.97\n35.59\n54.71\nZhu et al [146]\nTransformer, KB\n62.81\n68.23\n43.12\n58.47\nKhang and Cho [47]\nTransformer, GNN, Few-Shot Learning\n63.16\n52.83\nSun et al [104]\nTransformer, Bi-LSTM, GNN\n64.10\n64.22\n36.38\nLi et al [59]\nTransformer\n64.50\n61.94\n36.75\nGhosal et al [25]\nTransformer, GRU, KB\n65.28\n65.21\n38.15\n58.48\nTu et al [108]\nTransformer, GRU, KB\n65.69\n66.12\n39.47\nShen et al [98]\nTransformer\n65.94\n62.41\n34.73\n54.93\nLi et al [61]\nTransformer, Contrastive Learning [24]\n66.18\n69.70\n49.08\n56.46\nZhang et al [139]\nTransformer, Attention\n66.35\n38.93\n61.22\nLi et al [58]\nTransformer, GNN, KB\n66.96\n65.18\n38.88\n59.75\nXie et al [123]\nTransformer, KB, Attention\n66.98\n63.24\n57.30\nWang et al [114]\nTransformer, Bi-LSTM, CRF\n67.10\n58.36\n63.12\nTu et al [109]\nTransformer, Constrastive Learning\n67.16\n66.21\n40.23\n60.96\nZhang et al [137]\nTransformer, GNN\n67.68\n66.90\n40.69\n61.84\nShen et al [99]\nTransformer, DAGNN, GRU\n68.03\n63.65\n39.02\n59.33\nYang et al [129]\nTransformer, GNN, LSTM, KB\n68.31\n66.25\n40.23\n60.21\nTu et al [110]\nTransformer, GNN, Contrastive Learning\n68.49\n65.34\n39.20\nQuan et al [91]\nTransformer, GNN\n68.50\n63.80\n39.19\n59.39\nZa et al [136]\nTransformer, KB, GNN, GRU\n68.53\n63.92\n39.56\n59.78\nYang et al [128]\nTransformer, GNN\n68.73\n66.18\n46.11\n59.76\nSu et al [103]\nTransformer - Generative LLM, GNN, LSTM\n68.90\n67.50\nLiu et al [66]\nTransformer, Attention\n68.93\n65.37\nLiang et al [64]\nTransformer, GNN\n68.93\n64.17\n40.05\n64.18\nWu et al [121]*\nTransformer, Bi-LSTM, Attention\n69.09\nDuong et al [17]\nTransformer, GNN\n69.10\n63.82\n39.85\nMao et al [70]*\nTransformer\n69.23\n63.55\nZhang and Li [141]*\nTransformer, GNN\n69.6\n62.3\nSong et al [102]\nTransformer, Contrastive Learning\n69.74\n67.25\n40.94\nYang et al [126]\nTransformer, Adapter, Contrastive Learning\n69.81\n65.70\n38.75\n62.51\nXu and Yang [125]\nTransformer, Attention, Contrastive Learning\n69.91\n67.58\n40.79\n60.70\nZhang et al [142]*\nTransformer - Generative LLM\n69.93\n71.90\n40.05\nLi et al [57]*\nTransformer, GNN\n70.00\n58.94\nHou et al [35]\nTransformer, GNN, Contrastive Learning\n70.16\n66.65\n41.06\n62.19\nYang et al [127]\nTransformer, Variational Autoencoder [48]\n70.22\n65.94\n62.14\nTu et al [107]\nTransformer, GNN\n70.43\n66.19\n40.51\nYu et al [132]\nTransformer, Contrastive Learning\n70.41\n67.12\n40.24\nYun et al [133]*\nTransformer\n70.48\n67.37\nLi et al [55]*\nTransformer, GNN\n71.03\n61.77\nLi et al [56]*\nTransformer\n71.04\n66.70\nYao and Shi [131]*\nTransformer, GNN, LSTM, Bi-GRU\n71.21\n66.25\nLei et al [54]\nTransformer - Generative LLM\n71.39\n69.15\n41.37\nZhang et al [140]*\nTransformer, GNN\n71.60\n62.30"
        }
      ],
      "page": 19
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "2",
      "title": "Emotion classification based on biophysical signals and machine learning techniques",
      "authors": [
        "Oana Bȃlan",
        "Gabriela Moise",
        "Livia Petrescu",
        "Alin Moldoveanu",
        "Marius Leordeanu",
        "Florica Moldoveanu"
      ],
      "year": "2019",
      "venue": "Symmetry"
    },
    {
      "citation_id": "3",
      "title": "Eliciting mixed emotions: a meta-analysis comparing models, types, and measures",
      "authors": [
        "Raul Berrios",
        "Peter Totterdell",
        "Stephen Kellett"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "4",
      "title": "COMET: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "Antoine Bosselut",
        "Hannah Rashkin",
        "Maarten Sap",
        "Chaitanya Malaviya",
        "Asli Celikyilmaz",
        "Yejin Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "5",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "7",
      "title": "Hierarchical pre-training for sequence labelling in spoken dialog",
      "authors": [
        "Emile Chapuis",
        "Pierre Colombo",
        "Matteo Manica",
        "Matthieu Labeau",
        "Chloé Clavel"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "8",
      "title": "Smote: synthetic minority over-sampling technique",
      "authors": [
        "Kevin Nitesh V Chawla",
        "Lawrence Bowyer",
        "Philip Hall",
        "Kegelmeyer"
      ],
      "year": "2002",
      "venue": "Journal of artificial intelligence research"
    },
    {
      "citation_id": "9",
      "title": "Sdtn: Speaker dynamics tracking network for emotion recognition in conversation",
      "authors": [
        "Jiawei Chen",
        "Peijie Huang",
        "Guotai Huang",
        "Qianer Li",
        "Yuhong Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation",
      "authors": [
        "Kyunghyun Cho",
        "Bart Van Merriënboer",
        "Caglar Gulcehre",
        "Dzmitry Bahdanau",
        "Fethi Bougares",
        "Holger Schwenk",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "11",
      "title": "Transformer-XL: Attentive language models beyond a fixed-length context",
      "authors": [
        "Zihang Dai",
        "Zhilin Yang",
        "Yiming Yang",
        "Jaime Carbonell",
        "Quoc Le",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "12",
      "title": "",
      "authors": [
        "António Damásio",
        "Sentir",
        "-A Caminho Saber",
        "Consciência Da"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "13",
      "title": "GoEmotions: A dataset of fine-grained emotions",
      "authors": [
        "Dorottya Demszky",
        "Dana Movshovitz-Attias",
        "Jeongwoo Ko",
        "Alan Cowen",
        "Gaurav Nemade",
        "Sujith Ravi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "A survey of textual emotion recognition and its challenges",
      "authors": [
        "Jiawen Deng",
        "Fuji Ren"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "16",
      "title": "Negative emotions detection on online mental-health related patients texts using the deep learning with mha-bcnn model",
      "authors": [
        "Kodati Dheeraj",
        "Tene Ramakrishnudu"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "17",
      "title": "Residual relation-aware attention deep graph-recurrent model for emotion recognition in conversation",
      "authors": [
        "Anh-Quang Duong",
        "Ngoc-Huynh Ho",
        "Sudarshan Pant",
        "Seungwon Kim",
        "Soo-Hyung Kim",
        "Hyung-Jeong Yang"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "18",
      "title": "Basic emotions. Handbook of cognition and emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Basic emotions. Handbook of cognition and emotion"
    },
    {
      "citation_id": "19",
      "title": "Distributed representations, simple recurrent networks, and grammatical structure",
      "authors": [
        "Jeffrey L Elman"
      ],
      "year": "1991",
      "venue": "Machine learning"
    },
    {
      "citation_id": "20",
      "title": "On-line emotion recognition in a 3-d activation-valence-time continuum using acoustic and linguistic cues",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Alex Graves",
        "Björn Schuller",
        "Ellen Douglas-Cowie",
        "Roddy Cowie"
      ],
      "year": "2010",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "21",
      "title": "A multitask approach to continuous five-dimensional affect sensing in natural speech",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "22",
      "title": "Measuring nominal scale agreement among many raters",
      "authors": [
        "L Joseph",
        "Fleiss"
      ],
      "year": "1971",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition in conversations: A survey focusing on context, speaker dependencies, and fusion methods",
      "authors": [
        "Yao Fu",
        "Shaoyang Yuan",
        "Chi Zhang",
        "Juan Cao"
      ],
      "venue": "Electronics"
    },
    {
      "citation_id": "24",
      "title": "SimCSE: Simple contrastive learning of sentence embeddings",
      "authors": [
        "Tianyu Gao",
        "Xingcheng Yao",
        "Danqi Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "COSMIC: COmmonSense knowledge for eMotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "26",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "27",
      "title": "Bidirectional lstm networks for improved phoneme classification and recognition",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Jürgen Schmidhuber"
      ],
      "year": "2005",
      "venue": "International conference on artificial neural networks"
    },
    {
      "citation_id": "28",
      "title": "Evaluation of natural emotions using self assessment manikins",
      "authors": [
        "Michael Grimm",
        "Kristian Kroschel"
      ],
      "year": "2005",
      "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "29",
      "title": "Introducing the weighted trustability evaluator for crowdsourcing exemplified by speaker likability classification",
      "authors": [
        "Simone Hantke",
        "Erik Marchi",
        "Björn Schuller"
      ],
      "year": "2016",
      "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)"
    },
    {
      "citation_id": "30",
      "title": "Icon: interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Rada Mihalcea",
        "Erik Cambria",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "31",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "32",
      "title": "Classifying emotions in customer support dialogues in social media",
      "authors": [
        "Jonathan Herzig",
        "Guy Feigenblat",
        "Michal Shmueli-Scheuer",
        "David Konopnicki",
        "Anat Rafaeli",
        "Daniel Altman",
        "David Spivak"
      ],
      "year": "2016",
      "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "33",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "34",
      "title": "Detectivenn: Imitating human emotional reasoning with a recall-detectpredict framework for emotion recognition in conversations",
      "authors": [
        "Simin Hong",
        "Jun Sun",
        "Taihao Li"
      ],
      "year": "2024",
      "venue": "Detectivenn: Imitating human emotional reasoning with a recall-detectpredict framework for emotion recognition in conversations"
    },
    {
      "citation_id": "35",
      "title": "Enhancing emotion recognition in conversation via multi-view feature alignment and memorization",
      "authors": [
        "Guiyang Hou",
        "Yongliang Shen",
        "Wenqi Zhang",
        "Wei Xue",
        "Weiming Lu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "36",
      "title": "Mm-dfn: Multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lianxin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "37",
      "title": "Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai",
        "Dialoguecrn"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "38",
      "title": "Unimeec: Towards unified multimodal emotion recognition and emotion cause",
      "authors": [
        "Guimin Hu",
        "Zhihong Zhu",
        "Daniel Hershcovich",
        "Hasti Seifi",
        "Jiayuan Xie"
      ],
      "year": "2024",
      "venue": "Unimeec: Towards unified multimodal emotion recognition and emotion cause",
      "arxiv": "arXiv:2404.00403"
    },
    {
      "citation_id": "39",
      "title": "MMGCN: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "40",
      "title": "Causal discovery inspired unsupervised domain adaptation for emotion-cause pair extraction",
      "authors": [
        "Yuncheng Hua",
        "Yujin Huang",
        "Shuo Huang",
        "Tao Feng",
        "Lizhen Qu",
        "Chris Bain",
        "Richard Bassed",
        "Gholamreza Haffari"
      ],
      "year": "2024",
      "venue": "Causal discovery inspired unsupervised domain adaptation for emotion-cause pair extraction",
      "arxiv": "arXiv:2406.15490"
    },
    {
      "citation_id": "41",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "42",
      "title": "Attention is not Explanation",
      "authors": [
        "Sarthak Jain",
        "Byron Wallace"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "43",
      "title": "Conversational emotion-cause pair extraction with guided mixture of experts",
      "authors": [
        "Dongjin Jeong",
        "Jinyeong Bak"
      ],
      "year": "2023",
      "venue": "Proceedings of the 17th Conference of the European Chapter"
    },
    {
      "citation_id": "44",
      "title": "Conversation clique-based model for emotion recognition in conversation",
      "authors": [
        "Zhongquan Jian",
        "Jiajian Li",
        "Junfeng Yao",
        "Meihong Wang",
        "Qingqiang Wu"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "45",
      "title": "Automl-emo: Automatic knowledge selection using congruent effect for emotion identification in conversations",
      "authors": [
        "Dazhi Jiang",
        "Runguo Wei",
        "Jintao Wen",
        "Geng Tu",
        "Erik Cambria"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Real-time emotion recognition via attention gated hierarchical memory network",
      "authors": [
        "Wenxiang Jiao",
        "Michael Lyu",
        "Irwin King"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "47",
      "title": "Directed acyclic graphs with prototypical networks for few-shot emotion recognition in conversation",
      "authors": [
        "Yujin Kang",
        "Yoon-Sik Cho"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "48",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Kingma"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "49",
      "title": "From multilingual complexity to emotional clarity: Leveraging commonsense to unveil emotions in code-mixed dialogues",
      "authors": [
        "Shivani Kumar",
        "S Ramaneswaran",
        "Md Akhtar",
        "Tanmoy Chakraborty"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "50",
      "title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data",
      "authors": [
        "John Lafferty",
        "Andrew Mccallum",
        "Fernando Pereira"
      ],
      "year": "2001",
      "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML '01"
    },
    {
      "citation_id": "51",
      "title": "Different contextual window sizes based rnns for multimodal emotion detection in interactive conversations",
      "authors": [
        "Helang Lai",
        "Hongying Chen",
        "Shuangyan Wu"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "52",
      "title": "Graph based network with contextualized representations of turns in dialogue",
      "authors": [
        "Bongseok Lee",
        "Yong Choi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "53",
      "title": "Modeling mutual influence of interlocutor emotion states in dyadic spoken interactions",
      "authors": [
        "Chi-Chun Lee",
        "Carlos Busso",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2009",
      "venue": "Tenth Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "54",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "55",
      "title": "Joyful: Joint modality fusion and graph contrastive learning for multimoda emotion recognition",
      "authors": [
        "Dongyuan Li",
        "Yusong Wang",
        "Kotaro Funakoshi",
        "Manabu Okumura"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "56",
      "title": "Cfn-esa: A cross-modal fusion network with emotion-shift awareness for dialogue emotion recognition",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Yingjian Liu",
        "Zhigang Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "57",
      "title": "Ga2mif: Graph and attention based two-stage multisource information fusion for conversational emotion detection",
      "authors": [
        "Jiang Li",
        "Xiaoping Wang",
        "Guoqing Lv",
        "Zhigang Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "58",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Weiping Wang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "59",
      "title": "Hitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "60",
      "title": "Quantum-inspired neural network for conversational emotion recognition",
      "authors": [
        "Qiuchi Li",
        "Dimitris Gkoumas",
        "Alessandro Sordoni",
        "Jian-Yun Nie",
        "Massimo Melucci"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "61",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "Shimin Li",
        "Hang Yan",
        "Xipeng Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "62",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "63",
      "title": "EmoCaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Zaijing Li",
        "Fengxiao Tang",
        "Ming Zhao",
        "Yusen Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "64",
      "title": "S+PAGE: A speaker and position-aware graph neural network model for emotion recognition in conversation",
      "authors": [
        "Chen Liang",
        "Jing Xu",
        "Yangkun Lin",
        "Chong Yang",
        "Yongliang Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "65",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "66",
      "title": "Emotion interaction network for dialogue affective analysis",
      "authors": [
        "Yuchen Liu",
        "Jinming Zhao",
        "Jingwen Hu",
        "Ruichen Li",
        "Qin Jin",
        "Chu-Ren Dialogueein ; Nicoletta Calzolari",
        "Hansaem Huang",
        "James Kim",
        "Leo Pustejovsky",
        "Wanner",
        "Key-Sun",
        "Pum-Mo Choi",
        "Hsin-Hsi Ryu",
        "Lucia Chen",
        "Heng Donatelli",
        "Sadao Ji",
        "Patrizia Kurohashi",
        "Nianwen Paggio",
        "Seokhwan Xue",
        "Younggyun Kim",
        "Zhong Hahm",
        "Tony He",
        "Enrico Lee",
        "Santus"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "67",
      "title": "An iterative emotion interaction network for emotion recognition in conversations",
      "authors": [
        "Xin Lu",
        "Yanyan Zhao",
        "Yang Wu",
        "Yijian Tian",
        "Huipeng Chen",
        "Bing Qin"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "68",
      "title": "From extraction to generation: Multimodal emotion-cause pair generation in conversations",
      "authors": [
        "Heqing Ma",
        "Jianfei Yu",
        "Fanfan Wang",
        "Hanyu Cao",
        "Rui Xia"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "69",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "70",
      "title": "DialogueTRM: Exploring multi-modal emotional dynamics in a conversation",
      "authors": [
        "Yuzhao Mao",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "71",
      "title": "Interrater reliability: the kappa statistic",
      "authors": [
        "Mary Mchugh"
      ],
      "year": "2012",
      "venue": "Biochemia medica"
    },
    {
      "citation_id": "72",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "73",
      "title": "Basic Dimensions for a General Psychological Theory: Implications for Personality, Social, Environmental, and Developmental Studies",
      "authors": [
        "Albert Mehrabian"
      ],
      "year": "1980",
      "venue": "Oelgeschlager, Gunn & Hain"
    },
    {
      "citation_id": "74",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "Tomas Mikolov",
        "Kai Chen",
        "Greg Corrado",
        "Jeffrey Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "75",
      "title": "Advances in pretraining distributed word representations",
      "authors": [
        "Tomas Mikolov",
        "Edouard Grave",
        "Piotr Bojanowski",
        "Christian Puhrsch",
        "Armand Joulin"
      ],
      "year": "2018",
      "venue": "Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)"
    },
    {
      "citation_id": "76",
      "title": "Ethics sheet for automatic emotion recognition and sentiment analysis",
      "authors": [
        "M Saif",
        "Mohammad"
      ],
      "year": "2022",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "77",
      "title": "From utterance to text: The bias of language in speech and writing",
      "authors": [
        "David Olson"
      ],
      "year": "1977",
      "venue": "Harvard educational review"
    },
    {
      "citation_id": "78",
      "title": "Affective neuroscience: The foundations of human and animal emotions",
      "authors": [
        "Jaak Panksepp"
      ],
      "year": "2004",
      "venue": "Affective neuroscience: The foundations of human and animal emotions"
    },
    {
      "citation_id": "79",
      "title": "A self-attentive emotion recognition network",
      "authors": [
        "Harris Partaourides",
        "Kostantinos Papadamou",
        "Nicolas Kourtellis",
        "Ilias Leontiades",
        "Sotirios Chatzis"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "80",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methoailys in natural language processing (EMNLP)"
    },
    {
      "citation_id": "81",
      "title": "Context-dependent embedding utterance representations for emotion recognition in conversations",
      "authors": [
        "Patrícia Pereira",
        "Helena Moniz",
        "Isabel Dias",
        "Joao Carvalho"
      ],
      "year": "2023",
      "venue": "Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis"
    },
    {
      "citation_id": "82",
      "title": "Fuzzy fingerprinting transformer language-models for emotion recognition in conversations",
      "authors": [
        "Patrícia Pereira",
        "Rui Ribeiro",
        "Luísa Coheur",
        "Helena Moniz",
        "Joao Carvalho"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Fuzzy Systems"
    },
    {
      "citation_id": "83",
      "title": "Deep contextualized word representations",
      "authors": [
        "Matthew Peters",
        "Mark Neumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "84",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "85",
      "title": "A psychoevolutionary theory of emotions",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1982",
      "venue": "Social Science Information"
    },
    {
      "citation_id": "86",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Amir Zadeh",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "87",
      "title": "Beneath the tip of the iceberg: Current challenges and new directions in sentiment analysis research",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Rada Mihalcea"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "88",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder",
        "Gautam Naik",
        "Erik Cambria",
        "Rada Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "89",
      "title": "Recognizing emotion cause in conversations",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Deepanway Ghosal",
        "Rishabh Bhardwaj",
        "Samson Yu Bai Jian",
        "Pengfei Hong",
        "Romila Ghosh",
        "Abhinaba Roy",
        "Niyati Chhaya"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "90",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "91",
      "title": "Multi-party conversation modeling for emotion recognition",
      "authors": [
        "Xiaojun Quan",
        "Siyue Wu",
        "Junqing Chen",
        "Weizhou Shen",
        "Jianxing Yu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "92",
      "title": "Improving language understanding by generative pre-training",
      "authors": [
        "Alec Radford",
        "Karthik Narasimhan",
        "Tim Salimans",
        "Ilya Sutskever"
      ],
      "year": "2018",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "93",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI blog"
    },
    {
      "citation_id": "94",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "Fabien Ringeval",
        "Andreas Sonderegger",
        "Juergen Sauer",
        "Denis Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "95",
      "title": "Average jane, where art thou?-recent avenues in efficient machine learning under subjectivity uncertainty",
      "authors": [
        "Georgios Rizos",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems"
    },
    {
      "citation_id": "96",
      "title": "Learning representations by back-propagating errors",
      "authors": [
        "Geoffrey David E Rumelhart",
        "Ronald Hinton",
        "Williams"
      ],
      "year": "1986",
      "venue": "nature"
    },
    {
      "citation_id": "97",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "98",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "99",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "100",
      "title": "Summarize before aggregate: A global-to-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "Dongming Sheng",
        "Dong Wang",
        "Ying Shen",
        "Haitao Zheng",
        "Haozhuang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "101",
      "title": "Masked label prediction: Unified message passing model for semi-supervised classification",
      "authors": [
        "Yunsheng Shi",
        "Zhengjie Huang",
        "Shikun Feng",
        "Hui Zhong",
        "Wenjing Wang",
        "Yu Sun"
      ],
      "venue": "Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21"
    },
    {
      "citation_id": "102",
      "title": "Supervised prototypical contrastive learning for emotion recognition in conversation",
      "authors": [
        "Xiaohui Song",
        "Longtao Huang",
        "Hui Xue",
        "Songlin Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "103",
      "title": "Dynamic causal disentanglement model for dialogue emotion detection",
      "authors": [
        "Yuting Su",
        "Yichen Wei",
        "Weizhi Nie",
        "Sicheng Zhao",
        "Anan Liu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "104",
      "title": "A discourse-aware graph neural network for emotion recognition in multi-party conversation",
      "authors": [
        "Yang Sun",
        "Nan Yu",
        "Guohong Fu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "105",
      "title": "Directed acyclic graph neural networks",
      "authors": [
        "Veronika Thost",
        "Jie Chen"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph neural networks",
      "arxiv": "arXiv:2101.07965"
    },
    {
      "citation_id": "106",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "107",
      "title": "A training-free debiasing framework with counterfactual reasoning for conversational emotion detection",
      "authors": [
        "Geng Tu",
        "Ran Jing",
        "Bin Liang",
        "Min Yang",
        "Kam-Fai Wong",
        "Ruifeng Xu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "108",
      "title": "Sentiment-emotion-and context-guided knowledge selection framework for emotion recognition in conversations",
      "authors": [
        "Geng Tu",
        "Bin Liang",
        "Dazhi Jiang",
        "Ruifeng Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "109",
      "title": "Context or knowledge is not always necessary: A contrastive learning framework for emotion recognition in conversations",
      "authors": [
        "Geng Tu",
        "Bin Liang",
        "Ruibin Mao",
        "Min Yang",
        "Ruifeng Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "110",
      "title": "An empirical study on multiple knowledge from ChatGPT for emotion recognition in conversations",
      "authors": [
        "Geng Tu",
        "Bin Liang",
        "Bing Qin",
        "Kam-Fai Wong",
        "Ruifeng Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "111",
      "title": "Learning from disagreement: A survey",
      "authors": [
        "Alexandra Uma",
        "Tommaso Fornaciari",
        "Dirk Hovy",
        "Silviu Paun",
        "Barbara Plank",
        "Massimo Poesio"
      ],
      "year": "2021",
      "venue": "Journal of Artificial Intelligence Research"
    },
    {
      "citation_id": "112",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Ł Ukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "113",
      "title": "Generative emotion cause triplet extraction in conversations with commonsense knowledge",
      "authors": [
        "Fanfan Wang",
        "Jianfei Yu",
        "Rui Xia"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "114",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang",
        "Jun Ma",
        "Shaojun Wang",
        "Jing Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "115",
      "title": "Generalizing from a few examples: A survey on few-shot learning",
      "authors": [
        "Yaqing Wang",
        "Quanming Yao",
        "James Kwok",
        "Lionel Ni"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "116",
      "title": "Multi-scale receptive field graph model for emotion recognition in conversations",
      "authors": [
        "Jie Wei",
        "Guanyu Hu",
        "Anh Luu",
        "Xinyu Tuan",
        "Wenjing Yang",
        "Zhu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "117",
      "title": "Backpropagation through time: what it does and how to do it",
      "authors": [
        "Werbos Paul"
      ],
      "year": "1990",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "118",
      "title": "Attention is not not explanation",
      "authors": [
        "Sarah Wiegreffe",
        "Yuval Pinter"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "119",
      "title": "The feeling wheel: A tool for expanding awareness of emotions and increasing spontaneity and intimacy",
      "authors": [
        "Gloria Willcox"
      ],
      "year": "1982",
      "venue": "Transactional Analysis Journal"
    },
    {
      "citation_id": "120",
      "title": "Contextsensitive multimodal emotion recognition from speech and facial expression using bidirectional lstm modeling",
      "authors": [
        "Martin Wöllmer",
        "Angeliki Metallinou",
        "Florian Eyben",
        "Björn Schuller",
        "Shrikanth Narayanan"
      ],
      "year": "2010",
      "venue": "Proc. INTERSPEECH 2010"
    },
    {
      "citation_id": "121",
      "title": "Dialoguepcn: Perception and cognition network for emotion recognition in conversations",
      "authors": [
        "Xiaolong Wu",
        "Chang Feng",
        "Mingxing Xu",
        "Thomas Zheng",
        "Askar Hamdulla"
      ],
      "year": "2023",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "122",
      "title": "A comprehensive survey on graph neural networks",
      "authors": [
        "Zonghan Wu",
        "Shirui Pan",
        "Fengwen Chen",
        "Guodong Long",
        "Chengqi Zhang",
        "S Yu"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "123",
      "title": "Knowledge-interactive network with sentiment polarity intensity-aware multi-task learning for emotion recognition in conversations",
      "authors": [
        "Yunhe Xie",
        "Kailai Yang",
        "Cheng-Jie Sun",
        "Bingquan Liu",
        "Zhenzhou Ji"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "124",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "Songlong Xing",
        "Sijie Mai",
        "Haifeng Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "125",
      "title": "Mcm-csd: Multi-granularity context modeling with contrastive speaker detection for emotion recognition in real-time conversation",
      "authors": [
        "Yuan Xu",
        "Meng Yang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "126",
      "title": "Cluster-level contrastive learning for emotion recognition in conversations",
      "authors": [
        "Kailai Yang",
        "Tianlin Zhang",
        "Hassan Alhuzali",
        "Sophia Ananiadou"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "127",
      "title": "Disentangled variational autoencoder for emotion recognition in conversations",
      "authors": [
        "Kailai Yang",
        "Tianlin Zhang",
        "Sophia Ananiadou"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "128",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "Lin Yang",
        "Yi Shen",
        "Yue Mao",
        "Longjun Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "129",
      "title": "Emotion recognition in conversation based on a dynamic complementary graph convolutional network",
      "authors": [
        "Zhenyu Yang",
        "Xiaoyang Li",
        "Yuhu Cheng",
        "Tong Zhang",
        "Xuesong Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "130",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Russ Salakhutdinov",
        "Quoc V Le"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "131",
      "title": "Speaker-centric multimodal fusion networks for emotion recognition in conversations",
      "authors": [
        "Biyun Yao",
        "Wuzhen Shi"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "132",
      "title": "Emotion-anchored contrastive learning framework for emotion recognition in conversation",
      "authors": [
        "Fangxu Yu",
        "Junjie Guo",
        "Zhen Wu",
        "Xinyu Dai"
      ],
      "year": "2024",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2024"
    },
    {
      "citation_id": "133",
      "title": "TelME: Teacher-leading multimodal fusion network for emotion recognition in conversation",
      "authors": [
        "Taeyang Yun",
        "Hyunkuk Lim",
        "Jeonghwan Lee",
        "Min Song"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "134",
      "title": "Categorical versus dimensional models of affect: a seminar on the theories of Panksepp and Russell",
      "authors": [
        "Peter Zachar",
        "Ralph Ellis"
      ],
      "year": "2012",
      "venue": "Categorical versus dimensional models of affect: a seminar on the theories of Panksepp and Russell"
    },
    {
      "citation_id": "135",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "M Sayyed",
        "Jinho D Zahiri",
        "Choi"
      ],
      "year": "2018",
      "venue": "In 1st Workshop on Affective Content Analysis"
    },
    {
      "citation_id": "136",
      "title": "Esihgnn: Event-state interactions infused heterogeneous graph neural network for conversational emotion recognition",
      "authors": [
        "Xupeng Zha",
        "Huan Zhao",
        "Zixing Zhang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "137",
      "title": "DualGATs: Dual graph attention networks for emotion recognition in conversations",
      "authors": [
        "Duzhen Zhang",
        "Feilong Chen",
        "Xiuyi Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "138",
      "title": "Knowledge aware emotion recognition in textual conversations via multi-task incremental transformer",
      "authors": [
        "Duzhen Zhang",
        "Xiuyi Chen",
        "Shuang Xu",
        "Bo Xu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "139",
      "title": "Emotion recognition in conversation from variable-length context",
      "authors": [
        "Mian Zhang",
        "Xiabing Zhou",
        "Wenliang Chen",
        "Min Zhang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "140",
      "title": "A multi-level alignment and cross-modal unified semantic graph refinement network for conversational emotion recognition",
      "authors": [
        "Xiaoheng Zhang",
        "Weigang Cui",
        "Bin Hu",
        "Yang Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "141",
      "title": "A cross-modality context fusion and semantic refinement network for emotion recognition in conversation",
      "authors": [
        "Xiaoheng Zhang",
        "Yang Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "142",
      "title": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "authors": [
        "Yazhou Zhang",
        "Mengyao Wang",
        "Prayag Tiwari",
        "Qiuchi Li",
        "Benyou Wang",
        "Jing Qin"
      ],
      "year": "2023",
      "venue": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "arxiv": "arXiv:2310.11374"
    },
    {
      "citation_id": "143",
      "title": "Mutual conversational detachment network for emotion recognition in multi-party conversations",
      "authors": [
        "Weixiang Zhao",
        "Yanyan Zhao",
        "Bing Qin",
        "Chu-Ren Mucdn ; Nicoletta Calzolari",
        "Hansaem Huang",
        "James Kim",
        "Leo Pustejovsky",
        "Wanner",
        "Key-Sun",
        "Pum-Mo Choi",
        "Hsin-Hsi Ryu",
        "Lucia Chen",
        "Heng Donatelli",
        "Sadao Ji",
        "Patrizia Kurohashi",
        "Nianwen Paggio",
        "Seokhwan Xue",
        "Younggyun Kim",
        "Zhong Hahm",
        "Tony He",
        "Enrico Lee",
        "Santus"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "144",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "Peixiang Zhong",
        "Di Wang",
        "Chunyan Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "145",
      "title": "Graph neural networks: A review of methods and applications",
      "authors": [
        "Jie Zhou",
        "Ganqu Cui",
        "Shengding Hu",
        "Zhengyan Zhang",
        "Cheng Yang",
        "Zhiyuan Liu",
        "Lifeng Wang",
        "Changcheng Li",
        "Maosong Sun"
      ],
      "year": "2020",
      "venue": "AI Open"
    },
    {
      "citation_id": "146",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    }
  ]
}