{
  "paper_id": "2510.05749v1",
  "title": "Msf-Ser: Enriching Acoustic Modeling With Multi-Granularity Semantics For Speech Emotion Recognition",
  "published": "2025-10-07T10:11:50Z",
  "authors": [
    "Haoxun Li",
    "Yuqing Sun",
    "Hanlei Shi",
    "Yu Liu",
    "Leyuan Qu",
    "Taihao Li"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Dimensional Emotion Regression",
    "Multi-Granularity Semantics",
    "Prosodic Emphasis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Continuous dimensional speech emotion recognition captures affective variation along valence, arousal, and dominance, providing finer-grained representations than categorical approaches. Yet most multimodal methods rely solely on global transcripts, leading to two limitations: (1) all words are treated equally, overlooking that emphasis on different parts of a sentence can shift emotional meaning; (2) only surface lexical content is represented, lacking higher-level interpretive cues. To overcome these issues, we propose MSF-SER (Multi-granularity Semantic Fusion for Speech Emotion Recognition), which augments acoustic features with three complementary levels of textual semantics-Local Emphasized Semantics (LES), Global Semantics (GS), and Extended Semantics (ES). These are integrated via an intra-modal gated fusion and a cross-modal FiLM-modulated lightweight Mixture-of-Experts (FM-MOE). Experiments on MSP-Podcast and IEMOCAP show that MSF-SER consistently improves dimensional prediction, demonstrating the effectiveness of enriched semantic fusion for SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech Emotion Recognition (SER) has attracted increasing attention for its applications in human-computer interaction, mental health, and multimedia retrieval. Traditional approaches often rely on discrete emotion categories, which cannot fully capture the richness of human emotions. Recently, emotion dimensions have become popular and widely adopted, since they are more capable of capturing subtle variations and representing complex emotional states  [1] .\n\nEarly dimensional SER systems largely relied on handcrafted acoustic and prosodic features combined with conventional machine learning models  [2] . Recent advances include the TF-Mamba architecture proposed by Zhao et al.  [3]  for capturing temporal and frequency patterns, and efficient fine-tuning strategies for large SER models introduced by Aneesha et al  [4] .\n\nDespite progress in speech-only approaches, semantic representations derived from acoustic signals remain limited, as they are easily affected by noise, speaker variability, and pronunciation differences  [5] . Text provides a more stable source of semantics, and pre-trained text models are trained on much larger corpora than ⋆ Corresponding authors. Haoxun Li, et al. Copyright 2026 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, including reprinting/republishing, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work. DOI will be added upon IEEE Xplore publication. speech models, yielding richer and more reliable representations  [6] . This motivates the incorporation of textual semantics to complement acoustic features. However, most existing multimodal studies still rely solely on global transcripts, which introduces two major issues.\n\n(1) Global transcripts treat all words equally, overlooking the fact that not every word contributes to emotion and that emphasis on different parts of a sentence can shift its affective meaning. For example, in the sentence \"I really didn't mean that\", placing emphasis on \"really\" conveys a sense of sincerity or insistence, while stressing \"didn't\" instead conveys denial or even defensiveness. Such differences can drastically alter the perceived emotion. This motivates the introduction of locally emphasized semantics, which are derived from the prosodically most prominent segments of speech. These emphasized regions often convey the speaker's key semantic intent and exhibit stronger correlations with emotional expression.\n\n(2) Global transcripts only capture the explicit lexical content while lacking higher-level interpretive information, including but not limited to explanatory cues, situational factors, and paralinguistic attributes. Such information is essential for emotion recognition, since affective meaning is shaped not only by lexical content but also by inferred intent, communicative context, and speakerspecific characteristics. To address this gap, we incorporate priorknowledge semantics, where external audio-language models provide extended and multi-dimensional interpretations of the audio, enhancing textual semantics with information inferred from audio. Along with local emphasized semantics and global contextual semantics, these prior-knowledge representations constitute our multigranular semantic design.\n\nTo fully exploit enriched semantics, MSF-SER employs an intramodal gated fusion to adaptively integrate different semantic levels, and a FiLM-modulated lightweight Mixture-of-Experts (FM-MOE) for cross-modal integration. Together, these components allow semantic cues to effectively guide acoustic representation learning, leading to consistent improvements in dimensional emotion recognition.\n\nThe main contributions of our work are summarized as follows:\n\n• For speech emotion recognition, we supplement acoustic modeling with richer semantic information by extracting features at three complementary granularities.\n\n• We employ FM-MOE for cross-modal fusion, while an intramodal gating mechanism adaptively integrates global and local semantics to enhance acoustic feature learning.\n\n• MSF-SER achieves top-performing results across MSP-Podcast and IEMOCAP, thereby demonstrating the effectiveness and generalizability of our method. Our proposed framework adopts acoustic features as the backbone, while three levels of semantic features are incrementally incorporated as auxiliary signals. An intra-modal gating mechanism fuses global and local semantics, and cross-modal integration between acoustic and semantic representations is achieved through FM-MOE.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview",
      "text": "Our baseline employs a fine-tuned WavLM-large architecture, where raw audio is first encoded into frame-level acoustic representations by the WavLM encoder. The outputs are then aggregated using attentive statistics pooling to compute weighted mean and standard deviation, followed by a fully connected layer that predicts valence, arousal, and dominance. On this foundation, we introduce multi-granularity semantic features: LES, GS, and ES, as illustrated in Fig.  1 . These features interact with acoustic features through a two-stage fusion framework. For intra-modal fusion, we design a gated mechanism to adaptively integrate LES and GS, enhancing model sensitivity and stability. For inter-modal fusion, we propose FM-MOE. Specifically, Featurewise Linear Modulation (FiLM) applies dimension-wise scaling and shifting to acoustic representations conditioned on semantic features, ensuring fine-grained cross-modal interaction. Within FM-MOE, FiLM-modulated experts specialize in different emotional dimensions, and their outputs are adaptively combined through different routing weights. This design enables semantic cues to effectively guide acoustic learning while capturing dimension-specific dependencies.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Granularity Semantic Feature Extraction",
      "text": "Local Emphasized Semantics (LES). Prior work indicates that prosodic prominence mainly arises from pitch, energy, and duration  [7] . However, in noisy conditions, emphasis detection models trained on clean data, such as EmphaClass  [8] , often degrade. To make emphasis modeling more robust, we design a Local Emphasis Modeling Framework (LEMF), which identifies prosodically prominent words and transforms them into LES. LES highlight the most salient parts of speech that often carry the speaker's key intent and emotional salience. The specific implementation of LEMF is as follows:\n\nGiven a sentence u = {w1, . . . , wN } with audio signal x(t), where t denotes time, and its MFA-based TextGrid alignment  [9] , we extract three prosodic features for each word w:\n\n• Pitch (fpitch(w)): log-F0 via the PyWorld Harvest algorithm; • Energy (fenergy(w)): L2 norm of Short-Time Fourier Transform (STFT) using a 20ms window; • Duration (fduration(w)): mean phoneme duration in w.\n\nTo ensure comparability of features within a sentence, we compute sentence-level statistics (mean µi, standard deviation σi) for pitch fpitch(w) and energy fenergy(w). Each word-level feature is then normalized using Z-score standardization, as shown in Equation  1 .\n\nwhere fi(w) denotes the raw feature of w in the i-th prosodic dimension. Specifically, pitch corresponds to the maximum framelevel F0, energy to the mean frame energy, and duration to average phoneme length.\n\nSubsequently, we perform a weighted fusion of the three prosodic features to obtain the emphasis score s(w) for each word w as shown in Equation  2 :\n\nwhere (α, β, γ) = (1.0, 1.2, 0.8). The word with the highest score, along with its two adjacent words, forms the emphasis segment. This segment is then encoded by RoBERTa-Large  [10]  to derive LES features.\n\nGlobal Semantics (GS). We derive sentence-level semantics by transcribing speech with Whisper-ASR  [11]  and encoding the text using RoBERTa-Large.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Extended Semantics (Es).",
      "text": "Leveraging recent advances in largescale audio understanding, we employ Kimi-Audio  [12]  to generate six categories of extended speech and emotion-related information, as shown in Table  1 . These outputs are concatenated into descriptive text and encoded with RoBERTa-Large, yielding enriched semantic representations. For example, an extended semantic description may state: \"This is a male speaker, expressing frustrated (categorized as Angry), in a confrontation or argument in a public setting. The speaker is expressing anger due to a perceived lack of understanding or compliance from the other party. The speech is characterized by rising intonation, assertive tone, and a sense of urgency.\"\n\nTable  1 : Extended semantics information categories generated by Kimi-Audio.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Category Description",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Free Emotion Label Open-Domain Emotion Prediction Constrained Emotion Label",
      "text": "Mapping to dataset labels Emotion explanation\n\nReason for emotion prediction Scenario Potential situational context Paralinguistics Paralinguistic information Gender Gender information",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Intra-Modal Gated Fusion",
      "text": "Locally emphasized semantics capture salient emotional cues but lack holistic coverage, whereas global semantics provide comprehensive context but may introduce redundancy and noise. To integrate their complementary strengths, we employ a gated fusion mechanism, as shown in Equation  3 .\n\nwhere hsem is the fused representation, hlocal and hglobal denote local and global semantics, respectively, g is the learned gating weight, σ the Sigmoid activation, and W , b are trainable parameters.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fm-Moe",
      "text": "In dimensional emotion recognition, acoustic features remain the primary source of information, while text semantics provide complementary cues. To facilitate fine-grained cross-modal interaction, we employ a Feature-wise Linear Modulation (FiLM) layer  [13] , which applies dimension-wise scaling and shifting to acoustic representations conditioned on semantic features, as shown in Equation  5 . This design prioritizes speech as the dominant modality while leveraging semantics as auxiliary guidance, thereby mitigating the impact of semantic noise and improving fusion effectiveness.\n\nwhere haudio denotes the semantically modulated acoustic feature, haudio represents the input acoustic feature, γ and β are the scaling and bias parameters learned through the Multi-Layer Perceptron (MLP), and hsem is the semantic feature vector.\n\nTo capture the distinct cross-modal dependencies of different emotional attributes, we propose a lightweight Mixture-of-Experts  [14]  module following the shared WavLM backbone. It consists of three experts: an acoustic-only expert (Expert A), a textual semantic expert (Expert B) incorporating fused local and global semantics, and an extended semantic expert (Expert C) that integrates additional prior knowledge. Subsequently, for each emotional dimension d ∈ {V, A, D}, the output is a weighted combination of the experts, as shown in Equation 7.\n\nwhere f k denotes the k-th expert and π\n\nrepresents the learnable routing weight. The routing weights allow different emotional dimensions to adaptively emphasize relevant experts (e.g., Valence and Arousal typically rely more on Experts A and B, while Dominance emphasizes Experts C).",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "We conducted experiments on MSP-Podcast v1.12 and IEMOCAP. MSP-Podcast v1.12 is a large-scale corpus of spontaneous podcast speech with 84,260 training and 31,961 development utterances, annotated with ten categorical labels and dimensional ratings (arousal, valence, dominance, 1-7 scale), and officially split into training, development, and three test sets. IEMOCAP contains 10,039 utterances from ten actors across five sessions of scripted and spontaneous dialogues, annotated with categorical emotions and dimensional ratings on a 1-5 scale.\n\nWe train with AdamW (1 × 10 -5 learning rate), batch size 32, and gradient accumulation of 4. Each emotional dimension is predicted by an independent two-layer regression head with dropout 0.5 and layer normalization. For features, we use WavLM-Large as the acoustic encoder (hidden size 1024), freezing the CNN front-end and fine-tuning the Transformer layers, and RoBERTa-Large (1024 hidden size) for textual semantics. Training is performed on 8 RTX 4090 GPUs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Performance On Emotional Attribute Prediction",
      "text": "Our evaluation results on the MSP-Podcast dataset are reported on the development set, as shown in Table  2 . Model performance is measured using the Concordance Correlation Coefficient (CCC), which jointly accounts for both correlation and mean squared difference between predictions and ground truth, making it the standard metric for dimensional emotion recognition.\n\nWe adopt a baseline architecture built on WavLM-Large as the speech encoder, followed by an attentive statistics pooling layer and fully connected layers to perform regression prediction. The results demonstrate that incorporating semantic information at three different granularities consistently improves recognition performance. Specifically, introducing GS and LES yields greater gains for arousal and valence prediction, while ES mainly enhances dominance regression. Regarding inter-modal fusion strategies, both concatenation and attention degrade performance. We attribute this to the fact that, in speech-dominant tasks, these methods allow noisy textual semantics to interfere with the intrinsic structure of acoustic representations. In contrast, FiLM-based fusion effectively mitigates such noise and leads to more robust predictions. Moreover, applying intra-modal fusion on GS and LES further improves results compared to using either individually, with the gated approach outperforming the attention-based method. Finally, when all three levels of semantic information are jointly introduced and integrated with FM-MOE, our model achieves the best overall performance.\n\nWe further evaluated our model on the IEMOCAP dataset to validate its effectiveness. We adopt five-fold cross-validation, using four sessions for training and the remaining session for testing     [22]  0.625 0.675 0.599 0.633 DEER  [17]  0.625 0.711 0.548 0.628 PCM-le-noNorm  [18]  0.630 0.717 0.555 0.634 MSF-SER 0.632 0.680 0.601 0.638 in each fold, ensuring strict speaker independence. As shown in Table  3 , we compared our approach with multiple models. MSF-SER achieves the highest CCC scores on valence, dominance and the overall average, demonstrating its advantage in capturing subtle emotional cues across dimensions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Lemf",
      "text": "LEMF is designed to improve emphasis detection in spontaneous speech, where noise and speaking style variations often degrade performance. By leveraging prosodic features such as pitch, energy, and duration, LEMF can more accurately capture emphasized regions, yielding higher accuracy and robustness. As illustrated in Fig.  2  ,this feature-based approach not only enhances adaptability in spontaneous conditions but also provides reliable local semantic cues for downstream emotion recognition tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed MSF-SER, a multi-granularity semantic fusion framework for speech emotion recognition. By introducing LES, GS, and ES, and integrating them through gated fusion and FM-MOE, our approach enhances acoustic modeling with enriched textual cues. Experiments on MSP-Podcast and IEMOCAP show consistent improvements over strong baselines, establishing MSF-SER as a top-performing system. In future work, we plan to extend this design to cross-lingual scenarios and explore its integration with additional modalities such as visual signals.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Our proposed framework adopts acoustic features as the backbone, while three levels of semantic features are incrementally incorpo-",
      "page": 2
    },
    {
      "caption": "Figure 1: These features in-",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of emphasis detection by LEMF. The top fig-",
      "page": 4
    },
    {
      "caption": "Figure 2: ,this feature-based approach not only enhances adaptability in spon-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Output"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Study of dense network approaches for speech emotion recognition",
      "authors": [
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "3",
      "title": "Temporal-frequency state space duality: An efficient paradigm for speech emotion recognition",
      "authors": [
        "Jiaqi Zhao",
        "Fei Wang",
        "Kun Li"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Efficient finetuning for dimensional speech emotion recognition in the age of transformers",
      "authors": [
        "Aneesha Sampath",
        "James Tavernor",
        "Emily Provost"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "Learning finegrained cross modality excitement for speech emotion recognition",
      "authors": [
        "Hang Li",
        "Wenbiao Ding",
        "Zhongqin Wu"
      ],
      "year": "2020",
      "venue": "Learning finegrained cross modality excitement for speech emotion recognition",
      "arxiv": "arXiv:2010.12733"
    },
    {
      "citation_id": "6",
      "title": "What do self-supervised speech models know about words?",
      "authors": [
        "Ankita Pasad",
        "Chung-Ming Chien",
        "Shane Settle"
      ],
      "year": "2024",
      "venue": "What do self-supervised speech models know about words?"
    },
    {
      "citation_id": "7",
      "title": "Some acoustic correlates of word stress in american english",
      "authors": [
        "Philip Lieberman"
      ],
      "year": "1960",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "8",
      "title": "Emphassess: a prosodic benchmark on assessing emphasis transfer in speech-to-speech models",
      "authors": [
        "Maureen De Seyssel",
        "Antony D' Avirro",
        "Adina Williams"
      ],
      "year": "2023",
      "venue": "Emphassess: a prosodic benchmark on assessing emphasis transfer in speech-to-speech models",
      "arxiv": "arXiv:2312.14069"
    },
    {
      "citation_id": "9",
      "title": "Multi-matrix factorization attention",
      "authors": [
        "Jingcheng Hu",
        "Houyi Li",
        "Yinmin Zhang"
      ],
      "year": "2024",
      "venue": "Multi-matrix factorization attention",
      "arxiv": "arXiv:2412.19255"
    },
    {
      "citation_id": "10",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "11",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "12",
      "title": "Kimi-audio technical report",
      "authors": [
        "Ding Ding",
        "Zeqian Ju",
        "Yichong Leng"
      ],
      "year": "2025",
      "venue": "Kimi-audio technical report",
      "arxiv": "arXiv:2504.18425"
    },
    {
      "citation_id": "13",
      "title": "Film: Visual reasoning with a general conditioning layer",
      "authors": [
        "Ethan Perez",
        "Florian Strub",
        "Harm De Vries"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "14",
      "title": "Adaptive mixtures of local experts",
      "authors": [
        "Robert Jacobs",
        "Michael Jordan",
        "Steven Nowlan"
      ],
      "year": "1991",
      "venue": "Neural computation"
    },
    {
      "citation_id": "15",
      "title": "Bridging emotions across languages: Low rank adaptation for multilingual speech emotion recognition",
      "authors": [
        "Lucas Goncalves",
        "Donita Robinson",
        "Elizabeth Richerson"
      ],
      "year": "2024",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Modality-agnostic multimodal emotion recognition using a contrastive masked autoencoder",
      "authors": [
        "Georgios Chochlakis",
        "Turab Iqbal",
        "Hyun Woo",
        "Kang"
      ],
      "year": "2025",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Estimating the uncertainty in emotion attributes using deep evidential regression",
      "authors": [
        "Wen Wu",
        "Chao Zhang",
        "Philip Woodland"
      ],
      "year": "2023",
      "venue": "Estimating the uncertainty in emotion attributes using deep evidential regression",
      "arxiv": "arXiv:2306.06760"
    },
    {
      "citation_id": "18",
      "title": "Pitch contour model (pcm) with transformer cross-attention for speech emotion recognition",
      "authors": [
        "Minji Ryu",
        "Ji-Hyeon Hur",
        "Sung Kim"
      ],
      "year": "2025",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "19",
      "title": "Developing a high-performance framework for speech emotion recognition in naturalistic conditions challenge for emotional attribute prediction",
      "authors": [
        "Tiantian Thanathai Lertpetchpun",
        "Dani Feng",
        "Byrd"
      ],
      "year": "2025",
      "venue": "Developing a high-performance framework for speech emotion recognition in naturalistic conditions challenge for emotional attribute prediction",
      "arxiv": "arXiv:2506.10930"
    },
    {
      "citation_id": "20",
      "title": "The interspeech 2025 challenge on speech emotion recognition in naturalistic conditions",
      "authors": [
        "Lucas Reddy Naini",
        "Goncalves",
        "Ali N Salman"
      ],
      "year": "2025",
      "venue": "The interspeech 2025 challenge on speech emotion recognition in naturalistic conditions"
    },
    {
      "citation_id": "21",
      "title": "Unsupervised domain adaptation for speech emotion recognition using k-nearest neighbors voice conversion",
      "authors": [
        "Pravin Mote",
        "Berrak Sisman",
        "Carlos Busso"
      ],
      "year": "2024",
      "venue": "Proceedings of IN-TERSPEECH"
    },
    {
      "citation_id": "22",
      "title": "Generalization of self-supervised learning-based representations for cross-domain speech emotion recognition",
      "authors": [
        "Abinay Reddy Naini",
        "Mary Kohler",
        "Elizabeth Richerson"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}