{
  "paper_id": "2005.07777v1",
  "title": "Concealnet: An End-To-End Neural Network For Packet Loss Concealment In Deep Speech Emotion Recognition",
  "published": "2020-05-15T20:43:02Z",
  "authors": [
    "Mostafa M. Mohamed",
    "Björn W. Schuller"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Frame Loss",
    "Packet Loss Concealment",
    "End-to-End Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Packet loss is a common problem in data transmission, including speech data transmission. This may affect a wide range of applications that stream audio data, like streaming applications or speech emotion recognition (SER). Packet Loss Concealment (PLC) is any technique of facing packet loss. Simple PLC baselines are 0-substitution or linear interpolation. In this paper, we present a concealment wrapper, which can be used with stacked recurrent neural cells. The concealment cell can provide a recurrent neural network (ConcealNet), that performs real-time step-wise end-to-end PLC at inference time. Additionally, extending this with an end-to-end emotion prediction neural network provides a network that performs SER from audio with lost frames, end-to-end. The proposed model is compared against the fore-mentioned baselines. Additionally, a bidirectional variant with better performance is utilised. For evaluation, we chose the public RECOLA dataset given its long audio tracks with continuous emotion labels. ConcealNet is evaluated on the reconstruction of the audio and the quality of corresponding emotions predicted after that. The proposed ConcealNet model has shown considerable improvement, for both audio reconstruction and the corresponding emotion prediction, in environments that do not have losses with long duration, even when the losses occur frequently.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Packet Loss Concealment (PLC) is any technique that attempts to handle the effects of packet loss or overly delayed packets. This is a common problem in speech transmission using VoIP  [1] . This problem can affect the performance of many speech processing systems that assume a complete speech signal is transmitted, including Speech Emotions Recognition (SER).There has been a variety of classical techniques that attempts to solve the packet loss problem, for example using Hidden Markov Models (HMM)  [2]  and Linear Predictive Coding (LPC)  [3] . There are also encoding-based techniques  [4] . However, in the era of deep learning and the rise of a variety of generative networks, like sequential generative Recurrent Neural Networks  [5]  and Generative Adversarial Networks (GAN)  [6] , generating data in place of the lost packets is a promising avenue for advanced concealment techniques. There exist studies  [7]  that attempt to solve packet loss in the context of Automatic Speech Recognition (ASR). However, to the authors' best knowledge, there are no studies addressing PLC in the context of Speech Emotion Recognition. In an earlier work, the authors have investigated techniques how to train SER end-to-end models to be robust in the presence of frame-loss  [8] . However, we attempt here to address PLC directly to address this issue. Furthermore, this problem can happen on a variety of devices including mobile devices  [9] . Providing a neural network that can perform PLC end-to-end would be favourable, because it is easier to embed it into different applications without the need for extra processing. More importantly, there is a rise nowadays of hardware optimised for neural networks processing  [10] , hence having an end-to-end PLC solution would be the most suitable solution for future hardware. The contributions of this paper are providing such an end-to-end PLC neural network (we call it ConcealNet) and examining the effects of using it on SER in lossy environments.\n\nThe paper is divided as follows: in Section 2, we will review some existing techniques that are also used for PLC with different models or with different settings. In Section 3, the main approach will be presented. The experiments and evaluations are discussed in Section 4. Finally, in Section 5, the summary and the conclusion of the paper are discussed. A PLC approach is proposed in  [11] , which relies on features representing speech data. The concealment is done on feature level and then decoded, rather than executing it on the actual speech directly. The approach was realised in the context of enhancing Automatic Speech Recognition (ASR). Based on  [11] ,  [7]  implemented a PLC algorithm that operates directly on the speech data using LSTM-based neural networks. They also applied it on ASR, while evaluating on the TIMIT dataset  [12] . The main advantage of both approaches is that, they can be applied in a frame-by-frame fashion, which is suitable for real-time application on losses of small packets. Additionally, they have the potential to be extended to a neural-based end-to-end PLC. GAN-based approaches are utilised in  [13, 14] , where the generator adapts an architecture similar to an auto-encoder. The model uses audio of long segments (like 3 200 ms) to make predictions, which is longer than a typical packet size in VoIP being around 10-20 ms  [1, 11] . Such a setup is most effective for offline processing and for long losses. In  [15] , there is an approach facing PLC not in the context of speech, but rather the transmitted data from pose tracking sensors. They authors also chose LSTM-based RNNs, while having a two-state Markov Chain for packet loss injection.  [16]  are considering Cartesian Genetic Programming  [17]  for signal reconstruction, in some abstract setting. Another approach like  [18]  attempts to reconstruct STFT signals, under a variety of deformations like destructive interference and packet-loss, however, it is not directly addressing PLC in particular.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Approach",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Recurrent Generative Modelling",
      "text": "Given an input sequence x1, • • • , xT , where st-1 denotes the previous state of xt representing the whole preceding sequence, then, a recurrent neural cell is an operation R that computes an output y t and a next state st  [19] . Two effective and commonly used recurrent cells are gated recurrent cells like LSTM  [20]  and GRU  [21] . Additionally, R here could also refer to a stack of recurrent cells, such that at each time step, each cell takes input from the output of the preceding cell at the same time step.\n\nAs shown in  [5] , generative Recurrent Neural Networks (RNNs) can be used to generate data by training the cells to predict elements of sequences using the preceding elements. Inspired by this, we train a similar approach as a regression   1 : Generative RNN architecture that predicts a frame using the preceding frames.\n\nFigure  2 : Markov Chain M(pL, pN) that samples a binary sequence, that can be used as a mask for loss or non-loss.\n\ntask instead of classification, to enable an RNN G to generate audio segments. G will be later used to conceal packet loss by generating audio segments for the lost packets.\n\nTraining. We train the generative RNN G, using the input speech segments x1, x2,\n\n, by concatenating the segments and comparing that to the concatenation of the sequence x2, • • • , xT -1, xT as the corresponding ground truth prediction. This is optimising the loss function\n\n). The loss is 1 -ρc(x, y), where ρc(x, y) is the concordance correlation coefficient (CCC) which measures data reproducibility  [22] , given by:\n\nwhere µx, µy are the means, σ 2 x , σ 2 y are the variances, and\n\nxy is the covariance of x and y.\n\nStressed training. The models might need to generate several consecutive frames in environments of severe packet losses. We can enhance this by adapting a stressed training scheme. We can composite G three times to get y (1) = G(x) as before, in addition to y (2) = G(G(x)) and y (3) = G(G(G(x))).\n\nConsequently, we optimise:\n\nThis can be generalised for a deeper composition. However, it gets more expensive to train.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Processing.",
      "text": "The model processes speech with a sliding window of segments of duration 6.25 ms (corresponding to an array of length 100, in case of a 16 kHz sample rate). This is close to a typical packet duration of 10-20 ms  [1, 11] . This length compromises between two issues. The first is the speed of inference, the second is the number of trainable parameters and generalisation ability. Smaller segment duration needs much longer time for training and inference, because it processes a very long sequence linearly without parallelisation  [23] . Bigger segments oblige the model to have more parameters, which is more difficult to train. Furthermore, during training, the tracks are segmented into segments of length 20 s to allow fast training.\n\nHyperparameters. The hyperparameters space is explored using BOHB  [24] , a state-of-the-art tool for hyperparameter optimisation. The best architecture it discovered is shown in Table  1 . The training is performed applying an Adam optimiser  [25]  using the stressed training loss in Equation 2. To speed up training, we first train for 80 epochs without the stress training. Then, we continue with the stress training for 40 epochs. For stress training, we use a learning rate α = 0.003, otherwise α = 0.0045. A learning-rate decay 0.0015 is used, in addition to dropout layers  [26]  of a dropout rate 0.5 that are entered after each LSTM layer during training, to reduce overfitting.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Concealnet",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Recurrent Concealment Cells",
      "text": "Given is an input sequence x1  that estimates the next element of the input sequence, namely y t = xt+1. We introduce a wrapper concealment recurrent cell FR that uses R to fix x. The input of FR is (xt, Mt), and its previous state zt-1 = (xt, st-1). One step of the concealment cell is executed according to the equations:\n\nThe initial state is given by z0 = (p, s0), where p is a default-response vector, in case the initial frames were lost. In our implementation, we use p = 0.\n\nThe value of y t will be the same as xt if Mt = 1 (non-lost frame). Otherwise, it will be the generated value xt, which is the predicted element according to the cell R. After that, the predicted next element xt+1 is computed using R, in addition to the state st that will be used in the next time step. A visual demonstration of how this cell operates is depicted in Figure  1 .\n\nThis cell behaves similar to the PLC algorithm in  [7] . However, formulating it as a neural operation allows the models to perform PLC end-to-end from corrupt raw audio to concealed raw audio. This can be embedded to make end-to-end inference, from lossy speech to emotions directly (or other tasks).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "End-To-End Plc Inference",
      "text": "Putting together the aforementioned components, we extract the recurrent cells and Fully Connected layers  [27]  from the generative RNN G trained in Section 3.1. The extracted cells are then stacked to form one cell R, and then we wrap it using the concealment wrappers introduced in Subsection 3.2.1. Consequently, we construct an end-to-end PLC inference RNN (which we call ConcealNet) with the input sequence x1•••T , and a corresponding loss mask M1•••T to predict a fixed signal x1•••T . This resulting fixed signal is after applying PLC, where lost segments are concealed and non-lost segments are copied.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Bidirectional Concealment.",
      "text": "Bidirectional RNNs have shown promising improvements in ASR  [28] , which motivates us to introduce a bidirectional variant assuming non-causal processing is an option, e. g. , by a small buffer or in post-hoc application. If we train a backwards generative network and use the same architecture of ConcealNet on it (by reversing the input and output sequences), the results of those two networks (forward and backward) can be merged by averaging both to obtain a simple bidirectional variant of ConcealNet. This variant tends to have better performance generally. However, its main disadvantage is the inability to be used in real-time settings, because it assumes the knowledge about future context.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "The dataset that is used in the experiment is the RECOLA dataset  [29] . The dataset consists of 16 training tracks and 15 validation tracks. Each track consists of 5 minutes of audio  [29] , we downsampled them to 16 kHz. Each track is labelled with emotions across time and the labels were collected on a frequency of 25 Hz. However, we reduced this into 5 Hz using median pooling. Emotions are represented as two main features, namely arousal and valence.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Model",
      "text": "For emotions predictions, we use a state-of-the-art end-to-end model  [30]  to predict emotions from raw audio. The model predicts two dimensions across time, namely arousal and valence. The architecture we use consists of 3 convolution blocks, followed by 2 LSTM layers of 85 units, then a Fully Connected layer of 65 units, and a final output layer. Each convolution block consists of a convolution layer of 47 output channels followed by max pooling. The kernel sizes are  (27, 14, 3) , and the pooling sizes are (40, 20, 4) for the 3 blocks respectively.\n\nThis emotions model is appended to the ConcealNet presented in Subsection 3.2.2 to make end-to-end predictions of emotions from speech with lossy packets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Packet Loss Generation",
      "text": "To simulate the behaviour of lossy and non-lossy packets in a given sequence, we adapt the Markov Chain M(pL, pN) as shown in Figure  2 .  [31]  has shown it to be an effective approach for packet loss modelling; other models exist like a three-state model  [32]  to model burst behaviour.  [33]  reviews other models. Given a sequence of T frames, we sample a binary mask M by starting at the state N , then transitioning between the states N (for no-loss) and L (for loss) based on the transition probabilities until T states are enumerated. The sampled sequence of states is directly transformed into the mask M . This is a simple baseline, which replaces all the loss values by one constant value, which is 0  [34] . Even though this baseline is very simple, it serves the purpose of showing how important it is to solve the concealment problem and how far it can be improved.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "M",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Linear",
      "text": "Linear interpolation is a technique which conceals a lost segment using a linear equation joining the last point before the loss and the first point after the loss, and then predicting the lost values in between, according to the equation  [35] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments And Results",
      "text": "In order to evaluate different methods for Packet Loss Concealment (PLC), first, we use input signals x with a corresponding loss mask M (sampled by the two-state Markov Chain) and corresponding ground truth emotions labels y. Then, we use the end-to-end model to acquire the concealed signal x and the corresponding emotions labels ŷ. Consequently, we compare x against x to examine the quality of the concealment, and ŷ against y to examine the quality of the emotions prediction after PLC. In all scenarios, we use CCC  [22]  as the comparison metric, since it measures data reproducibility. The results of the concealment's quality on the audio data are shown in Table  2 , in addition to Table  3 , which shows the results of the emotions predictions after concealment. Eventually, we show the results of the stress training scheme in Table  4  Both versions of ConcealNet are performing much better in all scenarios of emotion prediction. Especially, where pL is not high (leq0.5), both versions of ConcealNet have a small drop in emotions predictions, even when the overall frame drop-rate is up to 64 %. For audio concealment, the bidirectional ConcealNet has the best performance, followed by forward ConcealNet. The results are degraded, however, in one scenario when pL = 0.9, the 0-concealment baseline achieves the best results in the loss concealment.\n\nThe scenario where pL is high is the scenario where ConcealNet experiences relatively long loss, and it is expected to recover pL/(1 -pL) ∼ 9 consecutive segments, for each loss occurrence, which is extremely challenging. However, we observe how the stress training has managed to conquer this problem, as shown by the improvements in Table  4 , where the stress trained models are generally overperforming especially in the scenarios with more losses.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, a concealment RNN (ConcealNet) was introduced. This consists of two main components: the first is a stacked generative recurrent cell R which is trained to predict elements of sequences given the preceding elements, and a wrapper FR for such a stacked cell. The wrapped recurrent cell can be used as a recurrent layer given an input sequence x and a corresponding binary mask M marking losses, to output a concealed sequence x. A generative RNN consisting of two LSTM layers was trained to be used by ConcealNet, in addition to an emotions model which was connected to the ConcealNet, to conceal audio and predict emotions end-to-end. A stress training scheme was introduced to improve the performance of ConcealNet on long-term losses. Furthermore, the proposed ConcealNet was used in two variants, one processing the sequence forwards and the other processing the sequence bidirectionally by averaging forwards and backwards. The fully reproducible experiments on the popular RECOLA continuous emotion database have shown that the proposed ConcealNet is getting considerably good results in scenarios without too long losses, even when they are frequent. In environments with short packet losses, after using ConcealNet, the degradation of speech emotion prediction is minor: for arousal, CCC dropped from 76.93 % to 75.99 %, while for valence, it dropped from 43.18 % to 39.81 %. The bidirectional variant of ConcealNet is achieving even better results. The scenario when there are long packet losses has been shown to be the most challenging as one may expect. However, a stress training technique was introduced to conquer this issue and it has shown an improvement of the results.\n\nFuture work can consider the usage of attention mechanisms and the introduction of generative approaches such as variants of generative adversarial topologies or variational solutions.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Demonstration of how the recurrent concealment cell",
      "page": 1
    },
    {
      "caption": "Figure 2: Markov Chain M(pL, pN) that samples a binary",
      "page": 2
    },
    {
      "caption": "Figure 3: Example of ﬁxed audio segment, demonstrating ConcealNet, bidirectional ConcealNet, the 0-substitution concealment, linear",
      "page": 3
    },
    {
      "caption": "Figure 1: This cell behaves similar to the PLC algorithm in [7].",
      "page": 3
    },
    {
      "caption": "Figure 2: [31] has shown it to be an effective approach",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Generative RNN architecture that predicts a frame training,wefirsttrainfor80epochswithoutthestresstraining.",
      "data": [
        {
          "Operation": "Input",
          "specs": "",
          "output shape": "(T, 100)"
        },
        {
          "Operation": "LSTM\nLSTM",
          "specs": "768\n768",
          "output shape": "(T, 768)\n(T, 768)"
        },
        {
          "Operation": "Fully Connected\nFully Connected",
          "specs": "256\n100",
          "output shape": "(T, 256)\n(T, 100)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 4: CCC percentage scores of the effects of the stress astresstrainingtechniquewasintroducedtoconquerthisissue",
      "data": [
        {
          "M parameters": "↓ drop %\npL\npN",
          "Arousal": "0-conc\ninterp\nForw\nBidir",
          "Valence": "0-conc\ninterp\nForw\nBidir"
        },
        {
          "M parameters": "0.1\n0.9\n10.16\n0.5\n0.9\n17.16\n0.1\n0.5\n35.83\n0.1\n0.1\n50.06\n0.5\n0.5\n50.28\n0.9\n0.9\n50.41\n0.5\n0.1\n64.52\n0.9\n0.5\n83.58\n0.9\n0.1\n90.15",
          "Arousal": "76.86\n73.83\n74.06\n76.66\n76.58\n68.41\n70.36\n75.99\n76.99\n68.09\n66.59\n76.18\n77.50\n69.15\n62.34\n76.55\n75.38\n58.26\n59.32\n73.42\n69.84\n63.08\n67.13\n67.33\n75.01\n59.04\n55.47\n73.43\n67.55\n56.36\n59.75\n64.11\n67.74\n58.27\n59.59\n65.50",
          "Valence": "43.11\n38.62\n38.58\n42.98\n42.86\n28.02\n31.05\n42.35\n41.95\n26.70\n23.52\n40.52\n41.19\n25.29\n16.92\n39.81\n39.90\n13.96\n14.11\n37.62\n36.70\n17.43\n22.04\n35.05\n36.61\n13.71\n11.32\n35.93\n26.55\n10.41\n11.59\n22.77\n20.93\n10.47\n11.19\n17.50"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: CCC percentage scores of the effects of the stress astresstrainingtechniquewasintroducedtoconquerthisissue",
      "data": [
        {
          "arousal": "-\nstress",
          "valence": "-\nstress",
          "audio": "-\nstress"
        },
        {
          "arousal": "76.66\n76.62\n75.99\n75.75\n76.18\n75.78\n76.55\n75.61\n73.42\n71.70\n69.84\n57.69\n73.43\n70.33\n67.55\n52.94\n67.74\n53.89",
          "valence": "43.03\n42.98\n42.35\n41.89\n40.67\n40.52\n39.90\n39.81\n37.62\n35.37\n36.70\n25.20\n35.93\n30.38\n26.55\n15.69\n20.93\n11.76",
          "audio": "98.66\n98.54\n95.16\n95.13\n94.14\n94.13\n90.70\n89.91\n83.97\n82.86\n56.15\n49.17\n76.27\n72.31\n28.25\n23.56\n18.46\n15.34"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Perceptual QoS assessment technologies for VoIP",
      "authors": [
        "A Takahashi",
        "H Yoshino",
        "N Kitawaki"
      ],
      "year": "2004",
      "venue": "IEEE Communications Magazine"
    },
    {
      "citation_id": "3",
      "title": "Efficient HMM-Based Estimation of Missing Features, with Applications to Packet Loss Concealment",
      "authors": [
        "B Borgström",
        "P Borgström",
        "A Alwan"
      ],
      "year": "2010",
      "venue": "Proceedings INTERSPEECH, 11th Annual Conference of the International Speech Communication Association. Makuhari, Chiba, Japan: ISCA"
    },
    {
      "citation_id": "4",
      "title": "Intelligent Audio Analysis",
      "authors": [
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Intelligent Audio Analysis"
    },
    {
      "citation_id": "5",
      "title": "Packet Loss Concealment Algorithm for VoIP Transmission in Unreliable Networks",
      "authors": [
        "A Janicki",
        "B Ksiundefinedundefinedak"
      ],
      "year": "2008",
      "venue": "Proceedings of the 2008 Conference on New Trends in Multimedia and Network Information Systems"
    },
    {
      "citation_id": "6",
      "title": "Generating Sequences With Recurrent Neural Networks",
      "authors": [
        "A Graves"
      ],
      "year": "2013",
      "venue": "CoRR"
    },
    {
      "citation_id": "7",
      "title": "Generative Adversarial Nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "Speech Prediction Using an Adaptive Recurrent Neural Network with Application to Packet Loss Concealment",
      "authors": [
        "R Lotfidereshgi",
        "P Gournay"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "9",
      "title": "I have vxxx bxx connexxxn!: Facing Packet Loss in Deep Speech Emotion Recognition",
      "authors": [
        "M Mohamed",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "I have vxxx bxx connexxxn!: Facing Packet Loss in Deep Speech Emotion Recognition"
    },
    {
      "citation_id": "10",
      "title": "Real-time Tracking of Speakers' Emotions, States, and Traits on Mobile Platforms",
      "authors": [
        "E Marchi",
        "F Eyben",
        "G Hagerer",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proceedings INTERSPEECH, 17th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "11",
      "title": "AI Benchmark: All About Deep Learning on Smartphones in 2019",
      "authors": [
        "A Ignatov",
        "R Timofte",
        "A Kulik",
        "S Yang",
        "K Wang",
        "F Baum",
        "M Wu",
        "L Xu",
        "L Van Gool"
      ],
      "year": "2019",
      "venue": "AI Benchmark: All About Deep Learning on Smartphones in 2019",
      "arxiv": "arXiv:1910.06663"
    },
    {
      "citation_id": "12",
      "title": "Packet Loss Concealment Based on Deep Neural Networks for Digital Speech Transmission",
      "authors": [
        "B.-K Lee",
        "J.-H Chang"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc"
    },
    {
      "citation_id": "13",
      "title": "Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1",
      "authors": [
        "J Garofolo",
        "L Lamel",
        "W Fisher",
        "J Fiscus",
        "D Pallett"
      ],
      "year": "1993",
      "venue": "NASA STI/Recon technical report n"
    },
    {
      "citation_id": "14",
      "title": "Speech Loss Compensation by Generative Adversarial Networks",
      "authors": [
        "Y Shi",
        "N Zheng",
        "Y Kang",
        "W Rong"
      ],
      "year": "2019",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
    },
    {
      "citation_id": "15",
      "title": "Audio inpainting with generative adversarial network",
      "authors": [
        "P Ebner",
        "A Eltelt"
      ],
      "year": "2020",
      "venue": "Audio inpainting with generative adversarial network",
      "arxiv": "arXiv:2003.07704"
    },
    {
      "citation_id": "16",
      "title": "Packet loss concealment with recurrent neural networks for wireless inertial pose tracking",
      "authors": [
        "X Xiao",
        "S Zarar"
      ],
      "year": "2018",
      "venue": "Proceedings IEEE 15th International Conference on Wearable and Implantable Body Sensor Networks (BSN)"
    },
    {
      "citation_id": "17",
      "title": "Signal Reconstruction Using Evolvable Recurrent Neural Networks",
      "authors": [
        "N Khan",
        "G Khan"
      ],
      "year": "2018",
      "venue": "Intelligent Data Engineering and Automated Learning -IDEAL 2018"
    },
    {
      "citation_id": "18",
      "title": "Cartesian genetic programming",
      "authors": [
        "J Miller"
      ],
      "year": "2011",
      "venue": "Cartesian Genetic Programming"
    },
    {
      "citation_id": "19",
      "title": "Deep filtering: Signal extraction using complex time-frequency filters",
      "authors": [
        "W Mack",
        "E Habets"
      ],
      "year": "2019",
      "venue": "Deep filtering: Signal extraction using complex time-frequency filters",
      "arxiv": "arXiv:1904.08369"
    },
    {
      "citation_id": "20",
      "title": "",
      "authors": [
        "I Goodfellow",
        "Y Bengio",
        "A Courville",
        "Deep Learning"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "21",
      "title": "Long Short-term Memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "22",
      "title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation",
      "authors": [
        "K Cho",
        "B Van Merrienboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation"
    },
    {
      "citation_id": "23",
      "title": "A concordance correlation coefficient to evaluate reproducibility",
      "authors": [
        "I Lawrence",
        "K Lin"
      ],
      "year": "1989",
      "venue": "Biometrics"
    },
    {
      "citation_id": "24",
      "title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU",
      "authors": [
        "K Hwang",
        "W Sung"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "25",
      "title": "BOHB: robust and efficient hyperparameter optimization at scale",
      "authors": [
        "S Falkner",
        "A Klein",
        "F Hutter"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "26",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, Conference Track Proceedings"
    },
    {
      "citation_id": "27",
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "authors": [
        "N Srivastava",
        "G Hinton",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2014",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "28",
      "title": "Learning internal representations by error propagation",
      "authors": [
        "D Rumelhart",
        "G Hinton",
        "R Williams"
      ],
      "year": "1985",
      "venue": "Learning internal representations by error propagation"
    },
    {
      "citation_id": "29",
      "title": "A comprehensive study of deep bidirectional LSTM RNNS for acoustic modeling in speech recognition",
      "authors": [
        "A Zeyer",
        "P Doetsch",
        "P Voigtlaender",
        "R Schlter",
        "H Ney"
      ],
      "year": "2017",
      "venue": "Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "31",
      "title": "End-to-End Speech Emotion Recognition Using Deep Neural Networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proceedings IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "32",
      "title": "The Gilbert-Elliott Model for Packet Loss in Real Time Services on the Internet",
      "authors": [
        "G Haßlinger",
        "O Hohlfeld"
      ],
      "year": "2008",
      "venue": "14th GI/ITG Conference-Measurement, Modelling and Evaluation of Computer and Communication Systems"
    },
    {
      "citation_id": "33",
      "title": "An Analysis of Packet Loss Models for Distributed Speech Recognition",
      "authors": [
        "B Milner",
        "A James"
      ],
      "year": "2004",
      "venue": "Proceedings INTERSPEECH, 8th International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "34",
      "title": "MAC-Layer Packet Loss Models for Wi-Fi Networks: A Survey",
      "authors": [
        "C Da Silva",
        "C Pedroso"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "35",
      "title": "A survey of packet loss recovery techniques for streaming audio",
      "authors": [
        "C Perkins",
        "O Hodson",
        "V Hardman"
      ],
      "year": "1998",
      "venue": "IEEE network"
    },
    {
      "citation_id": "36",
      "title": "Chapter 14 -Interpolation,\" in An Introduction to MATLAB Programming and Numerical Methods for Engineers",
      "authors": [
        "A Bayen",
        "T Siauw"
      ],
      "year": "2015",
      "venue": "Chapter 14 -Interpolation,\" in An Introduction to MATLAB Programming and Numerical Methods for Engineers"
    }
  ]
}