{
  "paper_id": "2206.02187v1",
  "title": "M2Fnet: Multi-Modal Fusion Network For Emotion Recognition In Conversation",
  "published": "2022-06-05T14:18:58Z",
  "authors": [
    "Vishal Chudasama",
    "Purbayan Kar",
    "Ashish Gudmalwar",
    "Nirmesh Shah",
    "Pankaj Wasnik",
    "Naoyuki Onoe"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion Recognition in Conversations (ERC) is crucial in developing sympathetic human-machine interaction. In conversational videos, emotion can be present in multiple modalities, i.e., audio, video, and transcript. However, due to the inherent characteristics of these modalities, multi-modal ERC has always been considered a challenging undertaking. Existing ERC research focuses mainly on using text information in a discussion, ignoring the other two modalities. We anticipate that emotion recognition accuracy can be improved by employing a multi-modal approach. Thus, in this study, we propose a Multi-modal Fusion Network (M2FNet) that extracts emotion-relevant features from visual, audio, and text modality. It employs a multi-head attention-based fusion mechanism to combine emotion-rich latent representations of the input data. We introduce a new feature extractor to extract latent features from the audio and visual modality. The proposed feature extractor is trained with a novel adaptive marginbased triplet loss function to learn emotion-relevant features from the audio and visual data. In the domain of ERC, the existing methods perform well on one benchmark dataset but not on others. Our results show that the proposed M2FNet architecture outperforms all other methods in terms of weighted average F1 score on well-known MELD and IEMOCAP datasets and sets a new state-of-theart performance in ERC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are the unseen mental states that are linked to thoughts and feelings  [23] . In the absence of physiological indications, they could only be detected by human actions such as textual utterances, visual gestures, and acoustic signals. Emotion Recognition in Conversations (ERC) seeks to recognize the human emotions in conversations depending on their textual, visual, and acoustic cues. Recently, ERC Figure  1 . Multi-modal data as input has become an essential task in multimedia content analysis and moderation. It is a prominent trait to understand the nature of the interaction between users and the content. It has applications in various tasks, namely, AI interviews, personalized dialog systems, sentiment analysis, and understanding the user's perception of the content from the platforms like YouTube, Facebook, and Twitter  [23] .\n\nIn literature, we can see that many state-of-the-art methods adopt text-based processing to perform robust ERC  [6, 11] , such methods do not take into consideration the vast amount of information present in the acoustic and visual modalities. Since the ERC data mainly consists of all three modalities, i.e., text, visual, and acoustic, we hypothesize that the robust fusion of these modalities can improve the performance and robustness of the existing systems. A sample of emotional expressions in three different modalities is presented in Figure  1  where the ERC system takes each modality as input and predicts the associated emotion.\n\nIn this paper, we propose a multi-modal fusion network (M2FNet) that takes advantage of the multi-modal nature of real-world media content by introducing a novel multi-head fusion attention layer. This layer combines features from different modalities to generate rich emotion-relevant representations by mapping the information from acoustic and visual features to the latent space of the textual features. In addition, we propose a new feature extractor model to ex- tract the deeper features from the audio and visual contents. Here, we introduce a new adaptive margin-based triplet loss function, which helps the proposed extractor to learn representations more effectively. Additionally, we propose a dual network inspired from  [13]  to combine the emotional content from the scene by taking into account the multiple people present in it. Furthermore, from the literature, we can see that state-of-the-art ERC methods perform well on one benchmark dataset, for example, IEMOCAP  [2]  while their performance degrades on more complex datasets like MELD  [22] . This motivates us to propose a robust multimodal ERC system.\n\nIn order to verify the robustness of the proposed network, one experiment is carried out to compare its performance with existing text-based and multi-modal ERC methods. This comparison is visualized in Figure  2  where results are given in terms of weighted average F1 score on MELD  [2]  and IEMOCAP  [22]  datasets. Here, it can be observed that the proposed M2FNet model obtains a higher weighted average F1 score than other models. Followings are our major contributions:\n\n• A novel multi-modal fusion network called M2FNet is proposed for emotion recognition in conversation.\n\n• A multi-head attention-based fusion layer is introduced, which aids the proposed system to combine latent representations of the different inputs.\n\n• To extract deeper relevant features from audio and visual modality utterances, we introduce a new feature extractor model.\n\n• In the feature extractor model, we propose a new adaptive margin-based triplet loss function that helps the proposed model to learn emotion-relevant features.\n\n• To take advantage of the scene's emotional content, we also present a weighted face model that considers multiple people present in the scene.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Emotion recognition in conversations (ERC) is different from traditional emotion recognition. Rather than treating emotions as static states, ERC involves emotional dynamics of a conversation, in which the context plays a vital role. Prior works on ERC mainly which use text, and audio features are proposed in  [4, 12] . In the past few years, datasets with visual, acoustic and textual cues have been made publicly available  [2, 22] . On these datasets, several deep learning methods are applied to recognize emotion. These techniques can be classified based on the type of data; either they merely utilize text or use multi-modal data (i.e. text, visual and audio).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Text-Based Methods",
      "text": "With the advent of the Transformer  [32] , the focus on text-based methods has recently increased. Due to the vast amount of information present in text data, current methods approach ERC as a purely text-based problem. Li et al.  [16]  use BERT  [5]  to encode the individual sentences and then uses a dialog level network for multitask learning on auxiliary tasks to generate better latent representations of the dialog as a whole. Furthermore, Li et al.  [14]  build on this by incorporating transformer at the dialog end. Jiangnan et al.  [15]  took this one step further by using the contextual representations from a BERT and transformer dialog network by designing three types of masks and utilizing them in three independent transformer blocks. The three designed masks learn the conventional context, Intra-Speaker, and Inter-Speaker dependency.\n\nIn  [6] , Ghosal et al. incorporates different elements of commonsense such as mental states, events, and causal relations to learn interactions between interlocutors participating in a conversation. Authors in  [7]  and  [28]  use Graph Neural networks to encode inter utterance and inter speaker relationships. Kim et al.  [11]  model contextual information by simply prepending speaker names to utterances and inserting separation tokens between the utterances in a dialogue. To generate contextualized utterance representations, Wang et al.  [33]  uses LSTM-based encoders to capture self and inter-speaker dependency of interlocutors. A directed acyclic graph (DAG) based ERC was introduced by Shen et al. in  [27]  which is an attempt to combine the strengths of conventional graph-based and recurrence-based neural networks. In  [38] , Zhu et al. propose a new model in which the transformer model fuses the topical and commonsense information to predict the emotion label. Recently, Song et al.  [29]  proposed the EmotionFlow model, which encodes the user's utterances via concatenating the context with an auxiliary question, and then, a random field is applied to capture the sequential information at the emotion level.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Modal Methods",
      "text": "Prior literature on using previous utterances to provide context with respect to the utterance in hand has set the benchmark for dyadic conversations. In  [8, 21] , authors use previous utterances of both parties in a dyadic conversation and the contextual information from the same to predict the emotional state of any given utterance. Majumder et al.  [18]  build on this by separately modeling the uni-modal contextual information and then using a hierarchical tri-modal feature level fusion for obtaining a rich feature representation of the utterance. DialogueRNN  [19]     [20]  investigate the differentiated multi-modal emotional behaviors from the intra-and intermodal perspectives. On a similar dataset, the CMU-Mosei methods like Loshchilov et al  [3]  and Tsai et al  [31]  use multi-head attention based fusion  [32]  for multi-modal emotion recognition.\n\nIn most of the previous work, methods do not consider distinct facial features that play a significant role in determining the emotional context of the conversation. They use the frames as a whole entity but do not extract the essential part of the frame (i.e., face). Additionally, most of these methods do not have an active fusion strategy other than simple concatenation to take advantage of the wealth of the information present in the form of visual and acoustic data.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Framework",
      "text": "This section first introduces the problem statement and then provide details of the architecture design of the proposed framework briefly.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Problem Statement",
      "text": "A dialog consists of k number of utterances (U ) along with their respective labels (Y ) arranged together with respect to time where each utterance is accompanied by it's respective video clip, speech segment and text transcript. Mathematically, a dialog for k number of utterances can be formulated as follows:\n\nHere, x i denotes the i th utterance made up of corresponding x t (text), x a (audio) and x v (visual) component, while y i indicates respective i th utterance's emotion label. The proposed network takes this data as input and assigns the right emotion to any given utterance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multi-Modal Fusion Network: M2Fnet",
      "text": "We propose a hierarchical framework called Multimodal Fusion network (i.e., M2FNet) which is illustrated in Initially, the features are extracted by the utterance level module independently. Then, at the dialog level extraction network, the model learns to predict the right emotion for each utterance by using the contextual information from the dialog as a whole. In the subsequent subsections, we briefly discuss the design steps occurring in both utterance and dialog level feature extraction.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Utterance Level Feature Extraction",
      "text": "From Figure  3 , one can see that there are k number of utterances and each utterance is made up of x t (text), x a (audio) and x v (visual) content. In this step, the features from each modality are extracted for each utterance separately before passing to the dialog level feature extraction network. Each modality's input signals are passed through their corresponding feature extractors for generating their embeddings.\n\nText: In order to provide deeper inter utterance context, the text modality data (i.e., x t ) are passed through the Text Feature Extractor module. Here, we employ a modified RoBERTa model (φ M -RoBERT a ) proposed by Kim et al.  [11]  as feature extractor. Every utterance's x t is accompanied by its preceding and next utterance text separated by the separator token < S >. The modified RoBERTa model is fine-tuned on this transcript and the respective utterance's labels. The last layer activations {F IT : F 1,T , F 2,T ......F k,T } obtained from the modified RoBERTa model by passing the utterance's text data can be represented by:\n\n(2) Here, F i IT denotes i th utterance's embeddings and D T denotes the size of the embeddings of text utterance.\n\nAudio: On the audio end, we introduce a new feature extractor model. The network design of the proposed feature extractor module is discussed briefly in Subsection 3.3. Initially, the audio contents are transformed into 2D Mel Spectrogram in RGB format and then passed through the feature extractor model. Here, the audio signal is first processed via different augmentation techniques like time warping and Additive White Gaussian Noise (AWGN) noise. Then the augmented signals are transformed into the corresponding Mel Spectrograms  [24] . For computing the Mel Spectrogram, the Short Time Fourier transform (STFT) is used with the frame length of 400 samples (25 ms) and hop length of 160 samples (10ms). We also use 128 Mel filter banks to generate the Mel Spectrogram  [30] .\n\nThe proposed extractor takes the Mel Spectrograms (i.e., x a ) as input and generate the corresponding feature embeddings {F IA : F 1,A , F 2,A ......F k,A }. The functionality of the proposed audio feature extractor module can be mathematically expressed as,\n\nwhere, F i IA is the i th utterance's embeddings and D A indicates the size of embeddings of audio utterance and the φ AF E denotes the function of introduced audio feature extractor module.\n\nVisual: In order to extract rich emotion-relevant features from the visual signal, we propose a dual network inspired from  [13]  that exploit not only human facial expression but also context information in a joint and boosting manner. For both tasks, we use our proposed extractor model (as discussed in Subsection 3.3) and train on the CASIA webface database  [35]  to extract the deeper features from the visual image. Following are the details of steps involved in the dual network:\n\n• To encode the context information of the scene as a whole our trained feature extractor model is performed on 15 successive frames of the utterance clip. Following which the features are max pooled over the frame axis to obtain the scene embeddings of the utterance.\n\n• To extract facial emotion-related features from the same 15 successive frames of the utterance clip, we propose a weighted Face Model (i.e., as visualized in Figure  3 ) which works as follows:\n\n-Given a frame, it is passed through a Multi-task Cascaded Convolutional Network (MTCNN)  [37]  to detect the faces present in the frame. This returns the bounding box of each face along with its confidence. Then each of the respective faces is passed through our trained feature extractor model to obtain emotion-relevant features of each respective face. Now, the areas of each bounding box accompanying the faces are normalized to bring their values between 0 and 1. Following this, a weighted sum is performed using the features of each face and their respective normalized areas to obtain the facial emotion feature of a frame.\n\n-The same process is followed for each of the 15 frames, and similarly, upon extraction, the features are max-pooled over the frame axis to obtain the facial features of the utterance.\n\nUpon extraction of features from each network, the scene embeddings are concatenated with the facial features to obtain a more comprehensive representation of the visual data in an utterance. Finally, this visual feature extractor's output {F IV : F 1,V , F 2,V ......F k,V } can be formulated as:\n\nwhere, F i IA is the i th utterance's embeddings and φ W F denotes the function operation of weighted face model while D V is the size of the feature embedding of the visual utterance. Finally, these embeddings (i.e., text, acoustic and visual) are then sent to the dialog level feature extractor as an input to learn the correct prediction of emotions for each utterance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Dialog Level Feature Extraction",
      "text": "The design diagram at dialog level feature extraction of the proposed M2FNet model is illustrated in Figure  3 . Each modality embeddings (i.e., F IT , F IA and F IV ) are passed through their corresponding network with a variable stack of transformer encoders  [32]  to learn the inter utterance context. The number of transformer encoders for text, audio, and visual modality is denoted by N T , N A , and N V , respectively. We also employ a local skip connection between each encoder to prevent the model from ignoring the lower-level features. The corresponding feature maps obtained from the encoders can be mathematically expressed as:\n\nHere, T r is the operation function of the transformer encoder.\n\nThe corresponding feature maps associated with the text, visual and audio (i.e., F T , F V , F A ) are passed to a novel Multi-Head Attention Fusion module that helps the network in incorporating visual and acoustic information. The network architecture of the attention fusion module is also depicted in Figure  3 . Here, the text features F T are used as input to fusion module as Query (Q) and Value (V) for the multi-head attention operation, and then the visual F V and acoustic F A features for the dialog are used as Key (K) in order to modulate the attention given to each utterance at any time-step. Hence, each individual modality is now mapped to the text vector space, and the respective features are concatenated and passed to a fully connected layer which outputs a vector ∈ R k•D T . The output of the fusion layer is passed through the next fusion layer along with the previous F A and F V feature maps. Here, the m number of multi-head attention fusion layers stacked together in order to generate the final feature outcome (F f usion ) as demonstrated below:\n\nIn the above equation, Φ indicates the learning function of the proposed Multi-Head Attention Fusion layer. The main difference in our Fusion strategy compared to the previous work using Multi-Head Attention is that our strategy involves changing the key across modalities while keeping the Query and Value the same to better modulate inter utterance attention and incorporate inter-modal information.\n\nThe feature outcome of the last multi-head attention fusion layer (i.e., F f usionm ) is concatenate with visual and acoustic feature maps (i.e., F V and F A ) as expressed below:\n\nFinally, we append two fully connected layers (FC) which generates the desired predicted output (i.e., Y i p =< y i 1 , y i 2 , ..., y i p >, where, i ∈ [1, k]) of our proposed system.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Feature Extractor Module",
      "text": "In order to fetch deep features from audio and visual contents, we propose a new feature extractor model and the same is illustrated in Figures 4. The proposed extractor is designed based on triplet network to leverage the importance of triplet loss function  [25] . Initially, the anchor, positive and negative samples have been generated as suggested in  [25]  for audio and visual modalities. Then these samples are passed through encoder network followed by projector module. Here, we use standard ResNet18  [9]  as a backbone of the encoder network while the projector consists a linear fully connected layer which project the embedding of encoder network to desired representations (i.e.,\n\nThe proposed extractor model is trained using weighted combination of three loss function i.e., adaptive margin triplet loss (i.e., L AM T ), covariance loss (i.e., L Cov ) and variance loss (i.e., L V ar ) functions. It can be expressed as follow:\n\nwhere, λ 1 , λ 2 and λ 3 are weighting factors that controls the distribution of different loss functions.\n\nIn  [25] , authors design the triplet loss function used to learn good representations of faces based on anchor, positive and negative samples. Here, authors have used a fixed margin value in their triplet loss function that helps to separate out the representations of positive and negative samples. However, in some cases where the positive or negative samples have the same distance with the anchor or the positive sample is only a bit closer to the anchor than the negative sample, the loss would be zero, and there would be no correction even though it should still be pulling the positive sample closer and pushing the negative sample away from the anchor. To overcome this issue, we propose a triplet loss function based on a adaptive margin value. This can be mathematically written as\n\nHere, D a,p s , D a,n s and D p,n s denotes the euclidean distance based similarity metric between representations of anchor and positive, anchor and negative, positive and negative samples, respectively. m AM is the adaptive margin which is calculated based on similarity and dissimilarity measures as\n\nIn addition, we also utilize the variance loss function proposed by Bardes et al.  [1]  which helps the proposed model to tackle the mode collapse issue. Mathematically, the variance loss function can be presented as,\n\nHere, V ar(Z) (i.e., 1\n\n) denotes the variance obtained from the corresponding representations, while Ẑ is mean of the corresponding representation.\n\nTo decorrelate the different dimensions of the representations, we adopt the covariance loss function  [1]  and the the same can be expressed mathematically for representation as\n\nwhere,\n\nT indicates the covariance matrix of corresponding representation.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Analysis And Discussion",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dataset",
      "text": "For fair comparison with state-of-the-art methods, we evaluate our proposed network (M2FNet) on Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [2] , and Multimodal EmotionLines Dataset (MELD)  [22]  benchmark datasets. The statistics of these datasets are reported in Table  1  and details are provided in Section 1 of the supplementary material. Both IEMOCAP and MELD are multimodal datasets with textual, visual, and acoustic data.\n\nMELD: The MELD  [22]  is a multimodal and multiparty dataset containing more than 1,400 conversations and 13,000 utterances from the Friends TV series. The utterances are annotated with one of the seven emotion labels (anger, disgust, sadness, joy, surprise, fear, and neutral). We use the pre-defined train/val split provided in the MELD dataset. The details of this dataset are given in Table  1 .\n\nIEMOCAP: The IEMOCAP database  [2]  is an acted, multimodal and multi-speaker database consisting of videos of dyadic sessions having approximately 12 hours of audiovisual data with text transcriptions. Each video contains a single conversation, which is segmented into multiple utterances. Each utterance is annotated with one of six emotion labels, i.e., happy, sad, neutral, angry, excited, and frustrated. The database statistics are given in Table  1 . We randomly select 10% of training conversations as evaluation split for computing the hyperparameters.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Setups And Hyper-Parameter Tuning",
      "text": "All experiments are carried out using a single NVIDIA GeForce RTX 3090 card. We adopt AdamW  [17]  as the optimizer with an initial learning rate 5e-4 with L 2 weight decay ranges between 5e-4 and 5e-5. Dropout is used with a rate between 0.4 and 0.5. The number of encoder layers in each modality's encoder (i.e., N T , N A , N V ) is tuned using a greedy scheme and set to 1 and 5 for MELD and IEMOCAP validation datasets, respectively. The number of multi-head attention fusion layers is set to 5 (i.e., = m) for both dataset. The proposed M2FNet framework is trained using the categorical cross-entropy on each utterances softmax output for each of M dialogs and their k utterances. Section 2 of the supplementary material provides training and validation performance details.\n\nTo extract deeper features from audio and visual contents, we introduce a new feature extractor model. Here, ResNet18  [9]  is used as encoder module while the projector module consists a fully connected layer which projects the embeddings of encoder network to desired representations (i.e., Z). Here we set the number of representations as Z = 300. For audio task, the extractor model is trained on Mel Spectrograms obtained from the corresponding audio   [35] . Here, the extractor model is trained using the loss function mentioned in Equation no. 9 in which the weighting factors λ 1 , λ 2 and λ 3 are set to 20, 5 and 1, respectively. The proposed extractor model is trained upto 60 and 100 epochs for audio and visual task, respectively using Adam optimizer with learning rate of 1e-4 and decay rate of 1e-6. We mainly employ weighted average F1 score as evaluation metric due to its suitability to test with imbalance dataset. Additionally, we present our results in terms of classification accuracy to evaluate the model performance.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Ablation Studies",
      "text": "To better understand the contribution of different modules in the proposed M2FNet model, we have conducted several ablation studies on both IEMOCAP and MELD datasets. The corresponding results are compared in terms of accuracy and weighted average F1 scores for MELD and IEMOCAP testing datasets.\n\nTo validate the impact of each modality, we train the pro- posed network with and/or without Text, Video, and Audio as input and without using the proposed Fusion module.\n\nFrom Table  2 , one can observe that concatenation of multimodal input with all three modalities obtain higher accuracy and weighted average F1 score than other scenarios such as using only one or two modalities. Furthermore, our fusion mechanism helps to enhance the accuracy by 2.53% and 0.57% for IEMOCAP and MELD datasets, respectively. However, in the case of weighted average F1 score, it obtains slightly inferior performance for the MELD dataset while improving it by 2.74% for the IEMOCAP dataset.\n\nOn the visual end, we utilize scene and weighted face embeddings. For understanding the importance of both embeddings, two more experiments have been carried out where the proposed network with individual embedding has been trained, and the corresponding results are given in Table 3. From the table, it can be observed here that the weighted face model or the scene encoding network on their own do not improve the results; however, when the network can access both, it significantly improves the results. This shows that both the context from the scene and the people in the scene are equally important for emotion recognition.\n\nWe also observe the effect of transformer encoders in the proposed framework. In these experiments, we set same number of transformer encoders (i.e., N A = N V = N T ) for each modality and observe its effect for different numbers. The corresponding results are presented in Table  4  where it is observed that N A = N V = N T = 1 gives best performance for MELD dataset while the N A = N T = N V = 5 setting helps the proposed framework to obtain higher performance for IEMOCAP testing dataset.\n\nIn the proposed model, we have set m = 5 number of Multi-Head Attention Fusion modules. To validate this, we train the proposed model with different numbers of the Multi-Head Attention Fusion modules and observe the corresponding accuracy and weighted average F1 score. This analysis is demonstrated in Table  5  where one can observe that the proposed model with five Multi-Head Attention Fusion modules (i.e., m = 5) obtains higher quantitative measures on both datasets.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Model Performance",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Comparative Analysis",
      "text": "To validate the robustness of the proposed network, we compare our proposed network with state-of-the-art textbased ERC systems in terms of weighted average F1 score and the same is presented in Table  6 . Here, one can notice that the proposed network has a state-of-the-art performance by obtaining superior quantitative results than previous methods on both datasets (i.e., 0.21% higher than previous best EmotionFlow  [29]  model on MELD dataset while 1.83% higher than that of the previous best DAG-ERC  [27]  model on IEMOCAP testing dataset).\n\nWhen compared with the existing multi-modal methods, our proposed M2FNet network shows a substantial im- provement compared to state-of-the-art multi-modal ERC methods. The obtained results are presented in Table  7  where one can notice that the proposed M2FNet model obtains 2.19% and 2.71% higher accuracy and weighted average F1 score on MELD dataset than that of previous best performance. Similarly, it set 0.77% and 0.63% higher accuracy and weighted average F1 sore than that of previous best DialogueTRM model  [20]  on IEMOCAP testing dataset.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Limitations",
      "text": "Our model sometimes gets confused and miss-classifies similar or close emotions such as Frustration and Anger, Happy and Excited. We can also observe that for highly imbalance data, our model misclassifies many emotions as the emotion with higher number of data samples. For example, many emotions are overwhelmingly predicted as Neutral for MELD dataset.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a robust multi-modal fusion network called M2FNet for the task of Emotion Recognition in Conversation. In M2FNet, we propose a multi-head fusion attention module that helps the network to extract rich features from multiple modalities. A new feature extractor model is introduced in the proposed design to learn the audio and visual features effectively. Here, a new adaptive margin triplet loss function is introduced, which helps the extractor module to learn representations effectively. A new weighted face model is proposed in our framework to learn the rich facial features. Detailed analysis shows that encoding both scene and face-related information is essential for emotion recognition. Similarly, we observed that multi-modal fusion is necessary to leverage information from multiple modalities present in an utterance. Finally, our experiments validate the robustness of the proposed network quantitatively on both benchmark datasets.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Multi-modal data as input",
      "page": 1
    },
    {
      "caption": "Figure 1: where the ERC system takes",
      "page": 1
    },
    {
      "caption": "Figure 2: Quantitative analysis on MELD and IEMOCAP datasets",
      "page": 2
    },
    {
      "caption": "Figure 2: where results are",
      "page": 2
    },
    {
      "caption": "Figure 3: Network design of the proposed framework.",
      "page": 3
    },
    {
      "caption": "Figure 3: The network is designed based on two levels of",
      "page": 3
    },
    {
      "caption": "Figure 3: , one can see that there are k number of ut-",
      "page": 3
    },
    {
      "caption": "Figure 3: ) which works as follows:",
      "page": 4
    },
    {
      "caption": "Figure 3: Here, the text features FT are used as",
      "page": 5
    },
    {
      "caption": "Figure 4: Network design of the proposed Extractor network.",
      "page": 5
    },
    {
      "caption": "Figure 5: Predictions made by the network on the MELD and",
      "page": 8
    },
    {
      "caption": "Figure 5: presents the performance of our model in terms",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 1: and details are provided in Section 1 of the sup-",
      "page": 6
    },
    {
      "caption": "Table 1: Statistics of the testing benchmark datasets: MELD and",
      "page": 6
    },
    {
      "caption": "Table 1: IEMOCAP: The IEMOCAP database [2] is an acted,",
      "page": 6
    },
    {
      "caption": "Table 2: Ablation Studies based comparison to validate the impact",
      "page": 7
    },
    {
      "caption": "Table 3: Ablation Studies based comparison to validate the impact",
      "page": 7
    },
    {
      "caption": "Table 4: Ablation Studies based comparison to validate the impact",
      "page": 7
    },
    {
      "caption": "Table 5: Ablation Studies based comparison to validate the impact",
      "page": 7
    },
    {
      "caption": "Table 2: , one can observe that concatenation of multi-",
      "page": 7
    },
    {
      "caption": "Table 5: where one can observe",
      "page": 7
    },
    {
      "caption": "Table 6: Quantitative comparison with text-based state-of-the-art",
      "page": 8
    },
    {
      "caption": "Table 6: Here, one can no-",
      "page": 8
    },
    {
      "caption": "Table 7: Quantitative comparison with multimodal-based state-of-",
      "page": 8
    },
    {
      "caption": "Table 7: where one can notice that the proposed M2FNet model ob-",
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Vicreg: Variance-invariance-covariance regularization for selfsupervised learning",
      "authors": [
        "Adrien Bardes",
        "Jean Ponce",
        "Yann Lecun"
      ],
      "year": "2021",
      "venue": "Vicreg: Variance-invariance-covariance regularization for selfsupervised learning",
      "arxiv": "arXiv:2105.04906"
    },
    {
      "citation_id": "2",
      "title": "Iemocap: interactive emotional dyadic motion capture database. Language Resources and Evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut"
      ],
      "year": "2008",
      "venue": "Iemocap: interactive emotional dyadic motion capture database. Language Resources and Evaluation"
    },
    {
      "citation_id": "3",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "Noé Tits",
        "Mathilde Brousmiche",
        "Stéphane Dupont"
      ],
      "year": "2020",
      "venue": "A transformer-based joint-encoding for emotion recognition and sentiment analysis"
    },
    {
      "citation_id": "4",
      "title": "Real-life emotions detection with lexical and paralinguistic cues on human-human call center dialogs",
      "authors": [
        "Laurence Devillers",
        "Laurence Vidrascu"
      ],
      "year": "2006",
      "venue": "Ninth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "6",
      "title": "COSMIC: commonsense knowledge for emotion identification in conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "F Alexander"
      ],
      "year": "2020",
      "venue": "COSMIC: commonsense knowledge for emotion identification in conversations"
    },
    {
      "citation_id": "7",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "8",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "Devamanyu Hazarika",
        "Soujanya Poria",
        "Amir Zadeh",
        "Erik Cambria",
        "Louis-Philippe Morency",
        "Roger Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "10",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "arxiv": "arXiv:2106.01978"
    },
    {
      "citation_id": "11",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "Taewoon Kim",
        "Piek Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "12",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "Min Chul",
        "Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "13",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "Jiyoung Lee",
        "Seungryong Kim",
        "Sunok Kim",
        "Jungin Park",
        "Kwanghoon Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "HiTrans: A transformer-based context-and speakersensitive model for emotion detection in conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "A hierarchical transformer with speaker modeling for emotion recognition in conversation",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Qingyi Si",
        "Weiping Wang"
      ],
      "year": "2020",
      "venue": "A hierarchical transformer with speaker modeling for emotion recognition in conversation",
      "arxiv": "arXiv:2012.14781"
    },
    {
      "citation_id": "16",
      "title": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "authors": [
        "Jingye Li",
        "Meishan Zhang",
        "Donghong Ji",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Multi-task learning with auxiliary speaker identification for conversational emotion recognition",
      "arxiv": "arXiv:2003.01478"
    },
    {
      "citation_id": "17",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Decoupled weight decay regularization",
      "arxiv": "arXiv:1711.05101"
    },
    {
      "citation_id": "18",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-based systems",
      "authors": [
        "Navonil Majumder",
        "Devamanyu Hazarika"
      ],
      "year": "2018",
      "venue": "Multimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-based systems"
    },
    {
      "citation_id": "19",
      "title": "Dialoguernn: An attentive RNN for emotion detection in conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2018",
      "venue": "Dialoguernn: An attentive RNN for emotion detection in conversations"
    },
    {
      "citation_id": "20",
      "title": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "authors": [
        "Yuzhao Mao",
        "Qi Sun",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li",
        "Jianping Shen"
      ],
      "year": "2020",
      "venue": "Dialoguetrm: Exploring the intra-and inter-modal emotional behaviors in the conversation",
      "arxiv": "arXiv:2010.07637"
    },
    {
      "citation_id": "21",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2018",
      "venue": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "Theory and applications of digital speech processing",
      "authors": [
        "Lawrence Rabiner",
        "Ronald Schafer"
      ],
      "year": "2010",
      "venue": "Theory and applications of digital speech processing"
    },
    {
      "citation_id": "25",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "Florian Schroff",
        "Dmitry Kalenichenko",
        "James Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Junqing Chen",
        "Xiaojun Quan",
        "Zhixian Xie"
      ],
      "year": "2020",
      "venue": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "arxiv": "arXiv:2012.08695"
    },
    {
      "citation_id": "27",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "Weizhou Shen",
        "Siyue Wu",
        "Yunyi Yang",
        "Xiaojun Quan"
      ],
      "year": "2021",
      "venue": "Directed acyclic graph network for conversational emotion recognition",
      "arxiv": "arXiv:2105.12907"
    },
    {
      "citation_id": "28",
      "title": "Summarize before aggregate: A globalto-local heterogeneous graph inference network for conversational emotion recognition",
      "authors": [
        "Dongming Sheng",
        "Dong Wang",
        "Ying Shen",
        "Haitao Zheng",
        "Haozhuang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "29",
      "title": "Emotionflow: Capture the dialogue level emotion transitions",
      "authors": [
        "Xiaohui Song",
        "Liangjun Zang",
        "Rong Zhang",
        "Songlin Hu",
        "Longtao Huang"
      ],
      "year": "2022",
      "venue": "Accepted in ICASSP"
    },
    {
      "citation_id": "30",
      "title": "A scale for the measurement of the psychological magnitude pitch",
      "authors": [
        "Stanley Stevens",
        "John Volkmann",
        "Edwin Broomell Newman"
      ],
      "year": "1937",
      "venue": "The journal of the acoustical society of america"
    },
    {
      "citation_id": "31",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Liang",
        "J Zico Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "32",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Yan Wang",
        "Jiayu Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "34",
      "title": "Robust multimodal emotion recognition from conversation with transformer-based crossmodality fusion",
      "authors": [
        "Baijun Xie",
        "Mariia Sidulova",
        "Chung Hyuk"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "35",
      "title": "Learning face representation from scratch",
      "authors": [
        "Dong Yi",
        "Zhen Lei",
        "Shengcai Liao",
        "Stan Li"
      ],
      "year": "2014",
      "venue": "Learning face representation from scratch",
      "arxiv": "arXiv:1411.7923"
    },
    {
      "citation_id": "36",
      "title": "Modeling both contextand speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "Dong Zhang",
        "Liangqing Wu",
        "Changlong Sun",
        "Shoushan Li",
        "Qiaoming Zhu",
        "Guodong Zhou"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19"
    },
    {
      "citation_id": "37",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "38",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "arxiv": "arXiv:2106.01071"
    }
  ]
}