{
  "paper_id": "2509.25458v1",
  "title": "Plug-And-Play Emotion Graphs For Compositional Prompting In Zero-Shot Speech Emotion Recognition",
  "published": "2025-09-29T20:06:03Z",
  "authors": [
    "Jiacheng Shi",
    "Hongfei Du",
    "Y. Alicia Hong",
    "Ye Gao"
  ],
  "keywords": [
    "Speech emotion recognition (SER)",
    "large audio-language models",
    "zero-shot prompting",
    "structured multimodal reasoning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large audio-language models (LALMs) exhibit strong zeroshot performance across speech tasks but struggle with speech emotion recognition (SER) due to weak paralinguistic modeling and limited cross-modal reasoning. We propose Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a framework that introduces structured Emotion Graphs (EGs) to guide LALMs in emotion inference without fine-tuning. Each EG encodes seven acoustic features (e.g., pitch, speech rate, jitter, shimmer), textual sentiment, keywords, and cross-modal associations. Embedded into prompts, EGs provide interpretable and compositional representations that enhance LALM reasoning. Experiments across SER benchmarks show that CCoT-Emo outperforms prior SOTA and improves accuracy over zero-shot baselines.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Large audio-language models (LALMs)  [1] [2] [3]  have recently demonstrated strong capabilities across diverse speechoriented tasks, including instruction-following, speech recognition, and audio-based question answering. Among these, speech emotion recognition (SER)  [4]  is critical for enabling emotionally intelligent human-computer interactions. Despite promising results, LALMs often focus on linguistic semantics and underrepresent paralinguistic cues such as pitch, speech rate, volume, jitter, shimmer, intensity, and articulation rate, all of which are crucial for emotional understanding  [5] .\n\nTo address these limitations, recent efforts have integrated Whisper-based  [6]  acoustic encoders with instruction-tuned language models. These include approaches that align ASR corpora and fine-tune models for emotion-aware generation  [5] , Whisper-BEATs dual-encoders with Q-Former and LoRAadapted backbones for multi-task instruction tuning  [7] , and joint ASR-emotion models under low-resource constraints  [8] . While effective, such methods rely on annotated corpora and task-specific training, which limits generalization.\n\nTo reduce reliance on fine-tuning and further improve models' understanding of emotional cues, prompting-based methods have emerged, leveraging chain-of-thought (CoT) reasoning to guide LALMs through explicit, step-by-step emotional inference  [9, 10] . These approaches aim to surface latent paralinguistic reasoning by prompting LALMs to interpret acoustic signals through intermediate, natural language explanations. However, due to their unstructured nature, CoT prompts are vulnerable to hallucination and error propagation. Moreover, their interpretability remains largely implicit, offering limited transparency into how emotional cues are derived from complex multimodal signals.\n\nInspired by recent advances in CoT prompting within the vision-language domain  [11] , which leverage structured representations of objects, attributes, and their relations to enable modular reasoning, we develop a zero-shot CoT approach that employs structured graph representations to enhance cross-modal reasoning and interpretability in audiolanguage emotion understanding.We introduce Compositional Chain-of-Thought Prompting for Emotion Reasoning (CCoT-Emo), a zero-shot prompting framework that equips large audio-language models (LALMs) with structured intermediate reasoning. CCoT-Emo constructs an Emotion Graph from input audio and transcripts by extracting seven acoustic features (e.g., pitch, speech rate, volume, jitter, shimmer, intensity, and articulation rate), textual sentiment, keywords, and their cross-modal associations. This graph is embedded into the prompt to serve as a compositional and interpretable reasoning trace. By replacing unstructured CoT reasoning with a symbolic graph, CCoT-Emo improves interpretability, reduces reliance on fine-tuning, and generalizes effectively across diverse speech emotion recognition benchmarks.\n\nOur contributions are as follows: (1) We introduce Compositional Chain-of-Thought for Emotion Reasoning (CCoT-Emo), a zero-shot prompting framework that leverages structured Emotion Graphs to enhance cross-modal compositional reasoning in large audio-language models. (2) We present CCoT-Emo as a plug-and-play, fine-tuning-free approach that improves interpretability and remains broadly compatible with diverse LALM architectures with minimal adaptation effort.\n\n(3) We conduct extensive evaluations showing that CCoT-Emo improves SER performance across multiple popular LALMs,",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Graph Generation",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Response Generation",
      "text": "Task: Recognize the emotion with keywords in English:\n\nEmotion Graph: {\"acoustic\": [{\"pitch\": \"high\"}, {\"volume\": \"loud\"}, {\"jitter\": \"low\"}, {\"shimmer\": \"low\"}, {\"intensity\": \"strong\"}, {\"speed rate\": \"slow\"}, {\"articulation rate\": \"fast\"}], \"text\":\n\n[{\"transcription\": \"Wow, that's amazing news!\"}, {\"sentiment\": \"positive\"}, {\"key word\": \"amazing\"}], \"relationships\":\n\n[{\"pitch\": {\"supports\": [\"sentiment\"]}}, {\"volume\": {\"supports\": [\"sentiment\"]}}, {\"rate\": {\"conflicts\": [\"sentiment\"]}}]} Fig.  1 . Overview of full CCoT-Emo. Stage 1 builds emotion graph (EG) by extracting acoustic features via digital signal processing (DSP)  [12]  and deriving textual sentiment, keywords, and cross-modal relationships using an LLM. Stage 2 prompts the LALM with the audio, EG, and task instruction to predict the emotion label. Components unique to CCoT-Emo are in bold.\n\nachieving gains of 9.1%, 8.3%, and 7.2% on Qwen2-Audio, Qwen2.5-Omni, and Kimi-Audio, respectively. It also outperforms the prior state-of-the-art method by 3.7% on average.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "Preliminaries. We study speech emotion recognition (SER) with large audio-language models (LALMs) that perform emotion reasoning over acoustic and textual inputs via language modeling. Given an audio input I and a prompt P in , the audio is encoded by a speech encoder ψ ϕ (•) and projected to discrete token-like embeddings via τ (•), while the text is tokenized by l(•). The audio and textual inputs are encoded and fused into a unified representation, which is then fed into the language model f θ (•) to generate the response R:\n\nThis token-level interface is common across instruction-tuned LALMs, regardless of specific choices for ψ ϕ or f θ .\n\nStep 1: Emotion Graph Generation. The first stage of our framework constructs an intermediate Emotion Graph E g , which explicitly represents the interaction between acoustic and textual emotional cues. This graph is built without taskspecific supervision and serves as a structured reasoning trace for downstream emotion inference. Seven acoustic features including pitch, speech rate, volume, jitter, shimmer, intensity, and articulation rate are extracted using standard digital signal processing (DSP) techniques  [12] . Specifically, we employ the openSMILE toolkit  [13]  to compute each feature over the full utterance. Each feature is then discretized into categorical labels: low, normal, or high, based on empirical value distributions computed over the corpus. These discrete labels serve as interpretable attribute tokens within the Emotion Graph, contributing diverse paralinguistic perspectives across prosodic variation, voice quality, and temporal dynamics.\n\nTextual features are extracted from the utterance transcription using a RoBERTa-based sentiment classifier, which predicts sentence-level sentiment polarity as positive, negative, or neutral. Emotionally salient keywords are additionally identified using KeyBERT  [14] , a semantic similarity-based extractor. These high-level textual representations support the construction of cross-modal relationships in the E g .\n\nCross-modal relationships are inferred using GPT-4. For example, the model is prompted with: \"Given acoustic cues (e.g., pitch: high, volume: loud, etc.), determine whether each one supports, contradicts, or is neutral with respect to the sentiment: positive.\" This enables the model to evaluate whether each acoustic cue reinforces or contradicts the emotional content of text, capturing nuanced multimodal interactions.\n\nThe final Emotion Graph E g integrates acoustic features from D audio , textual sentiment and keywords from P text , and their cross-modal relationships. All components are computed independently and serialized into a unified JSON structure, as shown in red under Response Generation in Fig.  1 . This structured representation provides a compositional and interpretable foundation for downstream emotional reasoning.\n\nStep 2: Response Generation. In the second stage, the emotion graph E g serves as a structured intermediate representa-",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Encoder-Based Classification Models",
      "text": "HuBERT-Large  [15]  64 tion for zero-shot emotion prediction. The model receives the audio input I and a prompt P out composed of and reasoning components. This prompt concatenates five elements, as illustrated in Figure  1 , enabling multimodal emotional inference without parameter updates.\n\nHere, I is a symbolic reference to the raw audio input, E g is the structured emotion graph generated in the previous stage, and C is a contextual instruction that guides the model to reason over multimodal content: \"Use the audio and emotion graph as context and answer the following question.\" The task-specific prompt P in specifies the SER objective: \"Task: Recognize the emotion with keywords in English: (A) Neutral (B) Happy (C) Sad (D) Surprised (E) Angry.\" To enforce format consistency, we append an output instruction S: \"Select the option letter from the provided choices to answer.\" This explicit formatting constraint reduces output ambiguity and is inspired by prompting strategies in LLaVA  [20] , though it remains adaptable to other generation formats. The final response R is generated by the model through joint reasoning over audio and text inputs. The raw audio input is encoded by a speech encoder ψ ϕ (I) and projected into discrete token-like representations via τ (•), while the textual prompt is embedded using a language tokenizer l(•):",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "Datasets and Models. We evaluate CCoT-Emo on four benchmarks: IEMOCAP, MELD, ESD, and MERBench. IEMOCAP contains 5,531 English utterances from five dyadic sessions;\n\nwe follow  [5]  in merging excited and happy, using 5-fold leaveone-session-out cross-validation. MELD comprises 8,851 English utterances from the Friends TV series labeled with seven emotions; we follow  [5]  in removing disgust and fear. ESD offers 33,443 utterances in English and Chinese across five emotions and diverse speakers. MERBench test1 and test2 are small Chinese subsets ( 350 utterances each) from movies and TV; we exclude the worried label following  [5] . As ESD and MERBench lack transcripts, we use Whisper for automatic transcription. Since CCoT-Emo is training-free, evaluations are conducted directly on test sets. We apply our method to Qwen2-Audio  [1] , Qwen2.5-Omni  [2] , and Kimi-Audio  [3] . Baselines. To evaluate the effectiveness of CCoT-Emo, we compare it with two zero-shot baselines. The first is direct prompting, where each LALM receives only a minimal task instruction without structured cues or intermediate reasoning, reflecting raw zero-shot performance. The second is Chain-of-Thought (CoT) prompting  [21] , where the model first generates intermediate emotional reasoning given the audio and a reasoning trigger (e.g., \"Let's think step by step\"), followed by a second-stage prompt combining the audio, task, and generated rationale for final prediction. We adopt the two-stage prompting setup from LLaVA  [20] , which has demonstrated improved consistency in multimodal tasks. These baselines isolate the impact of our graph-based compositional prompting.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Main Results",
      "text": "Speech Emotion Recognition. We evaluate the zero-shot performance of CCoT-Emo on five emotion recognition benchmarks using three LALMs: Qwen2-Audio, Qwen2.5-Omni, and Kimi-Audio. As shown in Table Replacing DSP-derived symbolic acoustic features with spectrograms yields an additional 5.9% drop, indicating that continuous and uninterpretable representations lack the structured abstraction required for zero-shot symbolic reasoning. Effect of LALM size. We compare CCoT-Emo on Qwen2.5-Omni 3B and 7B parameters. The 3B variant improves SER accuracy by 3.9%, while the 7B version achieves an 8.3% gain. This trend suggests that larger models better exploit structured intermediate representations for emotion reasoning. Effect of Emotion Graph size. We examine the effect of emotion graph length by varying token limits from 128 to 1024. A 256-token configuration yields the highest accuracy, while both shorter (128: -0.4%) and longer variants (512: -0.7%, 1024: -0.9%) degrade performance. These results indicate that compact, well-balanced emotion graphs offer optimal guidance for zero-shot multimodal reasoning.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "We introduce CCoT-Emo, a structured zero-shot prompting framework that enhances emotion reasoning in LALMs through intermediate Emotion Graphs encoding acoustic, textual, and cross-modal cues. Without fine-tuning or taskspecific supervision, CCoT-Emo consistently improves SER performance across five benchmarks. These results underscore the value of symbolic and interpretable representations for guiding multimodal emotional inference in large audio models.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of full CCoT-Emo. Stage 1 builds emotion graph (EG) by extracting acoustic features via digital signal",
      "page": 2
    },
    {
      "caption": "Figure 1: , enabling multimodal emotional infer-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "ABSTRACT",
          "yhong22@gmu.edu": "To reduce reliance on fine-tuning and further\nimprove"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "models’ understanding of emotional cues, prompting-based"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "Large audio-language models (LALMs) exhibit strong zero-",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "methods have emerged,\nleveraging chain-of-thought\n(CoT)"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "shot performance across speech tasks but struggle with speech",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "reasoning to guide LALMs through explicit, step-by-step emo-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "emotion recognition (SER) due to weak paralinguistic model-",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "tional inference [9,10]. These approaches aim to surface latent"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "ing and limited cross-modal reasoning. We propose Compo-",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "paralinguistic reasoning by prompting LALMs to interpret"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "sitional Chain-of-Thought Prompting for Emotion Reasoning",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "acoustic signals through intermediate, natural\nlanguage ex-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "(CCoT-Emo), a framework that\nintroduces structured Emo-",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "planations. However, due to their unstructured nature, CoT"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "tion Graphs\n(EGs)\nto guide LALMs\nin emotion inference",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "prompts are vulnerable to hallucination and error propagation."
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "without fine-tuning.\nEach EG encodes seven acoustic fea-",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "Moreover, their interpretability remains largely implicit, offer-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "tures (e.g., pitch, speech rate,\njitter, shimmer),\ntextual senti-",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "ing limited transparency into how emotional cues are derived"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "ment, keywords, and cross-modal associations. Embedded",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "from complex multimodal signals."
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "into prompts, EGs provide interpretable and compositional",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "Inspired by recent advances\nin CoT prompting within"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "representations that enhance LALM reasoning. Experiments",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "the vision-language domain [11], which leverage structured"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "across SER benchmarks show that CCoT-Emo outperforms",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "representations of objects, attributes, and their\nrelations to"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "prior SOTA and improves accuracy over zero-shot baselines.",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "enable modular reasoning, we develop a zero-shot CoT ap-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "proach that employs structured graph representations to en-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "Index Terms— Speech emotion recognition (SER), large",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "hance cross-modal\nreasoning and interpretability in audio-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "audio-language models, zero-shot prompting, structured multi-",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "language emotion understanding.We introduce Compositional"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "modal reasoning.",
          "yhong22@gmu.edu": ""
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "Chain-of-Thought Prompting for Emotion Reasoning (CCoT-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "Emo), a zero-shot prompting framework that equips large"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "1.\nINTRODUCTION",
          "yhong22@gmu.edu": "audio-language models (LALMs) with structured intermediate"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "",
          "yhong22@gmu.edu": "reasoning. CCoT-Emo constructs an Emotion Graph from in-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "Large audio-language models (LALMs) [1–3] have recently",
          "yhong22@gmu.edu": "put audio and transcripts by extracting seven acoustic features"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "demonstrated\nstrong\ncapabilities\nacross\ndiverse\nspeech-",
          "yhong22@gmu.edu": "(e.g., pitch, speech rate, volume, jitter, shimmer, intensity,"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "oriented tasks, including instruction-following, speech recog-",
          "yhong22@gmu.edu": "and articulation rate), textual sentiment, keywords, and"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "nition, and audio-based question answering. Among these,",
          "yhong22@gmu.edu": "their cross-modal associations. This graph is embedded into"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "speech emotion recognition (SER) [4] is critical for enabling",
          "yhong22@gmu.edu": "the prompt to serve as a compositional and interpretable rea-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "emotionally intelligent human-computer interactions. Despite",
          "yhong22@gmu.edu": "soning trace. By replacing unstructured CoT reasoning with a"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "promising results, LALMs often focus on linguistic semantics",
          "yhong22@gmu.edu": "symbolic graph, CCoT-Emo improves interpretability, reduces"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "and underrepresent paralinguistic cues such as pitch, speech",
          "yhong22@gmu.edu": "reliance on fine-tuning, and generalizes effectively across di-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "rate, volume,\njitter, shimmer,\nintensity, and articulation rate,",
          "yhong22@gmu.edu": "verse speech emotion recognition benchmarks."
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "all of which are crucial for emotional understanding [5].",
          "yhong22@gmu.edu": "Our contributions are as follows: (1) We introduce Com-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "To address these limitations, recent efforts have integrated",
          "yhong22@gmu.edu": "positional Chain-of-Thought for Emotion Reasoning (CCoT-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "Whisper-based [6] acoustic encoders with instruction-tuned",
          "yhong22@gmu.edu": "Emo), a zero-shot prompting framework that leverages struc-"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "language models. These include approaches that align ASR",
          "yhong22@gmu.edu": "tured Emotion Graphs to enhance cross-modal compositional"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "corpora and fine-tune models for emotion-aware generation [5],",
          "yhong22@gmu.edu": "reasoning in large audio-language models.\n(2) We present"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "Whisper-BEATs dual-encoders with Q-Former and LoRA-",
          "yhong22@gmu.edu": "CCoT-Emo as a plug-and-play, fine-tuning-free approach that"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "adapted backbones for multi-task instruction tuning [7], and",
          "yhong22@gmu.edu": "improves interpretability and remains broadly compatible with"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "joint ASR-emotion models under low-resource constraints [8].",
          "yhong22@gmu.edu": "diverse LALM architectures with minimal adaptation effort."
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "While effective, such methods rely on annotated corpora and",
          "yhong22@gmu.edu": "(3) We conduct extensive evaluations showing that CCoT-Emo"
        },
        {
          "{jshi12,\nhdu02,\nygao18}@wm.edu,": "task-specific training, which limits generalization.",
          "yhong22@gmu.edu": "improves SER performance across multiple popular LALMs,"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "“Wow, that's amazing news!”"
        },
        {
          "1. Emotion Graph Generation": "“Wow, that's amazing news!”\nI",
          "2. Response Generation": ""
        },
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "Emotion Graph:"
        },
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "{“acoustic”:"
        },
        {
          "1. Emotion Graph Generation": "Task: Recognize the emotion with keywords in English:",
          "2. Response Generation": "[{“pitch”: “high”}, {“volume”: “loud”}, {\"jitter\": \"low\"},"
        },
        {
          "1. Emotion Graph Generation": "(A) Neutral (B) Happy (C) Sad (D) Surprised (E) Angry \nPin",
          "2. Response Generation": "{“shimmer”: “low”}, {\"intensity\": \"strong\"},"
        },
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "{“speed rate”: “slow”}, {\"articulation rate\": \"fast\"}],"
        },
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "\"text\":"
        },
        {
          "1. Emotion Graph Generation": "For an audio and its associated  question, generate",
          "2. Response Generation": "[{\"transcription\": \"Wow, that's amazing news!\"},"
        },
        {
          "1. Emotion Graph Generation": "an emotion graph in JSON format with these",
          "2. Response Generation": "{\"sentiment\": \"positive\"}, {\"key word\": \"amazing\"}],"
        },
        {
          "1. Emotion Graph Generation": "elements:",
          "2. Response Generation": "“relationships”:"
        },
        {
          "1. Emotion Graph Generation": "1. Acoustic features: pitch, speech rate, and volume.",
          "2. Response Generation": "[{\"pitch\": {\"supports\": [\"sentiment\"]}},"
        },
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "{\"volume\": {\"supports\": [\"sentiment\"]}},"
        },
        {
          "1. Emotion Graph Generation": "2. Transcribed text, its sentiment, and textual",
          "2. Response Generation": ""
        },
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "{\"rate\": {\"conflicts\": [\"sentiment\"]}}]}"
        },
        {
          "1. Emotion Graph Generation": "keyword\nEin",
          "2. Response Generation": ""
        },
        {
          "1. Emotion Graph Generation": "3. Relationships between acoustic features and",
          "2. Response Generation": ""
        },
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "Given the audio and emotion graph, answer the question:"
        },
        {
          "1. Emotion Graph Generation": "textual sentiment, indicating whether they support",
          "2. Response Generation": ""
        },
        {
          "1. Emotion Graph Generation": "or conflict emotional expression.",
          "2. Response Generation": "Task: Recognize the emotion with keywords in English:"
        },
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "(A) Neutral (B) Happy (C) Sad (D) Surprised (E) Angry"
        },
        {
          "1. Emotion Graph Generation": "Emotion Graph",
          "2. Response Generation": ""
        },
        {
          "1. Emotion Graph Generation": "",
          "2. Response Generation": "S Select the option letter from the provided choices to answer"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Response": "R  = “B”\nLALM"
        },
        {
          "Response": ""
        },
        {
          "Response": ""
        },
        {
          "Response": "the LALM with the audio, EG, and task instruction to predict the emotion label. Components unique to CCoT-Emo are in bold."
        },
        {
          "Response": "toolkit [13] to compute each feature over the full utterance."
        },
        {
          "Response": "Each feature is then discretized into categorical\nlabels:\nlow,"
        },
        {
          "Response": "normal,\nor\nhigh,\nbased\non\nempirical\nvalue\ndistributions"
        },
        {
          "Response": "computed over\nthe corpus.\nThese discrete labels serve as"
        },
        {
          "Response": "interpretable attribute tokens within the Emotion Graph, con-"
        },
        {
          "Response": ""
        },
        {
          "Response": "tributing diverse paralinguistic perspectives across prosodic"
        },
        {
          "Response": "variation, voice quality, and temporal dynamics."
        },
        {
          "Response": ""
        },
        {
          "Response": "Textual features are extracted from the utterance transcrip-"
        },
        {
          "Response": ""
        },
        {
          "Response": "tion using a RoBERTa-based sentiment classifier, which pre-"
        },
        {
          "Response": ""
        },
        {
          "Response": "dicts sentence-level sentiment polarity as positive, negative, or"
        },
        {
          "Response": ""
        },
        {
          "Response": "neutral. Emotionally salient keywords are additionally iden-"
        },
        {
          "Response": ""
        },
        {
          "Response": "tified using KeyBERT [14], a semantic similarity-based ex-"
        },
        {
          "Response": ""
        },
        {
          "Response": "tractor. These high-level\ntextual representations support\nthe"
        },
        {
          "Response": ""
        },
        {
          "Response": "construction of cross-modal relationships in the Eg."
        },
        {
          "Response": ""
        },
        {
          "Response": "Cross-modal relationships are inferred using GPT-4. For"
        },
        {
          "Response": ""
        },
        {
          "Response": "example, the model is prompted with: “Given acoustic cues"
        },
        {
          "Response": "(e.g., pitch: high, volume: loud, etc.), determine whether each"
        },
        {
          "Response": ""
        },
        {
          "Response": "one supports, contradicts, or is neutral with respect to the sen-"
        },
        {
          "Response": "timent: positive.” This enables the model to evaluate whether"
        },
        {
          "Response": "each acoustic cue reinforces or contradicts the emotional con-"
        },
        {
          "Response": "tent of text, capturing nuanced multimodal interactions."
        },
        {
          "Response": "The final Emotion Graph Eg integrates acoustic features"
        },
        {
          "Response": "from Daudio, textual sentiment and keywords from Ptext, and"
        },
        {
          "Response": "their cross-modal relationships. All components are computed"
        },
        {
          "Response": "independently and serialized into a unified JSON structure,"
        },
        {
          "Response": "as shown in red under Response Generation in Fig. 1. This"
        },
        {
          "Response": "structured representation provides a compositional and inter-"
        },
        {
          "Response": "pretable foundation for downstream emotional reasoning."
        },
        {
          "Response": "Step 2: Response Generation. In the second stage, the emo-"
        },
        {
          "Response": "tion graph Eg serves as a structured intermediate representa-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: , CCoT-Emo consis-",
      "data": [
        {
          "Speech Emotion Recognition (Acc%)": "MELD"
        },
        {
          "Speech Emotion Recognition (Acc%)": ""
        },
        {
          "Speech Emotion Recognition (Acc%)": "53.2"
        },
        {
          "Speech Emotion Recognition (Acc%)": "54.8"
        },
        {
          "Speech Emotion Recognition (Acc%)": "54.6"
        },
        {
          "Speech Emotion Recognition (Acc%)": "55.2"
        },
        {
          "Speech Emotion Recognition (Acc%)": ""
        },
        {
          "Speech Emotion Recognition (Acc%)": "54.0"
        },
        {
          "Speech Emotion Recognition (Acc%)": "53.8"
        },
        {
          "Speech Emotion Recognition (Acc%)": "53.1"
        },
        {
          "Speech Emotion Recognition (Acc%)": "32.9"
        },
        {
          "Speech Emotion Recognition (Acc%)": "54.7"
        },
        {
          "Speech Emotion Recognition (Acc%)": "57.2"
        },
        {
          "Speech Emotion Recognition (Acc%)": "55.5"
        },
        {
          "Speech Emotion Recognition (Acc%)": "56.3"
        },
        {
          "Speech Emotion Recognition (Acc%)": "61.3(+6.2)"
        },
        {
          "Speech Emotion Recognition (Acc%)": "57.0"
        },
        {
          "Speech Emotion Recognition (Acc%)": "57.9"
        },
        {
          "Speech Emotion Recognition (Acc%)": "63.4(+6.4)"
        },
        {
          "Speech Emotion Recognition (Acc%)": "59.1"
        },
        {
          "Speech Emotion Recognition (Acc%)": "60.8"
        },
        {
          "Speech Emotion Recognition (Acc%)": "64.9(+5.5)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , CCoT-Emo consis-",
      "data": [
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "Kimi-Audio-ZS-CoT\n69.4\n60.8\n67.6\n47.1\n46.6\n58.3"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "Kimi-Audio-CCoT-Emo\n78.4(+6.9)\n64.9(+5.5)\n76.6(+7.1)\n63.4(+8.5)\n56.4(+7.5)\n67.7(+7.2)"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "Table 1.\nZero-shot speech emotion recognition (SER) accuracy (%) across five benchmark datasets: IEMOCAP, MELD, ESD,"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "MERBench test1, and MERBench test2. Results are reported for both encoder-based classification models and LLM-based"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "generative models. Bold indicates the best performance on each dataset, while underline indicates the second-best result."
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "tion for zero-shot emotion prediction. The model receives the\nwe follow [5] in merging excited and happy, using 5-fold leave-"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "one-session-out cross-validation. MELD comprises 8,851 En-\naudio input I and a prompt Pout composed of task and reason-"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "ing components. This prompt concatenates five elements, as\nglish utterances from the Friends TV series labeled with seven"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "illustrated in Figure 1, enabling multimodal emotional infer-\nemotions; we follow [5] in removing disgust and fear. ESD"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "ence without parameter updates.\noffers 33,443 utterances in English and Chinese across five"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "emotions and diverse speakers. MERBench test1 and test2 are"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "(2)\nPout = [I][Eg][C][Pin][S]"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "small Chinese subsets ( 350 utterances each) from movies and"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "TV; we exclude the worried label following [5]. As ESD and"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "Here, I is a symbolic reference to the raw audio input, Eg is"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "MERBench lack transcripts, we use Whisper for automatic"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "the structured emotion graph generated in the previous stage,"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "transcription. Since CCoT-Emo is training-free, evaluations"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "and C is a contextual\ninstruction that guides the model\nto"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "are conducted directly on test sets. We apply our method to"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "reason over multimodal content: \"Use the audio and emotion"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "Qwen2-Audio [1], Qwen2.5-Omni [2], and Kimi-Audio [3]."
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "graph as context and answer the following question.\" The"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "Baselines. To evaluate the effectiveness of CCoT-Emo, we"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "task-specific prompt Pin specifies the SER objective: \"Task:"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "compare it with two zero-shot baselines. The first\nis direct"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "Recognize the emotion with keywords in English: (A) Neutral"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "prompting, where each LALM receives only a minimal task"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "(B) Happy (C) Sad (D) Surprised (E) Angry.\" To enforce format"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "instruction without structured cues or intermediate reasoning,"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "the\nconsistency, we append an output\ninstruction S: \"Select"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "reflecting raw zero-shot performance. The second is Chain-of-"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "option letter from the provided choices to answer.\" This explicit"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "Thought (CoT) prompting [21], where the model first generates"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "formatting constraint reduces output ambiguity and is inspired"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "intermediate emotional reasoning given the audio and a rea-"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "by prompting strategies in LLaVA [20],\nthough it\nremains"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "soning trigger (e.g., “Let’s think step by step”), followed by a"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "adaptable to other generation formats.\nThe final\nresponse"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "second-stage prompt combining the audio, task, and generated"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "R is generated by the model\nthrough joint\nreasoning over"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "rationale for final prediction. We adopt the two-stage prompt-"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "audio and text\ninputs. The raw audio input\nis encoded by a"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "ing setup from LLaVA [20], which has demonstrated improved"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "speech encoder ψϕ(I) and projected into discrete token-like"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "consistency in multimodal tasks. These baselines isolate the"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "representations via τ (·), while the textual prompt is embedded"
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "impact of our graph-based compositional prompting."
        },
        {
          "Kimi-Audio [3]\n71.5\n59.1\n69.5\n53.7\n48.9\n60.5": "using a language tokenizer l(·):"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "Qwen2.5-Omni-7B-CCoT-Emo",
          "IEMOCAP": "71.2",
          "MELD": "63.4",
          "ESD": "74.2",
          "MERBench test1": "61.6",
          "MERBench test2": "54.9",
          "Overall": "65.3"
        },
        {
          "Model": "Qwen2.5-Omni-7B",
          "IEMOCAP": "64.7",
          "MELD": "57.0",
          "ESD": "59.3",
          "MERBench test1": "54.3",
          "MERBench test2": "50.1",
          "Overall": "57.0"
        },
        {
          "Model": "w/out Acoustic Attribute",
          "IEMOCAP": "69.0",
          "MELD": "61.1",
          "ESD": "69.2",
          "MERBench test1": "59.0",
          "MERBench test2": "53.3",
          "Overall": "62.4"
        },
        {
          "Model": "w/out Text Attribute",
          "IEMOCAP": "68.1",
          "MELD": "60.7",
          "ESD": "66.8",
          "MERBench test1": "58.3",
          "MERBench test2": "52.7",
          "Overall": "61.4"
        },
        {
          "Model": "w/out Cross-Modal Relationship",
          "IEMOCAP": "70.0",
          "MELD": "61.9",
          "ESD": "71.8",
          "MERBench test1": "60.2",
          "MERBench test2": "53.9",
          "Overall": "63.6"
        },
        {
          "Model": "w/ spectrograms featrue",
          "IEMOCAP": "67.6",
          "MELD": "59.8",
          "ESD": "63.5",
          "MERBench test1": "56.1",
          "MERBench test2": "50.4",
          "Overall": "59.4"
        },
        {
          "Model": "w/ LALMs generate acoustic attribute",
          "IEMOCAP": "70.2",
          "MELD": "62.3",
          "ESD": "71.5",
          "MERBench test1": "58.7",
          "MERBench test2": "52.3",
          "Overall": "63.1"
        },
        {
          "Model": "w/out JSON Format",
          "IEMOCAP": "70.1",
          "MELD": "62.2",
          "ESD": "73.0",
          "MERBench test1": "59.1",
          "MERBench test2": "54.8",
          "Overall": "63.9"
        },
        {
          "Model": "Qwen-2.5-Omni-7B with free-form CoT",
          "IEMOCAP": "70.9",
          "MELD": "62.3",
          "ESD": "71.5",
          "MERBench test1": "59.4",
          "MERBench test2": "52.9",
          "Overall": "63.4"
        },
        {
          "Model": "Qwen-2.5-Omni-3B",
          "IEMOCAP": "55.6",
          "MELD": "45.3",
          "ESD": "47.8",
          "MERBench test1": "45.9",
          "MERBench test2": "37.4",
          "Overall": "46.6"
        },
        {
          "Model": "Qwen-2.5-Omni-3B-CCoT-Emo",
          "IEMOCAP": "59.8",
          "MELD": "51.1",
          "ESD": "53.0",
          "MERBench test1": "50.2",
          "MERBench test2": "40.8",
          "Overall": "50.9"
        },
        {
          "Model": "128 Token Length",
          "IEMOCAP": "71.5",
          "MELD": "63.6",
          "ESD": "73.8",
          "MERBench test1": "61.0",
          "MERBench test2": "54.5",
          "Overall": "64.9"
        },
        {
          "Model": "512 Token Length",
          "IEMOCAP": "70.7",
          "MELD": "62.6",
          "ESD": "74.0",
          "MERBench test1": "61.2",
          "MERBench test2": "54.3",
          "Overall": "64.6"
        },
        {
          "Model": "1024 Token Length",
          "IEMOCAP": "70.4",
          "MELD": "62.5",
          "ESD": "74.1",
          "MERBench test1": "61.2",
          "MERBench test2": "54.2",
          "Overall": "64.4"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "128 Token Length\n71.5\n63.6": "512 Token Length\n70.7\n62.6",
          "73.8\n61.0\n54.5\n64.9": "74.0\n61.2\n54.3\n64.6"
        },
        {
          "128 Token Length\n71.5\n63.6": "1024 Token Length\n70.4\n62.5",
          "73.8\n61.0\n54.5\n64.9": "74.1\n61.2\n54.2\n64.4"
        },
        {
          "128 Token Length\n71.5\n63.6": "Table 2. SER accuracy (%) for CCoT-Emo ablations on five benchmarks. Each row reports the effect of removing individual",
          "73.8\n61.0\n54.5\n64.9": ""
        },
        {
          "128 Token Length\n71.5\n63.6": "components or modifying prompt/model configurations. Overall denotes the average accuracy across datasets.",
          "73.8\n61.0\n54.5\n64.9": ""
        },
        {
          "128 Token Length\n71.5\n63.6": "of-Thought (ZS-CoT) baselines across all models. Kimi-Audio",
          "73.8\n61.0\n54.5\n64.9": "gests that high-level semantic pretraining alone is insufficient"
        },
        {
          "128 Token Length\n71.5\n63.6": "with CCoT-Emo achieves the highest average accuracy of",
          "73.8\n61.0\n54.5\n64.9": "for capturing low-level acoustic detail, reaffirming the value"
        },
        {
          "128 Token Length\n71.5\n63.6": "67.7%, surpassing vanilla prompting by +7.2% and ZS-CoT by",
          "73.8\n61.0\n54.5\n64.9": "of deterministic and interpretable DSP features for reliable"
        },
        {
          "128 Token Length\n71.5\n63.6": "+9.4%, and improving over the previous state-of-the-art BLSP-",
          "73.8\n61.0\n54.5\n64.9": "zero-shot emotion inference."
        },
        {
          "128 Token Length\n71.5\n63.6": "Emo by 3.7%. Notably, on MERBench test1 and test2, it yields",
          "73.8\n61.0\n54.5\n64.9": "Component-wise ablation in Emotion Graph. We ablate"
        },
        {
          "128 Token Length\n71.5\n63.6": "+8.5% and +7.5% gains, respectively,\nindicating strong gen-",
          "73.8\n61.0\n54.5\n64.9": "each component of the emotion graph to quantify its contri-"
        },
        {
          "128 Token Length\n71.5\n63.6": "eralization to domain-shifted and long-form speech. Dataset-",
          "73.8\n61.0\n54.5\n64.9": "bution.\nRemoving cross-modal\nrelationships reduces SER"
        },
        {
          "128 Token Length\n71.5\n63.6": "specific baselines like BLSP-Emo and OSUM remain compet-",
          "73.8\n61.0\n54.5\n64.9": "accuracy by 1.7%, suggesting their role in aligning acoustic"
        },
        {
          "128 Token Length\n71.5\n63.6": "itive on IEMOCAP and MERBench, respectively, due to in-",
          "73.8\n61.0\n54.5\n64.9": "and textual signals. Excluding acoustic attributes (e.g., pitch,"
        },
        {
          "128 Token Length\n71.5\n63.6": "domain training and language-specific pretraining. CCoT-Emo",
          "73.8\n61.0\n54.5\n64.9": "volume, speech rate, etc.)\nresults in a 2.9% drop, highlight-"
        },
        {
          "128 Token Length\n71.5\n63.6": "also enhances Qwen2-Audio and Qwen2.5-Omni by +6.1%",
          "73.8\n61.0\n54.5\n64.9": "ing their prosodic relevance. The largest degradation (3.9%)"
        },
        {
          "128 Token Length\n71.5\n63.6": "and +8.3%, respectively. On ESD, Qwen2-Audio-CCoT-Emo",
          "73.8\n61.0\n54.5\n64.9": "occurs when textual attributes (sentiment, keywords) are re-"
        },
        {
          "128 Token Length\n71.5\n63.6": "reaches 71.6%, a +14.5% absolute improvement, underscoring",
          "73.8\n61.0\n54.5\n64.9": "moved, confirming their importance as direct semantic cues."
        },
        {
          "128 Token Length\n71.5\n63.6": "the benefit of structured audio-text conditioning. In contrast,",
          "73.8\n61.0\n54.5\n64.9": "Replacing DSP-derived symbolic acoustic features with spec-"
        },
        {
          "128 Token Length\n71.5\n63.6": "ZS-CoT underperforms across all settings,\nlikely due to ir-",
          "73.8\n61.0\n54.5\n64.9": "trograms yields an additional 5.9% drop, indicating that con-"
        },
        {
          "128 Token Length\n71.5\n63.6": "relevant intermediate reasoning disrupting emotion inference,",
          "73.8\n61.0\n54.5\n64.9": "tinuous and uninterpretable representations lack the structured"
        },
        {
          "128 Token Length\n71.5\n63.6": "particularly in datasets with long-range context. These results",
          "73.8\n61.0\n54.5\n64.9": "abstraction required for zero-shot symbolic reasoning."
        },
        {
          "128 Token Length\n71.5\n63.6": "demonstrate CCoT-Emo’s effectiveness in improving zero-shot",
          "73.8\n61.0\n54.5\n64.9": "Effect of LALM size. We compare CCoT-Emo on Qwen2.5-"
        },
        {
          "128 Token Length\n71.5\n63.6": "compositional reasoning in LALMs.",
          "73.8\n61.0\n54.5\n64.9": "Omni 3B and 7B parameters. The 3B variant improves SER"
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "accuracy by 3.9%, while the 7B version achieves an 8.3% gain."
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "This trend suggests that larger models better exploit structured"
        },
        {
          "128 Token Length\n71.5\n63.6": "5. ABLATION STUDIES",
          "73.8\n61.0\n54.5\n64.9": ""
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "intermediate representations for emotion reasoning."
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "Effect of Emotion Graph size. We examine the effect of"
        },
        {
          "128 Token Length\n71.5\n63.6": "JSON structure enhances EG utilization. To assess the im-",
          "73.8\n61.0\n54.5\n64.9": ""
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "emotion graph length by varying token limits from 128 to"
        },
        {
          "128 Token Length\n71.5\n63.6": "pact of representation format, we replace the structured JSON-",
          "73.8\n61.0\n54.5\n64.9": ""
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "1024. A 256-token configuration yields the highest accuracy,"
        },
        {
          "128 Token Length\n71.5\n63.6": "based Emotion Graph with an unstructured variant. This yields",
          "73.8\n61.0\n54.5\n64.9": ""
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "while both shorter\n(128:\n–0.4%) and longer variants (512:"
        },
        {
          "128 Token Length\n71.5\n63.6": "a 1.4% accuracy drop, suggesting that standardized formatting",
          "73.8\n61.0\n54.5\n64.9": ""
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "–0.7%, 1024:\n–0.9%) degrade performance.\nThese results"
        },
        {
          "128 Token Length\n71.5\n63.6": "enhances model interpretability and emotional inference.",
          "73.8\n61.0\n54.5\n64.9": ""
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "indicate that compact, well-balanced emotion graphs offer"
        },
        {
          "128 Token Length\n71.5\n63.6": "Replacing EGs with free-form CoT. To probe the effect",
          "73.8\n61.0\n54.5\n64.9": ""
        },
        {
          "128 Token Length\n71.5\n63.6": "",
          "73.8\n61.0\n54.5\n64.9": "optimal guidance for zero-shot multimodal reasoning."
        },
        {
          "128 Token Length\n71.5\n63.6": "of structural abstraction, we replace Emotion Graphs with",
          "73.8\n61.0\n54.5\n64.9": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "Pearson Education India, 1999."
        },
        {
          "7. REFERENCES": "[1] Yunfei Chu, Jin Xu, Qian Yang, Haojie Wei, Xipin Wei,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[13] Florian Eyben, Martin Wöllmer,\nand Björn Schuller,"
        },
        {
          "7. REFERENCES": "Zhifang Guo, Yichong Leng, Yuanjun Lv, Jinzheng He,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "“Opensmile: The munich versatile and fast open-source"
        },
        {
          "7. REFERENCES": "Junyang Lin, et al.,\n“Qwen2-audio technical\nreport,”",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "audio feature extractor,” in Proceedings of the 18th ACM"
        },
        {
          "7. REFERENCES": "arXiv preprint arXiv:2407.10759, 2024.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "international conference on Multimedia, 2010."
        },
        {
          "7. REFERENCES": "Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting\n[2]",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[14] Maarten Grootendorst,\n“Keybert: Minimal keyword"
        },
        {
          "7. REFERENCES": "Dang, et al., “Qwen2. 5-omni technical report,” arXiv",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "extraction with bert.,” 2020."
        },
        {
          "7. REFERENCES": "preprint arXiv:2503.20215, 2025.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[15] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,"
        },
        {
          "7. REFERENCES": "[3] Ding Ding, Zeqian Ju, Yichong Leng, Songxiang Liu,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrah-"
        },
        {
          "7. REFERENCES": "Tong Liu, Zeyu Shang, Kai Shen, Wei Song, Xu Tan,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "man Mohamed, “Hubert: Self-supervised speech repre-"
        },
        {
          "7. REFERENCES": "Heyi Tang, et al., “Kimi-audio technical report,” arXiv",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "sentation learning by masked prediction of hidden units,”"
        },
        {
          "7. REFERENCES": "preprint arXiv:2504.18425, 2025.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "IEEE/ACM transactions on audio, speech, and language"
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "processing, vol. 29, pp. 3451–3460, 2021."
        },
        {
          "7. REFERENCES": "[4] Moataz El Ayadi, Mohamed S Kamel, and Fakhri Karray,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "“Survey on speech emotion recognition: Features, clas-",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[16] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,"
        },
        {
          "7. REFERENCES": "sification schemes, and databases,” Pattern recognition,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "and Michael Auli, “wav2vec 2.0: A framework for self-"
        },
        {
          "7. REFERENCES": "vol. 44, no. 3, pp. 572–587, 2011.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "supervised learning of speech representations,” Advances"
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "in neural\ninformation processing systems, vol. 33, pp."
        },
        {
          "7. REFERENCES": "[5] Chen Wang, Minpeng Liao, Zhongqiang Huang, Junhong",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "12449–12460, 2020."
        },
        {
          "7. REFERENCES": "Wu, Chengqing Zong, and Jiajun Zhang, “Blsp-emo: To-",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "wards empathetic large speech-language models,” arXiv",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[17] Shu-wen Chen\net\nal.,\n“Wavlm:\nLarge-scale\nself-"
        },
        {
          "7. REFERENCES": "preprint arXiv:2406.03872, 2024.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "supervised pre-training for full stack speech processing,”"
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "IEEE Journal of Selected Topics in Signal Processing,"
        },
        {
          "7. REFERENCES": "[6] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "2022."
        },
        {
          "7. REFERENCES": "Christine McLeavey, and Ilya Sutskever, “Robust speech",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "recognition via large-scale weak supervision,” in Inter-",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[18]\nJiacheng Shi, Yanfu Zhang, and Ye Gao, “Clep-dg: Con-"
        },
        {
          "7. REFERENCES": "national conference on machine learning. PMLR, 2023,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "trastive learning for speech emotion domain generaliza-"
        },
        {
          "7. REFERENCES": "pp. 28492–28518.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "tion via soft prompt tuning,” in Proc. Interspeech 2025,"
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "2025, pp. 4498–4502."
        },
        {
          "7. REFERENCES": "[7] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang,",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[19] Keyu An,\nQian Chen,\nChong Deng,\nZhihao Du,"
        },
        {
          "7. REFERENCES": "“Salmonn: Towards generic hearing abilities for large",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "Changfeng Gao, Zhifu Gao, Yue Gu, Ting He, Hangrui"
        },
        {
          "7. REFERENCES": "language models,” arXiv:2310.13289, 2023.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "Hu, Kai Hu, et al.,\n“Funaudiollm: Voice understand-"
        },
        {
          "7. REFERENCES": "[8] Xuelong Geng, Kun Wei, Qijie Shao, Shuiyun Liu, Zhen-",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "ing and generation foundation models\nfor natural\nin-"
        },
        {
          "7. REFERENCES": "nan Lin, Zhixian Zhao, Guojian Li, Wenjie Tian, Peikun",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "arXiv preprint\nteraction between humans and llms,”"
        },
        {
          "7. REFERENCES": "Chen, Yangze Li,\net al.,\n“Osum: Advancing open",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "arXiv:2407.04051, 2024."
        },
        {
          "7. REFERENCES": "speech understanding models with limited resources in",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[20] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee,"
        },
        {
          "7. REFERENCES": "academia,” arXiv preprint arXiv:2501.13306, 2025.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "“Improved baselines with visual instruction tuning,” in"
        },
        {
          "7. REFERENCES": "John Murzaku and Owen Rambow,\n“Omnivox: Zero-\n[9]",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "Proceedings of the IEEE/CVF Conference on Computer"
        },
        {
          "7. REFERENCES": "shot emotion recognition with omni-llms,” arXiv preprint",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "Vision and Pattern Recognition, 2024, pp. 26296–26306."
        },
        {
          "7. REFERENCES": "arXiv:2503.21480, 2025.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[21] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-"
        },
        {
          "7. REFERENCES": "[10] Zhixian Zhao, Xinfa Zhu, Xinsheng Wang, Shuiyuan",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "taka Matsuo, and Yusuke Iwasawa,\n“Large language"
        },
        {
          "7. REFERENCES": "Wang, Xuelong Geng, Wenjie Tian, and Lei Xie, “Steer-",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "Advances in neural\nmodels are zero-shot\nreasoners,”"
        },
        {
          "7. REFERENCES": "ing language model to stable speech emotion recognition",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "information processing systems, vol. 35, 2022."
        },
        {
          "7. REFERENCES": "via contextual perception and chain of thought,” arXiv",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "[22] Subhabrata Dutta, Joykirat Singh, Soumen Chakrabarti,"
        },
        {
          "7. REFERENCES": "preprint arXiv:2502.18186, 2025.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "and Tanmoy Chakraborty, “How to think step-by-step:"
        },
        {
          "7. REFERENCES": "[11] Chancharik Mitra, Brandon Huang, Trevor Darrell, and",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "A mechanistic understanding of chain-of-thought reason-"
        },
        {
          "7. REFERENCES": "Roei Herzig, “Compositional chain-of-thought prompt-",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": "ing,” arXiv preprint arXiv:2402.18312, 2024."
        },
        {
          "7. REFERENCES": "ing for large multimodal models,” in Proceedings of the",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "IEEE/CVF Conference on Computer Vision and Pattern",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        },
        {
          "7. REFERENCES": "Recognition, 2024, pp. 14420–14431.",
          "[12] Alan V Oppenheim, Discrete-time signal processing,": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Qwen2-audio technical report",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Qian Yang",
        "Haojie Wei",
        "Xipin Wei",
        "Zhifang Guo",
        "Yichong Leng",
        "Yuanjun Lv",
        "Jinzheng He",
        "Junyang Lin"
      ],
      "year": "2024",
      "venue": "Qwen2-audio technical report",
      "arxiv": "arXiv:2407.10759"
    },
    {
      "citation_id": "3",
      "title": "Qwen2. 5-omni technical report",
      "authors": [
        "Jin Xu",
        "Zhifang Guo",
        "Jinzheng He",
        "Hangrui Hu",
        "Ting He",
        "Shuai Bai",
        "Keqin Chen",
        "Jialin Wang",
        "Yang Fan",
        "Kai Dang"
      ],
      "year": "2025",
      "venue": "Qwen2. 5-omni technical report",
      "arxiv": "arXiv:2503.20215"
    },
    {
      "citation_id": "4",
      "title": "Kimi-audio technical report",
      "authors": [
        "Ding Ding",
        "Zeqian Ju",
        "Yichong Leng",
        "Songxiang Liu",
        "Tong Liu",
        "Zeyu Shang",
        "Kai Shen",
        "Wei Song",
        "Xu Tan",
        "Heyi Tang"
      ],
      "year": "2025",
      "venue": "Kimi-audio technical report",
      "arxiv": "arXiv:2504.18425"
    },
    {
      "citation_id": "5",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Blsp-emo: Towards empathetic large speech-language models",
      "authors": [
        "Chen Wang",
        "Minpeng Liao",
        "Zhongqiang Huang",
        "Junhong Wu",
        "Chengqing Zong",
        "Jiajun Zhang"
      ],
      "year": "2024",
      "venue": "Blsp-emo: Towards empathetic large speech-language models",
      "arxiv": "arXiv:2406.03872"
    },
    {
      "citation_id": "7",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "8",
      "title": "Salmonn: Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang"
      ],
      "year": "2023",
      "venue": "Salmonn: Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "9",
      "title": "Osum: Advancing open speech understanding models with limited resources in academia",
      "authors": [
        "Xuelong Geng",
        "Kun Wei",
        "Qijie Shao",
        "Shuiyun Liu",
        "Zhennan Lin",
        "Zhixian Zhao",
        "Guojian Li",
        "Wenjie Tian",
        "Peikun Chen",
        "Yangze Li"
      ],
      "year": "2025",
      "venue": "Osum: Advancing open speech understanding models with limited resources in academia",
      "arxiv": "arXiv:2501.13306"
    },
    {
      "citation_id": "10",
      "title": "Omnivox: Zeroshot emotion recognition with omni-llms",
      "authors": [
        "John Murzaku",
        "Owen Rambow"
      ],
      "year": "2025",
      "venue": "Omnivox: Zeroshot emotion recognition with omni-llms",
      "arxiv": "arXiv:2503.21480"
    },
    {
      "citation_id": "11",
      "title": "Steering language model to stable speech emotion recognition via contextual perception and chain of thought",
      "authors": [
        "Zhixian Zhao",
        "Xinfa Zhu",
        "Xinsheng Wang",
        "Shuiyuan Wang",
        "Xuelong Geng",
        "Wenjie Tian",
        "Lei Xie"
      ],
      "year": "2025",
      "venue": "Steering language model to stable speech emotion recognition via contextual perception and chain of thought",
      "arxiv": "arXiv:2502.18186"
    },
    {
      "citation_id": "12",
      "title": "Compositional chain-of-thought prompting for large multimodal models",
      "authors": [
        "Chancharik Mitra",
        "Brandon Huang",
        "Trevor Darrell",
        "Roei Herzig"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Discrete-time signal processing",
      "authors": [
        "Alan Oppenheim"
      ],
      "year": "1999",
      "venue": "Discrete-time signal processing"
    },
    {
      "citation_id": "14",
      "title": "Opensmile: The munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Björn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "15",
      "title": "Keybert: Minimal keyword extraction with bert",
      "authors": [
        "Maarten Grootendorst"
      ],
      "year": "2020",
      "venue": "Keybert: Minimal keyword extraction with bert"
    },
    {
      "citation_id": "16",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "17",
      "title": "wav2vec 2.0: A framework for selfsupervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "Shu-Wen Chen"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Clep-dg: Contrastive learning for speech emotion domain generalization via soft prompt tuning",
      "authors": [
        "Jiacheng Shi",
        "Yanfu Zhang",
        "Ye Gao"
      ],
      "year": "2025",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "authors": [
        "Qian Keyu An",
        "Chong Chen",
        "Zhihao Deng",
        "Changfeng Du",
        "Zhifu Gao",
        "Yue Gao",
        "Ting Gu",
        "Hangrui He",
        "Kai Hu",
        "Hu"
      ],
      "year": "2024",
      "venue": "Funaudiollm: Voice understanding and generation foundation models for natural interaction between humans and llms",
      "arxiv": "arXiv:2407.04051"
    },
    {
      "citation_id": "21",
      "title": "Improved baselines with visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "22",
      "title": "Large language models are zero-shot reasoners",
      "authors": [
        "Takeshi Kojima",
        "Shane Shixiang",
        "Machel Gu",
        "Yutaka Reid",
        "Yusuke Matsuo",
        "Iwasawa"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
      "authors": [
        "Subhabrata Dutta",
        "Joykirat Singh",
        "Soumen Chakrabarti",
        "Tanmoy Chakraborty"
      ],
      "year": "2024",
      "venue": "How to think step-by-step: A mechanistic understanding of chain-of-thought reasoning",
      "arxiv": "arXiv:2402.18312"
    }
  ]
}