{
  "paper_id": "2509.04254v1",
  "title": "Mumtaffect: A Multimodal Multitask Affective Framework For Personality And Emotion Recognition From Physiological Signals",
  "published": "2025-09-04T14:30:02Z",
  "authors": [
    "Meisam Jamshidi Seikavandi",
    "Fabricio Batista Narcizo",
    "Ted Vucurevich",
    "Andrew Burke Dittberner",
    "Paolo Burelli"
  ],
  "keywords": [
    "Multimodal emotion recognition",
    "personality prediction",
    "physiological signals",
    "transformers",
    "multitask learning",
    "cognitive modeling",
    "affective computing",
    "theory of constructed emotion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present MuMTAffect, a novel Multimodal Multitask Affective Embedding Network designed for joint emotion classification and personality prediction (re-identification) from short physiological signal segments. MuMTAffect integrates multiple physiological modalities pupil dilation, eye gaze, facial action units, and galvanic skin response using dedicated transformer-based encoders for each modality and a fusion transformer to model cross-modal interactions. Inspired by the Theory of Constructed Emotion, the architecture explicitly separates core-affect encoding (valence-arousal) from higher-level conceptualization, thereby grounding predictions in contemporary affective neuroscience. Personality-trait prediction is leveraged as an auxiliary task to generate robust, user-specific affective embeddings, significantly enhancing emotion recognition performance. We evaluate MuMTAffect on the AFFEC dataset, demonstrating that stimulus-level emotional cues (Stim Emo) and galvanic skin response substantially improve arousal classification, while pupil and gaze data enhance valence discrimination. The inherent modularity of MuMTAffect allows effortless integration of additional modalities, ensuring scalability and adaptability. Extensive experiments and ablation studies underscore the efficacy of our multimodal multitask approach in creating personalized, context-aware affective computing systems, highlighting pathways for further advancements in cross-subject generalization. \n CCS Concepts ‚Ä¢ Computing methodologies ‚Üí Multitask learning; Multimodal fusion; ‚Ä¢ Human-centered computing ‚Üí Affective computing.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition using multimodal physiological signals is increasingly essential for building personalized, context-aware affective computing systems. Advances in multimodal learning have enabled integration of diverse physiological and behavioral cues such as eye gaze, pupil dilation, facial action units (AUs), and galvanic skin response (GSR) to improve emotion classification accuracy  [1, 9, 12, 38] . Moreover, incorporating contextual information in the form of stimulus-level emotion (Stim Emo) has proven effective at disambiguating subtle physiological patterns, enhancing both valence and arousal recognition  [28, 30, 33, 40] .\n\nContemporary affective neuroscience is guided by the Theory of Constructed Emotion (TCE)  [4] , which conceptualizes affective experiences as arising from a dynamic interplay between low-level core affect (valence and arousal) and higher-level conceptualization mediated by executive attention. Informed by TCE, MuMTAffect structures its pipeline so that modality-specific transformer encoders capture core-affect fluctuations, while a fusion transformer followed by task-specific attention layers implements the conceptual categorization stage, grounding model decisions in a neuroscientifically plausible framework.\n\nNevertheless, inter-individual variability in physiological responses remains a major challenge. Stable user traits such as personality captured by the Big Five dimensions modulate how emotion manifests physiologically  [18, 22, 44, 45] . Prior work has treated personality prediction and emotion recognition as separate tasks  [5, 13] , foregoing the opportunity for joint learning to yield richer, personalized embeddings.\n\nTo address these gaps, we propose MuMTAffect, a Multimodal Multitask Affective Embedding Network (see Figure  2 ) that simultaneously performs emotion classification and personality trait regression from synchronized physiological streams. Separate transformer based encoders for each modality (eye gaze, pupil dilation, facial AUs, and GSR) extract modality-specific temporal features  [35, 41] , which are then integrated by a fusion transformer to model cross-modal interactions. A task-specific attention mechanism disentangles core-affect (emotion) from trait-related (personality) representations, while auxiliary personality prediction guides the learning of user-specific affective embeddings.\n\nWe validate MuMTAffect on the AFFEC dataset  [15] , containing 5,356 trials of synchronized multimodal recordings from 73 participants. An extensive preprocessing pipeline yields fixed-length (400 timepoint) sequences enriched with trial-level summary statistics. Our experiments demonstrate that incorporating Stim Emo and GSR strongly boosts arousal F1, while eye-tracking modalities (gaze and pupil) enhance valence discrimination. Personality regression yields a high ùëÖ 2 for known users and provides a valuable auxiliary signal that improves emotion recognition, though full generalization to unseen subjects remains an open challenge  [46] .\n\nOur Contributions are as follows:\n\n‚Ä¢ We introduce MuMTAffect, a unified multimodal multitask framework grounded in the Theory of Constructed Emotion, capable of jointly predicting emotions and personality traits from physiological data. ‚Ä¢ We design modality-specific transformers that encode core affect, a fusion transformer that implements TCE-style conceptualization, and task-specific attention for disentangling affect and trait representations. ‚Ä¢ We develop a comprehensive preprocessing pipeline for the AFFEC dataset, producing synchronized, fixed-length multimodal trials with rich summary features. ‚Ä¢ We show that auxiliary personality prediction enhances emotion recognition performance, particularly under missing modality conditions, via personalized affective embeddings. ‚Ä¢ We conduct ablation studies quantifying each modality's contribution and the benefits of including Stim Emo, providing insights for scalable, context-aware affective computing systems.\n\nThe remainder of this paper reviews related work, describes our dataset processing, details the MuMTAffect architecture, presents experimental results, and concludes with discussions on limitations and future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Review Emotion Recognition From Physiological And Behavioral Signals",
      "text": "Affective computing aims to enable machines to recognize and respond to human emotions  [32] . Early approaches focused primarily on observable cues such as facial expressions and vocal tones. However, researchers have increasingly turned to physiological signals such as EEG, ECG, EMG, respiration, and GSR as these involuntary responses provide a more objective and less easily manipulated measure of affective states  [21] . These signals can reveal genuine emotional reactions even when individuals attempt to mask their feelings  [21] .\n\nRecent work has broadened emotion recognition by incorporating behavioral cues like eye tracking and facial action units (AUs) alongside traditional peripheral measures. For example, the multimodal benchmark by Soleymani et al.  [36]  combined facial videos (for AU and gaze analysis) with ECG, GSR, respiration, and skin temperature, illustrating the potential of fusing external and internal cues  [19, 37] . Advances in wearable sensors and deep learning have spurred the development of methods that employ 3D CNNs, hypercomplex networks (PHNN), and attention mechanisms to extract shared and complementary features from diverse signals  [17, 27, 43] . More recently, Transformer-based methods have also been applied to multimodal emotion recognition  [35, 41] , demonstrating improved fusion of complex biosignals such as eye gaze, GSR, and EEG.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Challenges In Emotion Recognition: Context And Individual Modulation",
      "text": "Despite progress, reliably inferring emotions remains challenging due to the inherent complexity and variability of human affect. Physiological responses vary widely due to factors such as context, environmental conditions, personal history, and cultural background  [20, 21] . For instance, the same level of physiological arousal may correspond to excitement in one context and anxiety in another. Similarly, subjective labeling inconsistencies further complicate the task. These challenges have motivated efforts to incorporate external context as a secondary source of information, with the aim of enhancing emotion recognition accuracy  [26, 28, 33] .\n\nAdditional complexities arise in specific subfields such as speech emotion recognition (SER) and facial emotion recognition (FER), where cultural biases, language differences, and varied annotation practices can lead to inconsistent performance. This underscores the need for robust, adaptable methods capable of handling a wide range of real-world conditions. Recent efforts have also highlighted the importance of benchmarking and evaluating models under noisy or less-controlled environments  [2, 7, 31] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multimodal Emotion Recognition And Fusion Strategies",
      "text": "Multimodal emotion recognition (MER) leverages information from multiple sensory channels to capture a more holistic picture of affective states  [30, 34] . In this context, stimulus-level emotional cues (referred to as Stim Emo) have emerged as an effective additional channel, providing contextual grounding that enriches the interpretation of physiological signals  [28, 30, 42] . Fusion strategies, whether early, intermediate, or late, have been explored extensively. Early fusion methods capture cross-modal correlations but often result in high-dimensional representations, whereas late fusion aggregates decisions from modality-specific classifiers but may miss inter-modal interactions. Recent approaches favor intermediate fusion with attention-based or hypergraph-based mechanisms to adaptively weight different modalities  [3, 8, 25, 39, 47] . For example, Koelstra et al.  [19]  fused EEG with peripheral signals to predict valence and arousal, and Iacono and Khan  [14]  achieved state-ofthe-art performance on the SEED-V dataset by combining EEG and eye-tracking features using CNNs.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Inter-Individual Variability And Personalization",
      "text": "One of the major challenges in emotion recognition is the significant inter-individual variability in physiological responses. Classifiers that perform well on subjects seen during training often experience substantial performance drops on unseen individuals due to differences in baseline states and emotional reactivity  [21] . Approaches such as data normalization, transfer learning, and domain adaptation have been proposed to mitigate these issues  [6] . Moreover, incorporating person-specific traits, such as those captured by the Big Five personality dimensions, can enhance emotion recognition by providing stable user-specific embeddings  [10, 18, 37, 40, 47] . In addition, demographic attributes such as gender can further modulate emotional expression  [45] , underscoring the need for personalized approaches  [46] . Recent multitask learning frameworks have leveraged personality as auxiliary tasks to refine shared representations, thereby improving both emotion prediction and user-specific modeling  [5, 13, 23, 29] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Limitations Of Existing Methods And Our Contributions",
      "text": "Despite these advances, current multimodal approaches have several limitations:\n\n‚Ä¢ Insufficient Context Integration: Many methods do not fully incorporate external context, limiting their ability to disambiguate subtle physiological variations. ‚Ä¢ Limited Personalization: User-specific traits are often handled through simple normalization rather than through explicit auxiliary tasks or learned embeddings, which restricts the ability to adapt to individual differences. ‚Ä¢ Fragmented Multitask Designs: Most existing works treat emotion recognition and auxiliary tasks (such as personality prediction) separately, missing the benefits of joint, end-toend learning.\n\nOur proposed method addresses these limitations by integrating stimulus-level emotional cues (Stim Emo) and by explicitly modeling personality as auxiliary tasks within a unified, end-to-end deep learning framework. This approach not only enhances emotion recognition through better context integration but also improves personalization by learning robust user-specific embeddings. By combining these strategies with multimodal fusion of Eye, Pupil, AU, and GSR signals, our method overcomes key challenges in generalizing to unseen subjects and in capturing the complex interrelations among physiological, behavioral, and contextual factors.\n\nIn summary, while previous multimodal methods have achieved promising results by leveraging diverse signals and fusion strategies, they often fall short in integrating context and personalizing models for individual differences. Our work builds upon this foundation by providing a holistic, multitask framework that simultaneously captures emotional and personality cues to deliver a more robust and personalized affective computing system  [11, 24] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset Cleaning And Preprocessing",
      "text": "We employ the publicly available AFFEC dataset  [15] , which collects multimodal signals (eye-gaze/pupil at 150 Hz, facial AUs at 40 Hz, GSR at 50 Hz) from 73 participants. Each participant viewed 88 scenario-primed video clips of a talking face completing a simulated dialogue, then rated their felt and perceived emotions on 9-point valence/arousal scales. The Big Five personality traits (Openness, Conscientiousness, Extraversion, Agreeableness, Neuroticism) were also collected via the BFI-44 questionnaire  [16] .  After synchronization, denoising, and removal of trials with missing modalities, the final cleaned dataset comprises 5,356 trials from 69 users. Each trial is accompanied by self-reported emotion ratings and baseline personality scores, providing a rich foundation for personalized emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Preprocessing Pipeline",
      "text": "3.1.1 Handling Missing Data and Synchronization. Trials with incomplete or corrupted signals (e.g., missing onset timestamps or event markers) were removed. Data from each modality were synchronized using common event markers, and only trials with complete data were retained.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Resampling And Fixed-Length Segmentation.",
      "text": "To handle variations in trial lengths, all data were downsampled to a fixed length of 400 timepoints:\n\n‚Ä¢ Numerical Signals: Linearly interpolated.\n\n‚Ä¢ Categorical Signals (e.g., trial stage flags): Downsampled using nearest-neighbor selection. This fixed-length representation allows for efficient batch processing while preserving the time dynamics of each modality.\n\n3.1.3 Modality-Specific Processing. Each modality was processed using tailored pipelines:\n\n‚Ä¢ Eye Data (Gaze and Pupil): Gaze coordinates, fixation metrics, and pupil size (computed from left/right measurements) are synchronized and resampled. ‚Ä¢ Facial Action Units (AUs): AU intensities and binary activations are extracted from facial videos, aligned with stimulus events, and downsampled. ‚Ä¢ GSR: GSR signals are cleaned, synchronized with event markers, and downsampled. Subsequent processing extracts phasic and tonic components.\n\n3.1.4 Integration of Trial-Level Metadata. In addition to the sequential data, each trial is augmented with:\n\n‚Ä¢ A flag indicating the stage within the trial.\n\n‚Ä¢ Trial-level summary features (e.g., mean pupil size, blink rate, AU statistics, and GSR peak metrics) as detailed in Table  2 . ‚Ä¢ Self-reported emotion ratings and personality scores.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Extracted Features Summary",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Final Dataset Statistics",
      "text": "After cleaning and preprocessing, the final dataset consists of:\n\n‚Ä¢ 5,356 trials from 69 users.\n\n‚Ä¢ Synchronized, fixed-length sequences (400 timepoints per trial) for each modality. ‚Ä¢ Trial-level metadata including stage flags, self-reported emotion ratings, and personality scores.\n\nThis comprehensive and multimodal dataset, along with our detailed preprocessing pipeline, provides a robust foundation for developing advanced, context-aware, and personalized emotion recognition models.\n\nFor further details on the AFFEC dataset, please refer to its original publication  [15] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Structure",
      "text": "Our proposed model, MuMTAffect, is structured to capture temporal sequences across four physiological modalities (Eye, Pupil, AU, GSR) and integrate them into a shared multimodal embedding optimized simultaneously for emotion recognition and personality prediction. MuMTAffect's modular design allows new physiological or behavioral modalities to be integrated by adding additional transformer encoders without major changes to the rest of the pipeline. Dedicated per-modality transformers, followed by a cross-modal fusion transformer and task-specific branches, enable learning of both modality-specific temporal patterns and crossmodal correlations (Figure  2 ). Exact layer depths, dimensions, and dropout rates are summarized in Table  3 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Modality-Specific Transformers",
      "text": "Each modality's time series (eye gaze sequence, pupil dilation, AU activation, GSR response) is processed by a modality-specific transformer encoder (depth = 1, ùëë model = 64, #heads = 2, FFN dim = 2048, dropout = 0.25). This separation captures:\n\n‚Ä¢ Distinct temporal patterns: e.g., pupil dynamics vs. GSR recovery times. ‚Ä¢ Time misalignment: different latencies in physiological reactions. After the transformer, we downsample from 400 time steps to ùëá =16 via deterministic equal-segment averaging to reduce sequence length before cross-modal fusion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Cross-Modal Fusion Transformer",
      "text": "The reduced sequences from all four modalities (each length ùëá =16) are projected to 32-dim vectors, concatenated, and passed to a fusion transformer (depth = 1, ùëë model = 128, #heads = 4, FFN dim = 2048, dropout = 0.25). Self-attention in this block learns cross-modal interactions and compensates for latency differences.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task-Specific Attention And Branches",
      "text": "A TaskAttentionTemporal module (ùëë model = 128, single-head scaled dot-product) uses learnable queries to route information into:\n\n(1) Personality Branch: two 1D convolutional blocks (128 ‚Üí 128, ùëò = 3, ùë† = 1 then ùëò = 3, ùë† = 2), BN+ReLU, branch dropout = 0.4. (2) Emotion Branch: three 1D convolutional blocks (128 ‚Üí 128, ùëò = 3, ùë† = 2 each), BN+ReLU, branch dropout = 0.25. A trial_embedding from summary features (e.g., mean pupil size, GSR peaks) is computed via two FC layers (64‚Üí64) with BN and dropout = 0.3.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Prediction Heads",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Loss Function And Training Pipeline",
      "text": "We optimize:\n\nwhere L personality is an ùúÄ-insensitive regression loss, and L emotion is weighted cross-entropy. Data Splits: ‚àº15% of users for test, remaining 85% split into train and validation (15% validation by user). Binned emotion classes: Valence/arousal (Likert 1-9) mapped to three bins: Low/Negative, Medium/Neutral, High/Positive.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Training Phases:",
      "text": "(1) Personality pretraining: ùõº = 1, 50 epochs, LR 10 -4 , exponential decay ùõæ = 0.99.  Each modality is processed by its own transformer encoder to capture modality-specific temporal patterns. A shared fusion transformer learns cross-modal interactions, followed by task-specific attention queries for personality (stable, user-centric) and emotion (dynamic, trial-centric). Hierarchical CNN and MLP layers further refine these features, enabling simultaneous prediction of personality and emotion states in a unified network.\n\n(2) Multitask: ùõº ‚âà 0.3, up to 100 epochs, LR 8√ó10 -4 (base), 5√ó10 -5 (personality head), 5√ó10 -4 (emotion head), decay ùõæ = 0.95, patience 5-10.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Analysis 5.1 Aggregate Performance Trends",
      "text": "Tables  4  and 5  present a focused view of the performance across key model configurations. Table  4  summarizes performance for full-modality configurations, comparing cases with and without Stim Emo. It includes detailed ùëÖ 2 values for the Big Five personality traits and per-class F1 scores for both valence and arousal. Table  5  analyzes the impact of missing modalities, highlighting how the inclusion or exclusion of certain signals affects performance metrics.\n\n‚Ä¢ High Personality ùëÖ 2 : Configurations with all four modalities (Eye, Pupil, AU, GSR) achieve ùëÖ 2 values around 0.94-0.95 across all Big Five traits (Table  4 , rows 2-5), confirming the value of multimodal signals for personality inference.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "‚Ä¢ Valence And Arousal F1:",
      "text": "-Valence macro-F1 generally lies in the 0.50-0.63 range, with the higher end occurring when Eye and AU signals are included (Table  4 , rows 0 and 4). -Arousal macro-F1 often peaks around 0.57-0.60 when GSR is present alongside Stim Emo, indicating the significance of GSR in capturing autonomic arousal cues. ‚Ä¢ Multitask Trade-Offs: Certain configurations obtain nearoptimal personality ùëÖ 2 (above 0.90) but exhibit moderate valence/arousal F1 scores, whereas others improve emotion metrics at the expense of slightly lower ùëÖ 2 . This pattern is consistent with standard multitask learning trade-offs.\n\n1 https://github.com/itubrainlab/MuMTAffect",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Modality Importance",
      "text": "Comparisons in Table  5  illustrate the contributions of each modality:\n\n‚Ä¢ Eye and Pupil: Removing these signals leads to notable drops in valence F1, reflecting the importance of visual and ocular cues in modeling user attention and appraisal. ‚Ä¢ GSR: Essential for arousal classification; configurations without GSR consistently show reduced arousal macro-F1. ‚Ä¢ Facial AUs: Impact both personality and emotion tasks.\n\nOmitting AUs diminishes overall performance across ùëÖ 2 and F1 measures.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Influence Of Stim Emo",
      "text": "Across Tables  4  and 5 , enabling Stim Emo usually yields an improvement in emotion metrics. Each paired row (Yes vs. No) in Table  5  shows a visible gain in macro-F1 for both valence and arousal when contextual emotional information (Stim Emo) is incorporated. These findings align with existing research on context-aware affect recognition.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "Grounded in the Theory of Constructed Emotion (TCE), our results highlight how MuMTAffect's architecture mirrors core-affect dynamics and conceptualization processes:\n\n‚Ä¢ Core-Affect Encoding via Modality Encoders: Early transformer encoders for Eye, Pupil, AU and GSR capture lowlevel valence and arousal fluctuations akin to TCE's coreaffect layer. The high ùëÖ 2 scores (‚âà0.94-0.95) on personality prediction confirm that these physiological streams reliably encode stable, trait-related baselines that inform affective state estimation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "‚Ä¢ Conceptualization Through Fusion And Task Queries:",
      "text": "The fusion transformer with task-specific attention implements the conceptualization stage, integrating cross-modal cues to categorize discrete emotions. Improvements in valence macro-F1 (up to 0.61) and arousal macro-F1 (up to 0.59) are observed when including the Stim Emo signal, which In our current work, the use of multitask trait priors and contextual cues helps to reduce this gap to some extent. Future studies should test and compare these strategies to better understand their effectiveness for subject-independent performance.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "In this work, we presented MuMTAffect, a transformer-based, theory-informed framework that fuses eye gaze, pupil dilation, facial AUs, GSR and contextual Stim Emo cues to produce simultaneous valence/arousal classification and Big Five personality prediction. By explicitly separating low-level core-affect encoding (via modality-specific transformers) from higher-level conceptualization and categorization (via fusion and task-specific attention), our design adheres to the Theory of Constructed Emotion while delivering strong empirical results: personality ùëÖ 2 ‚âà 0.94-0.95 and emotion macro-F1 scores above 0.55. Future work will focus on integrating additional physiological and contextual modalities (e.g., EEG, heart-rate variability, or semantic context from video/audio), adopting meta-learning or domain-adversarial techniques for rapid adaptation to unseen users, and replacing static Stim Emo labels with dynamic, real-time context embeddings. We also plan to introduce learnable task-weighting mechanisms to mitigate multitask trade-offs and to evaluate MuMTAffect in unconstrained, in-the-wild settings, addressing concerns like sensor noise, missing data, and privacy. These directions aim to further enhance personalization, robustness, and contextual sensitivity in real-world affective computing applications.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "As affective computing systems become more common in everyday life, it is important to understand and address their ethical implications. Our multimodal multitask framework is designed to improve the personalization and accuracy of emotion and personality recognition, but its use also raises several ethical concerns.\n\nPrivacy and Data Protection: Physiological signals such as eye gaze, facial expressions, and galvanic skin response are sensitive personal data. Strong privacy protections, secure data storage, and informed consent are essential. Any future implementation should follow relevant data protection laws, such as GDPR, and be transparent about how data is used.\n\nBias and Fairness: While personalization can improve performance, there is a risk of reinforcing existing biases, including those related to personality stereotypes. Testing on diverse populations and ongoing monitoring are important to ensure fair and equitable performance.\n\nMisuse of Technology: Advanced emotion and personality recognition can be misused for unauthorized surveillance, manipulation, or unfair decision-making. To reduce this risk, researchers should clearly state the intended uses, set usage guidelines, and support regulatory measures that prevent harmful applications.\n\nUser Autonomy and Consent: Users should have control over how their emotional and physiological data are used. Clear explanations, transparent agreements, and strong consent processes are necessary to maintain user trust.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Deployment Considerations And Ethics In Practice",
      "text": "Data minimization and privacy: Use on-device processing when possible, store only aggregated results, and allow users to enable or disable each modality. Latency and robustness: Use fixed-rate resampling (segment averaging to ùëá =16) and late fusion so the system can still operate if some modalities are missing or sensors fail. Fairness auditing: Track error rates across groups, compare macro-and weighted-F1 scores, check for drift over time, and allow human review of decisions. Context transparency: If Stim Emo is used, provide a clear on/off control and record its source for auditing.\n\nOverall, our work aims to support socially responsible affective computing by promoting transparency, continuous ethical review, and proactive measures to reduce risks.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: ) that simul-",
      "page": 1
    },
    {
      "caption": "Figure 1: Experimental setup showing a participant with",
      "page": 3
    },
    {
      "caption": "Figure 2: ). Exact layer depths, dimensions, and",
      "page": 4
    },
    {
      "caption": "Figure 2: Diagram of the MuMTAffect architecture for personality and emotion recognition using Eye, Pupil, AU, and GSR signals.",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for": ""
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for": "Fabricio Batista Narcizo"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for": "GN Advanced Science"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for": "Ballerup, Denmark"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for": "IT University of Copenhagen"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for": "Copenhagen, Denmark"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for": ""
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Copenhagen, Denmark": "ACM Reference Format:"
        },
        {
          "Copenhagen, Denmark": "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, An-"
        },
        {
          "Copenhagen, Denmark": ""
        },
        {
          "Copenhagen, Denmark": "drew Burke Dittberner, and Paolo Burelli. 2025. MuMTAffect: A Multimodal"
        },
        {
          "Copenhagen, Denmark": ""
        },
        {
          "Copenhagen, Denmark": "Multitask Affective Framework for Personality and Emotion Recognition"
        },
        {
          "Copenhagen, Denmark": ""
        },
        {
          "Copenhagen, Denmark": "from Physiological Signals. In Proceedings of\n. ACM, New York, NY, USA,"
        },
        {
          "Copenhagen, Denmark": ""
        },
        {
          "Copenhagen, Denmark": "9 pages."
        },
        {
          "Copenhagen, Denmark": ""
        },
        {
          "Copenhagen, Denmark": ""
        },
        {
          "Copenhagen, Denmark": ""
        },
        {
          "Copenhagen, Denmark": "1\nIntroduction"
        },
        {
          "Copenhagen, Denmark": ""
        },
        {
          "Copenhagen, Denmark": "Emotion recognition using multimodal physiological signals is in-"
        },
        {
          "Copenhagen, Denmark": "creasingly essential for building personalized, context-aware affec-"
        },
        {
          "Copenhagen, Denmark": "tive computing systems. Advances in multimodal\nlearning have"
        },
        {
          "Copenhagen, Denmark": "enabled integration of diverse physiological and behavioral cues"
        },
        {
          "Copenhagen, Denmark": "such as eye gaze, pupil dilation, facial action units (AUs), and gal-"
        },
        {
          "Copenhagen, Denmark": "vanic skin response (GSR) to improve emotion classification accu-"
        },
        {
          "Copenhagen, Denmark": "racy [1, 9, 12, 38]. Moreover, incorporating contextual information"
        },
        {
          "Copenhagen, Denmark": "in the form of stimulus-level emotion (Stim Emo) has proven ef-"
        },
        {
          "Copenhagen, Denmark": "fective at disambiguating subtle physiological patterns, enhancing"
        },
        {
          "Copenhagen, Denmark": "both valence and arousal recognition [28, 30, 33, 40]."
        },
        {
          "Copenhagen, Denmark": "Contemporary affective neuroscience is guided by the Theory"
        },
        {
          "Copenhagen, Denmark": "of Constructed Emotion (TCE) [4], which conceptualizes affective"
        },
        {
          "Copenhagen, Denmark": "experiences as arising from a dynamic interplay between low-level"
        },
        {
          "Copenhagen, Denmark": "core affect (valence and arousal) and higher-level conceptualization"
        },
        {
          "Copenhagen, Denmark": "mediated by executive attention. Informed by TCE, MuMTAffect"
        },
        {
          "Copenhagen, Denmark": "structures its pipeline so that modality-specific transformer en-"
        },
        {
          "Copenhagen, Denmark": "coders capture core-affect fluctuations, while a fusion transformer"
        },
        {
          "Copenhagen, Denmark": "followed by task-specific attention layers implements the concep-"
        },
        {
          "Copenhagen, Denmark": ""
        },
        {
          "Copenhagen, Denmark": "tual categorization stage, grounding model decisions in a neurosci-"
        },
        {
          "Copenhagen, Denmark": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "auxiliary signal": "generalization to unseen subjects remains an open challenge [46].",
          "that": "",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "Our Contributions are as follows:",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "‚Ä¢ We introduce MuMTAffect, a unified multimodal multitask",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "framework grounded in the Theory of Constructed Emotion,",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "capable of jointly predicting emotions and personality traits",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "from physiological data.",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "‚Ä¢ We design modality-specific transformers that encode core",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "affect, a fusion transformer that implements TCE-style con-",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "ceptualization, and task-specific attention for disentangling",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "affect and trait representations.",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "‚Ä¢ We develop a comprehensive preprocessing pipeline for the",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "AFFEC dataset, producing synchronized, fixed-length multi-",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "modal trials with rich summary features.",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "‚Ä¢ We show that auxiliary personality prediction enhances emo-",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "tion recognition performance, particularly under missing",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "modality conditions, via personalized affective embeddings.",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "‚Ä¢ We conduct ablation studies quantifying each modality‚Äôs",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "contribution and the benefits of including Stim Emo, provid-",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "",
          "that": "ing insights for scalable, context-aware affective computing",
          "improves emotion recognition,": "",
          "though full": ""
        },
        {
          "auxiliary signal": "systems.",
          "that": "",
          "improves emotion recognition,": "",
          "though full": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "regression from synchronized physiological streams. Separate trans-\nRecent work has broadened emotion recognition by incorporat-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "former based encoders for each modality (eye gaze, pupil dila-\ning behavioral cues like eye tracking and facial action units (AUs)"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "tion, facial AUs, and GSR) extract modality-specific temporal fea-\nalongside traditional peripheral measures. For example, the multi-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "tures [35, 41], which are then integrated by a fusion transformer to\nmodal benchmark by Soleymani et al. [36] combined facial videos"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "model cross-modal interactions. A task-specific attention mecha-\n(for AU and gaze analysis) with ECG, GSR, respiration, and skin tem-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "nism disentangles core-affect (emotion) from trait-related (person-\nperature, illustrating the potential of fusing external and internal"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "ality) representations, while auxiliary personality prediction guides\ncues [19, 37]. Advances in wearable sensors and deep learning have"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "the learning of user-specific affective embeddings.\nspurred the development of methods that employ 3D CNNs, hyper-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "We validate MuMTAffect on the AFFEC dataset [15], containing\ncomplex networks (PHNN), and attention mechanisms to extract"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "5,356 trials of synchronized multimodal recordings from 73 par-\nshared and complementary features from diverse signals [17, 27, 43]."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "ticipants. An extensive preprocessing pipeline yields fixed-length\nMore recently, Transformer-based methods have also been applied"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "(400 timepoint) sequences enriched with trial-level summary sta-\nto multimodal emotion recognition [35, 41], demonstrating im-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "tistics. Our experiments demonstrate that incorporating Stim Emo\nproved fusion of complex biosignals such as eye gaze, GSR, and"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "and GSR strongly boosts arousal F1, while eye-tracking modalities\nEEG."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "(gaze and pupil) enhance valence discrimination. Personality re-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "gression yields a high ùëÖ2 for known users and provides a valuable"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Challenges in Emotion Recognition: Context and"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "auxiliary signal\nthat\nimproves emotion recognition,\nthough full"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Individual Modulation"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "generalization to unseen subjects remains an open challenge [46]."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Despite progress, reliably inferring emotions remains challenging\nOur Contributions are as follows:"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "due to the inherent complexity and variability of human affect."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "‚Ä¢ We introduce MuMTAffect, a unified multimodal multitask"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Physiological responses vary widely due to factors such as con-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "framework grounded in the Theory of Constructed Emotion,"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "text, environmental conditions, personal history, and cultural back-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "capable of jointly predicting emotions and personality traits"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "ground [20, 21]. For instance, the same level of physiological arousal"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "from physiological data."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "may correspond to excitement in one context and anxiety in another."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "‚Ä¢ We design modality-specific transformers that encode core"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Similarly, subjective labeling inconsistencies further complicate the"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "affect, a fusion transformer that implements TCE-style con-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "task. These challenges have motivated efforts to incorporate exter-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "ceptualization, and task-specific attention for disentangling"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "nal context as a secondary source of information, with the aim of"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "affect and trait representations."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "enhancing emotion recognition accuracy [26, 28, 33]."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "‚Ä¢ We develop a comprehensive preprocessing pipeline for the"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Additional complexities arise in specific subfields such as speech"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "AFFEC dataset, producing synchronized, fixed-length multi-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "emotion recognition (SER) and facial emotion recognition (FER),"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "modal trials with rich summary features."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "where cultural biases, language differences, and varied annotation"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "‚Ä¢ We show that auxiliary personality prediction enhances emo-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "practices can lead to inconsistent performance. This underscores"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "tion recognition performance, particularly under missing"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "the need for robust, adaptable methods capable of handling a wide"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "modality conditions, via personalized affective embeddings."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "range of real-world conditions. Recent efforts have also highlighted"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "‚Ä¢ We conduct ablation studies quantifying each modality‚Äôs"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "the importance of benchmarking and evaluating models under"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "contribution and the benefits of including Stim Emo, provid-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "noisy or less-controlled environments [2, 7, 31]."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "ing insights for scalable, context-aware affective computing"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "systems."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Multimodal Emotion Recognition and Fusion"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "The remainder of this paper reviews related work, describes our"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Strategies"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "dataset processing, details the MuMTAffect architecture, presents"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Multimodal emotion recognition (MER) leverages information from"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "experimental results, and concludes with discussions on limitations"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "multiple sensory channels to capture a more holistic picture of"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "and future directions."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "affective states [30, 34]. In this context, stimulus-level emotional"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "cues (referred to as Stim Emo) have emerged as an effective addi-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "tional channel, providing contextual grounding that enriches the\n2\nLiterature Review"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "interpretation of physiological signals [28, 30, 42]. Fusion strategies,"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Emotion Recognition from Physiological and"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "whether early, intermediate, or late, have been explored extensively."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Behavioral Signals"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Early fusion methods capture cross-modal correlations but often"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Affective computing aims to enable machines to recognize and re-\nresult in high-dimensional representations, whereas late fusion ag-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "spond to human emotions [32]. Early approaches focused primarily\ngregates decisions from modality-specific classifiers but may miss"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "on observable cues such as facial expressions and vocal tones. How-\ninter-modal\ninteractions. Recent approaches favor intermediate"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "ever, researchers have increasingly turned to physiological signals\nfusion with attention-based or hypergraph-based mechanisms to"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "such as EEG, ECG, EMG, respiration, and GSR as these involuntary\nadaptively weight different modalities [3, 8, 25, 39, 47]. For example,"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "responses provide a more objective and less easily manipulated\nKoelstra et al. [19] fused EEG with peripheral signals to predict"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "measure of affective states [21]. These signals can reveal genuine\nvalence and arousal, and Iacono and Khan [14] achieved state-of-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "emotional reactions even when individuals attempt to mask their\nthe-art performance on the SEED-V dataset by combining EEG and"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "feelings [21].\neye-tracking features using CNNs."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "scenario-primed video clips of a talking face completing a simulated\nInter-Individual Variability and Personalization"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "dialogue, then rated their felt and perceived emotions on 9-point"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "One of the major challenges in emotion recognition is the significant"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "valence/arousal scales. The Big Five personality traits (Openness,"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "inter-individual variability in physiological responses. Classifiers"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Conscientiousness, Extraversion, Agreeableness, Neuroticism) were"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "that perform well on subjects seen during training often experience"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "also collected via the BFI-44 questionnaire [16]. Figures 1 show the"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "substantial performance drops on unseen individuals due to differ-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "sensor setup and a sample video frame. The final cleaned dataset"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "ences in baseline states and emotional reactivity [21]. Approaches"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "includes 5,356 complete trials, each with multimodal time series,"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "such as data normalization, transfer learning, and domain adap-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "trial-level statistics, and corresponding emotion/personality labels."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "tation have been proposed to mitigate these issues [6]. Moreover,"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "All trials were downsampled to 400 timepoints (selected after trying"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "incorporating person-specific traits, such as those captured by the"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "different values), preserving temporal flags such as first_fix,"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Big Five personality dimensions, can enhance emotion recognition"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "scenario, and video to indicate key stages."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "by providing stable user-specific embeddings [10, 18, 37, 40, 47]."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "In addition, demographic attributes such as gender can further"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "modulate emotional expression [45], underscoring the need for"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "personalized approaches [46]. Recent multitask learning frame-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "works have leveraged personality as auxiliary tasks to refine shared"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "representations, thereby improving both emotion prediction and"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "user-specific modeling [5, 13, 23, 29]."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Limitations of Existing Methods and Our"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Contributions"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Despite these advances, current multimodal approaches have sev-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Figure 1: Experimental setup showing a participant with"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "eral limitations:"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "sensors attached in AFFEC dataset [15]."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "‚Ä¢ Insufficient Context Integration: Many methods do not"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Contributions": "Despite these advances, current multimodal approaches have sev-"
        },
        {
          "Contributions": ""
        },
        {
          "Contributions": "eral limitations:"
        },
        {
          "Contributions": ""
        },
        {
          "Contributions": "‚Ä¢ Insufficient Context Integration: Many methods do not"
        },
        {
          "Contributions": "fully incorporate external context, limiting their ability to"
        },
        {
          "Contributions": "disambiguate subtle physiological variations."
        },
        {
          "Contributions": "‚Ä¢ Limited Personalization: User-specific traits are often han-"
        },
        {
          "Contributions": "dled through simple normalization rather than through ex-"
        },
        {
          "Contributions": "plicit auxiliary tasks or learned embeddings, which restricts"
        },
        {
          "Contributions": "the ability to adapt to individual differences."
        },
        {
          "Contributions": "‚Ä¢ Fragmented Multitask Designs: Most existing works treat"
        },
        {
          "Contributions": "emotion recognition and auxiliary tasks (such as personality"
        },
        {
          "Contributions": ""
        },
        {
          "Contributions": "prediction) separately, missing the benefits of joint, end-to-"
        },
        {
          "Contributions": ""
        },
        {
          "Contributions": "end learning."
        },
        {
          "Contributions": ""
        },
        {
          "Contributions": "Our proposed method addresses these limitations by integrating"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ",": "3.1.4",
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "In addition to the sequen-\nIntegration of Trial-Level Metadata.\n4.1\nModality-Specific Transformers"
        },
        {
          ",": "tial data, each trial is augmented with:",
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": ""
        },
        {
          ",": "",
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Each modality‚Äôs time series (eye gaze sequence, pupil dilation, AU"
        },
        {
          ",": "",
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "activation, GSR response)\nis processed by a modality-specific\n‚Ä¢ A flag indicating the stage within the trial."
        },
        {
          ",": "",
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "‚Ä¢ Trial-level summary features (e.g., mean pupil size, blink\ntransformer encoder (depth = 1, ùëëmodel = 64, #heads = 2, FFN"
        },
        {
          ",": "",
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "dim = 2048, dropout = 0.25). This separation captures:\nrate, AU statistics, and GSR peak metrics) as detailed in"
        },
        {
          ",": "",
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Table 2.\n‚Ä¢ Distinct temporal patterns: e.g., pupil dynamics vs. GSR"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "features.": "",
          "length before cross-modal fusion.": "4.2\nCross-modal Fusion Transformer"
        },
        {
          "features.": "",
          "length before cross-modal fusion.": ""
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "The reduced sequences from all four modalities (each length ùëá =16)"
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "are projected to 32-dim vectors, concatenated, and passed to a fusion"
        },
        {
          "features.": "Modality",
          "length before cross-modal fusion.": ""
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "transformer (depth = 1, ùëëmodel = 128, #heads = 4, FFN dim = 2048,"
        },
        {
          "features.": "Eye (Gaze)",
          "length before cross-modal fusion.": "dropout = 0.25). Self-attention in this block learns cross-modal"
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "interactions and compensates for latency differences."
        },
        {
          "features.": "Pupil",
          "length before cross-modal fusion.": ""
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "4.3\nTask-Specific Attention and Branches"
        },
        {
          "features.": "Facial AUs",
          "length before cross-modal fusion.": ""
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "= 128, single-head\nA TaskAttentionTemporal module (ùëëmodel"
        },
        {
          "features.": "",
          "length before cross-modal fusion.": ""
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "scaled dot-product) uses learnable queries to route information"
        },
        {
          "features.": "GSR",
          "length before cross-modal fusion.": ""
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "into:"
        },
        {
          "features.": "",
          "length before cross-modal fusion.": ""
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "(1) Personality Branch: two 1D convolutional blocks (128 ‚Üí"
        },
        {
          "features.": "",
          "length before cross-modal fusion.": "128, ùëò = 3, ùë† = 1 then ùëò = 3, ùë† = 2), BN+ReLU, branch dropout"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Table 2: Trial-Level Summary Features"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Modality\nSummary Features"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Eye/Pupil\nMean, STD, min, max of pupil size; fixation count; blink rate"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Facial AUs\nMean, STD, min, max of AU intensities; activation rates; composite expressions (e.g., smile/frown rates)"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "GSR\nPeak counts; mean, median, min, max, STD of SCR amplitude, onsets, rise time, recovery time; phasic/tonic statistics"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Modality Inputs [(28+12+46+15)*400]"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Modality Specific Transformers (nhead=2,dropout=0.25)"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Pupil\nPupil\nGaze\nGSR\nAU\nGaze\nGSR\nAU"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Modality Specific Embeddings [(4*16)*64]"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Pupil\nGaze\nGSR\nAU"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Intra-Modality Transformer\nTrial-Level Features Extractor"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[(17+128+40)*1]\n(nhead=8,dropout=0.25)"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Two Stream Attention Layers"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Pupil\nGaze\nGSR\nAU"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[64*64]\n[64*64]"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Hierarchical\nHierarchical"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "One-Hot\nTrial"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Convolutional\nConvolutional"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Emotion\nMLP"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Layer\nLayer"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Pooled Emotion Stream\n[6*1]\n[64*1]\n[64*1]\n[32*1]"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Personality\nStimulus\nTrial\nEmotion"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Embeddings\nEmbeddings\nEmbeddings\nEmbeddings"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Personality Predictor"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Emotion Classifier"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Personality Scores\nValance Classes\nArousal Classes"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Figure 2: Diagram of the MuMTAffect architecture for personality and emotion recognition using Eye, Pupil, AU, and GSR signals."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Each modality is processed by its own transformer encoder to capture modality-specific temporal patterns. A shared fusion"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "transformer learns cross-modal interactions, followed by task-specific attention queries for personality (stable, user-centric)"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "and emotion (dynamic, trial-centric). Hierarchical CNN and MLP layers further refine these features, enabling simultaneous"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "prediction of personality and emotion states in a unified network."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "(2) Multitask: ùõº ‚âà 0.3, up to 100 epochs, LR 8√ó10‚àí4\n(base),"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "5√ó10‚àí5 (personality head), 5√ó10‚àí4 (emotion head), decay"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "ùõæ = 0.95, patience 5‚Äì10."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Table 3: Specification of MuMTAffect"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "ùëëmodel"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "64"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "64"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "64"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "64"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "128"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "128"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "128"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "128"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "64"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "‚Äì"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "‚Äì"
        },
        {
          "Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "Total parameters (full config with Stim Emo): ‚âà 3.43M."
        },
        {
          "Personality head\n3 layers\n‚Äì": "(3) Fine-tuning: ùõº ‚âà 0.1, LR 10√ó lower, 20‚Äì30 epochs.",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "5.2\nModality Importance"
        },
        {
          "Personality head\n3 layers\n‚Äì": "Parameters are grouped (personality, emotion, base) for indepen-",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "Comparisons in Table 5 illustrate the contributions of each modal-"
        },
        {
          "Personality head\n3 layers\n‚Äì": "dent LRs and weight decay.",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "ity:"
        },
        {
          "Personality head\n3 layers\n‚Äì": "Full\nimplementation (including TaskAttentionTemporal and",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "‚Ä¢ Eye and Pupil: Removing these signals leads to notable"
        },
        {
          "Personality head\n3 layers\n‚Äì": "EmotionHead) is on GitHub1, ensuring reproducibility.",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "drops in valence F1, reflecting the importance of visual and"
        },
        {
          "Personality head\n3 layers\n‚Äì": "This multitask, multimodal pipeline integrates each modality‚Äôs",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "ocular cues in modeling user attention and appraisal."
        },
        {
          "Personality head\n3 layers\n‚Äì": "temporal dynamics, incorporates trial-level features, and disentan-",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "‚Ä¢ GSR: Essential for arousal classification; configurations with-"
        },
        {
          "Personality head\n3 layers\n‚Äì": "gles user-specific from trial-specific signals, improving personaliza-",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "out GSR consistently show reduced arousal macro-F1."
        },
        {
          "Personality head\n3 layers\n‚Äì": "tion and robustness.",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "‚Ä¢ Facial AUs:\nImpact both personality and emotion tasks."
        },
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "Omitting AUs diminishes overall performance across ùëÖ2 and"
        },
        {
          "Personality head\n3 layers\n‚Äì": "5\nResults and Analysis",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "F1 measures."
        },
        {
          "Personality head\n3 layers\n‚Äì": "5.1\nAggregate Performance Trends",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "5.3\nInfluence of Stim Emo"
        },
        {
          "Personality head\n3 layers\n‚Äì": "Tables 4 and 5 present a focused view of the performance across",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "key model configurations. Table 4 summarizes performance for",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "Across Tables 4 and 5, enabling Stim Emo usually yields an im-"
        },
        {
          "Personality head\n3 layers\n‚Äì": "full-modality configurations, comparing cases with and without",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "provement in emotion metrics. Each paired row (Yes vs. No) in Ta-"
        },
        {
          "Personality head\n3 layers\n‚Äì": "Stim Emo. It includes detailed ùëÖ2 values for the Big Five person-",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "ble 5 shows a visible gain in macro-F1 for both valence and arousal"
        },
        {
          "Personality head\n3 layers\n‚Äì": "ality traits and per-class F1 scores for both valence and arousal.",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "when contextual emotional information (Stim Emo) is incorporated."
        },
        {
          "Personality head\n3 layers\n‚Äì": "Table 5 analyzes the impact of missing modalities, highlighting how",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "These findings align with existing research on context-aware affect"
        },
        {
          "Personality head\n3 layers\n‚Äì": "the inclusion or exclusion of certain signals affects performance",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "recognition."
        },
        {
          "Personality head\n3 layers\n‚Äì": "metrics.",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": ""
        },
        {
          "Personality head\n3 layers\n‚Äì": "‚Ä¢ High Personality ùëÖ2: Configurations with all four modali-",
          "‚Äì": "",
          "‚Äì\n0.4\nMLP to 5 traits": "6\nDiscussion"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "metrics.": "‚Ä¢ High Personality ùëÖ2: Configurations with all four modali-"
        },
        {
          "metrics.": "ties (Eye, Pupil, AU, GSR) achieve ùëÖ2 values around 0.94‚Äì0.95"
        },
        {
          "metrics.": "across all Big Five traits (Table 4, rows 2‚Äì5), confirming the"
        },
        {
          "metrics.": "value of multimodal signals for personality inference."
        },
        {
          "metrics.": "‚Ä¢ Valence and Arousal F1:"
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "‚Äì Valence macro-F1 generally lies in the 0.50‚Äì0.63 range,"
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "with the higher end occurring when Eye and AU signals"
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "are included (Table 4, rows 0 and 4)."
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "‚Äì Arousal macro-F1 often peaks around 0.57‚Äì0.60 when GSR"
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "is present alongside Stim Emo, indicating the significance"
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "of GSR in capturing autonomic arousal cues."
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "‚Ä¢ Multitask Trade-Offs: Certain configurations obtain near-"
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "optimal personality ùëÖ2\n(above 0.90) but exhibit moderate"
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "valence/arousal F1 scores, whereas others improve emotion"
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "metrics at the expense of slightly lower ùëÖ2. This pattern is"
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "consistent with standard multitask learning trade-offs."
        },
        {
          "metrics.": ""
        },
        {
          "metrics.": "1https://github.com/itubrainlab/MuMTAffect"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": ""
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": ""
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": "Stim Emo"
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": ""
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": "‚úì"
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": ""
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": "‚úñ"
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": ""
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": "‚úì"
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": ""
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": "‚úñ"
        },
        {
          "Table 4: Performance of Multimodal Multitask Model with Detailed Personality and Emotion scores. Personality R2 shows the": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "O",
          "0.50": "C\nE",
          "0.56": "A",
          "0.53": "N",
          "0.40": "",
          "0.48": "Valence F1 Macro",
          "0.59": "Arousal F1 Macro",
          "0.46": "",
          "0.49": "Avg F1"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "0.95",
          "0.50": "0.95\n0.94",
          "0.56": "0.94",
          "0.53": "0.94",
          "0.40": "",
          "0.48": "0.55",
          "0.59": "0.59",
          "0.46": "",
          "0.49": "0.57"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "0.94",
          "0.50": "0.94\n0.94",
          "0.56": "0.94",
          "0.53": "0.94",
          "0.40": "",
          "0.48": "0.50",
          "0.59": "0.48",
          "0.46": "",
          "0.49": "0.49"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "0.34",
          "0.50": "0.40\n0.38",
          "0.56": "0.64",
          "0.53": "0.47",
          "0.40": "",
          "0.48": "0.53",
          "0.59": "0.57",
          "0.46": "",
          "0.49": "0.55"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "0.25",
          "0.50": "0.46\n0.37",
          "0.56": "0.57",
          "0.53": "0.47",
          "0.40": "",
          "0.48": "0.48",
          "0.59": "0.40",
          "0.46": "",
          "0.49": "0.44"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "0.94",
          "0.50": "0.92\n0.93",
          "0.56": "0.93",
          "0.53": "0.93",
          "0.40": "",
          "0.48": "0.55",
          "0.59": "0.59",
          "0.46": "",
          "0.49": "0.57"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "0.94",
          "0.50": "0.95\n0.95",
          "0.56": "0.96",
          "0.53": "0.95",
          "0.40": "",
          "0.48": "0.49",
          "0.59": "0.44",
          "0.46": "",
          "0.49": "0.47"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "0.08",
          "0.50": "0.30\n0.31",
          "0.56": "0.33",
          "0.53": "0.28",
          "0.40": "",
          "0.48": "0.44",
          "0.59": "0.53",
          "0.46": "",
          "0.49": "0.49"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "-0.16",
          "0.50": "0.11\n0.32",
          "0.56": "0.17",
          "0.53": "0.19",
          "0.40": "",
          "0.48": "0.37",
          "0.59": "0.32",
          "0.46": "",
          "0.49": "0.35"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "0.70",
          "0.50": "0.71\n0.73",
          "0.56": "0.55",
          "0.53": "0.63",
          "0.40": "",
          "0.48": "0.52",
          "0.59": "0.59",
          "0.46": "",
          "0.49": "0.55"
        },
        {
          "0.94": "",
          "0.50": "",
          "0.56": "",
          "0.53": "",
          "0.40": "",
          "0.48": "",
          "0.59": "",
          "0.46": "",
          "0.49": ""
        },
        {
          "0.94": "0.73",
          "0.50": "0.74\n0.78",
          "0.56": "0.71",
          "0.53": "0.79",
          "0.40": "",
          "0.48": "0.43",
          "0.59": "0.50",
          "0.46": "",
          "0.49": "0.47"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "contextualizes priors and sharpens the mapping from core": "affect to emotion labels.",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "Suggested Options and Trade-offs"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "‚Ä¢ Auxiliary Trait Learning as Prediction Context: Lever-",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "Differences in individual baselines and physiological reactivity can"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "aging personality as an auxiliary task provides user-specific",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "create a mismatch between training and test subjects, which lim-"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "priors that disambiguate ambiguous physiological patterns,",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "its generalization. Although we did not add new experiments to"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "particularly when modalities are missing. Emotion F1 in-",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "address this, several approaches could be explored in future work:"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "creases by 5‚Äì8% in partial-modality ablations, underscoring",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "(1) normalization methods that reduce subject-specific variation,"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "the value of personalized embeddings.",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "such as adaptive baselines or learnable normalization; (2) domain-"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "‚Ä¢ Multitask Trade-Offs and Adaptive Weighting: Consis-",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "adversarial",
          "Generalization to Unseen Subjects:": "training (e.g., gradient reversal) or alignment meth-"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "tent with multitask learning literature, optimizing person-",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "ods (e.g., correlation alignment) to make features less dependent"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "ality and emotion jointly introduces task interference. Con-",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "on subject identity; (3) meta-learning strategies that enable few-"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "figurations that maximize trait ùëÖ2 sometimes yield slightly",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "shot adaptation to new users at deployment; (4) test-time adapta-"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "lower emotion F1, suggesting future work on adaptive loss",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "tion techniques, such as entropy minimization or updating batch-"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "weighting or gradient modulation to balance core-affect and",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "normalization statistics, to adjust to covariate shift; (5) calibration"
        },
        {
          "contextualizes priors and sharpens the mapping from core": "conceptual labeling objectives.",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "methods, for example, per-user prototypes or small linear adapters."
        },
        {
          "contextualizes priors and sharpens the mapping from core": "‚Ä¢ Modality Contributions Aligned with Affective The-",
          "6.1": "",
          "Generalization to Unseen Subjects:": ""
        },
        {
          "contextualizes priors and sharpens the mapping from core": "",
          "6.1": "",
          "Generalization to Unseen Subjects:": "In our current work, the use of multitask trait priors and contex-"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "and replacing static Stim Emo labels with dynamic, real-time con-"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "text embeddings. We also plan to introduce learnable task-weighting"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "mechanisms to mitigate multitask trade-offs and to evaluate MuMTAf-"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "fect in unconstrained, in-the-wild settings, addressing practical con-"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "cerns like sensor noise, missing data, and privacy. These directions"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "aim to further enhance personalization, robustness, and contextual"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "sensitivity in real-world affective computing applications."
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "8\nEthical Impact Statement"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "As affective computing systems become more common in everyday"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "life, it is important to understand and address their ethical implica-"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "tions. Our multimodal multitask framework is designed to improve"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "the personalization and accuracy of emotion and personality recog-"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "nition, but its use also raises several ethical concerns."
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "Privacy and Data Protection: Physiological signals such as"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "eye gaze, facial expressions, and galvanic skin response are sensi-"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "tive personal data. Strong privacy protections, secure data storage,"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "and informed consent are essential. Any future implementation"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "should follow relevant data protection laws, such as GDPR, and be"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "transparent about how data is used."
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "Bias and Fairness: While personalization can improve perfor-"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "mance, there is a risk of reinforcing existing biases, including those"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "related to personality stereotypes. Testing on diverse populations"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "and ongoing monitoring are important to ensure fair and equitable"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "performance."
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "Misuse of Technology: Advanced emotion and personality"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "recognition can be misused for unauthorized surveillance, manipu-"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "lation, or unfair decision-making. To reduce this risk, researchers"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "should clearly state the intended uses, set usage guidelines, and"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "support regulatory measures that prevent harmful applications."
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "User Autonomy and Consent: Users should have control over"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "how their emotional and physiological data are used. Clear expla-"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "nations, transparent agreements, and strong consent processes are"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "necessary to maintain user trust."
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "8.1\nDeployment Considerations and Ethics in"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "Practice"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "Data minimization and privacy: Use on-device processing when"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "possible, store only aggregated results, and allow users to enable"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "or disable each modality. Latency and robustness: Use fixed-rate"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "resampling (segment averaging to ùëá =16) and late fusion so the"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "system can still operate if some modalities are missing or sensors"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "fail. Fairness auditing: Track error rates across groups, compare"
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": ""
        },
        {
          "domain-adversarial techniques for rapid adaptation to unseen users,": "macro- and weighted-F1 scores, check for drift over time, and allow"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "facial AUs, GSR and contextual Stim Emo cues to produce simulta-\nhuman review of decisions. Context transparency: If Stim Emo"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "neous valence/arousal classification and Big Five personality pre-\nis used, provide a clear on/off control and record its source for"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "diction. By explicitly separating low-level core-affect encoding (via\nauditing."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "modality-specific transformers) from higher-level conceptualiza-\nOverall, our work aims to support socially responsible affective"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "tion and categorization (via fusion and task-specific attention), our\ncomputing by promoting transparency, continuous ethical review,"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "design adheres to the Theory of Constructed Emotion while de-\nand proactive measures to reduce risks."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "livering strong empirical results: personality ùëÖ2 ‚âà 0.94‚àí0.95 and"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "emotion macro-F1 scores above 0.55."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Acknowledgments"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Future work will\nfocus on integrating additional physiologi-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "CNPq ‚Äî the Brazilian National Council for Scientific and Techno-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "cal and contextual modalities (e.g., EEG, heart-rate variability, or"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "logical Development ‚Äî provided financial support to the second"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "semantic context\nfrom video/audio), adopting meta-learning or"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "author (Fabricio Batista Narcizo) for his Ph.D. research projects"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "domain-adversarial techniques for rapid adaptation to unseen users,"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[grant no: 229760/2013-9]."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "and replacing static Stim Emo labels with dynamic, real-time con-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "text embeddings. We also plan to introduce learnable task-weighting"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "References"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "mechanisms to mitigate multitask trade-offs and to evaluate MuMTAf-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[1] Naveed Ahmed, Zaher Al Aghbari, and Shini Girija. 2023. A systematic survey on"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "fect in unconstrained, in-the-wild settings, addressing practical con-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "multimodal emotion recognition using learning algorithms.\nIntelligent Systems"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "with Applications 17 (2023), 200171. doi:10.1016/j.iswa.2022.200171\ncerns like sensor noise, missing data, and privacy. These directions"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[2]\nSina Alisamir, Fabien Ringeval, and Fran√ßois Portet. 2022. Multi-Corpus Affect"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "aim to further enhance personalization, robustness, and contextual"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Recognition with Emotion Embeddings and Self-Supervised Representations of"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "sensitivity in real-world affective computing applications.\nSpeech. In Proceedings of the 10th International Conference on Affective Computing"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "and Intelligent Interaction (ACII). IEEE, Nara, Japan."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[3] Anisha Ashwath, Michael Peechatt, Cecilia Alm, and Reynold Bailey. 2023. Early"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "8\nEthical Impact Statement\nvs. Late Multimodal Fusion for Recognizing Confusion in Collaborative Tasks."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "In Proceedings of the 11th International Conference on Affective Computing and"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "As affective computing systems become more common in everyday"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Intelligent Interaction (ACII). IEEE, Cambridge, MA, USA."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "life, it is important to understand and address their ethical implica-\nLisa Feldman Barrett. 2017. The theory of constructed emotion: an active infer-\n[4]"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "ence account of interoception and categorization. Social cognitive and affective\ntions. Our multimodal multitask framework is designed to improve"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "neuroscience 12, 1 (2017), 1‚Äì23."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "the personalization and accuracy of emotion and personality recog-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "A Privacy-\n[5] Mohamed Benouis, Yekta Said Can, and Elisabeth Andr√©. 2023."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "nition, but its use also raises several ethical concerns.\nPreserving Multi-Task Learning Framework for Emotion and Identity Recognition"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "from Multimodal Physiological Signals. In Proceedings of the 11th International"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Privacy and Data Protection: Physiological signals such as"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, Cam-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "eye gaze, facial expressions, and galvanic skin response are sensi-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "bridge, MA, USA."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[6] Xin Chai, Qisong Wang, Yongping Zhao, Yongqiang Li, Dan Liu, Xin Liu, and\ntive personal data. Strong privacy protections, secure data storage,"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Ou Bai. 2017. A Fast, Efficient Domain Adaptation Technique for Cross-Domain"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "and informed consent are essential. Any future implementation"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Electroencephalography (EEG)-Based Emotion Recognition. Sensors 17, 5 (2017),"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "should follow relevant data protection laws, such as GDPR, and be\n1014. doi:10.3390/s17051014"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[7] Carles Civit and Agata Lapedriza. 2023. Grounding the Evaluation of Affect\ntransparent about how data is used."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Recognition Models Beyond Dataset-Based Ground Truths. In Proceedings of the"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Bias and Fairness: While personalization can improve perfor-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "11th International Conference on Affective Computing and Intelligent Interaction"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "mance, there is a risk of reinforcing existing biases, including those\n(ACII). IEEE, Cambridge, MA, USA."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[8]\nSidney K. D‚ÄôMello and Jessica Kory. 2015.\nA Review and Meta-Analysis of"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "related to personality stereotypes. Testing on diverse populations"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Multimodal Affect Detection Systems. Comput. Surveys 47, 3 (2015), 43:1‚Äì43:36."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "and ongoing monitoring are important to ensure fair and equitable"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "doi:10.1145/2682899"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[9] Rayan Elalamy, Marios A. Fanourakis, and Guillaume Chanel. 2021. Multi-modal\nperformance."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Emotion Recognition Using Recurrence Plots and Transfer Learning on Physi-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Misuse of Technology: Advanced emotion and personality"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "ological Signals. In Proceedings of the 9th International Conference on Affective"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "recognition can be misused for unauthorized surveillance, manipu-\nIEEE, Nara, Japan (Virtual), 1‚Äì7.\nComputing and Intelligent\nInteraction (ACII)."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "doi:10.1109/ACII52823.2021.9597442\nlation, or unfair decision-making. To reduce this risk, researchers"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[10] Valeria Filippou, Mihalis A. Nicolaou, Nikolas Theodosiou, Georgia Panayiotou,"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "should clearly state the intended uses, set usage guidelines, and"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Elena Constantinou, Marios Theodorou, and Maria Panteli. 2023. Multimodal"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "support regulatory measures that prevent harmful applications.\nPrediction of Alexithymia from Physiological and Audio Signals. In Proceedings of"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "the 11th International Conference on Affective Computing and Intelligent Interaction"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "User Autonomy and Consent: Users should have control over"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "(ACII). IEEE, Cambridge, MA, USA."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "how their emotional and physiological data are used. Clear expla-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[11]\nSreyan Ghosh, Utkarsh Tyagi, S Ramaneswaran, Harshvardhan Srivastava, and"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Dinesh Manocha. 2022. Mmer: Multimodal multi-task learning for speech emo-\nnations, transparent agreements, and strong consent processes are"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "tion recognition. arXiv preprint arXiv:2203.16794 (2022)."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "necessary to maintain user trust."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[12] David Harris, Tom Arthur, Mark Wilson, and Sam Vine. 2023. Eye Tracking for"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Affective Computing in Virtual Reality Healthcare Applications. In Proceedings of"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "the 11th International Conference on Affective Computing and Intelligent Interaction"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "8.1\nDeployment Considerations and Ethics in"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "(ACII). IEEE, Cambridge, MA, USA."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[13] Nathan Henderson, Wookhee Min, Jonathan P. Rowe, and James C. Lester. 2021.\nPractice"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Enhancing Multimodal Affect Recognition with Multi-Task Affective Dynam-"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Data minimization and privacy: Use on-device processing when\nics Modeling.\nIn Proceedings of\nthe 9th International Conference on Affective"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "IEEE, Nara, Japan (Virtual), 1‚Äì8.\nComputing and Intelligent\nInteraction (ACII).\npossible, store only aggregated results, and allow users to enable"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "doi:10.1109/ACII52823.2021.9597432"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "or disable each modality. Latency and robustness: Use fixed-rate"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[14]\nF.\nIacono and M. Khan. 2024. Multimodal Fusion of EEG and Eye-Tracking"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "resampling (segment averaging to ùëá =16) and late fusion so the\nFeatures for Cross-Subject Emotion Recognition. Neurocomputing XXX (2024),"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "XXX‚ÄìXXX. Forthcoming."
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "system can still operate if some modalities are missing or sensors"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "[15] Meisam J. Seikavandi, Laurits Dixen, Jostein Fimland, Sree K. Desu, Ye Zserai,"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "fail. Fairness auditing: Track error rates across groups, compare"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "S. Lee, Antonia-Bianca, Maria Barrett, and Paolo Burelli. 2025. AFFEC - Advancing"
        },
        {
          ",\nMeisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Dittberner, & Paolo Burelli": "Face-to-Face Emotion Communication: A Multimodal Dataset.\nmacro- and weighted-F1 scores, check for drift over time, and allow"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[16] Oliver P John, Eileen M Donahue, and Robert L Kentle. 1991. Big five inventory.\nContext to Challenge Affect Modeling: A Study on FPS Games Engagement Pre-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Journal of personality and social psychology (1991).\ndiction. In Proceedings of the 12th International Conference on Affective Computing"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[17] R. Ju, H. Wang, and P. Li. 2024. A novel methodology for emotion recognition\nand Intelligent Interaction (ACII). IEEE, Glasgow, UK."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "through 62-lead EEG.\nJean Vroomen, and Bruno Rossion. 2013.\n[34] Gilles Pourtois, Beatrice de Gelder,\nFrontiers in Physiology 14 (2024), 1425582. doi:10.3389/"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "fphys.2023.1425582\nMultimodal emotion perception: from basic mechanisms to individual differences."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[18] Atieh Kashani,\nJohannes Pfau, and Magy Seif El-Nasr. 2023.\nAssessing the\nPhilosophical Transactions of the Royal Society B: Biological Sciences 368, 1623"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Impact of Personality on Affective States from Video Game Communication.\n(2013), 20130177."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[35]\nShrutika S. Sawant et al. 2023. Transformer-based Self-Supervised Representation\nIn Proceedings of the 11th International Conference on Affective Computing and"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Intelligent Interaction (ACII). IEEE, Cambridge, MA, USA.\nLearning for Emotion Recognition Using Bio-signal Feature Fusion. Proceedings"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[19]\nSander Koelstra, Christian M√ºhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan\nof the 2023 ACM International Conference on Multimedia Retrieval (2023), 682‚Äì690."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. 2012.\ndoi:10.1145/3576842.3615837"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "DEAP: A Database for Emotion Analysis Using Physiological Signals.\n[36] Mohammad Soleymani, Sadjad Asghari-Esfeden, Yun Fu, and Maja Pantic.\nIEEE"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "2016.\nAnalysis of EEG Signals and Facial Expressions for Continuous Emo-\nTransactions on Affective Computing 3, 1 (2012), 18‚Äì31. doi:10.1109/T-AFFC.2011."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "15\ntion Detection.\nIEEE Transactions on Affective Computing 7, 1 (2016), 17‚Äì28."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[20]\nFlorian Larradet, Fr√©d√©ric Dehais, Cl√©ment Cali√©, and Fr√©d√©ric Peysakhovich.\ndoi:10.1109/TAFFC.2015.2436926"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[37] Ramakrishnan Subramanian, Abhinav Dhall, Mohammad Nauman, Karan K.\n2020. A Review of Physiological and Subjective Assessments in Aviation. Frontiers"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Jain, and Nicu Sebe. 2018. ASCERTAIN: Emotion and Personality Recognition\nin Neuroscience 14 (2020), 594. doi:10.3389/fnins.2020.00594"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Using Commercial Sensors.\n[21] Xiaobai Li, Jie Tao, Guoying Zhao, and Matti Pietik√§inen. 2018. Affective State\nIEEE Transactions on Affective Computing 9, 2 (2018),"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Recognition from Facial Expressions: A Survey.\n147‚Äì160. doi:10.1109/TAFFC.2016.2625250\nIEEE Transactions on Affective"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[38] Ramanathan Subramanian, Julia Wache, Mohammad K. Abadi, Radu-Laurentiu\nComputing 9, 1 (2018), 18‚Äì37. doi:10.1109/TAFFC.2016.2518160"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Vieriu, Stefan Winkler, and Nicu Sebe. 2018. ASCERTAIN: Emotion and Per-\n[22]\nYang Li, Amirmohammad Kazameini, Yash Mehta, and Erik Cambria. 2022. Mul-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "sonality Recognition Using Commercial Sensors.\nIEEE Transactions on Affective\ntitask Learning for Emotion and Personality Traits Detection. Neurocomputing"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Computing 9, 2 (2018), 147‚Äì160. doi:10.1109/TAFFC.2016.2625250\n493 (2022), 340‚Äì350. doi:10.1016/j.neucom.2022.04.049"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Sanaz Taherzadeh, Akiri Surely, Vasundhara Misal, and Andrea Kleinsmith. 2022.\n[39]\n[23] Y. Li, A. Kazemeini, Y. Mehta, and E. Cambria. 2022. Multitask learning for"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Exploring Affective Dimension Perception from Bodily Expressions and Elec-\nemotion and personality traits detection. Applied Soft Computing 123 (2022),"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "trodermal Activity in Paramedic Simulation Training. In Proceedings of the 10th\n108986."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "International Conference on Affective Computing and Intelligent Interaction (ACII).\n[24] Y. Li, S. Ma, Z. Zhao, and Y. Wang. 2023. Multi-task learning and multi-fusion"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "IEEE, Nara, Japan.\naudiotext emotion recognition in conversation. Sensors 23, 7 (2023), 3601."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[40] Zhihang Tian, Dongmin Huang, Sijin Zhou, Zhidan Zhao, and Dazhi Jiang. 2021.\n[25] Christine L. Lisetti and Fadi Nasoz. 2004. Using Noninvasive Wearable Com-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Personality first in emotion: a deep neural network based on electroencephalo-\nputers to Recognize Human Emotions from Physiological Signals.\nEURASIP"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "gram channel attention for cross-subject emotion recognition.\nRoyal Society\nJournal on Advances in Signal Processing 2004, 11 (2004), 1672‚Äì1687. doi:10.1155/"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Open Science 8, 8 (2021), 201976. doi:10.1098/rsos.201976\nS1110865704406192"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[41]\nJuan V√°zquez-Rodr√≠guez, Gr√©goire Lefebvre, Julien Cumin, and James L. Crowley.\n[26]\nJiyao Liu, Lang He, Zhiwei Chen, Ziyi Chen, Yu Hao, and Dongmei Jiang. 2023."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "2022. Emotion Recognition with Pre-Trained Transformers Using Multimodal\nContext-Aware EEG-Based Perceived Stress Recognition Based on Emotion Tran-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Signals. In Proceedings of the 10th International Conference on Affective Computing\nsition Paradigm. In Proceedings of the 11th International Conference on Affective"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "and Intelligent Interaction (ACII). IEEE, Nara, Japan, 1‚Äì8. doi:10.1109/ACII55700.\nComputing and Intelligent Interaction (ACII). IEEE, Cambridge, MA, USA."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "2022.9953852\n[27] X. Lv, G. Zhao, and Y. Liu. 2022. EEG-based emotion recognition using hybrid"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[42]\nPatrik Vuilleumier, Antoine Armony, and Raymond Dolan. 2014. Neural process-\nCNN and LSTM classification. Frontiers in Computational Neuroscience 16 (2022),"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "ing of emotion in multimodal settings. Frontiers in Human Neuroscience 8 (2014),\n1019776. doi:10.3389/fncom.2022.1019776"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "822.\n[28] Aarti Malhotra, Garima Sharma, Rishikesh Kumar, Abhinav Dhall, and Jesse Hoey."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[43] X. Wang, Y. Zhou, and T. Zhang. 2024. EEG-based emotion recognition using\n2023. Social Event Context and Affect Prediction in Group Videos. In Proceed-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "multi-scale dynamic CNN and gated transformer. Frontiers in Neuroscience 17\nings of the 11th International Conference on Affective Computing and Intelligent"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "(2024), 11682401. doi:10.3389/fnins.2023.1168240\nInteraction (ACII). IEEE, Cambridge, MA, USA."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Pei Yang, Niqi Liu, Xinge Liu, Yezhi Shu, Wenqi Ji, Ziqi Ren, Jenny Sheng, Minjing\n[44]\nRegularizing representations for\n[29] M. Parthasarathy and S. Narayanan. 2018."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Yu, Ran Yi, Dan Zhang, and Yong-Jin Liu. 2024. A multimodal dataset for mixed\nemotional attributes with unsupervised auxiliary tasks.\nIn ICASSP 2018-2018"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "emotion recognition. Scientific Data 11 (2024), 847. doi:10.1038/s41597-024-03676-\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "4\n2446‚Äì2450."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[45] Huseyin E. Yildirim and Deniz Iren. 2023.\nInformative Speech Features Based\n[30]\nSilke Paulmann and Marc D. Pell. 2011.\nIs there an advantage for recognizing"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "on Emotion Classes and Gender in Explainable Speech Emotion Recognition.\nmulti-modal emotional stimuli? Emotion 11, 1 (2011), 1."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "In Proceedings of the 11th International Conference on Affective Computing and\n[31] Antonia Petrogianni, Lefteris Kapelonis, Nikos Antoniou, Sofia Eleftheriou, Petros"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Intelligent Interaction (ACII). IEEE, Cambridge, MA, USA.\nMitseas, Dimitris Sgouropoulos, Athanasios Katsamanis, Thodoris Giannakopou-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[46] Han Yu, Thomas Vaessen, Inez Myin-Germeys, and Akane Sano. 2021. Modality\nlos, and Shrikanth Narayanan. 2024. RobuSER: A Robustness Benchmark for"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Fusion Network and Personalized Attention in Momentary Stress Detection in\nSpeech Emotion Recognition. In Proceedings of the 12th International Conference"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "the Wild. In Proceedings of the 9th International Conference on Affective Computing\non Affective Computing and Intelligent Interaction (ACII). IEEE, Glasgow, UK."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "and Intelligent Interaction (ACII). IEEE, Nara, Japan (Virtual).\n[32] Rosalind W. Picard. 1997. Affective Computing. MIT Press."
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "[47] Guangyi Zhao, Xiaobai Li, Guoying Zhao, and Matti Pietik√§inen. 2018. Learning\n[33] Kosmas Pinitas, Nemanja Rasajki, Matthew Barthet, Maria Kaselimi, Konstanti-"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "Discriminative Representation for Multimodal Emotion Recognition.\nIEEE Trans-\nnos Makantasis, Antonios Liapis, and Georgios N. Yannakakis. 2024. Varying the"
        },
        {
          "MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition\n,": "actions on Multimedia 20, 9 (2018), 2526‚Äì2535. doi:10.1109/TMM.2018.2808766"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A systematic survey on multimodal emotion recognition using learning algorithms",
      "authors": [
        "Naveed Ahmed",
        "Zaher Al Aghbari",
        "Shini Girija"
      ],
      "year": "2023",
      "venue": "Intelligent Systems with Applications",
      "doi": "10.1016/j.iswa.2022.200171"
    },
    {
      "citation_id": "2",
      "title": "Multi-Corpus Affect Recognition with Emotion Embeddings and Self-Supervised Representations of Speech",
      "authors": [
        "Sina Alisamir",
        "Fabien Ringeval",
        "Fran√ßois Portet"
      ],
      "year": "2022",
      "venue": "Proceedings of the 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "3",
      "title": "Early vs. Late Multimodal Fusion for Recognizing Confusion in Collaborative Tasks",
      "authors": [
        "Anisha Ashwath",
        "Michael Peechatt",
        "Cecilia Alm",
        "Reynold Bailey"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "4",
      "title": "The theory of constructed emotion: an active inference account of interoception and categorization",
      "authors": [
        "Lisa Feldman"
      ],
      "year": "2017",
      "venue": "Social cognitive and affective neuroscience"
    },
    {
      "citation_id": "5",
      "title": "A Privacy-Preserving Multi-Task Learning Framework for Emotion and Identity Recognition from Multimodal Physiological Signals",
      "authors": [
        "Mohamed Benouis",
        "Yekta Said Can",
        "Elisabeth Andr√©"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "6",
      "title": "A Fast, Efficient Domain Adaptation Technique for Cross-Domain Electroencephalography (EEG)-Based Emotion Recognition",
      "authors": [
        "Xin Chai",
        "Qisong Wang",
        "Yongping Zhao",
        "Yongqiang Li",
        "Dan Liu",
        "Xin Liu",
        "Ou Bai"
      ],
      "year": "2017",
      "venue": "Sensors",
      "doi": "10.3390/s17051014"
    },
    {
      "citation_id": "7",
      "title": "Grounding the Evaluation of Affect Recognition Models Beyond Dataset-Based Ground Truths",
      "authors": [
        "Carles Civit",
        "Agata Lapedriza"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "8",
      "title": "A Review and Meta-Analysis of Multimodal Affect Detection Systems",
      "authors": [
        "Sidney Mello",
        "Jessica Kory"
      ],
      "year": "2015",
      "venue": "Comput. Surveys",
      "doi": "10.1145/2682899"
    },
    {
      "citation_id": "9",
      "title": "Multi-modal Emotion Recognition Using Recurrence Plots and Transfer Learning on Physiological Signals",
      "authors": [
        "Rayan Elalamy",
        "Marios Fanourakis",
        "Guillaume Chanel"
      ],
      "year": "2021",
      "venue": "Proceedings of the 9th International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII52823.2021.9597442"
    },
    {
      "citation_id": "10",
      "title": "Multimodal Prediction of Alexithymia from Physiological and Audio Signals",
      "authors": [
        "Valeria Filippou",
        "Mihalis Nicolaou",
        "Nikolas Theodosiou",
        "Georgia Panayiotou",
        "Elena Constantinou",
        "Marios Theodorou",
        "Maria Panteli"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "11",
      "title": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "authors": [
        "Sreyan Ghosh",
        "Utkarsh Tyagi",
        "S Ramaneswaran",
        "Harshvardhan Srivastava",
        "Dinesh Manocha"
      ],
      "year": "2022",
      "venue": "Mmer: Multimodal multi-task learning for speech emotion recognition",
      "arxiv": "arXiv:2203.16794"
    },
    {
      "citation_id": "12",
      "title": "Eye Tracking for Affective Computing in Virtual Reality Healthcare Applications",
      "authors": [
        "David Harris",
        "Tom Arthur",
        "Mark Wilson",
        "Sam Vine"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "13",
      "title": "Enhancing Multimodal Affect Recognition with Multi-Task Affective Dynamics Modeling",
      "authors": [
        "Nathan Henderson",
        "Wookhee Min",
        "Jonathan Rowe",
        "James Lester"
      ],
      "year": "2021",
      "venue": "Proceedings of the 9th International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII52823.2021.9597432"
    },
    {
      "citation_id": "14",
      "title": "Multimodal Fusion of EEG and Eye-Tracking Features for Cross-Subject Emotion Recognition",
      "authors": [
        "F Iacono",
        "M Khan"
      ],
      "year": "2024",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "15",
      "title": "AFFEC -Advancing Face-to-Face Emotion Communication: A Multimodal Dataset",
      "authors": [
        "J Meisam",
        "Laurits Seikavandi",
        "Jostein Dixen",
        "Fimland",
        "K Sree",
        "Ye Desu",
        "S Zserai",
        "Antonia Lee",
        "Maria Bianca",
        "Paolo Barrett",
        "Burelli"
      ],
      "year": "2025",
      "venue": "AFFEC -Advancing Face-to-Face Emotion Communication: A Multimodal Dataset"
    },
    {
      "citation_id": "16",
      "title": "Big five inventory",
      "authors": [
        "Eileen Oliver P John",
        "Robert Donahue",
        "Kentle"
      ],
      "year": "1991",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "17",
      "title": "A novel methodology for emotion recognition through 62-lead EEG",
      "authors": [
        "R Ju",
        "H Wang",
        "P Li"
      ],
      "year": "2024",
      "venue": "Frontiers in Physiology",
      "doi": "10.3389/fphys.2023.1425582"
    },
    {
      "citation_id": "18",
      "title": "Assessing the Impact of Personality on Affective States from Video Game Communication",
      "authors": [
        "Atieh Kashani",
        "Johannes Pfau",
        "Magy Seif El-Nasr"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "19",
      "title": "DEAP: A Database for Emotion Analysis Using Physiological Signals",
      "authors": [
        "Sander Koelstra",
        "Christian M√ºhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Thierry Pun"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2011.15"
    },
    {
      "citation_id": "20",
      "title": "A Review of Physiological and Subjective Assessments in Aviation",
      "authors": [
        "Florian Larradet",
        "Fr√©d√©ric Dehais"
      ],
      "year": "2020",
      "venue": "Frontiers in Neuroscience",
      "doi": "10.3389/fnins.2020.00594"
    },
    {
      "citation_id": "21",
      "title": "Affective State Recognition from Facial Expressions: A Survey",
      "authors": [
        "Xiaobai Li",
        "Jie Tao",
        "Guoying Zhao",
        "Matti Pietik√§inen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2518160"
    },
    {
      "citation_id": "22",
      "title": "Multitask Learning for Emotion and Personality Traits Detection",
      "authors": [
        "Yang Li",
        "Amirmohammad Kazameini",
        "Yash Mehta",
        "Erik Cambria"
      ],
      "year": "2022",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2022.04.049"
    },
    {
      "citation_id": "23",
      "title": "Multitask learning for emotion and personality traits detection",
      "authors": [
        "Y Li",
        "A Kazemeini",
        "Y Mehta",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "24",
      "title": "Multi-task learning and multi-fusion audiotext emotion recognition in conversation",
      "authors": [
        "Y Li",
        "S Ma",
        "Z Zhao",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "25",
      "title": "Using Noninvasive Wearable Computers to Recognize Human Emotions from Physiological Signals",
      "authors": [
        "Christine Lisetti",
        "Fadi Nasoz"
      ],
      "year": "2004",
      "venue": "EURASIP Journal on Advances in Signal Processing",
      "doi": "10.1155/S1110865704406192"
    },
    {
      "citation_id": "26",
      "title": "Context-Aware EEG-Based Perceived Stress Recognition Based on Emotion Transition Paradigm",
      "authors": [
        "Jiyao Liu",
        "Lang He",
        "Zhiwei Chen",
        "Ziyi Chen",
        "Yu Hao",
        "Dongmei Jiang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "27",
      "title": "EEG-based emotion recognition using hybrid CNN and LSTM classification",
      "authors": [
        "X Lv",
        "G Zhao",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "Frontiers in Computational Neuroscience",
      "doi": "10.3389/fncom.2022.1019776"
    },
    {
      "citation_id": "28",
      "title": "Social Event Context and Affect Prediction in Group Videos",
      "authors": [
        "Aarti Malhotra",
        "Garima Sharma",
        "Rishikesh Kumar",
        "Abhinav Dhall",
        "Jesse Hoey"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "29",
      "title": "Regularizing representations for emotional attributes with unsupervised auxiliary tasks",
      "authors": [
        "M Parthasarathy",
        "S Narayanan"
      ],
      "year": "2018",
      "venue": "ICASSP 2018-2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Is there an advantage for recognizing multi-modal emotional stimuli?",
      "authors": [
        "Silke Paulmann",
        "Marc Pell"
      ],
      "year": "2011",
      "venue": "Emotion"
    },
    {
      "citation_id": "31",
      "title": "RobuSER: A Robustness Benchmark for Speech Emotion Recognition",
      "authors": [
        "Antonia Petrogianni",
        "Lefteris Kapelonis",
        "Nikos Antoniou",
        "Sofia Eleftheriou",
        "Petros Mitseas",
        "Dimitris Sgouropoulos",
        "Athanasios Katsamanis"
      ],
      "year": "2024",
      "venue": "Proceedings of the 12th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "32",
      "title": "Affective Computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Varying the Context to Challenge Affect Modeling: A Study on FPS Games Engagement Prediction",
      "authors": [
        "Kosmas Pinitas",
        "Nemanja Rasajki",
        "Matthew Barthet",
        "Maria Kaselimi",
        "Konstantinos Makantasis",
        "Antonios Liapis",
        "Georgios Yannakakis"
      ],
      "year": "2024",
      "venue": "Proceedings of the 12th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "34",
      "title": "Multimodal emotion perception: from basic mechanisms to individual differences",
      "authors": [
        "Gilles Pourtois",
        "Beatrice De Gelder",
        "Jean Vroomen",
        "Bruno Rossion"
      ],
      "year": "2013",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "35",
      "title": "Transformer-based Self-Supervised Representation Learning for Emotion Recognition Using Bio-signal Feature Fusion",
      "authors": [
        "S Shrutika",
        "Sawant"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 ACM International Conference on Multimedia Retrieval",
      "doi": "10.1145/3576842.3615837"
    },
    {
      "citation_id": "36",
      "title": "Analysis of EEG Signals and Facial Expressions for Continuous Emotion Detection",
      "authors": [
        "Mohammad Soleymani",
        "Sadjad Asghari-Esfeden",
        "Yun Fu",
        "Maja Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2436926"
    },
    {
      "citation_id": "37",
      "title": "ASCERTAIN: Emotion and Personality Recognition Using Commercial Sensors",
      "authors": [
        "Ramakrishnan Subramanian",
        "Abhinav Dhall",
        "Mohammad Nauman",
        "Karan Jain",
        "Nicu Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2625250"
    },
    {
      "citation_id": "38",
      "title": "ASCERTAIN: Emotion and Personality Recognition Using Commercial Sensors",
      "authors": [
        "Ramanathan Subramanian",
        "Julia Wache",
        "Mohammad Abadi",
        "Radu-Laurentiu Vieriu",
        "Stefan Winkler",
        "Nicu Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2625250"
    },
    {
      "citation_id": "39",
      "title": "Exploring Affective Dimension Perception from Bodily Expressions and Electrodermal Activity in Paramedic Simulation Training",
      "authors": [
        "Sanaz Taherzadeh",
        "Akiri Surely",
        "Vasundhara Misal",
        "Andrea Kleinsmith"
      ],
      "year": "2022",
      "venue": "Proceedings of the 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "40",
      "title": "Personality first in emotion: a deep neural network based on electroencephalogram channel attention for cross-subject emotion recognition",
      "authors": [
        "Zhihang Tian",
        "Dongmin Huang",
        "Sijin Zhou",
        "Zhidan Zhao",
        "Dazhi Jiang"
      ],
      "year": "2021",
      "venue": "Royal Society Open Science",
      "doi": "10.1098/rsos.201976"
    },
    {
      "citation_id": "41",
      "title": "Emotion Recognition with Pre-Trained Transformers Using Multimodal Signals",
      "authors": [
        "Juan V√°zquez-Rodr√≠guez",
        "Gr√©goire Lefebvre",
        "Julien Cumin",
        "James Crowley"
      ],
      "year": "2022",
      "venue": "Proceedings of the 10th International Conference on Affective Computing and Intelligent Interaction (ACII)",
      "doi": "10.1109/ACII55700.2022.9953852"
    },
    {
      "citation_id": "42",
      "title": "Neural processing of emotion in multimodal settings",
      "authors": [
        "Patrik Vuilleumier",
        "Antoine Armony",
        "Raymond Dolan"
      ],
      "year": "2014",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "43",
      "title": "EEG-based emotion recognition using multi-scale dynamic CNN and gated transformer",
      "authors": [
        "X Wang",
        "Y Zhou",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "Frontiers in Neuroscience",
      "doi": "10.3389/fnins.2023.1168240"
    },
    {
      "citation_id": "44",
      "title": "A multimodal dataset for mixed emotion recognition",
      "authors": [
        "Pei Yang",
        "Niqi Liu",
        "Xinge Liu",
        "Yezhi Shu",
        "Wenqi Ji",
        "Ziqi Ren",
        "Jenny Sheng",
        "Minjing Yu",
        "Ran Yi",
        "Dan Zhang",
        "Yong-Jin Liu"
      ],
      "year": "2024",
      "venue": "Scientific Data",
      "doi": "10.1038/s41597-024-03676-4"
    },
    {
      "citation_id": "45",
      "title": "Informative Speech Features Based on Emotion Classes and Gender in Explainable Speech Emotion Recognition",
      "authors": [
        "E Huseyin",
        "Deniz Yildirim",
        "Iren"
      ],
      "year": "2023",
      "venue": "Proceedings of the 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "46",
      "title": "Modality Fusion Network and Personalized Attention in Momentary Stress Detection in the Wild",
      "authors": [
        "Han Yu",
        "Thomas Vaessen",
        "Inez Myin-Germeys",
        "Akane Sano"
      ],
      "year": "2021",
      "venue": "Proceedings of the 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "47",
      "title": "Learning Discriminative Representation for Multimodal Emotion Recognition",
      "authors": [
        "Guangyi Zhao",
        "Xiaobai Li",
        "Guoying Zhao",
        "Matti Pietik√§inen"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2018.2808766"
    }
  ]
}