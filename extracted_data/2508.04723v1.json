{
  "paper_id": "2508.04723v1",
  "title": "Wearable Music2Emotion : Assessing Emotions Induced By Ai-Generated Music Through Portable Eeg-Fnirs Fusion",
  "published": "2025-08-05T12:25:35Z",
  "authors": [
    "Sha Zhao",
    "Song Yi",
    "Yangxuan Zhou",
    "Jiadong Pan",
    "Jiquan Wang",
    "Jie Xia",
    "Shijian Li",
    "Shurong Dong",
    "Gang Pan"
  ],
  "keywords": [
    "Music-induced emotion",
    "EEG",
    "fNIRS"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotions critically influence mental health, driving interest in music-based affective computing via neurophysiological signals with Braincomputer Interface techniques. While prior studies leverage music's accessibility for emotion induction, three key limitations persist: (1) Stimulus Constraints: Music stimuli are confined to small corpora due to copyright and curation costs, with selection biases from heuristic emotion-music mappings that ignore individual affective profiles. (2) Modality Specificity: Overreliance on unimodal neural data (e.g., EEG) ignores complementary insights from cross-modal signal fusion. (3) Portability Limitation: Cumbersome setups (e.g., 64+ channel gel-based EEG caps) hinder real-world applicability due to procedural complexity and portability barriers. To address these limitations, we propose MEEtBrain, a portable and multimodal framework for emotion analysis (valence/arousal), integrating AI-generated music stimuli with synchronized EEG-fNIRS acquisition via a wireless headband. By MEEtBrain, the music stimuli can be automatically generated by AI on a large scale, eliminating subjective selection biases while ensuring music diversity. We use our developed portable device that is designed in a lightweight headband-style and uses dry electrodes, to simultaneously collect EEG and fNIRS recordings. A 14-hour dataset from 20 participants was collected in the first recruitment to validate the framework's efficacy, with AI-generated music eliciting target emotions (valence/arousal). We are actively expanding our multimodal dataset (44 participants in the latest dataset) and make it publicly available to promote further research and practical applications. The dataset is available at https://zju-bmi-lab.github.io/ZBra. CCS Concepts: ‚Ä¢ Human-centered computing ‚Üí HCI design and evaluation methods; ‚Ä¢ Computing methodologies ‚Üí Cognitive science.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotions significantly impact human health and are linked to mental health disorders, such as depression, anxiety, attention deficit hyperactivity disorder, and internet addiction  [11, 28, 41] . This underscores the necessity of accurate emotion recognition and effective regulation strategies in disease prevention and therapeutic management. Music facilitates emotional regulation, supported by its prevalent application in daily stress reduction and mood enhancement. Compared to other media (e.g., videos), music's high accessibility makes it a more practical and scalable tool for emotional regulation. Prior research has examined music-induced emotions and their interplay  [2, 3] , yet current paradigms primarily rely on subjective self-reports to evaluate emotional responses. Such approaches are inherently limited by introspective biases (e.g., retrospective inaccuracies) and inter-subject variability in affective labeling  [39] [40] [41] . Thus, establishing an objective mapping between affective states and music patterns is critical, as this systematic approach is imperative to advance toward precision music-based emotion regulation.\n\nEmotions are closely tied to neural dynamics of the brain. For instance, elevated affective states (e.g., joy) correlate with heightened cortical activation, whereas depressive states associate with reduced neural oscillatory activity. Such neural dynamics can be captured via brain-computer interface (BCI) systems, which decode neural signatures and enable direct interaction with external environments. Consequently, non-invasive BCI modalities (e.g., EEG, electroencephalography) offer a valid framework for objective emotion research due to their intrinsic safety and ease of use. Emerging studies have leveraged these techniques to discriminate affective states through neural signals  [23-25, 49, 51, 52, 55] . Notably, recent advances have targeted music-induced emotion recognition and regulation through brain signals, demonstrating efficacy  [6, 14, 22] .\n\nNevertheless, critical limitations persist on the stimuli music, brain signal modality, and device portability.  (1)  Musical stimuli limitation: Existing studies predominantly employ small-scale music corpora (e.g., 6  [27] , 16  [35]  and 20  [46] ), constrained by copyright barriers and curation costs  [39] . This could cause statistical bias or even risking model overfitting during emotion recognition. Moreover, the music stimuli are usually chosen subjectively based on experimenters' heuristic assumptions about emotion-music mappings, neglecting idiosyncratic affective profiles across individuals. It could reduce the model robustness and generalizability  [13, 39] . Besides, widely recognized musical pieces activate pre-existing autobiographical associations, thereby negatively affecting intended emotion induction through extraneous cognitive processing (e.g., memory retrieval) rather than acoustically-driven affective responses  [16, 35, 39] .\n\n(2) Brain signal modality constraint: Most of affective computing paradigms rely on unimodal neural signal (e.g., EEG-only)  [6] . While EEG excels in temporal resolution and remains the dominant modality for non-invasive affective computing  [35, 45] , its spatial specificity is intrinsically constrained by scalp-level signal attenuation. Multimodal fusion (e.g., EEG-fNIRS, functional Near Infrared Spectroscopy) could mitigate this by providing complementary clues for analyzing emotions. For instance, fNIRS capture distinct patterns of hemodynamic responses  [12] , and performs well in emotion recognition  [42] .\n\n(3) Portability limitation: While music's intrinsic accessibility facilitates emotion induction in a convenient fashion, existing paradigms overlook another critical role to promote the musicinduced emotion: the device portability. Cumbersome setups (e.g., 64+ channel EEG caps requiring conductive gel) demand specialized infrastructure and operator expertise, increasing costs and restricting scalability  [2, 3] . Multimodal monitoring (e.g., EEG-EMG, electromyogram-ECG, electrocardiogram) necessitates more than 30 minutes for setup  [14] . These operational bottlenecks fundamentally constrain real-world applicability.\n\nTo address the aforementioned limitations, we propose a portable and multimodal framework for analyzing emotions induced by AI-generated music and collecting EEG and fNIRS through our portable headband. For the first challenge, we use music generated by AIGC (AI-Generated Content) to induce emotions instead of selecting well-known music pieces, overcoming the limitation in the scale, diversity, subjective selection and bias emotion responses. We first design prompt templates based on the Valence-Arousal model  [43] , and then the prompt sentences created by the templates are input to automatically generate music clips. For the second and third challenges, we use our developed portable device to simultaneously collect EEG and fNIRS recordings both of which are used for emotion recognition. Our device is designed in a lightweight headband-style and uses dry electrodes, making it easy to wear and user-friendly. We design data collection paradigm and acquire multimodal brain signals in the real-world. On the basis of the dataset, the generated music clips are verified and the emotion-inducing effects are evaluated, indicating the generated music effectively evokes the intended emotions. Our contributions are as follows:\n\n‚Ä¢ We propose a portable and multi-modal framework for systematic acquisition of brain signals induced by AI-generated music, through a wireless portable headband. Evaluated on our collected real-world dataset, this framework overcomes the limitations in music stimuli, signal modality and portability, promoting the real-world applicability and accessibility of emotion regulation. ‚Ä¢ The music stimuli are automatically generated by AIGC techniques, which allow for creation on a large scale without relying on subjective selection. Verified on our real-world dataset, the generated music can effectively evokes emotions, addressing the issues in terms of the small scale, limited diversity and bias emotion responses.\n\n‚Ä¢ The acquisition device is portable and multimodal integrated, easy to wear and simultaneously recording EEG and fNIRS for emotion analysis. We collected a real-world dataset of EEG and fNIRS from 20 individuals in the first recruitment, with a total duration of about 14 hours. We are actively expanding our multimodal dataset (44 participants in the latest dataset) and make it publicly available to promote further research and practical applications. 2 Related Work",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Music-Induced Emotions And Physiological Responses",
      "text": "The studies of music-induced emotions have been guided by two dominant theoretical models: the discrete emotion model and the dimensional/continuous emotion model, which includes Valence-Arousal bipolar coordinate system proposed by Russell  [43] . The discrete emotion model posits that emotions fall into fundamental categories such as happiness, misery, and anger, each of which is biologically \"hardwired\" and characterized by distinct and separate physiological, neural, and expressive markers  [9, 35] . In contrast, the dimensional/continuous emotion model represents emotions along continuous axes, such as\n\nValence (pleasantness-unpleasantness) and Arousal (activation-deactivation)  [18, 26] .\n\nMusic-induced emotions are often accompanied by measurable physiological changes, making physiological signals a valuable resource for emotion recognition. Several modalities have been explored to assess emotional responses, such as heart rate, skin conductance, and respiratory patterns. It was found that rhythmic features of music, such as tempo and articulation, influence these physiological signals, driving changes such as increased heart rate and faster breathing  [18, 19] . Techniques such as EEG, functional near-infrared spectroscopy (fNIRS), and positron emission tomography (PET) have also been employed\n\nto measure music-induced emotions. Notably, pleasant music has been shown to enhance prefrontal cortex activity while inhibiting the amygdala, facilitating emotional regulation  [35, 39] . Recent advances in multi-modal frameworks integrate various physiological signals, such as EEG and fNIRS, to provide a comprehensive understanding of emotional responses to music  [33, 42] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emotion Recognition Using Physiological Signals",
      "text": "EEG has been extensively studied as a tool for emotion recognition owing to its ability to capture real-time brain activity. Early methods relied on manually extracted features, such as power spectral density and coherence, combined with threshold-based rules for classification  [36] . In comparison, machine learning techniques, such as Linear Discriminant Analysis (LDA)  [50] ,\n\nSupport Vector Machines (SVM)  [30, 35, 50] , and K-Nearest Neighbor (KNN)  [38] , provide more sophisticated, data-driven approaches to pattern recognition. However, they still rely heavily on feature engineering. With the rapid development of deep learning techniques, models such as Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), and\n\nTransformers have shown remarkable improvements in both classification accuracy and robustness  [8, 34, 37, 48, 53, 54] . For example, Atkinson and Campos  [2]  employed kernel classifiers combined with a feature selection method to perform binary classification on the DEAP dataset  [26] .\n\nWhile some studies have investigated the role of various physiological signals in emotion recognition and regulation  [23, 47] ,\n\nthey seldom emphasize practical usability. For instance, Kim and Andr√©  [23]  employed a four-channel biosensor system to measure electromyogram (EMG), electrocardiogram (ECG), skin conductivity, and respiration changes for emotion recognition.\n\nHowever, the complexity of setup, and vulnerability to various interferences considerably limit the practical applicability of such systems in real-world environments. In our study, we attempt to strike a balance between device portability and accuracy in emotion recognition, using EEG-fNIRS multimodal data to achieve the highest accuracy while ensuring applicability to daily life. (b) Template design.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Examples",
      "text": "HAHV: A very cheerful and energetic music piece, featuring vibrant guitar riffs and a dynamic drum beat, with a mood that feels exciting and triumphant. It is suitable for a festival or party celebration.\n\nLAHV: A very peaceful and calm music piece, featuring soft piano and gentle strings, with a mood that feels smoothing and tender.\n\nIt is suitable for meditation or a quiet evening.\n\n(c) Examples of template. 3 MEEtBrain Framework",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Overview",
      "text": "We propose a novel framework, named MEEtBrain, to collect multimodal brain signals induced by AI-generated music for the analysis of human affective states, shown in Figure  1 . First, we construct an AI-generated music library as emotion stimuli, where music clips are automatically generated on a large scale by a generative AI model, addressing the copyright issues and overcoming the limitation of the music clips scale. For the automatic generation, we design prompt templates following the Russell's Valence-Arousal circumplex  [43]  and considering personalized interests in music. We build different prompt sentences as input to the AI generative model for each emotion state, and the generated music clips are labeled with the corresponding emotion state. We then evaluate the obtained music clips by recruiting volunteers to rate, ensuring the matching between each music clip and its labeled emotion state. Sequentially, we design a paradigm for multimodal brain signals collection, consisting of two sessions and four blocks in each session. Each subject listens to different music clips and EEG and fNIRS are collected at the same time. In particular, each recruited subject is required to rate the music clip at the end of the collection procedure, including the valence, arousal, and liking. Based on the collected data, we conduct comprehensive analysis of emotion states, and even recognize emotion states from the signals.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ai-Generated Music Library Construction",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "3.2.1",
      "text": "Valence-Arousal music library design. Inspired by affective neuroscience studies  [26, 43] , we construct a music library based on Russell's Valence-Arousal model, which represents emotions in a 2D space. The valence axis reflects emotional positivity, from unpleasant (e.g., sadness) to pleasant (e.g., joy), while the arousal axis indicates emotional intensity, from low (e.g., calm) to high (e.g., excitement). Each music clip is assumed to evoke a specific emotional state. Following a protocol similar to that of DEAP  [26] , we select clips representing four emotion types: HAHV, HALV, LAHV, and LALV.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Automatic Music Generation.",
      "text": "To evoke emotions using music clips, we first need to create some music clips.\n\nConsidering the copyright issues and limitation of music clip scale in existing studies, we try to automatically generate music clips taking advantage of the AIGC techniques, which can generate content based on the input keywords or requirements (prompt). In this way, the music clips can be automatically generated on a large scale. Here, we adopt MUSICGEN model  [10] ,\n\nwhich is an autoregressive, transformer-based model for controllable music generation. It generates music clips considering both textual descriptions and melodic features. In other words, one prompt sentence is input, and MUSICGEN generates the required music clip. Intuitively, the input prompt is quite important for the music clips generated by MUSICGEN. Therefore, we devise a prompt template, making the generated music clips effectively evoke emotions.\n\nFigure  2  shows our prompt template: a single sentence describing music from three aspects-specific emotions, instrument styles, and contextual scenarios. Placeholders are filled with descriptive elements to customize prompts. The valence adjective indicates emotional polarity (e.g., happy, sad), while the arousal adjective reflects intensity (e.g., energetic, calm). The instrumentation or style specifies instruments or genre (e.g., guitar, piano, drum beats), considering personal preference. The emotional tone conveys the overall mood (e.g., soothing, exciting), and the context or scenario suggests appropriate use cases (e.g., party, quiet evening). Prompts cover four emotional states: HAHV, HALV, LAHV, and LALV. We generate hundreds of 9DOHQFH $URXVDO\n\nFig.  3 . The rating score of each generated music clip given by the volunteers. Each point represents a music clip, with its coordinates corresponding to the scores assigned by the volunteers. The legend displays the labels initially assigned to the music clips, while the red dashed line delineates the final decision boundary for the music clips. music clips using iterative prompt variations via MUSICGEN, labeling each clip based on its prompt. In total, 236 clips are created, evenly distributed across the four emotion types.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Music Clips Screening.",
      "text": "In order to guarantee the matching between the music clips and the labeled emotion state, we perform a music clip screening process. We first exclude music clips with technical flaws (e.g., abrupt noise, extended silent intervals), and there are 157 ones retained. Then, we recruit volunteers to rate the generated music clips by scoring each clip on a scale from 1 to 9 along two dimensions: Valence and Arousal. A Valence score of 1 denotes \"painful\", whereas a score of 9 represents \"pleasant\". Similarly, an Arousal score of 1 indicates low arousal, while a score of 9 signifies high arousal.\n\nFigure  3  illustrates the rating outcomes from the 10 evaluators for the 157 music clips. As we expected, the majority of the music clips reliably evoked the target emotions, and the evaluators' scores closely align with the labels of the music clips.\n\nWe further select the music clips that significantly align with the intended emotional states according to the distribution shown in Figure  3  We first design a collection paradigm inspired by the collection of FACED dataset  [7] , shown in Figure  4 . There are two sessions involved, and four blocks in each session. Each block consists of five trials, and each trial lasts around 90 seconds. Prior to the start of each trial, there is a 5-second preparation period when participants are instructed to close their eyes and adjust their state of mind. Then, the music is played in 60 seconds, during which the participants are required to keep their eyes closed, remain still, and avoid head or eye movements to better focus on the music and engage with its emotional nuances. After the music concludes, the participants rate the Valence, Arousal, and Liking of the piece. The Liking should not be confused with Valence. It explores one's tastes or interests in the music they listened to rather than their emotional responses.\n\nFor example, some individuals may enjoy music that makes them feel sad  [26] . At the end of each complete trial, participants are allowed to rest for 15 seconds. To minimize the potential influence of emotional alternation, the same type of emotional music is used for all the five trials within a single block. Additionally, participants are required to solve a few simple arithmetic problems between blocks to maintain their focus. Under this paradigm, each participant completes two sessions in 80 minutes.  performance has been evaluated in  [32] .\n\nWe obtain high-quality data with its capability to capture event-related potentials and hemodynamic responses in the prefrontal area. Specifically, the EEG signals are collected at a rate of 250 HZ from Fp1 and Fp2 electrodes referenced to the left earlobe A1 according to the international 10-20 system. Simultaneously, fNIRS signals are recorded, capturing hemodynamics from the bilateral frontal cortex with 8 optodes. The fNIRS data is sampled at 25 Hz using two wavelengths: infrared (850 nm) and near-infrared (735 nm). Its modular design minimizes spatial interference and signal crosstalk, while an integrated EEG pre-amplifier enhances signal quality by reducing noise and distortion. Advanced filtering and motion correction techniques further refine the fNIRS signals, and a high source-switching frequency improves crosstalk suppression.  noting that, for each EEG and fNIRS recording, its emotion label is from the rating scores of valance and arousal given by the corresponding participant. Specifically, the arousal or valence label of the data is based on a 5-point threshold, where a score above 5 is high and a score below 5 is low. In particular, when the score is 5, the label of the data is marked as the arousal or valence of the music. For example, for one recording, if the subjective rating score of arousal and valance are 3 and 7, respectively, its emotion label is LAHV. We first performed preprocessing on EEG and fNIRS recordings, respectively. EEG was preprocessed through MNE-python toolbox  [17] . We manually excluded the segments affected by device-related artifacts. Then, a bandpass filter (0.1-40 Hz) was applied. To ensure the alignment between each EEG segments and its emotional label of each music clip, the recordings were epoched from the 25th to 55th seconds post-stimulus onset, that are assumed to capture the critical phase of the trial. Finally, baseline correction was performed on each epoched segment.\n\n4.2.2 fNIRS preprocessing. fNIRS preprocessing was conducted using Python 3.12 and SciPy 1.15, following standardized pipelines. First, we converted raw optical signals to optical density changes, and a 0.5-4 Hz band-pass filter was applied to obtain the photoplethysmogram (PPG) signal, which reflects heart rate. Subsequently, concentration changes in oxyhemoglobin (HbO), deoxyhemoglobin (HbR), and total hemoglobin (HbT) were computed using the Modified Beer-Lambert Law  [4, 20, 29] ,\n\na widely adopted method for converting light intensity measurements into hemodynamic metrics. Baseline correction was performed by subtracting the mean value of a 5-second pre-stimulus baseline from each channel's signal during the task, ensuring task-related activity was decoupled from resting-state variations. Finally, a 0.01-0.1 Hz band-pass filter was applied to remove low-frequency systemic artifacts, including cardiac pulsations (‚â§ 1 Hz), respiratory venous waves (‚â§ 0.2 Hz), and Mayer wave oscillations (‚â§ 0.1 Hz), which are known to confound fNIRS signals  [42] . +$+9 /$/9 +$/9 /$+9 6HOIUHSRUWUDWLQJV $URXVDO /LNLQJ 9DOHQFH Fig.  7 . Distribution of self-report rating scores across music groups during the collection experiment.\n\n5 Results and Analysis",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Validation Of The Ai-Generated Music Clips",
      "text": "Previous studies show that mode and rhythmic articulation significantly affect Valence, while tempo and rhythmic articulation influence Arousal; melodic direction shows the weakest emotional correlation  [16, 21] . To validate this on our generated music, we extracted the following structural features: 1) Tempo: which measures music speed in beats per minute (BPM) and ranges from slow to fast, estimated via beat-tracking  [15] ; 2) Rhythmic Articulation, describing how notes are performed-ranging from staccato (short, detached) to legato (smooth, connected)-is derived from the inverse of the mean zero-crossing rate (ZCR);\n\n3) Mode, which defines the tonal quality of a piece (e.g., minor for sadder tones and major for happier tones), is determined by comparing chroma features (pitch class distributions) to predefined major and minor templates using dot product similarity; 4) Pitch Range, referring to the span between the highest and lowest notes in a piece, is measured as the difference between the highest and lowest valid pitches; 5) Melodic Direction, indicating the overall pitch movement in a melody (ascending or descending), is analyzed by computing the ratio of ascending to descending intervals, with scores inversely mapped to emphasize descending trends. All the extracted metrics were further scaled to a range of  [1, 7]  for consistency and analysis.\n\nThe results are shown in the Figure  6 . To assess group differences, we conducted a one-way ANOVA test on the structural features across the four emotion categories (HAHV, HALV, LAHV, LALV ). As we can see, rhythmic articulation and pitch range exhibit a highly significant effect on the Arousal dimension (ùëù<0.00001). Mode (major/minor) and tempo showed statistically significant differences across the four groups (ùëù<0.05). Melodic direction displayed no significant variation (ùëù>0.5). These findings are consistent with the conclusions from prior work  [16, 21] , indicating the effectiveness of our generated music clips in evoking emotions.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Evaluation Of The Emotion-Inducing Effect",
      "text": "We further evaluated the emotion-inducing effect of our generated music, by comparing the rating scores given by the participants and the initial labels of the music clips. We conducted a statistical analysis of Valence/Arousal scores of the participants in each emotion groups, shown in Figure  7 . As we can see, most rating scores given by the participants are aligned with the emotion of each group. It further indicates that our generated music can effectively evoke the corresponding emotion states. When the valence score is relatively high, the liking score is also higher, indicating most participants prefer to the music that can evoke high valance (the valence score and liking score are strongly positively correlated, shown in Table  2 ). There are few outliers for each group, which may be due to the individual differences in music.\n\nWe examined correlation of Liking, Arousal, and Valence scores, by conducting a Pearson correlation analysis shown in Table  2 . As expected, we found a significant and strongly positive correlation between between Valence and Liking. Most individuals prefer to music that evokes positive emotions such as happiness and joy, although some may have a preference to sad music. Besides, Arousal and Valence are not independent, exhibiting a relatively low correlation. This finding is consistent with the results found in DEAP dataset  [26] .\n\nFor each recording, we also plotted a histogram, where the x-axis is the emotion groups, and the y-axis is the frequency of music clips with the corresponding emotion label, shown in Figure  8 . It can be seen, in each emotion group, each type of music successfully induces a high proportion of the targeted emotions. It further proves that our generated music can effectively evoke the corresponding emotion. Besides, some LAHV stimuli induced higher arousal than expected, shown in both Figure  3  and 8. This is consistent with the well-validated ratings for the International Affective Picture System (IAPS)  [31]  and the International Affective Digital Sounds system (IADS)  [5] , as well as in the study by DEAP Koelstra et al.  [26] . This suggests a general difficulty in eliciting emotions by high valence but low arousal.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Brain Signals And Emotions",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Eeg And Emotion.",
      "text": "To identify the neural correlation of emotional experience, we analyze the relationship between the relative EEG spectral powers and self-reported ratings. The relative spectral power refers to the ratio between the sum of the spectral powers in the frequency band of interest (delta, theta, alpha, beta, and gamma) and the sum of full-band spectral power  [7] . Here, we focused on the final 30 seconds of the 60-second music period, and divided it into multiple 3-second epochs.\n\nFor each epoch, we calculated the sum of spectral power within the five frequency bands, and calculated the total spectral power across all the frequencies, averaging these values across two EEG channels. Finally, we computed the mean relative spectral power for all the epochs within the 30-second window for each trial. To determine whether significant differences existed in mean relative spectral power among different emotion groups, we performed a one-way ANOVA test, shown in Table  4 . There are significant differences in the mean relative power of the beta frequency bands across the music groups (ùëù<0.001). Subsequently, we conducted post hoc tests using Tukey's Honestly Significant Difference (HSD) test for the beta frequency bands  [1] . There are significant differences in beta band power between the LAHV group and the other three groups.\n\n5.3.2 fNIRS and emotion. We also explored how fNIRS signals reflect emotion. To this end, for each 60-second epoch, we computed the mean and variance of HbO, HbR, and HbT concentrations using data from the last 30 seconds across all the eight channels. This process yielded a 48-dimensional feature set. Subsequently, we employed Pearson's correlation analysis to investigate the relationship between these features and the corresponding emotion labels. Some certain features exhibit significant correlations with the emotion labels, such as the variance of HbR concentration in channel 3 (ùëù<0.01). Notably, all the significant correlations are observed between the variance metrics and the emotion labels, suggesting that the dynamic changes in hemoglobin concentration may encode emotion-related information. These findings highlight the potential of fNIRS-based measures to capture neurophysiological signals related to emotional states.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Emotion Recognition",
      "text": "We tried to recognize emotion using the collected brain signals from only the first 20 participants. Here, the emotion recognition task refers to binary classification of valence and arousal, respectively. We explored the predictive ability of PPG-only, (3) HbO/HbR/HbT-only, (4) EEG+PPG, (5) EEG+HbO/HbR/HbT, (6) EEG+PPG+HbO/HbR/HbT. For classification, we implemented a novel deep neural network architecture, extending the Conformer model  [44]  with a dedicated fNIRS-specific branch for signal fusion. We tested the performance under both cross-subject and intra-subject paradigms. For cross-subject classification, a leave-one-subject-out strategy was adopted, and for intra-subject classification, we used the 10-fold cross validation strategry. Mean accuracy (ACC) and macro-averaged F1 score (MF1) were used for performance measurement.\n\nAs we can see from Table  3 , the combination of all the modalities performs the best across most of the tasks.\n\nIntegrating multiple modalities significantly improves classification performance for both Valence and Arousal, compared to using single modality. Notably, PPG is the most powerful for Valence recognition in ACC among the multiple modalities, and EEG performs the best in Arousal classification. Meanwhile, PPG performs better in classifying Arousal than Valence, around 3% higher in ACC. Individuals' heart rate (PPG) is likely to exhibit noticeable fluctuations when listening to fast-paced music that could affect arousal. Interestingly, Arousal classification performance is consistently better than that of Valence in ACC and MF1 across all the tasks. This suggests that Arousal is more reliably detectable using the available physiological signals.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In order to promote the accessibility of emotion regulation, we propose a portable and multimodal framework for analyzing emotions induced by AI-generated music and collecting EEG and fNIRS through a portable headband. We address three key limitations of existing studies: (1) restricted stimulus diversity, (2) single-modality of signals, and (3) lack of portability. AIGC techniques are introduced to automatically generate music stimuli based on our designed prompt templates. The music stimuli can be generated on a large scale, and independent of subjective selection. Then we use our portable headband to collect EEG and fNIRS signals when participants are listening to the music stimuli. We collected a real-world dataset of EEG and fNIRS from 20 individuals in the first recruitment, with a total duration of about 14 hours. Based on the dataset, the generated music clips were verified and the emotion-inducing effects were evaluated, indicating the generated music effectively evoke the intended emotions. We also successfully recognize emotions combining EEG and fNIRS. We are actively expanding our multimodal dataset (44 participants in the latest dataset) and make it publicly available to promote further research and practical applications. The dataset is available at https://zju-bmi-lab.github.io/ZBra.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of MEEtBrain framework.",
      "page": 3
    },
    {
      "caption": "Figure 2: Explanation of Emotion Categories and Template Design. In figure (a), we provide a straightforward explanation of the four categories",
      "page": 4
    },
    {
      "caption": "Figure 1: First, we construct an AI-generated music library as emotion stimuli,",
      "page": 4
    },
    {
      "caption": "Figure 2: shows our prompt template: a single sentence describing music from three aspects‚Äîspecific emotions, instrument",
      "page": 4
    },
    {
      "caption": "Figure 3: The rating score of each generated music clip given by the volunteers. Each point represents a music clip, with its coordinates",
      "page": 5
    },
    {
      "caption": "Figure 3: illustrates the rating outcomes from the 10 evaluators for the 157 music clips. As we expected, the majority of the",
      "page": 5
    },
    {
      "caption": "Figure 3: As shown, different emotion states are concentrated in different regions, and so we adopt different",
      "page": 5
    },
    {
      "caption": "Figure 4: There are two sessions",
      "page": 5
    },
    {
      "caption": "Figure 4: Multimodal brain signals collection paradigm.",
      "page": 6
    },
    {
      "caption": "Figure 5: Headband device for collecting EEG and fNIRS",
      "page": 6
    },
    {
      "caption": "Figure 5: It integrates",
      "page": 6
    },
    {
      "caption": "Figure 6: Structural features of music (linearly scaled to the range [1,7]) across four music groups.",
      "page": 8
    },
    {
      "caption": "Figure 7: Distribution of self-report rating scores across music groups during the collection experiment.",
      "page": 8
    },
    {
      "caption": "Figure 6: To assess group differences, we conducted a one-way ANOVA test on the structural",
      "page": 8
    },
    {
      "caption": "Figure 7: As we can see, most rating scores given by the participants",
      "page": 8
    },
    {
      "caption": "Figure 8: Histogram of participants with rating scores for each music category.",
      "page": 9
    },
    {
      "caption": "Figure 8: It can be seen, in each emotion group, each type of",
      "page": 9
    },
    {
      "caption": "Figure 3: and 8. This is consistent with the well-validated ratings for the International Affective Picture System (IAPS)",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "\u0000+\u0000$\u0000/\u00009": "\u0000/\u0000$\u0000/\u00009",
          "\u0000+\u0000$\u0000+\u00009": "\u0000+\u0000$\u0000+\u00009\n\u0000+\u0000$\u0000/\u00009\n\u0000/\u0000$\u0000+\u00009\n\u0000/\u0000$\u0000+\u00009\n\u0000/\u0000$\u0000/\u00009"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Tukey's honestly significant difference (HSD) test",
      "authors": [
        "Herv√© Abdi",
        "Lynne Williams"
      ],
      "year": "2010",
      "venue": "Encyclopedia of research design"
    },
    {
      "citation_id": "2",
      "title": "Improving BCI-based emotion recognition by combining EEG feature selection and kernel classifiers",
      "authors": [
        "John Atkinson",
        "Daniel Campos"
      ],
      "year": "2016",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "3",
      "title": "Calibration free meta learning based approach for subject independent EEG emotion recognition",
      "authors": [
        "Swapnil Bhosale",
        "Rupayan Chakraborty",
        "Sunil Kumar"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "4",
      "title": "Essai d'Optique, sur la gradation de la lumiere",
      "authors": [
        "Pierre Bouguer"
      ],
      "year": "1729",
      "venue": "Essai d'Optique, sur la gradation de la lumiere"
    },
    {
      "citation_id": "5",
      "title": "The International affective digitized sounds (IADS): stimuli, instruction manual and affective ratings",
      "authors": [
        "Margaret Bradley",
        "Peter Lang"
      ],
      "year": "1999",
      "venue": "The International affective digitized sounds (IADS): stimuli, instruction manual and affective ratings"
    },
    {
      "citation_id": "6",
      "title": "Eeg-music emotion recognition: Challenge overview",
      "authors": [
        "Salvatore Calcagno",
        "Simone Carnemolla",
        "Isaak Kavasidis",
        "Simone Palazzo",
        "Daniela Giordano",
        "Concetto Spampinato"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "A large finer-grained affective computing EEG dataset",
      "authors": [
        "Jingjing Chen",
        "Xiaobin Wang",
        "Chen Huang",
        "Xin Hu",
        "Xinke Shen",
        "Dan Zhang"
      ],
      "year": "2023",
      "venue": "A large finer-grained affective computing EEG dataset"
    },
    {
      "citation_id": "8",
      "title": "Conditional probabilistic-based domain adaptation for cross-subject EEG-based emotion recognition",
      "authors": [
        "Shichao Cheng",
        "Yifan Wang",
        "Jiawei Mei",
        "Guang Lin",
        "Jianhai Zhang",
        "Wanzeng Kong"
      ],
      "year": "2025",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "9",
      "title": "From affect programs to dynamical discrete emotions",
      "authors": [
        "Giovanna Colombetti"
      ],
      "year": "2009",
      "venue": "Philosophical Psychology"
    },
    {
      "citation_id": "10",
      "title": "Simple and controllable music generation",
      "authors": [
        "Jade Copet",
        "Felix Kreuk",
        "Itai Gat",
        "Tal Remez",
        "David Kant",
        "Gabriel Synnaeve",
        "Yossi Adi",
        "Alexandre D√©fossez"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Affective science and health: the importance of emotion and emotion regulation",
      "authors": [
        "David Desteno",
        "James Gross",
        "Laura Kubzansky"
      ],
      "year": "2013",
      "venue": "Health psychology"
    },
    {
      "citation_id": "12",
      "title": "Understanding the effect of listening to music, playing music, and singing on brain function: A scoping review of fNIRS studies",
      "authors": [
        "Keya Ding",
        "Jingwen Li",
        "Xuemei Li",
        "Hui Li"
      ],
      "year": "2024",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "13",
      "title": "A review of music and emotion studies: Approaches, emotion models, and stimuli. Music Perception",
      "authors": [
        "Tuomas Eerola",
        "Jonna Vuoskoski"
      ],
      "year": "2012",
      "venue": "An Interdisciplinary Journal"
    },
    {
      "citation_id": "14",
      "title": "A closed-loop, music-based brain-computer interface for emotion mediation",
      "authors": [
        "Stefan Ehrlich",
        "Kat Agres",
        "Cuntai Guan",
        "Gordon Cheng"
      ],
      "year": "2019",
      "venue": "PloS one"
    },
    {
      "citation_id": "15",
      "title": "Beat tracking by dynamic programming",
      "authors": [
        "Ellis Daniel"
      ],
      "year": "2007",
      "venue": "Journal of New Music Research"
    },
    {
      "citation_id": "16",
      "title": "Relationships between musical structure and psychophysiological measures of emotion",
      "authors": [
        "Patrick Gomez",
        "Brigitta Danuser"
      ],
      "year": "2007",
      "venue": "Emotion"
    },
    {
      "citation_id": "17",
      "title": "MEG and EEG data analysis with MNE-Python",
      "authors": [
        "Alexandre Gramfort",
        "Martin Luessi",
        "Eric Larson",
        "Denis Engemann",
        "Daniel Strohmeier",
        "Christian Brodbeck",
        "Roman Goj",
        "Mainak Jas",
        "Teon Brooks",
        "Lauri Parkkonen"
      ],
      "year": "2013",
      "venue": "Frontiers in Neuroinformatics"
    },
    {
      "citation_id": "18",
      "title": "Automatic ECG-based emotion recognition in music listening",
      "authors": [
        "Yu-Liang Hsu",
        "Jeen-Shing Wang",
        "Wei-Chun Chiang",
        "Chien-Han Hung"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "On the Relationships between Music-induced Emotion and Physiological Signals",
      "authors": [
        "Xiao Hu",
        "Fanjie Li",
        "Tzi-Dong Jeremy Ng"
      ],
      "year": "2018",
      "venue": "On the Relationships between Music-induced Emotion and Physiological Signals"
    },
    {
      "citation_id": "20",
      "title": "Effects of processing methods on fNIRS signals assessed during active walking tasks in older adults",
      "authors": [
        "Meltem Izzetoglu",
        "Roee Holtzer"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "21",
      "title": "Perceived emotional expression in synthesized performances of a short melody: Capturing the listener's judgment policy",
      "authors": [
        "Patrik Juslin"
      ],
      "year": "1997",
      "venue": "Musicae scientiae"
    },
    {
      "citation_id": "22",
      "title": "Naturalistic music EEG dataset-hindi (nmed-h)",
      "authors": [
        "Blair Kaneshiro",
        "T Duc",
        "Jacek Nguyen",
        "Anthony Dmochowski",
        "Jonathan Norcia",
        "Berger"
      ],
      "year": "2016",
      "venue": "Naturalistic music EEG dataset-hindi (nmed-h)"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition based on physiological changes in music listening",
      "authors": [
        "Jonghwa Kim",
        "Elisabeth Andr√©"
      ],
      "year": "2008",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "24",
      "title": "Towards a neural basis of music-evoked emotions",
      "authors": [
        "Stefan Koelsch"
      ],
      "year": "2010",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "25",
      "title": "Brain correlates of music-evoked emotions",
      "authors": [
        "Stefan Koelsch"
      ],
      "year": "2014",
      "venue": "Nature reviews neuroscience"
    },
    {
      "citation_id": "26",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "27",
      "title": "Modeling emotional content of music using system identification",
      "authors": [
        "David Mark D Korhonen",
        "M Ed Clausi",
        "Jernigan"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "28",
      "title": "Going to the heart of the matter: do negative emotions cause coronary heart disease",
      "authors": [
        "D Laura",
        "Ichiro Kubzansky",
        "Kawachi"
      ],
      "year": "2000",
      "venue": "Journal of psychosomatic research"
    },
    {
      "citation_id": "29",
      "title": "Photometria sive de mensura et gradibus luminis, colorum et umbrae",
      "authors": [
        "Johann Heinrich"
      ],
      "year": "1760",
      "venue": "Photometria sive de mensura et gradibus luminis, colorum et umbrae"
    },
    {
      "citation_id": "30",
      "title": "Real-time EEG-based emotion monitoring using stable features",
      "authors": [
        "Zirui Lan",
        "Olga Sourina",
        "Lipo Wang",
        "Yisi Liu"
      ],
      "year": "2016",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "31",
      "title": "International affective picture system (IAPS): Technical manual and affective ratings",
      "authors": [
        "Margaret Peter J Lang",
        "Bruce Bradley",
        "Cuthbert"
      ],
      "year": "1997",
      "venue": "International affective picture system (IAPS): Technical manual and affective ratings"
    },
    {
      "citation_id": "32",
      "title": "Hybrid Integrated Wearable Patch for Brain EEG-fNIRS Monitoring",
      "authors": [
        "Boyu Li",
        "Mingjie Li",
        "Jie Xia",
        "Jin Hao",
        "Shurong Dong",
        "Jikui Luo"
      ],
      "year": "2024",
      "venue": "Sensors"
    },
    {
      "citation_id": "33",
      "title": "EEG-fNIRS-Based Music Emotion Decoding and Individualized Music Generation",
      "authors": [
        "Haolun Li",
        "Yixuan Zeng",
        "Ziqian Bai",
        "Wenhao Li",
        "Kai Wu",
        "Jing Zhou"
      ],
      "year": "2024",
      "venue": "2024 5th International Conference on Intelligent Computing and Human-Computer Interaction (ICHCI)"
    },
    {
      "citation_id": "34",
      "title": "Hierarchical convolutional neural networks for EEG-based emotion recognition",
      "authors": [
        "Jinpeng Li",
        "Zhaoxiang Zhang",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "35",
      "title": "EEG-based emotion recognition in music listening",
      "authors": [
        "Yuan-Pin Lin",
        "Chi-Hong Wang",
        "Tzyy-Ping Jung",
        "Tien-Lin Wu",
        "Shyh-Kang Jeng",
        "Jyh-Horng Jeng-Ren Duann",
        "Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "36",
      "title": "Real-time EEG-based human emotion recognition and visualization",
      "authors": [
        "Yisi Liu",
        "Olga Sourina",
        "Minh Nguyen"
      ],
      "year": "2010",
      "venue": "2010 international conference on cyberworlds"
    },
    {
      "citation_id": "37",
      "title": "Emotion analysis of EEG signals using proximity-conserving auto-encoder (PCAE) and ensemble techniques",
      "authors": [
        "R Mathumitha",
        "Maryposonia"
      ],
      "year": "2025",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "38",
      "title": "Wavelet-based emotion recognition system using EEG signal",
      "authors": [
        "Zeynab Mohammadi",
        "Javad Frounchi",
        "Mahmood Amiri"
      ],
      "year": "2017",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "39",
      "title": "A systematic review on the neural effects of music on emotion regulation: Implications for music therapy practice",
      "authors": [
        "Kimberly Sena"
      ],
      "year": "2013",
      "venue": "Journal of music therapy"
    },
    {
      "citation_id": "40",
      "title": "The social and applied psychology of music",
      "authors": [
        "Adrian North",
        "David Hargreaves"
      ],
      "year": "2008",
      "venue": "The social and applied psychology of music"
    },
    {
      "citation_id": "41",
      "title": "Uses of music in everyday life",
      "authors": [
        "Adrian North",
        "David Hargreaves",
        "Jon Hargreaves"
      ],
      "year": "2004",
      "venue": "Music perception"
    },
    {
      "citation_id": "42",
      "title": "Multi-modal integration of EEG-fNIRS for characterization of brain activity evoked by preferred music",
      "authors": [
        "Lina Qiu",
        "Yongshi Zhong",
        "Qiuyou Xie",
        "Zhipeng He",
        "Xiaoyun Wang",
        "Yingyue Chen",
        "Jiahui Chang'an A Zhan",
        "Pan"
      ],
      "year": "2022",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "43",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "44",
      "title": "EEG conformer: Convolutional transformer for EEG decoding and visualization",
      "authors": [
        "Yonghao Song",
        "Qingqing Zheng",
        "Bingchuan Liu",
        "Xiaorong Gao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "45",
      "title": "Application of deep belief networks in eeg-based dynamic music-emotion recognition",
      "authors": [
        "Nattapong Thammasan",
        "Ken-Ichi Fukui",
        "Masayuki Numao"
      ],
      "year": "2016",
      "venue": "2016 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "46",
      "title": "Regulation of emotions by listening to music in emotional situations",
      "authors": [
        "Mirjam Thoma",
        "Stefan Ryf",
        "Ulrike Ehlert",
        "M Urs",
        "M Nater",
        "Baroni",
        "R Addessi",
        "M Caterina",
        "Costa"
      ],
      "year": "2006",
      "venue": "Regulation of emotions by listening to music in emotional situations"
    },
    {
      "citation_id": "47",
      "title": "Emotion recognition with pre-trained transformers using multimodal signals",
      "authors": [
        "Juan Vazquez-Rodriguez",
        "Gr√©goire Lefebvre",
        "Julien Cumin",
        "James Crowley"
      ],
      "year": "2022",
      "venue": "2022 10th international conference on affective computing and intelligent interaction (ACII)"
    },
    {
      "citation_id": "48",
      "title": "Global Context MambaVision for EEG-based Emotion Recognition",
      "authors": [
        "Hao Wang",
        "Li Xu",
        "Yuntao Yu",
        "Weiyue Ding",
        "Yiming Xu"
      ],
      "year": "2025",
      "venue": "ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Cbramod: A criss-cross brain foundation model for eeg decoding",
      "authors": [
        "Jiquan Wang",
        "Sha Zhao",
        "Zhiling Luo",
        "Yangxuan Zhou",
        "Haiteng Jiang",
        "Shijian Li",
        "Tao Li",
        "Gang Pan"
      ],
      "year": "2024",
      "venue": "Cbramod: A criss-cross brain foundation model for eeg decoding",
      "arxiv": "arXiv:2412.07236"
    },
    {
      "citation_id": "50",
      "title": "Emotional state classification from EEG data using machine learning approach",
      "authors": [
        "Xiao-Wei Wang",
        "Dan Nie",
        "Bao-Liang Lu"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "51",
      "title": "M-MDD: A multi-task deep learning framework for major depressive disorder diagnosis using EEG",
      "authors": [
        "Yilin Wang",
        "Sha Zhao",
        "Haiteng Jiang",
        "Shijian Li",
        "Tao Li",
        "Gang Pan"
      ],
      "year": "2025",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "52",
      "title": "Diffmdd: A diffusion-based deep learning framework for mdd diagnosis using eeg",
      "authors": [
        "Yilin Wang",
        "Sha Zhao",
        "Haiteng Jiang",
        "Shijian Li",
        "Benyan Luo",
        "Tao Li",
        "Gang Pan"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "53",
      "title": "TC-Net: A Transformer Capsule Network for EEG-based emotion recognition",
      "authors": [
        "Yi Wei",
        "Yu Liu",
        "Chang Li",
        "Juan Cheng",
        "Rencheng Song",
        "Xun Chen"
      ],
      "year": "2023",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "54",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "55",
      "title": "BrainUICL: An unsupervised individual continual learning framework for EEG applications",
      "authors": [
        "Yangxuan Zhou",
        "Sha Zhao",
        "Jiquan Wang",
        "Haiteng Jiang",
        "Shijian Li",
        "Tao Li",
        "Gang Pan"
      ],
      "venue": "The Thirteenth International Conference on Learning Representations"
    }
  ]
}