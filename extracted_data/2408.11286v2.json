{
  "paper_id": "2408.11286v2",
  "title": "Video Emotion Open-Vocabulary Recognition Based On Multimodal Large Language Model",
  "published": "2024-08-21T02:17:18Z",
  "authors": [
    "Mengying Ge",
    "Dongkai Tang",
    "Mingyang Li"
  ],
  "keywords": [
    "open-vocabulary",
    "data generation",
    "multi-model co-judgment"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition is a task of great concern. However, traditional data sets are based on fixed labels, resulting in models that often focus on main emotions and ignore detailed emotional changes in complex scenes. This report introduces the solution of using MLLMs technology to generate open-vocabulary emotion labels from a video. The solution includes the use of framework, data generation and processing, training methods, results generation and multi-model co-judgment. In the MER-OV (Open-Word Emotion Recognition) of the MER2024 challenge, our method achieved significant advantages, leading to its superior capabilities in complex emotion computation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition technology occupies a pivotal position in the field of human-computer interaction. This technology is committed to accurately capturing and identifying people's complex emotional states by integrating multiple modal information such as visual, auditory and textual semantics  [1] .Currently, most research in this field focuses on model construction based on fixed label data sets. Although the objectivity of labels is strived for through a multi-person voting mechanism, the subjectivity and diversity of emotions themselves make it difficult for a single vocabulary to fully depict the full picture of individual emotions.\n\nWith the rapid advancement of large language models (LLMs) technology, many open problems have been solved unprecedentedly. However, in the field of multimodal emotion recognition, the application research of such technologies is still insufficient. Cutting-edge research such as MER2024  [2]  has evaluated the sentiment analysis capabilities of MLLMs such as Video-LLaMA  [3] , SALMONN  [4] , mPLUG-Owl  [5] , Qwen-Audio  [6] , and GPT-4V  [7] . These models have shown performance that exceeds heuristic baselines, demonstrating their potential in emotion understanding. In particular, the introduction of AffectGPT  [8]  is not only committed to the accurate prediction of emotions, but also provides reasonable explanations behind the predictions, promoting the development of interpretable multimodal emotion reasoning.\n\nThis report explores in depth some research on multimodal emotion recognition using MLLMs technology, covering the optimization of infrastructure selection, refined data generation and processing processes, efficient training methods, and innovative solutions for result generation. The core research highlights are summarized as follows:\n\n• Emotion recognition training based on InternVL framework. In view of the excellent performance of InternVL-Chat-V1.5 (hereinafter referred to as InternVL)  [9]  in cross-domain tasks, we conducted in-depth finetuning on this basis. By using the generated character emotion description data and performing lora fine-tuning, InternVL's ability to parse character expressions was significantly enhanced. Experimental results show that this refined training significantly improves the performance of the model in emotion recognition tasks.\n\n• Research on trimodal open vocabulary sentiment recognition. The AffectGPT framework innovatively proposed a sentiment clue analysis framework that aligns the three modalities of image, speech, and text. We verified its superiority in open vocabulary sentiment recognition through SFT in downstream tasks, demonstrating the unique advantages of trimodal fusion in complex sentiment analysis.\n\n• Synergy between MLLMs and traditional discriminative models. Although traditional discriminative multimodal emotion recognition models can accurately capture the main emotion labels, they are slightly insufficient in capturing subtle emotions. MLLMs are good at capturing these subtle changes. Therefore, we explored a synergistic strategy that combines the advantages of both, aiming to achieve a more comprehensive and accurate judgment of character emotions through complementary effects.\n\n2 Proposed Method",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Internvl Finetuning",
      "text": "InternVL, an open-source multimodal large language model released by Shanghai Artificial Intelligence Laboratory, has shown excellent performance (SOTA) on the public test set, so we chose it as a benchmark and deeply explored its capabilities in both zero-shot and fine-tune modes.\n\nIn the zero-shot scenario, we directly use the open-source InternVL model without any task-specific training. Given that InternVL supports multi-image input, we frame the video and divide it into six parts, randomly selecting one frame from each part, and finally using these six frames as input. During zero-shot reasoning, we designed a specific prompt format: \"These pictures are different frames of the same video. The words spoken by the characters in the picture are {text}. Assuming that you are an expert in the field of emotion, please describe the expression of the character in the picture in detail, and based on the above description, use a few words to summarize his expression in the format of [\"**]\". {text} is the text content obtained by speech recognition .\n\nIn the fine-tune stage, we fine-tuned InternVL for downstream tasks based on the Swift framework  [10] . Given that the verification results on the MER2024-SEMI track show that a large amount of human-centric data has a significant improvement on the emotion recognition task, we also adopted a similar strategy on the MER2024-OV track, that is, fine-tuning on a human-centric dataset. However, due to the scarcity of training data, we cleverly used the generation capabilities of Qwen-VL  [11]  and CogVLM  [12]  to create more captions data, and then used InternVL itself to screen these generated results to ensure the quality and relevance of the training data. This innovative data augmentation method effectively alleviates the problem of insufficient training data and improves the generalization ability of the model.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments Based On Affectgpt",
      "text": "In the field of multimodal emotion recognition, the accuracy of emotion prediction can be significantly improved by integrating information from multiple modalities such as video, audio, and text for comprehensive judgment. The study of MERBench  [13]  profoundly reveals the key role of the audio branch in discriminative emotion recognition methods.\n\nGiven that most MLLMs mainly focus on understanding images or videos and text, AffectGPT innovatively introduces an audio branch based on the VideoLLava framework, aiming to comprehensively summarize the emotional state of the characters in the video by deeply analyzing the clues of these three modalities.\n\nWe designed the experiment based on the AffectGPT framework, using 332 description data provided by the official During the testing phase, we applied our trained model to perform predictive inference on 66 test set samples.\n\nThe model analyzed the emotional states of individuals in videos from three independent perspectives: video, audio, and text. Finally, to obtain an open vocabulary list for emotion descriptions, we utilized the InternVL large language model to refine and summarize the analysis results at the lexical level, ultimately generating model outputs with rich emotional vocabularies that provide comprehensive and precise analyses for emotion recognition.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Multi-Model Co-Judgment",
      "text": "When the accuracy of a single model reaches a bottleneck and is difficult to further improve, a multi-model integration strategy becomes a natural choice. In the context of the MER2024-OV competition, we also followed this idea. As representatives of large visual multi-modal models, AffectGPT and InternVL have the advantage of overall understanding and reasoning capabilities of video content, and can generally capture and summarize the expression information appearing in the video. However, in comparison, traditional multi-modal algorithms show higher accuracy on specific prediction tasks. In view of this, we cleverly combined the strengths of the two and spliced and fused the prediction results of the multi-modal model in the MER2024-SEMI track, the output of AffectGPT, and the predictions of InternVL, aiming to improve the performance through the complementarity between them. Overall precision and recall. Experimental results show that this strategy significantly improves the overall evaluation index and verifies the effectiveness of multi-model integration in improving model performance.\n\n3 Experiments and Analysis to compare text similarity. Specifically, for each pair of generated captions, we input them into InternVL and set the prompt to: \"Please judge whether the emotions described in these two sentences are similar and give a score between 0 and 1. \" Through this step, we can effectively eliminate low-quality captions with a similarity lower than 0.9, and for caption pairs with a similarity higher than 0.9, we randomly select one of them to join the training set.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dataset",
      "text": "Finally, we successfully generated 26,000 pairs of high-quality captions based on the open source dataset CH-SIMS-v2  [15] . These captions will be used as the final training set to improve the performance of our model in facial emotion analysis. we also set the maximum input length to 8192 to support the processing of more complex image and text information.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Analysis.",
      "text": "In the experiment on the InternVL model, we conducted a comparative evaluation between the zeroshot test and lora fine-tuning based on the 322 officially provided data sets. The core measurement standard of the experiment is Avg score. The specific definition of this indicator can refer to the MER2024 specification. From the experimental results Table  1 , we observed that after lora fine-tuning, the Avg score significantly increased by 3%, which strongly supports our hypothesis: fine-tuning on a human-centered data set can effectively promote the model's performance in downstream tasks.\n\nFurthermore, when delving deeper into the inference process, we noticed the impact of face region preprocessing on the final performance. Through comparative experiments, we found that the Avg score is 11% higher than using the entire image as input for prediction compared to the strategy of first cutting out the face and performing alignment processing. This finding may be attributed to the fact that after the face is cut out, some detailed information is lost due to the reduction in resolution, thus affecting the prediction accuracy of the model. In order to further improve model performance, we also tried a multi-model integration strategy. Specifically, we integrated the zero-shot results of InternVL, the results after lora fine-tuning, and the output of the discriminative model based on this. As can be seen from the data in Table  1 , although the integrated model has a slight decrease in accuracy, it significantly improves the Recall value, which in turn promotes the increase in the overall Avg score.\n\nThis shows that through a reasonable model integration strategy, we can effectively improve the recall rate while maintaining a certain accuracy, thereby optimizing the overall performance.\n\nTable  3  shows the performance of AffectGPT on a 66-word open vocabulary test set after fine-tuning. The results prove that the framework can effectively acquire the ability to understand complex emotions with only a small amount of SFT training data, and shows excellent generalization performance on the test set. This report primarily describes the technical approach we used in the MER2024-OV track. We mainly fine-tuned In-ternVL on human-related data, which helps the model better understand and extract facial expressions from details such as facial expressions, body movements, and surrounding environments. Additionally, we leveraged the powerful multimodal capabilities of AffectGPT, which integrates speech, vision, and text, and performed fine-tuning on small batches of data, which also had a positive impact on the entire task. Finally, we integrated multiple models to improve recall, and the final results indicate that our methods provided a significant advantage.\n\nHowever, this approach also has limitations. Model integration, while improving recall, reduced precision. The reason is that large models, while capable of understanding primary expressions, can also introduce many irrelevant or secondary expressions, leading to a decrease in accuracy. For future work, we can consider how to reduce the output of invalid expressions from large models, retaining only a few critical expressions.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "Language Model"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "MENGYING GE, BOSS ZhiPin, China"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "DONGKAI TANG, BOSS ZhiPin, China"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "MINGYANG LI, BOSS ZhiPin, China"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "Multimodal emotion recognition is a task of great concern. However, traditional data sets are based on ﬁxed labels, resulting in models"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "that often focus on main emotions and ignore detailed emotional changes in complex scenes. This report introduces the solution of"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "using MLLMs technology to generate open-vocabulary emotion labels from a video. The solution includes the use of framework, data"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "generation and processing, training methods, results generation and multi-model co-judgment. In the MER-OV (Open-Word Emotion"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "Recognition) of the MER2024 challenge, our method achieved signiﬁcant advantages,\nleading to its superior capabilities in complex"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "emotion computation."
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "Additional Key Words and Phrases: open-vocabulary, data generation, multi-model co-judgment"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "ACM Reference Format:"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "Mengying Ge, Dongkai Tang, and Mingyang Li. 2024. Video Emotion Open-vocabulary Recognition Based on Multimodal Large"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "Language Model.\n1, 1 (August 2024), 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "1\nIntroduction"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "Multimodal emotion recognition technology occupies a pivotal position in the ﬁeld of human-computer interaction."
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "This technology is committed to accurately capturing and identifying people’s complex emotional states by integrating"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "multiple modal\ninformation such as visual, auditory and textual semantics[1].Currently, most research in this ﬁeld"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "focuses on model construction based on ﬁxed label data sets. Although the objectivity of labels is strived for through"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "a multi-person voting mechanism, the subjectivity and diversity of emotions themselves make it diﬃcult for a single"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "vocabulary to fully depict the full picture of individual emotions."
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "With the rapid advancement of large language models (LLMs) technology, many open problems have been solved"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "unprecedentedly. However, in the ﬁeld of multimodal emotion recognition, the application research of such technolo-"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "gies is still\ninsuﬃcient. Cutting-edge research such as MER2024[2] has evaluated the sentiment analysis capabilities"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "of MLLMs such as Video-LLaMA[3], SALMONN[4], mPLUG-Owl[5], Qwen-Audio[6], and GPT-4V[7]. These models"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "have shown performance that exceeds heuristic baselines, demonstrating their potential in emotion understanding. In"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "particular, the introduction of AﬀectGPT[8] is not only committed to the accurate prediction of emotions, but also pro-"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "vides reasonable explanations behind the predictions, promoting the development of interpretable multimodal emotion"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "reasoning."
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "This report explores in depth some research on multimodal emotion recognition using MLLMs technology, cover-"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "ing the optimization of\ninfrastructure selection, reﬁned data generation and processing processes, eﬃcient\ntraining"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "methods, and innovative solutions for result generation. The core research highlights are summarized as follows:"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large": "• Emotion recognition training based on InternVL framework.\nIn view of\nthe excellent performance of"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Mengying Ge et al.": "InternVL’s ability to parse character expressions was signiﬁcantly enhanced. Experimental results show that this"
        },
        {
          "Mengying Ge et al.": "reﬁned training signiﬁcantly improves the performance of the model in emotion recognition tasks."
        },
        {
          "Mengying Ge et al.": "• Research on trimodal open vocabulary sentiment recognition. The AﬀectGPT framework innovatively"
        },
        {
          "Mengying Ge et al.": "proposed a sentiment clue analysis framework that aligns the three modalities of image, speech, and text. We ver-"
        },
        {
          "Mengying Ge et al.": "iﬁed its superiority in open vocabulary sentiment recognition through SFT in downstream tasks, demonstrating"
        },
        {
          "Mengying Ge et al.": "the unique advantages of trimodal fusion in complex sentiment analysis."
        },
        {
          "Mengying Ge et al.": "• Synergy between MLLMs and traditional discriminative models. Although traditional discriminative mul-"
        },
        {
          "Mengying Ge et al.": "timodal emotion recognition models can accurately capture the main emotion labels, they are slightly insuﬃ-"
        },
        {
          "Mengying Ge et al.": "cient in capturing subtle emotions. MLLMs are good at capturing these subtle changes. Therefore, we explored a"
        },
        {
          "Mengying Ge et al.": "synergistic strategy that combines the advantages of both, aiming to achieve a more comprehensive and accurate"
        },
        {
          "Mengying Ge et al.": "judgment of character emotions through complementary eﬀects."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "judgment of character emotions through complementary eﬀects.": "2\nProposed Method"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "2.1\nInternVL Finetuning"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "InternVL, an open-source multimodal"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "has shown excellent performance (SOTA) on the public test set, so we chose it as a benchmark and deeply explored its"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "capabilities in both zero-shot and ﬁne-tune modes."
        },
        {
          "judgment of character emotions through complementary eﬀects.": ""
        },
        {
          "judgment of character emotions through complementary eﬀects.": "that InternVL supports multi-image input, we frame the video and divide it into six parts, randomly selecting one frame"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "from each part, and ﬁnally using these six frames as input. During zero-shot reasoning, we designed a speciﬁc prompt"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "format: \"These pictures are diﬀerent frames of the same video. The words spoken by the characters in the picture are"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "{text}. Assuming that you are an expert in the ﬁeld of emotion, please describe the expression of the character in the"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "picture in detail, and based on the above description, use a few words to summarize his expression in the format of"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "[„**]\". {text} is the text content obtained by speech recognition ."
        },
        {
          "judgment of character emotions through complementary eﬀects.": ""
        },
        {
          "judgment of character emotions through complementary eﬀects.": "the veriﬁcation results on the MER2024-SEMI track show that a large amount of human-centric data has a signiﬁcant"
        },
        {
          "judgment of character emotions through complementary eﬀects.": ""
        },
        {
          "judgment of character emotions through complementary eﬀects.": "ﬁne-tuning on a human-centric dataset. However, due to the scarcity of training data, we cleverly used the generation"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "capabilities of Qwen-VL[11] and CogVLM[12] to create more captions data, and then used InternVL itself to screen"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "these generated results to ensure the quality and relevance of the training data. This innovative data augmentation"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "method eﬀectively alleviates the problem of insuﬃcient"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "model."
        },
        {
          "judgment of character emotions through complementary eﬀects.": "2.2\nExperiments Based on AﬀectGPT"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "In the ﬁeld of multimodal emotion recognition, the accuracy of emotion prediction can be signiﬁcantly improved by"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "integrating information from multiple modalities such as video, audio, and text for comprehensive judgment. The study"
        },
        {
          "judgment of character emotions through complementary eﬀects.": ""
        },
        {
          "judgment of character emotions through complementary eﬀects.": "Given that most MLLMs mainly focus on understanding images or videos and text, AﬀectGPT innovatively introduces"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "an audio branch based on the VideoLLava framework, aiming to comprehensively summarize the emotional state of"
        },
        {
          "judgment of character emotions through complementary eﬀects.": "the characters in the video by deeply analyzing the clues of these three modalities."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "We designed the experiment based on the AﬀectGPT framework, using 332 description data provided by the oﬃcial"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "MER2024 competition, and scientiﬁcally divided them into a training set (accounting for 3/4, i.e., 266 samples) and a test"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "set (accounting for 1/4, i.e., 66 samples). In the training stage, we used these training data to perform sft (soft ﬁne-tuning)"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "optimization training on the model, and guided the model through the following carefully designed prompts: \"###Hu-"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "man: Close your eyes, open your ears and you imagine only based on the sound that <Audio><AudioHere></Audio>."
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "\"Close your ears, open your eyes and you see that <Video><ImageHere></Video>. The subtitle content of this video"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "is <Subtitle>subtitle</Subtitle>. Now as an expert in the ﬁeld of emotions, please focus on the facial expressions, body"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "movements, environment, acoustic information, subtitle content, etc., in the video to discern clues related to the emo-"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "tions of the individual. Please provide a detailed description and ultimately predict the emotional state of the individual"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "in the video. ###Assistant:\""
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "During the testing phase, we applied our trained model\nto perform predictive inference on 66 test set samples."
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "The model analyzed the emotional states of individuals in videos from three independent perspectives: video, audio,"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "and text. Finally, to obtain an open vocabulary list for emotion descriptions, we utilized the InternVL large language"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "model to reﬁne and summarize the analysis results at the lexical\nlevel, ultimately generating model outputs with rich"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "emotional vocabularies that provide comprehensive and precise analyses for emotion recognition."
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "2.3\nMulti-Model Co-judgment"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "When the accuracy of a single model reaches a bottleneck and is diﬃcult to further improve, a multi-model integration"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "strategy becomes a natural choice. In the context of the MER2024-OV competition, we also followed this idea. As rep-"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "resentatives of large visual multi-modal models, AﬀectGPT and InternVL have the advantage of overall understanding"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "and reasoning capabilities of video content, and can generally capture and summarize the expression information ap-"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "pearing in the video. However,\nin comparison, traditional multi-modal algorithms show higher accuracy on speciﬁc"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "prediction tasks. In view of this, we cleverly combined the strengths of the two and spliced and fused the prediction"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "results of the multi-modal model in the MER2024-SEMI track, the output of AﬀectGPT, and the predictions of InternVL,"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "aiming to improve the performance through the complementarity between them. Overall precision and recall. Experi-"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "mental results show that this strategy signiﬁcantly improves the overall evaluation index and veriﬁes the eﬀectiveness"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "of multi-model integration in improving model performance."
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "3\nExperiments and Analysis"
        },
        {
          "Video Emotion Open-vocabulary Recognition Based on Multimodal Large Language Model\n3": "3.1\nDataset"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , we observed that after lora fine-tuning, the Avg score significantly increased by 3%,",
      "data": [
        {
          "4\nMengying Ge et al.": "3.1.1\nDataset Partitioning. Given the high cost of producing video, audio, and text emotion data, the MER2024 com-"
        },
        {
          "4\nMengying Ge et al.": "petition only provided 322 emotion description data to the contestants.\nIn order to enhance the training eﬀect and"
        },
        {
          "4\nMengying Ge et al.": "stability of the model, during the training process, we not only made full use of the 322 oﬃcially provided emotion"
        },
        {
          "4\nMengying Ge et al.": "description data, but also introduced the general\ntext and image dataset MiniGPT-4[14] as supplementary training"
        },
        {
          "4\nMengying Ge et al.": "materials.\nIn addition,\nin order to verify the generalization ability of the model, we randomly selected 66 of the 322"
        },
        {
          "4\nMengying Ge et al.": "data as a test set to evaluate the performance of the model in diﬀerent scenarios. The speciﬁc distribution of the data"
        },
        {
          "4\nMengying Ge et al.": "is shown in Table 1."
        },
        {
          "4\nMengying Ge et al.": "3.1.2\nDataset Generating. Since generative large models usually require paired Image-Text data formats during ﬁne-"
        },
        {
          "4\nMengying Ge et al.": "tuning, and our ﬁne-tuning goal\nfocuses on face-centered sentiment analysis,\nthere is a lack of a large number of"
        },
        {
          "4\nMengying Ge et al.": "high-quality datasets on facial expressions suitable for large model training in existing open source datasets. Therefore,"
        },
        {
          "4\nMengying Ge et al.": "we adopted the method of generating data to make up for this deﬁciency. The main process is to use the capabilities"
        },
        {
          "4\nMengying Ge et al.": "of Qwen-VL and CogVLM to generate captions (descriptive text) that meet the needs. When generating captions, we"
        },
        {
          "4\nMengying Ge et al.": "carefully designed the prompt, \"As an expert\nin the ﬁeld of emotions, pay close attention to the facial expressions,"
        },
        {
          "4\nMengying Ge et al.": "body movements, environment, and subtitle content of\nthe characters in the image to capture clues closely related"
        },
        {
          "4\nMengying Ge et al.": "to personal emotions, and provide detailed descriptions based on this, and ﬁnally predict the emotional state of the"
        },
        {
          "4\nMengying Ge et al.": "characters in the image.\" This design is intended to ensure that the generated caption can fully and accurately reﬂect the"
        },
        {
          "4\nMengying Ge et al.": "emotional information in the image. Subsequently, in order to improve the quality of training data, we used InternVL"
        },
        {
          "4\nMengying Ge et al.": "to compare text similarity. Speciﬁcally, for each pair of generated captions, we input them into InternVL and set the"
        },
        {
          "4\nMengying Ge et al.": "prompt to: \"Please judge whether the emotions described in these two sentences are similar and give a score between"
        },
        {
          "4\nMengying Ge et al.": "0 and 1.\" Through this step, we can eﬀectively eliminate low-quality captions with a similarity lower than 0.9, and for"
        },
        {
          "4\nMengying Ge et al.": "caption pairs with a similarity higher than 0.9, we randomly select one of them to join the training set."
        },
        {
          "4\nMengying Ge et al.": "Finally, we successfully generated 26,000 pairs of high-quality captions based on the open source dataset CH-SIMS-"
        },
        {
          "4\nMengying Ge et al.": "v2[15]. These captions will be used as the ﬁnal training set to improve the performance of our model in facial emotion"
        },
        {
          "4\nMengying Ge et al.": "analysis."
        },
        {
          "4\nMengying Ge et al.": "3.2\nResults and Analysis"
        },
        {
          "4\nMengying Ge et al.": "3.2.1\nSettings.\nIn the ﬁne-tuning training of AﬀectGPT, we focused on optimizing its Audio Q-Former and Video Q-"
        },
        {
          "4\nMengying Ge et al.": "Former layers. The training process used 4 A800 GPUs with a batch size of 4 and lasted for 100 training cycles (epochs),"
        },
        {
          "4\nMengying Ge et al.": "which took a total of about 33 hours. For the ﬁne-tuning of InternVL, we relied on the Swift framework and used lora"
        },
        {
          "4\nMengying Ge et al.": "(Low-Rank Adaptation)\ntechnology to ﬁne-tune parameters to reduce the amount of calculation while maintaining"
        },
        {
          "4\nMengying Ge et al.": "model performance. The training dataset consists of 26,000 carefully constructed image-text pairs. During the training"
        },
        {
          "4\nMengying Ge et al.": "process, 8 A800 GPUs were used for acceleration, and the batch size was 1 to stabilize the training process. In addition,"
        },
        {
          "4\nMengying Ge et al.": "we also set the maximum input length to 8192 to support the processing of more complex image and text information."
        },
        {
          "4\nMengying Ge et al.": "3.2.2\nAnalysis.\nIn the experiment on the InternVL model, we conducted a comparative evaluation between the zero-"
        },
        {
          "4\nMengying Ge et al.": "shot test and lora ﬁne-tuning based on the 322 oﬃcially provided data sets. The core measurement standard of\nthe"
        },
        {
          "4\nMengying Ge et al.": "experiment\nis Avg score. The speciﬁc deﬁnition of\nthis indicator can refer to the MER2024 speciﬁcation. From the"
        },
        {
          "4\nMengying Ge et al.": "experimental\nresults Table 1, we observed that after lora ﬁne-tuning,\nthe Avg score signiﬁcantly increased by 3%,"
        },
        {
          "4\nMengying Ge et al.": "which strongly supports our hypothesis: ﬁne-tuning on a human-centered data set can eﬀectively promote the model’s"
        },
        {
          "4\nMengying Ge et al.": "performance in downstream tasks."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "due to the reduction in resolution, thus aﬀecting the prediction accuracy of the model.": ""
        },
        {
          "due to the reduction in resolution, thus aﬀecting the prediction accuracy of the model.": "model"
        },
        {
          "due to the reduction in resolution, thus aﬀecting the prediction accuracy of the model.": "InternVL-Chat-V1.5(zero-shot)"
        },
        {
          "due to the reduction in resolution, thus aﬀecting the prediction accuracy of the model.": "InternVL-Chat-V1.5(zero-shot)"
        },
        {
          "due to the reduction in resolution, thus aﬀecting the prediction accuracy of the model.": "InternVL-Chat-V1.5(ﬁne-tune)"
        },
        {
          "due to the reduction in resolution, thus aﬀecting the prediction accuracy of the model.": "InternVL-Chat-V1.5(zero-shot+ﬁne-tune)"
        },
        {
          "due to the reduction in resolution, thus aﬀecting the prediction accuracy of the model.": "InternVL-Chat-V1.5(zero-shot+ﬁne-tune) + discriminative model"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1] Rosalind W Picard. Aﬀective computing. MIT press, 2000."
        },
        {
          "References": "[2] Zheng Lian, Haiyang Sun, Licai Sun, Zhuofan Wen, Siyuan Zhang, Shun Chen, Hao Gu, Jinming Zhao, Ziyang Ma, Xie Chen, et al. Mer 2024:"
        },
        {
          "References": "Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition. arXiv preprint arXiv:2404.17113, 2024."
        },
        {
          "References": "[3] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model for video understanding.\narXiv preprint"
        },
        {
          "References": "arXiv:2306.02858, 2023."
        },
        {
          "References": "[4] Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, and Chao Zhang. Salmonn: Towards generic hearing"
        },
        {
          "References": "abilities for large language models. arXiv preprint arXiv:2310.13289, 2023."
        },
        {
          "References": "[5] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl:"
        },
        {
          "References": "Modularization empowers large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023."
        },
        {
          "References": "[6] Yunfei Chu, Jin Xu, Xiaohuan Zhou, Qian Yang, Shiliang Zhang, Zhijie Yan, Chang Zhou, and Jingren Zhou. Qwen-audio: Advancing universal"
        },
        {
          "References": "audio understanding via uniﬁed large-scale audio-language models. arXiv preprint arXiv:2311.07919, 2023."
        },
        {
          "References": "[7] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang. The dawn of lmms: Preliminary explo-"
        },
        {
          "References": "rations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 9(1):1, 2023."
        },
        {
          "References": "[8] Zheng Lian, Haiyang Sun, Licai Sun, Jiangyan Yi, Bin Liu, and Jianhua Tao. Aﬀectgpt: Dataset and framework for explainable multimodal emotion"
        },
        {
          "References": "recognition. arXiv preprint arXiv:2407.07653, 2024."
        },
        {
          "References": "[9] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far"
        },
        {
          "References": "are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024."
        },
        {
          "References": "[10] George A Reis, Jonathan Chang, Neil Vachharajani, Ram Rangan, and David I August. Swift: Software implemented fault tolerance. In International"
        },
        {
          "References": "symposium on Code generation and optimization, pages 243–254. IEEE, 2005."
        },
        {
          "References": "[11]\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large"
        },
        {
          "References": "vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023."
        },
        {
          "References": "[12] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual"
        },
        {
          "References": "expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023."
        },
        {
          "References": "[13] Zheng Lian, Licai Sun, Yong Ren, Hao Gu, Haiyang Sun, Lan Chen, Bin Liu, and Jianhua Tao. Merbench: A uniﬁed evaluation benchmark for"
        },
        {
          "References": "multimodal emotion recognition. arXiv preprint arXiv:2401.03429, 2024."
        },
        {
          "References": "[14] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced"
        },
        {
          "References": "large language models. arXiv preprint arXiv:2304.10592, 2023."
        },
        {
          "References": "[15] Wenmeng Yu, Hua Xu, Fanyang Meng, Yilin Zhu, Yixiao Ma, Jiele Wu, Jiyun Zou, and Kaicheng Yang. Ch-sims: A chinese multimodal sentiment"
        },
        {
          "References": "analysis dataset with ﬁne-grained annotation of modality.\nIn Proceedings of the 58th annual meeting of the association for computational linguistics,"
        },
        {
          "References": "pages 3718–3727, 2020."
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "3",
      "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "4",
      "title": "Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang",
        "Salmonn"
      ],
      "year": "2023",
      "venue": "Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "5",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "6",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "7",
      "title": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "authors": [
        "Zhengyuan Yang",
        "Linjie Li",
        "Kevin Lin",
        "Jianfeng Wang",
        "Chung-Ching Lin",
        "Zicheng Liu",
        "Lijuan Wang"
      ],
      "year": "2023",
      "venue": "The dawn of lmms: Preliminary explorations with gpt-4v (ision)",
      "arxiv": "arXiv:2309.17421"
    },
    {
      "citation_id": "8",
      "title": "Affectgpt: Dataset and framework for explainable multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Jiangyan Yi",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Affectgpt: Dataset and framework for explainable multimodal emotion recognition",
      "arxiv": "arXiv:2407.07653"
    },
    {
      "citation_id": "9",
      "title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites",
      "authors": [
        "Zhe Chen",
        "Weiyun Wang",
        "Shenglong Hao Tian",
        "Zhangwei Ye",
        "Erfei Gao",
        "Wenwen Cui",
        "Kongzhi Tong",
        "Jiapeng Hu",
        "Zheng Luo",
        "Ma"
      ],
      "year": "2024",
      "venue": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites",
      "arxiv": "arXiv:2404.16821"
    },
    {
      "citation_id": "10",
      "title": "Swift: Software implemented fault tolerance",
      "authors": [
        "George Reis",
        "Jonathan Chang",
        "Neil Vachharajani",
        "Ram Rangan",
        "David August"
      ],
      "year": "2005",
      "venue": "International symposium on Code generation and optimization"
    },
    {
      "citation_id": "11",
      "title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Shusheng Yang",
        "Shijie Wang",
        "Sinan Tan",
        "Peng Wang",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "arxiv": "arXiv:2308.12966"
    },
    {
      "citation_id": "12",
      "title": "Cogvlm: Visual expert for pretrained language models",
      "authors": [
        "Weihan Wang",
        "Qingsong Lv",
        "Wenmeng Yu",
        "Wenyi Hong",
        "Ji Qi",
        "Yan Wang",
        "Junhui Ji",
        "Zhuoyi Yang",
        "Lei Zhao",
        "Xixuan Song"
      ],
      "year": "2023",
      "venue": "Cogvlm: Visual expert for pretrained language models",
      "arxiv": "arXiv:2311.03079"
    },
    {
      "citation_id": "13",
      "title": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Yong Ren",
        "Hao Gu",
        "Haiyang Sun",
        "Lan Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Merbench: A unified evaluation benchmark for multimodal emotion recognition",
      "arxiv": "arXiv:2401.03429"
    },
    {
      "citation_id": "14",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    },
    {
      "citation_id": "15",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Fanyang Meng",
        "Yilin Zhu",
        "Yixiao Ma",
        "Jiele Wu",
        "Jiyun Zou",
        "Kaicheng Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    }
  ]
}