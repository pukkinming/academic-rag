{
  "paper_id": "2311.00394v1",
  "title": "An Analysis Of Large Speech Models-Based Representations For Speech Emotion Recognition",
  "published": "2023-11-01T09:40:40Z",
  "authors": [
    "Adrian Bogdan Stânea",
    "Vlad Striletchi",
    "Cosmin Striletchi",
    "Adriana Stan"
  ],
  "keywords": [
    "speech emotion recognition",
    "large speech models",
    "signal-based features",
    "self-supervised learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large speech models-derived features have recently shown increased performance over signal-based features across multiple downstream tasks, even when the networks are not finetuned towards the target task. In this paper we show the results of an analysis of several signal-and neural models-derived features for speech emotion recognition. We use pretrained models and explore their inherent potential abstractions of emotions. Simple classification methods are used so as to not interfere or add knowledge to the task. We show that, even without finetuning, some of these large neural speech models' representations can enclose information that enables performances close to, and even beyond state-of-the-art results across six standard speech emotion recognition datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition (SER) can provide an important additional information channel in various speech-enabled applications. As opposed to the linguistic contents which can be, to some extent, controlled by the speaker, the emotions lie deeper within the cognitive processes and require more active effort on behalf of the person. However, similar to speech variability, emotion realisation variability can add a higher complexity to their classification task. Also, in the absence of the visual channel, spoken emotions can be easily mistaken by the listeners.\n\nIn terms of classification methods and architectures, the literature shows a wide interest for SER, with studies ranging from simple signal-based analyses to more recent deep architectures. In most of these studies, purposely built sets of features are used.\n\nAnother problem with SER lies within the lack of extended spoken corpora with natural emotion elicitation. The common evaluation benchmarks for SER are a set of acted speech datasets, in which professional or naive speakers were instructed to perform a pre-defined or spontaneous spoken interaction in a desired emotion.\n\nIn this paper we aim to perform an analysis over the use of various signal features, including large speech models-and self supervised learning-based (SSL) models-derived embeddings, in an effort to accurately classify the emotions present in 6 different datasets. We look into 9 sets of features and use simple classification methods to determine the intrinsic abstract representations of emotion that they may contain.\n\nThe paper is organised as follows: Section II gives a brief overview of the state-of-the-art on speech emotion recognition. Section III introduces the methodology and used speech datasets. Section IV presents and discusses the numeric results, while conclusions are drawn in Section V.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Emotion recognition has long been a topic of interest for the speech research community  [1] . Initial studies focused on simple signal-based measures, such as Mel Frequency Cepstral Coefficients, Linear Predictive Coding Coefficients or the Teager Operator. With the advent of deep neural networks, the focus shifted towards large architectures which may be better at disentangling the emotion representation within the speech signal. Some of the recent studies attaining state-of-the-art results are focused on either recurrent or convolutional neural networks  [2] -  [5] . For example,  [2]  compares high-dimensional MFCC input with frequency-domain and time-domain learning filters within the network. The main architecture is based on a time-delay neural network (TDNN). Ye et al.  [3]  use a Gated Multi-scale Temporal Convolutional Network, which aims to create a unique component for learning emotional causality representation using a multi-scale receptive field that captures the temporal dynamics of emotions. Additionally, the network incorporates skip connections to merge high-level features from different gated convolution blocks, enabling it to capture nuanced changes in emotion within human speech. GM-TCNet takes Mel-frequency cepstral coefficients as inputs and utilises the gated temporal convolutional module to generate the highlevel features. Wen et al.  [4]  introduce an architecture based on capsule nets and transfer learning. It also deals with crosscorpus evaluation, for which it uses an adversarial module.\n\nCurrently, the best performing network across several SER datasets is the one of  [5] . The networks consists of so called temporally-aware blocks (TABs) which processes cropped 4 second-length segments from each utterance in a forward and reverse time flow. Within this study, we also include the use of large pretrained speech models-derived features. By making use of vast amounts of speech data within their training step, these models are presumed to have learned abstract representations of speech, which are not easily noticeable or detectable with standard signal processing procedures. As a result, this type of features have been included in multiple other studies, such as  [6] , where the wav2vec features are passed through two dense layers, ReLU activation and dropout. The results are reported on two datasets, IEMOCAP  [7]  and RAVDESS  [8] , and surpass the state-of-the-art results.\n\nThe main contribution of our work is linked to evaluating several large speech models trained on non-SER prediction tasks and the features they produce, as well as standard signalbased features (i.e. Mel frequency cepstral coefficients and Mel spectrogram) for SER classification. The features are passed through shallow and simple algorithms such that they do not learn additional high-level features in the process, but rather exploit the abstract or deterministic representations fed as input.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "To determine the extent to which very deep neural networks may perform an abstraction task that involves the representation of emotion, we select six standard SER datasets, extract nine feature sets, and use simple classification algorithms for evaluation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Speech Datasets",
      "text": "We perform our analysis on a set of six widely adopted SER datasets: CREMA-D  [9] , EMOVO  [10] , EMODB  [11] , IEMOCAP  [7] , RAVDESS  [8] , and SAVEE  [12] . An overview of the these datasets is shown in Table  I .  1  From Table  I  we can notice that not all emotions are present in all datasets, and in some of them the emotion categories may be merged or split according to the dataset designer's choice. This means that, aside from the complex task of speech emotion recognition, the lack of consensus across research groups with regards to the number and realisation of emotions in speech posses additional problems. Some other important facts to be noticed about the datasets is their different duration, as well as their multilingualism: IEMOCAP, RAVDESS, CREMA-D and SAVEE are English datasets, EMOVO is Italian, and EMODB is German. These axes of variation enable us to understand how the different speech features behave in a closer to reallife scenario, and if any of these features are in some way correlated to any of the axes.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Speech Features And Pretrained Models 1) Signal Features:",
      "text": "In terms of simple signal-based features we use two common representations within the speech research community:\n\n• The Mel Spectrogram -is a simple auditory perceptionbased frequency shift of the speech spectrum. The Mel Spectrograms were extracted using the Librosa 2  Python library. Each spectrogram was built using 80 filter banks to process the raw signal at a 22.05 kHz sampling rate. A Hanning window was used to weight the analysis window of length 2048, with a hop length of 512. The Mel Spectrograms were averaged across the time domain.\n\n• Mel Frequency Cepstral Coefficients (MFCC) -are designed to mimic the human auditory system's perception of sound by emphasising relevant spectral information and reducing the impact of irrelevant noise and other audio variations. The cepstrum is a homomorphic transform based on the logarithmic representation of the speech spectrum. The Mel scale transform adds information with respect to the auditory perception of spectral components. The MFCCs were extracted using the Librosa library, using the same Hanning weighting and window length of 2048 with a hop length of 512. In this manner, 39 coefficients were extracted at each step and their mean value across time was computed for each individual signal. 2) Speaker and language embedding networks: For the current analysis we used a set of pretrained models available in the NeMo framework  [13] .  3  NeMo is a large library of deep models released by NVIDIA which can be easily used within this framework. We selected the following pretrained models:\n\n• SpeakerNet [14] -uses 1D depth-wise separable convolutions, and x-vector based pooling layers. The architecture is lightweight and maps variable length utterances into a fixed length embedding;\n\n• TitaNet  [15]  -uses the same 1D depth-wise separable convolutions, but combined with squeeze-and-excitation layers. A similar pooling layer is used to obtain a fixed length representation;\n\n• Ecapa TDNN -is similar to the Speech Brain architecture and uses the ECAPA-TDNN structure  [16] . The difference is that, instead of the residual blocks, the NeMo implementation uses group convolution blocks of single dilation.\n\n• AmberNet -it is very similar to the TitaNet architecture, yet its training objective was to discriminate between spoken languages. This network was selected in our study to verify if other speech-based tasks may also learn information about the speech emotions. 3) Self-Supervised Learning networks: Self-supervised learning (SSL) networks are omnipresent in various speechrelated applications with results going beyond the state-of-theart across numerous tasks. Their main advantage is the fact that within their training procedure, they are guided towards finding an optimal compressed representation of speech. As long as the representation is not finetuned towards a target task, it can be assumed that it includes the different dimensions of speech variability, such as speaker, intonation, recording conditions, and possibly, even emotions. We, therefore, include in our study a set of three SSL pretrained models available in the S3PRL 4  framework. S3PRL is built as a wrapper over multiple speech models released by various research groups. This unifies the use of these models and makes it easier to test several representations in downstream tasks. We selected the following three models:\n\n• wav2vec  [17]  -is one of the most commonly used selfsupervised representations across various applications. Its initial task was that of automatic speech recognition. It uses a convolutional neural network (CNN) to directly learn representations from raw audio waveforms without requiring any explicit phonetic or linguistic annotations.\n\nThe model utilises a contrastive objective function to encourage the encoder to learn to differentiate between positive and negative examples, enhancing the robustness and generalisation of the learned representations.\n\n• Decoar  [18]  -is a semi-supervised model which uses a large amount of unlabelled audio data for an initial representation learning. The representation is based on filterbank features extracted from a context frame. The so called deep contextualised acoustic representations (De-CoAR) are then fine-tuned towards an automatic speech recognition system.\n\n• HuBERT  [19]  -was released as a speech-based version of the BERT text models. It utilises a combination of contrastive learning and the model's transformer-based architecture to extract discriminative and context-rich features from raw audio waveforms. HuBERT has demonstrated impressive performance in pretraining speech representations and has been used as a backbone for various downstream speech processing tasks, such as automatic speech recognition (ASR) and speaker verification. No finetuning of any of the deep models was performed, and the official releases for them were used. All networks take raw waveforms as input.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Data Pre-Processing And Ser Prediction Models",
      "text": "The main purpose of this study is to perform an analysis over various speech representations that may inherently incorporate high-level information regarding the emotions elicited in the utterances. And we did not aim at surpassing the stateof-the-art results in speech emotion recognition. Therefore, we only used lightweight classification algorithms trained on the speech representations described in the previous subsection.\n\nWe selected two classification methods: a support vector machine-based classifier (SVC) and a four layer deep feed forward neural network (DNN):\n\n• The used SVC is the one implemented in Scikit-Learn  5 and had a the regularisation parameter equal to 10 and the gamma parameter was set to 0.001;\n\n• The DNN consists of a series of layers, specifically designed for the task of emotion classification. The input layer accepts the feature tensor for processing. Subsequently, a set of two hidden layers incrementally transform the input into a more abstract representation by halving the output size with each layer, which results in the third layer having an output size that is 8 times smaller than the initial feature vector. Complementary to this transformation, a batch normalisation layer and a Rectified Linear Unit (ReLU) activation function are utilised alongside a dropout layer to prevent overfitting. The DNN was implemented in Pytorch. 6  Also, because our task is not to temporally locate the emotions within a spoken utterance, we used time-domain averaged representations. As a result, the only change across the classification algorithms is the number of input features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Results",
      "text": "The results of our analysis are shown in Table  IV . We also list the state-of-the-art results across the six datasets. These pertain to the following studies: for CREMA-D -  [20] ; IEMOCAP -  [21] ; for EMOVO, EMODB, RAVDESS, and SAVEE -  [5] . We need to note here that since our submission, Ye et al.  [5]  have updated their reported results with lower performance on average. Yet we maintain their top reported results in order to show that the different embeddings still include a large amount of non-speaker related information.\n\nThe performance measures are the unweighted average recall (U AR) and the weighted average recall (W AR): where k is the emotion class id, and α k is the weighting coefficient for the respective class. α k is computed as the fraction of samples which pertain to class k within the complete set of test samples. U AR is a special case of W AR where all α k are equal to 1/k. T P counts the true positive samples of a class, and F N refers to the false negative counts. The recall, as opposed to the accuracy, measures how many positive samples were correctly identified from the total number of positive examples, rather than a total count from all classes. In a multi-class classification task, the recall is computed for each individual class, and then averaged to obtain the task's recall measure. If the same weight is given to each class, irrespective of its representation within the test set, the recall is then unweighted. The unweighted average recall in multiclass classification is equal to the accuracy. On the other hand, if the percentage of samples from a class within the test set is used to weight the class recall, then the final measure is called weighted average recall. The recall measure has been widely adopted in the SER studies, and we adopt it here, as well. Also because there are no official splits of the datasets into training and testing subsets, we perform a 5-fold cross validation for each dataset and report the averaged results. Some very important results arise from the table: except for the CREMA-D dataset, all our results are very close, and in some cases surpass the state-of-the-art results (last row of the table). For the EMODB and SAVEE datasets, the TitaNet features combined with the simple DNN algorithm show an average 2% absolute increase in both the UAR and WAR measures. The best performing set of features across all the datasets is that extracted with the TitaNet architecture.\n\nAs expected, simple signal-derived features (i.e. Mel spectrogram and MFCCs) are not as proficient as the DNNderived ones. One interesting aspect is that the AmberNet architecture which was trained for language recognition is able to provide rich speech representations that include discriminative information about the speech emotion. Also, all speaker embedding networks have a better performance, as opposed to the SSL-features which seem to be less representative for the emotion prediction. A side result of this analysis is again the fact that there representations, although trained to disentangle the speech variation axes, are not yet providing such decomposition power, similar to the results shown in  [22] .\n\nV. CONCLUSIONS In this paper, we performed an analysis over several speech feature sets to find the best representation for speech emotion recognition. These feature sets are either derived from signal processing, such as Mel spectrograms or MFCCs, while a separate set of features come from large pretrained speech models. Because we did not want to surpass the state-of-theart results on SER, but rather to look into better emotion representations, the prediction algorithms in the evaluation were rather simple. An SVM classifier and a four layer feed forward network were used to predict the emotion classes across six SER datasets.\n\nOur evaluation showed that, at least for some of the datasets, these LSM-derived features already include relevant information for emotion recognition, although they were not trained for this particular objective. As a result, for example the embeddings from TitaNet and Decoar features on the EMODB, SAVEE, IEMOCAP datasets are very close or surpass the state-of-the-art results. We may argue that in this case, the use of more advanced prediction architectures can be convoluted, and that more attention should be paid to the speech representation.\n\nAs future work, we plan to include the best feature sets resulted from this study into state-of-the-art networks pur-posely designed for SER. We assume that these features should simplify this complex task and attain better results. A separate task is that of finding more accurate speech datasets, either by sampling from the online resources and manually annotating the emotion information, or by accessing real data from relevant sources such as call-centres where this information may already be annotated by the operators.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset\nLanguage\nEmotions\nSamples\nDuration\nSpeakers\nM/F": "CREMA-D [9]\nChinese\nangry, disgusted,\nfearful, happy, neutral,\nsad\n7442\n5h15’24\n91\n48/43"
        },
        {
          "Dataset\nLanguage\nEmotions\nSamples\nDuration\nSpeakers\nM/F": "EMOVO [10]\nItalian\nangry, disgusted,\nfearful, happy, neutral,\nsad, surprised\n588\n30’\n6\n3/3"
        },
        {
          "Dataset\nLanguage\nEmotions\nSamples\nDuration\nSpeakers\nM/F": "EMODB [11]\nGerman\nangry, disgusted,\nfearful, happy, neutral,\nsad, bored\n535\n24’\n10\n5/5"
        },
        {
          "Dataset\nLanguage\nEmotions\nSamples\nDuration\nSpeakers\nM/F": "IEMOCAP [7]\nEnglish\nangry, happy, neutral, sad\n5531\n11h 37’\n10\n5/5"
        },
        {
          "Dataset\nLanguage\nEmotions\nSamples\nDuration\nSpeakers\nM/F": "RAVDESS [8]\nEnglish\nangry, disgusted,\nfearful, happy, sad, surprised, calm\n1440\n1h 28’\n24\n12/12"
        },
        {
          "Dataset\nLanguage\nEmotions\nSamples\nDuration\nSpeakers\nM/F": "SAVEE [12]\nEnglish\nangry, disgusted,\nfearful, happy, neutral,\nsad, surprised\n480\n30’\n4\n4/0"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SpeakerNet": "TitaNet",
          "SVM\nDNN": "SVM\nDNN",
          "58.90\n58.88\n57.09\n57.22": "65.41\n65.36\n63.61\n63.54",
          "75.79\n75.35\n84.84\n83.96": "85.58\n85.04\n91.73\n91.25",
          "90.56\n90.09\n95.82\n95.21": "94.99\n94.58\n98.34\n97.92",
          "67.32\n66.86\n67.27\n66.27": "69.78\n69.39\n70.63\n69.89",
          "79.44\n78.68\n82.95\n82.78": "80.97\n80.69\n85.18\n85.49",
          "72.84\n75.00\n79.88\n80.83": "83.11\n85.21\n88.94\n88.75",
          "74.14\n74.14\n77.97\n77.71": "79.97\n80.05\n83.07\n82.81"
        },
        {
          "SpeakerNet": "Ecapa TDNN",
          "SVM\nDNN": "SVM\nDNN",
          "58.90\n58.88\n57.09\n57.22": "59.63\n59.65\n60.04\n60.04",
          "75.79\n75.35\n84.84\n83.96": "82.10\n81.82\n88.79\n88.54",
          "90.56\n90.09\n95.82\n95.21": "93.18\n93.27\n97.29\n97.08",
          "67.32\n66.86\n67.27\n66.27": "68.47\n68.00\n68.9\n68.09",
          "79.44\n78.68\n82.95\n82.78": "76.27\n76.25\n81.42\n81.74",
          "72.84\n75.00\n79.88\n80.83": "77.25\n79.17\n84.57\n84.38",
          "74.14\n74.14\n77.97\n77.71": "76.15\n76.36\n80.17\n79.98"
        },
        {
          "SpeakerNet": "AmberNet",
          "SVM\nDNN": "SVM\nDNN",
          "58.90\n58.88\n57.09\n57.22": "68.78\n68.64\n70.53\n70.2",
          "75.79\n75.35\n84.84\n83.96": "69.52\n69.23\n80.16\n79.58",
          "90.56\n90.09\n95.82\n95.21": "89.78\n90.47\n94.94\n94.38",
          "67.32\n66.86\n67.27\n66.27": "71.62\n71.18\n71.92\n70.92",
          "79.44\n78.68\n82.95\n82.78": "87.24\n87.43\n89.41\n89.65",
          "72.84\n75.00\n79.88\n80.83": "77.30\n78.96\n85.36\n85.21",
          "74.14\n74.14\n77.97\n77.71": "77.37\n77.65\n82.05\n81.66"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "wav2vec": "Decoar",
          "SVM\nDNN": "SVM\nDNN",
          "67.77\n67.63\n69.38\n69.18": "67.61\n67.43\n70.04\n69.8",
          "71.29\n71.43\n82.33\n82.5": "65.83\n65.98\n83.72\n82.5",
          "93.53\n93.27\n96.83\n96.25": "86.27\n87.29\n97.82\n97.29",
          "70.29\n69.50\n69.09\n68.33": "71.93\n71.42\n70.75\n69.8",
          "73.64\n73.89\n78.96\n79.51": "70.73\n71.11\n80.34\n80.63",
          "67.33\n69.38\n75.75\n75.83": "57.11\n60.21\n76.02\n74.58",
          "73.97\n74.18\n78.72\n78.60": "69.91\n70.57\n79.78\n79.10"
        },
        {
          "wav2vec": "HuBERT",
          "SVM\nDNN": "SVM\nDNN",
          "67.77\n67.63\n69.38\n69.18": "70.72\n70.46\n72.62\n72.50",
          "71.29\n71.43\n82.33\n82.5": "31.15\n30.44\n39.14\n37.92",
          "93.53\n93.27\n96.83\n96.25": "89.70\n90.09\n96.13\n95.63",
          "70.29\n69.50\n69.09\n68.33": "71.60\n70.91\n71.43\n70.28",
          "73.64\n73.89\n78.96\n79.51": "80.97\n80.69\n76.18\n75.97",
          "67.33\n69.38\n75.75\n75.83": "57.50\n61.88\n69.21\n69.79",
          "73.97\n74.18\n78.72\n78.60": "66.94\n67.41\n70.78\n70.35"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Emotion Identification from Raw Speech Signals Using DNNs",
      "authors": [
        "M Sarma",
        "P Ghahremani",
        "D Povey",
        "N Goel",
        "K Sarma",
        "N Dehak"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "3",
      "title": "Gm-tcnet: Gated multi-scale temporal convolutional network using emotion causality for speech emotion recognition",
      "authors": [
        "J Ye",
        "X.-C Wen",
        "X Wang",
        "Y Luo",
        "C.-L Wu",
        "L.-Y Chen",
        "K Liu"
      ],
      "year": "2022",
      "venue": "Speech Commun"
    },
    {
      "citation_id": "4",
      "title": "Ctl-mtnet: A novel capsnet and transfer learning-based mixed task net for single-corpus and cross-corpus speech emotion recognition",
      "authors": [
        "X Wen",
        "J Ye",
        "Y Luo",
        "Y Xu",
        "X Wang",
        "C Wu",
        "K Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence",
      "doi": "10.24963/ijcai.2022/320"
    },
    {
      "citation_id": "5",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "J Ye",
        "X Cheng Wen",
        "Y Wei",
        "Y Xu",
        "K Liu",
        "H Shan"
      ],
      "year": "2023",
      "venue": "Proceedings of ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "7",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "E Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "8",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2015",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "9",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "EMOVO Corpus: an Italian Emotional Speech Database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "International Conference on Language Resources and Evaluation"
    },
    {
      "citation_id": "11",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of German emotional speech"
    },
    {
      "citation_id": "12",
      "title": "Machine Audition: Principles, Algorithms and Systems",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2010",
      "venue": "Machine Audition: Principles, Algorithms and Systems"
    },
    {
      "citation_id": "13",
      "title": "A toolbox for construction and analysis of speech datasets",
      "authors": [
        "E Bakhturina",
        "V Lavrukhin",
        "B Ginsburg"
      ],
      "year": "2021",
      "venue": "NeurIPS Datasets and Benchmarks"
    },
    {
      "citation_id": "14",
      "title": "Speakernet: 1d depth-wise separable convolutional network for text-independent speaker recognition and verification",
      "authors": [
        "N Koluguri",
        "J Li",
        "V Lavrukhin",
        "B Ginsburg"
      ],
      "year": "2020",
      "venue": "Speakernet: 1d depth-wise separable convolutional network for text-independent speaker recognition and verification"
    },
    {
      "citation_id": "15",
      "title": "Titanet: Neural model for speaker representation with 1d depth-wise separable convolutions and global context",
      "authors": [
        "N Koluguri",
        "T Park",
        "B Ginsburg"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "ECAPA-TDNN: Emphasized channel attention, propagation and aggregation in TDNN based speaker verification",
      "authors": [
        "B Desplanques",
        "J Thienpondt",
        "K Demuynck"
      ],
      "year": "2020",
      "venue": "Interspeech 2020. ISCA"
    },
    {
      "citation_id": "17",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Deep contextualized acoustic representations for semi-supervised speech recognition",
      "authors": [
        "S Ling",
        "Y Liu",
        "J Salazar",
        "K Kirchhoff"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Coordvit: A novel method of improve vision transformer-based speech emotion recognition using coordinate information concatenate",
      "authors": [
        "J.-Y Kim",
        "S.-H Lee"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Electronics, Information, and Communication (ICEIC)"
    },
    {
      "citation_id": "21",
      "title": "Context-Dependent Domain Adversarial Neural Network for Multimodal Emotion Recognition",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang",
        "Z Yang",
        "R Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Residual information in deep speaker embedding architectures",
      "authors": [
        "A Stan"
      ],
      "year": "2022",
      "venue": "Mathematics"
    }
  ]
}