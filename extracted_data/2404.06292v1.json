{
  "paper_id": "2404.06292v1",
  "title": "Nemo: Dataset Of Emotional Speech In Polish",
  "published": "2024-04-09T13:18:52Z",
  "authors": [
    "Iwona Christop"
  ],
  "keywords": [
    "emotional speech",
    "speech corpus",
    "Polish speech"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition has become increasingly important in recent years due to its potential applications in healthcare, customer service, and personalization of dialogue systems. However, a major issue in this field is the lack of datasets that adequately represent basic emotional states across various language families. As datasets covering Slavic languages are rare, there is a need to address this research gap. This paper presents the development of nEMO, a novel corpus of emotional speech in Polish. The dataset comprises over 3 hours of samples recorded with the participation of nine actors portraying six emotional states: anger, fear, happiness, sadness, surprise, and a neutral state. The text material used was carefully selected to represent the phonetics of the Polish language adequately. The corpus is freely available under the terms of a Creative Commons license (CC BY-NC-SA 4.0).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Automatic speech recognition is used in various aspects of life, ranging from customer service systems to virtual assistants and chatbots. As speech is the most natural form of communication for humans, human-computer interaction systems strive to minimize written interfaces.\n\nIncorporating speech emotion recognition in dialogue systems may not seem like an obvious next step. However, this development could significantly refine the personalization of virtual assistants, enabling them to respond appropriately to the emotional state of the user. The impersonal nature of current dialogue systems often results in user discouragement  (Schuller, 2018) .\n\nThe potential applications of emotion recognition extend beyond customer service. For example, in the context of emergency calls, the ability to distinguish emotions such as fear or sadness could be crucial in assessing the authenticity of the call. Similarly,  Abbaschian et al. (2021)  demonstrated its utility within therapy sessions, where the analysis of a patient's emotions could offer insights into their psychological state.\n\nAlthough speech emotion recognition is acknowledged as a critical area of research, the field still faces challenges, most notably the scarcity of highquality, diverse datasets. Most available corpora contain simulated emotional expressions that do not accurately reflect human emotions  (Abbaschian et al., 2021) . Additionally, the development of resources for various natural language processing and speech processing tasks often overlooks lowresource languages.\n\nPolish being the second most spoken language within the Slavic language family exemplifies a significant gap in resources for speech emotion recog-nition. This study aims to bridge this gap with the introduction and analysis of the nEMO dataset, a novel corpus of Polish emotional speech.\n\nThe following section provides an overview of existing emotional speech corpora. Section 3 describes the methodology used to create the nEMO dataset, including the selection of linguistic content, emotional states, and participant actors. Section 4 provides an overview of the final dataset. Section 5 describes the evaluation of the dataset using various machine learning techniques. Finally, section 6 provides conclusions and considerations for future work.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The reliability of speech emotion recognition systems depends on the quality of data used for training. Although finding a corpus for Slavic languages poses a challenge, there are valuable datasets available for other language families. This indicates a global interest and effort in the development of emotional speech datasets.\n\nThe datasets of emotional speech can generally be classified into three categories based on the method used to acquire recordings: natural, seminatural, and simulated.\n\nNatural datasets contain authentic emotional states instead of exaggerated expressions. These datasets are typically sourced from YouTube videos, television series, and call center recordings. While this approach prevents overfitting to artificial emotions, it comes with other complications. The continuity and dynamics of the recordings can complicate emotion detection, and multiple emotional states may be present simultaneously. Additionally, we have no control over the selection of presented emotions. Furthermore, the subjective nature of In the case of semi-natural datasets, actors attempt to portray emotions based on a given script, relying on their own perception of the content. While this approach strives to align with natural speech, the expressions may seem unnatural due to the specified context. Similar to natural datasets, the interpretation and portrayal of emotional states are actor-dependent, and manual labeling is required  (Abbaschian et al., 2021) .\n\nFor simulated datasets, professional actors read the same text while portraying different emotions. This approach allows for the definition of an arbitrary number of discrete emotional states, simplifying comparative analysis across different corpora and ensuring full control over copyrights. However, a major limitation of such datasets is that the enacted emotions may seem unnatural and distorted from conversational speech  (Abbaschian et al., 2021; Schuller, 2018) .\n\nAlthough natural datasets may seem like the best choice due to their authentic representation of human emotions, the majority of existing resources utilize the simulated approach because of its vast advantages. The selection of the approach is usually followed by determining the spectrum of emotions to be included. Existing datasets consist of portrayals of various emotional states, including anger, fear, happiness, sadness, and neutral state. The curation of linguistic material that accurately reflects the phonetics of the targeted language is the final essential step.\n\nTable  1  presents a comparative overview of several well-known emotional speech datasets: CREMA-D  (Cao et al., 2014) ,  EMO-DB (Burkhardt et al., 2005) , IEMOCAP  (Busso et al., 2008) , RAVDESS  (Livingstone and Russo, 2018) , TESS (Pichora-Fuller and Dupuis, 2020), and URDU  (Latif et al., 2018) . It summarizes each dataset by providing information on its language, category, total number of samples, number of contributing speakers, and the range of emotions covered. This comparison provides an understanding of the diversity of existing resources in the field of speech emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotional States",
      "text": "We chose the simulated approach for the development of the nEMO dataset due to its numerous advantages. This method allowed us to define emotions in a distinct and categorical manner, while also providing control over copyrights.\n\nThe nEMO dataset focuses on six basic emotions: anger, fear, happiness, sadness, surprise, and neutral state. These emotions were selected due to their differences in valence, arousal, and dominance, which facilitates their discrimination  (Plutchik and Kellerman, 1980) . Furthermore, these emotional states are frequently depicted in existing corpora, allowing for an increase in the number of samples for basic emotions. At craft fairs, you can see many unique products.\n\nTable  2 : Representative sentences from the nEMO datasets that demonstrate the phonetic diversity of Polish, along with their English translations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Linguistic Content",
      "text": "In the development of the nEMO dataset, a simulated approach was used where each actor was required to record the same set of utterances for each of the six different emotional states. To ensure optimal duration of the recording sessions, an effort was made to minimize the number of utterances as much as possible. Additionally, the linguistic content was prepared to sufficiently represent the phonetics of the Polish language.\n\nThe initial step in preparing linguistic material involved identifying 90 uncommon phonemes present in the Polish language. For each phoneme, a word in which it appears was selected and used in a sentence. This resulted in the creation of 90 sentences, each containing at least one uncommon phoneme.\n\nIt should be noted that all phrases are semantically correct and could be used in everyday conversations. Furthermore, the emotional neutrality of the sentences was not emphasized.\n\nTable  2  shows examples of the sentences used in the recordings.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Actors",
      "text": "Nine actors, four female and five male, were involved in the development of the nEMO dataset. They ranged in age from 20 to 30 and were all native speakers of Polish.\n\nThree of the participants were qualified voice actors. To avoid exaggerated emotional expressions, which are common among theatrical performers, individuals without formal acting training were also included.\n\nThe dataset was balanced by strategically incorporating non-professional speakers to reduce the potential for exaggerated emotional portrayals. Although concerns were raised about the adequacy of emotional expression by non-actors, analysis revealed no significant differences between professionals and amateurs, both audibly and in spectral data analysis. To maintain the dataset's integrity, we excluded any recordings that raised doubts about their emotional authenticity through human evaluation.\n\nDuring the recording sessions, actors were given explicit instructions to focus on depicting a single emotional state at a time. Feedback was provided constantly to all participants, particularly aiding those without professional experience. This guidance often included encouragement to use nonverbal cues, such as facial expressions and gestures, to enhance the authenticity of emotional expression. For example, actors were instructed to smile when portraying happiness, in order to enhance the emotional quality of the recordings.\n\nAll participants provided consent for the use and distribution of their voices in the form of audio recordings. This agreement covers the rights to use, preserve, process, and reproduce the audio content. It also includes provisions for unrestricted distribution and public availability under the terms of a Creative Commons license (CC BY-NC-SA 4.0), ensuring ethical and legal use of the dataset in subsequent research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Recording Setup",
      "text": "The recordings were conducted in a home setting to better reflect a natural environment. Each recording session involved one actor and lasted approximately two hours, excluding post-processing or sample evaluation.\n\nThe utterances were captured using Mozos MKIT-900 Pro, a cardioid condenser microphone with a 192 kHz sampling rate, ~42 dB sensitivity, and frequency response from 100 Hz to 18 kHz. The recording equipment also included a sponge",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Final Dataset",
      "text": "The nEMO dataset underwent human evaluation, and only recordings that accurately captured the intended emotional state were included. The resulting dataset contains 4,481 audio recordings, a total of more than three hours of speech. Table  3  shows the distribution of samples per emotional state.\n\nThe final version of the nEMO dataset includes a comprehensive set of attributes. These attributes consist of an audio sample, a label corresponding to the emotional state, and both original and normalized transcriptions of the utterance. Additionally, metadata related to the speaker's ID, gender, and age are included.\n\nThe nEMO dataset is available as a single entity and is not divided into predefined training and test splits. This approach allows researchers and developers to customize the splits according to their specific needs. The dataset's diverse attributes make it a valuable asset for a broad spectrum of speech processing tasks. It was primarily designed to facilitate speech emotion recognition and contains recordings annotated with one of six discrete emotional states: anger, fear, happiness, sadness, surprise, or neutral. Additionally, the dataset includes detailed metadata on the speaker, making it suitable for various audio classification tasks. Furthermore, including both orthographic and normalized transcriptions for each sample enhances the dataset's utility for automatic speech recognition tasks. The linguistic content has been carefully selected to showcase a broad spectrum of the phonetics of the Polish language. Additionally, the emotional speech recordings, complemented by transcriptions, lay the groundwork for the development of text-to-speech systems capable of generating emotional speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Evaluation",
      "text": "To ensure accurate representation of designated emotional states, three experimental evaluations were conducted using machine learning algorithms. The dataset was randomly divided into training and test sets at an 80:20 ratio. It is important to note that the splits were not speaker-independent.\n\nThe experiments utilized three classifiers: Support Vector Machine (SVM), Logistic Regression, and Random Forest. These classifiers leveraged Mel-frequency cepstral coefficients (MFCCs) as input features. From each audio recording, 20 MFCCs were computed and then averaged to obtain a one-dimensional feature vector. This approach, based on basic features and machine learning methods, was chosen to provide interpretable results of the experiments. The performance of the classifiers is shown in Figure  1 , indicating satisfactory outcomes. Random Forest achieved the highest accuracy at 83.95%. The confusion matrices for all classifiers are shown in Figure  2 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Svm",
      "text": "The SVM classifier achieved 59.64% accuracy and a well-balanced precision-recall ratio with a macro F1 score of 58.17%. However, the classifier exhibited a tendency for errors in recognizing the emotional state of surprise. The notably lower recall rate for this emotional state suggests a need for reevaluation of these particular samples.\n\nThe Logistic Regression model produced comparable results to the SVM in terms of accuracy and  The errors made by both models were mainly related to misclassification between emotions with similar levels of arousal, highlighting the complexity of the dataset and the nuanced challenge it poses. In contrast, the Random Forest classifier achieved the highest accuracy and demonstrated consistent performance across all emotional states.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusions",
      "text": "This paper presents the development and evaluation of the nEMO dataset, a novel Polish emotional speech corpus. This dataset, which contains recordings of nine actors delivering 90 sentences in six different emotional states, consists of 4,481 samples, representing over three hours of speech data. It includes a wide range of metadata fields, including raw audio recordings, emotional state labels, speaker gender and age, and utterance transcriptions, thereby extending its utility beyond its primary goal for various speech processing tasks.\n\nThe robustness of the dataset was evaluated using three machine learning methods: SVM, Logistic Regression, and Random Forest. The satisfactory performance of these classifiers proves the effectiveness of the dataset in capturing the nuanced phonetics of Polish emotional speech, highlighting its potential as a valuable asset for speech emotion recognition research.\n\nGiven the availability of specialized corpora in the field of speech emotion recognition, the nEMO dataset is an important resource aimed at bridging this gap. Future work for the corpus includes extensive development, including further human evaluation and the inclusion of additional recordings from more actors to further increase its diversity and applicability.\n\nThe nEMO dataset is available under a Creative\n\nCommons license (CC BY-NC-SA 4.0) on platforms such as the Hugging Face 1  website and GitHub 2  to enable further research and development in the field of speech emotion recognition. By making nEMO publicly available, we hope to encourage progress in the field and the creation of effective emotion recognition systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Acknowledgments",
      "text": "The author would like to thank all the actors whose voices are the essence of the nEMO dataset. Their commitment and dedication were instrumental in capturing the diverse emotional states that this corpus seeks to represent. Their contribution is the foundation of this research. Special thanks are due to Dr. Marek Kubis, whose invaluable support and insightful guidance were crucial throughout the research and development of this project.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Bibliographical References",
      "text": "",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Performance metrics of SVM, Logistic",
      "page": 4
    },
    {
      "caption": "Figure 1: , indicating satisfactory outcomes. Ran-",
      "page": 4
    },
    {
      "caption": "Figure 2: The SVM classifier achieved 59.64% accuracy",
      "page": 4
    },
    {
      "caption": "Figure 2: Comparison of confusion matrices for SVM, Logistic Regression, and Random Forest classifiers",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: An overview of popular emotional speech datasets, highlighting language, type, number of",
      "data": [
        {
          "Dataset": "CREMA-D",
          "Language": "English",
          "Type": "simulated",
          "Number of\nsamples": "7,742",
          "Number of\nspeakers": "91",
          "Emotions": "anger, disgust,\nfear, happi-\nness, neutral, sadness"
        },
        {
          "Dataset": "EMO-DB",
          "Language": "German",
          "Type": "simulated",
          "Number of\nsamples": "700",
          "Number of\nspeakers": "10",
          "Emotions": "anger, boredom, disgust,\nfear,\nhappiness,\nneutral,\nsadness"
        },
        {
          "Dataset": "IEMOCAP",
          "Language": "English",
          "Type": "semi-natural",
          "Number of\nsamples": "1,150",
          "Number of\nspeakers": "10",
          "Emotions": "anger, disgust, excitement,\nfear,\nfrustration,\nhappi-\nness,\nneutral,\nsadness,\nsurprise"
        },
        {
          "Dataset": "RAVDESS",
          "Language": "English",
          "Type": "simulated",
          "Number of\nsamples": "2,496",
          "Number of\nspeakers": "24",
          "Emotions": "anger, calm, disgust, fear,\nhappiness,\nneutral,\nsad-\nness, surprise"
        },
        {
          "Dataset": "TESS",
          "Language": "English",
          "Type": "simulated",
          "Number of\nsamples": "2,800",
          "Number of\nspeakers": "2",
          "Emotions": "anger, disgust,\nfear, hap-\npiness, neutral, sadness,\nsurprise"
        },
        {
          "Dataset": "URDU",
          "Language": "Urdu",
          "Type": "natural",
          "Number of\nsamples": "400",
          "Number of\nspeakers": "38",
          "Emotions": "anger, happiness, neutral,\nsadness"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Phoneme": "˜\ni",
          "Word": "zima",
          "Word in IPA": "[\"ý˜ima]",
          "Sentence in Polish": "Zima to czas jeżdżenia\nna sankach.",
          "Sentence in English": "Winter\nis\nthe\ntime\nfor\nsledding."
        },
        {
          "Phoneme": "b",
          "Word": "baza",
          "Word in IPA": "[baza]",
          "Sentence in Polish": "Baza\nwojskowa\njest\nstrategicznym punktem\ndla armii.",
          "Sentence in English": "The military base is a\nstrategic\npoint\nfor\nthe\narmy."
        },
        {
          "Phoneme": "ţj",
          "Word": "racja",
          "Word in IPA": "[\"ra“ţjja]",
          "Sentence in Polish": "Racja,\nten pomysł\njest\nnajlepszy.",
          "Sentence in English": "Right,\nthis\nidea is\nthe\nbest."
        },
        {
          "Phoneme": "sj",
          "Word": "pasja",
          "Word in IPA": "[pasjja]",
          "Sentence in Polish": "Piłka\nnożna\nto moja\npasja.",
          "Sentence in English": "Soccer is my passion."
        },
        {
          "Phoneme": "l˚j",
          "Word": "rzemieślniczy",
          "Word in IPA": "˚j\n[­Z˜Emjj˙ECl\n\"ñi“Ù1]",
          "Sentence in Polish": "Na\ntargach\nrzemieśl-\nniczych\nmożna\nzobaczyć wiele\nunika-\ntowych produktów.",
          "Sentence in English": "At craft\nfairs,\nyou can\nsee many unique prod-\nucts."
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "2",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Michael Keutmann",
        "Ruben Gur",
        "Ani Nenkova",
        "Ragini Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2014.2336244"
    },
    {
      "citation_id": "3",
      "title": "Cross Lingual Speech Emotion Recognition: Urdu vs. Western Languages",
      "authors": [
        "Siddique Latif",
        "Adnan Qayyum",
        "Muhammad Usman",
        "Junaid Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)",
      "doi": "10.1109/FIT.2018.00023"
    },
    {
      "citation_id": "4",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "5",
      "title": "Toronto emotional speech set (TESS)",
      "authors": [
        "Kathleen Pichora-Fuller",
        "Kate Dupuis"
      ],
      "year": "2020",
      "venue": "Toronto emotional speech set (TESS)",
      "doi": "10.5683/SP2/E8H2MF"
    },
    {
      "citation_id": "6",
      "title": "Emotion: Theory, Research, and Experience",
      "authors": [
        "Robert Plutchik",
        "Henry Kellerman"
      ],
      "year": "1980",
      "venue": "Emotion: Theory, Research, and Experience"
    },
    {
      "citation_id": "7",
      "title": "Speech Emotion Recognition: Two Decades in a Nutshell, Benchmarks, and Ongoing Trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Commun. ACM",
      "doi": "10.1145/3129340"
    }
  ]
}