{
  "paper_id": "2103.03700v1",
  "title": "Analyzing The Influence Of Dataset Composition For Emotion Recognition",
  "published": "2021-03-05T14:20:59Z",
  "authors": [
    "A. Sutherland",
    "S. Magg",
    "C. Weber",
    "S. Wermter"
  ],
  "keywords": [
    "such as \"monologue\" and \"acting\"",
    "with each video being split into utterances."
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognizing emotions from text in multimodal architectures has yielded promising results, surpassing video and audio modalities under certain circumstances. However, the method by which multimodal data is collected can be significant for recognizing emotional features in language. In this paper, we address the influence data collection methodology has on two multimodal emotion recognition datasets, the IEMOCAP dataset and the OMG-Emotion Behavior dataset, by analyzing textual dataset compositions and emotion recognition accuracy. Experiments with the full IEMOCAP dataset indicate that the composition negatively influences generalization performance when compared to the OMG-Emotion Behavior dataset. We conclude by discussing the impact this may have on HRI experiments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "One of the key components of affective computing is the ability to recognize emotion features in humans. Numerous approaches focus on combining features from multiple input modalities  [1] . The most common modalities are centred on features collected from video and audio stimuli since these features are the easiest to collect and label. Recent experiments on multimodal emotion recognition have made prolific use of features of language and text. Poria et al.  [4]  in particular have shown impressive results using CNNs when combining text with other modalities on experimental data, however, little attention has been spent on how and why textual features are able to perform so well in a multimodal context when compared to visual and acoustic features.\n\nWe address this issue by analyzing how the composition of the IEMOCAP dataset influences neuron activation during classification and show how current usage of the IEMOCAP manifests in visible over-fitting. Experimental results indicate that a network pretrained on the IEMOCAP barely performs as well as a network pretrained on the OMG-Emotion Behavior dataset  [7]  when attempting to classify the other dataset, in spite of being a larger dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Dataset Descriptions",
      "text": "We make use of the IEMOCAP dataset  [2]  and the OMG Emotion-Behavior dataset  [7] . The IEMOCAP dataset consists of videos containing either scripted or improvised utterances. Videos are recorded in different sessions, with different actors in each session. We merge the \"Happy\" and \"Excited\" data samples in the IEMOCAP in order to be able to emulate previous results from Poria et al.  [4]  and {sutherland, magg, weber, wermter}@informatik.uni-hamburg.de The OMG Emotion-Behavior dataset  [7]  consists of videos labelled through crowd-sourcing. Each video is annotated within the context of a longer clip, with both categorical and continuous emotion labels. Categorical labels include Anger, Neutral, Happy, Sad, Surprise, Fear, and Disgust. We use 4656 samples consisting of 639 Sad, 665 Anger, 1794 Neutral, and 1558 Happy samples to compare a network's performance on the IEMOCAP for the same emotions. Videos were selected using a web-crawler that searched for Youtube videos, with keywords such as \"monologue\" and \"acting\", with each video being split into utterances.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Experimental Dataset Analyses",
      "text": "This experiment determines the influence of the IEMO-CAP dataset construction method on emotion recognition from text. Previous works have found emotion features at both word  [4]  and semantic levels  [3] . We use the pretrained Google News Word Embedding  [6]  and a semantic embedding of size 50 trained on frames, from SEMAFOR  [5] , unimodally and multimodally as input to a 1D-CNN architecture. The architecture has channels for each input with a dropout of 0.2, a temporal convolution layer with a kernel size of 3, a stride of 1, and a filter size of 150, a global max pooling layer, a penultimate fully connected layer of size 32 and a final softmax layer of size 4. Late fusion between inputs is performed via concatenating max pooling outputs before feeding them to the fully connected layer. Classification accuracies from 10-fold cross-validations with 8-1-1 splits can be seen in Table  I",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iv. Results & Discussion",
      "text": "In Table  I  we see the results of emotion classification accuracy for input feature combinations. Clearly word embeddings are the primary contributor to classification results and that the network attains higher accuracy on the IEMOCAP as opposed to the OMG. Table  II  shows results of pretraining the network on one dataset and classifying samples from the other. We see that the full IEMOCAP performs equal to or worse than the OMG and IEMOCAP improvised samples when generalizing, despite the full IEMOCAP's high performance on its own test data.\n\nWe believe this over-performance during testing but underperformance during generalization was due to the subset of scripted values in the IEMOCAP. To support this, in Figure  1 , we visualize the activation values attained when classifying scripted IEMOCAP utterances during the process of 10-fold cross-validation. We see abnormally high neuron activations in the 0.95 to 1 bracket, indicating a high certainty of correctness. The reason for this over-confidence is visualized in Figure  2 , where we show that scripted IEMOCAP utterances have a high number of data-points that partially overlap with others due to sentences being scripted and nearly identical. This is likely the reason for the textual modalities high performance in previously reported multimodal architectures that used the IEMOCAP dataset.\n\nIn summary, we have shown a reason for textual modalities high performance in previous works using the IEMOCAP in this manner. This over-fitting will also influence the understanding of natural language in applications, including HRI scenarios. Robot agents could place far greater salience in specific words than is desirable, such as the word \"beast\" in the IEMOCAP only occurring in angry data-points, leading to angry prone classifications. In future work, we suggest that researchers proceed with caution when selecting and applying datasets, as incorrect training procedures can lead to illogical and unreliable behaviour from models.",
      "page_start": 1,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Scripted utterance softmax activation frequencies predicted by",
      "page": 2
    },
    {
      "caption": "Figure 2: , where we show that scripted IEMOCAP utterances",
      "page": 2
    },
    {
      "caption": "Figure 2: The proportion of data that overlaps with some other data point",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Words",
          "IEMOCAP": "67.504(1.436)",
          "IEMOCAPImprov.": "62.738(2.490)",
          "OMG": "44.200(1.377)"
        },
        {
          "Column_1": "Semantics",
          "IEMOCAP": "52.672(2.306)",
          "IEMOCAPImprov.": "47.330(3.512)",
          "OMG": "39.371(1.337)"
        },
        {
          "Column_1": "Fusion",
          "IEMOCAP": "67.718(2.235)",
          "IEMOCAPImprov.": "63.075(1.876)",
          "OMG": "43.557(2.165)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "Words",
          "I→O": "34.487(0.862)",
          "IImprov.→O": "35.195(1.174)",
          "O→I": "35.589(0.710)"
        },
        {
          "Column_1": "Semantics",
          "I→O": "30.483(1.152)",
          "IImprov.→O": "33.251(0.766)",
          "O→I": "31.193(1.059)"
        },
        {
          "Column_1": "Fusion",
          "I→O": "35.217(1.636)",
          "IImprov.→O": "35.612(1.023)",
          "O→I": "35.185(0.547)"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "2",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition from text using semantic labels and separable mixture models",
      "authors": [
        "C Wu",
        "Z Chuang",
        "Y Lin"
      ],
      "year": "2006",
      "venue": "ACM transactions on Asian language information processing (TALIP)"
    },
    {
      "citation_id": "4",
      "title": "Convolutional MKL based multimodal emotion recognition and sentiment analysis",
      "authors": [
        "S Poria",
        "I Chaturvedi",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "Data Mining (ICDM), 2016 IEEE 16th International Conference on"
    },
    {
      "citation_id": "5",
      "title": "A probabilistic frame-semantic parser",
      "authors": [
        "D Das",
        "N Schneider",
        "D Chen",
        "N Smith"
      ],
      "year": "2010",
      "venue": "A probabilistic frame-semantic parser"
    },
    {
      "citation_id": "6",
      "title": "Efficient estimation of word representations in vector space",
      "authors": [
        "T Mikolov",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Efficient estimation of word representations in vector space",
      "arxiv": "arXiv:1301.3781"
    },
    {
      "citation_id": "7",
      "title": "The omg-emotion behavior dataset",
      "authors": [
        "P Barros",
        "N Churamani",
        "E Lakomkin",
        "H Siqueira",
        "A Sutherland",
        "S Wermter"
      ],
      "year": "2018",
      "venue": "The omg-emotion behavior dataset",
      "arxiv": "arXiv:1803.05434"
    }
  ]
}