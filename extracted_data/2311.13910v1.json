{
  "paper_id": "2311.13910v1",
  "title": "Dialogue Quality And Emotion Annotations For Customer Support Conversations",
  "published": "2023-11-23T10:56:14Z",
  "authors": [
    "John Mendonça",
    "Patrícia Pereira",
    "Miguel Menezes",
    "Vera Cabarrão",
    "Ana C. Farinha",
    "Helena Moniz",
    "João Paulo Carvalho",
    "Alon Lavie",
    "Isabel Trancoso"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Task-oriented conversational datasets often lack topic variability and linguistic diversity. However, with the advent of Large Language Models (LLMs) pretrained on extensive, multilingual and diverse text data, these limitations seem overcome. Nevertheless, their generalisability to different languages and domains in dialogue applications remains uncertain without benchmarking datasets. This paper presents a holistic annotation approach for emotion and conversational quality in the context of bilingual customer support conversations. By performing annotations that take into consideration the complete instances that compose a conversation, one can form a broader perspective of the dialogue as a whole. Furthermore, it provides a unique and valuable resource for the development of text classification models. To this end, we present benchmarks for Emotion Recognition and Dialogue Quality Estimation and show that further research is needed to leverage these models in a production setting. * Joint first authors. † Work partially conducted as a visiting scholar at CMU. Agent: Delivery usually takes place within 1-7 working days after dispatch but this can vary depending on the couriers availability in your area.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Artificial Intelligence (AI) has evolved to become a ubiquitous technology in our lives. Yet, its performance is limited by the amount of data it is trained on. Therefore, and in order to maximise the rewards of such technology, substantial research and engineering effort has been devoted to collecting and annotating data according to needs and goals.\n\nOne of the main limitations of most task-oriented conversational datasets is their lack of variability. The majority of these datasets are collected in controlled environments where annotators are Table  1 : Adapted example of a portion of a dialogue from the MAIA DE-1 subset, from the point of view of the Agent (which receives and sends messages in English). The customer interacts with the agent in their corresponding language (in this case German). This is achieved by employing Machine Translation on both ends  (DE → EN and EN → DE) .\n\nencouraged to follow specific guidelines, and are limited to a restrictive set of topics, and outcomes (El  Asri et al., 2017; Budzianowski et al., 2018; Rastogi et al., 2020) . This leads to highly struc-tured dialogues that do not accurately reflect genuine conversations. In contrast, customer support conversations provide a broader range of topics and contexts, and are more linguistically diverse  (Lowe et al., 2015) . Furthermore, most datasets are monolingual, resulting in a lack of representation of diverse linguistic and cultural features such as tone and idiomatic expressions  (Gonçalo Oliveira et al., 2022) .\n\nOne approach to equip NLP models with multilingual and diverse domain knowledge capabilities is to leverage LLMs pretrained on extensive amounts of publicly available data  (Conneau et al., 2020; Xue et al., 2021; OpenAI, 2023) . However, lacking benchmarking dialogue datasets, it is not clear these models, applied to dialogue, are able to fully generalise to other languages and/or domains, even if other dimensions of variability remain unchanged.\n\nThis paper builds upon the original MAIA dataset release by adding extensive annotations of emotion and dialogue quality at different granularity levels, thus allowing a holistic approach at understanding the dynamics of conversations in the context of customer support. The MAIA dataset is a collection of genuine bilingual customer support conversations initially released as a challenge dataset for the WMT Chat sharedtask  (Farinha et al., 2022) . In these conversations, which are powered by Machine Translation, the agent communicates with the customer exclusively in English, whereas the customer interacts with the agent exclusively in their native language. Our annotations cover 612 dialogues accounting for around 25k sentences, covering diverse topics, ranging from account registration issues, payment and delivery clarifications and aftersale services. Languages includes German (DE), Brazilian-Portuguese (PT_BR) and European Portuguese (PT_PT).\n\nWe argue that the MAIA dataset and the accompanying annotations have unique value in the field of customer support and conversational agents. The comprehensive annotations conducted enable the analysis of the relations between several dialogue sub-qualities and emotion. Furthermore, they can be used as a training and benchmark dataset for text classification in these distinctive settings. For instance, one could leverage this dataset for the construction of dialogue systems that support customer-agent interaction processes. Classifica-tion models trained on this data could assist customer service agents (human or machine) by measuring customer emotions and dialogue qualities in real-time and provide the agent with feedback on the fluidity and success of the dialog.\n\nTo kick-start this research, this paper provides benchmarks for Emotion Recognition and Dialogue Quality Estimation. Results show that existing models are not strong enough to perform on par with other benchmarks, indicating significant future work research will be required to reduce this performance gap.\n\nIn summary, the primary contributions of this work are as follow: The paper is structured as follows: Section 2 provides a brief literature review on task-oriented dialogues and their annotations. In Section 3, the MAIA dataset construction pipeline is presented, including the anonymization and annotation steps. The dataset is formally presented in Section 4, delving into the uniqueness of the dataset and its contributions to research. Existing AI-powered approaches for customer support chat such as Emotion Recognition in Conversations and Dialogue Evaluation are benchmarked in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Task-Oriented Dialogue Datasets",
      "text": "Perhaps the most well known open-source customer support datasets are TweetSumm  (Feigenblat et al., 2021)  and the Ubuntu Dialogue Corpus  (Lowe et al., 2015) . In both datasets, the language used is exclusively English. TweetSumm contains customer support interactions between customers and companies crawled from Twitter, whereas Ubuntu extracts its dialogues from the Ubuntu chat logs. The main difference between the Ubuntu dataset and TweetSum is the fact the former is constrained by the nature of the platform itself, typically resulting in limited turn interactions where the agent inevitably steers the customer to a dedicated costumer service chat platform. The Ubuntu dataset, similarly to MAIA, does not have this limitation and consists of live multi-turn dyadic conversations. However, unlike Ubuntu, the MAIA dataset contains customer support conversations of 4 different products and companies, where the agent is a representative of the company. This contrasts with Ubuntu, where the participant offering support is typically an experienced user without any official affiliation with Ubuntu. As such, the conversational dynamics between the two datasets are quite different, with the MAIA dataset showing more diverse emotions.\n\nOther relevant public resources of task-oriented dialogue corpora include the MultiWoz and associated datasets  (Budzianowski et al., 2018) . These datasets are frequently used in the context of taskoriented dialogue, where an agent assists a customer in well defined tasks such as reservations. Unlike the MAIA dataset, the interactions are collected using English speaking crowdworkers, lacking representation of other languages. Additionally, the strict guidelines result in \"sterile\" and structured interactions that lack complexity known to real-world customer support interactions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dialogue Annotations",
      "text": "One of the most widely used dialogue benchmark datasets with emotion annotations is DailyDialog  (Li et al., 2017) , built from websites used to practice English and labelled with the six Ekman's basic emotions  (Ekman, 1999) . In the realm of customer support,  Herzig et al. (2016)  collected and annotated data in terms of emotions from two North America based customer support Twitter accounts. A particularity of this work is that a different set of emotion classes was used for the agent and customer. Furthermore, annotators were asked to indicate the intensity of each possible emotion, allowing for a multi-class setting.\n\nWith respect to quality annotations, the goal of most human annotation work is to evaluate dialogue systems or to validate proposed automated metrics. As such, two approaches are typically employed: annotators either interact with the system in a live setting and rate it, or evaluate existing responses given a context which was fed to the system. In the context of task-oriented dialogue, annotating Task Success  (Walker et al., 1997) , User Satisfaction and/or Emotion  (Schmitt et al., 2012)  are the norm. However, for open-domain dialogue, the focus has been mostly on annotating system responses on several notions of quality  (See et al., 2019; Mehri and Eskenazi, 2020) , since these dialogues are open in nature. To the best of our knowledge, this work is the first one to provide human judgements of customer support conversations with both task-oriented and open domain dialogue quality annotations at the turn and dialogue-level.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Processing And Annotations",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Collection And Anonymization",
      "text": "The conversations that compose this corpus are extracted from the original WMT22 Chat sharedtask dataset  (Farinha et al., 2022) . It consists of dialogues obtained from companies that provide customer support and that gave written consent to use their data for research purposes 2  . This was achieved by using a mix of proprietary anonymization tools and human annotations was used to anonymize all PII (Personally Identifiable Information) from the data 3  .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Annotations",
      "text": "The annotations were conducted by expert linguists in the given language. A single annotator for each language was used to fully annotate the dataset. Given its structure, we annotated the dataset along three dimensions: Sentence level: corresponding to a single message; Turn level: one or more sentences sent by one of the participants within a given time frame. Dialogue level: a succession of turns between the customer and agent denoting the full conversation. Considering dialogues are collaborative acts between speakers, we annotated data from both participants, customer and agent. This allowed us to evaluate the interaction as a whole and understand how one's action may impact the following response and how that affects the outcome of the conversation. A fully annotated dialogue is presented in Appendix B.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sentence Level Evaluation",
      "text": "The metrics used to assess each sentence are as follows:\n\nThe Correctness metric was expressed resorting to three different scores measuring the sentence fluency. A score of 0 applies to a sentence indicated ungrammaticalities at several levels, both in terms of structure and in terms of orthography, originating a sentence that is difficult to understand. A score of 1 indicates that the analysed sentence contains minor mistakes but still remains fully understandable. A score of 2 was used when the sentence showed no mistakes and was fully understandable and coherent.\n\nThe Templated metric measured the type of sentence. For each sentence, a score of 0 was given for non-templated sentences, and a score of 1 for templated sentences. Note that by templated sentences we refer to predefined scripts used by customer support agents.\n\nThe Engagement metric was also expressed as one of two scores, measuring the level of engagement from both conversation parties. A score of 0 indicates a lack of engagement, whereas with a score 1 the participant was fully engaged in the conversation.\n\nBesides the above-mentioned metrics, we also found to be reasonable to measure real emotions that usually go hand in hand within a customer support scenario. Following the previous strategy, the assessment was provided at a sentence-level, identifying the emotions conveyed by each sentence. The set of emotions used are as follows: Happiness; Empathy; Neutral; Disappointment; Confusion; Frustration; Anger; and Anxiety. We selected these emotions because upon analyzing the dataset we observed that these were the most common emotions displayed from a pool of several customer support emotions. With regards to empathy, it is a crucial emotion to analyze to measure agent performance. In terms of emotion annotation, and since a situation often triggers multiple emotions, annotators had the opportunity to select multiple emotions for a single sentence, ranking from the main emotion expressed to the others that are less evident. For example, a customer can be both disappointed and frustrated.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Turn Level Evaluation",
      "text": "The annotation process was designed to measure the interaction between participants within a dialogue. Since dialogues are a multi-tier architecture structure engineered not just around sentences but also around turns, it was necessary to account for these compositional properties. An analysis at the turn level allowed us to understand the overall mood and attitude of the turn-taker w.r.t what was previously stated by the other dialogue participant, at any given stage of the conversation. As a metric deeply dependent of the previously sentences, it is important to note that the initial turns were considered as non-evaluatable, since their function within the dialogue is to set the tone and the context that allow the newly started conversation to flow. The set of categories used for the turn taking evaluation were as follow:\n\nThe category Understanding measured how well the participant was able to understand the message from the other dialogue participant, with a score of 0 meaning the understandability was somehow compromised, and the score 1 meaning understandability was reached.\n\nSensibleness measured the response appropriateness to what was previously stated by the other dialogue shareholder. A score of 0 means the response did not follow what was previously stated or requested, indicating that the current turn-taker ignored the conversation history. Conversely, a score of 1 indicates that the turn-taker acknowledged the conversation history and provided a suitable response.\n\nPoliteness measured the courtesy level of each participant towards one another. A score of 0 shows disrespect, discourtesy inter alia concerning the  remaining participant; score 1 shows the participant was at worst civil and respectful.\n\nThe category Interaction Quality (IQ) was adapted from  Schmitt and Ultes (2015)  and scores the turn-taker disposition regarding the previous turn issued by the other dialogue part-taker. This category metric ranges from 1 to 5. With a score of 1, the turn-taker found the previous response to be extremely unsatisfactory; score 2, unsatisfactory; score 3, somewhat unsatisfactory; score 4, somewhat satisfactory; score 5, satisfactory.\n\nWith the above metrics we were able to have a better outlook of the different types of customers and agents, distinguishing behaviour and attitude patterns within a customer support dialogue.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dialogue Level Evaluation",
      "text": "Lastly, we focused on the full dialogue, measuring the conversation in terms of:\n\nDropped Conversation responds to the questions: \"Was the conversation terminated without a conclusion?\" and/or \"Was the conversation dropped?\". A score 0 means the conversation reached its end. Conversely, a score of 1 means a dropped conversation, i.e., the conversation did not reach its end, implying that the issue was not resolved.\n\nTask Success dwells with the success of the interaction. This category responds to the following question: \"Was the agent able to fulfil the customer's request?\" The dialogue success was measured according to the following scores:\n\n• A score of 1 means the agent failed to understand and fulfil the customer's request; • A score of 2 means the agent understood the request but failed to satisfy it in any way; • A score of 3 means the agent understood the customer's request and either partially satisfies the request or provided information on how the request can be fulfilled; • A score of 4 means the agent understood and satisfied the customer request, but provided more information than what the customer requested or took unnecessary turns before meeting the request; • A score of 5 means the agent understood and satisfied the customer request completely and efficiently.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Interannotator Agreement (Iaa)",
      "text": "Since all annotators were also fluent in European Portuguese (PT-PT), we conducted a trial annotation using 10 dialogues of the corresponding subset to gauge inter-annotator agreement between the annotations. The observed agreement is presented in Table  2   4  . Of note, we observe that IQ and Task Success are the annotations that have the lowest agreement, which is expected given the highly subjective nature of these annotations and the fact they are annotated using a Likert Scale. By mapping these annotations to a binary decision (joining the last 2 and 3 ranks together for IQ and Task Success, respectively), the (full/partial) agreement increases to (87.4/12.6) and (80.00/20.00) for IQ and Task Success, respectively.   The dataset consists of a total of 612 dialogues, split into 5 subsets of different languages and/or companies (identified using a unique integer). Table 3 presents the statistical information of the dataset and corresponding subsets. Additional statistics on the quality annotations is presented in Table  4 , with Figure  1  illustrating the emotion distribution.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Structure",
      "text": "Whilst the majority of dialogues follows a typical turn-taking approach, we find some instances where one of the participants breaks the flow of the conversation. This occurs when the next turn taker does not respond within an appropriate time frame (according to the other side). This is especially true at the end of the dialogues, where the customer terminates the conversation abruptly, irrespective of whether the issue was resolved. Additionally, these interactions are aided by automated system that responds on behalf of the agent: (1) when the customer doesn't reply within a given time frame, resulting in the system reminding the customer of the ongoing customer support interaction before terminating the conversation; (2) at the end of the dialogues, requesting customer satisfaction survey and providing additional steps, if applicable. Emotion correlates with interaction quality and dialogue success. We hypothesise a positive correlation between emotion and dialogue success levels since the emotions of the interlocutors are related with the outcome of the experiment. This can be observed in Figure  2 , where we note a rise in empathy and happiness, together with a decrease in negative emotions. Simultaneously, a positive correlation between emotion and Interaction Quality (IQ) should also be observed. For each turn, we mapped the emotions into a 3 class sentiment (-1,0,1) and report a Pearson and Spearman correlation of 0.4136 and 0.5494, respectively.  nature of the dialogue itself, which generally involves the agent dictating steps and/or terms and conditions pertaining to the product, which are verbatim of existing content.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Observations And Discussion",
      "text": "Low quality interactions can be recovered successfully. Figure  4  presents a use case where a decrease of IQ is observed and rectified by the agent, resulting in a positive outcome: Around turn 21 we observe a large degradation in IQ which is paired with frustration. This is a result of the responses by the agent being templated and ineffective to solve the issue at hand. This is further exacerbated due to the lack of understanding between the participants, which is eventually resolved, increasing the quality of the interaction.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Benchmark Evaluation",
      "text": "Given the focus of the annotation work was on emotions and dialogue quality, in this section we evaluate existing mainstream approaches for emotion recognition and automatic dialogue evaluation.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Emotion Recognition In Conversations",
      "text": "State-of-the-art approaches for Emotion Recognition in Conversations (ERC) produce representations of each sentence using pretrained language models and then model the interactions between these representations with classification modules. Approaches such as leveraging conversational context or speaker specific modelling typically resort to architectures such as gated and graph neural networks  (Poria et al., 2019) .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experiments",
      "text": "For our benchmark, we finetuned a pretrained Encoder model, more specifically XLM-RoBERTa  (Conneau et al., 2020) . We conducted train/dev/test splits at the dialogue level for each subset, employing a distribution of 70%/10%/20%, respectively, and ensuring the original distribution of emotion classes on all splits whenever possible. During training and evaluation, we used the source text while considering only the primary emotion labels, disregarding secondary emotion annotations. Performance is evaluated using Macro, Micro and individual emotion label F1 scores across all languages and the whole dataset. Additional training details are available in Appendix A.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results",
      "text": "Results for this benchmark are presented in Table  5 . We report a Macro-f1 score of 47.98 for the whole MAIA dataset. This result is within the performance of typical ERC models for other datasets that also have an imbalanced class distribution. The most represented Neutral class has a high F1 score across all subsets, heavily influencing the Micro-F1 score. Other well represented classes such as Empathy and Anxiety also have high F-scores, whereas minority classes have lower scores. In some subsets, individual emotion labels present very low to null F1 scores, again, a result of the class imbalance issues. In fact, due to the limited number of examples for these emotions in some subsets, a handful of missclassifications yield single digit F1 scores.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Automatic Dialogue Evaluation",
      "text": "Most competitive metrics for turn-level dialogue evaluation leverage pretrained Encoder models that are finetuned using well-defined self-supervised tasks  (Yeh et al., 2021; Zhang et al., 2021) . These approaches generate synthetically negative samples from the original dialogue data, thereby circumventing limitations w.r.t the lack of quality annotated dialogues. However, it isn't clear these approaches extend to task-oriented dialogues and/or Multilingual models, since dialogue data is exclusively open-domain and in English. As such, the MAIA dataset can be used as a benchmark to study these characteristics.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Experiments",
      "text": "Similar to approaches mentioned above, we finetuned XLM-RoBERTa for ENG (Engagement) using the ENDEX data  (Xu et al., 2022) ; and VSP (Valid Sentence Prediction) and NSP (Next Sentence Prediction) using self-supervised data generated from DailyDialog  (Li et al., 2017) . VSP is mostly concerned with the syntactic fluency of the response, which maps to Correctness and Templated; NSP evaluates textual entailment, which maps to Understanding and Sensibleness; Finally, since we have Engagement annotations, the evaluation of the ENG submetric is straightforward. The mapping between these submetrics and the remaining annotations is less obvious, but most evaluation frameworks that leverage these submetrics have shown positive correlations with quality aspects that do not map to the submetrics  (Yeh et al., 2021) .\n\nFor this task, we mapped existing sentence-level annotations to turn-level by selecting the minimum of the given turn. For simplicity, we report the Balanced Accuracy Score (BAS), which in this case corresponds to the average recall obtained on the positive (1) and negative (0) classes. The BAS for outputting a single class is 0.5. As such, we consider always outputting the majority class as the baseline. For Correctness, we considered a turn to be positive when all sentences have a score higher than 0; for IQ, only turns with a score of 4 or 5 are labelled positive. We indicate results for both languages, i.e, the context-response pairs from the point of view of the Customer (CST) (original language, with agent text translated) and the Agent (AGT) (in English, customer text translated). Note that, in this case, we conducted zero-shot inference on customer languages using models finetuned only on English data. Additional details available in A.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Results",
      "text": "For ease of reading, we aggregate the results of all subsets and report the BAS in Table  6 . It is clear some models are best suited to predict only some subqualities. However, despite ENG being trained on engagement data, it underperforms NSP on the Engagement annotation. This may be related to the training data itself: Engagement in the context of open-domain dialogue is different than in customer support. Further, we observe that most models only slightly outperform just predicting the positive class. This means typical approaches for automatic subquality prediction are insufficient to adequately predict low quality responses on the MAIA dataset.\n\nComparing the results for AGT against CST we note that the trained models do not consistently outperform on a given language. This may indicate finetuning a multilingual encoder with English dialogue data only achieves reasonable results in a multilingual setting. However, it is important to point out (1) that the agent converses in English;\n\n(2) the result that is most sensible to linguistic differences is VSP for Correctness (since it looks at the syntax), and here we see that the model underperforms for the other languages.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusions",
      "text": "This paper presents a comprehensive emotion and dialogue quality annotation for the MAIA dataset, a collection of genuine bilingual customer support conversations. All in all, we annotate 612 dialogues amounting to over 24k sentences. Besides allowing for an opportunity to study the dynamics of Machine Translation aided customer support conversations, it also provides a novel opportunity to benchmark and explore applications of existing and future NLP models applied to dialogue.\n\nResults on the different benchmarks indicate there is still room for improving existing models. LLMs such as  GPT-4 (OpenAI, 2023)  show impressive classification and generation capabilities, and may prove useful in augmenting existing customer support datasets to new languages and tasks. These in turn can be used to build data-driven classifiers or end-to-end conversational agents that are robust to new languages and domains.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations",
      "text": "Perhaps the main limitation of this work concerns the lack of several annotators for each subset. Even with well defined guidelines, individual biases may affect the annotations, especially for dialogue quality as it is highly subjective  (Smith et al., 2022) . By having several annotators evaluate the conversations, one could've leveraged \"the wisdom of the crowd\", but this approach also comes with its own limitations  (Jain, 2010) . Ideally we would've employed several expert annotators, but were only able to recruit a single expert for each language. In any case, we conducted a trial annotation where all annotators participated and report moderate to strong agreement on a subset of the dataset.\n\nAnother limitation pertains to the dataset itself. Despite being structured and evaluated as a dyadic interaction, the actual conversations may not follow this structure. For instance, whenever one of the participants takes to long to respond, the other may follow-up on its original turn with a reminder.\n\nGiven we do not have access to this temporal information, these sentences were lumped together into a single turn. Also pertaining to metadata information is the lack of the original customer support guidelines. This makes the Templated annotation a subjective observation from the point of view of the customer. However, since we are framing this annotation from a quality perspective, we believe our annotation accurately reflects the perception of quality from the P.O.V of the customer. et al.  (2023) . In detail, a token representing the speaker was added for each turn, and a history length of 3 turns was used. We applied a regression head consisting of a 2-layer MLP with a hidden size of 1024 and a hyperbolic tangent function as activation for prediction. A learning rate of 3e-6 for 3 epochs using a batch size of 16 was used. Evaluation was conducted every 10,000 steps. The best performing model on the evaluation set was selected for testing. It seems that our system did not process a request from you before your cycle renewed which is why you were charged.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B Example Dialogue",
      "text": "Parece que nosso sistema não processou uma solicitação de você antes da renovação do seu ciclo, e é por isso que você foi cobrado.",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Emotion distribution of the MAIA dataset.",
      "page": 6
    },
    {
      "caption": "Figure 1: illustrating the emotion",
      "page": 6
    },
    {
      "caption": "Figure 2: Proportion of non-neutral Emotion Rates",
      "page": 6
    },
    {
      "caption": "Figure 2: , where we note a rise in",
      "page": 6
    },
    {
      "caption": "Figure 3: Pairwise Pearson correlation matrix of sen-",
      "page": 6
    },
    {
      "caption": "Figure 3: , subqualities such as Understanding,",
      "page": 6
    },
    {
      "caption": "Figure 4: Evolution of the annotation Interaction Quality over a dialogue, together with relevant sentence and turn",
      "page": 7
    },
    {
      "caption": "Figure 4: presents a use case where a de-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 4: , with Figure 1 illustrating the emotion",
      "data": [
        {
          "empathy\nhappiness\ndisapointment": "confusion\nfrustration\nanger\nanxiety"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: It is clear",
      "data": [
        {
          "Model": "VSP\nNSP\nENG",
          "Correctness\nTemplated\nEngagement\nUnderstanding\nSensibleness\nPoliteness\nIQ": "0.6361\n0.6541\n0.5307\n0.4667\n0.5112\n0.4943\n0.5091\n0.5083\n0.5734\n0.5831\n0.5603\n0.5444\n0.4645\n0.4842\n0.5205\n0.5795\n0.4545\n0.5374\n0.5484\n0.5510\n0.4740"
        },
        {
          "Model": "VSP\nNSP\nENG",
          "Correctness\nTemplated\nEngagement\nUnderstanding\nSensibleness\nPoliteness\nIQ": "0.7061\n0.6073\n0.5083\n0.4601\n0.4648\n0.4973\n0.5165\n0.5182\n0.5657\n0.5864\n0.5821\n0.5850\n0.4888\n0.5029\n0.5443\n0.5794\n0.4503\n0.5514\n0.5548\n0.5756\n0.4742"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Text": "SRC",
          "Sentence": "Emo",
          "Turn": "Und\nSen\nIQ\nPol",
          "Dialogue": "DC\nTS"
        },
        {
          "Text": "Oii, fui cobrada por um plano\nq n estou usando, e estou\nsolicitando retorno\nGood day :)\nThanks for reaching out and\nI’m sorry for any confusion here!\n#PRS_ORG# memberships\nrenew each month unless you\nrequest a cancellation via one\nof our agents.\nIt seems that our system did not\nprocess a request from you before\nyour cycle renewed which is why\nyou were charged.\nUnfortunately we are not able to\nrefund #PRS_ORG# memberships\nretroactively.\nOur refund policy is stated in our\nHelp Center here for further reference\n#URL#\nE tem como eu pssar a assinatura pra\noutra pessoa???\nPq meu dinheiro n é de graça pra ser\ngasto a toa\nWe apologize for the frustration here.\nAs an exception given the circumstances,\nI refunded your most recent membership\ncharged, and I cancelled your membership\nimmediately.\nYou can expect this refund to arrive in 5-7\ndays depending on your bank/carrier, and\nyou won’t be charged again moving forward.\nIn the meantime, you can view the refunded\ncharge on the billing page in your Account Settings.\nPlease let me know if you have any other\nquestions or if there is anything else that\nI can help with.\nMuito obrigada!!\nMy pleasure.\nIs there anything else I can help with?\nIt seems like you’re busy right now, so\nI’m going to close out the chat.\nIf you have any other questions or want\nto get back in contact with us, you can\ndo so here: #URL#\nHave a great day!",
          "Sentence": "Frustratiom\nNeutral\nNeutral\nNeutral\nNeutral\nNeutral\nNeutral\nFrustration\nFrustration\nNeutral\nNeutral\nNeutral\nNeutral\nNeutral\nNeutral\nNeutral\nNeutral\nNeutral\nNeutral\nNeutral",
          "Turn": "NA\nNA\nNA\n1\n1\n1\n5\n1\n1\n1\n2\n0\n1\n1\n5\n1\n1\n1\n5\n1\n1\n1\n5\n1",
          "Dialogue": "1\n5"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "MultiWOZ -a largescale multi-domain Wizard-of-Oz dataset for taskoriented dialogue modelling",
      "authors": [
        "Paweł Budzianowski",
        "Tsung-Hsien Wen",
        "Bo-Hsiang Tseng",
        "Iñigo Casanueva",
        "Stefan Ultes",
        "Milica Osman Ramadan",
        "Gašić"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "doi": "10.18653/v1/D18-1547"
    },
    {
      "citation_id": "2",
      "title": "A coefficient of agreement for nominal scales. Educational and Psychological Measurement",
      "authors": [
        "Jacob Cohen"
      ],
      "year": "1960",
      "venue": "A coefficient of agreement for nominal scales. Educational and Psychological Measurement",
      "doi": "10.1177/001316446002000104"
    },
    {
      "citation_id": "3",
      "title": "Unsupervised cross-lingual representation learning at scale",
      "authors": [
        "Alexis Conneau",
        "Kartikay Khandelwal",
        "Naman Goyal",
        "Vishrav Chaudhary",
        "Guillaume Wenzek",
        "Francisco Guzmán",
        "Edouard Grave",
        "Myle Ott",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.747"
    },
    {
      "citation_id": "4",
      "title": "Basic emotions. Handbook of cognition and emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Basic emotions. Handbook of cognition and emotion"
    },
    {
      "citation_id": "5",
      "title": "Frames: a corpus for adding memory to goal-oriented dialogue systems",
      "authors": [
        "Layla Asri",
        "Hannes Schulz",
        "Shikhar Sharma",
        "Jeremie Zumer",
        "Justin Harris",
        "Emery Fine",
        "Rahul Mehrotra",
        "Kaheer Suleman"
      ],
      "year": "2017",
      "venue": "Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue",
      "doi": "10.18653/v1/W17-5526"
    },
    {
      "citation_id": "6",
      "title": "Findings of the WMT 2022 shared task on chat translation",
      "authors": [
        "M Ana C Farinha",
        "Marianna Amin Farajian",
        "Patrick Buchicchio",
        "Fernandes",
        "G José",
        "Helena De Souza",
        "Moniz",
        "F André",
        "Martins"
      ],
      "year": "2022",
      "venue": "Proceedings of the Seventh Conference on Machine Translation (WMT)"
    },
    {
      "citation_id": "7",
      "title": "TWEETSUMM -a dialog summarization dataset for customer service",
      "authors": [
        "Guy Feigenblat",
        "Chulaka Gunasekara",
        "Benjamin Sznajder",
        "Sachindra Joshi",
        "David Konopnicki",
        "Ranit Aharonov"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021",
      "doi": "10.18653/v1/2021.findings-emnlp.24"
    },
    {
      "citation_id": "8",
      "title": "Daniel Martins, Catarina Silva, and Ana Alves",
      "authors": [
        "Gonçalo Hugo",
        "Patrícia Oliveira",
        "Ferreira"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "9",
      "title": "Classifying emotions in customer support dialogues in social media",
      "authors": [
        "Jonathan Herzig",
        "Guy Feigenblat",
        "Michal Shmueli-Scheuer",
        "David Konopnicki",
        "Anat Rafaeli",
        "Daniel Altman",
        "David Spivak"
      ],
      "year": "2016",
      "venue": "Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
      "doi": "10.18653/v1/W16-3609"
    },
    {
      "citation_id": "10",
      "title": "Investigation of governance mechanisms for crowdsourcing initiatives",
      "authors": [
        "Radhika Jain"
      ],
      "year": "2010",
      "venue": "Sustainable IT Collaboration Around the Globe. 16th Americas Conference on Information Systems, AMCIS 2010"
    },
    {
      "citation_id": "11",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "12",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Yanran Li",
        "Hui Su",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Ziqiang Cao",
        "Shuzi Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "The Ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems",
      "authors": [
        "Ryan Lowe",
        "Nissan Pow",
        "Iulian Serban",
        "Joelle Pineau"
      ],
      "year": "2015",
      "venue": "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue",
      "doi": "10.18653/v1/W15-4640"
    },
    {
      "citation_id": "14",
      "title": "Unsupervised evaluation of interactive dialog with DialoGPT",
      "authors": [
        "Shikib Mehri",
        "Maxine Eskenazi"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "15",
      "title": "Towards multilingual automatic open-domain dialogue evaluation",
      "authors": [
        "John Mendonça",
        "Alon Lavie",
        "Isabel Trancoso"
      ],
      "year": "2023",
      "venue": "Proceedings of the 24th Annual Meeting of the Special Interest Group on Discourse and Dialogue"
    },
    {
      "citation_id": "16",
      "title": "Gpt-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report"
    },
    {
      "citation_id": "17",
      "title": "Deconstruct to reconstruct a configurable evaluation metric for open-domain dialogue systems",
      "authors": [
        "Vitou Phy",
        "Yang Zhao",
        "Akiko Aizawa"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.368"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "Soujanya Poria",
        "Navonil Majumder",
        "Rada Mihalcea",
        "Eduard Hovy"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "19",
      "title": "Towards scalable multi-domain conversational agents: The schema-guided dialogue dataset",
      "authors": [
        "Abhinav Rastogi",
        "Xiaoxue Zang",
        "Srinivas Sunkara",
        "Raghav Gupta",
        "Pranav Khaitan"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
      "doi": "10.1609/aaai.v34i05.6394"
    },
    {
      "citation_id": "20",
      "title": "Interaction quality: Assessing the quality of ongoing sporiaken dialog interaction by experts-and how it relates to user satisfaction",
      "authors": [
        "Alexander Schmitt",
        "Stefan Ultes"
      ],
      "year": "2015",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2015.06.003"
    },
    {
      "citation_id": "21",
      "title": "A parameterized and annotated spoken dialog corpus of the CMU let's go bus information system",
      "authors": [
        "Alexander Schmitt",
        "Stefan Ultes",
        "Wolfgang Minker"
      ],
      "year": "2012",
      "venue": "Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12)"
    },
    {
      "citation_id": "22",
      "title": "What makes a good conversation? how controllable attributes affect human judgments",
      "authors": [
        "Abigail See",
        "Stephen Roller",
        "Douwe Kiela",
        "Jason Weston"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1170"
    },
    {
      "citation_id": "23",
      "title": "Learning an unreferenced metric for online dialogue evaluation",
      "authors": [
        "Koustuv Sinha",
        "Prasanna Parthasarathi",
        "Jasmine Wang",
        "Ryan Lowe",
        "William Hamilton",
        "Joelle Pineau"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.acl-main.220"
    },
    {
      "citation_id": "24",
      "title": "Human evaluation of conversations is an open problem: comparing the sensitivity of various methods for evaluating dialogue agents",
      "authors": [
        "Eric Smith",
        "Orion Hsu",
        "Rebecca Qian",
        "Stephen Roller",
        "Y-Lan Boureau",
        "Jason Weston"
      ],
      "year": "2022",
      "venue": "Proceedings of the 4th Workshop on NLP for Conversational AI",
      "doi": "10.18653/v1/2022.nlp4convai-1.8"
    },
    {
      "citation_id": "25",
      "title": "PARADISE: A framework for evaluating spoken dialogue agents",
      "authors": [
        "Marilyn Walker",
        "Diane Litman",
        "Candace Kamm",
        "Alicia Abella"
      ],
      "year": "1997",
      "venue": "35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the European Chapter",
      "doi": "10.3115/976909.979652"
    },
    {
      "citation_id": "26",
      "title": "En-Dex: Evaluation of dialogue engagingness at scale",
      "authors": [
        "Guangxuan Xu",
        "Ruibo Liu",
        "Fabrice Harel-Canada",
        "Nischal Reddy Chandra",
        "Nanyun Peng"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2022"
    },
    {
      "citation_id": "27",
      "title": "Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer",
      "authors": [
        "Linting Xue",
        "Noah Constant",
        "Adam Roberts",
        "Mihir Kale",
        "Rami Al-Rfou",
        "Aditya Siddhant"
      ],
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/2021.naacl-main.41"
    },
    {
      "citation_id": "28",
      "title": "A comprehensive assessment of dialog evaluation metrics",
      "authors": [
        "Yi-Ting Yeh",
        "Maxine Eskenazi",
        "Shikib Mehri"
      ],
      "year": "2021",
      "venue": "The First Workshop on Evaluations and Assessments of Neural Conversation Systems",
      "doi": "10.18653/v1/2021.eancs-1.3"
    },
    {
      "citation_id": "29",
      "title": "Automatic evaluation and moderation of open-domain dialogue systems",
      "authors": [
        "Chen Zhang",
        "João Sedoc",
        "L Haro",
        "Rafael Banchs",
        "Alexander Rudnicky"
      ],
      "year": "2021",
      "venue": "Automatic evaluation and moderation of open-domain dialogue systems"
    }
  ]
}