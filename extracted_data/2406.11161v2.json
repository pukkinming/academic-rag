{
  "paper_id": "2406.11161v2",
  "title": "Emotion-Llama: Multimodal Emotion Recognition And Reasoning With Instruction Tuning",
  "published": "2024-06-17T03:01:22Z",
  "authors": [
    "Zebang Cheng",
    "Zhi-Qi Cheng",
    "Jun-Yan He",
    "Jingdong Sun",
    "Kai Wang",
    "Yuxiang Lin",
    "Zheng Lian",
    "Xiaojiang Peng",
    "Alexander Hauptmann"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Accurate emotion perception is crucial for various applications, including humancomputer interaction, education, and counseling. However, traditional singlemodality approaches often fail to capture the complexity of real-world emotional expressions, which are inherently multimodal. Moreover, existing Multimodal Large Language Models (MLLMs) face challenges in integrating audio and recognizing subtle facial micro-expressions. To address this, we introduce the MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained annotated samples across diverse emotional categories. This dataset enables models to learn from varied scenarios and generalize to real-world applications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly integrates audio, visual, and textual inputs through emotion-specific encoders. By aligning features into a shared space and employing a modified LLaMA model with instruction tuning, Emotion-LLaMA significantly enhances both emotional recognition and reasoning capabilities. Extensive evaluations show Emotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap (7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI challenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations on DFEW dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion perception plays a vital role in applications such as human-computer interaction  [17] [18] [19] 80] , educational assistance  [43] , and psychological counseling  [7, 42] . While single-modality approaches, including facial expression recognition  [45, 57, 70, 87] , text emotion analysis  [25, 49, 41] , and audio emotion recognition  [31, 39, 47] , have shown effectiveness, real-world emotional data is often multimodal, integrating text, audio, and images.\n\nDespite extensive multimodal fusion methods having achieved promising improvements  [13, 14, 16, 56, 58, 59, 77, 91, 96, 100, 103] , they mainly focus on feature interaction and modality completion, remaining under-explored for knowledge-level interaction which is essential for emotional reasoning of humans. Recently, Multimodal Large Language Models (MLLMs) have excelled in tasks such as visual-language understanding  [50, 65] , visual question answering  [104] , and video understanding  [35, 63, 86] . However, for emotion recognition  [45] , models like GPT-4 with Vision (GPT-4V) still face two main challenges: the inability to process audio and the failure to recognize micro-expressions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "This section presents our proposed Emotion-LLaMA model, which consists of three key components: the MERR dataset construction (Sec. 3.1), the Multimodal Emotion-LLaMA model architecture (Sec. 3.2), and the training procedures (Sec. 3.3).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Merr Dataset Construction",
      "text": "The MERR dataset is constructed through a comprehensive process of emotion annotation in video data, as outlined in Algorithm 1 and Figure  1 . First, human faces are extracted from each video frame using the OpenFace toolkit, which detects and scores Action Units (AUs)  [28, 69]  to identify the frame with the maximum cumulative intensity:\n\nwhere S aui represents the intensity of each AU. These AUs are mapped to facial expression descriptions C ved (Tables  10  and 11 ) to accurately depict facial movements. Next, MiniGPT-v2  3  analyzes the peak frame to extract contextual information C vod , such as activities and environment (Figure  1 ), facilitating the identification of latent emotional elements within the background context. Qwen-Audio  4 processes audio segments to extract nuances in speech and vocal tone, generating emotion-related descriptions C atd . Visual and audio information are concatenated into a raw multimodal description, integrating sensory inputs to enhance the contextual supplementation for lexical subtitles. Lexical subtitles C ls are integrated into the multimodal description, providing textual context that complements the audio and visual data. LLaMA-3  5  refines these annotations by aggregating unimodal descriptions (C ved , C vod , C atd , C ls ) into a detailed multimodal description C md , following instructions and examples in Table  12 . Finally, the comprehensive description C md is used to annotate the peak frame, ensuring the video is annotated with detailed emotional descriptors. The MERR dataset extends the range of emotional categories and annotations beyond those found in existing datasets (Table  15 ). Each sample is annotated with an emotion label and described in terms of its emotional expression. The dataset was initially auto-annotated with coarse-grained labels for 28,618 samples from a large pool of unannotated data using LLaMA-3, and was later refined to include 4,487 samples with fine-grained annotations, carefully selected by experts. Figure  5  shows that, compared to other datasets, MERR encompasses a wider range of emotional categories. More details of MERR dataset construction are provided in the project homepage and the Appendix A.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Emotion-Llama Model",
      "text": "The proposed Multimodal Emotion Large Language Model (Emotion-LLaMA) architecture, depicted in Figure  2 , comprises an audio encoder E aud , a visual encoder E vis , and a multimodal large language Algorithm 1 Multimodal Emotion Annotation Procedure\n\nRequire: Video frames V = f 1 , f 2 , . . . , f n , Audio stream A Ensure: Annotated video with comprehensive emotional descriptors for the peak emotional expression frame 1: Initialize I peak ← 0 2: Initialize F rame peak ← ∅ 3: for each frame f k in V do\n\nwhere ϕ, Ω, and E denote the LLaMA language model  [85] , vision pre-processor, and multimodal encoder, respectively. Ô represents the formatted output text result. The multimodal encoder E consists of audio, vision, and text prompt encoders. Input V ideo is pre-processed to construct the frame sequence V and F rame peak (Sec. 3.1).\n\nMultimodal Prompt Template. To address the intricate needs of emotional understanding, we craft a structured multimodal prompt template incorporating descriptive captions and emotion flags (as detailed in Table  16  and 17 ), directing the LLM to decipher latent correlations between emotional states and corresponding visual or auditory content. The template is denoted as:\n\nMultiview Multimodal Encoder. To capture emotional cues in audio and visual modalities, we leverage the HuBERT  [39]  model as our audio encoder E aud and a multiview visual encoder E vis . HuBERT extracts a comprehensive auditory representation u aud from the input audio signal, exhibiting remarkable performance in emotion recognition tasks.\n\nWe use a vision preprocessor to unify vision modalities, including facial sequences and peak frame extracted from the input video. Three visual encoders E vis = E vis glo , E vis loc , E vis temp are employed to comprehensively extract complementary multi-view visual emotional features:\n\n• Local Encoder: A ViT-structured model pre-trained by the MAE scheme  [82]  extracts static facial expression features. A facial sequence is fed into the local encoder, and the output frame-wise features are fused by average pooling, producing the local visual feature u vis loc = AVG(E vis loc (V )). • Temporal Encoder: A VideoMAE  [84]  model, produces the temporal feature u vis temp = E vis temp (V ) of a facial sequence, learning facial dynamics that indicate emotional states and offering a temporal dynamic view of human emotion.\n\n• Global Encoder: A ViT-structured model, EVA  [32] , initialized with official pre-trained weights, produces the visual feature u vis glo = E vis glo (Frame peak ), capturing not only facial expressions but also background context.\n\nMultimodal Integration and Tokenization. We use the LLaMA tokenizer, employing a byte-pair encoding (BPE) model based on SentencePiece  [48] , to address open vocabulary challenges and facilitate efficient processing of textual inputs. For multimodal emotional reasoning, a modified generate method iteratively selects the most probable tokens, producing contextually appropriate and emotionally nuanced responses.\n\nTo integrate audio and visual features with text tokens, we introduce a linear projection mechanism that transforms these features into a common dimensional space. This involves trainable linear mappings σ, which include σ aud for the audio token, and σ vis glo , σ vis loc , and σ vis temp for the visual tokens. Specifically, we apply σ to convert multimodal feature u into language embedding tokens T : T = σ • u, with u = u aud , u vis glo , u vis loc , u vis temp  (3)  The resulting multimodal tokens T comprise a single audio token ⟨T aud ⟩, three visual tokens ⟨T vis glo ⟩, ⟨T vis loc ⟩, and ⟨T vis temp ⟩, and a sequence of text tokens ⟨T tex 0 ⟩, . . . , ⟨T tex N ⟩. These tokens are fused through the inner cross-attention mechanism of Emotion-LLaMA, enabling it to capture and reason about the emotional content in the multimodal input.\n\nBy employing this linear projection and multimodal token representation, Emotion-LLaMA processes and integrates information from various modalities, leveraging the strengths of the underlying LLaMA model while incorporating essential emotional cues from audio and visual sources. Further details of the Emotion-LLaMA Model are provided in the code repository.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "Training Of Emotion-Llama Model",
      "text": "We design a multi-task learning scheme to simultaneously supervise the model in learning emotional reasoning and recognition. The ground truth output and labels are converted and concatenated as standard text by a formatted template for autoregressive loss calculation  [99] . Iterative random instruction sampling (see Table  16  and 17 for full list of instructions) for emotional reasoning and recognition tasks during training guides the model to develop a comprehensive understanding of emotions. Typically, Emotion-LLaMA is trained in a coarse-to-fine manner, consisting of the Pre-training and Multimodal Instruction Tuning:\n\nStage 1: Pretraining. Initially, the model is trained on 28,618 coarse-grained samples from the MERR dataset. Distinct tasks help the model grasp emotions from multiple perspectives. This phase involves simple descriptions or classifications, facilitating the rapid alignment of multimodal feature tokens (⟨T aud ⟩, ⟨T vis glo ⟩, ⟨T vis loc ⟩, and ⟨T vis temp ⟩) to the word embedding space  [10, 97] . Stage 2: Multimodal Instruction Tuning. The pretrained Emotion-LLaMA model is then refined using fine-grained instructional datasets to enhance its capacity for emotion recognition and reasoning. This stage utilizes multimodal instruction tuning datasets, incorporating 4,487 fine-grained annotated descriptions for comprehensive reasoning from the MERR dataset. The tuning process is extended to diverse sources, including MER2023  [59]  and DFEW  [45] , which feature precisely annotated emotional categories. This phase ensures that the model not only identifies emotions accurately but also understands the underlying context and reasoning behind each emotion. More details are in the code repository and the Appendix B.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "To verify the effectiveness of Emotion-LLaMA, we conducted extensive evaluations across four different datasets: MER2023  [59] , MER2024  [60] , DFEW  [45] , and EMER  [62] . Notably, we utilized the MERR dataset for pre-training the model and then fine-tuned it on target datasets for evaluation.\n\nEmotion Recognition Evaluation. We performed instruction tuning on the MER2023 and DFEW datasets, allowing the model to integrate the emotional knowledge acquired during pretraining. To test the generalizability of our model, we used three datasets: MER2023, MER2024, and DFEW. These datasets are multimodal emotion recognition datasets composed of movie and TV series clips, each annotated with various emotion categories. For fair comparisons, we evaluated Emotion-LLaMA on MER2023-SEMI and MER2024-NOISE using the F1 score. We also compared it with other MLLMs and state-of-the-art (SOTA) methods using unweighted average recall (UAR) and weighted average recall (WAR) on the DFEW dataset. Additionally, we used the average of accuracy and recall scores as evaluation metrics on the MER2024-OV dataset.\n\nEmotion Reasoning Evaluation. The EMER dataset differs from traditional emotion datasets by including emotion trigger labels, such as facial micro-expressions, tone of speech, and video context information, in addition to emotion categories. To assess the emotional reasoning capabilities of different MLLMs on the EMER dataset, we employ ChatGPT to score their predictions, focusing on three key aspects:  (1)  the degree of overlap between emotion-related clues, (2) the degree of overlap between summarized emotional states, and (3) the completeness of the reasoning process across modalities. This multi-faceted evaluation provides a rigorous and in-depth assessment of the models' ability to understand and explain emotions in a multimodal context.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "For the global visual encoder, we employ the EVA model with full images sized at 448×448 pixels as input. For the local and temporal visual encoders, we first crop and align the faces within the images, then hierarchical sample 16 facial images as inputs for the MAE and VideoMAE models. The audio is handled by the HuBERT-Chinese large model. The extracted emotional features are transformed into a 4096-dimensional space via linear layers before being concatenated with text tokens.\n\nDuring the tuning process, we froze the visual and audio backbones, focusing on training the linear projection layer. For the language model (LLM), we utilize LLaMA2-chat (7B) equipped with LoRA for parameter-efficient tuning. Following the Minigpt-v2 approach, we fine-tune the query and value projection matrices (W q and W v ) by setting r = 64 and α = 16. Consequently, the trainable parameters of Emotion-LLaMA totaled only 34 million, representing a mere 0.495% of the overall parameter count. We train on 4*A100 GPUs for 300,000 steps, which takes around 20 hours. Detailed information can be found on the project homepage and in the code repository.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods",
      "text": "To comprehensively evaluate the performance of Emotion-LLaMA, we compared it with several state-of-the-art (SOTA) methods across different datasets. Multimodal Emotion Recognition Results. Table  2  presents the comparison results on the DFEW dataset. In the zero-shot scenario, Emotion-LLaMA demonstrates superior capabilities compared to all other MLLMs, showcasing its strong generalization ability. Notably, the majority of MLLMs scored zero in the disgust category, with GPT-4V achieving only 10.34%. This may be attributed to safety constraints on the term \"disgust\" within large language models, indicating a need for further exploration. Additionally, different MLLMs tend to favor predicting a specific emotion category, resulting in higher scores for those categories but lower recall scores overall. In contrast, Emotion-LLaMA maintains a more balanced prediction across all categories, ultimately achieving the highest WAR score of 59.37%. After fine-tuning, Emotion-LLaMA achieves the highest Unweighted Average Recall (UAR) and Weighted Average Recall (WAR) scores, further indicating its exceptional performance in emotion recognition tasks. These results highlight the effectiveness of our model in adapting to new datasets and accurately identifying emotions across various modalities. Overall, the results of Emotion-LLaMA's performance highlight the effectiveness of our approach in accurately recognizing emotions from multimodal data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Multimodal Emotion Recognition Challenge",
      "text": "To further validate the effectiveness of our proposed Emotion-LLaMA model, we conducted experiments on the MER2023 and MER2024 Challenge, comparing it with previous state-of-the-art methods.The results, presented in Table  3 , demonstrate that our model, which maps audio and visual features to the textual space, achieves the highest F1 score across various modalities. This approach significantly enhances the context of the textual modality by providing a more comprehensive understanding of the information, thereby outperforming other models. By integrating audio, visual, and textual data, Emotion-LLaMA can better capture the nuances of emotional expression, leading to more accurate and reliable emotion recognition.\n\nThe MER2024 Challenge introduced a new Open-Vocabulary Multimodal Emotion Recognition (MER-OV) task. Unlike traditional tasks, MER-OV focuses on recognizing any number of labels across diverse categories, aiming for a more nuanced and precise description of emotional states.\n\nAs shown in Table  4 , Emotion-LLaMA outperforms other mainstream multimodal models, yielding an 8.52% improvement in average accuracy and recall compared to GPT-4V, and achieving the highest zero-shot score among all participating large multimodal models. These results showcase the robustness and versatility of our approach in handling complex multimodal data for emotion recognition tasks, making it a promising solution for real-world applications. A 0.5481 HuBERT  [39]  A 0.8511 ResNet  [37]  V 0.4132 MAE  [36]  V 0.5547 VideoMAE  [84]  V 0.6068 RoBERTa  [66]  T 0.4061 BERT  [25]  T 0.4360 MacBERT  [24]  T 0.4632 MER2023-Baseline  [59]  A, V 0.8675 MER2023-Baseline  [59]  A, V, T 0.8640 Transformer  [8]  A, V, T 0.8853 FBP  [13]  A, V, T 0.8855 VAT  [26]  A, V 0.8911 Emotion-LLaMA A, V 0.8905 Emotion-LLaMA A, V, T 0.9036",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ground Truth",
      "text": "In the video, although she had a smile on her face, she questioned the other person, \"Are you really stupid or just pretending?\" expressing her dissatisfaction and anger towards the other person's act of pretending to be foolish. Therefore, it can be inferred that his emotional state is angry.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Pandagpt",
      "text": "The emotional state of this person appears to be positive and relaxed. They are smiling and laughing, indicating that they find the situation amusing or enjoyable. The woman's reaction to the man's joke suggests she is not offended or angry but finds it funny or amusing. This means that the interaction between the two individuals is friendly, not tense or uncomfortable.\n\nVideoChat-Embed Based on the video, the lady appears to be quite upset and angry. She discusses her financial losses and how her boyfriend seems indifferent to her feelings. She expresses a desire to spend money. The lady's facial expressions and gestures, such as frowning and clutching her purse tightly...",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Valley",
      "text": "From the video, we can infer that the woman might be in a neutral or mixed emotional state. In some scenes, she is smiling or making amusing faces, suggesting she feels happy or amused. . .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Emotion-Llama",
      "text": "In the video, the woman's facial expressions, including a furrowed brow, lifted eyebrows, and a slightly lowered lip, suggest a mix of emotions. Her tone and intonation convey a sense of frustration, and her words reveal that she is questioning whether the other person is silly, indicating a sense of annoyance. Therefore, it is inferred that her emotional state is anger.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Qualitative Analysis Of Emotion Reasoning",
      "text": "To illustrate the qualitative performance of Emotion-LLaMA, we present a detailed comparison of emotion reasoning results across different models. I would like to meet with you to discuss a matter.\n\nNo appointment and not going out. Impossible.\n\nIs it necessary to increase it to 110V? Sorry to trouble everyone.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Evaluation",
      "text": "We conducted a series of ablation experiments to explore the effectiveness of each component of the proposed Emotion-LLaMA. More ablation experiments, which examine factors affecting instructiontuning performance, including data quantity and quality as well as hyperparameters, are presented in Appendix C. In Table  7 , we present the impact of different instruction data on the instruction-tuning of Emotion-LLaMA. 'Raw' refers to the direct concatenation of visual and audio descriptions as instructions for training Emotion-LLaMA, which yielded the poorest performance. When trained using the coarsegrained set from the MERR dataset, Emotion-LLaMA achieved scores of 7.41 and 5.56 for clue and label overlap, respectively. This marks an improvement of 1.87 and 1.25 over the 'Raw' approach, demonstrating that coarse-grained annotations generated by the LLaMA-3 model effectively integrate emotional cues to capture genuine emotional expressions. Notably, further instruction-tuning using the fine-grained set from the MERR dataset resulted in additional gains of 0.42 and 0.69 in clue and label overlap, respectively, indicating that fine-grained annotations offer higher quality data and further enhance the performance of instruction-tuning.\n\nTo better understand the effect of the sample selection strategy, we compared our strategy against traditional semi-supervised approaches, as shown in",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "A Merr Dataset Details",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A.1 Emotion Categories And Annotations",
      "text": "The Multimodal Emotion Recognition and Reasoning (MERR) dataset covers a diverse range of emotion categories, including some that are often overlooked or challenging to distinguish, as shown in Figure  4  and Figure  5 . The dataset includes nine emotion categories: neutral, happy, angry, worried, surprise, sad, fear, doubt, and contempt. While the first seven categories are commonly addressed in most emotion datasets, MERR stands out by also focusing on doubt and contempt. These two categories are often underrepresented due to the difficulty in collecting sufficient samples and the potential for confusion with other emotions. Doubt, for example, can easily be mistaken for worry, as both emotions involve a sense of uncertainty and concern. However, there are subtle differences in facial expressions and contextual cues that can help distinguish between the two. Doubt often involves a more questioning or skeptical facial expression, with raised eyebrows and a slight frown, whereas worry tends to have a more anxious or apprehensive appearance, with furrowed brows and a downturned mouth. Contempt, on the other hand, is frequently misclassified as happiness due to the presence of a smile. However, the smile associated with contempt is often a scornful or dismissive one, accompanied by a slight sneer or a raised upper lip. The context and manner in which the smile is displayed can help differentiate between genuine happiness and contemptuous expression.\n\nTo accurately categorize these challenging emotions, the MERR dataset relies on rich multimodal descriptions that provide a comprehensive understanding of the emotional state and its context. These descriptions go beyond simple categorical labels and offer detailed insights into the facial expressions, body language, vocal cues, and situational factors that contribute to the emotional interpretation. Table  12  presents a template for the multimodal descriptions used in MERR, showcasing the different components that are captured for each sample. The descriptions include a visual expression component that focuses on the specific facial movements and action units associated with the emotion, a visual objective component that describes the overall scene and context, an audio tone component that captures the vocal cues and intonation, and a textual component that provides the transcribed speech or dialogue. To further illustrate the value of these detailed annotations, Tables  13  and 14  provide specific examples of annotated samples for doubt and contempt, respectively.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "A.2 Data Filtering And Pseudo-Labeling",
      "text": "To create a high-quality dataset for multimodal emotion recognition and reasoning, we employed a data filtering and pseudo-labeling process. This process aimed to identify video segments with strong emotional expressions and assign them initial emotion labels based on facial cues.\n\nFirst, we used OpenFace 6  to extract faces from the video segments. OpenFace is a state-of-the-art tool for facial behavior analysis that can detect and track facial landmarks, head pose, eye gaze, and facial action units (AUs). AUs are a widely used system for describing facial muscle movements, with each AU corresponding to a specific muscle or group of muscles. After extracting the faces, we aligned them to a canonical pose to facilitate accurate AU detection. OpenFace then analyzed the aligned faces to identify the presence and intensity of various AUs throughout each video segment. Next, we utilized the detected AUs to assign pseudo-labels to the video segments. As shown in Table  10 , certain combinations of AUs are strongly correlated with specific emotions. For example, the combination of AU05 (upper lid raiser) and AU26 (jaw drop) is often associated with the emotion of surprise. Similarly, the presence of AU04 (brow lowerer) and AU15 (lip corner depressor) is indicative of sadness. By leveraging these known AU combinations, we created a rule-based system to assign pseudo-labels to the video segments. If a segment exhibited a specific combination of AUs with sufficient intensity, it was assigned the corresponding emotion label. This process allowed us to identify samples that displayed strong emotional expression characteristics based on facial cues alone. Through this data filtering and pseudo-labeling process, we selected a total of 28,618 samples from the initial pool of video segments. These samples were assigned pseudo-labels corresponding to the nine emotion categories in the MERR dataset: neutral, happy, angry, worried, surprise, sad, fear, doubt, and contempt. It is important to note that while the pseudo-labels provided a valuable starting point for annotation, they were not relied upon as ground truth. In subsequent stages of the dataset construction process, human annotators reviewed and refined these labels, taking into account additional context from the visual, audio, and textual modalities.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "A.3 Instruction Collection And Multimodal Annotation",
      "text": "To provide rich, multimodal annotations for the MERR dataset, we collected instructions and descriptions from various sources, focusing on four key aspects: visual expression, visual context, audio tone, and multimodal integration.\n\nVisual Expression Description. In videos, natural actions such as blinking and speaking can lead to different combinations of Action Units (AUs) being extracted from various frames. To accurately represent the current emotion, it is crucial to determine the most relevant AUs. As illustrated in Figure  6 , our approach involves analyzing the amplitude values of the AUs to identify the \"emotional peak frame\", which captures the most intense emotional expression.\n\nThe specific steps for identifying the emotional peak frame are as follows:\n\n1. Identify the AUs present in all frames of the video segment.\n\n2. Sum the amplitude values of these AUs for each frame.\n\n3. Determine whether the combinations of these AUs match the pseudo-label.\n\n4. Select the frame with the highest total amplitude as the emotional peak frame.\n\nOnce the emotional peak frame is determined, its corresponding AUs are mapped to visual expression descriptions using the guidelines provided in Table  11 . These descriptions provide a detailed account of the facial movements and expressions associated with the emotion displayed in the peak frame.\n\nVisual Objective Description. To provide a comprehensive understanding of the emotional context in each video, we utilize the MiniGPT-v2  [10]  model to generate descriptions of the visual scene. By inputting the complete emotional peak frame, which captures the most intense emotional expression, MiniGPT-v2 can analyze and describe various aspects of the video, such as the environment, character actions, and object interactions. These visual objective descriptions offer valuable insights into the situational context surrounding the emotional expression. For example, if a character is shown in a dimly lit room with a concerned facial expression, the model might generate a description like \"The scene takes place in a dark, shadowy room. The character appears to be sitting alone, with a worried look on their face, fidgeting with their hands.\" This description provides additional information about the setting and the character's body language, which can help in interpreting their emotional state. Moreover, the model can also identify and describe relevant objects or elements in the scene that may contribute to the emotional context. For instance, if a character is holding a letter and appears upset, the model might mention the presence of the letter in its description, suggesting that the content of the letter could be related to the character's emotional response.\n\nAudio Tone Description. In addition to visual cues, audio plays a crucial role in conveying emotional information. The tone, intonation, and prosodic features of a speaker's voice can provide valuable insights into their emotional state, often revealing subtle nuances that may not be apparent from visual cues alone. To capture these audio cues, we employ the Qwen-Audio  [22]  model, which is specifically designed to analyze and describe the emotional content of speech. By processing the audio track of each video segment, it can generate detailed descriptions of the speaker's tone and intonation. For example, if a character is speaking with a trembling voice and a high pitch, the model might generate a description like \"The speaker's voice is shaky and high-pitched, indicating a sense of fear or anxiety.\n\nThere are noticeable pauses and hesitations in their speech, suggesting uncertainty or distress.\" This description captures the emotional nuances conveyed through the speaker's vocal delivery, providing additional context for understanding their emotional state. Moreover, Qwen-Audio can also identify and describe other relevant audio cues, such as sighs, laughter, or changes in speaking rate, which can further contribute to the emotional interpretation. For instance, if a character is speaking rapidly and laughing, the model might generate a description like \"The person in the video speaks with a cheerful tone.\"\n\nMultimodal Description. To generate initial coarse-grained emotional descriptions, we concatenate the information obtained from all modalities (visual expression, visual context, audio tone, and textual content). These surface-level descriptions for the 28,618 pseudo-labeled samples are used in the first stage of pre-training to help the model align emotional features with the textual semantic space. We input all the collected emotional clues into the LLaMA-3 model for further refinement. LLaMA-3 sifts through the clues, identifies the most relevant ones, and combines them to generate a comprehensive emotional description. This process helps to filter out any erroneous or contradictory descriptions that may have been present in the initial set of emotional clues. Additionally, we remove duplicate or overabundant samples to ensure a balanced and diverse dataset. Through these refinement processes, the final MERR dataset contains 4,487 samples, each accompanied by a detailed multimodal description. Table  12  presents an example of the annotation format used for each sample in the dataset. Finally, four domain experts manually review the refined descriptions and use a voting process to select fine-grained samples, assessing whether the video descriptions are reasonable and if the emotional reasoning is accurate. By collecting and integrating instructions and descriptions from multiple modalities, we have created a rich and informative dataset that captures the complexities of emotional expressions in real-world scenarios. The multimodal annotations in MERR enable models to learn more comprehensive and nuanced representations of emotions, leading to improved performance on emotion recognition and reasoning tasks.",
      "page_start": 19,
      "page_end": 20
    },
    {
      "section_name": "A.4 Data Statistics And Comparisons",
      "text": "Video Duration Distribution. Figure  7  presents the distribution of video durations in the MERR dataset. The majority of the samples have a length between 2 and 4 seconds, which aligns with the typical duration of short, emotionally expressive video clips. This duration range strikes a balance between capturing sufficient context for emotion recognition and maintaining a manageable data size for processing and annotation. Shorter clips may lack the necessary context to fully understand the emotional state, while longer clips can be more challenging to annotate and may contain multiple or changing emotions. The concentration of samples in the 2-4 second range also reflects the natural temporal dynamics of emotional expressions. Most emotions are conveyed through relatively brief, intense bursts of facial movements, vocalizations, and body language. By focusing on this duration range, the MERR dataset captures the core expressive moments while minimizing the inclusion of neutral or ambiguous segments. Furthermore, the consistent duration range across the dataset facilitates the development of emotion recognition models that can operate on fixed-length input sequences. This consistency simplifies the data preprocessing and model architecture design, as the models can be optimized for the specific temporal scale of the emotional expressions.\n\nComparison with Previous Datasets. To highlight the unique features and contributions of the MERR dataset, we compare it with several related datasets in Table  15 . The MER2023  [59]  and DFEW  [45]  datasets primarily focus on discrete emotion category labels, providing a classificationoriented perspective on emotion recognition. While these datasets are valuable for developing models that can predict specific emotion categories, they lack the detailed descriptions and contextual information necessary for deeper emotion understanding. On the other hand, datasets like EmoSet  [93]  and EmoVIT  [92]  offer visual descriptions of emotional expressions, capturing the facial cues and body language associated with different emotions. However, these datasets do not include information from other modalities, such as audio or text, which can provide crucial insights into the emotional context and help disambiguate complex or subtle expressions. The EMER  [62]  dataset stands out as one of the few existing datasets that contain multimodal emotion descriptions, incorporating information from visual, audio, and textual modalities. However, due to the high cost and effort involved in manual annotation, EMER is limited to only 100 samples, which may not be sufficient for training robust and generalizable emotion recognition models.\n\nIn contrast, the MERR dataset offers a comprehensive and large-scale resource for multimodal emotion recognition and reasoning. With 28,618 coarse-grained and 4,487 fine-grained annotated samples, MERR provides a diverse range of emotional expressions across nine categories, including challenging ones like doubt and contempt. The extensive multimodal descriptions in MERR, encompassing visual expressions, visual context, audio tone, and textual information, enable a holistic understanding of emotional states and their situational context. Moreover, the MERR dataset's inclusion of detailed emotion reasoning annotations sets it apart from other datasets. By capturing the thought process and rationale behind the emotion labels, these annotations facilitate the development of models that can not only recognize emotions but also explain and justify their predictions. This level of interpretability is crucial for building trust and transparency in human-computer interaction scenarios. As such, MERR has the potential to advance the field of affective computing and contribute to the development of more intelligent and empathetic human-computer interaction systems.\n\nAssessing the Quality of Dataset Instructions. We conducted a human evaluation of the annotation process by randomly selecting 20 video samples from each of the nine emotion categories. We then randomly shuffled the fine-grained annotations. Five volunteers evaluated the consistency and relevance of the video descriptions by scoring them. Each volunteer rated a total of 180 descriptions on a scale from 0 to 5. The evaluation criteria included:\n\n1. Accuracy of the visual modality description. 2. Accuracy of the audio modality description.\n\n3. Accuracy of the textual modality description. 4. Correctness of the reasoning process. 5. Correctness of the reasoning result.\n\nAs shown in the Table  9 , the average score for the human evaluation of the MERR dataset is 4.258, indicating that the annotations are of high quality and align with real-world logic. Secondly, the \"Neutral\" category received the lowest score among all categories, suggesting that when a character's emotion is neutral, the facial and audio cues are weaker, making automatic annotation more challenging. We will address these findings and discuss related limitations in future work. Details of the human evaluation, including the code and assessment results, can be accessed through the code repository. Table  10 : Correspondence between facial expression labels and Action Units (AUs). Each expression label is associated with a unique combination of AUs, allowing for accurate mapping between facial movements and emotional categories.\n\n• \"happy\": [ \"AU06\", \"AU12\", \"AU14\"]\n\n• \"angry\": [ \"AU04\", \"AU05\", \"AU07\", \"AU23\", \"AU10\", \"AU17\"]\n\n• \"worried\": [ \"AU28\", \"AU20\"]\n\n• \"surprise\": [ \"AU01\", \"AU02\", \"AU05\", \"AU26\"]\n\n• \"sad\": [ \"AU04\", \"AU01\", \"AU14\", \"AU15\"]\n\n• \"fear\": [ \"AU01\", \"AU02\", \"AU04\", \"AU05\", \"AU07\", \"AU20\", \"AU26\"]\n\n• \"doubt\": [ \"AU25\"]\n\n• \"contempt\": [ \"AU12\", \"AU10\", \"AU15\", \"AU17\"] Table  11 : Mapping of Action Units (AUs) to their corresponding textual descriptions. Each AU represents a specific facial muscle movement, and the textual descriptions provide a human-interpretable explanation of the visual cues associated with each AU.\n\n• \"AU01\": [\"Inner brow raiser\", \"Frown\", \"Eyebrow raised\", \"Head lifting wrinkles\", \"Lift eyebrows\"]\n\n• \"AU02\": [\"Outer brow raiser\", \"Outer brow lift\", \"Elevate outer brow\", \"Outer brow arch\"]\n\n• \"AU04\": [\"Brow Lowerer\", \"Frowns furrowed\", \"Lower eyebrows\", \"A look of disapprroval\"]\n\n• \"AU05\": [\"Upper Lid Raiser\", \"Pupil enlargement\", \"Eyes widened\", \"Lift upper eyelids\", \"Raise upper eyelids\"]\n\n• \"AU06\": [\"Cheek Raiser\", \"smile, Pleasure\", \"Slight decrease in eyebrows\", \"Eyes narrowing\", \"Slightly lower eyebrows\"]\n\n• \"AU07\": [\"Lid Tightener\", \"Facial tightness\", \"Tightening of eyelids\"]\n\n• \"AU09\": [\"Nose Wrinkler\", \"Wrinkle the nose\", \"Curl the nose\", \"Make a face\", \"Pucker the nose\"]\n\n• \"AU10\": [\"Upper Lip Raiser\", \"Curl the lips upwards\", \"Upper lip lift\", \"Lips apart showing teeth\"]\n\n• \"AU12\": [\"Lip Corner Puller\", \"Toothy smile\", \"Grinning\", \"Big smile\", \"Show teeth\"]\n\n• \"AU14\": [\"Dimpler\", \"Cheek dimple\", \"Indentation when smiling\", \"Hollow on the face when smiling\"]\n\n• \"AU15\": [\"Lip Corner Depressor\", \"Downturned corners of the mouth\", \"Downward mouth curvature\", \"Lower Lip Depressor\"]\n\n• \"AU17\": [\"Chin Raiser\", \"Lift the chin\", \"Chin held high\", \"Lips arching\", \"Lips forming an upward curve\"]\n\n• \"AU20\": [\"Lip stretcher\", \"Tense lips stretched\", \"Anxiously stretched lips\", \"Nasal flaring\", \"Nostrils enlarge\"]\n\n• \"AU23\": [\"Lip Tightener\", \"Tighten the lips,' 'Purse the lips\", \"Press the lips together\"]\n\n• \"AU25\": [\"Lips part\", \"Open the lips\", \"Slightly puzzled\", \"lips slightly parted\"]\n\n• \"AU26\": [\"Jaw Drop\", \"Mouth Stretch\", \"Open mouth wide\", \"Wide-mouthed\", \"Lips elongated\"]\n\n• \"AU28\": [\"Lip Suck\", \"Purse lips\", \"Pucker lips\", \"Draw in lips\", \"Bring lips together\"]   Question: The woman in the video is talking to a man, possibly discussing something important or sharing her thoughts and feelings. The person's expression and action include eyes widened, wide-mouthed, and speaking with a happy voice, saying: \"Oh my.\" Please sort out the correct emotional clues and infer why the person in the video feels surprise.\n\nAnswer: In the video, a woman is conversing with a man. Her facial expressions, with eyes widened and mouth wide open, clearly show surprise. She amplifies this visual cue with an excited tone as she exclaims, \"Oh my.\" This combination of voice and expression indicates that she is experiencing surprise, likely in response to unexpected news shared during the conversation.\n\nTable  13 : Example of the emotion category \"doubt\". The table presents a multimodal data sample associated with the \"doubt\" category, including the visual expression description, audio tone description, and textual content. This example highlights the subtle and complex nature of the \"doubt\" emotion and the importance of considering multiple modalities for accurate recognition and reasoning.",
      "page_start": 21,
      "page_end": 25
    },
    {
      "section_name": "Lexical Subtitle:",
      "text": "\"That must not be his real name.\" Audio tone description:\n\n\"The woman's tone sounds natural.\" Visual expression description:\n\n\"Lips slightly parted.\" Visual objective description:\n\n\"The person in the video is a woman wearing traditional Chinese clothing and a gold headpiece.\" Multimodal description: \"In the video, the woman's lips are slightly parted. Her tone is natural, but her words express skepticism as she says, 'That must not be his real name.' Her questioning of the authenticity of the name suggests that she is experiencing doubt.\" Table  14 : Example of the emotion category \"contempt\". The table showcases a multimodal data sample corresponding to the \"contempt\" emotion, encompassing the visual expression description, audio tone description, and textual content. This example illustrates the nuanced and challenging characteristics of the \"contempt\" category, emphasizing the need for a comprehensive multimodal approach to capture its subtleties.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Lexical Subtitle:",
      "text": "\"Isn't he who he is?\" Audio tone description:\n\n\"The person in the video speaks in a normal tone.\" Visual expression description:\n\n\"Downturned corners of the mouth, lips apart showing teeth, lip Corner Puller, chin held high.\" Visual objective description:\n\n\"The person in the video is a man with long black hair and is wearing a white shirt and a brown vest.\" Multimodal description: \"In the video, the person's facial expressions (downturned mouth corners, lips apart showing teeth, and lip corner puller) and a sarcastic smirk before speaking suggest a sense of superiority or disdain. The words, 'Isn't he who he is?', convey mockery or ridicule.\"",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "B Training And Implementation Details B.1 Instructions For Multimodal Emotion Recognition",
      "text": "In the multimodal emotion recognition task, the goal is to predict the emotional category label for a given video sample. To guide the model, we use the task identifier [emotion] and provide a set of instructions that prompt the model to classify the emotion displayed in the video. Table  16  presents the list of instructions used for the emotion recognition task. These instructions are designed to be clear, concise, varied in their phrasing to encourage the model to learn a robust understanding of the task. The instructions ask the model to determine the specific emotion portrayed in the video, choosing from a predefined set of categories: happy, sad, neutral, angry, worried, surprise, fear, contempt, and doubt. By using multiple instructions with slight variations in wording, we aim to prevent the model from overfitting to specific patterns or phrases and instead focus on the underlying task of identifying the emotional state based on the multimodal cues present in the video. During training, these instructions are randomly sampled and paired with the corresponding video samples and their associated annotations. The model learns to attend to the relevant visual, audio, and textual features and map them to the appropriate emotion label based on the provided instruction. • \"Please determine which emotion label in the video represents: happy, sad, neutral, angry, worried, surprise, fear, contempt, doubt.\" • \"Identify the displayed emotion in the video: is it happy, sad, neutral, angry, worried, or surprise, fear, contempt, doubt?\" • \"Determine the emotional state shown in the video, choosing from happy, sad, neutral, angry, worried, surprise, fear, contempt or doubt.\" • \"Please ascertain the specific emotion portrayed in the video, whether it be happy, sad, neutral, angry, worried, surprise, fear, contempt or doubt.\" • \"Assess and label the emotion evident in the video: could it be happy, sad, neutral, angry, worried, surprise, fear, contempt, doubt?\"",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "B.2 Instructions For Multimodal Emotion Reasoning",
      "text": "In addition to recognizing the emotional category, we also want the model to be able to reason about the emotional state based on the available multimodal cues. This task requires a deeper understanding of how the different modalities contribute to the overall emotional interpretation and the ability to explain the reasoning behind the predicted emotion label. To support this task, we use the task identifier [reason] and provide a set of instructions that prompt the model to analyze the multimodal cues and provide a rationale for the predicted emotion. Table  17  shows the list of instructions used for the emotion reasoning task. These instructions ask the model to integrate information from various modalities, such as facial expressions, vocal tone, and the intended meaning behind the spoken words, to infer the emotional state of the person in the video. The model is expected to not only predict the emotion label but also provide a coherent explanation of how the different cues contribute to that prediction. By training the model with these reasoning instructions, we aim to develop its ability to understand the complex interplay between the different modalities and to generate human-interpretable explanations for its predictions. This reasoning capability is crucial for building trust and transparency in the emotion recognition system and facilitating more natural and engaging human-computer interactions. During training, the reasoning instructions are randomly sampled and paired with the video samples and their associated multimodal annotations. The model learns to attend to the relevant cues across modalities, combine them in a meaningful way, and generate a reasoning trace that justifies the predicted emotion label. The implementation of the Emotion-LLaMA model involves several key components and design choices that enable it to effectively learn and reason about emotions from multimodal data. In this section, we provide a detailed overview of the main implementation details, including the training approach, model architecture, and inference process.",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "B.3.1 Multi-Task Learning Approach",
      "text": "One of the core aspects of the Emotion-LLaMA model is its ability to simultaneously learn and perform multiple tasks related to emotion understanding. Specifically, the model is trained using a multi-task learning approach, where the emotion recognition and emotion reasoning tasks are learned in parallel. In the emotion recognition task, the model learns to predict the appropriate emotion label for a given multimodal input, such as a video clip accompanied by audio and text. The model is trained to map the input features to one of the predefined emotion categories, such as happy, sad, angry, or neutral. On the other hand, the emotion reasoning task focuses on generating human-interpretable explanations for the predicted emotion labels. Given a multimodal input, the model learns to analyze the various cues and generate a natural language explanation that justifies the predicted emotion based on the available evidence. By training the model to perform both tasks simultaneously, we allow it to share representations and learn complementary skills across tasks. This multi-task learning approach has several benefits. First, it enables the model to develop a more comprehensive understanding of emotions by learning to recognize the emotional state and explain the reasoning behind it. Second, it encourages the model to learn more robust and generalizable representations by leveraging the commonalities and differences between the two tasks. During training, for each video sample input into the model, we randomly select either the emotion recognition or emotion reasoning task to enhance the model's generalization and robustness.",
      "page_start": 28,
      "page_end": 29
    },
    {
      "section_name": "B.3.2 Coarse-To-Fine Training Strategy",
      "text": "To facilitate the learning of nuanced and detailed emotional representations, we employ a coarseto-fine training strategy. This strategy involves two main stages: pre-training on coarse-grained annotations and fine-tuning on fine-grained annotations. In the pre-training stage, the model is trained on the coarse-grained annotations from the MERR dataset. These annotations provide a simple description of emotions, such as visual expression, audio tone. By training on these coarsegrained annotations, the model learns to capture the general emotional tone and develop an initial understanding of the emotional content in the multimodal data. After the pre-training stage, we proceed to the fine-tuning stage, where the model is exposed to the fine-grained annotations from the MERR dataset. These annotations offer more detailed and specific emotion descriptions, such as environmental information, body movements, and the emotional nuances inferred from tone and textual subtitles. By fine-tuning the model on these fine-grained descriptions, we enhance its emotional understanding and enable it to capture more nuanced expressions and variations in scenarios that closely resemble real-world contexts. The coarse-to-fine training strategy offers several advantages. First, it allows the model to gradually learn more complex and detailed emotional representations, starting from a solid foundation of general emotional understanding. Second, it helps to prevent overfitting and improves the model's generalization ability by providing a hierarchy of emotional annotations to learn from. During the fine-tuning stage, we typically use a smaller learning rate and a shorter training duration compared to the pre-training stage. This allows the model to adapt its representations to the fine-grained labels without drastically changing the learned features from the pre-training stage.",
      "page_start": 28,
      "page_end": 29
    },
    {
      "section_name": "B.3.3 Modality-Specific Encoders",
      "text": "To effectively process and integrate information from multiple modalities, the Emotion-LLaMA model employs modality-specific encoders. These encoders are designed to extract meaningful features and representations from each modality, capturing the unique characteristics and cues relevant to emotion understanding. For the visual modality, we use a combination of three encoders: MAE (Masked Autoencoders)  [36] , VideoMAE  [84] , and EVA (Efficient Video Analysis)  [32] . The MAE encoder focuses on capturing local facial features and expressions, which are crucial for recognizing emotions. It learns to reconstruct masked patches of the input frames, enabling it to capture fine-grained details of facial movements and micro-expressions. The VideoMAE encoder, on the other hand, is designed to capture the temporal dynamics and motion patterns in the video data. It learns to predict the masked frames in a video sequence, allowing it to model the evolution of emotional expressions over time.\n\nBy capturing the temporal context, VideoMAE helps the model to understand the progression and transitions between different emotional states. Finally, the EVA encoder is used to capture the global scene understanding and contextual information in the visual data. It processes the entire video frames and learns to extract high-level features that represent the overall scene and environment. This global context is important for interpreting the emotional state of individuals in relation to their surroundings and the situational factors. For the audio modality, we employ the HuBERT (Hidden-Unit BERT)  [39]  encoder, which is a state-of-the-art model for speech representation learning. HuBERT is trained on a large corpus of unlabeled speech data and learns to predict the hidden units of a pre-trained teacher model. By doing so, it captures rich vocal representations that encode prosodic features, such as intonation, stress, and rhythm, which are indicative of emotional states. Finally, for the textual modality, we use the LLaMA tokenizer to handle the transcribed speech or dialogue. The LLaMA tokenizer  [85]  is based on a byte-level Byte Pair Encoding (BPE)  [78]  algorithm, which allows for efficient and flexible tokenization of the input text. The tokenized text is then passed through the LLaMA model, which learns to capture the semantic and emotional content in the language data.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "B.3.4 Unified Representation Space",
      "text": "To enable the Emotion-LLaMA model to reason about emotions across modalities, we project the outputs from the modality-specific encoders into a unified representation space. This is achieved by applying a linear transformation to the features extracted by each encoder, mapping them to a common embedding dimension. In addition to the projected features, we also introduce learned special tokens that are concatenated with the multimodal embeddings. These special tokens serve as task-specific indicators and provide additional context for the model to distinguish between the emotion recognition and emotion reasoning tasks. The resulting unified representation is then fed into the LLaMA model, which uses its self-attention mechanism to attend to and reason about the multimodal cues in a holistic manner. The LLaMA model learns to capture the interactions and dependencies between the different modalities, enabling it to make informed predictions and generate coherent explanations. By operating in a unified representation space, the Emotion-LLaMA model can effectively integrate and align information from multiple modalities, allowing for cross-modal reasoning and understanding. This unified space facilitates the model's ability to capture the complex dynamics of emotional expressions and generate meaningful insights based on the combined evidence from visual, audio, and textual cues.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "B.3.5 Training Objective And Optimization",
      "text": "The training objective of the Emotion-LLaMA model for both the emotion recognition and emotion reasoning tasks is based on the language modeling loss. This loss function, commonly used in natural language generation tasks, measures the likelihood of generating the ground-truth tokens given the input multimodal data. By minimizing this loss, the model learns to produce coherent and contextually relevant tokens that align with the provided reasoning examples or emotion categories, effectively capturing the underlying patterns and relationships in the data. To optimize the model parameters during training, we employ the Adam optimizer, a widely adopted and efficient optimization algorithm in deep learning. Adam adapts the learning rate for each parameter based on historical gradients, providing a robust and adaptive optimization process. By leveraging the adaptive learning rates, Adam helps the model converge faster and find better local minima, leading to improved performance and generalization. To train the large language model component of Emotion-LLaMA efficiently, we utilize the Low-Rank Adaptation (LoRA) technique. LoRA significantly reduces the number of trainable parameters while preserving the model's pre-existing world knowledge. By adapting only a small set of low-rank matrices, LoRA allows the model acquire domain-specific knowledge related to emotions, such as nuances in tone of speech and facial expressions, without overwriting or losing the valuable information learned during pre-training. This technique strikes a balance between efficiency and effectiveness, enabling the model to specialize in the emotion domain while retaining its general language understanding capabilities. For a more detailed of the training objective, optimization process, and the use of LoRA, please refer to the source code  7  .",
      "page_start": 29,
      "page_end": 30
    },
    {
      "section_name": "C Experiments & Demonstration C.1 Evaluation Metrics",
      "text": "To comprehensively assess the performance of the Emotion-LLaMA model on various datasets, we employ a range of evaluation metrics tailored to the specific characteristics and requirements of each dataset. These metrics are designed to provide a fair and thorough evaluation of the model's emotion recognition and reasoning capabilities, taking into account factors such as class imbalance and the importance of both majority and minority classes.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "C.1.1 Dfew Dataset",
      "text": "For the DFEW dataset, we utilize two main evaluation metrics: Weighted Average Recall (WAR) and Unweighted Average Recall (UAR). These metrics are particularly well-suited for imbalanced datasets, where the distribution of samples across different emotion categories is not uniform. WAR focuses on the model's ability to recognize the majority classes, which have a larger number of samples in the dataset. It is calculated as the weighted sum of the recall values for each emotion category, where the weights are determined by the proportion of samples in each category:\n\nwhere N is the total number of emotion categories, n i is the number of samples in category i, and Recall i is the recall value for category i. On the other hand, UAR ensures that the model's capability to identify the minority classes, which have fewer samples, is not overlooked. It is calculated as the unweighted average of the recall values across all emotion categories:\n\nBy considering both WAR and UAR, we can assess the model's performance on the majority and minority classes independently, providing a more comprehensive evaluation. A high WAR indicates that the model is effective at recognizing the most prevalent emotions, while a high UAR suggests that the model is capable of identifying even the less frequent emotions accurately. The combination of these metrics offers a balanced assessment, ensuring that it can handle the class imbalance present in the DFEW dataset and provide reliable emotion recognition across all categories.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "C.1.2 Mer2023 Dataset",
      "text": "For the MER2023 dataset, we employ the Weighted Average F-score (WAF) as the primary evaluation metric. WAF is a composite metric that combines precision and recall, providing a single value that reflects the model's overall performance. The F-score for each emotion category is calculated as the harmonic mean of precision and recall:\n\nwhere Precision i and Recall i are the precision and recall values for emotion category i, respectively. The WAF is then computed as the weighted average of the F-scores across all emotion categories, with the weights determined by the proportion of samples in each category:\n\nwhere N is the total number of emotion categories, n i is the number of samples in category i, and F i is the F-score for category i. By incorporating both precision and recall, WAF provides a balanced measure of the model's ability to correctly identify emotions while minimizing false positives and false negatives. It takes into account the class imbalance present in the dataset, giving more weight to the performance on the majority classes while still considering the minority classes. WAF offers several advantages over using recall or accuracy alone. It reduces the impact of class imbalance on the evaluation results, ensuring that the model's performance on the minority classes is not overshadowed by its performance on the majority classes. Additionally, WAF is more robust to noise and outliers, as it considers both the true positive and false positive rates. By using WAF as the evaluation metric for the MER2023 dataset, we can obtain a comprehensive assessment of the Emotion-LLaMA model, taking into account the precision and recall of emotion recognition across all categories.",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "C.1.3 Emer Dataset",
      "text": "For the EMER dataset, we employ a unique evaluation approach that leverages the reasoning capabilities of ChatGPT (i.e., gpt-3.5-turbo-16k-0613), a large language model. The evaluation focuses on assessing the quality and coherence of the emotional reasoning provided by the Emotion-LLaMA model. We follow the evaluation prompt template shown in Table  18 , where {gt_reason} represents the ground truth emotional description provided in the EMER dataset, and {pred_reason} represents the emotional reasoning generated by the Emotion-LLaMA model or other baseline models.\n\nThe evaluation prompt is designed to elicit an objective score from ChatGPT based on a set of predefined scoring rules. These rules assess the quality of the generated reasoning in terms of its alignment with the ground truth description, the coherence and logicality of the reasoning process, and the extent to which it captures the relevant emotional cues and evidence.\n\nTo ensure a fair and unbiased evaluation, ChatGPT is provided with a clear set of scoring guidelines. These guidelines instruct ChatGPT to consider factors such as the overlap between the predicted and ground truth reasoning, the presence of contradictory or irrelevant information, and the overall clarity and coherence of the generated explanation. ChatGPT then assigns a score to each generated reasoning based on these guidelines, providing a quantitative measure of the model's performance.\n\nAlong with the numeric score, ChatGPT also provides a brief justification for the assigned score, highlighting the strengths and weaknesses of the generated reasoning.\n\nBy leveraging ChatGPT's language understanding and reasoning capabilities, we can obtain a more comprehensive and nuanced evaluation of the Emotion-LLaMA model's performance on the EMER dataset. This approach goes beyond simple metrics like accuracy or F-score and assesses the model's ability to generate coherent and meaningful explanations for the predicted emotions.\n\nTable  18 : Prompt for calculating the degree of overlap between emotion-related clues. The table provides a structured prompt template that guides the evaluation of the Emotion-LLaMA model's reasoning capabilities. By comparing the model's generated explanations with the ground truth, the prompt enables the quantitative assessment of the model's ability to identify and articulate relevant emotion-related cues.\n\nBelow, the \"Actual Description\" and \"Predicted Description\" of a character are given. Please follow the steps below to calculate the score for the \"Predicted Description\". The score should range from 1 to 10. In the end, only output the numerical value of the predicted score along with the reasoning.\n\n1. Summarize the emotional state description of the character from the \"Actual Description\". 2. Summarize the emotional state description of the character from the \"Predicted Description\".\n\n3. Calculate the overlap between the \"Predicted Description\" and the \"Actual Description\". The higher the overlap, the higher the score. 4. Output format: 'Predicted Score': Predicted Score; 'Reason': Reason Input:\n\n\"Actual Description\": {gt_reason} \"Predicted Description\": {pred_reason} Output:",
      "page_start": 32,
      "page_end": 33
    },
    {
      "section_name": "C.2 Additional Results And Analysis",
      "text": "In this section, we present additional results and analysis to further explore the performance and characteristics of the Emotion-LLaMA model. We examine the confusion matrices for the MER2023 and DFEW datasets, discuss the challenges posed by class imbalance, and provide detailed scoring cases from the EMER dataset to showcase the model's emotion reasoning capabilities.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "C.2.1 Confusion Matrices",
      "text": "Figure  8  presents the confusion matrices for the MER2023 and DFEW datasets, providing insights into the model's performance across different emotion categories. In both datasets, the categories \"angry,\" \"happy,\" and \"sad\" are the most common and distinctive emotions encountered in realworld scenarios. These emotions often have clear and pronounced facial expressions, vocal cues, and language patterns, making them relatively easier for the Emotion-LLaMA model to learn and recognize accurately. The confusion matrices show that the model achieves high accuracy for these prevalent emotion categories. The majority of the samples belonging to \"angry,\" \"happy,\" and \"sad\" are correctly classified, with minimal confusion between them. This indicates that the model has successfully learned the discriminative features and patterns associated with these emotions and can effectively distinguish between them. However, the confusion matrices also highlight the challenges posed by less frequent emotion categories, such as \"disgust\" and \"fear.\" Due to the inherent class imbalance present in multimodal emotion recognition datasets, these categories have significantly fewer training samples compared to the more common emotions. The limited availability of training data for \"disgust\" and \"fear\" makes it more difficult for the model to learn and generalize the patterns and cues specific to these emotions. As a result, the model may struggle to accurately recognize and classify samples belonging to these categories, leading to higher confusion rates and lower performance compared to the more prevalent emotions. The confusion matrices reveal that a notable proportion of \"disgust\" and \"fear\" samples are misclassified as other emotions, such as \"angry\" or \"sad.\" This suggests that the model may be relying on overlapping or ambiguous features that are shared between these emotions, leading to confusion and misclassification.\n\nNote that addressing the class imbalance issue is a crucial challenge in multimodal emotion recognition. Further research and exploration are needed to develop techniques that can effectively handle the imbalanced distribution of emotion categories and improve the model's performance on less frequent emotions. Potential approaches to mitigate class imbalance include data augmentation techniques, such as oversampling the minority classes or generating synthetic samples, to increase the represen-tation of underrepresented emotions in the training data. Additionally, employing class-weighted loss functions or adaptive learning strategies that focus on the minority classes during training can help the model learn more robust and generalizable features for these emotions. By tackling the class imbalance problem and improving the model's ability to recognize and classify less frequent emotions accurately, we can enhance the overall performance and practicality of the Emotion-LLaMA model in real-world applications. Moreover, the ablation study also reveals that the performance gains gradually diminish as the data proportion approaches the full dataset. This suggests that there may be a saturation point beyond which adding more training data yields diminishing returns in terms of performance improvement. Understanding the relationship between data quantity and model performance is crucial for efficient resource allocation and practical deployment considerations. The ablation study results provide guidance on the minimum data requirements for the Emotion-LLaMA model to achieve satisfactory performance and help identify the optimal trade-off between data size and computational resources.",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "C.2.2 Ablation Study Of Hyperparameters",
      "text": "",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "C.2.3 Mer2024 Challenge",
      "text": "The MER2024 Challenge  [60]  has recently garnered significant attention for its focus on integrating multimodalities to identify human emotional states. This initiative aims to address the limitations of existing technologies, which often struggle to meet the demands of practical applications. The challenge seeks to advance research in areas such as multimodal modeling of human affect, modality robustness in affect recognition, low-resource affect recognition, human affect synthesis in multimedia, privacy in affective computing, and applications in health, education, and entertainment. The challenge's theme has attracted a large number of researchers from around the globe to discuss recent advancements and future directions for robust multimodal emotion recognition, generating many new insights  [9, 29, 74, 79] .\n\nThe MER2024 Challenge has introduced three distinct tracks: (1) MER-SEMI, which provides over 110,000 unlabeled samples for semi-supervised or self-supervised learning; (2) MER-NOISE, which adds noise to test videos to simulate more realistic variations in modality conditions such as background noise and blurry videos, thereby evaluating system robustness; (3) MER-OV, which requires models to extract richer and more subtle emotions in an open-vocabulary manner, aiming to mitigate the intrinsic subjectivity in the emotion annotation process and the inherent ambiguity of MER. Therefore, the MER-NOISE and MER-OV tracks represent the most realistic expressions of emotions and data distributions, presenting both challenges and practical applications.\n\nThe MER-NOISE track emphasizes enhancing noise robustness in emotion recognition systems, as noise is ubiquitous in practical settings. Ensuring clear audio streams and high-resolution video frames can be challenging. This track specifically targets two prevalent types of noise: audio additive noise and image blur, encouraging participants to utilize data augmentation techniques and other innovative methods to bolster the resilience of emotion recognition systems  [30] . Due to the integration of multimodal information for reasoning, our proposed Emotion-LLaMA significantly reduces the impact of noise, resulting in outstanding performance in the MER-NOISE track with a weighted average F-score (WAF) of 84.52%, surpassing all other teams. Furthermore, we utilized the predictions from Emotion-LLaMA as pseudo-labels to enhance the performance of Conv-Attention  [15] . Ultimately, our team leveraged the robust capabilities of Emotion-LLaMA to secure first place in the MER-NOISE track, exceeding the second and third place teams by 1.47% and 1.65%, respectively. Detailed rankings and scores are shown in Figure  10 . The MER-OV track introduces the concept of open-vocabulary emotion recognition, addressing the subjectivity and ambiguity inherent in emotion labeling. Traditional datasets often limit label spaces to a few discrete categories, relying on multiple annotators and majority voting, which can overlook valid but non-candidate or minority labels. Participants generate a diverse array of labels, aiming for a more nuanced and accurate representation of emotional states.  [33, 98] . As shown in Table  4 , Emotion-LLaMA demonstrates greater generalization capabilities compared to other multimodal large language models.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "C.2.4 Detailed Scoring Cases",
      "text": "To showcase the emotion reasoning capabilities of the Emotion-LLaMA model, we present detailed scoring cases from the EMER dataset in Table  19  and Table  20 .\n\nIn these tables, we compare the emotion reasoning results generated by the Emotion-LLaMA model with those of other multimodal large language models. The evaluation is performed using the prompts from Table  18 , which assess the quality and coherence of the generated reasoning against the ground truth emotional descriptions. The scoring cases demonstrate the superiority of the Emotion-LLaMA model in accurately identifying and interpreting the emotional cues present in the multimodal data.\n\nThe model successfully captures the facial details, vocal tone, and linguistic content, and integrates them to arrive at the correct emotion labels.\n\nIn Table  19 , the Emotion-LLaMA model accurately identifies the subtle facial expressions and the slightly raised voice of the speaker, indicating a sense of doubt or uncertainty. By combining these cues with the content of the spoken dialogue, the model generates a coherent and logical reasoning that aligns well with the ground truth description. Similarly, in Table  20 , the Emotion-LLaMA model correctly recognizes the facial cues of raised eyebrows and a slight frown, suggesting a state of surprise or disbelief. The model also takes into account the emphatic tone of the speaker and the context of the conversation to infer the underlying emotion accurately.\n\nIn contrast, the other multimodal large language models struggle to provide accurate and coherent reasoning in these cases. They may focus on irrelevant details, generate inconsistent explanations, or fail to capture the nuances of the emotional expressions. The detailed scoring cases highlight the effectiveness of the Emotion-LLaMA model in performing multimodal emotion reasoning.\n\nTable  19 : Comparison of reasoning results, scores, and evaluation criteria between Emotion-LLaMA and other popular MLLMs for \"sample_00002348\". The table presents a detailed analysis of the emotional reasoning capabilities of different models on a specific data sample. It highlights the superiority of Emotion-LLaMA in generating accurate, coherent, and well-justified explanations, as evidenced by the higher scores and more comprehensive evaluation criteria.",
      "page_start": 36,
      "page_end": 37
    },
    {
      "section_name": "An Example Of Multimodal Emotion Reasoning",
      "text": "Subtitle (En) Ah. Wei Wei is pregnant.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Ground Truth",
      "text": "In the video, she found out that her friend is pregnant and he was very surprised. She spoke faster, raised her voice, and suddenly sat up from the sofa. She opened her mouth wide, widened her eyes, and raised her eyebrows. Therefore, it can be inferred that her emotional state is surprise.\n\nPandaGPT This person's emotional state appears to be positive and happy, as they are smiling while sitting on the bed. The presence of a pregnancy subtitle indicates that they are expecting a child, which is likely the source of their happiness and excitement.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Criteria",
      "text": "The prediction mentions the character sitting on the bed smiling, and a subtitle about pregnancy indicates that they are expecting a child, suggesting a positive and happy emotional state. Although the prediction does not explicitly mention the character's surprised emotional state, there is some overlap between the described positive and happy emotions and the surprised emotion in the actual description, leading to a higher prediction score. Score 7.0\n\nVideoChat-Embed Based on the video, it can be inferred that this person is clearly unhappy, as they are on the phone, shaking their head, and displaying a pained expression. The fact that they are crying indicates they are deeply troubled or under significant stress. The video mentions a pregnant woman, so this person may be upset about a recently born baby. However, without more information about the phone conversation or context, it is difficult to determine the exact emotional state.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Criteria",
      "text": "The \"Predicted Description\" about the character's emotional state has some overlap with the \"Actual Description\", as both mention their emotional condition. However, the \"Predicted Description\" also mentions other possible emotional states, such as unhappiness and pain, which do not fully align with the surprise described in the \"Actual Description\". Therefore, the score is 6. Score 6.0",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Valley",
      "text": "From the video, we can infer that this woman is in a calm and relaxed emotional state. She sits on the bed, occasionally smiling while talking on the phone. Her facial expressions and body language indicate that she feels comfortable and is enjoying the conversation. Additionally, the woman is wearing a white shirt, which contributes to a casual and relaxed appearance.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Criteria",
      "text": "The \"Actual Description\" describes the male protagonist's emotional state as surprised, whereas the \"Predicted Description\" describes the female protagonist's emotional state as calm and relaxed. The emotional states described are completely different, with very low overlap, therefore the prediction score is low. Score 3.0",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Video-Chatgpt",
      "text": "From the video, we can see that the individual is happy and excited about the news of the slight pregnancy. The woman is on the phone with a smile on her face, which indicates her happiness and excitement.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Criteria",
      "text": "The \"Actual Description\" describes the character's emotional state as surprised, while the \"Predicted Description\" describes it as happy and excited. Although the emotional states are not exactly the same, both are positive emotional states and are based on observations from the video. Therefore, it can be considered that there is some degree of overlap between the two, and the prediction score is 6. Score 6.0",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Video-Llama",
      "text": "The video shows a young woman sitting on a sofa, talking on her mobile phone. From the description, we can see that she is a young Asian female wearing a white dress. The lady is looking at the camera, which suggests that she is on a call. She appears to be laughing or smiling, indicating that she is in a good mood. The background features a white ceiling with a black clock, suggesting that the lady is in a comfortable setting. Overall, the video portrays a happy woman enjoying a phone conversation while relaxing on the sofa.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Criteria",
      "text": "According to the \"Actual Description,\" the character displays a surprised emotional state, whereas the \"Predicted Description\" describes the character's emotional state as happy and relaxed. Although there is no obvious overlap between the two, based on the descriptions, it can be speculated that the character might experience some pleasant surprises during the phone conversation. Therefore, a prediction score of 6 is given. Score 6.0\n\nEmotion-LLaMA In the video, a woman is seen sitting on a couch, displaying facial expressions and body language that suggest surprise and alertness. Her raised eyebrows, strong emotional tone, and quickened breathing further reinforce this interpretation. Therefore, it is inferred that her emotional state is surprise.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Criteria",
      "text": "The predicted description closely matches the actual description regarding the emotional state of the person, including consistent descriptions of facial expressions, tone of voice, and breathing. Therefore, the score is high. Score 9.0 Ground Truth In the video, he is dissatisfied and angry with the other person's disrespectful behavior towards Teacher Wang, and is currently criticizing the other person. The volume has been raised and the speech speed has also increased in the backend of the video. Therefore, it is inferred that his emotional state is anger.",
      "page_start": 37,
      "page_end": 38
    },
    {
      "section_name": "Pandagpt",
      "text": "The individual appears to be depressed or frustrated as they sit in a chair and talk on the phone with someone, hand on their forehead. The subtitles suggest they are criticizing their teacher, Mr. Wang, and expressing dissatisfaction with his teaching methods. This implies that the person may be facing some challenges or disagreements with their teacher, leading to their negative emotional state.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Criteria",
      "text": "The predicted description of the emotional state overlaps somewhat with the actual description of the emotional state, both mentioning dissatisfaction and criticism towards the teacher. Although the predicted description refers to feelings of depression or frustration, which differ from the anger mentioned in the actual description, both express dissatisfaction with the teaching methods. Therefore, the prediction of the emotional state in the predicted description can be considered accurate. Due to the high degree of overlap, a high score is awarded. Score 7.0\n\nVideoChat-Embed From the video, we can infer the person's emotional state by observing their facial expressions and tone of voice. The individual appears confident and assertive during the conversation with the teacher, indicating that they are neither overly anxious nor uncomfortable. On the other hand, the teacher feels frustrated by the student's lack of cooperation and professionalism during the dialogue.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Criteria",
      "text": "The predicted description mentions that the individual appears confident and assertive during the conversation with the teacher, which partially overlaps with the actual description where the character is dissatisfied and angry with the disrespectful behavior towards Mr. Wang. Additionally, the predicted description also notes the teacher's frustration due to the student's lack of cooperation and professionalism in the dialogue, which aligns to some extent with the emotional state described in the actual context. Therefore, given the considerable overlap between the predicted and actual descriptions, a score of 7 is assigned. Score 7.0",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Valley",
      "text": "From the video, we can infer that the man is likely in a professional setting, possibly a doctor's office, as he is dressed in a white shirt and tie. He is also looking at a clipboard, which suggests he might be reviewing some medical records or notes. Moreover, the video shows the man conversing with a woman, who could be a patient or colleague, and there is a computer and a chair in the room. Based on these visual cues, we can deduce that the man's emotional state is likely calm, focused, and professional, as he is engaged in a conversation and appears to be handling medical or work-related tasks.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Criteria",
      "text": "The predicted description of the emotional state partially overlaps with the actual description, as both mention the man's emotional state. In the actual description, he is angry, while in the predicted description, he is calm, focused, and professional. Although the two descriptions are not entirely consistent, both involve emotional states, so a certain score can be justified. Score 7.0",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Video-Chatgpt",
      "text": "The person in the video appears to be an older man who is conversing with a younger woman. He is dressed in a white coat and seated across from her. The man talks to the woman in a calm manner, while she listens attentively.\n\nBased on these visual cues, we can infer that the man is likely a doctor or a professional, and the woman is either his patient or colleague. The man converses with the woman in a professional manner, indicating that he is providing her with medical advice or discussing professional matters. Solely based on visual clues, it is not possible to determine the man's emotional state, but it is clear that he is engaging in a conversation with the woman.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Criteria",
      "text": "Based on the emotional state described in the \"Actual Description,\" where the character feels dissatisfied and angry, and the \"Predicted Description,\" which does not mention the character's emotional state, it is impossible to determine the character's true emotional state. The overlap is minimal, hence a score of 5 is assigned. Score 5.0\n\nVideo-LLaMA This video showcases a man and a woman in a professional office setting. The woman is speaking to the man, who appears to be a doctor, discussing a case she is handling. They are seated next to a table covered with documents. The woman seems unhappy or stressed while speaking. Throughout the video, the sound of a clock ticking can be heard in the background. The scene is shot in a medium frame with both individuals sitting by the table ...",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Criteria",
      "text": "According to the \"Actual Description,\" the character is angry, whereas the \"Predicted Description\" does not clearly define the character's emotional state but merely describes their conversation in a professional office setting. Therefore, there is a low degree of overlap between the predicted and actual descriptions, resulting in a score of 6. Score 6.0\n\nEmotion-LLaMA In the video, a doctor sits at a desk, displaying facial expressions-a furrowed brow, raised upper eyelids-that suggest anger or frustration. Along with a tone that implies annoyance or irritation, his words, \"How can you talk to Mr. Wang like that?\" convey a sense of disapproval and disappointment. This indicates that the person in the video is experiencing anger due to the other person's misbehavior. Therefore, it is inferred that his emotional state is anger.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Criteria",
      "text": "The predicted description of the character's emotional state highly overlaps with the actual description, including aspects such as facial expressions, tone, and words, all conveying feelings of anger. Due to this significant overlap, a high prediction score is awarded. Score 9.0",
      "page_start": 41,
      "page_end": 41
    },
    {
      "section_name": "C.3 Demonstration Of Emotion-Llama",
      "text": "To showcase the capabilities and potential applications of the Emotion-LLaMA model, we have developed an interactive demo that allows users to experience its emotion recognition and reasoning functionalities firsthand. The demo provides a user-friendly interface for inputting multimodal data, such as images, videos, and text, and receiving real-time emotion predictions and explanations.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "C.3.1 General Task Performance",
      "text": "Figure  12  illustrates the demo's performance on general tasks, such as face detection and question answering. These tasks demonstrate the versatility and robustness of the Emotion-LLaMA model beyond its core emotion understanding capabilities. In the face detection task, the demo takes an input image and accurately identifies and localizes the faces present in the image. The model's ability to detect faces is crucial for subsequent emotion recognition, as it allows the system to focus on the relevant regions of interest and extract facial features effectively. The demo also showcases the model's question-answering capabilities. Given a question and an associated image or video, the Emotion-LLaMA model can generate accurate and contextually relevant answers. By leveraging its multimodal understanding, the model can reason about the visual content and provide informative responses to user queries. These general task examples highlight the Emotion-LLaMA model's ability to handle a wide range of tasks that involve visual perception, language understanding, and reasoning. The model's performance on these tasks demonstrates its potential to be integrated into various applications, such as intelligent assistants and human-computer interaction interfaces.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "C.3.2 Multimodal Emotion Recognition And Reasoning",
      "text": "Figure  13  focuses on the core functionalities of the Emotion-LLaMA model: multimodal emotion recognition and reasoning. The demo allows users to input various combinations of visual, audio, and textual data and receive real-time emotion predictions and explanations. In the emotion recognition task, the user can provide an image or video depicting a person's facial expressions, body language, and contextual cues. The Emotion-LLaMA model processes the visual input, extracting relevant features and patterns, and predicts the most likely emotion category based on its trained knowledge.\n\nThe demo displays the predicted emotion label along with the corresponding confidence score, indicating the model's level of certainty in its prediction. This information helps users interpret the model's output and assess the reliability of the emotion recognition result.\n\nIn addition to emotion recognition, the demo enables users to explore the model's emotion reasoning capabilities. By providing a multimodal input, such as a video clip accompanied by audio and text, users can request the Emotion-LLaMA model to generate a natural language explanation for its predicted emotion. The model analyzes the multimodal data, considering the facial expressions, vocal cues, and linguistic content, and generates a coherent and human-like explanation for the identified emotion. The generated explanation highlights the specific cues and patterns that contributed to the model's prediction, providing insights into its reasoning process. The emotion reasoning feature of the demo is particularly valuable for applications that require transparent and interpretable emotion understanding. By providing explanations alongside the predicted emotions, the Emotion-LLaMA model enables users to gain a deeper understanding of the factors influencing the model's decisions and fosters trust in the system's outputs.\n\nThe demo also allows users to compare the Emotion-LLaMA model's performance with other baseline models or human annotations. By presenting the emotion predictions and explanations from multiple sources side by side, users can assess the model's accuracy, coherence, and alignment with human judgments. This comparative analysis feature of the demo facilitates the evaluation and validation of the Emotion-LLaMA model's performance in real-world scenarios. It provides a platform for researchers, developers, and end-users to explore the model's strengths, identify areas for improvement, and gather insights for further refinement and adaptation.",
      "page_start": 41,
      "page_end": 41
    },
    {
      "section_name": "C.3.3 Potential Applications And Impact",
      "text": "The Emotion-LLaMA demo serves as a powerful showcase of the model's capabilities and highlights its potential applications across various domains. Some of the key areas where the Emotion-LLaMA model can make a significant impact include:   Is it a boy or a girl in the video?\n\nBoy.\n\nWhat color of clothes is the boy wearing in the video?\n\nRed.\n\n[detection] face.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "What Expression?",
      "text": "The boy in the video is wearing a red jacket and has a surprised expression on his face, with his mouth open and eyes wide.\n\nIn what state is the person in the video, say the following: \"Do you really think so?\"\n\nIn the video, the person is sitting on a chair, looking at someone or something, and their facial expression is neutral, with their eyebrows slightly raised the tone of their voice is questioning, as they say \"do you really think so?\" This suggests that they are engaged in a conversation or discussion, possibly asking for clarification or seeking confirmation about something.\n\n[audio] Analyze the speaker's voice in the video.\n\nIn the video, the speaker's tone sounds normal.\n\n[visual] What are the emotions of the woman in the video?\n\nIn the video, the woman's facial expressions and actions convey a sense of happiness and contentment. She is standing in a bathroom, likely enjoying her time alone or engaging in personal grooming, and her smile suggests that she is feeling relaxed. NeurIPS Paper Checklist Justification: The paper addresses the limitations associated with the generation of instruction-follow data, specifically noting that a small subset of the samples exhibits mismatches between emotional cues. Guidelines: Sec. 3.1, Fiture 4\n\n• The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. • The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\n• The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. • The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. • The authors should reflect on the factors that influence the performance of the approach.\n\nFor example, a facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or a speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. • The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. • If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. • While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, a worse outcome might be that reviewers discover limitations that aren't acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations.",
      "page_start": 40,
      "page_end": 43
    },
    {
      "section_name": "Theory Assumptions And Proofs",
      "text": "Question: For each theoretical result, does the paper provide the full set of assumptions and a complete (and correct) proof?\n\nAnswer: [NA] Justification: The paper includes formula derivation of the model's trainable parameters and the principles of model training. The question regarding theoretical results is marked as not applicable (NA) because the paper primarily focuses on how to construct instruction-follow data and how to construct instructions during training. Guidelines: Sec. 3.1, Sec. 3.3\n\n• The answer NA means that the paper does not include theoretical results.\n\n• All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. • All assumptions should be clearly stated or referenced in the statement of any theorems.\n\n• The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide a short proof sketch to provide intuition. • Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. • Theorems and Lemmas that the proof relies upon should be properly referenced.",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "Experimental Result Reproducibility",
      "text": "Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)?\n\nAnswer: [Yes] Justification: The paper fully discloses all information necessary to reproduce the main experimental results. It includes detailed explanations of the experimental procedures, covering aspects such as hyperparameters and instruction templates. Guidelines: Sec. 3.3, Sec. 4.2\n\n• The answer NA means that the paper does not include experiments.\n\n• If the paper includes experiments, a No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. • If the contribution is a dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. • Depending on the contribution, reproducibility can be accomplished in various ways.\n\nFor example, if the contribution is a novel architecture, describing the architecture fully might suffice, or if the contribution is a specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to a hosted model (e.g., in the case of a large language model), releasing of a model checkpoint, or other means that are appropriate to the research performed. • While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility.\n\nIn the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. • It should be clear whether the error bar is the standard deviation or the standard error of the mean. • It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis of Normality of errors is not verified. • For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). • If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text.",
      "page_start": 44,
      "page_end": 46
    },
    {
      "section_name": "Experiments Compute Resources",
      "text": "Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments?\n\nAnswer: [Yes]\n\nJustification: The paper provides detailed information about the computing resources used in the experiments. It specifies the types of compute workers (e.g., GPU models), the memory requirements, and the execution times for each training step.\n\nGuidelines: Sec. 4.2\n\n• The answer NA means that the paper does not include experiments.\n\n• The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. • The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. • The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didn't make it into the paper).",
      "page_start": 46,
      "page_end": 46
    },
    {
      "section_name": "Code Of Ethics",
      "text": "Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\n\nAnswer:  [Yes]  Justification: This work adheres fully to the NeurIPS Code of Ethics. All datasets employed in the research conform to standards on Copyright and Fair Use, ensuring ethical compliance in terms of data acquisition and utilization. This careful adherence supports the integrity and ethical standards expected in scientific research.\n\nGuidelines: Sec. 4.1\n\n• The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\n• If the authors answer No, they should explain the special circumstances that require a deviation from the Code of Ethics. • The authors should make sure to preserve anonymity (e.g., if there is a special consideration due to laws or regulations in their jurisdiction).",
      "page_start": 46,
      "page_end": 47
    },
    {
      "section_name": "Broader Impacts",
      "text": "Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed?\n\nAnswer: [Yes]\n\nJustification: The paper discusses the potential positive societal impacts of the research, particularly in the fields of human-computer interaction and intelligent education. It explores how the advancements in these areas can enhance user experience and educational outcomes, reflecting on the benefits that the technology can bring to society.\n\nGuidelines: Sec. 1\n\n• The answer NA means that there is no societal impact of the work performed.\n\n• If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. • Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. • The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is a direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that a generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. • The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. • If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how a system learns from feedback over time, improving the efficiency and accessibility of ML).",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Safeguards",
      "text": "Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have a high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)?\n\nAnswer:  [Yes]  Justification: In this work, while generating emotion-related descriptions for unlabeled samples, samples associated with political content and those potentially containing discriminatory material were intentionally filtered out to prevent misuse and negative impacts.\n\nGuidelines: Sec. 3.1\n\n• The answer NA means that the paper poses no such risks.\n\n• Released models that have a high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. • Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. • We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make a best faith effort.\n\n12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected?\n\nAnswer:  [Yes]  Justification: In this work, all open-source models and datasets utilized adhere to their respective licenses. These resources are appropriately credited within the paper, with explicit mentions of their licenses and terms of use.\n\nGuidelines: Sec. 3.2, Sec. 4.1\n\n• The answer NA means that the paper does not use existing assets.\n\n• The authors should cite the original paper that produced the code package or dataset.\n\n• The authors should state which version of the asset is used and, if possible, include a URL. • The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\n• For scraped data from a particular source (e.g., website), the copyright and terms of service of that source should be provided. • If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of a dataset. • For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. • If this information is not available online, the authors are encouraged to reach out to the asset's creators.",
      "page_start": 47,
      "page_end": 48
    },
    {
      "section_name": "New Assets",
      "text": "Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets?\n\nAnswer:  [Yes]  Justification: In this work, the newly generated instruction-follow data is thoroughly documented, including details about its structure, purpose, and the methods used for generation and collection. Comprehensive statistical analyses have been conducted, and all relevant documentation has been made openly available.\n\nGuidelines: Sec. 3.1\n\n• The answer NA means that the paper does not release new assets.\n\n• Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. • The paper should discuss whether and how consent was obtained from people whose asset is used. • At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. • We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. • For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review.",
      "page_start": 48,
      "page_end": 49
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: First, human faces are extracted from each video frame",
      "page": 3
    },
    {
      "caption": "Figure 1: Example of the MERR dataset: It includes audio tone description, lexical subtitle, visual",
      "page": 3
    },
    {
      "caption": "Figure 2: , comprises an audio encoder Eaud, a visual encoder Evis, and a multimodal large language",
      "page": 3
    },
    {
      "caption": "Figure 2: Architecture of Emotion-LLaMA, which integrates audio, visual, and text inputs for",
      "page": 5
    },
    {
      "caption": "Figure 3: Visualization of the output probability distribution for multimodal emotion recognition by",
      "page": 9
    },
    {
      "caption": "Figure 3: displays the recognition results of Emotion-LLaMA in comparison to other models. The",
      "page": 9
    },
    {
      "caption": "Figure 4: and Figure 5. The dataset includes nine emotion categories: neutral, happy, angry, worried,",
      "page": 18
    },
    {
      "caption": "Figure 6: , our approach involves analyzing the amplitude values of the AUs to identify the “emotional",
      "page": 19
    },
    {
      "caption": "Figure 7: presents the distribution of video durations in the MERR",
      "page": 20
    },
    {
      "caption": "Figure 4: Distribution and analysis of Action Units (AUs) in the MERR dataset. The top part displays",
      "page": 23
    },
    {
      "caption": "Figure 5: Comparative analysis of label distributions across the DFEW, MER2023, EMER, and",
      "page": 24
    },
    {
      "caption": "Figure 6: Overview of the video expression annotation process using Action Units (AUs). The top",
      "page": 24
    },
    {
      "caption": "Figure 7: Distribution of video durations in the MERR dataset. The histogram illustrates the range",
      "page": 26
    },
    {
      "caption": "Figure 8: presents the confusion matrices for the MER2023 and DFEW datasets, providing insights",
      "page": 33
    },
    {
      "caption": "Figure 8: Confusion matrices for multimodal emotion recognition datasets. The figure presents",
      "page": 33
    },
    {
      "caption": "Figure 9: presents additional ablation study results, providing further insights into the impact of",
      "page": 34
    },
    {
      "caption": "Figure 9: compare the performance of the model across different learning rates. It is observed that the choice",
      "page": 34
    },
    {
      "caption": "Figure 9: Ablation study results. (a) illustrates the impact of different learning rates on the model’s",
      "page": 34
    },
    {
      "caption": "Figure 10: Figure 10: Ranking and Scores of Teams in the MER-Noise Track of MER2024.",
      "page": 35
    },
    {
      "caption": "Figure 12: illustrates the demo’s performance on general tasks, such as face detection and question",
      "page": 39
    },
    {
      "caption": "Figure 13: focuses on the core functionalities of the Emotion-LLaMA model: multimodal emotion",
      "page": 39
    },
    {
      "caption": "Figure 11: Online demo interface of the Emotion-LLaMA model. The figure presents a screenshot",
      "page": 40
    },
    {
      "caption": "Figure 12: Detailed examples of general tasks performed by the Emotion-LLaMA model. The figure",
      "page": 41
    },
    {
      "caption": "Figure 13: Detailed examples of multimodal emotion recognition and reasoning performed by",
      "page": 42
    }
  ],
  "tables": [
    {
      "caption": "Table 4: Performance (%) of Multimodal Large",
      "data": [
        {
          "Emotion-LLaMA": "Emotion-LLaMA",
          "A,V": "A,V,T",
          "0.8905": "0.9036"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "Jean-Baptiste Alayrac",
        "Jeff Donahue",
        "Pauline Luc",
        "Antoine Miech",
        "Iain Barr",
        "Yana Hasson",
        "Karel Lenc",
        "Arthur Mensch",
        "Katherine Millican",
        "Malcolm Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "2",
      "title": "Jmi at semeval 2024 task 3: Two-step approach for multimodal ecac using in-context learning with gpt and instructiontuned llama models",
      "authors": [
        "Mohammed Abbas Ansari",
        "Chandni Saxena",
        "Tanvir Ahmad"
      ],
      "year": "2024",
      "venue": "Jmi at semeval 2024 task 3: Two-step approach for multimodal ecac using in-context learning with gpt and instructiontuned llama models",
      "arxiv": "arXiv:2403.04798"
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Anas Awadalla",
        "Irena Gao",
        "Joshua Gardner",
        "Jack Hessel",
        "Yusuf Hanafy",
        "Wanrong Zhu",
        "Yonatan Kalyani Marathe",
        "Samir Bitton",
        "Jenia Gadre",
        "Simon Jitsev",
        "Pang Kornblith",
        "Gabriel Wei Koh",
        "Mitchell Ilharco",
        "Ludwig Wortsman",
        "Schmidt",
        "Openflamingo"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Shusheng Yang",
        "Shijie Wang",
        "Sinan Tan",
        "Peng Wang",
        "Junyang Lin",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-vl: A frontier large vision-language model with versatile abilities",
      "arxiv": "arXiv:2308.12966"
    },
    {
      "citation_id": "6",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "Psychological counseling and character analysis algorithm based on image emotion",
      "authors": [
        "Min Cao",
        "Zhendong Wan"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "8",
      "title": "Semi-supervised multimodal emotion recognition with class-balanced pseudo-labeling",
      "authors": [
        "Haifeng Chen",
        "Chujia Guo",
        "Yan Li",
        "Peng Zhang",
        "Dongmei Jiang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Improving multimodal emotion recognition by leveraging acoustic adaptation and visual alignment",
      "authors": [
        "Haifeng Chen",
        "Xi Li",
        "Dongmei Jiang",
        "Lei Xie"
      ],
      "year": "2024",
      "venue": "Improving multimodal emotion recognition by leveraging acoustic adaptation and visual alignment",
      "arxiv": "arXiv:2409.05015"
    },
    {
      "citation_id": "10",
      "title": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "authors": [
        "Jun Chen",
        "Deyao Zhu",
        "Xiaoqian Shen",
        "Xiang Li",
        "Zechun Liu",
        "Pengchuan Zhang",
        "Raghuraman Krishnamoorthi",
        "Vikas Chandra",
        "Yunyang Xiong",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-v2: large language model as a unified interface for vision-language multi-task learning",
      "arxiv": "arXiv:2310.09478"
    },
    {
      "citation_id": "11",
      "title": "Shikra: Unleashing multimodal llm's referential dialogue magic",
      "authors": [
        "Keqin Chen",
        "Zhao Zhang",
        "Weili Zeng",
        "Richong Zhang",
        "Feng Zhu",
        "Rui Zhao"
      ],
      "year": "2023",
      "venue": "Shikra: Unleashing multimodal llm's referential dialogue magic",
      "arxiv": "arXiv:2306.15195"
    },
    {
      "citation_id": "12",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Yin Chen",
        "Jia Li",
        "Shiguang Shan",
        "Meng Wang",
        "Richang Hong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Semi-supervised multimodal emotion recognition with expression mae",
      "authors": [
        "Zebang Cheng",
        "Yuxiang Lin",
        "Zhaoru Chen",
        "Xiang Li",
        "Shuyi Mao",
        "Fan Zhang",
        "Daijun Ding",
        "Bowen Zhang",
        "Xiaojiang Peng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "14",
      "title": "Mips at semeval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models",
      "authors": [
        "Zebang Cheng",
        "Fuqiang Niu",
        "Yuxiang Lin",
        "Zhi-Qi Cheng",
        "Bowen Zhang",
        "Xiaojiang Peng"
      ],
      "year": "2024",
      "venue": "Mips at semeval-2024 task 3: Multimodal emotion-cause pair extraction in conversations with multimodal language models",
      "arxiv": "arXiv:2404.00511"
    },
    {
      "citation_id": "15",
      "title": "Sztu-cmu at mer2024: Improving emotion-llama with conv-attention for multimodal emotion recognition",
      "authors": [
        "Zebang Cheng",
        "Shuyuan Tu",
        "Dawei Huang",
        "Minghan Li",
        "Xiaojiang Peng",
        "Zhi-Qi Cheng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Sztu-cmu at mer2024: Improving emotion-llama with conv-attention for multimodal emotion recognition",
      "arxiv": "arXiv:2408.10500"
    },
    {
      "citation_id": "16",
      "title": "Gsrformer: Grounded situation recognition transformer with alternate semantic attention refinement",
      "authors": [
        "Zhi-Qi Cheng",
        "Qi Dai",
        "Siyao Li",
        "Teruko Mitamura",
        "Alexander Hauptmann"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Video ecommerce: Towards online video advertising",
      "authors": [
        "Zhi-Qi Cheng",
        "Yang Liu",
        "Xiao Wu",
        "Xian-Sheng Hua"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM international conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Video ecommerce++: Toward large scale online video advertising",
      "authors": [
        "Zhi-Qi Cheng",
        "Xiao Wu",
        "Yang Liu",
        "Xian-Sheng Hua"
      ],
      "year": "2017",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "19",
      "title": "Video2shop: Exact matching clothes in videos to online shopping images",
      "authors": [
        "Zhi-Qi Cheng",
        "Xiao Wu",
        "Yang Liu",
        "Xian-Sheng Hua"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "20",
      "title": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality",
      "authors": [
        "Wei-Lin Chiang",
        "Zhuohan Li",
        "Zi Lin",
        "Ying Sheng",
        "Zhanghao Wu",
        "Hao Zhang",
        "Lianmin Zheng",
        "Siyuan Zhuang",
        "Yonghao Zhuang",
        "Joseph Gonzalez",
        "Ion Stoica",
        "Eric Xing"
      ],
      "year": "2023",
      "venue": "Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality"
    },
    {
      "citation_id": "21",
      "title": "Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2022",
      "venue": "Scaling language modeling with pathways",
      "arxiv": "arXiv:2204.02311"
    },
    {
      "citation_id": "22",
      "title": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "authors": [
        "Yunfei Chu",
        "Jin Xu",
        "Xiaohuan Zhou",
        "Qian Yang",
        "Shiliang Zhang",
        "Zhijie Yan",
        "Chang Zhou",
        "Jingren Zhou"
      ],
      "year": "2023",
      "venue": "Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models",
      "arxiv": "arXiv:2311.07919"
    },
    {
      "citation_id": "23",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Eric Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "arxiv": "arXiv:2210.11416"
    },
    {
      "citation_id": "24",
      "title": "Revisiting pre-trained models for chinese natural language processing",
      "authors": [
        "Yiming Cui",
        "Wanxiang Che",
        "Ting Liu",
        "Bing Qin",
        "Shijin Wang",
        "Guoping Hu"
      ],
      "year": "2020",
      "venue": "Revisiting pre-trained models for chinese natural language processing",
      "arxiv": "arXiv:2004.13922"
    },
    {
      "citation_id": "25",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "26",
      "title": "Learning aligned audiovisual representations for multimodal sentiment analysis",
      "authors": [
        "Chaoyue Ding",
        "Daoming Zong",
        "Baoxiang Li",
        "Ken Zheng",
        "Dinghao Zhou",
        "Jiakui Li",
        "Qunyan Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st International Workshop on Multimodal and Responsible Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "PaLM-E: An embodied multimodal language model",
      "authors": [
        "Danny Driess",
        "Fei Xia",
        "S Mehdi",
        "Corey Sajjadi",
        "Aakanksha Lynch",
        "Brian Chowdhery",
        "Ayzaan Ichter",
        "Jonathan Wahid",
        "Quan Tompson",
        "Tianhe Vuong",
        "Yu"
      ],
      "year": "2023",
      "venue": "PaLM-E: An embodied multimodal language model",
      "arxiv": "arXiv:2303.03378"
    },
    {
      "citation_id": "28",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "29",
      "title": "Leveraging contrastive learning and self-training for multimodal emotion recognition with limited labeled samples",
      "authors": [
        "Qi Fan",
        "Yutong Li",
        "Yi Xin",
        "Xinyu Cheng",
        "Guanglai Gao",
        "Miao Ma"
      ],
      "year": "2024",
      "venue": "Leveraging contrastive learning and self-training for multimodal emotion recognition with limited labeled samples",
      "arxiv": "arXiv:2409.04447"
    },
    {
      "citation_id": "30",
      "title": "Learning noise-robust joint representation for multimodal emotion recognition under realistic incomplete data scenarios",
      "authors": [
        "Qi Fan",
        "Haolin Zuo",
        "Rui Liu",
        "Zheng Lian",
        "Guanglai Gao"
      ],
      "year": "2023",
      "venue": "Learning noise-robust joint representation for multimodal emotion recognition under realistic incomplete data scenarios",
      "arxiv": "arXiv:2311.16114"
    },
    {
      "citation_id": "31",
      "title": "Lssed: a large-scale dataset and benchmark for speech emotion recognition",
      "authors": [
        "Weiquan Fan",
        "Xiangmin Xu",
        "Xiaofen Xing",
        "Weidong Chen",
        "Dongyan Huang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "32",
      "title": "Exploring the limits of masked visual representation learning at scale",
      "authors": [
        "Yuxin Fang",
        "Wen Wang",
        "Binhui Xie",
        "Quan Sun",
        "Ledell Wu",
        "Xinggang Wang",
        "Tiejun Huang",
        "Xinlong Wang",
        "Yue Cao",
        "Eva"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Video emotion open-vocabulary recognition based on multimodal large language model",
      "authors": [
        "Mengying Ge",
        "Dongkai Tang",
        "Mingyang Li"
      ],
      "year": "2024",
      "venue": "Video emotion open-vocabulary recognition based on multimodal large language model",
      "arxiv": "arXiv:2408.11286"
    },
    {
      "citation_id": "34",
      "title": "Imagebind: One embedding space to bind them all",
      "authors": [
        "Rohit Girdhar",
        "Alaaeldin El-Nouby",
        "Zhuang Liu",
        "Mannat Singh",
        "Kalyan Vasudev Alwala",
        "Armand Joulin",
        "Ishan Misra"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "35",
      "title": "Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models",
      "authors": [
        "Yuxiang Guo",
        "Faizan Siddiqui",
        "Yang Zhao",
        "Rama Chellappa",
        "Shao-Yuan Lo",
        "Stimuvar"
      ],
      "year": "2024",
      "venue": "Spatiotemporal stimuli-aware video affective reasoning with multimodal large language models",
      "arxiv": "arXiv:2409.00304"
    },
    {
      "citation_id": "36",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "37",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "38",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2017",
      "venue": "2017 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "39",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "40",
      "title": "Language is not all you need: Aligning perception with language models",
      "authors": [
        "Shaohan Huang",
        "Li Dong",
        "Wenhui Wang",
        "Yaru Hao",
        "Saksham Singhal",
        "Shuming Ma",
        "Tengchao Lv",
        "Lei Cui",
        "Owais Khan Mohammed",
        "Qiang Liu"
      ],
      "year": "2023",
      "venue": "Language is not all you need: Aligning perception with language models",
      "arxiv": "arXiv:2302.14045"
    },
    {
      "citation_id": "41",
      "title": "Beyond sentiment analysis: A review of recent trends in text based sentiment analysis and emotion detection",
      "authors": [
        "Lai Po",
        "Suraya Alias"
      ],
      "year": "2023",
      "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics"
    },
    {
      "citation_id": "42",
      "title": "Emotion recognition, emotion expression, and cultural display rules: Implications for counseling",
      "authors": [
        "Ashley Hutchison",
        "Larry Gerstein"
      ],
      "year": "2017",
      "venue": "Journal of Asia Pacific Counseling"
    },
    {
      "citation_id": "43",
      "title": "A survey of emotion recognition methods with emphasis on e-learning environments",
      "authors": [
        "Maryam Imani",
        "Gholam Ali Montazer"
      ],
      "year": "2019",
      "venue": "Journal of Network and Computer Applications"
    },
    {
      "citation_id": "44",
      "title": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "authors": [
        "Srinivasan Iyer",
        "Xi Victoria Lin",
        "Ramakanth Pasunuru",
        "Todor Mihaylov",
        "Dániel Simig",
        "Ping Yu",
        "Kurt Shuster",
        "Tianlu Wang",
        "Qing Liu",
        "Punit Singh Koura"
      ],
      "year": "2022",
      "venue": "Opt-iml: Scaling language model instruction meta learning through the lens of generalization",
      "arxiv": "arXiv:2212.12017"
    },
    {
      "citation_id": "45",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "46",
      "title": "Grounding language models to images for multimodal generation",
      "authors": [
        "Jing Yu Koh",
        "Ruslan Salakhutdinov",
        "Daniel Fried"
      ],
      "year": "2023",
      "venue": "Grounding language models to images for multimodal generation",
      "arxiv": "arXiv:2301.13823"
    },
    {
      "citation_id": "47",
      "title": "Large raw emotional dataset with aggregation mechanism",
      "authors": [
        "Vladimir Kondratenko",
        "Artem Sokolov",
        "Nikolay Karpov",
        "Oleg Kutuzov",
        "Nikita Savushkin",
        "Fyodor Minkin"
      ],
      "year": "2022",
      "venue": "Large raw emotional dataset with aggregation mechanism",
      "arxiv": "arXiv:2212.12266"
    },
    {
      "citation_id": "48",
      "title": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "authors": [
        "Taku Kudo",
        "John Richardson"
      ],
      "year": "2018",
      "venue": "Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing",
      "arxiv": "arXiv:1808.06226"
    },
    {
      "citation_id": "49",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "Shanglin Lei",
        "Guanting Dong",
        "Xiaoping Wang",
        "Keheng Wang",
        "Sirui Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "50",
      "title": "Large vision-language models as emotion recognizers in context awareness",
      "authors": [
        "Yuxuan Lei",
        "Dingkang Yang",
        "Zhaoyu Chen",
        "Jiawei Chen",
        "Peng Zhai",
        "Lihua Zhang"
      ],
      "year": "2024",
      "venue": "Large vision-language models as emotion recognizers in context awareness",
      "arxiv": "arXiv:2407.11300"
    },
    {
      "citation_id": "51",
      "title": "Mimic-it: Multi-modal in-context instruction tuning",
      "authors": [
        "Bo Li",
        "Yuanhan Zhang",
        "Liangyu Chen",
        "Jinghao Wang",
        "Fanyi Pu",
        "Jingkang Yang",
        "Chunyuan Li",
        "Ziwei Liu"
      ],
      "year": "2023",
      "venue": "Mimic-it: Multi-modal in-context instruction tuning",
      "arxiv": "arXiv:2306.05425"
    },
    {
      "citation_id": "52",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "Hanting Li",
        "Hongjing Niu",
        "Zhaoqing Zhu",
        "Feng Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "53",
      "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "54",
      "title": "Videochat: Chat-centric video understanding",
      "authors": [
        "Kunchang Li",
        "Yinan He",
        "Yi Wang",
        "Yizhuo Li",
        "Wenhai Wang",
        "Ping Luo",
        "Yali Wang",
        "Limin Wang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Videochat: Chat-centric video understanding",
      "arxiv": "arXiv:2305.06355"
    },
    {
      "citation_id": "55",
      "title": "Mvbench: A comprehensive multi-modal video understanding benchmark",
      "authors": [
        "Kunchang Li",
        "Yali Wang",
        "Yinan He",
        "Yizhuo Li",
        "Yi Wang",
        "Yi Liu",
        "Zun Wang",
        "Jilan Xu",
        "Guo Chen",
        "Ping Luo"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "56",
      "title": "Mm-tts: A unified framework for multimodal, prompt-induced emotional text-to-speech synthesis",
      "authors": [
        "Xiang Li",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Mm-tts: A unified framework for multimodal, prompt-induced emotional text-to-speech synthesis",
      "arxiv": "arXiv:2404.18398"
    },
    {
      "citation_id": "57",
      "title": "Two in one go: Single-stage emotion recognition with decoupled subjectcontext transformer",
      "authors": [
        "Xinpeng Li",
        "Teng Wang",
        "Jian Zhao",
        "Shuyi Mao",
        "Jinbao Wang",
        "Feng Zheng",
        "Xiaojiang Peng",
        "Xuelong Li"
      ],
      "year": "2024",
      "venue": "Two in one go: Single-stage emotion recognition with decoupled subjectcontext transformer",
      "arxiv": "arXiv:2404.17205"
    },
    {
      "citation_id": "58",
      "title": "Decoupled multimodal distilling for emotion recognition",
      "authors": [
        "Yong Li",
        "Yuanzhi Wang",
        "Zhen Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "59",
      "title": "Multi-label learning, modality robustness, and semisupervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "60",
      "title": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Zhuofan Wen",
        "Siyuan Zhang",
        "Shun Chen",
        "Hao Gu",
        "Jinming Zhao",
        "Ziyang Ma",
        "Xie Chen"
      ],
      "year": "2024",
      "venue": "Mer 2024: Semi-supervised learning, noise robustness, and open-vocabulary multimodal emotion recognition",
      "arxiv": "arXiv:2404.17113"
    },
    {
      "citation_id": "61",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haiyang Sun",
        "Kang Chen",
        "Zhuofan Wen",
        "Hao Gu",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "62",
      "title": "Explainable multimodal emotion reasoning",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Mingyu Xu",
        "Haiyang Sun",
        "Ke Xu",
        "Zhuofan Wen",
        "Shun Chen",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Explainable multimodal emotion reasoning",
      "arxiv": "arXiv:2306.15401"
    },
    {
      "citation_id": "63",
      "title": "Video-llava: Learning united visual representation by alignment before projection",
      "authors": [
        "Bin Lin",
        "Bin Zhu",
        "Yang Ye",
        "Munan Ning",
        "Jin Peng",
        "Li Yuan"
      ],
      "year": "2023",
      "venue": "Video-llava: Learning united visual representation by alignment before projection",
      "arxiv": "arXiv:2311.10122"
    },
    {
      "citation_id": "64",
      "title": "Llava-next: Improved reasoning, ocr, and world knowledge",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Yuheng Li",
        "Bo Li",
        "Yuanhan Zhang",
        "Sheng Shen",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Llava-next: Improved reasoning, ocr, and world knowledge"
    },
    {
      "citation_id": "65",
      "title": "Visual instruction tuning. NeurIPS",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2024",
      "venue": "Visual instruction tuning. NeurIPS"
    },
    {
      "citation_id": "66",
      "title": "A robustly optimized bert pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov",
        "Roberta"
      ],
      "year": "2019",
      "venue": "A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "67",
      "title": "Video assistant with large language model enhanced ability",
      "authors": [
        "Ruipu Luo",
        "Ziwang Zhao",
        "Min Yang",
        "Junwei Dong",
        "Minghui Qiu",
        "Pengcheng Lu",
        "Tao Wang",
        "Zhongyu Wei",
        "Valley"
      ],
      "year": "2023",
      "venue": "Video assistant with large language model enhanced ability",
      "arxiv": "arXiv:2306.07207"
    },
    {
      "citation_id": "68",
      "title": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "authors": [
        "Muhammad Maaz",
        "Hanoona Rasheed",
        "Salman Khan",
        "Fahad Shahbaz Khan"
      ],
      "year": "2023",
      "venue": "Video-chatgpt: Towards detailed video understanding via large vision and language models",
      "arxiv": "arXiv:2306.05424"
    },
    {
      "citation_id": "69",
      "title": "Affectiva-mit facial expression dataset (am-fed): Naturalistic and spontaneous facial expressions collected",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby",
        "Thibaud Senechal",
        "May Amr",
        "Jeffrey Cohn",
        "Rosalind Picard"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition workshops"
    },
    {
      "citation_id": "70",
      "title": "Patt-lite: Lightweight patch and attention mobilenet for challenging facial expression recognition",
      "authors": [
        "Jia Le Ngwe",
        "Ming Kian",
        "Chin Lim",
        "Thian Poo Lee",
        "Ong"
      ],
      "year": "2023",
      "venue": "Patt-lite: Lightweight patch and attention mobilenet for challenging facial expression recognition",
      "arxiv": "arXiv:2306.09626"
    },
    {
      "citation_id": "71",
      "title": "Gpt-4v(ision) system card",
      "year": "2023",
      "venue": "Gpt-4v(ision) system card"
    },
    {
      "citation_id": "72",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "73",
      "title": "Kosmos-2: Grounding multimodal large language models to the world",
      "authors": [
        "Zhiliang Peng",
        "Wenhui Wang",
        "Li Dong",
        "Yaru Hao",
        "Shaohan Huang",
        "Shuming Ma",
        "Furu Wei"
      ],
      "year": "2023",
      "venue": "Kosmos-2: Grounding multimodal large language models to the world",
      "arxiv": "arXiv:2306.14824"
    },
    {
      "citation_id": "74",
      "title": "Multimodal emotion recognition with vision-language prompting and modality dropout",
      "authors": [
        "Q Anbin",
        "Zhongliang Liu",
        "Xinyong Zhou",
        "Jinba Xiao",
        "Fengrun Zhang",
        "Qi Gan",
        "Ming Tao",
        "Gaozheng Zhang",
        "Lu Zhang"
      ],
      "year": "2024",
      "venue": "Multimodal emotion recognition with vision-language prompting and modality dropout",
      "arxiv": "arXiv:2409.07078"
    },
    {
      "citation_id": "75",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "76",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "Colin Raffel",
        "Noam Shazeer",
        "Adam Roberts",
        "Katherine Lee",
        "Sharan Narang",
        "Michael Matena",
        "Yanqi Zhou",
        "Wei Li",
        "Peter Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "77",
      "title": "Text-and feature-based models for compound multimodal emotion recognition in the wild",
      "authors": [
        "Nicolas Richet",
        "Soufiane Belharbi",
        "Haseeb Aslam",
        "Emilie Meike",
        "Manuela Schadt",
        "Gustave González-González",
        "Alessandro Cortal",
        "Marco Koerich",
        "Alain Pedersoli",
        "Simon Finkel",
        "Bacon"
      ],
      "year": "2024",
      "venue": "Text-and feature-based models for compound multimodal emotion recognition in the wild",
      "arxiv": "arXiv:2407.12927"
    },
    {
      "citation_id": "78",
      "title": "Neural machine translation of rare words with subword units",
      "authors": [
        "Rico Sennrich",
        "Barry Haddow",
        "Alexandra Birch"
      ],
      "year": "2015",
      "venue": "Neural machine translation of rare words with subword units",
      "arxiv": "arXiv:1508.07909"
    },
    {
      "citation_id": "79",
      "title": "Audio-guided fusion techniques for multimodal emotion analysis",
      "authors": [
        "Pujin Shi",
        "Fei Gao"
      ],
      "year": "2024",
      "venue": "Audio-guided fusion techniques for multimodal emotion analysis",
      "arxiv": "arXiv:2409.05007"
    },
    {
      "citation_id": "80",
      "title": "Human computer interaction",
      "authors": [
        "Gaurav Sinha",
        "Rahul Shahi",
        "Mani Shankar"
      ],
      "year": "2010",
      "venue": "2010 3rd International Conference on Emerging Trends in Engineering and Technology"
    },
    {
      "citation_id": "81",
      "title": "One model to instruction-follow them all",
      "authors": [
        "Yixuan Su",
        "Tian Lan",
        "Huayang Li",
        "Jialu Xu",
        "Yan Wang",
        "Deng Cai",
        "Pandagpt"
      ],
      "year": "2023",
      "venue": "One model to instruction-follow them all",
      "arxiv": "arXiv:2305.16355"
    },
    {
      "citation_id": "82",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "83",
      "title": "Towards generic hearing abilities for large language models",
      "authors": [
        "Changli Tang",
        "Wenyi Yu",
        "Guangzhi Sun",
        "Xianzhao Chen",
        "Tian Tan",
        "Wei Li",
        "Lu Lu",
        "Zejun Ma",
        "Chao Zhang",
        "Salmonn"
      ],
      "year": "2023",
      "venue": "Towards generic hearing abilities for large language models",
      "arxiv": "arXiv:2310.13289"
    },
    {
      "citation_id": "84",
      "title": "Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training",
      "authors": [
        "Zhan Tong",
        "Yibing Song",
        "Jue Wang",
        "Limin Wang"
      ],
      "year": "2022",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "85",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "Hugo Touvron",
        "Louis Martin",
        "Kevin Stone",
        "Peter Albert",
        "Amjad Almahairi",
        "Yasmine Babaei",
        "Nikolay Bashlykov",
        "Soumya Batra",
        "Prajjwal Bhargava",
        "Shruti Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "86",
      "title": "Implicit temporal modeling with learnable alignment for video recognition",
      "authors": [
        "Shuyuan Tu",
        "Qi Dai",
        "Zuxuan Wu",
        "Zhi-Qi Cheng",
        "Han Hu",
        "Yu-Gang Jiang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "87",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Shijian Lu",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "88",
      "title": "Large language model is also an open-ended decoder for vision-centric tasks",
      "authors": [
        "Wenhai Wang",
        "Zhe Chen",
        "Xiaokang Chen",
        "Jiannan Wu",
        "Xizhou Zhu",
        "Gang Zeng",
        "Ping Luo",
        "Tong Lu",
        "Jie Zhou",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Large language model is also an open-ended decoder for vision-centric tasks",
      "arxiv": "arXiv:2305.11175"
    },
    {
      "citation_id": "89",
      "title": "Self-instruct: Aligning language model with self generated instructions",
      "authors": [
        "Yizhong Wang",
        "Yeganeh Kordi",
        "Swaroop Mishra",
        "Alisa Liu",
        "Noah Smith",
        "Daniel Khashabi",
        "Hannaneh Hajishirzi"
      ],
      "year": "2022",
      "venue": "Self-instruct: Aligning language model with self generated instructions",
      "arxiv": "arXiv:2212.10560"
    },
    {
      "citation_id": "90",
      "title": "Benchmarking generalization via in-context instructions on 1,600+ language tasks",
      "authors": [
        "Yizhong Wang",
        "Swaroop Mishra",
        "Pegah Alipoormolabashi",
        "Yeganeh Kordi",
        "Amirreza Mirzaei",
        "Anjana Arunkumar",
        "Arjun Ashok",
        "Arut Selvan Dhanasekaran",
        "Atharva Naik",
        "David Stap"
      ],
      "year": "2022",
      "venue": "Benchmarking generalization via in-context instructions on 1,600+ language tasks",
      "arxiv": "arXiv:2204.07705"
    },
    {
      "citation_id": "91",
      "title": "Incomplete multimodality-diffused emotion recognition",
      "authors": [
        "Yuanzhi Wang",
        "Yong Li",
        "Zhen Cui"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "92",
      "title": "Emovit: Revolutionizing emotion insights with visual instruction tuning",
      "authors": [
        "Hongxia Xie",
        "Chu-Jun Peng",
        "Yu-Wen Tseng",
        "Hung-Jen Chen",
        "Chan-Feng Hsu",
        "Hong-Han Shuai",
        "Wen-Huang Cheng"
      ],
      "year": "2024",
      "venue": "Emovit: Revolutionizing emotion insights with visual instruction tuning",
      "arxiv": "arXiv:2404.16670"
    },
    {
      "citation_id": "93",
      "title": "Emoset: A large-scale visual emotion dataset with rich attributes",
      "authors": [
        "Jingyuan Yang",
        "Qirui Huang",
        "Tingting Ding",
        "Dani Lischinski",
        "Danny Cohen-Or",
        "Hui Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "94",
      "title": "mplug-owl: Modularization empowers large language models with multimodality",
      "authors": [
        "Qinghao Ye",
        "Haiyang Xu",
        "Guohai Xu",
        "Jiabo Ye",
        "Ming Yan",
        "Yiyang Zhou",
        "Junyang Wang",
        "Anwen Hu",
        "Pengcheng Shi",
        "Yaya Shi"
      ],
      "year": "2023",
      "venue": "mplug-owl: Modularization empowers large language models with multimodality",
      "arxiv": "arXiv:2304.14178"
    },
    {
      "citation_id": "95",
      "title": "Mammoth: Building math generalist models through hybrid instruction tuning",
      "authors": [
        "Xiang Yue",
        "Xingwei Qu",
        "Ge Zhang",
        "Yao Fu",
        "Wenhao Huang",
        "Huan Sun",
        "Yu Su",
        "Wenhu Chen"
      ],
      "year": "2023",
      "venue": "Mammoth: Building math generalist models through hybrid instruction tuning",
      "arxiv": "arXiv:2309.05653"
    },
    {
      "citation_id": "96",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "97",
      "title": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "authors": [
        "Hang Zhang",
        "Xin Li",
        "Lidong Bing"
      ],
      "year": "2023",
      "venue": "Video-llama: An instruction-tuned audio-visual language model for video understanding",
      "arxiv": "arXiv:2306.02858"
    },
    {
      "citation_id": "98",
      "title": "Time-sensitive multimodal emotion recognition with microexpression dynamics in video dialogues",
      "authors": [
        "Liyun Zhang",
        "Microemo"
      ],
      "year": "2024",
      "venue": "Time-sensitive multimodal emotion recognition with microexpression dynamics in video dialogues",
      "arxiv": "arXiv:2407.16552"
    },
    {
      "citation_id": "99",
      "title": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "authors": [
        "Renrui Zhang",
        "Jiaming Han",
        "Aojun Zhou",
        "Xiangfei Hu",
        "Shilin Yan",
        "Pan Lu",
        "Hongsheng Li",
        "Peng Gao",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Llama-adapter: Efficient fine-tuning of language models with zero-init attention",
      "arxiv": "arXiv:2303.16199"
    },
    {
      "citation_id": "100",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "Sitao Zhang",
        "Yimu Pan",
        "James Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "101",
      "title": "OPT: Open pre-trained transformer language models",
      "authors": [
        "Susan Zhang",
        "Stephen Roller",
        "Naman Goyal",
        "Mikel Artetxe",
        "Moya Chen",
        "Shuohui Chen",
        "Christopher Dewan",
        "Mona Diab",
        "Xian Li",
        "Xi Victoria Lin"
      ],
      "year": "2022",
      "venue": "OPT: Open pre-trained transformer language models",
      "arxiv": "arXiv:2205.01068"
    },
    {
      "citation_id": "102",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "103",
      "title": "Exploring emotion features and fusion strategies for audio-video emotion recognition",
      "authors": [
        "Hengshun Zhou",
        "Debin Meng",
        "Yuanyuan Zhang",
        "Xiaojiang Peng",
        "Jun Du",
        "Kai Wang",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "2019 International conference on multimodal interaction"
    },
    {
      "citation_id": "104",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    }
  ]
}