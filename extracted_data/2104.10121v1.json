{
  "paper_id": "2104.10121v1",
  "title": "On The Impact Of Word Error Rate On Acoustic-Linguistic Speech Emotion Recognition: An Update For The Deep Learning Era",
  "published": "2021-04-20T17:10:01Z",
  "authors": [
    "Shahin Amiriparian",
    "Artem Sokolov",
    "Ilhan Aslan",
    "Lukas Christ",
    "Maurice Gerczuk",
    "Tobias Hübner",
    "Dmitry Lamanov",
    "Manuel Milling",
    "Sandra Ottl",
    "Ilya Poduremennykh",
    "Evgeniy Shuranov",
    "Björn W. Schuller"
  ],
  "keywords": [
    "emotion recognition",
    "automatic speech recognition",
    "computational paralinguistics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Text encodings from automatic speech recognition (ASR) transcripts and audio representations have shown promise in speech emotion recognition (SER) ever since. Yet, it is challenging to explain the effect of each information stream on the SER systems. Further, more clarification is required for analysing the impact of ASR's word error rate (WER) on linguistic emotion recognition per se and in the context of fusion with acoustic information exploitation in the age of deep ASR systems. In order to tackle the above issues we create transcripts from the original speech by applying three modern ASR systems, including an end-toend model trained with recurrent neural network-transducer loss, a model with connectionist temporal classification loss, and a WAV2VEC framework for self-supervised learning. Afterwards, we use pre-trained textual models to extract text representations from the ASR outputs and the gold standard. For extraction and learning of acoustic speech features, we utilise OPENSMILE, OPENXBOW, DEEPSPECTRUM, and AUDEEP. Finally, we conduct decision-level fusion on both information streams -acoustics and linguistics. Using the best development configuration, we achieve state-of-the-art unweighted average recall values of 73.6 % and 73.8 % on the speaker-independent development and test partitions of IEMOCAP, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As technology is becoming increasingly ubiquitous, speech input is gaining popularity as an accessible interaction modality. The rise of voice assistants, e. g., Amazon's Alexa, exemplifies this trend. While today's technologies may understand speech commands well, the conversation quality is still far from what we as humans experience in interpersonal communication. Emotional expressions are a key part of interpersonal communication. They are embodied in our gestures, body posture, and speech. Humans typically express and recognise emotional speech effortlessly, while, for machines, recognising emotions in speech is still a hard challenge.\n\nIn this paper, we present an update to previous research (i. e.,  [1] ) on the trade-off between automatic speech recogni-tion (ASR) accuracy (i. e., Word error rate (WER)) and linguistic emotion recognition, and the impact thereof on the later fusion with voice based emotion recognition. Such an update is urgently required, as I) most papers analysing the fusion of acoustics and linguistics use human transcripts and not actual ASR (e. g.,  [2] ,  [3] ,  [4] ), hence, oversimplifying the problem, and II) practically no systematic investigation of the WER on linguistic speech emotion recognition (SER) exists, besides  [1]  -however, more than a decade since this investigation has witnessed massive improvements in ASR in the era of deep ASR approaches, and III) the modelling of linguistic information itself has changed dramatically with the advent of deep text modelling and the existence of large pre-trained according models. Hence, a reinvestigation is urgently needed. To provide a comprehensive and ecologically valuable overview, we juxtapose and contrast variations of contemporary solutions for ASR and feature sets for emotion recognition from both text and voice. We highlight the best performing fusion solution, that to the best of our knowledge sets a new state-of-the-art. Further, we describe in detail the overall system that we use for our experiments. Moreover, we provide different variations of every system's component.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we introduce feature extraction methods that are well suited to process acoustic and linguistic cues. The features are used as inputs to Support Vector Machines (SVMs) and, therefore, build the basis for our SER analysis. We further introduce several ASR approaches, which will be investigated with respect to their WER and corresponding suitability in the SER context.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Audio Features",
      "text": "We examine four different audio feature sets. The first feature set is extracted with the OPENSMILE toolkit using the Com-ParE_2016.conf configuration file  [5] . It contains 6 373 static features resulting from the computation of functionals (statistics) over low-level descriptor (LLD) contours 1  [5, 6] . A full description of the feature set can be found in  [7] .\n\nIn addition to the default Computational Paralinguistics Challenge (ComParE) feature set, we provide Bag-of-Audio-Words (BoAW) features by using OPENXBOW  [8] . These have been applied successfully for, e. g., acoustic event detection  [9]  and speech-based emotion recognition  [10] . After a quantisation based on a codebook, audio chunks are represented as histograms of acoustic LLDs. One codebook is learnt for 65 LLDs from the COMPARE feature set, and another one for 65 deltas of these LLDs. Codebook generation is done by random sampling from the LLDs and its deltas in the training data. Each LLD and delta is assigned to 10 audio words from the codebooks with the lowest Euclidean distance. Subsequently, both BoAW representations are concatenated. Finally, a logarithmic term frequency weighting is applied to compress the numeric range of the histograms.\n\nThe feature extraction DEEP SPECTRUM toolkit 2  is applied to obtain deep representations from the input audio data utilising pre-trained Convolutional Neural Networks (CNNs)  [11] . DEEP SPECTRUM features have been shown to be effective, e. g., for speech processing  [12, 13]  and sentiment analysis  [14] . First, audio signals are transformed into Mel-spectrogram plots using a Hanning window of width 32 ms and an overlap of 16 ms. From these, 128 Mel frequency bands are computed. The spectrograms are then forwarded through a pre-trained DENSENET121  [15]  and the activations from the 'avg_pool' layer are extracted, resulting in a 1 024 dimensional feature vector.\n\nAnother feature set is obtained through unsupervised representation learning with recurrent sequence-to-sequence autoencoders, using AUDEEP 3    [16, 17] . This feature set models the inherently sequential nature of audio with Recurrent Neural Networks (RNNs) within the encoder and decoder networks  [16, 17] . First, Mel-scale spectrograms are extracted from the raw waveforms. In order to eliminate some background noise, power levels are clipped below four different given thresholds in these spectrograms. The number of thresholds results in four separate sets of spectrograms per data set. Subsequently, a distinct recurrent sequence-to-sequence autoencoder is trained on each of these sets of spectrograms in an unsupervised way, i. e., without any label information. The learnt representations of a spectrogram are then extracted as feature vectors for the corresponding instance. Finally, these feature vectors are concatenated to obtain the final feature vector. For the results shown in Table  1 , the autoencoders' hyperparameters are not fine tuned.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Text Features",
      "text": "DeepMoji, proposed by Felbo et al.  [18] , is a model pre-trained for emotion-related text classification tasks. It consists of two bidirectional long short-term memory (LSTM) layers, followed by an attention layer and yields a sentence encoding of length 2 304. Even though DeepMoji is pre-trained on emotional tweets only, the authors show that it also performs well for other kinds of emotional text data, e. g., reports of emotional experiences. We extract DeepMoji sentence encodings via the PyTorch implementation TorchMoji  4  .\n\nMoreover, we employ several variants of Bidirectional Encoder Representations from Transformers (BERT)  [19]  that has set new standards for many text processing tasks in recent years. In its base configuration, BERT consists of 12 transformer (  [20] ) encoder layers. This network is pre-trained on large text data sets using two unsupervised language modelling tasks, namely masked word prediction and next sentence prediction. Here, we employ the pre-trained BERT-base model to obtain sentence encodings. The output of the last layer's hidden state for the special token [CLS], followed by one tanh-activated linear layer (pooler_output) is considered as the sentence encoding.\n\nALBERT (A Lite BERT)  [21]  is a popular variant of BERT. It is of the same size as the original BERT model but comes with considerably less parameters due to parameter sharing across layers and factorisation of the embedding matrix. Furthermore, the next sentence prediction task in BERT's pre-training has been replaced by sentence order prediction, i. e., deciding whether two sentences are given in the correct order. ALBERT has been shown to outperform BERT on many tasks. Similar to our BERT baseline, we take the pooler_output of ALBERT in its base version as our sentence encoding.\n\nAnother recent variant of the BERT language model (LM) is given by ELECTRA  [22] , referring to an alternative method of pre-training transformer language models. In this approach, corrupted input words are detected. First, a generator model corrupts the input sentence. Then, the discriminator, i. e., the actual language model, predicts for every word whether it has been changed by the generator or not. BERT-like transformer networks pre-trained in this fashion outperform other BERT variants on several tasks. The architecture of the model is nearly identical to BERT-base. We take the embedding of the special token [CLS] as the sentence encoding. For all three BERT variants, huggingface implementations and pre-trained weights 5  6 7 are used to extract 768 features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Automatic Speech Recognition",
      "text": "To obtain text encodings from audio waveforms, we employ several pre-trained ASR systems: a system based on QuartzNet (QN), streaming Transformer Transducer (TT) and wav2vec (W2V). Transformer Transducer (TT)  [23]  is an end-to-end model trained with RNN-Transducer (RNN-T) loss  [24] . Its encoder implementation entails Transformer blocks with multiheaded self-attention masking future context making the network suitable for stream audio processing. The label encoder of this architecture can be interpreted as a small built-in internal LM as it takes the previous predicted output label as input. The joint network combines audio and label encoder outputs and passes them to the the final softmax. Our solution is trained on LibriSpeech  [25] , CommonVoice  [26] , and Tedlium  [27] . Additionally, we utilise our internal audio sets with various eastern accents for model fine-tuning. In total, about 4 000 hours of speech are used for TT training. For tokenisation, we use the Byte-Pair Encoding (BPE)  [28]  sentence-piece model with a vocabulary size of 4 096 items. Despite the fact that the system has an internal LM, we additionally evaluate our model in combination with an external LM based on the transformer architecture. The LM is trained on 30 gigabytes of corpora that include wiki texts and books. Furthermore, cold fusion is used to connect the outputs of the LM to the external LM with the lambda parameter set to 0.2.\n\nQuartzNet (QN)  [29]  is a Connectionist Temporal Classification (CTC)  [30]  loss based model composed of blocks with separable convolutions and residual connections between them, with a fully connected decoder at the end. The model has fewer parameters than TT while still showing near state-of-the-art accuracy. In our experiments, first, a pre-trained configuration with 15 blocks and 5 sub-blocks in each block is used. Subsequently, we fine-tune it on the dataset with British accents and recordings generated by a text-to-speech (TTS) system. For training the QN model, we employ around 2 000 hours of internal and public datasets. Unlike TT, we set up the configuration to predict graphemes. By default, the CTC loss does not consider the use of a built-in LM. We use a 4-gram statistical language model learnt on Gigaword  [31]  and fuse it with QN during the inference decoding.\n\nThe wav2vec (W2V)  [32]  system is a new framework for self-supervised training. The model can be broken down into three parts: i) a feature encoder, which represents speech waveforms in latent states that concurrently goes further to a contextualised representation part, ii) a quantisation module, iii) a contextualised representation joined with Transformers learns the relative positional information in the latent states. A quantization module represents the infinite output of a feature encoder to a discrete set via product quantization. The framework exploits the CTC loss for training. We use a pre-trained large model from the official repository 8 . First, we choose the checkpoint obtained by the training on 60 k hours of LibriVox, subsequently, we finetune it with additional 960 hours of LibriSpeech. For the external language model, we fuse W2V, which is the same as for QN.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "The Interactive Emotional Dyadic Motion Capture (IEMOCAP) dataset  [33]  is an English emotion dataset that comes with both textual transcriptions and raw audio information. The dataset contains scripted and improvised dialogues between 5 female and 5 male speakers. In order to be consistent with previous research with IEMOCAP, we choose the main emotions happiness (fused with excitement), sadness, anger, and neutral. The choice of emotions results in 5 531 utterances totaling 7.0 hours of audio data. In literature, there is no agreement on the partitioning of the dataset. In our experiments, we split the IEMOCAP dataset speaker-independently into session 1 -3 for training, session 4 for development, and session 5 for testing.\n\nLibriSpeech  [25]  is a publicly available and popular dataset for speech recognition system experiments and evaluations. It includes transcriptions for around 960 hours of public domain audio books dictated by many speakers. For our experiments, we use test-clean part which is about 4.5 hours of audio with 20 male and 20 female speakers. 8 https://github.com/pytorch/fairseq/tree/master/examples/wav2vec",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Automatic Speech Recognition",
      "text": "We evaluate each of our models on LibriSpeech test-clean with and without the external LM to compare the accuracy. We measure Word error rate (WER) and character error rate (CER). As two models are adopted for different accents, state-of-theart results for the chosen data are not expected. On the contrary, the W2V trained on data with the same distribution as LibriSpeech and setups with this model show more precise predictions for both datasets. The evaluation on test-clean compares the accuracy of models on public and well-known data. Our measurements for IEMOCAP and LibriSpeech test-clean are demonstrated in Table  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Late Fusion",
      "text": "We apply late fusion in form of majority voting on the basis of SVM predictions on the individual feature sets (cf. Figure  1 ). We evaluate all combinations of at least three feature sets from the audio, Gold Standard (GS) human transcribed spoken content, and ASR-based spoken content transcription. Table  4  summarises our results. Taking into account a high number of feature set combinations, we restrict our evaluation of the ASR-based text features to the ASR system with the best overall baseline performance which is W2V with LM. Feature set combinations, based on the GS text only, clearly outperform those, which are based on ASR-generated text. Considering the fact that this result is in line with performance of the individual feature sets (showed in Table 1), most likely, this behaviour is caused by the WER of the ASR system. However, when combining audio and text features, both ASR-based and GS-based feature sets lead to a similar performance, being higher than any individual information stream, both on average and for the best performance. Combinations of ASR-based and GS-based features show no improvement in the average Unweighted Average Recall (UAR) compared to the GS feature sets, most likely due to the similarity of the features. When considering all combinations of available feature sets the average and best performance can be further increased, which can most reasonably be explained by the vast number of combinations. Several combinations consisting of audio, GS-TEXT and ASR-TEXT features -including the combination GS-BERT, GS-DEEPMOJI, ASR-BERT, ASR-DEEPMOJI, ASR-ELECTRA, AUDEEP, DEEPSPECTRUM, OPENSMILE, BOAW -achieve the best observed performance of 73.6 % UAR on the development set and a corresponding UAR of 73.8 % on the test set.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Discussion",
      "text": "When comparing the performance of different ASR systems in Table  2  and Table  1 , a correlation between low WER values and high UAR values becomes obvious. Accordingly the Gold Standard System using human annotations, which is considered to have a much lower WER than any of the ASR systems, clearly achieves the highest UAR. A similar effect has previously been observed in  [1] , however, utilising a -from today's point of view -outdated ASR systems with a much more limited vocabulary size. Table  4  suggests that a higher number of considered feature sets leads to a higher UAR on average. This effect is known in general, however, it should be noted that the pairwise dependence of classifiers plays a considerable role in such a late fusion system  [34] . A pairwise dependence of ASR-based and GSbased feature sets could therefore explain why combinations of both sets perform worse than combinations which combine either ASR-based or GS-based feature sets with audio-based feature sets. Assuming a high dependence between ASR-based and GS-based feature sets would further suggest that a wellsuited weighted fusion method combining only audio-based and ASR-based features might further increase results towards the best-performing configuration introduced in 3.3, which combines two instances of BERT and DeepMoji features.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we presented current ASR systems to create transcriptions for the linguistic SER. Without adapting the ASR systems to the target database IEMOCAP, we were able to achieve state-of-the-art results by fusing acoustic and linguistic information. We further observed that higher WERs on the ASR systems lead to higher UAR values for emotion recognition.\n\nFor future work, the number of feature sets and the respective feature set sizes can be reduced in order to increase computational efficiency. Furthermore, evaluation could be performed on more natural or in-the-wild databases. We mainly chose IEMOCAP as it is widely established and thereby suitable for comparison with state-of-the-art approaches. Moreover, IEMO-CAP contains transcriptions making it easier to evaluate the impact of WER achieved by the ASR on the final emotion classification. Finally, the fusion with ASR could be implemented on the levels of the embeddings instead of the text.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgements",
      "text": "",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "manuel.milling@informatik.uni-augsburg.de,\nsandra.ottl@informatik.uni-augsburg.de,"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "poduremennykh.ilya@huawei.com,\nevgeniy.shuranov@huawei.com,\nschuller@ieee.org"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "tion (ASR) accuracy (i. e., Word error rate (WER)) and linguistic\nAbstract"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "emotion recognition, and the impact thereof on the later fusion"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "Text encodings from automatic speech recognition (ASR) tran-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "with voice based emotion recognition. Such an update is urgently"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "scripts and audio representations have shown promise in speech"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "required, as I) most papers analysing the fusion of acoustics"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "emotion recognition (SER) ever since. Yet, it is challenging to"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "and linguistics use human transcripts and not actual ASR (e. g.,"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "explain the effect of each information stream on the SER systems."
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "[2], [3], [4]), hence, oversimplifying the problem, and II) practi-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "Further, more clariﬁcation is required for analysing the impact of"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "cally no systematic investigation of the WER on linguistic speech"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "ASR’s word error rate (WER) on linguistic emotion recognition"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "emotion recognition (SER) exists, besides [1] – however, more"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "per se and in the context of fusion with acoustic information"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "than a decade since this investigation has witnessed massive"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "exploitation in the age of deep ASR systems. In order to tackle"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "improvements in ASR in the era of deep ASR approaches, and"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "the above issues we create transcripts from the original speech"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "III) the modelling of linguistic information itself has changed"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "by applying three modern ASR systems,\nincluding an end-to-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "dramatically with the advent of deep text modelling and the"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "end model trained with recurrent neural network-transducer loss,"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "existence of large pre-trained according models. Hence, a re-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "a model with connectionist\ntemporal classiﬁcation loss, and a"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "investigation is urgently needed. To provide a comprehensive"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "WAV2VEC framework for self-supervised learning. Afterwards,"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "and ecologically valuable overview, we juxtapose and contrast"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "we use pre-trained textual models to extract text representations"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "variations of contemporary solutions for ASR and feature sets"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "from the ASR outputs and the gold standard. For extraction and"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "for emotion recognition from both text and voice. We highlight"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "learning of acoustic speech features, we utilise OPENSMILE,"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "the best performing fusion solution, that to the best of our knowl-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "OPENXBOW, DEEPSPECTRUM, and AUDEEP. Finally, we con-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "edge sets a new state-of-the-art. Further, we describe in detail"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "duct decision-level fusion on both information streams – acous-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "the overall system that we use for our experiments. Moreover,"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "tics and linguistics. Using the best development conﬁguration,"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "we provide different variations of every system’s component."
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "we achieve state-of-the-art unweighted average recall values of"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "73.6 % and 73.8 % on the speaker-independent development and"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "2. Methodology\ntest partitions of IEMOCAP, respectively."
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "Index Terms: emotion recognition, automatic speech recogni-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "In this section, we introduce feature extraction methods that"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "tion, computational paralinguistics"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "are well suited to process acoustic and linguistic cues.\nThe"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "features are used as inputs to Support Vector Machines (SVMs)"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "1.\nIntroduction\nand, therefore, build the basis for our SER analysis. We further"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "introduce several ASR approaches, which will be investigated"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "As technology is becoming increasingly ubiquitous, speech input"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "with respect to their WER and corresponding suitability in the"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "is gaining popularity as an accessible interaction modality. The"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "SER context."
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "rise of voice assistants, e. g., Amazon’s Alexa, exempliﬁes this"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "trend. While today’s technologies may understand speech com-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "2.1. Audio Features"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "mands well, the conversation quality is still far from what we as"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "humans experience in interpersonal communication. Emotional"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "We examine four different audio feature sets. The ﬁrst feature"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "expressions are a key part of interpersonal communication. They"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "set\nis extracted with the OPENSMILE toolkit using the Com-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "are embodied in our gestures, body posture, and speech. Humans"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "ParE_2016.conf conﬁguration ﬁle [5].\nIt contains 6 373 static"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "typically express and recognise emotional speech effortlessly,"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "features resulting from the computation of functionals (statis-"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "while, for machines, recognising emotions in speech is still a"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "tics) over low-level descriptor (LLD) contours1\n[5, 6]. A full"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "hard challenge."
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "description of the feature set can be found in [7]."
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "In this paper, we present an update to previous research"
        },
        {
          "tobias.huebner@informatik.uni-augsburg.de,\nlamanov.dmitry@huawei.com,": "1https://github.com/audeering/opensmile\n(i. e., [1]) on the trade-off between automatic speech recogni-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In addition to the default Computational Paralinguistics": "Challenge (ComParE)\nfeature set, we provide Bag-of-Audio-",
          "masked word prediction and next sentence prediction. Here,": "we employ the pre-trained BERT-base model to obtain sentence"
        },
        {
          "In addition to the default Computational Paralinguistics": "Words (BoAW) features by using OPENXBOW [8]. These have",
          "masked word prediction and next sentence prediction. Here,": "encodings. The output of the last\nlayer’s hidden state for the"
        },
        {
          "In addition to the default Computational Paralinguistics": "been applied successfully for, e. g., acoustic event detection [9]",
          "masked word prediction and next sentence prediction. Here,": "special token [CLS], followed by one tanh-activated linear layer"
        },
        {
          "In addition to the default Computational Paralinguistics": "and speech-based emotion recognition [10]. After a quantisation",
          "masked word prediction and next sentence prediction. Here,": "(pooler_output) is considered as the sentence encoding."
        },
        {
          "In addition to the default Computational Paralinguistics": "based on a codebook, audio chunks are represented as histograms",
          "masked word prediction and next sentence prediction. Here,": "ALBERT (A Lite BERT) [21] is a popular variant of BERT."
        },
        {
          "In addition to the default Computational Paralinguistics": "of acoustic LLDs. One codebook is learnt for 65 LLDs from",
          "masked word prediction and next sentence prediction. Here,": "It is of the same size as the original BERT model but comes with"
        },
        {
          "In addition to the default Computational Paralinguistics": "the COMPARE feature set, and another one for 65 deltas of",
          "masked word prediction and next sentence prediction. Here,": "considerably less parameters due to parameter sharing across"
        },
        {
          "In addition to the default Computational Paralinguistics": "these LLDs. Codebook generation is done by random sampling",
          "masked word prediction and next sentence prediction. Here,": "layers and factorisation of the embedding matrix. Furthermore,"
        },
        {
          "In addition to the default Computational Paralinguistics": "from the LLDs and its deltas in the training data. Each LLD",
          "masked word prediction and next sentence prediction. Here,": "the next sentence prediction task in BERT’s pre-training has been"
        },
        {
          "In addition to the default Computational Paralinguistics": "and delta is assigned to 10 audio words from the codebooks",
          "masked word prediction and next sentence prediction. Here,": "replaced by sentence order prediction,\ni. e., deciding whether"
        },
        {
          "In addition to the default Computational Paralinguistics": "with the lowest Euclidean distance. Subsequently, both BoAW",
          "masked word prediction and next sentence prediction. Here,": "two sentences are given in the correct order. ALBERT has been"
        },
        {
          "In addition to the default Computational Paralinguistics": "representations are concatenated. Finally, a logarithmic term",
          "masked word prediction and next sentence prediction. Here,": "shown to outperform BERT on many tasks. Similar to our BERT"
        },
        {
          "In addition to the default Computational Paralinguistics": "frequency weighting is applied to compress the numeric range",
          "masked word prediction and next sentence prediction. Here,": "baseline, we take the pooler_output of ALBERT in its base"
        },
        {
          "In addition to the default Computational Paralinguistics": "of the histograms.",
          "masked word prediction and next sentence prediction. Here,": "version as our sentence encoding."
        },
        {
          "In addition to the default Computational Paralinguistics": "The feature extraction DEEP SPECTRUM toolkit2 is applied",
          "masked word prediction and next sentence prediction. Here,": "Another recent variant of the BERT language model (LM)"
        },
        {
          "In addition to the default Computational Paralinguistics": "to obtain deep representations from the input audio data util-",
          "masked word prediction and next sentence prediction. Here,": "is given by ELECTRA [22], referring to an alternative method"
        },
        {
          "In addition to the default Computational Paralinguistics": "ising pre-trained Convolutional Neural Networks (CNNs) [11].",
          "masked word prediction and next sentence prediction. Here,": "of pre-training transformer language models. In this approach,"
        },
        {
          "In addition to the default Computational Paralinguistics": "DEEP SPECTRUM features have been shown to be effective, e. g.,",
          "masked word prediction and next sentence prediction. Here,": "corrupted input words are detected. First, a generator model"
        },
        {
          "In addition to the default Computational Paralinguistics": "for speech processing [12, 13] and sentiment analysis [14]. First,",
          "masked word prediction and next sentence prediction. Here,": "corrupts the input sentence. Then,\nthe discriminator,\ni. e.,\nthe"
        },
        {
          "In addition to the default Computational Paralinguistics": "audio signals are transformed into Mel-spectrogram plots using a",
          "masked word prediction and next sentence prediction. Here,": "actual language model, predicts for every word whether it has"
        },
        {
          "In addition to the default Computational Paralinguistics": "Hanning window of width 32 ms and an overlap of 16 ms. From",
          "masked word prediction and next sentence prediction. Here,": "been changed by the generator or not. BERT-like transformer"
        },
        {
          "In addition to the default Computational Paralinguistics": "these, 128 Mel frequency bands are computed. The spectrograms",
          "masked word prediction and next sentence prediction. Here,": "networks pre-trained in this fashion outperform other BERT"
        },
        {
          "In addition to the default Computational Paralinguistics": "are then forwarded through a pre-trained DENSENET121 [15]",
          "masked word prediction and next sentence prediction. Here,": "variants on several tasks. The architecture of the model is nearly"
        },
        {
          "In addition to the default Computational Paralinguistics": "and the activations from the ‘avg_pool’ layer are extracted, re-",
          "masked word prediction and next sentence prediction. Here,": "identical to BERT-base. We take the embedding of the special"
        },
        {
          "In addition to the default Computational Paralinguistics": "sulting in a 1 024 dimensional feature vector.",
          "masked word prediction and next sentence prediction. Here,": "token [CLS] as the sentence encoding.\nFor all\nthree BERT"
        },
        {
          "In addition to the default Computational Paralinguistics": "Another feature set is obtained through unsupervised repre-",
          "masked word prediction and next sentence prediction. Here,": "variants, huggingface implementations and pre-trained weights 5"
        },
        {
          "In addition to the default Computational Paralinguistics": "sentation learning with recurrent sequence-to-sequence autoen-",
          "masked word prediction and next sentence prediction. Here,": "6 7 are used to extract 768 features."
        },
        {
          "In addition to the default Computational Paralinguistics": "coders, using AUDEEP3\n[16, 17]. This feature set models the",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "inherently sequential nature of audio with Recurrent Neural Net-",
          "masked word prediction and next sentence prediction. Here,": "2.3. Automatic Speech Recognition"
        },
        {
          "In addition to the default Computational Paralinguistics": "works (RNNs) within the encoder and decoder networks [16, 17].",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "To obtain text encodings from audio waveforms, we employ"
        },
        {
          "In addition to the default Computational Paralinguistics": "First, Mel-scale spectrograms are extracted from the raw wave-",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "several pre-trained ASR systems: a system based on QuartzNet"
        },
        {
          "In addition to the default Computational Paralinguistics": "forms.\nIn order\nto eliminate some background noise, power",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "(QN),\nstreaming Transformer Transducer\n(TT) and wav2vec"
        },
        {
          "In addition to the default Computational Paralinguistics": "levels are clipped below four different given thresholds in these",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "(W2V).\nTransformer Transducer\n(TT)\n[23]\nis an end-to-end"
        },
        {
          "In addition to the default Computational Paralinguistics": "spectrograms. The number of thresholds results in four separate",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "model\ntrained with RNN-Transducer\n(RNN-T)\nloss [24].\nIts"
        },
        {
          "In addition to the default Computational Paralinguistics": "sets of spectrograms per data set. Subsequently, a distinct re-",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "encoder implementation entails Transformer blocks with multi-"
        },
        {
          "In addition to the default Computational Paralinguistics": "current sequence-to-sequence autoencoder is trained on each of",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "headed self-attention masking future context making the network"
        },
        {
          "In addition to the default Computational Paralinguistics": "these sets of spectrograms in an unsupervised way, i. e., without",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "suitable for stream audio processing. The label encoder of this"
        },
        {
          "In addition to the default Computational Paralinguistics": "any label information. The learnt representations of a spectro-",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "architecture can be interpreted as a small built-in internal LM"
        },
        {
          "In addition to the default Computational Paralinguistics": "gram are then extracted as feature vectors for the corresponding",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "as it\ntakes the previous predicted output\nlabel as input.\nThe"
        },
        {
          "In addition to the default Computational Paralinguistics": "instance. Finally, these feature vectors are concatenated to obtain",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "joint network combines audio and label encoder outputs and"
        },
        {
          "In addition to the default Computational Paralinguistics": "the ﬁnal feature vector. For the results shown in Table 1,\nthe",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "passes them to the the ﬁnal softmax. Our solution is trained"
        },
        {
          "In addition to the default Computational Paralinguistics": "autoencoders’ hyperparameters are not ﬁne tuned.",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "on LibriSpeech [25], CommonVoice [26], and Tedlium [27]."
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "Additionally, we utilise our\ninternal audio sets with various"
        },
        {
          "In addition to the default Computational Paralinguistics": "2.2. Text Features",
          "masked word prediction and next sentence prediction. Here,": ""
        },
        {
          "In addition to the default Computational Paralinguistics": "",
          "masked word prediction and next sentence prediction. Here,": "eastern accents for model ﬁne-tuning.\nIn total, about 4 000"
        },
        {
          "In addition to the default Computational Paralinguistics": "DeepMoji, proposed by Felbo et al.\n[18], is a model pre-trained",
          "masked word prediction and next sentence prediction. Here,": "hours of speech are used for TT training. For tokenisation, we"
        },
        {
          "In addition to the default Computational Paralinguistics": "for emotion-related text classiﬁcation tasks.\nIt consists of two",
          "masked word prediction and next sentence prediction. Here,": "use the Byte-Pair Encoding (BPE) [28] sentence-piece model"
        },
        {
          "In addition to the default Computational Paralinguistics": "bidirectional long short-term memory (LSTM) layers, followed",
          "masked word prediction and next sentence prediction. Here,": "with a vocabulary size of 4 096 items. Despite the fact that the"
        },
        {
          "In addition to the default Computational Paralinguistics": "by an attention layer and yields a sentence encoding of length",
          "masked word prediction and next sentence prediction. Here,": "system has an internal LM, we additionally evaluate our model"
        },
        {
          "In addition to the default Computational Paralinguistics": "2 304. Even though DeepMoji is pre-trained on emotional tweets",
          "masked word prediction and next sentence prediction. Here,": "in combination with an external LM based on the transformer"
        },
        {
          "In addition to the default Computational Paralinguistics": "only, the authors show that it also performs well for other kinds",
          "masked word prediction and next sentence prediction. Here,": "architecture. The LM is trained on 30 gigabytes of corpora that"
        },
        {
          "In addition to the default Computational Paralinguistics": "of emotional\ntext data, e. g., reports of emotional experiences.",
          "masked word prediction and next sentence prediction. Here,": "include wiki texts and books. Furthermore, cold fusion is used"
        },
        {
          "In addition to the default Computational Paralinguistics": "We extract DeepMoji sentence encodings via the PyTorch imple-",
          "masked word prediction and next sentence prediction. Here,": "to connect\nthe outputs of the LM to the external LM with the"
        },
        {
          "In addition to the default Computational Paralinguistics": "mentation TorchMoji4.",
          "masked word prediction and next sentence prediction. Here,": "lambda parameter set to 0.2."
        },
        {
          "In addition to the default Computational Paralinguistics": "Moreover, we employ several variants of Bidirectional En-",
          "masked word prediction and next sentence prediction. Here,": "QuartzNet (QN) [29] is a Connectionist Temporal Classiﬁ-"
        },
        {
          "In addition to the default Computational Paralinguistics": "coder Representations from Transformers (BERT) [19] that has",
          "masked word prediction and next sentence prediction. Here,": "cation (CTC) [30] loss based model composed of blocks with"
        },
        {
          "In addition to the default Computational Paralinguistics": "set new standards for many text processing tasks in recent years.",
          "masked word prediction and next sentence prediction. Here,": "separable convolutions and residual connections between them,"
        },
        {
          "In addition to the default Computational Paralinguistics": "In its base conﬁguration, BERT consists of 12 transformer ([20])",
          "masked word prediction and next sentence prediction. Here,": "with a fully connected decoder at the end. The model has fewer"
        },
        {
          "In addition to the default Computational Paralinguistics": "encoder layers. This network is pre-trained on large text data",
          "masked word prediction and next sentence prediction. Here,": "parameters than TT while still showing near state-of-the-art ac-"
        },
        {
          "In addition to the default Computational Paralinguistics": "sets using two unsupervised language modelling tasks, namely",
          "masked word prediction and next sentence prediction. Here,": "curacy. In our experiments, ﬁrst, a pre-trained conﬁguration with"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: SER comparison of linguistic features on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus (Chance level:": "25.0 % UAR). Every text feature extractor is tested with different ASR systems as input as well as with the gold standard (GS). UAR:"
        },
        {
          "Table 1: SER comparison of linguistic features on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus (Chance level:": ""
        },
        {
          "Table 1: SER comparison of linguistic features on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus (Chance level:": ""
        },
        {
          "Table 1: SER comparison of linguistic features on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus (Chance level:": "Dev"
        },
        {
          "Table 1: SER comparison of linguistic features on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus (Chance level:": "61.7"
        },
        {
          "Table 1: SER comparison of linguistic features on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus (Chance level:": "57.6"
        },
        {
          "Table 1: SER comparison of linguistic features on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus (Chance level:": "47.9"
        },
        {
          "Table 1: SER comparison of linguistic features on the Interactive Emotional Dyadic Motion Capture (IEMOCAP) corpus (Chance level:": "56.9"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "15 blocks and 5 sub-blocks in each block is used. Subsequently,",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "3.2. Automatic Speech Recognition"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "we ﬁne-tune it on the dataset with British accents and recordings",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "We evaluate each of our models on LibriSpeech test-clean with"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "generated by a text-to-speech (TTS) system. For training the",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "and without the external LM to compare the accuracy. We mea-"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "QN model, we employ around 2 000 hours of internal and pub-",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "sure Word error\nrate (WER) and character error\nrate (CER)."
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "lic datasets. Unlike TT, we set up the conﬁguration to predict",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "As two models are adopted for different accents, state-of-the-"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "graphemes. By default, the CTC loss does not consider the use",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "art\nresults for\nthe chosen data are not expected. On the con-"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "of a built-in LM. We use a 4-gram statistical\nlanguage model",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "trary,\nthe W2V trained on data with the same distribution as"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "learnt on Gigaword [31] and fuse it with QN during the inference",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "LibriSpeech and setups with this model show more precise pre-"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "decoding.",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "dictions for both datasets. The evaluation on test-clean compares"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "The wav2vec (W2V) [32] system is a new framework for",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "the accuracy of models on public and well-known data. Our"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "self-supervised training. The model can be broken down into",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "measurements for\nIEMOCAP and LibriSpeech test-clean are"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "three parts:\ni) a feature encoder, which represents speech wave-",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "demonstrated in Table 2."
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "forms in latent states that concurrently goes further to a con-",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "textualised representation part, ii) a quantisation module, iii) a",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "Table 2: Evaluation results of ASRs on LibriSpeech test-clean"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "contextualised representation joined with Transformers learns",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "and IEMOCAP waveforms. WER: Word error rate. CER: Char-"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "the relative positional information in the latent states. A quantiza-",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "acter error rate."
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "tion module represents the inﬁnite output of a feature encoder to",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "a discrete set via product quantization. The framework exploits",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "SetUp\nLibriSpeech\nIEMOCAP"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "the CTC loss for\ntraining. We use a pre-trained large model",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "WER [%]\nCER [%]\nWER [%]\nCER [%]"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "from the ofﬁcial repository8. First, we choose the checkpoint",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "obtained by the training on 60 k hours of LibriVox, subsequently,",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "QN\n15.69\n7.26\n41.21\n25.43"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "we ﬁnetune it with additional 960 hours of LibriSpeech. For the",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "QN + LM\n12.98\n7.35\n42.14\n31.21"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "external\nlanguage model, we fuse W2V, which is the same as",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "TT\n4.98\n1.89\n31.81\n22.30"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "TT + LM\n4.93\n1.86\n31.47\n22.08"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "for QN.",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "2.16\n0.57\nW2V\n25.71\n13.56"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": "22.23\n13.37\nW2V + LM\n2.24\n0.63"
        },
        {
          "56.9\n56.2\nELECTRA BASE\n44.2\n43.1\n46.7\n43.2": "3. Experiments",
          "53.3\n49.9\n48.2\n45.5\n47.1\n45.6\n52.6\n49.7": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": "resulting model of DEEPSPECTRUM is a DENSENET201 with"
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": "128 Mel bins and the viridis colour map."
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": ""
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": "Dev\nTest\n[UAR %]"
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": ""
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": "57.8\nOPENSMILE (ComParE_2016)\n58.5"
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": ""
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": "OPENXBOW (N = 2 000)\n55.7\n59.1"
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": ""
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": "59.8\nDEEPSPECTRUM\n53.2"
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": ""
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": "AUDEEP (X = −60 dB)\n55.0\n53.3"
        },
        {
          "Table 3: SER Results of audio features on IEMOCAP. The best": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: and Table 1, a correlation between low WER values supportedbyRSF(RussianScienceFoundation)grant20-71-",
      "data": [
        {
          "the UAR of the best performing feature set combination on the": "development set, as well as the corresponding performance of",
          "on more natural or\nin-the-wild databases. We mainly chose": "IEMOCAP as it is widely established and thereby suitable for"
        },
        {
          "the UAR of the best performing feature set combination on the": "said combination on the test set.",
          "on more natural or\nin-the-wild databases. We mainly chose": "comparison with state-of-the-art approaches. Moreover, IEMO-"
        },
        {
          "the UAR of the best performing feature set combination on the": "",
          "on more natural or\nin-the-wild databases. We mainly chose": "CAP contains transcriptions making it easier\nto evaluate the"
        },
        {
          "the UAR of the best performing feature set combination on the": "[UAR %]",
          "on more natural or\nin-the-wild databases. We mainly chose": "impact of WER achieved by the ASR on the ﬁnal emotion clas-"
        },
        {
          "the UAR of the best performing feature set combination on the": "",
          "on more natural or\nin-the-wild databases. We mainly chose": "siﬁcation. Finally, the fusion with ASR could be implemented"
        },
        {
          "the UAR of the best performing feature set combination on the": "AUDIO",
          "on more natural or\nin-the-wild databases. We mainly chose": ""
        },
        {
          "the UAR of the best performing feature set combination on the": "",
          "on more natural or\nin-the-wild databases. We mainly chose": "on the levels of the embeddings instead of the text."
        },
        {
          "the UAR of the best performing feature set combination on the": "GS-TEXT",
          "on more natural or\nin-the-wild databases. We mainly chose": ""
        },
        {
          "the UAR of the best performing feature set combination on the": "ASR-TEXT",
          "on more natural or\nin-the-wild databases. We mainly chose": ""
        },
        {
          "the UAR of the best performing feature set combination on the": "AUDIO + GS-TEXT",
          "on more natural or\nin-the-wild databases. We mainly chose": "6. Acknowledgements"
        },
        {
          "the UAR of the best performing feature set combination on the": "AUDIO + ASR-TEXT",
          "on more natural or\nin-the-wild databases. We mainly chose": ""
        },
        {
          "the UAR of the best performing feature set combination on the": "ASR-TEXT + GS-TEXT",
          "on more natural or\nin-the-wild databases. We mainly chose": "This research was partly supported by the Affective Comput-"
        },
        {
          "the UAR of the best performing feature set combination on the": "ALL SYSTEMS",
          "on more natural or\nin-the-wild databases. We mainly chose": ""
        },
        {
          "the UAR of the best performing feature set combination on the": "",
          "on more natural or\nin-the-wild databases. We mainly chose": "ing & HCI\nInnovation Research Lab between Huawei Tech-"
        },
        {
          "the UAR of the best performing feature set combination on the": "",
          "on more natural or\nin-the-wild databases. We mainly chose": "nologies and University of Augsburg. We acknowledge fund-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: and Table 1, a correlation between low WER values supportedbyRSF(RussianScienceFoundation)grant20-71-",
      "data": [
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "features to the ASR system with the best overall baseline perfor-"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "mance which is W2V with LM. Feature set combinations, based"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "on the GS text only, clearly outperform those, which are based on"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "ASR-generated text. Considering the fact that this result is in line"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "with performance of the individual feature sets (showed in Ta-"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "ble 1), most likely, this behaviour is caused by the WER of the"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "ASR system. However, when combining audio and text features,"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "both ASR-based and GS-based feature sets lead to a similar per-"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "formance, being higher than any individual information stream,"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "both on average and for the best performance. Combinations"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "of ASR-based and GS-based features show no improvement in"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "the average Unweighted Average Recall\n(UAR) compared to"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "the GS feature sets, most likely due to the similarity of the fea-"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "tures. When considering all combinations of available feature"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "sets the average and best performance can be further increased,"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "which can most reasonably be explained by the vast number of"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "combinations. Several combinations consisting of audio, GS-"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "TEXT and ASR-TEXT features – including the combination"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "GS-BERT, GS-DEEPMOJI, ASR-BERT, ASR-DEEPMOJI,"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "ASR-ELECTRA, AUDEEP, DEEPSPECTRUM, OPENSMILE,"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "BOAW – achieve the best observed performance of 73.6 % UAR"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "on the development set and a corresponding UAR of 73.8 % on"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "the test set."
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "Table 4: Results of the majority voting late fusion. The possible"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "number of\nfeature set combinations (#), as well as the mean"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "UAR and standard deviation are reported. We further provide"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "the UAR of the best performing feature set combination on the"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "development set, as well as the corresponding performance of"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "said combination on the test set."
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "#\nMean Dev\nMax Dev\nTest\n[UAR %]"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "59.9 ± 0.1\nAUDIO\n5\n60.8\n63.3"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "60.3 ± 1.5\nGS-TEXT\n5\n61.7\n63.0"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "54.6 ± 1.0\nASR-TEXT\n5\n55.8\n56.2"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "65.0 ± 3.5\nAUDIO + GS-TEXT\n219\n71.0\n69.9"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "63.4 ± 3.2\nAUDIO + ASR-TEXT\n219\n69.5\n70.5"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "58.8 ± 3.6\nASR-TEXT + GS-TEXT\n219\n63.3\n64.8"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "66.2 ± 3.8\n73.6\n73.8\nALL SYSTEMS\n4017"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "4. Discussion"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": ""
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "When comparing the performance of different ASR systems in"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "Table 2 and Table 1, a correlation between low WER values"
        },
        {
          "Figure 1: A general overview of our emotion recognition by multi-modal fusion of different trained Support Vector Machines (SVMs).": "and high UAR values becomes obvious. Accordingly the Gold"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Pre-training of deep bidirectional\ntransformers for language un-"
        },
        {
          "7. References": "F. Metze, A. Batliner, F. Eyben, T. Polzehl, B. Schuller,\nand\n[1]",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "derstanding,”\nin Proceedings\nof\nthe\n2019 Conference\nof\nthe"
        },
        {
          "7. References": "S. Steidl, “Emotion recognition using imperfect speech recog-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "North American Chapter of\nthe Association for Computational"
        },
        {
          "7. References": "nition,” in Proc. Interspeech, 01 2010, pp. 478–481.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Linguistics: Human Language Technologies, 2019, pp. 4171–"
        },
        {
          "7. References": "[2]\nE. Cambria, D. Hazarika, S. Poria, A. Hussain, and R. B. V. Subra-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "4186."
        },
        {
          "7. References": "maanyam, “Benchmarking multimodal sentiment analysis,” 2017.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N."
        },
        {
          "7. References": "[3]\nJ. Cho, R. Pappagari, P. Kulkarni, J. Villalba, Y. Carmiel, and",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,”"
        },
        {
          "7. References": "N. Dehak, “Deep neural networks for emotion recognition com-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "arXiv preprint arXiv:1706.03762, 2017."
        },
        {
          "7. References": "bining audio and transcripts,” arXiv preprint arXiv:1911.00432,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[21]\nZ. Lan, M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Sori-"
        },
        {
          "7. References": "2019.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "cut, “Albert: A lite bert for self-supervised learning of language"
        },
        {
          "7. References": "[4] M. Chen and X. Zhao, “A multi-scale fusion framework for bi-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "representations,” arXiv preprint arXiv:1909.11942, 2019."
        },
        {
          "7. References": "modal speech emotion recognition,” in Proc. Interspeech, 2020, pp.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[22] K. Clark, M.-T. Luong, Q. V. Le, and C. D. Manning, “Electra:"
        },
        {
          "7. References": "374–378.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Pre-training text encoders as discriminators rather than generators,”"
        },
        {
          "7. References": "[5]\nF. Eyben, F. Weninger, F. Groß, and B. Schuller, “Recent Devel-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "arXiv preprint arXiv:2003.10555, 2020."
        },
        {
          "7. References": "opments in openSMILE,\nthe Munich Open-Source Multimedia",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[23]\nZhang, Q., Lu, H., Sak, H., Tripathi, A., McDermott, E., Koo,"
        },
        {
          "7. References": "Feature Extractor,” in Proc. ACM Multimedia, Barcelona, Spain,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "S.,\nand Kumar,\nS.,\n“Transformer\ntransducer:\nA streamable"
        },
        {
          "7. References": "2013, pp. 835–838.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "speech recognition model with transformer encoders and rnn-t"
        },
        {
          "7. References": "[6] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "loss,” International Conference on Acoustics, Speech and Signal"
        },
        {
          "7. References": "F. Ringeval, M. Chetouani, F. Weninger, F. Eyben, E. Marchi,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Processing (ICASSP), pp. 7829–7833, 2020."
        },
        {
          "7. References": "M. Mortillaro, H. Salamin, A. Polychroniou, F. Valente,\nand",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[24] Graves, Alex., “Sequence transduction with recurrent neural net-"
        },
        {
          "7. References": "S. Kim, “The Interspeech 2013 Computational Paralinguistics",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "works,” arXiv preprint arXiv:11211.3711, 2012."
        },
        {
          "7. References": "Challenge: Social Signals, Conﬂict, Emotion, Autism,” in Proc.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "Interspeech, Lyon, France, 2013, pp. 148–152.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[25]\nPanayotov, V., Chen, G., Povey, D. and Khudanpur, S.,, “Lib-"
        },
        {
          "7. References": "[7]\nF. Weninger, F. Eyben, B. Schuller, M. Mortillaro, and K. R.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "rispeech:\nan asr corpus based on public domain audio books,”"
        },
        {
          "7. References": "Scherer, “On the Acoustics of Emotion in Audio: What Speech,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "EEE international\nconference on acoustics,\nspeech and signal"
        },
        {
          "7. References": "Music and Sound have in Common,” Frontiers in Emotion Science,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "processing, pp. 5206–5210, 2015."
        },
        {
          "7. References": "vol. 4, pp. 1–12, 2013.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[26] Ardila R, Branson M, Davis K, Henretty M, Kohler M, Meyer"
        },
        {
          "7. References": "[8] M. Schmitt and B. W. Schuller, “openXBOW – Introducing the",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "J, Morais R, Saunders L, Tyers FM, Weber G.,\n“Common"
        },
        {
          "7. References": "Passau Open-Source Crossmodal Bag-of-Words Toolkit,” Journal",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "voice: A massively-multilingual speech corpus.” arXiv preprint"
        },
        {
          "7. References": "of Machine Learning Research, vol. 18, pp. 1–5, 2017.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "arXiv:1912.06670, 2019."
        },
        {
          "7. References": "[9] H. Lim, M. J. Kim, and H. Kim, “Robust Sound Event Classi-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[27] Rousseau, A., Deléglise, P. and Esteve, Y., “Ted-lium:\nan au-"
        },
        {
          "7. References": "ﬁcation Using LBP-HOG Based Bag-of-Audio-Words Feature",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "tomatic speech recognition dedicated corpus,” LREC, pp. 5206–"
        },
        {
          "7. References": "Representation,” in Proc. Interspeech, Dresden, Germany, 2015,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "5210, 2012."
        },
        {
          "7. References": "pp. 3325–3329.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[28] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation"
        },
        {
          "7. References": "[10] M. Schmitt, F. Ringeval, and B. Schuller, “At the Border of Acous-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "of\nrare words with subword units,” in Proceedings of\nthe 54th"
        },
        {
          "7. References": "tics and Linguistics: Bag-of-Audio-Words for the Recognition of",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Annual Meeting of the Association for Computational Linguistics"
        },
        {
          "7. References": "Emotions in Speech,” in Proc.\nInterspeech, San Francisco, CA,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "(Volume 1: Long Papers).\nBerlin, Germany: Association for"
        },
        {
          "7. References": "2016, pp. 495–499.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Computational Linguistics, Aug. 2016, pp. 1715–1725. [Online]."
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Available: https://www.aclweb.org/anthology/P16-1162"
        },
        {
          "7. References": "S. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, M. Freitag,\n[11]",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "S. Pugachevskiy, and B. Schuller, “Snore sound classiﬁcation using",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[29] Kriman,\nSamuel,\nStanislav Beliaev,\nBoris Ginsburg,\nJoce-"
        },
        {
          "7. References": "image-based deep spectrum features,” in Proc. Interspeech 2017,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "lyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary,"
        },
        {
          "7. References": "Stockholm, Sweden, 2017, pp. 3512–3516.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Jason\nLi,\nand Yang\nZhang,\n“Quartznet:\nDeep\nautomatic"
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "speech\nrecognition with\n1d\ntime-channel\nseparable\nconvolu-"
        },
        {
          "7. References": "[12]\nS. Amiriparian, “Deep representation learning techniques for au-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "tions,” International Conference on Acoustics, Speech and Signal"
        },
        {
          "7. References": "dio signal processing,” Ph.D. dissertation, Technische Universität",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Processing (ICASSP), pp. 6124–6128, 2020."
        },
        {
          "7. References": "München, 2019.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "[13]\nS. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, S. Pugachevskiy,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[30] Graves, Alex, Santiago Fernández, Faustino Gomez, and Jürgen"
        },
        {
          "7. References": "and B. Schuller, “Bag-of-deep-features: Noise-robust deep feature",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Schmidhuber., “Sequence transduction with recurrent neural net-"
        },
        {
          "7. References": "representations for audio analysis,” in Proc. IJCNN, Rio de Janeiro,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "works,” In Proceedings of\nthe 23rd international conference on"
        },
        {
          "7. References": "Brazil, 2018, pp. 2419–2425.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Machine learning, pp. 369–376, 2012."
        },
        {
          "7. References": "S. Amiriparian, N. Cummins, S. Ottl, M. Gerczuk, and B. Schuller,\n[14]",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[31] V.\nD.\nB.\nNapoles\nC,\nGormley MR,\n“Annotated\ngiga-"
        },
        {
          "7. References": "“Sentiment analysis using image-based deep spectrum features,” in",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "word,”\nProceedings\nof\nthe\nJoint Workshop\non\nAutomatic"
        },
        {
          "7. References": "Proc. ACIIW 2017, San Antonio, TX, 2017, pp. 26–29.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Knowledge\nBase\nConstruction\nand Web-scale\nKnowledge"
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Extraction (AKBC-WEKEX), pp. 95–100, 2012."
        },
        {
          "7. References": "[15] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "“Densely connected convolutional networks,” in Proc. CVPR, 2017,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[32] Baevski, Alexei, Henry Zhou, Abdelrahman Mohamed,\nand"
        },
        {
          "7. References": "pp. 4700–4708.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Michael Auli, “wav2vec 2.0: A framework for self-supervised"
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "learning of speech representations,” 34th Conference on Neural"
        },
        {
          "7. References": "S. Amiriparian, M. Freitag, N. Cummins, and B. Schuller, “Se-\n[16]",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "Information Processing Systems (NeurIPS 2020), 2020."
        },
        {
          "7. References": "quence to Sequence Autoencoders for Unsupervised Represen-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "tation Learning from Audio,” in Proc. DCASE 2017, Munich,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[33] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim,"
        },
        {
          "7. References": "Germany, 2017, pp. 17–21.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "J. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap: Interactive"
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "emotional dyadic motion capture database,” Language, resources"
        },
        {
          "7. References": "[17] M. Freitag, S. Amiriparian, S. Pugachevskiy, N. Cummins, and",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "and evaluation, vol. 42, no. 4, p. 335, 2008."
        },
        {
          "7. References": "B. Schuller, “auDeep: Unsupervised Learning of Representations",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "from Audio with Deep Recurrent Neural Networks,” Journal of",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "[34]\nL. Kuncheva, C. Whitaker, C. Shipp, and R. Duin, “Limits on the"
        },
        {
          "7. References": "Machine Learning Research, vol. 18, pp. 1–5, 2018.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "majority vote accuracy in classier fusion,” Formal Pattern Analysis"
        },
        {
          "7. References": "",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": "and Applications, vol. 6, pp. 22–31, 04 2003."
        },
        {
          "7. References": "[18] B. Felbo, A. Mislove, A. Søgaard, I. Rahwan, and S. Lehmann,",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "“Using millions of emoji occurrences to learn any-domain repre-",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "sentations for detecting sentiment, emotion and sarcasm,” arXiv",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        },
        {
          "7. References": "preprint arXiv:1708.00524, 2017.",
          "[19]\nJ. Devlin, M.-W. Chang, K. Lee,\nand K. Toutanova,\n“Bert:": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition using imperfect speech recognition",
      "authors": [
        "F Metze",
        "A Batliner",
        "F Eyben",
        "T Polzehl",
        "B Schuller",
        "S Steidl"
      ],
      "year": "2010",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "3",
      "title": "Benchmarking multimodal sentiment analysis",
      "authors": [
        "E Cambria",
        "D Hazarika",
        "S Poria",
        "A Hussain",
        "R Subramaanyam"
      ],
      "year": "2017",
      "venue": "Benchmarking multimodal sentiment analysis"
    },
    {
      "citation_id": "4",
      "title": "Deep neural networks for emotion recognition combining audio and transcripts",
      "authors": [
        "J Cho",
        "R Pappagari",
        "P Kulkarni",
        "J Villalba",
        "Y Carmiel",
        "N Dehak"
      ],
      "year": "2019",
      "venue": "Deep neural networks for emotion recognition combining audio and transcripts",
      "arxiv": "arXiv:1911.00432"
    },
    {
      "citation_id": "5",
      "title": "A multi-scale fusion framework for bimodal speech emotion recognition",
      "authors": [
        "M Chen",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Recent Developments in openSMILE, the Munich Open-Source Multimedia Feature Extractor",
      "authors": [
        "F Eyben",
        "F Weninger",
        "F Groß",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "7",
      "title": "The Interspeech 2013 Computational Paralinguistics Challenge: Social Signals, Conflict, Emotion, Autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi",
        "M Mortillaro",
        "H Salamin",
        "A Polychroniou",
        "F Valente",
        "S Kim"
      ],
      "year": "2013",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "8",
      "title": "On the Acoustics of Emotion in Audio: What Speech, Music and Sound have in Common",
      "authors": [
        "F Weninger",
        "F Eyben",
        "B Schuller",
        "M Mortillaro",
        "K Scherer"
      ],
      "year": "2013",
      "venue": "Frontiers in Emotion Science"
    },
    {
      "citation_id": "9",
      "title": "openXBOW -Introducing the Passau Open-Source Crossmodal Bag-of-Words Toolkit",
      "authors": [
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "10",
      "title": "Robust Sound Event Classification Using LBP-HOG Based Bag-of-Audio-Words Feature Representation",
      "authors": [
        "H Lim",
        "M Kim",
        "H Kim"
      ],
      "year": "2015",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "At the Border of Acoustics and Linguistics: Bag-of-Audio-Words for the Recognition of Emotions in Speech",
      "authors": [
        "M Schmitt",
        "F Ringeval",
        "B Schuller"
      ],
      "year": "2016",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Snore sound classification using image-based deep spectrum features",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "M Freitag",
        "S Pugachevskiy",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Deep representation learning techniques for audio signal processing",
      "authors": [
        "S Amiriparian"
      ],
      "year": "2019",
      "venue": "Deep representation learning techniques for audio signal processing"
    },
    {
      "citation_id": "14",
      "title": "Bag-of-deep-features: Noise-robust deep feature representations for audio analysis",
      "authors": [
        "S Amiriparian",
        "M Gerczuk",
        "S Ottl",
        "N Cummins",
        "S Pugachevskiy",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Proc. IJCNN"
    },
    {
      "citation_id": "15",
      "title": "Sentiment analysis using image-based deep spectrum features",
      "authors": [
        "S Amiriparian",
        "N Cummins",
        "S Ottl",
        "M Gerczuk",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proc. ACIIW 2017"
    },
    {
      "citation_id": "16",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "17",
      "title": "Sequence to Sequence Autoencoders for Unsupervised Representation Learning from Audio",
      "authors": [
        "S Amiriparian",
        "M Freitag",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proc. DCASE 2017"
    },
    {
      "citation_id": "18",
      "title": "auDeep: Unsupervised Learning of Representations from Audio with Deep Recurrent Neural Networks",
      "authors": [
        "M Freitag",
        "S Amiriparian",
        "S Pugachevskiy",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "19",
      "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "authors": [
        "B Felbo",
        "A Mislove",
        "A Søgaard",
        "I Rahwan",
        "S Lehmann"
      ],
      "year": "2017",
      "venue": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm",
      "arxiv": "arXiv:1708.00524"
    },
    {
      "citation_id": "20",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter"
    },
    {
      "citation_id": "21",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need",
      "arxiv": "arXiv:1706.03762"
    },
    {
      "citation_id": "22",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2019",
      "venue": "Albert: A lite bert for self-supervised learning of language representations",
      "arxiv": "arXiv:1909.11942"
    },
    {
      "citation_id": "23",
      "title": "Electra: Pre-training text encoders as discriminators rather than generators",
      "authors": [
        "K Clark",
        "M.-T Luong",
        "Q Le",
        "C Manning"
      ],
      "year": "2020",
      "venue": "Electra: Pre-training text encoders as discriminators rather than generators",
      "arxiv": "arXiv:2003.10555"
    },
    {
      "citation_id": "24",
      "title": "Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss",
      "authors": [
        "Q Zhang",
        "H Lu",
        "H Sak",
        "A Tripathi",
        "E Mcdermott",
        "S Koo",
        "S Kumar"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Sequence transduction with recurrent neural networks",
      "authors": [
        "Alex Graves"
      ],
      "year": "2012",
      "venue": "Sequence transduction with recurrent neural networks"
    },
    {
      "citation_id": "26",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "EEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "27",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "R Ardila",
        "M Branson",
        "K Davis",
        "M Henretty",
        "M Kohler",
        "J Meyer",
        "R Morais",
        "L Saunders",
        "F Tyers",
        "G Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massively-multilingual speech corpus",
      "arxiv": "arXiv:1912.06670"
    },
    {
      "citation_id": "28",
      "title": "Ted-lium: an automatic speech recognition dedicated corpus",
      "authors": [
        "A Rousseau",
        "P Deléglise",
        "Y Esteve"
      ],
      "year": "2012",
      "venue": "LREC"
    },
    {
      "citation_id": "29",
      "title": "Neural machine translation of rare words with subword units",
      "authors": [
        "R Sennrich",
        "B Haddow",
        "A Birch"
      ],
      "year": "2016",
      "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "30",
      "title": "Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions",
      "authors": [
        "Samuel Kriman",
        "Stanislav Beliaev",
        "Boris Ginsburg",
        "Jocelyn Huang",
        "Oleksii Kuchaiev",
        "Vitaly Lavrukhin",
        "Ryan Leary",
        "Jason Li",
        "Yang Zhang"
      ],
      "year": "2020",
      "venue": "International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Sequence transduction with recurrent neural networks",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Faustino Gomez",
        "Jürgen Schmidhuber"
      ],
      "year": "2012",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "32",
      "title": "Annotated gigaword",
      "authors": [
        "V Napoles",
        "C Gormley"
      ],
      "year": "2012",
      "venue": "Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction (AKBC-WEKEX)"
    },
    {
      "citation_id": "33",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Henry Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "34th Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language, resources and evaluation"
    },
    {
      "citation_id": "35",
      "title": "Limits on the majority vote accuracy in classier fusion",
      "authors": [
        "L Kuncheva",
        "C Whitaker",
        "C Shipp",
        "R Duin"
      ],
      "year": "2003",
      "venue": "Formal Pattern Analysis and Applications"
    }
  ]
}