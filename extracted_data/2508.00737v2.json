{
  "paper_id": "2508.00737v2",
  "title": "How Llms Are Shaping The Future Of Virtual Reality",
  "published": "2025-08-01T16:08:05Z",
  "authors": [
    "Süeda Özkaya",
    "Santiago Berrezueta-Guzman",
    "Stefan Wagner"
  ],
  "keywords": [
    "Accessibility",
    "Affective Computing",
    "AI Game Mastering",
    "Ethical AI",
    "Immersive Games",
    "Large Language Models (LLMs)",
    "Memory Management",
    "Multimodal Interaction",
    "NPC Interaction",
    "Personalized Gameplay",
    "Procedural Storytelling",
    "Real-Time Systems",
    "Virtual Reality (VR)"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions, accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer-reviewed studies published between 2018 and 2025, we identify key application domains-ranging from emotionally intelligent NPCs and procedurally generated storytelling to AI-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The integration of Large Language Models (LLMs) into Virtual Reality (VR) games represents a transformative step in the evolution of interactive digital environments  [1, 2] . LLMs are neural networks trained on extensive text data to produce language that resembles human speech. They have progressed quickly in their abilities and uses, evolving from text-generating tools to agents engaging in real-time dialogue, narrative design, and adaptive learning  [3, 4] . In parallel, VR games have evolved from simulations to immersive worlds that leverage spatial computing, haptic feedback, and embodied interaction  [5] . The convergence of these technologies offers unprecedented opportunities to enhance interactivity, narrative depth, emotional engagement, and accessibility in digital games  [6, 7] .\n\nThe core focus of gaming and simulation development on immersive experiences has generated rising interest in artificial intelligence (AI) applications, especially LLMs for creating dynamic and human-like gameplay  [8, 9] . The exploration of LLMs as creative tools for virtual reality experiences has emerged because these models enable complex non-player character (NPC) dialogues and generate storylines and environments that adapt to player choices  [10, 11] . The models' context-sensitive language capabilities enable personalized gameplay that becomes more accessible and inclusive, thus expanding educational and training possibilities and entertainment options  [12, 13] .\n\nThis research paper explores the developing relationship between LLMs and VR games to determine their applications for improving core immersive gameplay elements. This research investigates the following key questions: -RQ1. How do LLMs contribute to more emotionally intelligent and lifelike NPC interactions? -RQ2. In what ways can they support procedural storytelling and adaptive narratives? -RQ3. How do they affect personalization, accessibility, and user experience in immersive environments? -RQ4. What challenges and limitations -technical, ethical, and practical-must be addressed while achieving their full potential? -RQ5. How can future research leverage emerging trends in multimodal AI, reinforcement learning, affective computing, and open-source tools to build scalable, ethically responsible, and emotionally attuned VR systems?\n\nTo answer these questions, we conduct a comprehensive literature review that examines current research and systems across six application domains: (1) dynamic NPC interactions and emotional intelligence, (2) procedural storytelling and narrative generation, (3) intelligent game mastering and adaptive control systems, (4) personalized player experience, (5) accessibility, inclusivity, and usability, and (6) challenges and limitations including ethical concerns and deployment barriers. Additionally, we analyzed the current state of the art before defining the main opportunities and constraints forming the future direction of LLM-based VR gaming.\n\nThis paper contributes to the expanding field of intelligent digital worlds by evaluating the combined impact of LLMs and VR on gameplay and critically reviewing their technological convergence. It aims to support researchers, developers, and designers seeking to build more engaging, equitable, and responsive VR environments through LLMs' responsible and creative use. Figure  1  provides an overview of this review's organization and thematic flow.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Background",
      "text": "Understanding the integration of Large Language Models (LLMs) into Virtual Reality (VR) games requires a foundational overview of both technologies and their evolution.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Large Language Models (Llms)-Overview",
      "text": "LLMs have experienced considerable advancement in recent years, moving from theoretical ideas to more advanced and scalable architectures like OpenAI's Generative Pre-trained Transformers (GPT) series, Meta's Large Language Model Meta AI (LLaMA), and Google's PaLM. The development of large-scale transformer-based architectures has facilitated significant progress in generating text, creating dialogue systems, and enabling procedural content generation  [3] .\n\nLLMs began with early autoregressive models, which focused on predicting the next word in a sentence based on the words that came before. At first, this approach was a small area of research in Natural Language Processing (NLP). However, it gained major attention after the release of GPT-2 in 2019, where it showed that transformer models trained on massive text datasets could generate high-quality, coherent language-and that their output could be shaped using carefully designed prompts  [14, 15, 16] .\n\nOpenAI's GPT models have been leading in developing LLMs, driving major progress in natural language processing  [17] . GPT-3, with 175 billion parameters, impressed researchers with its ability to generate fluent and meaningful text, which led to its use in chatbots and interactive AI tools  [18] . Later versions-GPT-3.5 and GPT-4-further improved these abilities by using Reinforcement Learning from Human Feedback (RLHF), which helped the models give more accurate and helpful responses  [4, 16, 19] .\n\nIn addition to OpenAI's work, other powerful transformerbased models have been developed. Bidirectional Encoder Representations from Transformers (BERT) introduced bidirectional training, which made it better at understanding context and improved tasks like text classification and natural language inference. LLaMA was trained only on publicly available data, making it more accessible for open-source use. While BERT focused on deep understanding of text, LLaMA aimed to offer smaller, more efficient models without losing performance compared to other top models  [20, 21, 22] .\n\nAs LLMs have grown in size and capability, new multimodal models like GPT-4V  [23]  and Large Language and Vision Assistant (LLaVA)  [24]  have expanded their use beyond just text. These models can now understand images, generate speech, and support interactive storytelling. They are also being improved to use memory and computing power more efficiently while becoming more accurate and flexible.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Virtual Reality Games-Overview",
      "text": "Virtual Reality (VR) gaming has come a long way, moving from simple simulations to highly immersive and interactive experiences. This progress has been driven by hardware, software, and AI advances, especially in areas like NLP and content generation.\n\nThe beginnings of VR can be traced back to 1968, when Ivan Sutherland and his student Bob Sproull created the first computer-based VR system, known as the \"Sword of Damocles\". It used head tracking to display a basic 3D wireframe view that changed with the user's perspective. While it wasn't a game, it introduced key ideas, like perspective-based interaction, that still form the foundation of modern VR gaming  [25, 5] .\n\nThe 3D Internet allows users to move around and interact with digital objects in space, rather than just clicking on flat screens. A well-known example is Second Life  [26] , a virtual world where people can socialize and explore using avatars. It combines AI, 3D headsets, motion sensors, and even holographic displays to create more immersive experiences  [27] .\n\nAfter early VR experiments in the 1980s, the release of consumer-friendly devices like the Oculus Rift, HTC Vive, and PlayStation VR changed the gaming world. These headsets gave players access to realistic virtual worlds with features like motion tracking, haptic feedback (touch sensations), 3D visuals, and the use of photogrammetry  [28] .\n\nAs VR technology improved, games became more realistic, featuring better physics, stronger hardware, smarter characters, and stories that change based on player choices. Modern VR games now include AI-powered characters, voice interaction, and multiple communication methods, making the experience feel more natural and immersive, like talking to real people.\n\nVR gaming hardware includes two main parts: output devices and input devices. Output devices, like head-mounted displays (HMDs), show 3D visuals and provide a wide field of view to make the experience realistic. These can be mobile (like Google Cardboard or Samsung GearVR) or wired (like Oculus Rift, HTC Vive, and PlayStation VR), and often include motion sensors and sometimes eye tracking for better performance. To simulate touch, devices like vests, gloves, and fullbody suits provide haptic feedback, letting players feel things like force, wind, or temperature. All these tools work together using motion sensors and tracking systems to deliver accurate and responsive gameplay  [5] .\n\nDespite its rapid development, the VR gaming sector still faces key challenges, including high production costs, limited content, motion sickness, and hardware accessibility. The immersive potential of VR is well recognized, but the expense of headsets and the need for high-performance computers continue to hinder widespread adoption  [5] . Additionally, some users experience nausea or disorientation during extended play, often due to latency or mismatched sensory input  [29] . Developing VR content also demands specialized tools and expertise, creating barriers for smaller studios. These factors highlight that technical and usability issues must be addressed for VR gaming to scale sustainably.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "The Intersection Of Llms And Vr Games",
      "text": "The convergence of LLMs and VR technologies represents a significant advancement in developing interactive digital environments, particularly in entertainment-based and serious games  [30] . The convergence of these technologies enables developers to create innovative immersive experiences, dialogue systems, and educational simulations  [1, 2] .\n\nInitial applications of LLMs in VR have focused on improving user interaction and supporting developers. One of the earliest documented integrations involved helping novice developers create VR content. Tools like ChatGPT assist with code suggestions, debugging, and explaining concepts within development environments such as Unity, making the VR development process more accessible  [6] .\n\nBeyond development support, LLMs are increasingly used in real-time VR environments to generate narrative content, enable dynamic interactions, and simulate intelligent NPCs. These conversational agents are especially valuable in educational and training applications, where they provide adaptive feedback and act as virtual tutors  [7, 31] .\n\nA key use of LLMs in VR is in AI-driven dialogue systems. Unlike scripted interactions, LLMs support unscripted, contextaware conversations that enhance immersion. For instance, the \"LearningverseVR\" platform  [31]  uses generative AI to create NPCs with distinct personalities and backgrounds, enabling learners to engage in personalized, natural dialogue while exploring content at their own pace.\n\nVarious VR game genres have integrated LLM-based assistants to guide players through challenges using voice interaction  [32, 33, 34, 35] . These systems offer real-time problem-solving and adapt to player input, enhancing engagement through natural, human-like communication.\n\nThe applications of LLMs in VR span several key domains, including NPC interaction, game mastering, accessibility, personalization, and ethics. Figure  2  provides a visual overview of these areas, many of which are explored in depth throughout this paper to highlight current implementations and future opportunities.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "This study employed a structured literature review to examine how Large Language Models (LLMs) are applied in Virtual Reality (VR) games. The goal was to identify current applications, implementation strategies, and key technical and ethical challenges.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Literature Search Strategy",
      "text": "A comprehensive search was carried out across five leading academic databases: IEEE Xplore, ACM Digital Library, Scopus, Web of Science, and Google Scholar. These sources were chosen for their extensive coverage of peer-reviewed research in computer science, artificial intelligence (AI), and immersive technologies.\n\nVarious search terms were used in different Boolean combinations (AND, OR) to capture a broad and relevant set of studies. To improve precision, we used Boolean logic to combine general terms (e.g., \"virtual reality\") with model-specific terms (e.g., \"GPT-4\", \"conversational agent\"), and excluded non-relevant acronym matches. Wherever supported by the database, double quotation marks (e.g., \"virtual reality\", \"NPC dialogue\") were used to ensure exact phrase matching and reduce noise in search outputs.\n\nThese search terms targeted the intersection of LLMs and VR across multiple application areas such as NPC interaction, procedural storytelling, accessibility, personalization, and system performance. Table  1  summarizes the main categories and example search terms used during the database queries.\n\nClear inclusion criteria were established to guide the selection of relevant studies. These criteria prioritized recency, relevance to LLM and VR integration, and practical application in gaming or adjacent immersive contexts. Table  2  presents the conditions that studies had to meet to be considered in the final review.\n\nAlongside the inclusion conditions, exclusion criteria were defined to eliminate irrelevant or low-quality sources. These criteria ensured that only studies with substantial technical contributions and contextual alignment were analyzed. Table  3  outlines the reasons for omitting certain works from the review.\n\nWhile the primary focus of the research review was on recent publications, some earlier works were also included, particularly those published before 2015. These were not used mainly to analyze \"Key Applications\" or \"Challenges and Limitations\" parts, but served as foundational sources to explain core concepts and historical context in the Background section.\n\nThe initial database queries returned a total of 528 records across five academic databases. After removing duplicates, 422 papers remained. Title-based relevance filtering further reduced the pool to 250 studies, which were then screened based on their abstracts. Of these, 89 papers were selected for full-text review due to their relevance to LLM integration in VR or transferable insights into virtual game design. Ultimately, 62 papers were included in the final literature review.\n\nDue to the limited number of studies focused exclusively on \"LLMs in VR games,\" the inclusion criteria were purposefully extended to papers addressing related topics, such as LLMs in non-VR games or LLM-powered interactions in VR simulations, which were analyzed to extract applicable insights. A snowballing method was also employed, wherein frequently cited and conceptually central studies were traced backward from reference sections.\n\nThe distribution of reviewed papers by publication year reflects a sharp increase in research interest, especially after 2023, peaking in 2024 and 2025 (see Figure  3 ).\n\nFollowing the full-text analysis of 62 selected studies, a thematic categorization was conducted to structure the findings and identify primary research directions. The classification framework was based on the scope of each study and the recurring concepts in the literature. Studies were grouped according to their primary application domains, key findings, and the challenges they addressed. Two main thematic categories emerged:\n\n• Key Applications of LLMs in VR Games We divided these categories into subcategories based on common research objectives, technical approaches, and experimental setups. The classification is aligned with the structure of the literature review section in this paper, which includes:  Peer-reviewed journal articles, conference papers, or technical reports with publicly available full texts Topical Relevance Focused on the integration of LLMs into VR environments, with a primary emphasis on gaming contexts Related Applications Papers addressing adjacent VR domains such as education, training, or accessibility using LLMs, provided they offer transferable insights for VR gaming  This taxonomy provides a structured synthesis of current research and helps identify emerging opportunities and gaps in integrating LLMs with VR games. It also enables comparative evaluation of different technical and design strategies.",
      "page_start": 3,
      "page_end": 6
    },
    {
      "section_name": "• Challenges And Limitations In Implementation",
      "text": "Figure  4  illustrates the approximate number of papers associated with each key application area to visualize the distribution of reviewed studies across these categories. Note that individual studies may span multiple categories if they contribute meaningfully to more than one domain.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Key Applications Of Llms In Vr Games",
      "text": "As the convergence of Large Language Models (LLMs) and Virtual Reality (VR) advances, various impactful use cases have emerged across the VR gaming landscape.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dynamic Npc Interactions And Emotional Intelligence",
      "text": "Recent advancements in LLMs have enabled the development of more responsive and emotionally intelligent NPCs within VR environments. Modern NPCs move away from fixed, pre-programmed behaviors because they now use LLMs to generate dynamic conversational abilities, emotional expression, and adaptive interaction strategies  [8] . This section combines essential research findings to show how LLMs improve character behavior and dialogue delivery in virtual reality environments.\n\nEmotionally Expressive NPCs with LLMs. The use of LLMs such as GPT-3.5 and GPT-4 to give NPCs facial expressions, gestures, and emotionally human-like dialogue is on the rise.\n\nNormoyle et al.,  [10]  used GPT-3.5 to create facial expressions, body movements, and lip-syncing for game characters based on what was being said. They used the Facial Action Coding System (FACS) and Laban Movement Analysis (LMA) to guide the animations. Their study was done in a 3D pointand-click game, not real-time VR. One limitation was that small changes in prompts could cause inconsistent animations, a known issue with LLMs. Despite this, the study shows that LLMs can automatically help generate emotional character behavior, saving time and making interactions feel more lifelike.\n\nBuilding on this, Marincioni et al.  [36]  studied how LLMs could assign emotions like Happy, Sad, Angry, or Neutral to NPCs in a mystery game, and how these emotions affected players. Interestingly, players often reacted positively even to negative emotions, such as gratitude toward angry NPCs. This reveals how emotionally expressive NPCs can create complex psychological responses. The study shows that giving NPCs emotional depth using LLMs can greatly enhance immersion and shape the overall gameplay experience.\n\nPersonality and Conversational Naturalism. Consistency in NPC personality is essential for believable and immersive interactions. Hasani and Udjaja  [37]  proposed an early framework combining generative dialogue, emotional cues, and multimodal interaction to support personality-consistent, contextaware responses. Building on this, Zhu et al.,  [38]  found that users retained more information and felt more immersed when engaging with human-like avatars than abstract ones. Similarly, Tonini's international study  [33]  showed that voice-driven AI NPCs enhanced user experience through emotionally engaging and polite communication, though issues like latency and limited memory reduced sustained immersion.\n\nMemory, Consistency, and Long-Term Interaction. Ensuring consistent dialogue over time remains a key challenge in AIdriven NPC design. Zheng et al.,  [39]  proposed a dual-memory system (MemoryRepository) that mimics human-like forgetting and summarization, allowing NPCs to recall both recent and long-term interactions. Tested with models like GPT-4, GPT-3.5, and ChatGLM, the system improved dialogue continuity, engagement, and immersion. In a related approach, Jahangiri et al.,  [40]  focused on optimizing performance by combining LLMs with Pursuit Learning Automata (PLA). Their hybrid system enabled faster responses and dynamically adjusted dialogue tone to match player preferences, balancing emotional richness with real-time scalability.\n\nMultimodal and Nonverbal Interactions. LLMs also support multimodal NPC interactions by combining voice, gaze, and gesture, making characters more lifelike and responsive. Players tend to prefer NPCs that recognize physical gestures, such as waving or nodding, and provide real-time feedback through cues like lip-syncing and state lights. Yin and Xiao  [41, 42]  analyzed 47 VR games and found that physical actions significantly enhance immersion. Players expected NPCs to respond to proximity, gestures, and eye contact, making them feel more aware and reactive. Maslych et al.  [43]  further emphasized the role of feedback cues like state lights, gaze, and facial expressions during conversations, which increased users' trust and engagement. Even simple indicators during system response times, such as loading bars, reassured users that the NPC was actively processing their input. Sissler  [9]  developed an opensource Unity framework using GPT-3.5 that integrates voice, gestures, and animated facial expressions. The study showed that synchronized multimodal responses improved NPC believability and helped players feel heard. These findings highlight that real-time multimodal feedback is key to creating immersive and socially engaging LLM-driven NPCs in VR.\n\nAcross the reviewed studies, LLMs such as GPT-3.5 and GPT-4 are widely adopted to create emotionally expressive, socially aware NPCs capable of real-time dialogue. While many systems simulate facial expressions, gestures, and vocal affect, most were tested outside of fully immersive VR settings. Research consistently highlights the importance of consistent personality, emotional depth, and long-term memory in maintaining user immersion. However, limitations remain in sustaining coherence over extended interactions and minimizing latency in real-time environments. The integration of multimodal cues-voice, gaze, gesture-has proven especially effective in enhancing believability and player engagement.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Procedural Storytelling And Narrative Generation",
      "text": "LLMs are changing how games tell stories by automatically creating quests, dialogues, scenes, and storylines that adjust to player actions and preferences. This flexibility shapes players' experiences, making gameplay more immersive and personalized. Recent research has explored ways LLMs support storytelling, such as generating new story paths, building dynamic quests, and adapting to the game's context. These studies show that LLMs can improve interactive storytelling, though challenges with consistency and coherence remain.\n\nOne study used GPT-4-powered NPCs in an interactive fiction game where players could speak freely instead of choosing from pre-written options. This led to unexpected storylines and character relationships that the designers had not planned-players who liked exploring enjoyed this freedom to create complex and personal narratives. However, the system sometimes repeated itself or gave inconsistent replies due to memory limitations  [11] . Another project, PANGeA, combined LLMs with branching logic to create quests and dialogue in a turn-based RPG. The game world changed based on player decisions, leading to unique and replayable stories. While this approach gave players more variety, it sometimes produced plot inconsistencies, especially during long play sessions  [44] .\n\nProcedural Quest and Dialogue Generation. LLMs have been widely explored for generating quests and dialogues in roleplaying games (RPGs). One study fine-tuned GPT-2 using 978 quests from existing games, resulting in a model called Quest-GPT-2. This system produced more varied and creative quests than traditional retrieval-based methods, and human evaluators found that about 20% of the generated quests were usable without significant edits. However, the model struggled with coherence, often creating quests with unclear goals or inconsistent character relationships, especially in multi-step storylines  [45] .\n\nAnother approach used knowledge graphs alongside LLMs to improve coherence and relevance. The system combined information about characters, history, and player choices to generate quests and dialogues aligned better with the game world. Human reviewers rated these quests higher for fluency and logical consistency than standard LLM output. Still, the method faced challenges with memory retention and maintaining story consistency over longer play sessions  [46] .\n\nFurther development came through a persona-based framework that used LLMs to generate consistent character dialogues across different scenes. This system used \"persona cards\" to define character traits and \"scene cards\" to give context, which helped LLMs maintain each NPC's personality over time. Combining prompt engineering and fine-tuning, even smaller LLMs (with around 7 billion parameters) could produce high-quality, personality-rich dialogues  [47, 48] .\n\nScene, Context, and Environment-Aware Storytelling. Recent studies have focused on how LLMs can generate narratives and dialogue that adapt to in-game environments and context. One notable system, SceneCraft, used GPT-4 to create interactive scenes and cutscenes by combining predefined templates with probabilistic variation. Developers could define scene structures, and the system would expand them into coherent story events. While the generated scenes were engaging and consistent in individual instances, maintaining character and world consistency across multiple scenes remained a challenge  [49] .\n\nRadež and Bohak  [50]  introduced a system that enabled NPCs to generate dialogue based on their awareness of the game environment. Using panoramic image capture and semantic segmentation, NPCs could reference nearby objects and spatial relationships, creating more believable and immersive interactions. Players appreciated the added realism, but the system's high computational demands limited its application in real-time VR settings.\n\nLi et al.,  [51]  proposed a schema-based prompting method for GPT-4 Turbo agents to handle spatial interactions in VR, such as pointing, grabbing, or navigating scenes. Their system generated dialogue based on environmental cues, object properties, and user actions. The agents were tested in various role-play scenarios and showed practical spatial reasoning and responsiveness. However, they sometimes hallucinated object references and struggled in more complex environments.\n\nEarlier work by Hämäläinen et al.,  [52]  demonstrated a system that adapted NPC dialogue in Fallout 4 based on gameplay variables like health or quest progress. Instead of generating new lines, the system rephrased existing ones to better fit the player's current state. While effective for personalization, it lacked memory of past interactions and could not support dynamic long-term conversations.\n\nThe reviewed studies demonstrate that LLMs can significantly enhance procedural storytelling by generating dynamic quests, dialogues, and scene-aware narratives. Techniques such as fine-tuning, prompt engineering, and knowledge graph integration help maintain coherence and character consistency, though challenges persist with memory retention and logical continuity across extended sessions. Systems like SceneCraft and schema-based prompting show promise in generating context-aware scenes and spatially grounded dialogue, yet often face scalability limitations in real-time VR settings. Overall, while LLMs expand narrative flexibility and personalization, consistent world-building and long-term dialogue coherence remain open challenges for future development.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Intelligent Game Masters For Vr And Adaptive Systems",
      "text": "Recent advancements in LLMs have made it possible to create AI Dungeon Masters (DMs) capable of managing playerdriven narratives and improvising gameplay in real time. Several studies show that AI DMs can take over key storytelling responsibilities typically handled by human game masters, enhancing the role-playing experience. For instance, ChatGPT has been explored as a DM for tabletop role-playing games (TTRPGs) like Dungeons & Dragons. It was able to generate coherent narratives and respond to player input dynamically. However, the study also noted limitations, such as delayed responses and limited emotional engagement, which affected player immersion  [53] .\n\nTo support novice game masters, Kelly et al. expanded a tool called Shoelace by adding LLM-based dialogue suggestions and information retrieval. This helped users manage scenes and improvise more effectively, especially beginners  [54] .\n\nAnother study focused on improving AI DMs by integrating function calling into LLMs. In the context of Jim Henson's Labyrinth: The Adventure Game, the system used two types of functions: one for simulating dice rolls and another for updating the game state. Combining both functions led to more consistent and engaging storytelling, as the AI could better follow game rules and handle random events  [55] .\n\nResearch has also looked into how the personality of AI game masters affects players. Findings show that players respond more positively to friendly and cooperative AI DMs, suggesting that the tone and demeanor of the AI can influence both gameplay and player emotions  [56] .\n\nAdaptive and Interactive AI Systems in VR Games. LLMs are now used in tabletop games and as adaptive assistants and world managers in dynamic virtual environments. One early example involved a GPT-based voice assistant in a low-cost VR escape room, which provided hints and story cues based on the game's context. While this improved gameplay through adaptive responses, it faced challenges such as response delays and limited real-time flexibility  [57] .\n\nBeyond desktop and cloud-based systems, some studies explored lightweight mobile VR applications. For example, Khan et al. created a multiplayer VR carrom game where players competed against an AI opponent using Bluetooth controls and first-person vision. This demonstrated an early attempt to integrate AI-driven decision-making into mobile VR platforms  [58] .\n\nIn more complex scenarios, LLMs have been used to control multi-agent teams in adversarial search-and-rescue games. These AI agents outperformed traditional strategic planning and opponent modeling models by using advanced prompting methods such as Zero-shot Chain-of-Thought (CoT) and iterative cue-based learning  [59] .\n\nThe LLMR framework furthers this by offering a modular system for managing interactive virtual worlds. It uses multiple GPT-based modules to handle scene understanding, task planning, and debugging. This setup enables real-time 3D scene creation with fewer errors and greater coherence than using a single LLM alone  [60] . Together, these studies show how LLMs can support adaptive, responsive, and intelligent control of VR game environments.\n\nLLMs are increasingly leveraged to function as intelligent game masters and adaptive agents in VR, capable of facilitating improvisational storytelling, rule-based decision-making, and multi-agent coordination. Studies show that ChatGPT and similar models can effectively manage narrative flow and simulate dynamic events, though response delays and limited emotional depth still affect immersion. Integrations with function calling, tone customization, and scene management tools like Shoelace and LLMR have improved coherence and flexibility. However, most applications remain experimental or limited to lightweight systems, highlighting the need for further optimization for realtime, large-scale VR environments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Personalized Player Experience",
      "text": "LLMs offer new ways to personalize gameplay in VR by enabling adaptive dialogue, emotional feedback, and contextaware storytelling. Personalization now goes beyond adjusting difficulty or settings-it involves creating AI-driven agents that can understand, remember, and respond to players in socially intelligent and emotionally engaging ways. While some aspects have been discussed earlier, this section focuses on broader strategies such as creativity support, player modeling, and emotionally tailored narratives.\n\nOne primary use of LLMs in personalization is enabling natural conversations between players and virtual agents. Studies have shown that players remember more and engage longer when interacting with LLM-powered avatars, especially when the agent remembers previous interactions and maintains a human-like personality  [38, 33] .\n\nA promising direction involves using LLMs to support player creativity. Lin et al.,  [61]  developed a VR brainstorming system where ChatGPT-powered NPCs acted as creative partners. These assistants offered voice suggestions, summarized discussions, and retrieved relevant information in real time. The system encouraged divergent thinking and collaborative idea generation by understanding the ongoing conversation, turning the AI into a co-creator rather than just a tool.\n\nTucek  [62]  explored emotionally personalized storytelling, where NPCs adapted to each player's social identity, emotional state, and choices. These emotionally aware agents used LLMs to generate real-time dialogue aligned with the player's perspective, aiming to foster empathy and deeper narrative engagement.\n\nOther studies focused on personalizing VR experiences through familiarity. Guo et al.,  [63]  found that players responded better to NPCs that looked or sounded familiar. In exergames, avatars that reflected users' preferences improved enjoyment and performance, particularly for more self-conscious players. These results show that even simple visual or auditory customization can enhance user experience in measurable ways.\n\nLLMs are enabling highly personalized VR experiences by supporting emotionally aware, conversationally adaptive, and context-sensitive virtual agents. Studies highlight that memory retention, personality continuity, and co-creative dialogue enhance user engagement and satisfaction. Creative assistance, emotionally aligned storytelling, and familiarity-based customization have been shown to foster empathy, enjoyment, and performance. However, most implementations remain smallscale or experimental, and sustaining long-term personalization in complex, dynamic environments remains a challenge for future work.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Accessibility, Inclusivity, And Usability",
      "text": "LLMs are increasingly used in VR environments to improve accessibility, inclusivity, and usability. Their integration supports the development of adaptive systems that address diverse user needs, including individuals with disabilities, language differences, or limited technological experience. This section explores how LLMs enhance VR through multimodal assistance, personalized dialogue, and culturally sensitive design.\n\nMultimodal and Sensory Accessibility. One key advantage of LLMs is their ability to generate natural language explanations for users with sensory limitations. Multimodal models like GPT-4V enable scene descriptions via text-to-speech, helping visually impaired users navigate virtual spaces. For example, EnVisionVR interprets 360-degree scenes to provide real-time audio feedback on spatial layouts and object locations  [64] .\n\nLLMs also support inclusive design by enabling dynamic, user-adaptive interactions. Bozkir et al.,  [13]  argue that LLMpowered NPCs can adjust to different user needs through prompt engineering and fine-tuning, offering more personalized and equitable experiences than static, pre-scripted agents.\n\nVR Games for Mental Health and Well-being. Baghaei et al.  [65]  conducted a design-driven study exploring how individualized virtual reality (iVR) environments could enhance mental health outcomes, particularly among young people aged 18-25. Drawing on prior work by Falconer et al.  [66] , they implemented a VR experience aimed at increasing selfcompassion as a pathway to alleviating depressive symptoms. Participants could personalize key aspects of the virtual environment, including the avatar, therapeutic setting, and avatar behaviors. The study found that such personalized experiences were perceived as more meaningful, emotionally engaging, and safer than standardized VR therapy. Personalization-especially when tailored to the user's identity, emotional state, and goals-was shown to enhance users' motivation and sense of connection. These findings support the potential of iVR to provide scalable, user-centered mental health interventions.\n\nBaghaei et al.  [67]  conducted a scoping review of 34 studies that used VR to treat depression and anxiety. Their findings indicate that the majority of included studies reported positive therapeutic outcomes when VR was used as part of a treatment strategy. Notably, nine of these studies applied cognitive behavioral therapy (CBT) within or alongside VR environments, all of which reported a reduction in symptoms. The review highlighted that VR-based CBT was not only effective but also practical for clinicians, allowing for standardized delivery, repeatability, and increased patient engagement. The authors concluded that VR shows strong potential for structured mental health interventions, especially when it leverages immersive interaction and controlled exposure through techniques like VRET (Virtual Exposure Therapy).\n\nChitale et al.  [68]  presented a scoping review focused on the use of both video games and VR for assessing anxiety and depression. Out of 4566 records initially screened, 10 studies were included, split evenly between VR and videogame-based approaches. An important trend noted in the findings was that studies on anxiety predominantly used VR, while those on depression leaned toward traditional video games. A few studies incorporated machine learning techniques, and only two were clinical trials. Most studies yielded encouraging outcomes, suggesting that both modalities could be useful tools for assessment. However, the authors stressed the limited availability of high-quality clinical evidence and recommended closer collaboration with mental health professionals to ensure safety and privacy in future development.\n\nGiven their adaptability, conversational fluency, and capacity for emotionally responsive interaction, LLMs hold strong potential to enhance these therapeutic VR experiences, particularly by supporting personalized narratives, mood-aware guidance, and dynamic user engagement in mental health contexts  [69] .\n\nSocial and Educational Inclusion. Beyond accessibility, LLMs have shown potential in fostering inclusion for individuals with diverse cognitive and learning needs. Li et al.  [70]  implemented LLM-based chatbots within VR job interview simulations designed for autistic users. These virtual agents offered personalized, voice-based feedback in low-pressure, repeatable environments. The structured yet flexible format helped users build communication skills while maintaining a sense of psychological safety, an essential aspect for neurodivergent learners navigating real-world scenarios.\n\nSimilarly, Voultsiou et al.  [12]  explored the use of LLMpowered assistants in VR learning environments tailored to students with special educational needs, including autism. Their findings indicate that AI-driven guidance enhanced learner en-gagement and comprehension, especially when combined with multimodal inputs like visual cues or simplified language. However, they also observed that the current systems often lack sufficient depth in personalization and struggle to maintain long-term contextual awareness, which limits their effectiveness across extended educational sessions.\n\nAdditional studies on usability show that integrating natural input modalities such as hand tracking further improves interaction quality. Geetha et al. and Krupka et al.  [71, 42]  emphasize that users-particularly those unfamiliar with game controllers-benefit from gesture-based systems that provide intuitive, real-time feedback. These affordances make VR more approachable for a broader range of users, from children with learning difficulties to older adults or those with motor impairments.\n\nCultural and Contextual Usability. Cultural usability in VR is gaining attention as a means to make virtual environments more relatable and engaging for diverse user groups. LLMs, with their capacity for dynamic language generation and contextual adaptation, are increasingly being used to enhance cultural relevance in VR narratives. Lau et al.,  [34]  explored this in a Scottish curling game, where NPCs used culturally appropriate language and expressions. Participants reported that the familiar tone and regional references significantly improved their sense of presence and emotional connection to the experience, demonstrating that localized dialogue-powered by LLMs-can heighten user engagement in culturally specific scenarios.\n\nSimilarly, Subandi et al.  [72]  developed a VR shopping simulation designed to preserve and promote Indonesian textile heritage. Users were guided by LLM-enabled NPCs through the traditional Sasirangan fabric-making process. The agents not only narrated historical context but also responded to questions, allowing for interactive exploration. This use of LLMs for cultural storytelling helped users engage with intangible cultural knowledge in a personalized and immersive manner, suggesting new possibilities for cultural preservation and education through interactive AI.\n\nIn broader educational and heritage applications, LLMs have been used to power intelligent virtual tutors capable of delivering contextualized instruction. For instance, Ayre et al.,  [73]  created a GPT-4-based assistant for a virtual chemistry lab. This tutor provided step-by-step instructions and real-time support tailored to users' actions, effectively acting as a dynamic guide. Users reported increased understanding and autonomy, attributing it to the tutor's ability to interpret the learning context and offer personalized feedback.\n\nTogether, these studies show how LLMs enhance cultural and contextual usability in VR by offering adaptive, linguistically nuanced, and locally grounded interactions. This not only improves accessibility for diverse populations but also enriches the educational and emotional value of VR content.\n\nLLMs improve accessibility, inclusivity, and usability in VR through adaptive multimodal and context-aware interactions. Users with sensory limitations benefit from LLMs because they provide real-time audio guidance, adaptive dialogue, and intu-itive interfaces that enhance VR navigation and responsiveness. The application of LLMs shows great promise for therapeutic interventions because they create emotionally responsive and personalized VR scenarios that benefit patients undergoing anxiety, depression, and PTSD treatments.\n\nThe implementation of LLM-based assistants in educational and social training environments has enhanced communication abilities and learning outcomes and user confidence for autism and cognitive difference users, while gesture-based inputs make the system more accessible to new users. The implementation of LLMs with cultural and contextual adaptations through localized narratives and intelligent tutoring systems demonstrates their ability to enhance user engagement in heritage, educational, and commercial VR experiences. Future research needs to resolve essential challenges, which include deep personalization capabilities, sustained memory retention, and real-time system performance limitations.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Challenges And Limitations",
      "text": "While LLMs offer transformative possibilities for VR games, their integration introduces significant technical, ethical, and usability challenges.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Computational And Performance Constraints",
      "text": "Although many studies explore how language models can enhance interactive systems, most have not yet been tested in real immersive VR settings. Instead, evaluations are often done on desktop platforms, where performance issues like latency, motion tracking, and multimodal input are less demanding. This creates a gap in our understanding of how LLMs behave under real-time, resource-intensive VR conditions, where delays or instability can negatively impact user experience.\n\nSeveral studies report that using LLMs in VR requires substantial computational resources. For example, Maslych et al.  [43]  found that even with local deployment and optimizations like automatic speech recognition (ASR), text-to-speech (TTS), and behavior-state modeling, response times averaged 3.2 seconds-too slow for real-time interaction. Running LLMs locally instead of via cloud APIs helps reduce delay, while behavior-state modeling, which defines agent states like listening or speaking, supports more synchronized interactions. Still, these methods don't fully solve latency issues in VR.\n\nJahangiri and Rahmani  [40]  observed longer delays-over 20 seconds-in LLM-based NPC systems. They combined LLMs with Pursuit Learning Automata (PLA) to address this, creating a hybrid setup that reduced response time to under one second. While promising, this approach still requires careful tuning and is difficult to generalize across different VR environments.\n\nMemory limitations are another critical barrier. As Zheng et al.  [39]  point out, LLMs struggle to maintain consistent conversations over time. Their MemoryRepository system mimics human memory by summarizing past interactions, helping sustain dialogue coherence. However, this adds processing demands, which may not scale well in complex or multi-character VR scenarios.\n\nMaking NPCs aware of their environment adds more complexity. Radež and Bohak  [50]  used image capture and semantic segmentation to let NPCs reference objects and spaces around them. While this improves realism, the real-time processing it requires is complex to achieve on typical consumer VR hardware, making widespread use difficult without sacrificing performance.\n\nSissler  [9]  demonstrated improved NPC dialogue using GPT-3.5 in Unity, but delays from REST API calls still reduced immersion. The study recommends switching to stream-based architectures for faster response. It also highlights the need for expert prompt engineering to achieve natural conversations. Significantly, while LLMs enhance language-based interactions, key NPC behaviors-like movement and planning-still depend on traditional scripting, limiting full autonomy.\n\nLLMs offer rich linguistic and expressive capabilities for VR, but their integration into real-time immersive environments faces significant technical challenges, including latency, memory constraints, and computational overhead. Even with local deployment and optimizations, current systems often fall short of the responsiveness needed for seamless interaction, with some reporting delays up to 20 seconds. Approaches such as hybrid architectures, memory repositories, behaviorstate modeling, and stream-based communication offer partial improvements, yet scalability remains limited. To bridge the gap between expressive AI and immersive VR design, future work should focus on lightweight models, edge computing, and tighter integration with traditional game logic.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Ethical And Safety Considerations",
      "text": "Integrating LLMs into VR games has led to rapid progress in interactive storytelling, emotional expression, and intelligent gameplay. However, as these systems become more adaptive and human-like, a critical question arises: Can we trust AI agents that learn and respond to us in real time? While the potential for immersive and personalized experiences is exciting, it also brings serious ethical and safety concerns, particularly in VR environments where users may build emotional bonds with AI characters  [74] .\n\nOne primary concern is privacy. VR systems collect detailed biometric and behavioral data, such as gaze, voice, movement patterns, and emotional cues. Unlike traditional web apps, this data is continuous and fine-grained. Garrido et al.,  [75]  showed that just a few minutes of telemetry data, like eye tracking and EEG signals, can reveal private information such as a user's gender, income level, or emotional state. These findings emphasize the need for stricter safeguards around data use in immersive settings. One design solution might be to implement consent-aware logging mechanisms, which inform users of what data is being collected and allow them to enable or disable specific data tracking modalities.\n\nIn addition to privacy, LLM-generated content raises risks of bias and misinformation. Yang et al.,  [76]  found that GPTbased agents in mixed-initiative gameplay (MIG) can produce biased or misleading stories, especially problematic in educational or therapeutic games. When LLMs are used without proper moderation, they can unintentionally reinforce harmful stereotypes or distort learning outcomes. Proactive bias mitigation can be addressed at the prompt level through controlled prompt engineering and content filtering techniques tailored to sensitive domains such as education or therapy.\n\nWaghale et al.,  [77]  also warn that LLMs can introduce unfairness into gameplay, especially in multiplayer or competitive environments. Bias in training data or algorithm design may lead to advantages or disadvantages for specific player groups. Procedural content generation using big data can unintentionally reinforce cultural or gender stereotypes. At the same time, using sensitive data to personalize gameplay raises significant privacy concerns.\n\nAnother challenge is the emotional impact of AI-driven empathy systems. Tucek  [62]  showed that emotionally responsive digital characters behave unpredictably or generate inappropriate content; they can harm user trust or reinforce negative perceptions, especially when the goal is to foster empathy toward marginalized communities. To reduce user confusion or mistrust, transparent AI feedback systems can be used-for instance, by showing visual indicators when an NPC is adapting its behavior in real time.\n\nTanksale  [7]  adds that LLMs used in immersive Web3D environments pose additional risks when combining real-time personalization with procedural generation. Without oversight, these systems may create biased or culturally insensitive content, especially when trained on unfiltered internet data.\n\nFinally, as Damianova  [30]  emphasizes, ethical considerations must be built into the design process, not added after deployment. Developers should take responsibility for ethical practices by integrating fairness, inclusivity, and safety principles throughout the design cycle.\n\nThese studies highlight the urgent need for responsible AI design, content moderation, and strong privacy protections in VR. Without clear ethical safeguards, the line between helpful personalization and harmful manipulation becomes dangerously thin.\n\nAs LLMs bring emotional intelligence and real-time responsiveness into VR games, they also introduce significant ethical and safety risks. Studies consistently show that privacy concerns, algorithmic bias, and unpredictable emotional impacts are not hypothetical-they are already emerging in practice. While adaptive AI systems enhance personalization, they also risk reinforcing harmful stereotypes or manipulating user behavior without clear consent. Addressing these concerns requires embedding fairness, transparency, and safety protocols into every stage of design and deployment, particularly as immersive AI interactions grow more lifelike and emotionally persuasive.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "User Experience And Immersion Issues",
      "text": "Creating virtual characters that feel truly lifelike remains one of the biggest challenges in VR game design, especially when using LLMs for NPC dialogue. At the same time, these models can generate fluent and responsive language, which alone does not guarantee engaging or believable interactions in immersive environments. Research shows that the biggest obsta-cles to user experience are unnatural reactions, inconsistent personality, limited conversational structure, and memory lapses over time.\n\nMaslych et al.,  [43]  conducted a pilot study revealing low realism scores (3.12 out of 7) for LLM-driven avatars in taskbased VR scenarios. The main issues were minimal animations, limited to basic lip-sync and head movement, which broke immersion. Participants noted that adding facial expressions, idle behaviors, and body motion could improve believability. Visual feedback cues, such as state lights and loading indicators, also played a key role in maintaining user trust by signaling that the avatar was actively listening or processing input.\n\nTonini's international study on voice-based VR gameplay highlighted similar issues. While players appreciated LLMpowered NPCs' emotional tone and responsiveness, they also found conversations repetitive and sometimes generic. The lack of dialogue variety and slow reaction times made interactions feel scripted rather than natural. Players enjoyed the freedom of open voice interaction, but the underlying AI often failed to sustain flexible, emotionally rich conversations over time  [33] .\n\nA mixed-reality study showed that human-like avatars significantly improved memory retention and immersion compared to symbolic or abstract characters  [38] . This suggests that avatar design, specifically realism, expressiveness, and embodiment, is critical for building emotional user connections. However, delivering this level of engagement requires more than fluent speech. Multimodal feedback, personality modeling, and memory-aware systems must work together to create believable and responsive virtual characters  [78] .\n\nNarrative consistency is another primary concern, especially in emergent gameplay. Peng et al.,  [11]  showed that while players can freely co-create stories with LLM-driven characters, this often leads to fragmented or inconsistent plotlines. When systems fail to remember player actions or goals, the story can feel disjointed and lose its emotional impact.\n\nThese findings show that while LLM-based NPCs can deliver moments of intense immersion, they still struggle with sustaining realistic, emotionally coherent, and context-aware interactions. To address this, future work must improve animation, clarity of feedback, narrative memory, and long-term emotional engagement.\n\nAnother significant barrier to immersive experience in VR is cybersickness-a form of motion-induced discomfort that affects many users, particularly during fast-paced or unstructured gameplay. It is usually like a physiological response marked as nausea, disorientation, or dizziness. To address this issue, the literature offers a variety of techniques designed to detect and reduce the severity and frequency of cybersickness symptoms  [79] .\n\nPhysiological signal analysis has shown great promise for cybersickness detection. Islam et al.  [80]  proposed a deep learning-based method that uses heart rate, breathing rate, heart rate variability, and galvanic skin response to automatically detect and predict cybersickness severity. Their simplified CNN-LSTM model achieved 97.44% accuracy for current state detection and 87.38% for predicting future symptoms, outperforming traditional classifiers. This method provides a robust, real-time solution by leveraging subtle physiological changes that correlate with user-reported discomfort.\n\nIn addition to physiological signal analysis approaches, Monteiro et al.  [81]  demonstrated that trajectory compression rate can also be used as a marker to identify cybersickness during VR gameplay. The authors found a clear correlation between variations in compression rate and users' Discomfort Scores, indicating that changes in movement patterns-such as increased rotation or erratic navigation-are linked to higher levels of sickness. A simple neural network model using compression rate and its variation as input was able to accurately predict whether discomfort would increase or decrease over time.\n\nFurthermore, Wang et al.  [82]  presented a novel method for predicting simulator sickness (SS) in real time using only ingame character movement and eye-tracking data, without the need for expensive or external physiological sensors. The authors trained a long short-term memory (LSTM) neural network on data collected from three VR games and achieved an SS prediction accuracy of 83.4% for players with high sensitivity to SS. Their findings support the hypothesis that intense character motion and negative eye movement patterns are strong indicators of SS in VR environments.\n\nWhile LLM-powered NPCs enhance user interaction through fluent dialogue and emotional tone, they often fall short in delivering sustained immersion due to limited animation, repetitive responses, and poor narrative memory. Studies show that visual feedback, avatar realism, and consistent personality cues are essential for believable interactions. However, fragmented storytelling and generic conversations remain common, especially over time. Achieving deeper engagement will require integrating expressive multimodal feedback, persistent memory, and emotionally aware behavior into LLM-driven virtual characters.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Scalability And Deployment",
      "text": "While many LLM-based prototypes in VR show promising capabilities, scaling them for real-world, large-scale applications remains a significant challenge. Transitioning from controlled lab settings to multiplayer or persistent virtual environments requires more than model performance-it demands robust infrastructure, cost-effective deployment, and compatibility with consumer hardware. As VR games become more complex and interactive, these demands intensify.\n\nOne of the main bottlenecks is the high computational cost of real-time LLM inference, especially in multi-agent settings where several NPCs must perceive, reason, and respond simultaneously. Techniques like retrieval-augmented generation and modular prompting aim to reduce memory load and latency, but their effectiveness is limited in fast-paced, interactive environments  [43, 60] . Multi-module systems like LLMR, while offering improved scene understanding, often face execution delays due to orchestration overhead, including planning, debugging, and memory updates  [60] .\n\nHybrid system designs offer one potential solution. For example, combining LLMs with Pursuit Learning Automata (PLA) allows agents to learn user tone preferences and select pre-generated responses instead of generating them from scratch. This reduces processing load and supports smoother, long-term dialogue  [77, 40] . However, these methods are still in early stages and have only been tested in limited, single-user RPG setups, raising questions about scalability to multiplayer environments.\n\nFrom a deployment standpoint, technical hurdles also persist. Many LLM-based VR systems rely on cloud APIs, leading to latency, privacy concerns, and service interruptions-all of which affect real-time performance. Integrating LLMs into engines like Unity often requires third-party middleware and custom tools, increasing development time and limiting crossplatform compatibility. While frameworks like SceneCraft showcase the narrative potential of LLMs, they still need optimization for real-time, in-engine deployment  [49] .\n\nMemory and model management are also critical. Persistent agents must track long-term context, manage session memory, and coordinate with other agents. Current memory systems, such as those proposed by Buongiorno et al. and Zheng et al.  [44, 39] , improve dialogue continuity and increase system resource demands. These systems become unsustainable in larger, ongoing game worlds without efficient memory pruning, modular retrieval, and compression strategies.\n\nUltimately, scaling LLMs for real-time VR applications will require an integrated approach combining efficient local inference, modular design, optimized memory, and seamless engine integration. LLM-powered VR games will unlikely move beyond experimental prototypes into mainstream, persistent virtual worlds without these developments.\n\nWhile LLMs demonstrate strong potential in VR game prototypes, their deployment at scale faces major challenges. High computational demands, latency from cloud APIs, and orchestration overhead limit real-time performance, especially in multi-agent or multiplayer settings. Hybrid systems and modular memory frameworks offer partial solutions, but they remain largely untested in large-scale environments. To transition from experimental setups to persistent virtual worlds, future systems must integrate lightweight inference, robust memory management, and native engine compatibility for seamless, scalable deployment. Beyond these LLM-specific challenges, users are also confronted with cybersickness-a more general limitation of VR environments, and this physiological discomfort can severely impact immersion.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Future Directions",
      "text": "As the integration of LLMs into VR games continues to evolve, several key areas emerge that will shape the next generation of immersive, intelligent, and ethically responsible digital experiences. This section outlines promising future directions based on current trends and gaps identified throughout this review.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Advances In Ai For More Realistic Vr Gaming",
      "text": "One of the most promising developments lies in the evolution of multimodal AI models that integrate language, vision, and audio understanding to support holistic, context-aware interaction. Systems such as GPT-4V  [23]  and LLaVA  [24]  already demonstrate the capacity to interpret both images and text, which, when integrated into VR environments, can lead to NPCs that \"see\" and \"hear\" alongside players. These systems could track user gaze, interpret gestures, analyze visual scenes, and generate emotionally resonant, non-verbal responses in real time. Future advancements may enable avatars capable of fullbody interaction understanding, dynamic emotion simulation, and persistent spatial memory across scenes, leading to virtual characters that feel more genuinely human and socially intelligent.\n\nFurthermore, integrating LLMs with emotion recognition and affective computing will likely enhance their capacity for emotionally attuned interactions  [83, 84] . By incorporating data from facial expression tracking, voice tone analysis, and physiological inputs (e.g., heart rate, EEG), NPCs could dynamically respond to player moods, stress levels, or engagement patterns, enabling deeper player-character connections, particularly in therapeutic or educational applications  [85] .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Integration With Other Technologies",
      "text": "Beyond improved realism, future systems will benefit from the convergence of LLMs with complementary AI technologies. For instance, reinforcement Learning (RL) can be used to refine NPC behavior through iterative experience-based optimization, allowing characters to adapt over time and learn from player interactions. With LLMs' generative flexibility, RL could support characters who evolve with players' styles, forming long-term bonds or gameplay strategies  [86] .\n\nProcedural content generation (PCG) is another area where LLMs can synergize with rule-based or stochastic systems to produce personalized worlds, quests, and story arcs in real time. Instead of replacing traditional PCG algorithms, LLMs can enrich them by filling narrative, dialogue, and context-sensitive decision trees with fluid, naturalistic language.\n\nThe Internet of Things (IoT) and wearable technology also offer integration potential for health-focused or location-aware VR experiences  [87, 83] . For example, LLM-driven virtual coaches could adapt content or difficulty in real-time based on a player's heart rate, body temperature, or environment, enhancing fitness, therapy, or safety applications. In a fitness game, if a player's heart rate exceeds a safe threshold, an LLM assistant could reduce the workout intensity while explaining the reasoning and suggesting hydration tips.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Ethical Ai Development For Games",
      "text": "With increasing immersion and personalization come rising ethical stakes. Future systems must embed ethical frameworks directly into the design pipeline, prioritizing bias mitigation, content moderation, explainability, and informed consent  [88] . This is especially urgent as AI characters become more emotionally responsive and persuasive, particularly among vulnerable populations such as children or neurodivergent users.\n\nIt will be key to building transparent AI systems that can explain their decisions, avoid reinforcing stereotypes, and flag potentially harmful outputs. This will also involve developing new benchmarking tools and ethical evaluation protocols that assess emotional impact, fairness, and long-term influence in gameplay, criteria often neglected in traditional usability testing. For example, a narrative-based VR simulation for conflict resolution teaching would use AI characters who modify their communication style and cultural references according to player background and emotional state to provide respectful guidance that avoids cultural or socioeconomic bias  [89] .\n\nUser data privacy must also be foregrounded. With VR systems collecting detailed biometric, motion, and interaction data, LLM-powered applications must ensure secure handling, precise opt-in mechanisms, and compliance with emerging privacy regulations  [90] . Local processing or edge computing may reduce data transfer risks while enabling more responsive realtime AI.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Open-Source And Industry Trends",
      "text": "The growing open-source ecosystem is accelerating the democratization of LLM development and deployment. Models like LLaMA, Mistral, and Baichuan offer competitive performance and increased transparency, allowing smaller studios and researchers to experiment with AI-driven game mechanics without being locked into proprietary APIs. Open frameworks like LangChain, Hugging Face Transformers, and Unity GPT integrations also make it easier to develop modular, customizable LLM agents tailored to specific gameplay scenarios  [91] .\n\nIn parallel, industry leaders like Meta, NVIDIA, and Ope-nAI actively invest in AI-native game engines and toolkits for generative NPCs, suggesting that large-scale, real-time LLM integration will soon become commercially viable. The release of real-time streaming APIs, quantized model deployment solutions, and multimodal interfaces will further reduce latency and memory constraints, facilitating scalable use in complex multi-agent virtual worlds.\n\nFuture research collaborations between academia and industry will be essential to ensure that these tools remain innovative, ethical, and inclusively designed. The open-source and commercial sectors should align around shared principles of transparency, accessibility, and responsible AI deployment.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "This paper has examined the transformative role of Large Language Models in reshaping the landscape of Virtual Reality games. Through a comprehensive review of 62 recent studies, we analyzed the integration of LLMs across key domains: dynamic NPC interactions, procedural storytelling, intelligent game mastering, personalized experiences, accessibility design, and performance-aware deployment.\n\nOur findings indicate that LLMs significantly expand the design space for VR games, enabling more adaptive, expressive, and emotionally resonant virtual characters. They support unscripted narrative branching, real-time user interaction, and socially intelligent behavior that redefines immersion. Moreover, LLMs open new pathways for inclusive and accessible VR systems, offering tailored experiences to diverse user populations, including those with disabilities or language barriers. However, significant challenges remain. Real-time responsiveness, memory management, and latency limit large-scale deployment in complex VR scenarios. Ethical concerns-such as bias, privacy, and emotional manipulation-are amplified by the immersive nature of these experiences and require careful consideration throughout the design and testing processes. Scalability also presents a significant barrier as models must be optimized for resource-constrained hardware and sustained multi-agent interaction.\n\nFuture research should focus on advancing multimodal and hybrid AI systems, integrating reinforcement learning, affective computing, and spatial awareness. Developers and researchers must prioritize ethical AI development by embedding fairness, transparency, and safety into game mechanics and content generation pipelines. Collaboration across open-source communities and industry partners will be essential to make intelligent, inclusive, and creative VR experiences more accessible and impactful. By combining technical innovation with responsible design, LLMs have the potential to fundamentally reshape how we build, experience, and interact within virtual worlds-ushering in a new era of intelligent, human-centered digital play.",
      "page_start": 14,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: provides an overview of this review’s organization",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of the paper structure and organization.",
      "page": 2
    },
    {
      "caption": "Figure 2: provides a visual overview",
      "page": 3
    },
    {
      "caption": "Figure 2: Application areas of Large Language Models in Virtual Reality games",
      "page": 3
    },
    {
      "caption": "Figure 3: Distribution of reviewed papers by publication year.",
      "page": 4
    },
    {
      "caption": "Figure 4: illustrates the approximate number of papers asso-",
      "page": 6
    },
    {
      "caption": "Figure 4: Categorization of reviewed papers based on key application areas.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LLMs in VR Games": "AI Storytelling",
          "“LLMs\nin VR Games”,\n“GPT in VR Games”,\n“NPC Dialogues\nin Virtual Reality”,\n“LLM-based conversational agents in VR”": "“Large Language Models in Gaming”, “Procedural Storytelling with AI”, “AI-generated nar-\nratives in immersive environments”"
        },
        {
          "LLMs in VR Games": "NPCs and Game Masters",
          "“LLMs\nin VR Games”,\n“GPT in VR Games”,\n“NPC Dialogues\nin Virtual Reality”,\n“LLM-based conversational agents in VR”": "“AI-driven NPC interactions\nin VR”, “Intelligent Game Masters\nin VR”, “LLMs as Game\nMasters in VR”, “AI-controlled characters in virtual environments”"
        },
        {
          "LLMs in VR Games": "Player Personalization",
          "“LLMs\nin VR Games”,\n“GPT in VR Games”,\n“NPC Dialogues\nin Virtual Reality”,\n“LLM-based conversational agents in VR”": "“Personalized Player Experience with LLMs”, “Adaptive narrative systems in VR”, “Behavior-\ndriven dialogue generation”"
        },
        {
          "LLMs in VR Games": "Performance Issues",
          "“LLMs\nin VR Games”,\n“GPT in VR Games”,\n“NPC Dialogues\nin Virtual Reality”,\n“LLM-based conversational agents in VR”": "“LLM latency in VR games”, “Real-time response in AI-driven gameplay”, “Computational\noverhead in immersive environments”"
        },
        {
          "LLMs in VR Games": "Memory and Ethics",
          "“LLMs\nin VR Games”,\n“GPT in VR Games”,\n“NPC Dialogues\nin Virtual Reality”,\n“LLM-based conversational agents in VR”": "“LLM memory management for virtual agents”, “Ethical issues in VR AI”, “Long-term inter-\naction in AI systems”"
        },
        {
          "LLMs in VR Games": "Bias and Safety",
          "“LLMs\nin VR Games”,\n“GPT in VR Games”,\n“NPC Dialogues\nin Virtual Reality”,\n“LLM-based conversational agents in VR”": "“Bias in LLM-based NPCs”, “Trust and safety in conversational agents”, “Content moderation\nin AI-driven games”"
        },
        {
          "LLMs in VR Games": "Immersion",
          "“LLMs\nin VR Games”,\n“GPT in VR Games”,\n“NPC Dialogues\nin Virtual Reality”,\n“LLM-based conversational agents in VR”": "“Immersion and believability in AI-driven VR”, “User experience with AI NPCs”, “User ex-\nperiences in VR”"
        },
        {
          "LLMs in VR Games": "Deployment",
          "“LLMs\nin VR Games”,\n“GPT in VR Games”,\n“NPC Dialogues\nin Virtual Reality”,\n“LLM-based conversational agents in VR”": "“Deployment challenges for LLMs in VR”, “Scalability of LLMs in VR games”, “Integration\nof LLMs into game engines”"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Time Frame": "Publication Type",
          "Studies published between 2018 and 2025, covering the period in which transformer-based LLMs emerged\nand were first explored in immersive technologies": "Peer-reviewed journal articles, conference papers, or technical reports with publicly available full texts"
        },
        {
          "Time Frame": "Topical Relevance",
          "Studies published between 2018 and 2025, covering the period in which transformer-based LLMs emerged\nand were first explored in immersive technologies": "Focused on the integration of LLMs into VR environments, with a primary emphasis on gaming contexts"
        },
        {
          "Time Frame": "Related\nApplica-\ntions",
          "Studies published between 2018 and 2025, covering the period in which transformer-based LLMs emerged\nand were first explored in immersive technologies": "Papers addressing adjacent VR domains such as education, training, or accessibility using LLMs, provided\nthey offer transferable insights for VR gaming"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Language": "Scope",
          "Studies not written in English, to ensure consistency and accessibility during the review process": "Publications unrelated to either LLMs or VR/immersive environments, or papers where the two technologies\nwere discussed independently without meaningful integration"
        },
        {
          "Language": "Depth of Contribu-\ntion",
          "Studies not written in English, to ensure consistency and accessibility during the review process": "Conceptual or commentary papers without technical implementation, empirical evaluation, or practical de-\nsign frameworks involving LLMs in VR"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Stage": "Total papers retrieved from databases",
          "Papers": "528"
        },
        {
          "Stage": "After duplicate removal",
          "Papers": "422"
        },
        {
          "Stage": "After title-based filtering",
          "Papers": "250"
        },
        {
          "Stage": "Abstract-screened papers",
          "Papers": "250"
        },
        {
          "Stage": "Full-text papers reviewed",
          "Papers": "89"
        },
        {
          "Stage": "Final papers included in review",
          "Papers": "62"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Llms in eduverse: Llmintegrated english educational game in metaverse",
      "authors": [
        "K Plupattanakit",
        "P Suntichaikul",
        "P Taveekitworachai",
        "R Thawonmas",
        "J White",
        "K Sookhanaphibarn",
        "W Choensawat"
      ],
      "year": "2024",
      "venue": "2024 IEEE 13th Global Conference on Consumer Electronics (GCCE)",
      "doi": "10.1109/GCCE62371.2024.10760474"
    },
    {
      "citation_id": "2",
      "title": "Improving collaborative learning performance based on llm virtual assistant",
      "authors": [
        "R Wei",
        "K Li",
        "J Lan"
      ],
      "year": "2024",
      "venue": "2024 13th International Conference on Educational and Information Technology (ICEIT), Institute of Electrical and Electronics Engineers",
      "doi": "10.1109/ICEIT61397.2024.10540942"
    },
    {
      "citation_id": "3",
      "title": "Large language models: A survey",
      "authors": [
        "S Minaee",
        "T Mikolov",
        "N Nikzad",
        "M Chenaghlu",
        "R Socher",
        "X Amatriain",
        "J Gao"
      ],
      "year": "2024",
      "venue": "Large language models: A survey",
      "arxiv": "arXiv:2402.06196"
    },
    {
      "citation_id": "4",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Proceedings of the 34th International Conference on Neural Information Processing Systems (NeurIPS 2020), NeurIPS '20"
    },
    {
      "citation_id": "5",
      "title": "State of the art of virtual reality technology",
      "authors": [
        "C Anthes",
        "R García-Hernández",
        "M Wiedemann",
        "D Kranzlmüller"
      ],
      "year": "2016",
      "venue": "State of the art of virtual reality technology",
      "doi": "10.1109/AERO.2016.7500674"
    },
    {
      "citation_id": "6",
      "title": "Leveraging large language models for enhanced vr development: Insights and challenges",
      "authors": [
        "A Alkhayat",
        "B Ciranni",
        "R Tumuluri",
        "R Tulasi"
      ],
      "year": "2024",
      "venue": "2024 IEEE Gaming, Entertainment, and Media Conference",
      "doi": "10.1109/GEM61861.2024.10585495"
    },
    {
      "citation_id": "7",
      "title": "2023 International Conference on Computational Science and Computational Intelligence (CSCI)",
      "authors": [
        "V Tanksale"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Computational Science and Computational Intelligence (CSCI)",
      "doi": "10.1109/CSCI62032.2023.00045"
    },
    {
      "citation_id": "8",
      "title": "Large language models and video games: A preliminary scoping review",
      "authors": [
        "P Sweetser"
      ],
      "venue": "Proceedings of the 6th ACM Conference on Conversational User Interfaces, CUI '24",
      "doi": "10.1145/3640794.3665582"
    },
    {
      "citation_id": "9",
      "title": "Enhancing non-player characters in unity 3d using gpt-3",
      "authors": [
        "J Sissler"
      ],
      "year": "2024",
      "venue": "ACM Games",
      "doi": "10.1145/3662003"
    },
    {
      "citation_id": "10",
      "title": "Using llms to animate interactive story characters with emotions and personality",
      "authors": [
        "A Normoyle",
        "J Sedoc",
        "F Durupinar"
      ],
      "year": "2024",
      "venue": "2024 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)",
      "doi": "10.1109/VRW62533.2024.00124"
    },
    {
      "citation_id": "11",
      "title": "Playerdriven emergence in llm-driven game narrative",
      "authors": [
        "X Peng",
        "J Quaye",
        "S Rao",
        "W Xu",
        "P Botchway",
        "C Brockett",
        "N Jojic",
        "G Desgarennes",
        "K Lobb",
        "M Xu",
        "J Leandro",
        "C Jin",
        "B Dolan"
      ],
      "year": "2024",
      "venue": "2024 IEEE Conference on Games (CoG)",
      "doi": "10.1109/CoG60054.2024.10645607"
    },
    {
      "citation_id": "12",
      "title": "A systematic review of ai, vr, and llm applications in special education: Opportunities, challenges, and future directions",
      "authors": [
        "E Voultsiou",
        "L Moussiades"
      ],
      "year": "2025",
      "venue": "Education and Information Technologies",
      "doi": "10.1007/s10639-025-13550-4"
    },
    {
      "citation_id": "13",
      "title": "Embedding large language models into extended reality: Opportunities and challenges for inclusion, engagement, and privacy",
      "authors": [
        "E Bozkir",
        "S Özdel",
        "K Lau",
        "M Wang",
        "H Gao",
        "E Kasneci"
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM Conversational User Interfaces (CUI '24)",
      "doi": "10.1145/3640794.3665563"
    },
    {
      "citation_id": "14",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI Blog"
    },
    {
      "citation_id": "15",
      "title": "Large language models and games: A survey and roadmap",
      "authors": [
        "R Gallotta",
        "G Todd",
        "M Zammit",
        "S Earle",
        "A Liapis",
        "J Togelius",
        "G Yannakakis"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Games",
      "doi": "10.1109/TG.2024.3461510"
    },
    {
      "citation_id": "16",
      "title": "A survey of llm datasets: From autoregressive model to ai chatbot",
      "authors": [
        "F Du",
        "X.-J Ma",
        "J.-R Yang",
        "Y Liu",
        "C.-R Luo",
        "X.-B Wang",
        "H.-O Jiang",
        "X Jing"
      ],
      "year": "2024",
      "venue": "Journal of Computer Science and Technology",
      "doi": "10.1007/s11390-024-3767-3"
    },
    {
      "citation_id": "17",
      "title": "History, development, and principles of large language models: An introductory survey, AI and EthicsPublished",
      "authors": [
        "Z Wang",
        "Z Chu",
        "T Doan",
        "S Ni",
        "M Yang",
        "W Zhang"
      ],
      "year": "2024",
      "venue": "History, development, and principles of large language models: An introductory survey, AI and EthicsPublished",
      "doi": "10.1007/s43681-024-00583-7"
    },
    {
      "citation_id": "18",
      "title": "",
      "authors": [
        "M Zong",
        "B Krishnamachari"
      ],
      "year": "2022",
      "venue": "",
      "arxiv": "arXiv:2212.00857"
    },
    {
      "citation_id": "19",
      "title": "Large language models' expert-level global history knowledge benchmark",
      "authors": [
        "J Hauser",
        "D Kondor",
        "J Reddish",
        "M Benam",
        "E Cioni",
        "F Villa",
        "J Bennett",
        "D Hoyer",
        "P Francois",
        "P Turchin",
        "R Del Rio-Chanona"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "doi": "10.18653/v1/N19-1423"
    },
    {
      "citation_id": "21",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar",
        "A Rodriguez",
        "A Joulin",
        "E Grave",
        "G Lample"
      ],
      "year": "2023",
      "venue": "Llama: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "22",
      "title": "Exploring language models: A comprehensive survey and analysis",
      "authors": [
        "A Singh"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Research Methodologies in Knowledge Management, Artificial Intelligence and Telecommunication Engineering (RMKMATE)",
      "doi": "10.1109/RMKMATE59243.2023.10369423"
    },
    {
      "citation_id": "23",
      "title": "Gpt-4 technical report",
      "year": "2024",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "24",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2023",
      "venue": "Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS '23"
    },
    {
      "citation_id": "25",
      "title": "Origin of gaming in virtual reality",
      "authors": [
        "D Adhikary",
        "A Maheta"
      ],
      "year": "2017",
      "venue": "International Journal of Recent Trends in Engineering & Research (IJRTER)"
    },
    {
      "citation_id": "26",
      "title": "Second life: A new platform for education",
      "authors": [
        "Q Zhu",
        "T Wang",
        "Y Jia"
      ],
      "year": "2007",
      "venue": "First IEEE International Symposium on Information Technologies and Applications in Education",
      "doi": "10.1109/ISITAE.2007.4409270"
    },
    {
      "citation_id": "27",
      "title": "",
      "authors": [
        "Geeksforgeeks"
      ],
      "venue": ""
    },
    {
      "citation_id": "28",
      "title": "From reality to virtual worlds: The role of photogrammetry in game development",
      "authors": [
        "S Berrezueta-Guzman",
        "A Koshelev",
        "S Wagner"
      ],
      "year": "2025",
      "venue": "From reality to virtual worlds: The role of photogrammetry in game development",
      "arxiv": "arXiv:2505.16951"
    },
    {
      "citation_id": "29",
      "title": "A discussion of cybersickness in virtual environments",
      "authors": [
        "J Laviola"
      ],
      "year": "2000",
      "venue": "SIGCHI Bulletin",
      "doi": "10.1145/333329.333344"
    },
    {
      "citation_id": "30",
      "title": "Serious games supported by virtual reality-literature review",
      "authors": [
        "N Damianova",
        "S Berrezueta-Guzman"
      ],
      "year": "2025",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2025.3544022"
    },
    {
      "citation_id": "31",
      "title": "Developing an immersive game-based learning platform with generative artificial intelligence and virtual reality technologies -\"learningversevr",
      "authors": [
        "Y Song",
        "K Wu",
        "J Ding"
      ],
      "year": "2024",
      "venue": "Computers & Education: X Reality",
      "doi": "10.1016/j.cexr.2024.100069"
    },
    {
      "citation_id": "32",
      "title": "Integrating gpt as an assistant for low-cost virtual reality escape-room games",
      "authors": [
        "A Rychert",
        "M Ganuza",
        "M Selzer"
      ],
      "year": "2024",
      "venue": "IEEE Computer Graphics and Applications",
      "doi": "10.1109/MCG.2024.3426314"
    },
    {
      "citation_id": "33",
      "title": "A study of player experience and interaction in a voice interaction vr game featuring ai-driven non-player characters, Master's thesis",
      "authors": [
        "L Tonini"
      ],
      "year": "2024",
      "venue": "A study of player experience and interaction in a voice interaction vr game featuring ai-driven non-player characters, Master's thesis"
    },
    {
      "citation_id": "34",
      "title": "Evaluating usability and engagement of large language models in virtual reality for traditional scottish curling",
      "authors": [
        "K Lau",
        "E Bozkir",
        "H Gao",
        "E Kasneci"
      ],
      "year": "2024",
      "venue": "Evaluating usability and engagement of large language models in virtual reality for traditional scottish curling",
      "arxiv": "arXiv:2408.09285"
    },
    {
      "citation_id": "35",
      "title": "Language-driven play: Large language models as game-playing agents in slay the spire",
      "authors": [
        "B Bateni",
        "J Whitehead"
      ],
      "venue": "Proceedings of the 19th International Conference on the Foundations of Digital Games, FDG '24",
      "doi": "10.1145/3649921.3650013"
    },
    {
      "citation_id": "36",
      "title": "The effect of llm-based npc emotional states on player emotions: An analysis of interactive game play",
      "authors": [
        "A Marincioni",
        "M Miltiadous",
        "K Zacharia",
        "R Heemskerk",
        "G Doukeris",
        "M Preuss",
        "G Barbero"
      ],
      "year": "2024",
      "venue": "2024 IEEE Conference on Games (CoG), Institute of Electrical and Electronics Engineers",
      "doi": "10.1109/CoG60054.2024.10645631"
    },
    {
      "citation_id": "37",
      "title": "Immersive experience with non-player characters: Dynamic dialogue",
      "authors": [
        "M Hasani",
        "Y Udjaja"
      ],
      "year": "2021",
      "venue": "2021 1st International Conference on Computer Science and Artificial Intelligence (ICCSAI)",
      "doi": "10.1109/ICCSAI53272.2021.9609725"
    },
    {
      "citation_id": "38",
      "title": "Free-form conversation with human and symbolic avatars in mixed reality",
      "authors": [
        "J Zhu",
        "R Kumaran",
        "C Xu",
        "T Höllerer"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), Institute of Electrical and Electronics Engineers",
      "doi": "10.1109/ISMAR59233.2023.00090"
    },
    {
      "citation_id": "39",
      "title": "Memoryrepository for ai npc",
      "authors": [
        "S Zheng",
        "K He",
        "L Yang",
        "J Xiong"
      ],
      "year": "2024",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2024.3393485"
    },
    {
      "citation_id": "40",
      "title": "Balancing game satisfaction and resource efficiency: Llm and pursuit learning automata for npc dialogues",
      "authors": [
        "M Jahangiri",
        "P Rahmani"
      ],
      "year": "2024",
      "venue": "2024 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA), Institute of Electrical and Electronics Engineers",
      "doi": "10.1109/HORA61326.2024.10550450"
    },
    {
      "citation_id": "41",
      "title": "Press a or wave: User expectations for npc interactions and nonverbal behaviour in virtual reality",
      "authors": [
        "M Yin",
        "R Xiao"
      ],
      "year": "2024",
      "venue": "Proc. ACM Hum.-Comput. Interact",
      "doi": "10.1145/3677098"
    },
    {
      "citation_id": "42",
      "title": "Toward realistic hands gesture interface: Keeping it simple for developers and machines",
      "authors": [
        "E Krupka",
        "K Karmon",
        "N Bloom",
        "D Freedman",
        "I Gurvich",
        "A Hurvitz",
        "I Leichter",
        "Y Smolin",
        "Y Tzairi",
        "A Vinnikov",
        "A Bar-Hillel"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems, CHI '17, Association for Computing Machinery",
      "doi": "10.1145/3025453.3025508"
    },
    {
      "citation_id": "43",
      "title": "Takeaways from applying llm capabilities to multiple conversational avatars in a vr pilot study",
      "authors": [
        "M Maslych",
        "C Pumarada",
        "A Ghasemaghaei",
        "J Laviola"
      ],
      "year": "2025",
      "venue": "Takeaways from applying llm capabilities to multiple conversational avatars in a vr pilot study",
      "arxiv": "arXiv:2501.00168"
    },
    {
      "citation_id": "44",
      "title": "Procedural artificial narrative using generative ai for turn-based, roleplaying video games",
      "authors": [
        "S Buongiorno",
        "L Klinkert",
        "Z Zhaung",
        "T Chawla",
        "C Clark"
      ],
      "year": "2024",
      "venue": "Procedural artificial narrative using generative ai for turn-based, roleplaying video games",
      "doi": "10.1609/aiide.v20i1.31876"
    },
    {
      "citation_id": "45",
      "title": "Generating role-playing game quests with gpt language models",
      "authors": [
        "S Värtinen",
        "P Hämäläinen",
        "C Guckelsberger"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Games",
      "doi": "10.1109/TG.2022.3228480"
    },
    {
      "citation_id": "46",
      "title": "Personalized quest and dialogue generation in role-playing games: A knowledge graphand language model-based approach",
      "authors": [
        "T Ashby",
        "B Webb",
        "G Knapp",
        "J Searle",
        "N Fulda"
      ],
      "venue": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, CHI '23, Association for Computing Machinery",
      "doi": "10.1145/3544548.3581441"
    },
    {
      "citation_id": "47",
      "title": "Towards immersive computational storytelling: Card-framework for enhanced persona-driven dialogues",
      "authors": [
        "L Bingli",
        "D Vargas"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Games",
      "doi": "10.1109/TG.2024.3466898"
    },
    {
      "citation_id": "48",
      "title": "Baichuan",
      "authors": [
        "A Yang",
        "B Xiao",
        "B Wang",
        "B Zhang",
        "C Bian",
        "C Yin",
        "C Lv",
        "D Pan",
        "D Wang",
        "D Yan",
        "F Yang",
        "F Deng",
        "F Wang",
        "F Liu",
        "G Ai",
        "G Dong",
        "H Zhao",
        "H Xu",
        "H Sun",
        "H Zhang",
        "H Liu",
        "J Ji",
        "J Xie",
        "J Dai",
        "K Fang",
        "L Su",
        "L Song",
        "L Liu",
        "L Ru",
        "L Ma",
        "M Wang",
        "M Liu",
        "M Lin",
        "N Nie",
        "P Guo",
        "R Sun",
        "T Zhang",
        "T Li",
        "T Li",
        "W Cheng",
        "W Chen",
        "X Zeng",
        "X Wang",
        "X Chen",
        "X Men",
        "X Yu",
        "X Pan",
        "Y Shen",
        "Y Wang",
        "Y Li",
        "Y Jiang",
        "Y Gao",
        "Y Zhang",
        "Z Zhou",
        "Z Wu"
      ],
      "year": "2023",
      "venue": "Baichuan",
      "arxiv": "arXiv:2309.10305"
    },
    {
      "citation_id": "49",
      "title": "Automating interactive narrative scene generation in digital games with large language models",
      "authors": [
        "V Kumaran",
        "J Rowe",
        "B Mott",
        "J Lester"
      ],
      "venue": "Proceedings of the Nineteenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, AIIDE '23",
      "doi": "10.1609/aiide.v19i1.27504"
    },
    {
      "citation_id": "50",
      "title": "Integrating environmental awareness into npcs: Contextual conversational interaction in games",
      "authors": [
        "G Radež",
        "C Bohak"
      ],
      "venue": "Proceedings of the Human-Computer Interaction Slovenia 2024 (HCI-SI 2024)"
    },
    {
      "citation_id": "51",
      "title": "Exploring large language modeldriven agents for environment-aware spatial interactions and conversations in virtual reality role-play scenarios",
      "authors": [
        "Z Li",
        "H Zhang",
        "C Peng",
        "R Peiris"
      ],
      "year": "2025",
      "venue": "2025 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)",
      "doi": "10.1109/VR59515.2025.00025"
    },
    {
      "citation_id": "52",
      "title": "Creative contextual dialog adaptation in an open world rpg",
      "authors": [
        "M Hämäläinen",
        "K Alnajjar"
      ],
      "year": "2019",
      "venue": "Proceedings of the 14th International Conference on the Foundations of Digital Games, FDG '19",
      "doi": "10.1145/3337722.3341865"
    },
    {
      "citation_id": "53",
      "title": "Exploring the potential of chatgpt as a dungeon master in dungeons & dragons tabletop game",
      "authors": [
        "T Triyason"
      ],
      "venue": "Proceedings of the 13th International Conference on Advances in Information Technology, IAIT '23",
      "doi": "10.1145/3628454.3628457"
    },
    {
      "citation_id": "54",
      "title": "Towards computational support with language models for ttrpg game masters",
      "authors": [
        "J Kelly",
        "M Mateas",
        "N Wardrip-Fruin"
      ],
      "venue": "Proceedings of the 18th International Conference on the Foundations of Digital Games, FDG '23",
      "doi": "10.1145/3582437.3587202"
    },
    {
      "citation_id": "55",
      "title": "You have thirteen hours in which to solve the labyrinth: Enhancing ai game masters with function calling",
      "authors": [
        "J Song",
        "A Zhu",
        "C Callison-Burch"
      ],
      "year": "2024",
      "venue": "You have thirteen hours in which to solve the labyrinth: Enhancing ai game masters with function calling",
      "arxiv": "arXiv:2409.06949"
    },
    {
      "citation_id": "56",
      "title": "Dungeons, dragons, and emotions: A preliminary study of player sentiment in llm-driven ttrpgs",
      "authors": [
        "X You",
        "P Taveekitworachai",
        "S Chen",
        "M Gursesli",
        "X Li",
        "Y Xia",
        "R Thawonmas"
      ],
      "venue": "Proceedings of the 19th International Conference on the Foundations of Digital Games, FDG '24",
      "doi": "10.1145/3649921.3656991"
    },
    {
      "citation_id": "57",
      "title": "Integrating gpt as an assistant for low-cost virtual reality escape-room games",
      "authors": [
        "A Rychert",
        "M Ganuza",
        "M Selzer"
      ],
      "year": "2024",
      "venue": "IEEE Computer Graphics and Applications",
      "doi": "10.1109/MCG.2024.3426314"
    },
    {
      "citation_id": "58",
      "title": "Virtual reality in multiplayer carrom game with artificial intelligence",
      "authors": [
        "M Khan",
        "F Hassan",
        "M Usman",
        "U Ansari",
        "S Noor"
      ],
      "year": "2018",
      "venue": "12th International Conference on Mathematics",
      "doi": "10.1109/MACS.2018.8628394"
    },
    {
      "citation_id": "59",
      "title": "Seek-and-take games of heterogeneous agent teams with large language model",
      "authors": [
        "H Li",
        "P Yi",
        "D Wei",
        "W Bai"
      ],
      "year": "2024",
      "venue": "2024 China Automation Congress (CAC)",
      "doi": "10.1109/CAC63892.2024.10864895"
    },
    {
      "citation_id": "60",
      "title": "Real-time prompting of interactive worlds using large language models",
      "authors": [
        "F La Torre",
        "C Fang",
        "H Huang",
        "A Banburski-Fahey",
        "J Fernandez",
        "J Lanier"
      ],
      "venue": "Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI '24, Association for Computing Machinery",
      "doi": "10.1145/3613904.3642579"
    },
    {
      "citation_id": "61",
      "title": "Application of chatgpt-integrated npcs to enhance virtual reality brainstorming",
      "authors": [
        "X.-K Lin",
        "C.-W Shiu",
        "N.-Y Pai"
      ],
      "year": "2024",
      "venue": "2024 IEEE 13th Global Conference on Consumer Electronics (GCCE)",
      "doi": "10.1109/GCCE62371.2024.10760995"
    },
    {
      "citation_id": "62",
      "title": "Enhancing empathy through personalized ai-driven experiences and conversations with digital humans in video games",
      "authors": [
        "T Tucek"
      ],
      "year": "2024",
      "venue": "Companion Proceedings of the 2024 Annual Symposium on Computer-Human Interaction in Play, CHI PLAY Companion '24, Association for Computing Machinery",
      "doi": "10.1145/3665463.3678856"
    },
    {
      "citation_id": "63",
      "title": "Who's watching me?: Exploring the impact of audience familiarity on player performance, experience, and exertion in virtual reality exergames",
      "authors": [
        "Z Guo",
        "W Xu",
        "J Zhang",
        "H Wang",
        "C.-H Lo",
        "H.-N Liang"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Symposium on Mixed and Augmented Reality (IS-MAR)",
      "doi": "10.1109/ISMAR59233.2023.00077"
    },
    {
      "citation_id": "64",
      "title": "Envisionvr: A scene interpretation tool for visual accessibility in virtual reality",
      "authors": [
        "J Chen",
        "R Galindo Esparza",
        "V Garaj",
        "P Kristensson",
        "J Dudley"
      ],
      "year": "2025",
      "venue": "Envisionvr: A scene interpretation tool for visual accessibility in virtual reality",
      "arxiv": "arXiv:2502.03564"
    },
    {
      "citation_id": "65",
      "title": "Time to get personal: Individualised virtual reality for mental health",
      "authors": [
        "N Baghaei",
        "L Stemmet",
        "A Hlasnik",
        "K Emanov",
        "S Hach",
        "J Naslund",
        "M Billinghurst",
        "I Khaliq",
        "H.-N Liang"
      ],
      "year": "2020",
      "venue": "Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3334480.3382932"
    },
    {
      "citation_id": "66",
      "title": "Embodying self-compassion within virtual reality and its effects on patients with depression",
      "authors": [
        "C Falconer",
        "A Rovira",
        "J King",
        "P Gilbert",
        "A Antley",
        "P Fearon",
        "N Ralph",
        "M Slater",
        "C Brewin"
      ],
      "year": "2016",
      "venue": "BJPsych Open",
      "doi": "10.1192/bjpo.bp.115.002147"
    },
    {
      "citation_id": "67",
      "title": "Virtual reality for supporting the treatment of depression and anxiety: Scoping review",
      "authors": [
        "N Baghaei",
        "V Chitale",
        "A Hlasnik",
        "L Stemmet",
        "H.-N Liang",
        "R Porter"
      ],
      "year": "2021",
      "venue": "JMIR Mental Health",
      "doi": "10.2196/29681"
    },
    {
      "citation_id": "68",
      "title": "The use of videogames and virtual reality for the assessment of anxiety and depression: A scoping review, Games for",
      "authors": [
        "V Chitale",
        "N Baghaei",
        "D Playne",
        "H.-N Liang",
        "Y Zhao",
        "A Erensoy",
        "Y Ahmad"
      ],
      "year": "2022",
      "venue": "Health Journal",
      "doi": "10.1089/g4h.2021.0227"
    },
    {
      "citation_id": "69",
      "title": "A therapeutic roleplaying vr game for children with intellectual disabilities",
      "authors": [
        "S Berrezueta-Guzman",
        "W Chen",
        "S Wagner"
      ],
      "year": "2025",
      "venue": "A therapeutic roleplaying vr game for children with intellectual disabilities",
      "arxiv": "arXiv:2507.19114"
    },
    {
      "citation_id": "70",
      "title": "Exploring the use of large language model-driven chatbots in virtual reality to train autistic individuals in job communication skills",
      "authors": [
        "Z Li",
        "P Babar",
        "M Barry",
        "R Peiris"
      ],
      "year": "2024",
      "venue": "Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems, CHI EA '24, Association for Computing Machinery",
      "doi": "10.1145/3613905.3651996"
    },
    {
      "citation_id": "71",
      "title": "Human interaction in virtual and mixed reality through hand tracking",
      "authors": [
        "S Geetha",
        "G Aditya",
        "C Reddy",
        "G Nischith"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)",
      "doi": "10.1109/CONECCT62155.2024.10677239"
    },
    {
      "citation_id": "72",
      "title": "Sasirangan cloth recognition and shopping experience simulation based on virtual reality game with non-player character integration",
      "authors": [
        "A Subandi",
        "S Syahidi",
        "K Zakiah",
        "A Kiyokawa",
        "M Riyadi",
        "Noor"
      ],
      "year": "2022",
      "venue": "2022 8th International HCI and UX Conference in Indonesia (CHIuXiD)",
      "doi": "10.1109/CHIuXiD57244.2022.10009705"
    },
    {
      "citation_id": "73",
      "title": "Implementation of an Artificial Intelligence (AI) Instructional Support System in a Virtual Reality (VR) Thermal-Fluids Laboratory",
      "venue": "Implementation of an Artificial Intelligence (AI) Instructional Support System in a Virtual Reality (VR) Thermal-Fluids Laboratory",
      "doi": "10.1115/IMECE2023-112683"
    },
    {
      "citation_id": "74",
      "title": "Pushing the boundaries of immersion and storytelling: A technical review of unreal engine",
      "authors": [
        "O Sobchyshak",
        "S Berrezueta-Guzman",
        "S Wagner"
      ],
      "year": "2025",
      "venue": "Pushing the boundaries of immersion and storytelling: A technical review of unreal engine",
      "arxiv": "arXiv:2507.08142"
    },
    {
      "citation_id": "75",
      "title": "Sok: Data privacy in virtual reality",
      "authors": [
        "G Garrido",
        "V Nair",
        "D Song"
      ],
      "year": "2023",
      "venue": "Proceedings on Privacy Enhancing Technologies 2024",
      "doi": "10.56553/popets-2024-0003"
    },
    {
      "citation_id": "76",
      "title": "Gpt for games: An updated scoping review",
      "authors": [
        "D Yang",
        "E Kleinman",
        "C Harteveld"
      ],
      "year": "2020",
      "venue": "Gpt for games: An updated scoping review"
    },
    {
      "citation_id": "77",
      "title": "Ai in gaming: From simple algorithms to complex agents, in: 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)",
      "authors": [
        "A Waghale",
        "N Potdukhe",
        "R Rewatkar"
      ],
      "year": "2024",
      "venue": "Ai in gaming: From simple algorithms to complex agents, in: 2024 2nd DMIHER International Conference on Artificial Intelligence in Healthcare, Education and Industry (IDICAIEI)",
      "doi": "10.1109/IDICAIEI61867.2024.10842756"
    },
    {
      "citation_id": "78",
      "title": "Integrating personality into digital humans: A review of llm-driven approaches for virtual reality",
      "authors": [
        "I Brito",
        "J Dollis",
        "F Färber",
        "P Ribeiro",
        "R Sousa",
        "A Galvão Filho"
      ],
      "year": "2025",
      "venue": "Integrating personality into digital humans: A review of llm-driven approaches for virtual reality",
      "arxiv": "arXiv:2503.16457"
    },
    {
      "citation_id": "79",
      "title": "Reduction of cybersickness in head mounted displays use: A systematic review and taxonomy of current strategies",
      "authors": [
        "S Ang",
        "J Quarles"
      ],
      "year": "2023",
      "venue": "Frontiers in Virtual Reality",
      "doi": "10.3389/frvir.2023.1027552"
    },
    {
      "citation_id": "80",
      "title": "Automatic detection and prediction of cybersickness severity using deep neural networks from user's physiological signals",
      "authors": [
        "R Islam",
        "Y Lee",
        "M Jaloli",
        "I Muhammad",
        "D Zhu",
        "P Rad",
        "Y Huang",
        "J Quarles"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)",
      "doi": "10.1109/ISMAR50242.2020.00066"
    },
    {
      "citation_id": "81",
      "title": "Using trajectory compression rate to predict changes in cybersickness in virtual reality games",
      "authors": [
        "D Monteiro",
        "H.-N Liang",
        "X Tang",
        "P Irani"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)",
      "doi": "10.1109/ISMAR52148.2021.00028"
    },
    {
      "citation_id": "82",
      "title": "Real-time prediction of simulator sickness in virtual reality games",
      "authors": [
        "J Wang",
        "H.-N Liang",
        "D Monteiro",
        "W Xu",
        "J Xiao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Games",
      "doi": "10.1109/TG.2022.3178539"
    },
    {
      "citation_id": "83",
      "title": "Affective computing in virtual reality: Emotion recognition from brain and heartbeat dynamics using wearable sensors",
      "authors": [
        "J Marín-Morales",
        "J Higuera-Trujillo",
        "A Greco",
        "J Guixeres",
        "C Llinares",
        "E Scilingo",
        "M Alcañiz",
        "G Valenza"
      ],
      "year": "2018",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-018-32063-4"
    },
    {
      "citation_id": "84",
      "title": "Eye tracking for affective computing in virtual reality healthcare applications",
      "authors": [
        "D Harris",
        "T Arthur",
        "M Wilson",
        "S Vine"
      ],
      "year": "2023",
      "venue": "11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW) (2023)",
      "doi": "10.1109/ACIIW59127.2023.10388102"
    },
    {
      "citation_id": "85",
      "title": "Eeg-based affective computing in virtual reality with a balancing of the computational efficiency and recognition accuracy",
      "authors": [
        "G Pei",
        "Q Shang",
        "S Hua",
        "T Li",
        "J Jin"
      ],
      "year": "2024",
      "venue": "Computers in Human Behavior",
      "doi": "10.1016/j.chb.2023.108085"
    },
    {
      "citation_id": "86",
      "title": "Deep learning for video game playing",
      "authors": [
        "N Justesen",
        "P Bontrager",
        "J Togelius",
        "S Risi"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Games",
      "doi": "10.1109/TG.2018.2881352"
    },
    {
      "citation_id": "87",
      "title": "Digital twins: The confluence of virtual reality with iot",
      "authors": [
        "Z Lv",
        "J Lloret",
        "H Mauri",
        "J Song",
        "Wang"
      ],
      "year": "2023",
      "venue": "IEEE Consumer Electronics Magazine",
      "doi": "10.1109/MCE.2023.3296868"
    },
    {
      "citation_id": "88",
      "title": "The role of explainable ai in the research field of ai ethics",
      "authors": [
        "H Vainio-Pekka",
        "M -O. Agbese",
        "M Jantunen",
        "V Vakkuri",
        "T Mikkonen",
        "R Rousi",
        "P Abrahamsson"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Interactive Intelligent Systems",
      "doi": "10.1145/3599974"
    },
    {
      "citation_id": "89",
      "title": "Security and privacy in virtual reality: a literature survey",
      "authors": [
        "A Giaretta"
      ],
      "year": "2025",
      "venue": "Virtual Reality",
      "doi": "10.1007/s10055-024-01079-9"
    },
    {
      "citation_id": "90",
      "title": "Protection of privacy in biometric data",
      "authors": [
        "I Natgunanathan",
        "A Mehmood",
        "Y Xiang",
        "G Beliakov",
        "J Yearwood"
      ],
      "year": "2016",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2016.2535120"
    },
    {
      "citation_id": "91",
      "title": "A survey on large language model-based game agents",
      "authors": [
        "S Hu",
        "T Huang",
        "G Liu",
        "R Kompella",
        "F Ilhan",
        "S Tekin",
        "Y Xu",
        "Z Yahn",
        "L Liu"
      ],
      "year": "2025",
      "venue": "A survey on large language model-based game agents",
      "arxiv": "arXiv:2404.02039"
    }
  ]
}