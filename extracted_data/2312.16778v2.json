{
  "paper_id": "2312.16778v2",
  "title": "Adversarial Representation With Intra-Modal And Inter-Modal Graph Contrastive Learning For Multimodal Emotion Recognition",
  "published": "2023-12-28T01:57:26Z",
  "authors": [
    "Yuntao Shou",
    "Tao Meng",
    "Wei Ai",
    "Nan Yin",
    "Keqin Li"
  ],
  "keywords": [
    "Adversarial Representation Learning",
    "Feature Fusion",
    "Graph Contrastive Representation Learning",
    "Multimodal Emotion Recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the release of increasing open-source emotion recognition datasets on social media platforms (e.g., Weibo, Twitter, and Meta, etc.) and the rapid development of computing resources, multimodal emotion recognition tasks (MER) have begun to receive widespread research attention. The MER task extracts and fuses complementary semantic information from different modalities, which can classify the speaker's emotions. However, the existing feature fusion methods have usually mapped the features of different modalities into the same feature space for information fusion, which can not eliminate the heterogeneity between different modalities. Therefore, it is challenging to make the subsequent emotion class boundary learning. To tackle the above problems, we have proposed a novel Adversarial Representation with Intra-Modal and Inter-Modal Graph Contrastive for Multimodal Emotion Recognition (AR-IIGCN) method. Firstly, we input video, audio, and text features into a multi-layer perceptron (MLP) to map them into separate feature spaces. Secondly, we build a generator and a discriminator for the three modal features through adversarial representation, which can achieve information interaction between modalities and eliminate heterogeneity among modalities. Thirdly, we introduce contrastive graph representation learning to capture intra-modal and inter-modal complementary semantic information and learn intra-class and inter-class boundary information of emotion categories. Specifically, we construct a graph structure for three modal features and perform contrastive representation learning on nodes with different emotions in the same modality and the same emotion in different modalities, which can improve the feature representation ability of nodes. Finally, we use MLP to complete the emotional classification of the speaker. Extensive experimental works show that the ARL-IIGCN method can significantly improve emotion recognition accuracy on IEMOCAP and MELD datasets. Furthermore, since AR-IIGCN is a general multimodal fusion and contrastive learning method, it can be applied to other multimodal tasks in a plug-and-play manner, e.g., humour detection.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "I. Introduction",
      "text": "T He multimodal emotion recognition task (MER) can combine the semantic information with different modal Corresponding Author: Tao Meng (mengtao@hnan.edu.cn) Y. Shou, T. Meng, and W. Ai are with School of computer and Information Engineering, Central South University of Forestry and Technology, Hunan 410004, China. (mengtao@hnan.edu.cn, yuntaoshou@csuft.edu.cn, aiwei@hnan.edu.cn) N. Yin is with Mohamed bin Zayed University of Artificial Intelligence, UAE. (nan.yin@mbzuai.ac.ae) K. L is with the Department of Computer Science, State University of New York, New Paltz, New York 12561, USA. (lik@newpaltz.edu) Specifically, there is heterogeneity among modalities in feature fusion with common space, which leads to the misalignment of peaks between modalities.\n\nfeatures (e.g., text, video, audio, etc.), which identifies the emotion of the speaker at the current moment  [1] ,  [2] ,  [3] ,  [4] . With the continuous development of deep learning technology and computing resources, MER has been increasingly used in many practical social media scenarios. For example, in a human-computer dialogue system, the interactive system can obtain the user's current emotional state of the user according to the data analysis of the human-computer dialogue. Then, it can generate words to fit the scene. Therefore, accurate identification of the user's emotional state has become a practical application value  [5] .\n\nHowever, MER can eliminate the multimodal heterogeneous data modality gap because video, audio, and text feature distributions are inconsistent in space. The current mainstream feature fusion method can eliminate the gap of different modal data, which is to map into the same feature space for feature representation  [6] . For example, Tensor Fusion Network (TFN)  [7]  uses the tensor outer product operation to map different modal features into a three-dimensional feature space, which is used for the fusion representation of multimodal feature vectors. Low-rank Fusion Network (LFN)  [8]  utilizes lowrank decomposition operations to combine correlated feature vectors highly and fuse three modal features. However, the above methods forcibly map different modal features into a common representation space, which can not eliminate their heterogeneity. It is presented the Fig.  1   to a unified space. Fig.  1(c ) illustrates the distribution of multimodal features obtained through adversarial representation learning. The above phenomenon is indicated that the spatial distribution learned by adversarial learning is much discriminative  [9] ,  [10] .\n\nFor the problem with existing deep learning methods, they have failed to capture intra-class and inter-class semantic information, which is differentiated  [11] ,  [12] ,  [13] . However, current mainstream research has mainly focused on capturing complementary semantic information between modalities, which ignores the relationship between modalities and emotion categories  [14] . For instance, Hu et al.  [15]  proposed Multimodal Fusion via Deep Graph Convolution Network (MMGCN) to fuse dialogue relations and complementary semantic information of different modalities through GCN. Moreover, Liu et al.  [16]  proposed a Multimodal Fusion Network (MFN) and used an attention mechanism to consider the importance of different modalities. It was obtained as a multimodal fusion vector with modal interactions, which was challenging to learn clear class boundaries between different emotion categories. However, many studies  [17] ,  [18] ,  [19]  have investigated capturing the relationship between different modalities and emotion categories, which improves emotion classification.\n\nTo eliminate the heterogeneity between different modalities and capture the intra-modal, inter-modal complementary semantic information, intra-class, and inter-class differences, it is still a problem to be solved.\n\nTo solve the above problem, we propose a novel Adversarial Representation with Intra-Modal and Inter-Modal Graph Contrastive Learning for Multimodal Emotion Recognition, i.e., AR-IIGCN. Firstly, we use RoBERTa, DenseNet, and Bi-LSTM-based Encoder to obtain semantic information in text, video, and audio. Secondly, we input the extracted three modality features into a multi-layer perceptron (MLP) to map them into separate feature spaces. Thirdly, we build a generator and discriminator for the three modal features. It is used adversarial learning to achieve cross-modal feature fusion and eliminate the heterogeneity between different modalities. Fourthly, we construct a new graph contrastive representation learning architecture, which captures complementary semantic information within and between modalities and intra-class and inter-class differences. It is performed contrastive representation learning on nodes with different emotions in the same modality and nodes with the same emotion in different modalities, which obtains a more precise representation of the boundary distribution. Finally, we have successfully used MLP for emotion classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Our Contributions",
      "text": "Therefore, MER can consider eliminating the heterogeneity among the three modalities of video, audio, and text, which can capture the complementary semantic information within and between modalities and the intra-class and inter-class differences. show that the emotion recognition effect of ARL-IIGCN is better than the existing comparison algorithms. The rest of this paper is organized as follows. Section 2 presents the related work of prior MER. Section 3 describes the multi-modal emotion recognition task and presents the multi-modal data processing flow. Section 4 illustrates the proposed neural network AR-IIGCN. Section 5 describes the datasets and evaluation metrics used. The related experimental results and discussion on the IEMOCAP and MELD datasets are shown in Section 6. Finally, we conclude our work and illustrate future work.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multimodal Emotion Recognition In Conversation",
      "text": "As an interdisciplinary study (e.g., brain science and cognitive science, etc.), MER has received extensive attention from researchers  [20] . The current mainstream MER research has mainly included sequential context modelling, speaker relationship modelling, and multimodal feature fusion modelling. The sequential context modelling method has mainly combined the semantic information of the context, which classifies the emotion at the current moment. The speaker relationship modelling method extracts the semantic information of the dialogue relationship between speakers through the graph convolution operation. The multimodal feature fusion modelling method achieves cross-modal feature fusion by capturing intramodal and inter-modal complementary semantic information.\n\nIn the modelling method based on sequential context, Poria et al.  [21]  proposed bidirectional long short-term memory (Bi-LSTM), which can extract contextual semantic information of forward and reverse sequence. However, bc-LSTM has a limited ability to model long-distance context dependencies. To respond to the above problems, Beard et al.  [22]  proposed recursive multi-attention (RM), which is used multi-gated memory units to update the memory network iteratively. Therefore, it is realized in the memory of global context information. Although sequential context-based modelling can achieve specific results in emotion recognition, it ignores the intra-modal and inter-modal complementary semantic information.\n\nIn the modelling method based on multimodal feature fusion, Zadeh et al.  [7]  proposed Tensor Fusion Network (TFN) to map multimodal features into three-dimensional space through tensor outer product operation, which realizes information interaction between multimodal features. However, the feature dimension of TFN is high, which is prone to an overfitting effect. To alleviate the problems of TFN, Liu et al.  [8]  proposed a Low-rank Fusion Network (LFN) to realize dimensionality reduction of tensors through low-rank decomposition operations, which has achieved performance improvement in emotion recognition. Moreover, Hu et al.  [15]  proposed Multimodal Fusion via Deep Graph Convolution Network (MMGCN), which can effectively utilize the complementary semantic information between multimodal features. Although the above methods can achieve cross-modal feature fusion, they have mapped the features of different modalities into the same feature space. It is challenging to eliminate the heterogeneity between different modalities.\n\nIn the modelling method based on speaker relationship, Ren et al.  [23]  proposed Latent Relation-Aware Graph Convolutional Network (LR-GCN) and constructed a speaker relation graph. Then, a multi-head attention mechanism was introduced to capture latent relations between utterances. However, the fully connected graphs introduce noise information. Nie et al.  [24]  proposed that the Correlation-based Graph Convolutional Network (C-GCN) method captured the correlation inter and intra-modalities, which realized the effective use of multimodal information. Although the modelling method based on speaker relationships can fully use the semantic information of speaker dialogue relationships and cross-modal semantic information, it ignores the differences between different emotion categories.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Generative Adversarial Learning",
      "text": "In multimodal emotion recognition, data imbalance is a common problem, which leads to biased learning of the model  [25] . Therefore, researchers have begun adopting generative adversarial learning to generate new samples that fit the original data distribution. Specifically, previous work generates new samples by minimizing the data distribution learned by the generator and the discriminator.\n\nSu et al.  [26]  proposed Corpus-Aware Emotional CycleGAN (CAEmoCyGAN) and introduced a target-to-source generator to generate new samples that more closely matched the original data distribution. CAEmoCyGAN can enhance the ability of the model to learn unbiased representations. Moreover, Chang et al.  [27]  proposed Adversarial Cross Corpora Integration (ACCI) and used an adversarial autoencoder to generate samples with contextual semantic information. It used emotion labels as additional constraints for the model. Although new samples are generated by generative adversarial learning, it can effectively alleviate the data imbalance problem, which can not eliminate the heterogeneity among data of different modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Contrastive Learning",
      "text": "Self-supervised learning (SL) is an essential branch of deep learning (DL), which has received increasing research attention because of its powerful ability to learn representations. Contrastive representation learning (CRL) is one of the representative methods. Specifically, CRL learns discriminative features by continuously shrinking the distance (e.g., Euclidean distance and Mahalanobis distance, etc.) between positive and negative samples and expanding the distance between positive and negative samples. Previous works have obtained representations of features by maximizing the mutual information (MI) between model inputs and learned representations.\n\nLi et al.  [28]  proposed contrastive predictive coding (CPC) to address the lack of large-scale datasets for emotion recognition tasks. Through unsupervised contrastive representation learning, CPC can learn latent emotional semantic information from unlabeled data. Furthermore, Kim et al.  [29]  proposed contrastive adversarial learning (CAL) to solve the problem of existing methods, which relied on supervised information. CAL is used to learn complex semantic emotional information by comparing samples with strong emotional features and samples with weaker emotions. Wang et al.  [30]  designed a new architecture composed of three networks (i.e., FacesNet, SceneNet, and ObjectsNet) to improve the feature fusion ability of the model. It was used to solve the problem of missing essential semantic information. Although contrastive representation learning can enhance the representation of emotional information, the above methods ignore the intra-modal and inter-modal information interaction and intra-class and inter-class contrastive representation learning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Preliminary Information",
      "text": "This section defines the Multimodal Conversational Emotion Recognition Task (MCER) in mathematical terms. In addition, It is described the data preprocessing methods of different modalities as follows: (1) Word Embedding: To eliminate the ambiguity of words, this paper is used RoBERTa  [31]  to obtain the embedding representation of word vectors (2) Visual Feature Extraction: It is used DenseNet  [32]  to capture deeper image features in videos and reduce the introduction of noisy information. (3) Audio Feature Extraction: The encoder architecture is adopted  [33]  to extract audio signals from different speakers.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Problem Definition",
      "text": "The task of MER is to predict the sentiment class of test utterances in the dataset. It is assumed that there are N speakers in a dialogue context, and the set of speakers can be represented as S = {P 1 , P 2 , . . . , P N }. The dialogue context is sorted according to the order in which each speaker speaks, which can be expressed as D = {d 1 , d 2 , . . . , d T }, where T is the total number of test utterances. This paper defines a mapping function f to construct the index relationship between speakers and test utterances, which is expressed as H = {δ f1 , δ f2 , . . . , δ fi }, where i represents the i-th test utterances. Our task is to classify the emotion of each utterance δ fi .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Multimodal Feature Extraction",
      "text": "The experimental datasets of IEMOCAP and MELD can consist of three modalities, which are stored in text, video, and audio, respectively. For the features of different modalities, a specific data preprocessing method for feature extraction is used to obtain feature vector representations with less noise and rich semantic information. It is described the features encoded for each modality as follows.\n\n1) Word Embedding: To disambiguate words and obtain feature vectors with rich semantic information, we have used the RoBERTa model  [31]  to encode words. In this paper, it is used sentence-level encoding to encode each utterance of the speaker, and obtain a contextual semantic representation\n\nWhere m is the dimension of word embedding. Due to limited computing resources, it is collected the first 100-dimensional vectors encoded by the RoBERTa model as our word embedding representation ξ u .\n\n2) Visual Feature Extraction: The speaker's facial expression and behaviour reflect the inner emotional state of the speaker. Therefore, we capture the speaker's facial expressions and action changes from the video frames, thereby extracting semantic information related to the speaker's emotional changes. Moreover, it is used the DenseNet model to obtain a 512-dimensional feature vector ξ v .\n\n3) Audio Feature Extraction: The fluctuation of the voice in the audio signal also reflects the emotional changes in the speaker's heart. Sometimes, a person's actions may not truly reflect his emotions, but the tone changes cannot be faked. Therefore, we have used the encoder structure to extract the speaker's audio features ξ a , where ξ a is a 100-dimensional feature vector. Specifically, the encoder comprises Bi-LSTM, an attention layer, and a fully connected layer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Methodology",
      "text": "To increase the performance of multimodal emotion recognition, we have proposed a novel Adversarial Representation with Intra-Modal and Inter-Modal Graph Contrastive Learning for Multimodal Emotion Recognition, namely AR-IIGCN. The overall architecture of AR-IIGCN is shown in Fig.  1 . The AR-IIGCN consists of data preprocessing, multimodal feature fusion, graph contrastive representation learning and emotion classification. In the data preprocessing stage, RoBERTa, DenseNet and Encoder are used to extract text, video and audio features. In the multimodal feature fusion stage, we build a generator and a discriminator for text, video, and audio to remove modality heterogeneity in an adversarial learning manner. In the graph contrastive learning stage, we have constructed two contrastive losses to perform intra-modal and inter-modal contrastive learning and intra-class and inter-class contrastive learning, respectively. Finally, we used MLP for emotion classification in the emotion classification stage.\n\nA. The Design of the AR-IIGCN Structure 1) TGAN: Tri-modal Generative Adversarial Networks: Firstly, the MLP is used to dimensionally align the three modality features and map them into three separate feature spaces. The formulas are as follows:\n\nwhere d denotes the dimension that maps the three modal features to a separate representation space. f mlp (•) represents the MLP layer.\n\nSecondly, we build a text generator and a text discriminator. The input of the text generator is audio features ξa and video features ξv . The input of the text discriminator is the fused features generated by the text generator containing three modal information. The objective optimization function of the text generator is shown as follows:\n\nwhere G T and D T represent text generator and text discriminator, ξa ∼ P data represents sampling samples from the data that conforms to the audio feature distribution law, and ξv ∼ P data represents sampling samples from the data that conforms to the video feature distribution law.\n\nThe objective optimization function of the text discriminator is discussed as follows:\n\nwhere E T ∼Pdata (T ) represents sampling samples from the data that conforms to the original data distribution.\n\nThirdly, it is build an audio generator and an audio discriminator. The input to the audio generator is text features and video features. The input of the audio discriminator is the fused features generated by the audio generator, which contains three modal information. The objective optimization function for the audio generator is calculated as follows:\n\nwhere G A and D A represent audio generator and audio discriminator, ξu ∼ P data represents sampling samples from the data that conforms to the text feature distribution law. The objective optimization function for the audio discriminator is shown as follows: max\n\nFinally, we built a video generator and a video discriminator. The input of the video generator is text features and audio features. The input of the video discriminator is the fused features generated by the video generator containing three modal information. The objective optimization function for the video generator is as presented follows:\n\nwhere G V and D V represent video generator and video discriminator.\n\nThe objective optimization function for the video discriminator is shown as follows: max\n\nIt should be noted that after training the three-modal generative confrontation network, We input the output of MLP into GCL for training of subsequent tasks.\n\n2) Speaker Relation Graph Construction: A graph structure is used to extract semantic information about speaker dialogue relations. Specifically, we construct a directed graph of speaker relations G M = {V M , E M , R M , W M } for the three modal features of video, audio and text respectively, where M ∈ {T, V, A}, the node\n\nis the weight of the edge r M ij , and r M ∈ R M is the edge type. Since the computational complexity of GCN is O(n 2 ), it leads to high computational resources required. Therefore, we set the context window size to 10.\n\nTo capture the key semantic information in the nodes, we use the attention mechanism is used to calculate the weight of the edge,, which and performs information aggregation according to the edge weight. Firstly, we use MLP to dynamically learn the correlation between node i and node j. The formula is defined as follows:\n\nwhere W M ϑ1 , W M ϑ2 are learnable network parameters, and ⊕ represents the vector concatenation operation.\n\nSecondly, we use a softmax function to normalize the correlation between node i and node j, which obtains the attention score for each edge. The formula is defined as follows:\n\nwhere N i represents the first-order neighbour nodes of node i. The larger ω M ij represents the stronger correlation between node i and node j.\n\nFinally, we have updated the node representations using a GCN followed by a GELU activation function. The formula for GCN encoding is shown as follows:\n\n) where N r i is the set of first-order neighbor nodes of node i under the edge relationship r ∈ R, |N r i | is the modulus of N r i , and ψ M i (t) is the feature vector encoded by GCN.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Iccl: Intra-Class And Inter-Class Contrastive Learning",
      "text": "ICCL aims to learn intra-class and inter-class semantic information with differences through contrastive learning. In IMCL, positive samples are represented by samples of the same modality and the same class, while samples of the same modality and different classes represent negative samples. The softmax function is used to normalize the representations of positive and negative samples so that the similarity between them ranges between 0 and 1. Specifically, the intra-class and inter-class contrastive loss are defined as follows:\n\n) where χ M i and δ M j belong to samples of the same modality. However, if Eq. (  11 ) is used as a contrastive loss, the model may fall into a local optimal solution. i.e., (µ M ) T χ M i can be minimized but (µ M ) T δ j cannot be maximized. The above situation occurs when the similarity between negative sample pairs is 0. No matter how much the similarity between positive sample pairs is, the contrastive loss of the model tends to the minimum value. The desired goal is that (µ M ) T χ M i can be minimized and (µ M ) T δ j can be maximized. Therefore, we introduce a regularization term to ensure that the similarity between positive sample pairs can be maximized and the similarity between negative sample pairs can be minimized. The formula is defined as follows:\n\nwhere L R ICCL is the regularization loss for ICCL. With ICCL, the model can learn the intra-class and inter-class difference information.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Imcl: Intra-Modal And Inter-Modal Contrastive Learning",
      "text": "IMCL aims to learn complementary semantic information between modalities, which obtains a more discriminative embedding representation through a contrastive learning method. Specifically, in IMCL, positive samples are represented by samples of the same class in the same modality, while samples of the different classes in different modalities represent negative samples. The softmax function normalises the representations of positive and negative samples, and the similarity ranges from 0 to 1. The intra-modal and inter-modal contrastive loss is defined as follows:\n\nwhere µ M denotes the anchor embedded representation, N denotes the number of positive samples, M denotes the number of negative samples, χ M i and δ M j denote the embedded representations of positive and negative samples, respectively. It is noted that χ M i and δ M j are the same modality and different classes.\n\nFor ICCL, regularisation terms are introduced to strengthen the similarity between positive sample pairs, which reduces the similarity between negative samples. The formula is defined as follows:\n\nwhere L IM CL is the regularization loss for IMCL. IMCL encourages high similarity between samples of the same class in the same modality, which forces low similarity between samples of the same class in different modalities. With IMCL, the model can learn intra-modal and inter-modal complementary semantic information.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Emotion Inference Subnetwork",
      "text": "After the multimodal feature vectors pass through the matching attention layer, each contextual utterance can be represented as a multimodal fusion vector z f . We use a multi-layer perceptron deep neural network called the Emotion Inference Subnetwork G s with weights W conditioned on z f . The multi-layer perceptron (MLP) consists of two fully connected layers with ReLU activation functions and connects them to a decision layer. The maximum likelihood function of the Emotion Inference Subnetwork G s is defined as follows, where φ is the label of emotion prediction:\n\nwhere L CLS is emotion classification loss for the model. The smaller L CLS , the better the emotion classification effect.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Model Training",
      "text": "The intra-modal, inter-modal, and intra-class and inter-class contrastive losses are obtained by weighted summation of IMCL and ICCL. The formula is defined as follows:\n\nThe overall loss for model training is obtained by summing the classification loss and the contrastive loss. The formula for the model training loss is defined as follows:\n\nwhere L overall is the overall loss of the model. The smaller L overall is, the better the training effect of the model is.\n\nThe entire inference process of the AR-IIGCN pseudocode is contained in Algorithm 1. for i = 1, 2, . . . , L/B do\n\n7:\n\nUpdate generator and discriminator parameters by Eq. (2-7).\n\n.\n\n15:\n\nObtain the node representation ψ t , ψ a , ψ v by Eq. (\n\nSample positive and negative samples χ M = {ψ t i , ψ a i , ψ v i }.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "18:",
      "text": "Calculate ICCL and IMCL losses by Eq. (  12 ) and Eq. (  14 ).\n\n19:\n\nObtain predicted emotion labels ŷi via MLP.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "20:",
      "text": "Update network parameters by Eq. (  17 ).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "21:",
      "text": "end for 22: end for 23: Return the predicted emotion labels ŷ.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "F. Implementation Details",
      "text": "In this section, we have described the implementation details of the model during training. We have divided the benchmark dataset into three parts. The first part is the training set for model training, and the second part is the validation set for updating the network parameters. The third part is the test set for evaluating the emotional prediction effect of the model. In addition, the ratio of the training set, test set, and validation set is 8:1:1. The experimental environment is the Windows 10 operating system, and the hardware driver is a computer with Nvidia RTX 3090. It is used Python 3.8 and Pytorch 1.9.1 versions to complete the construction of deep learning algorithms. To evaluate the effective convergence of the model, this paper is used the highly stable Adam algorithm  [10]  to optimize the network parameters. In addition, it is set as the epoch size to 60, batch size to 32, learning rate to 0.0005, dropout to 0.5, and weight decay coefficient to 0.00001.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "V. Experiments",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Benchmark Dataset Used",
      "text": "The MELD  [34]  and IEMOCAP  [35]  multimodal conversation datasets are often used for comparative experiments in MERC. Table  1  shows the distribution of MELD and IEMOCAP datasets in each emotion category. We introduce the situation of the two datasets as follows.\n\nThe Interactive Emotional Dyadic Motion Capture Database (IEMOCAP) contains three modalities, namely video, audio, and text. Therefore, IEMOCAP is a multimodal dataset, and the use of multimodal emotion recognition methods can enhance the prediction effect of the model. A total of 10 actors and actresses are included in the IEMOCAP dataset, and they communicate in an interactive way. For each conversation, it is annotated by multiple sentiment experts, avoiding the subjectivity of human annotation. In addition, the IEMO-CAP dataset contains a total of six emotions, namely \"sad\", \"happy\", \"angry\", \"neutral\", \"frustrated\" and \"excited\".\n\nThe Multi-modal EmotionLines Dataset (MELD) is also a multi-modal dataset whose corpus consists of dialogues from the TV series Friends. Similar to the IEMOCAP dataset, each conversation is also annotated by multiple sentiment experts. In addition, the MELD dataset contains a total of seven emotions, namely \"disgust\", \"anger\", \"joy\", \"fear\", \"sadness\", \"neutral\", and \"surprise\".",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "To compare the emotion recognition effect of our algorithm and other baseline algorithms, we use four evaluation indicators: 1) Accuracy; 2) F1; 3) Weighted average accuracy (WAA); 4) Weighted average F1 (WAF1). We define these four indicators as follows:\n\n1) Accuracy j is the prediction accuracy of the model on the j-th emotion category, and its formula is defined as follows:\n\nwhere M is the number of samples that the model correctly predicts on the j-th sentiment. N is the total number of samples included in the j-th sentiment. T i j represents the ith sample on the j-th sentiment. P k j means that the model predicts the k-th sample correctly on the jth category of emotion, and P k j ∈ [0, 1]. Relatively speaking, the higher the Accuracy j value, the higher the confidence of the model prediction.\n\n2) The value of F 1 j represents the f1-score predicted by the model on the j-th emotion category, and its formula is defined as follows:\n\nT P , P j F P × Precision P j T P , P j F N\n\nRecall P j T P , P j F P + Precision P j T P , P j F N  (19)  and Precision P j T P , P j F N =\n\nRecall P j T P , P j F P = P j T P P j T P ∪ P j F P  (20)  where, P j T P represents the number of positive samples predicted on the j-th emotion, and P j F N represents the number of negative samples predicted on the j-th emotion. P j F P represents the number of other emotion categories identified by the model as the jth positive samples. P recision(P j T P , P j F N ) represents the precision of the model's recognition on the j-th type of emotion, and Recall(P j T P , P j F P ) represents the recall of the model's recognition on the j-th type of emotion. The F1 value combines the two metrics of precision and recall. In particular, the higher the F1 value, the higher the confidence of the model's emotion prediction.\n\n3) WAA takes into account the problem of imbalanced sentiment categories and is the weighted average of the prediction accuracy of the model across all categories. The weight of the sample is inversely proportional to the number of samples, and the formula is defined as follows:\n\nwhere τ is the total number of sentiment categories in the dataset. The higher the WAA, the better the comprehensive prediction effect of the model. 4) Like WAA, WF1 also considers the problem of imbalanced sentiment category divisions, which is the weighted average of the model's f1-score across all categories. The weight of the sample is inversely proportional to the number of samples, and the formula is defined as follows:\n\nSpecifically, the higher the WF1, the higher the prediction confidence of the model.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "C. Baseline Models",
      "text": "We do extensive comparative experiments on two popular datasets to count the emotion recognition effect of the model proposed in this paper. Some recent comparison algorithms are described below:\n\nTextCNN: The TextCNN proposed by Kim  [36]  uses Convolutional Neural Networks (CNN) for emotion recognition of dialogues. TextCNN exploits the local attention mechanism of convolution kernels to extract contextual utterances with emotional polarity in texts. However, TextCNN cannot model the context of long-range dependencies and can only model unimodal features.\n\nbc-LSTM: The bidirectional LSTM (bc-LSTM) proposed by Poria et al.  [21]  can not only model long-range contextual dependencies, but also extract contextual information in DialogueGCN: DialogueGCN proposed by Ghosal et al.  [37]  is the first to use graph convolutional neural networks (GCNs) to model speaker relations. DialogueGCN simulates the dialogue relationship between speakers by constructing a fully connected directed graph, which can fuse the contextual semantic information and the semantic information of the dialogue relationship between speakers. However, the fully connected graph constructed by DialogueGCN may introduce noisy information.\n\nCTNet: The Conversational Transformer Network proposed by Lian et al.  [38]  comprehensively considers intra-modal and inter-modal modeling, and captures long-range contextual information by using a cross-modal Conversational Transformer architecture.\n\nLR-GCN: The LR-GCN proposed by Ren et al.  [23]  not only utilizes GCN to model the relationship between speakers, but also utilizes a multi-head attention mechanism to model the latent relationship between utterances. In addition, to speed up the convergence of the model, LR-GCN also introduces a residual structure to transfer more gradient information. LR-GCN has achieved good experimental results.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Vi. Results And Discussion",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Comparison With Baselines",
      "text": "This paper compares our proposed emotion recognition algorithm AR-IIGCN with other deep learning algorithms. Table  1  and Table  2  show the recognition accuracy and F1 value of all algorithms on each emotion category on the IEMOCAP and MELD datasets, and the average accuracy and F1 value of the model. Experimental results demonstrate the superiority of our algorithm.\n\nIEMOCAP: As shown in Table  1 , AR-IIGCN has the best emotion recognition effect on the IEMOCAP dataset, and the WAA and WF1 values are 70.46% and 70.36%, respectively.\n\nIn addition, AR-IIGCN has the highest accuracy rate on the \"happy\" and \"angry\" classes, and the highest F1 value on the \"happy\" and \"excited\" classes, while the accuracy and F1 value of other categories are slightly lower than other comparison algorithms. The reason is that AR-IIGCN comprehensively considers the heterogeneity of modalities, the intramodal and inter-modal complementary semantic information, and the intra-class and inter-class differences. The emotion recognition effect of LR-GCN is second, and the values of WAA and WF1 are 68.52% and 68.35%, respectively. The reason why LR-GCN is less effective than AR-IIGCN is that it ignores the heterogeneity of modalities, which leads to poor learning effect of subsequent class boundaries. The effects of other algorithms are relatively poor, and they do not consider the heterogeneity of modalities and the intra-class and interclass differences.\n\nMELD: As shown in Table  2 , AR-IIGCN has the best emotion recognition effect on the MELD dataset, and the WAA and WF1 values are 64.14% and 64.01%, respectively. In addition, AR-IIGCN has the highest accuracy on the \"neutral\", \"surprise\", \"fear\", \"joy\" and \"sadness\" categories, the F1 values on the \"neutral\", \"surprise\", \"joy\", \"sadness\" and \"angry\" categories are the highest, while the accuracy and F1 values in other categories are slightly lower than other comparison algorithms. In the \"fear\" and \"disgust\" categories, the recognition accuracy and F1 value of AR-IIGCN and other models are low, because the MELD dataset has a serious category imbalance problem.\n\nThe analysis of the above experimental results illustrates the superior performance of AR-IIGCN, which can effectively learn the class boundary information of emotions.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Importance Of The Modalities",
      "text": "Since different modal features contain different semantic information, we explored the emotion recognition effect of different modal features on the IEMOCAP and MELD datasets. As shown in Table  3 , text features perform best in emotion recognition in single-modal experiments, with WA values of 65.4% and 60.8%, and WF1 values of 60.8% and 60.1% in IEMOCAP and MELD datasets, respectively. We think this is because text is the most direct way for speakers to express their emotions, and it contains the least noisy information. Audio features perform second best for emotion recognition, while video features perform the worst. We think this is because video features contain too much noise information, and it is difficult for the model to extract key information. The emotion recognition effect of the combination of text, audio and video features is the best in all experiments, because the model effectively utilizes the complementary semantic information between modalities. The above experimental phenomena also prove the rationality of our mode fusion layer design.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Effectiveness Of Cross-Modal Feature Fusion",
      "text": "In this section, to compare the difference between our proposed multimodal feature fusion method and other methods in multi-modal emotion recognition, we compare our method combining trimodal generative adversarial networks and graph contrastive learning with the other four feature fusion methods.\n\nAdd: The Add method combines the feature vectors by summing the multimodal features, which ignores the information interaction between the multimodal features.\n\nConcatenate: The Concatenate method is a splicing operation of multi-modal features, which does not model multimodal features within and between modalities.\n\nTFN: TFN method models the fusion between multimodal features through tensor outer product operations.\n\nLFM: LFM fuses multimodal features through low-rank tensors. As shown in Table  4 , compared with other fusion methods, our cross-modal fusion method achieves the best emotion recognition performance, with WA values of 70.5% and 64.1% and WF1 values of 70.4% and 64.0% on IEMOCA and MELD datasets, respectively. Specifically, our method improves the WA value by 15.3% and 6.6% and the WF1 value by 15.4% and 8.1% over the Add method on the IEMOCAP and MELD datasets, respectively. This is because the Add method cannot eliminate the heterogeneity among modalities and cannot utilize complementary semantic information between modalities. Compared with the Concatenate method, our method improves the WA value by 12.2% and 6.9%, and the WF1 value by 5.5% and 6.9%, respectively. Similar to the Add method, the Concatenate method cannot take advantage of complementary semantic information between modals. Compared with the Add and Concatenate methods, the Tensor Fusion and Low-rank Fusion methods have significantly improved results, because they utilize complementary semantic information between modalities, and the Low-rank Fusion method can reduce redundant information between modalities . However, they cannot eliminate the heterogeneity between modalities. The above experiments illustrate the superiority of our designed cross-modal approach and the necessity of eliminating modality heterogeneity.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "D. Equal Parameter Experiments",
      "text": "To illustrate that our method AR-IIGCN does not improve the performance of the model due to the increase in the number   As shown in Table  5 , the WA and WF1 values of emotion recognition of the bc-LSTM and DialogueRNN models decreased when the number of parameters increased. In addition, in the process of observing the model training, we find that the model is more prone to overfitting as the number of parameters increases. Therefore, the above phenomenon shows that our model architecture outperforms existing emotion recognition algorithms.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Comparison Of Modality Margin Β",
      "text": "Since the modal margin β is a hyperparameter in this paper, we have conducted extensive experiments to verify the effect of different margins β on emotion recognition. As shown in Fig.  4 (a), on the IEMOCAP dataset, our model performs best in emotion recognition with a margin β = 0.8, with a WF1 value of 70.4%. When the margin is too small (e.g., β = 0.5/0.7), the contrastive learning ability of the model is poor resulting in still large modal gaps. On the contrary, if the margin is too large (e.g., β = 0.9), complementary semantic information between different modalities may be lost. On the MELD dataset, our model performs best in emotion recognition with a margin β = 0.9, with a WF1 value of 70.4%. Similar to on the IEMPCAP dataset, too small margins can lead to large modal gaps. Therefore, choosing a good margin has an important impact on the training effect of the model.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Iemocap Meld",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "F. Comparison Of Contrastive Learning Methods",
      "text": "To explore the effectiveness of our designed graph contrastive learning mechanism, we compared different contrastive learning methods, i.e., Supervised Contrastive Learning (SCL), Supervised Cluster-level Contrastive Learning (SCCL).\n\nAs shown in Fig.  4 (b), we use RoBERTa-large as our text encoder to obtain rich context semantic information. For the results on different comparative learning methods, SCL achieves a 1.4% improvement over the RoBERTa baseline on the IEMOCAP dataset and a 1.7% improvement on the MELD dataset. The performance improvement of SCCL on the IEMOCAP and MELD datasets is higher than SCL, achieving 2.7% and 3.2% improvements respectively compared to the RoBERTa baseline. AR-IIGCN has the highest performance improvement on the IEMOCAP and MELD datasets, achieving 3.9% and 3.9% improvements compared to the RoBERTa baseline, respectively.\n\nThe above experimental phenomena illustrate the effectiveness of our designed intra-modal and inter-modal, and intraclass and inter-class contrastive learning mechanism.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "G. Batch Size Stability",
      "text": "We use different batch sizes to verify the stability of model training on IEMOCAP and MELD datasets. As shown in Fig.  5 , We set the batch size to range from 2 0 = 1 to 2 5 = 32. According to the experimental results, the model has the best emotion classification effect when the batch size is 16. When each training step sets a small batch size (i.e., when only a small number of samples are used), the model cannot extract effective features in different modalities, and their contrastive learning effect will be relatively poor.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "H. Extended Research",
      "text": "To verify the scalability of our fusion and contrast mechanism to other multimodal studies, we apply our method to the task of multimodal humor detection. As shown in Table  6 , C-MFN (C) means using only contextual information without punchlines. C-MFN (P) means using only punchlines with no contextual information, and C-MFN represents using punchlines and contextual information. We embed our TGAN, IMCL, and ICCL mechanisms into the C-MFN method, and experimental results show that our method outperforms the C-MFN method in any combination of modalities. Specifically, we use accuracy as the evaluation metric for humor detection, and our method can achieve improvements ranging from 1.48% to 7.57%. Experimental results show that our method can be applied not only to multimodal emotion recognition tasks, but also to other multimodal tasks.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "I. Ablation Study",
      "text": "To verify the rationality of our module design, we use RoBERTa-Base and RoBERTa-Large as our text encoders to conduct ablation experiments. As shown in Fig.  6 (b), for the results on RoBERTa-Large, AR-IIGCN achieves the best experimental results with WF1 values of 70.4% and 64.0% on the IEMOCAP and MELD datasets, respectively. The emotional effect of RoBERTa-Large with IMCL is second, and the WF1 values are 68.9% and 63.5%, respectively. The emotional effect of RoBERTa-Large with ICCL is worse than RoBERTa-Large with IMCL, and the WF1 values are 68.1% and 62.9% respectively. The emotional effect of RoBERTa-Large with TGAN is only slightly better than the RoBERTa-Large baseline, with WF1 values of 67.3% and 61.7%, respectively. The experimental results show that the intra-modal and inter-modal contrastive learning is the most critical for the training of the model, which is beneficial for the model to fuse complementary multi-modal semantic information. Intraclass and inter-class contrastive learning is also important for the training of the model, which facilitates the class boundary learning of the model. Removing the heterogeneity of modalities is the basis for subsequent model learning.\n\nAs shown in Fig.  6 (a), for the results on RoBERTa-Base, Similar conclusions are drawn from the results of RoBERTa-Base. In addition, the emotion recognition effect of RoBERTa-Large is better than RoBERTa-Base.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vii. Conclusion And Future Work",
      "text": "In this paper, we propose a novel Adversarial Representation with Intra-Modal and Inter-Modal Graph Contrastive Learning for Multimodal Emotion Recognition (AR-IIGCN) model, which enables cross-modal feature fusion, intra-modal and inter-modal contrasting representation learning, and intraclass and inter-class representation learning. Specifically, we firstly introduce a cross-modal feature fusion method based on adversarial learning to eliminate the heterogeneity among different modalities. Secondly, to comprehensively consider the relationship between intra-modality and inter-modality and the relationship between intra-class and inter-class, we design a novel graph contrastive learning architecture to enhance the representation ability of nodes by increasing the distance between different emotion labels of the same modality and shrinking the distance between the same emotion of different modalities. we use a multi-layer perceptron (MLP) for emotion classification.\n\nIn future work, we consider using diffusion models for feature fusion across modalities to generate fused features that contain more semantic information. In addition, we will also consider transferring our method to other multimodal tasks.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Viii. Ackonwledgments",
      "text": "",
      "page_start": 13,
      "page_end": 13
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustrative example of the effect of different feature fusion methods on",
      "page": 1
    },
    {
      "caption": "Figure 1: as an example. Fig.",
      "page": 1
    },
    {
      "caption": "Figure 1: (c) illustrates the distribution of",
      "page": 2
    },
    {
      "caption": "Figure 2: The overall framework of the Adversarial Representation Learning with Intra-Modal and Inter-Modal Graph Contrastive Learning consists of a data",
      "page": 5
    },
    {
      "caption": "Figure 3: The overall graph contrastive representation learning process refers to intra-modal, inter-modal, and intra-class and inter-class comparisons.",
      "page": 6
    },
    {
      "caption": "Figure 4: Experimental results on RoBERTa-Large. (a) Effect of different modal margins β on model training results. (b) Effect of different contrastive learning",
      "page": 11
    },
    {
      "caption": "Figure 4: (a), on the IEMOCAP dataset, our model performs",
      "page": 11
    },
    {
      "caption": "Figure 5: We use different batch sizes with RoBERTa-Large to verify the",
      "page": 11
    },
    {
      "caption": "Figure 6: Ablation experiments on IEMOCAP and MELD datasets. (a) We use RoBERTa-Base as a text encoder to explore the impact of TGAN, IMCL, and",
      "page": 12
    },
    {
      "caption": "Figure 4: (b), we use RoBERTa-large as our",
      "page": 12
    },
    {
      "caption": "Figure 5: , We set the batch size to range from 20 = 1 to 25 = 32.",
      "page": 12
    },
    {
      "caption": "Figure 6: (a), for the results on RoBERTa-Base,",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "… … \nNegtive Samples": "… … \nNegtive Samples"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: , AR-IIGCN has the best",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Happy\nSad\nNeutral\nAngry\nExcited\nFrustrated\nAverage(w)"
        },
        {
          "Methods": "",
          "IEMOCAP": "Acc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nWAA WF1"
        },
        {
          "Methods": "TextCNN\nbc-LSTM\nbc-LSTM+Att\nDialogueRNN\nDialogueGCN\nCT-Net\nLR-GCN\nAR-IIGCN",
          "IEMOCAP": "27.73 29.81\n57.14 53.83\n34.36 40.13\n61.12 52.47\n46.11 50.09\n62.94 55.78\n48.93 48.17\n29.16 34.49\n57.14 60.81\n54.19 51.80\n57.03 56.75\n51.17 57.98\n67.12 58.97\n55.23 54.98\n30.56 35.63\n56.73 62.09\n57.55 53.00\n59.41 59.24\n52.84 58.85\n65.88 59.41\n56.32 56.19\n25.63 33.11\n75.14 78.85\n58.56 59.24\n64.76 65.23\n80.27 71.85\n61.16 58.97\n63.42 62.74\n89.14 84.45\n40.63 42.71\n61.97 63.54\n67.51 64.14\n65.46 63.08\n64.13 66.90\n65.21 64.14\n69.08 65.82\n47.97 51.36\n78.01 79.94\n72.98 67.21\n85.35 78.74\n52.27 58.83\n68.01 67.55\n68.26 68.91\n54.24 55.51\n81.67 79.14\n59.13 63.84\n69.47 69.02\n76.37 74.05\n68.52 68.35\n71.88 69.96\n70.46 70.36\n74.64 81.58\n67.25 63.80\n73.79 68.37\n82.66 79.15\n60.45 63.95"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 4: , compared with other fusion methods,",
      "data": [
        {
          "Methods": "",
          "MELD": "Neutral\nSurprise\nFear\nSadness\nJoy\nDisgust\nAnger\nAverage(w)"
        },
        {
          "Methods": "",
          "MELD": "Acc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nWAA WF1"
        },
        {
          "Methods": "TextCNN\nbc-LSTM\nbc-LSTM+Att\nDialogueRNN\nCT-Net\nAR-IIGCN",
          "MELD": "76.23 74.91\n43.35 45.51\n4.63 3.71\n18.25 21.17\n46.14 49.47\n8.91 8.36\n35.33 34.51\n56.35 55.01\n78.45 73.84\n46.82 47.71\n3.84 5.46\n22.47 25.19\n51.61 51.34\n4.31 5.23\n36.71 38.44\n57.51 55.94\n70.45 75.55\n46.43 46.35\n0.00 0.00\n21.77 16.27\n49.30 50.72\n0.00 0.00\n41.77 40.71\n58.51 55.84\n72.12 73.54\n54.42 49.47\n1.61 1.23\n23.97 23.83\n52.01 50.74\n1.52 1.73\n41.01 41.54\n56.12 55.97\n11.62 11.27\n75.61 77.45\n51.32 52.76\n5.14 10.09\n30.91 32.56\n54.31 56.08\n42.51 44.65\n61.93 60.57\n81.10 81.19\n56.16 57.24\n47.41 37.32\n65.92 65.92\n64.14 64.01\n6.90 5.06\n2.94 2.94\n42.28 45.22"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: , the WA and WF1 values of emotion 60",
      "data": [
        {
          "Method": "bc-LSTM\nbc-LSTM⋄",
          "IEMOCAP\nMELD\nParams\nWAA\nWF1\nWAA\nWF1": "0.53M\n55.2\n54.9\n57.1\n56.4\n14.68M\n52.9\n52.7\n53.3\n52.9"
        },
        {
          "Method": "DialogueRNN\nDialogueRNN⋄",
          "IEMOCAP\nMELD\nParams\nWAA\nWF1\nWAA\nWF1": "13.19M\n63.4\n62.7\n56.1\n56.0\n14.68M\n62.7\n62.2\n54.8\n54.1"
        },
        {
          "Method": "AR-IIGCN",
          "IEMOCAP\nMELD\nParams\nWAA\nWF1\nWAA\nWF1": "70.5\n70.4\n64.1\n64.0\n14.68M"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Attentionemotion-enhanced convolutional lstm for sentiment analysis",
      "authors": [
        "F Huang",
        "X Li",
        "C Yuan",
        "S Zhang",
        "J Zhang",
        "S Qiao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "2",
      "title": "Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2023",
      "venue": "Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "arxiv": "arXiv:2312.10579"
    },
    {
      "citation_id": "3",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2312.06337"
    },
    {
      "citation_id": "4",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "5",
      "title": "Time-frequency representation and convolutional neural network-based emotion recognition",
      "authors": [
        "S Khare",
        "V Bajaj"
      ],
      "year": "2020",
      "venue": "IEEE transactions on neural networks and learning systems"
    },
    {
      "citation_id": "6",
      "title": "Cauain: Causal aware interaction network for emotion recognition in conversations",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI"
    },
    {
      "citation_id": "7",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Efficient low-rank multimodal fusion with modalityspecific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "9",
      "title": "Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng",
        "K Li"
      ],
      "year": "2023",
      "venue": "Czl-ciae: Clip-driven zeroshot learning for correcting inverse age estimation",
      "arxiv": "arXiv:2312.01758"
    },
    {
      "citation_id": "10",
      "title": "Graph information bottleneck for remote sensing segmentation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng"
      ],
      "year": "2023",
      "venue": "Graph information bottleneck for remote sensing segmentation",
      "arxiv": "arXiv:2312.02545"
    },
    {
      "citation_id": "11",
      "title": "A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "J Du",
        "H Liu",
        "K Li"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "12",
      "title": "Prediction model of dow jones index based on lstm-adaboost",
      "authors": [
        "R Ying",
        "Y Shou",
        "C Liu"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communications, Information System and Computer Engineering (CISCE)"
    },
    {
      "citation_id": "13",
      "title": "Object detection in medical images based on hierarchical transformer and mask mechanism",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "C Xie",
        "H Liu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "14",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "S Yang",
        "K Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "15",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "Multi-modal fusion network with complementarity and importance for emotion recognition",
      "authors": [
        "S Liu",
        "P Gao",
        "Y Li",
        "W Fu",
        "W Ding"
      ],
      "year": "2023",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition in context",
      "authors": [
        "R Kosti",
        "J Alvarez",
        "A Recasens",
        "A Lapedriza"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "19",
      "title": "Context-aware emotion recognition networks",
      "authors": [
        "J Lee",
        "S Kim",
        "S Kim",
        "J Park",
        "K Sohn"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Pirnet: Personality-enhanced iterative refinement network for emotion recognition in conversation",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "21",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Multi-modal sequence fusion via recursive attention for emotion recognition",
      "authors": [
        "R Beard",
        "R Das",
        "R Ng",
        "P Gopalakrishnan",
        "L Eerens",
        "P Swietojanski",
        "O Miksik"
      ],
      "year": "2018",
      "venue": "Proceedings of the 22nd Conference on Computational Natural Language Learning"
    },
    {
      "citation_id": "23",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "24",
      "title": "C-gcn: correlation based graph convolutional network for audio-video emotion recognition",
      "authors": [
        "W Nie",
        "M Ren",
        "J Nie",
        "S Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "25",
      "title": "Hierarchical context-based emotion recognition with scene graphs",
      "authors": [
        "S Wu",
        "L Zhou",
        "Z Hu",
        "J Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "26",
      "title": "Integrating multi-label contrastive learning with dual adversarial graph neural networks for crossmodal retrieval",
      "authors": [
        "S Qian",
        "D Xue",
        "Q Fang",
        "C Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Learning enhanced acoustic latent representation for small scale affective corpus with adversarial cross corpora integration",
      "authors": [
        "C.-M Chang",
        "C.-C Lee"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "28",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Contrastive adversarial learning for person independent facial emotion recognition",
      "authors": [
        "D Kim",
        "B Song"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "30",
      "title": "A self-fusion network based on contrastive learning for group emotion recognition",
      "authors": [
        "X Wang",
        "D Zhang",
        "H.-Z Tan",
        "D.-J Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "31",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "32",
      "title": "Densely connected convolutional networks",
      "authors": [
        "G Huang",
        "Z Liu",
        "L Van Der Maaten",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "33",
      "title": "Transfer learning from speaker verification to multispeaker text-to-speech synthesis",
      "authors": [
        "Y Jia",
        "Y Zhang",
        "R Weiss",
        "Q Wang",
        "J Shen",
        "F Ren",
        "P Chen",
        "R Nguyen",
        "I Pang",
        "Y Moreno",
        "Wu"
      ],
      "year": "2018",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations"
    },
    {
      "citation_id": "35",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "36",
      "title": "Convolutional neural networks for sentence classification",
      "authors": [
        "Y Kim"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). ACL"
    },
    {
      "citation_id": "37",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "38",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    }
  ]
}