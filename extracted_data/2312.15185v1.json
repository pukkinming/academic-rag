{
  "paper_id": "2312.15185v1",
  "title": "Emotion2Vec: Self-Supervised Pre-Training For Speech Emotion Representation",
  "published": "2023-12-23T07:46:55Z",
  "authors": [
    "Ziyang Ma",
    "Zhisheng Zheng",
    "Jiaxin Ye",
    "Jinchao Li",
    "Zhifu Gao",
    "Shiliang Zhang",
    "Xie Chen"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We propose emotion2vec, a universal speech emotion representation model. emotion2vec is pre-trained on open-source unlabeled emotion data through self-supervised online distillation, combining utterance-level loss and framelevel loss during pre-training. emotion2vec outperforms state-of-the-art pre-trained universal models and emotion specialist models by only training linear layers for the speech emotion recognition task on the mainstream IEMOCAP dataset. In addition, emotion2vec shows consistent improvements among 10 different languages of speech emotion recognition datasets. emotion2vec also shows excellent results on other emotion tasks, such as song emotion recognition, emotion prediction in conversation, and sentiment analysis. Comparison experiments, ablation experiments, and visualization comprehensively demonstrate the universal capability of the proposed emotion2vec. To the best of our knowledge, emotion2vec is the first universal representation model in various emotion-related tasks, filling a gap in the field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Extracting emotional representation from speech is an essential step of various emotional tasks such as speech emotion recognition (SER) and sentiment analysis. Traditional methods employ Filter Banks (FBanks) or Mel Frequency Cepstrum Coefficients (MFCCs) as speech features. These features are not rich in semantic information, resulting in limited performance on emotional tasks. Popular methods utilize features extracted from speech-based self-supervised learning (SSL) pre-trained models, leading to a significant performance improvement.\n\nOne potential challenge blocking further performance improvement is that these SSL models are not entirely suitable for emotional tasks.  Wang et al. (2021)  explore no fine-tuning, partial finetuning, and entire fine-tuning with some SSL models for SER on the IEMOCAP dataset  (Busso et al., 2008) , and give some empirical conclusions. While this is an ad-hoc solution, on the one hand, finetuning SSL models requires a large computational cost, on the other hand, these conclusions may be data-specific or model-constrained. Recently,  Chen et al. (2023a)  proposed an SER model named Vesper, which is obtained by model distillation from WavLM-large  (Chen et al., 2022)  with emotion data. Vesper is designed to perform the SER task, whose universal representation capability still needs to be demonstrated. Accordingly, a universal speech-based emotion representation model is urgently needed in the field.\n\nHere we propose emotion2vec, a universal emotion representation model that can be used to extract speech features for diverse emotion tasks. Self-supervised pre-training is performed on 262 hours of open-source emotion data with an online distillation paradigm to obtain emotion2vec. Considering that both whole-play information and local details convey emotion, we propose a pre-training strategy combining utterance-level loss and framelevel loss. On the mainstream IEMOCAP dataset, the downstream linear model trained with features extracted from emotion2vec outperforms all the mainstream SSL models and the latest specialist models. emotion2vec is tested on 13 datasets including 10 languages, and the results show that emotion2vec exhibits language generalization ability. Moreover, in addition to the SER task, we also experimented with emotion2vec features on song emotion recognition, emotion prediction in conversation, and sentiment analysis. The results indicate that emotion2vec has excellent task generalization ability. Extensive ablation experiments and visualization analysis demonstrate the effectiveness of our pre-training methods and the versatility of the proposed emotion2vec model. Self-supervised learning has achieved remarkable success in the field of representation learning, showcasing its efficacy across natural language processing  (Devlin et al., 2019; Liu et al., 2019; Radford et al., 2019; Brown et al., 2020) , computer vision  (Grill et al., 2020; He et al., 2020; Bao et al., 2021; He et al., 2022) , as well as speech processing  (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022; Baevski et al., 2022) . For speech representation learning, all SSL models can be classified into two categories according to the selfsupervised targets utilized during pre-training  (Ma et al., 2023b) : 1) Offline targets. 2) Online targets. Models employing offline targets often require a well-trained teacher model before the pre-training stage, to extract self-supervised targets. Representative models of this type are  HuBERT (Hsu et al., 2021) , WavLM  (Chen et al., 2022)  using K-means targets, and PBERT  (Wang et al., 2022) , MonoBERT&PolyBERT  (Ma et al., 2023c)  using phoneme-based targets. Models using online targets do not need a pre-trained teacher model in advance, while the teacher models are constantly updated during the pre-training phase, with an online distillation paradigm. Representative models of this type are data2vec  (Baevski et al., 2022) , data2vec 2.0  (Baevski et al., 2023 ) using framelevel mask language model (MLM) loss, and CA-DINO  (Han et al., 2023)  using utterance-level cross-entropy loss. emotion2vec is pre-trained combining both utterance-level loss and frame-level loss, leading to a superior speech emotion representation model.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Representation",
      "text": "We present the first universal speech emotion representation model, whereas most of the previous works directly employ speech pre-training models  (Pepino et al., 2021; Li et al., 2022) , or finetune the pre-training models on their specific emotional data with specific emotional tasks (mostly SER)  (Morais et al., 2022; Chen and Rudnicky, 2023) , to extract speech emotion representation. A series of works investigate the SER performance of wav2vec 2.0  (Wang et al., 2021) , HuBERT  (Wang et al., 2021) , as well as WavLM  (Ioannides et al., 2023) , either fine-tuning or not. A recent work  (Ma et al., 2023a)  found that data2vec features also have a good representation ability in the SER task.\n\nFor speech emotion representation in other emotion tasks, such as multimodal emotion recognition, popular practice  (Li et al., 2023a)  is similar to what is mentioned above.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methods",
      "text": "Here we mainly introduce the self-supervised pretraining method of the proposed emotion2vec, for which the core is to train the model with Utterancelevel Loss and Frame-level Loss using Online Distillation paradigm.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Pipeline",
      "text": "As shown in Figure  1 , emotion2vec contains two networks in the pre-training phase, which are the teacher network T and the student network S. Both models share the same model architecture, including a feature extractor F composed of multi-layer convolutional neural networks and a backbone network B composed of multi-layer Transformers. These modules can be configured with different architectures, which will be described in Section 4.1. Given a raw audio utterance\n\nthe Teacher T and the Student S respectively utilize feature extractors F T and F S to obtain the downsampled features Z = [z 1 , • • • , z Nz ], which can be written as:\n\nZ S = F S (X).\n\n(2)\n\nFor the teacher network T , the downsampled features Z T are directly fed into the backbone network B T . For the student network S, the downsampled features Z S are masked l consecutive frames with probability p for each frame as the start. Then learnable utterance embedding\n\nis placed in the front before being fed into the backbone network B S . The formula can be written as follows:\n\nwhere Y T is the average of the output embedding of the top k layer Transformer Block B T i . Utterance-level output embedding U S and framelevel output embedding Y S are the outputs of the student backbone network B S . M ask is the applying mask operation. Y T , Y S and U S are the same in the hidden layer dimensions, where Y T and Y S have the same N z temporal dimensions, while U S has N u temporal dimensions, respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Utterance-Level Loss",
      "text": "Utterance-level loss constructs an utterance-level pretext task to learn the global emotion. We use mean squared error (MSE) to calculate the loss, which can be written as:\n\nwhere\n\nwhich means that utterance-level loss L U tt is computed by temporal pooling results of Y T and U S .\n\nHere we propose three ways to compute utterancelevel loss, which we call token embedding, chunk embedding, and global embedding, as shown in Figure  2 .\n\nToken Embedding Token embedding employs a single token to represent global emotion information encoded by the student network S. More explicitly, we set N u to 1 in the learnable utterance embedding\n\nChunk Embedding Chunk embedding employs multiple tokens to represent global emotion information. In this case, more global information can be aggregated within the chunk.\n\nGlobal Embedding In the case of utilizing global embedding, no additional utterance tokens are added. We use temporal pooling of frame-level output embedding Y S instead of U S to compute the loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Utterance-Level",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Frame-Level Loss",
      "text": "Frame-level loss constructs a frame-wise pretext task to learn the context emotion. We only compute the loss on the masked part, which is the common practice for a mask language modeling(MLM) pretext task. The frame-level loss L F rm can be expressed as:\n\nwhere M denotes the index sequence of frame-level output embedding Y S being masked, and M denotes the total number of tokens being masked.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Online Distillation",
      "text": "Online distillation is a self-supervised learning strategy for teacher-student learning, where the student network updates parameters by backpropagation and the teacher network updates parameters with an exponentially moving average (EMA)  (Grill et al., 2020) . For the student network S, the total loss L for backpropagation is a combination of frame-level loss L F rm and utterance-level loss L U tt , donated as:\n\nwith a tunable weight α. For the teacher network T , The parameters θ T 0 are initialized as the same parameters of the student network θ S 0 , and then are updated with EMA within each mini-batch, donated as:\n\nwhere τ is a parameter that increases linearly during pre-training. In practice, within each minibatch the parameters of teacher feature extractor F T are copied directly from F S , while the parameters of teacher backbone network B T are updated with EMA from B T and B S .\n\n4 Experiments Setup",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Initial Model",
      "text": "Different initial models lead to different architectures of feature extractors F, backbone networks B, and initialization parameters θ 0 . Here we adopt two models, data2vec 2  and data2vec 2.0 3  , both of which have the same feature extractor design but different backbone network designs. The feature extractor F is a 7-layer 1-D convolutional neural network with kernel sizes (5, 2, 2, 2, 2, 2, 2) and strides  (10, 3, 3, 3, 3, 2, 2) , resulting in 320x downsampling. Given the raw audio input X at a 16000 Hz sample rate, the output representations Z are 50\n\nHz with dimension 512. Then a linear projection for dimension transformation from 512 to 768 is applied, followed by the mask operation to construct the input for the backbone network B. Here we briefly introduce different backbone networks in data2vec and data2vec 2.0.\n\ndata2vec The backbone network B contains a 5-layer learnable convolutional positional encoding followed by a 12-layer standard Transformer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training Details",
      "text": "Self-supervised Pre-training In the pre-training phase, we train emotion2vec with 262 hours of unlabeled emotion data shown in Figure  1  with different initial models. For the training overhead, The pre-training is conducted on 4 NVIDIA A10 Tensor Core GPUs, and we simulate 16 GPUs by setting the update frequency to 4. We train emo-tion2vec for 100 epochs, each of which takes about 37 minutes. We use a dynamic batchsize, where the maximum number of tokens is 1 × 10 6 . For the optimizing strategy, we use Adam with a learning rate of 7.5 × 10 -5 and a weight decay of 1 × 10 -2 . We train emotion2vec using a cosine learning rate scheduler, with 5% proportion of linear warm-up.\n\nFor the student model, each time step of the input has a probability of p = 0.5 to be the start index, and the subsequent l = 5 time steps are masked. The hyperparameter α that controls the loss weight is set to 1. For the teacher model, we use the average of the top k = 8 blocks of the transformer layer outputs for providing the training targets. We apply a linearly increasing strategy for τ from τ s = 0.999 to τ e = 0.99999 for the teacher parameters exponentially moving average.\n\nSupervised Fine-tuning All model architectures of diverse downstream tasks are designed to be as simple as possible, to demonstrate the representation ability of the pretrained model. For the non-sequential task, following the common practice of SUPERB  (Yang et al., 2021) , we use two linear layers with a ReLU activation function sandwiched between them. For the sequential task, we use two layers of gated recurrent units (GRU) to make predictions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "A summary of the datasets employed in our experiments is presented in Table  1 . There are 18 emotional datasets including 10 different languages: 9 in English, and 1 in Mandarin, Bangla, French, German, Greek, Italian, Persian, Russian, and Urdu.\n\nFor each dataset, it can be categorized in terms of Pretrain (i.e., whether used during the pre-training phase), Downstream (i.e., whether tested in the downstream task), Source (i.e., where samples collected), Emo (i.e., number of emotion categories), Spk (i.e., number of speakers), Lang, (i.e., Language), #Utts (i.e., number of utterances), and #Hours (i.e., total duration of samples). Speech data is extracted from these datasets and uniformly processed into a single channel of 16k Hz.\n\nIn the pretraining phase, we utilize five large-scale English datasets, including IEMO-CAP  (Busso et al., 2008) , MELD  (Poria et al., 2019) , MEAD  (Wang et al., 2020) , CMU-MOSEI  (Zadeh et al., 2018) , and MSP-Podcast  (Martinez-Lucas et al., 2020) , resulting in a total of 262 hours. The IEMOCAP corpus contains a total of 5 sessions and 10 different speakers, with each session being a conversation of two exclusive speakers. MELD is a multi-party conversational dataset containing about 13,847 utterances from 1,433 dialogues collected from the TV series 'Friends'. MEAD is a talking-face video corpus featuring 60 actors and actresses talking with 8 different emotions at three different intensity levels. CMU-MOSEI is a multimodal dataset from YouTube for sentiment and emotion analysis in videos. MSP-Podcast is collected from podcast recordings that discuss a variety of topics like politics, sports, and movies.\n\nDifferent datasets are used to test different downstream tasks with various languages. For main results in Section 5.2, we report cross-validation (CV) results on the IEMOCAP dataset. The original labels cover five classes, to be consistent and comparable with previous methods  (Ye et al., 2023; Chen et al., 2023b) , we merge 'excited' with 'happy' to better balance the size of each emotion class, resulting in four classes. We conduct both leave-onesession-out 5-fold CV and leave-one-speaker-out 10-fold CV. Moreover, we report results on MELD under its original split setup, and RAVDESS (Livingstone and Russo, 2018), SAVEE  (Jackson and Haq, 2014)  datasets under a random leave-one-out 10-fold CV setup, which implies at each fold, all samples within the dataset are randomly split into 80%, 10%, and 10% samples in training, validation, and testing sets. Among them, speech in RAVDESS and SAVEE datasets is not seen in the pre-training stage, which demonstrates the generalization of the proposed model on out-of-domain corpora.\n\nFor language generalization task in Section 5.3, we report CV results for 9 out-of-domain datasets, including 1 in Mandarin (M3ED  (Zhao et al., 2022) ), Bangla (SUBESCO  (Sultana et al., 2021) ), French (CaFE  (Gournay et al., 2018) ), German (EmoDB  (Burkhardt et al., 2005) ), Greek (AESDD  (Vryzas et al., 2018) ), Italian (EMOVO  (Costantini et al., 2014) ), Persian (ShEMO  (Mohamad Nezami et al., 2019) ), Russain (RESD  (Lubenets et al.) ), and Urdu (URDU  (Latif et al., 2018) ). If not specified, language generalization results are obtained using the random leaveone-out 10-fold CV as we mentioned above unless the dataset provides a set partition. Such as the RESD dataset, we follow its original split setup with 280 testing samples and 1116 training samples. Additionally, we allocate 10% from the training samples for validation and others for training.\n\nFor task generalization task in Section 5.4. We tested other speech emotion tasks, including song emotion recognition, emotion prediction in conversation, and sentiment analysis, on RAVDESS-Song (Livingstone and Russo, 2018), IEMOCAP and CMU-MOSI  (Zadeh et al., 2016)  & CMU-MOSEI  (Zadeh et al., 2018) . For song emotion recognition and emotion prediction in conversation, we report CV results. For sentiment analysis, we report results with its original split setup. To be comparable with previous work, the experimental setup varies according to the specific task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "We apply commonly used evaluation metrics, weighted accuracy (WA), unweighted accuracy (UA), and weighted average F1 (WF1), to evaluate the performance of speech emotion tasks. WA corresponds to the overall accuracy and UA corresponds to the average class-wise accuracy. WF1 is a comprehensive evaluation, especially for the situation of sample imbalance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Main Results",
      "text": "The results are shown in Table  2 , where we compare different SSL pre-trained models on the IEMOCAP dataset, as well as larger-scale pretrained models, and the latest specialist models designed for SER tasks. We follow the evaluation of SUPERB  (Yang et al., 2021) , freezing the pre-trained model and training downstream linear layers with the hidden dimensional set to 256. As can be seen from the table, emotion2vec outperforms all existing SSL pre-trained models, across all base models with similar parameters and large models with greater parameters. Compared with Versper-12, an SER model obtained by distillation from WavLM-large, emotion2vec works better with fewer parameters. TIM-NET  (Ye et al., 2023) , MSTR  (Li et al., 2023b) , and DST  (Chen et al., 2023b)  are the latest SER specialist models, respectively, which use different scales of upstream features and downstream networks. The proposed emotion2vec model outperforms or per-forms on par with these models with only linear layers, while their downstream networks have 2x, 135x, and 114x more parameters than emotion2vec, respectively. We provide the results of leave-onesession-out five-fold cross-validation and leaveone-speaker-out ten-fold cross-validation for reference.\n\nWe also conduct experiments on other mainstream English datasets to prove the generalization of emotion2vec in Table  3 . MELD is a noisy dataset used to test the SER performance of the model in complex environments. RAVDESS and SAVEE are out-of-domain datasets with respective recording environments. Experimental results show that emotion2vec exhibits state-of-the-art performance on different datasets in different environments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Language Generalization",
      "text": "Given the various languages, the SER datasets exhibit notable domain shifts. The generalization of the model to unseen language is critically important for SER. We validate the generalization of emotion2vec and other baselines on the out-ofdomain language SER datasets. We follow the evaluation of SUPERB  (Yang et al., 2021) , freezing the pre-trained models and training downstream linear layers with the hidden dimensional set to 256, where the WavLM-base, WavLM-base+, data2vec, dat2vec 2.0, and emotion2vec are our implementations following the practice above. As shown in Table  4 , emotion2vec outperforms all the SSL baseline methods on the 9 different lingual datasets in Table  2 : SER task performance of different SSL pre-trained models on the IEMOCAP dataset. The setting of the downstream models follows SUPERB  (Yang et al., 2021)  to use linear layers to test the representation ability of different upstream models. \"LS-960\" means LibriSpeech 960 hours, \"LL-60k\" means LibriLight 60k hours, and \"Mix-94k\" means 94k hours of data including LibriLight, VoxPopuli, and GigaSpeech. For emotion data, \"LSED-206\" means LSED 206 hours, and \"Emo-262\" refers to the 262 hours of pre-training data in Table  1 . Models are tested using leave-one-session-out five-fold cross-validation with 20% from the training set used as the validation set for each session. Models with underline are leave-one-speaker-out ten-fold cross-validation with 8 speakers for training, 1 speaker for validation, and 1 speaker for testing within each fold. Models with * imply the same fold for both validation and testing, for a fair comparison as some work uses this principle. We also compare with larger-scale pre-trained models and the latest specialist models designed for SER tasks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Model",
      "text": "Pre-training Corpus Upstream #Upstream Params Downstream #Downstream Params WA(%) ↑ Self-supervised Model small size wav2vec  (Schneider et al., 2019)  LS-960 Proposed 32.54M Linear 0.13M 59.79 vq-wav2vec  (Baevski et al., 2019)  34.15M 0.20M 58.24 base size wav2vec 2.0  (Baevski et al., 2020)  LS     (Koh and Dubnov, 2021)  Freeze 71.0 --SpecMAE  (Sadok et al., 2023)  Finetune 54.5 -53.9 VQ-MAE-S (Patch-tf)  (Sadok et al., 2023)     (Sadok et al., 2023)  85.8 -85.7\n\nterms of WA, UA, and WF1. These results demonstrate that emotion2vec captures the emotion patterns across languages and shows state-of-the-art performance.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Task Generalization",
      "text": "In order to verify the generalization of the model, in addition to speech emotion recognition, we tested other speech emotion tasks, including song emotion recognition, emotion prediction in conversation, and sentiment analysis.\n\nSong Emotion Recognition Song emotion recognition is a sub-task of music emotion recognition (MER), which aims to identify the emotion expressed in a singing voice. Following common practice, we perform five-fold cross-validation with randomly shuffled data, and remain one fold unseen during each training, to demonstrate the generalization of the features. WavLM-base, WavLM-base+, data2vec, dat2vec 2.0, and emotion2vec are our implementations, following the practice above. The results for L 3 -NET, SpecMAE, and VQ-MAE-S are taken from their papers. As shown in Table  5 , emotion2vec outperforms all known SSL models even without finetuning the model in the song emotion recognition task.\n\nEmotion Prediction in Conversation Emotion prediction in conversation (EPC) refers to predicting the future emotional state of a specific speaker based on historical conversation information. We reproduce  Shi et al.'s (2023)  method except that the speech features are obtained from our proposed emotion2vec. Briefly, the model employs several GRUs with a hierarchical structure for emotion prediction. Each prediction takes the previous 6 turns of the dialogue, in which one speaker can say multiple utterances in each turn. The network dimensions, hyperparameters, and training strategies are kept the same as the reference implementation with leave-one-speaker-out 10-fold crossvalidation. For the speech modality, the input is 768-dimensional emotion2vec features. For the text modality, the input is 378-dimensional BERT  (Devlin et al., 2019)  features. For the speech + text multimodal, the input is a concatenation of emo-tion2vec features and BERT features, which also remains the same as the reference implementation.\n\nAs shown in Table  6 , with speech features replaced with emotion2vec in the EPC task, there are performance gains in both speech single modality and speech-text multi-modality.   (Zadeh et al., 2016)  and CMU-MOSEI  (Zadeh et al., 2018) , respectively. Also in line with  Lian et al.'s (2023)  practice, we utilize the mean of the last four layers' features of the pretrained model, to train the downstream linear layers.\n\nAs shown in Table  7 , emotion2vec outperforms pretrained data2vec and WavLM with self-supervised learning and pretrained Whisper Encoder  (Radford et al., 2023)  with supervised learning utilizing ASR task.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study",
      "text": "If not specified, results are obtained using the standard leave-one-session-out 5-fold cross-validation on the IEMOCAP dataset.\n\nInitialization Method In this experiment, we explore the impact of initialization methods on performance. data2vec and data2vec 2.0 are two representative models trained with online distillation, both of which are pre-trained on Librispeech 960 hours. As shown in Table  8 , initializing with a pretrained model would be better than the cold start method. Model initializing with data2vec 2.0 performs better than the one initializing with data2vec.   Utterance-level Loss Weight In this experiment, we compare the impact of utterance-level loss weight on performance. As shown in Table  11 , weighting utterance-level loss and frame-level loss with a ratio of 1:1 works best.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Visualization",
      "text": "To investigate the intuitive effect of emotion2vec and other SSL baselines on emotion representation learning, we visualize the representations learned by WavLM, data2vec, and emotion2vec through the UMAP technique  (McInnes et al., 2018)  in Figure  3  and Figure  4 . We conduct the leave-onesession-out evaluation strategy on IEMOCAP, and the 8:2 hold-out evaluation on SUBESCO, both of which we randomly select 10% samples from the training set as the validation set. Specifically, for a fair comparison, the representations from the first linear layer are visualized after an identical training phase for different SSL models. Figure  3  visualizes different SSL models to represent arousal. In a sense, arousal refers to emotional intensity. Figure  3       heavy overlapping between high and low arousal emotion classes. In contrast, Figure  3  (c) shows that the high arousal and low arousal representations are clustered receptively, and the feature distribution exhibits a trend transitioning from high arousal to low arousal, which is more reasonable compared to other methods. Figure  4  shows the ability of different SSL models to represent discrete emotion classes. As Figure  4  (a) and Figure  4  (b) show, WavLM and data2vec suffer from class confusion problems. On the contrary, the features learned by emotion2vec demonstrate a higher intraclass compactness and a much larger inter-class margin. The results indicate that emotion2vec provides more class-discriminative and emotion-aware representations to support its superior performance.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose emotion2vec, a universal emotion representation model. emotion2vec is pre-trained on 262 hours of unlabeled emotion data through self-supervised online distillation, leading to universal emotion representation ability. We prove that our strategy of combining utterancelevel loss and frame-level loss during emotion pretraining is effective. Extensive experiments demonstrate that the proposed emotion2vec has the ability to extract emotion representation across different tasks, languages, and scenarios. In the future, we will explore the scaling law of emotion representation models, namely how to provide a better representation with more data and larger parameters.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , emotion2vec contains two",
      "page": 2
    },
    {
      "caption": "Figure 1: The overall framework of emotion2vec. During the pre-training phase, emotion2vec conducts online dis-",
      "page": 3
    },
    {
      "caption": "Figure 2: Token Embedding",
      "page": 3
    },
    {
      "caption": "Figure 2: Different ways to compute utterance-level loss",
      "page": 3
    },
    {
      "caption": "Figure 3: and Figure 4. We conduct the leave-one-",
      "page": 9
    },
    {
      "caption": "Figure 3: visualizes different SSL models to rep-",
      "page": 9
    },
    {
      "caption": "Figure 3: (a) and Figure 3 (b) show",
      "page": 9
    },
    {
      "caption": "Figure 3: UMAP visualizations of learned features on downstream SER task from WavLM, data2vec, and emo-",
      "page": 10
    },
    {
      "caption": "Figure 4: UMAP visualizations of learned features on downstream SER task from WavLM, data2vec, and emo-",
      "page": 10
    },
    {
      "caption": "Figure 3: (c) shows",
      "page": 10
    },
    {
      "caption": "Figure 4: shows the",
      "page": 10
    },
    {
      "caption": "Figure 4: (a) and Figure 4",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 2: SER task performance of different SSL pre-trained models on the IEMOCAP dataset. The setting of",
      "data": [
        {
          "Model": "",
          "WA(%) ↑\nUA(%) ↑\nWF1(%) ↑": "SAVEE"
        },
        {
          "Model": "WavLM-base\nWavLM-base+\ndata2vec\ndata2vec 2.0\nemotion2vec",
          "WA(%) ↑\nUA(%) ↑\nWF1(%) ↑": "42.08\n38.46\n38.93\n43.54\n39.27\n42.19\n82.50\n82.26\n82.37\n82.94\n83.13\n83.03\n84.38\n84.45\n82.30"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: SER task performance of different SSL pre-trained models on the IEMOCAP dataset. The setting of",
      "data": [
        {
          "Model": "",
          "WA(%) ↑\nUA(%) ↑\nWF1(%) ↑": "CAFE (Fr)",
          "WF1(%) ↑\nUA(%) ↑\nWF1(%) ↑": "RESD (Ru)"
        },
        {
          "Model": "WavLM-base\nWavLM-base+\ndata2vec\ndata2vec 2.0\nemotion2vec",
          "WA(%) ↑\nUA(%) ↑\nWF1(%) ↑": "31.61\n32.02\n30.88\n31.40\n33.39\n30.40\n57.10\n57.68\n57.36\n71.51\n72.98\n71.50\n74.52\n75.26\n74.53",
          "WF1(%) ↑\nUA(%) ↑\nWF1(%) ↑": "56.17\n56.17\n55.69\n55.00\n55.19\n55.08\n49.42\n49.77\n48.97\n64.08\n64.33\n64.17\n64.75\n65.04\n64.53"
        },
        {
          "Model": "Model",
          "WA(%) ↑\nUA(%) ↑\nWF1(%) ↑": "EMOVO (It)",
          "WF1(%) ↑\nUA(%) ↑\nWF1(%) ↑": "M3ED (Zh)"
        },
        {
          "Model": "WavLM-base\nWavLM-base+\ndata2vec\ndata2vec 2.0\nemotion2vec",
          "WA(%) ↑\nUA(%) ↑\nWF1(%) ↑": "40.17\n40.34\n37.36\n40.34\n41.98\n40.11\n51.21\n51.97\n49.82\n60.69\n61.27\n60.79\n61.21\n62.97\n60.89",
          "WF1(%) ↑\nUA(%) ↑\nWF1(%) ↑": "44.03\n18.90\n34.50\n45.09\n20.18\n36.49\n44.44\n21.10\n37.77\n47.50\n24.12\n41.74\n49.15\n26.98\n44.38"
        },
        {
          "Model": "Model",
          "WA(%) ↑\nUA(%) ↑\nWF1(%) ↑": "ShEMO (Fa)",
          "WF1(%) ↑\nUA(%) ↑\nWF1(%) ↑": "URDU (Ur)"
        },
        {
          "Model": "WavLM-base\nWavLM-base+\ndata2vec\ndata2vec 2.0\nemotion2vec",
          "WA(%) ↑\nUA(%) ↑\nWF1(%) ↑": "67.27\n46.60\n65.63\n66.73\n44.29\n65.12\n70.80\n53.96\n69.84\n77.90\n62.03\n76.96\n79.97\n66.04\n79.56",
          "WF1(%) ↑\nUA(%) ↑\nWF1(%) ↑": "71.00\n70.25\n70.82\n67.25\n68.68\n67.47\n71.75\n72.67\n71.83\n77.50\n78.42\n77.12\n81.50\n81.87\n81.60"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "↑ WF1(%) ↑ WA(%) ↑ UA(%) ↑ WF1(%) ↑ WA(%) ↑ UA(%) ↑ WF1(%) ↑ MELD",
      "authors": [
        "↑ Ua(%)"
      ],
      "venue": "RAVDESS SAVEE WavLM-base"
    },
    {
      "citation_id": "2",
      "title": "Efficient self-supervised learning with contextualized target representations for vision, speech and language",
      "authors": [
        "Alexei Baevski",
        "Arun Babu",
        "Wei-Ning Hsu",
        "Michael Auli"
      ],
      "year": "2023",
      "venue": "Proc. IMCL"
    },
    {
      "citation_id": "3",
      "title": "2022. data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "Alexei Baevski",
        "Wei-Ning Hsu",
        "Qiantong Xu",
        "Arun Babu",
        "Jiatao Gu",
        "Michael Auli"
      ],
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "4",
      "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "Alexei Baevski",
        "Steffen Schneider",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "6",
      "title": "BEiT: BERT pre-training of image Transformers",
      "authors": [
        "Hangbo Bao",
        "Li Dong",
        "Songhao Piao",
        "Furu Wei"
      ],
      "year": "2021",
      "venue": "Proc. ICLR"
    },
    {
      "citation_id": "7",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "8",
      "title": "A database of German emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "9",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "10",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "Li-Wei Chen",
        "Alexander Rudnicky"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "11",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang",
        "Zhengyang Chen",
        "Yu Wu",
        "Shujie Liu",
        "Zhuo Chen",
        "Jinyu Li",
        "Naoyuki Kanda",
        "Takuya Yoshioka",
        "Xiong Xiao"
      ],
      "year": "2022",
      "venue": "Proc. JSTSP"
    },
    {
      "citation_id": "12",
      "title": "Vesper: A compact and effective pretrained model for speech emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Peihao Chen",
        "Xiangmin Xu"
      ],
      "year": "2023",
      "venue": "Vesper: A compact and effective pretrained model for speech emotion recognition"
    },
    {
      "citation_id": "13",
      "title": "Jianxin Pang, and Lan Du. 2023b. DST: Deformable speech Transformer for emotion recognition",
      "authors": [
        "Weidong Chen",
        "Xiaofen Xing",
        "Xiangmin Xu"
      ],
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "14",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "Giovanni Costantini",
        "Iacopo Iaderola",
        "Andrea Paoloni",
        "Massimiliano Todisco"
      ],
      "year": "2014",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "15",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proc. NAACL"
    },
    {
      "citation_id": "16",
      "title": "A Canadian French emotional speech dataset",
      "authors": [
        "Philippe Gournay",
        "Olivier Lahaie",
        "Roch Lefebvre"
      ],
      "year": "2018",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "17",
      "title": "Bootstrap your own latent: a new approach to self-supervised learning",
      "authors": [
        "Jean-Bastien Grill",
        "Florian Strub",
        "Florent Altché",
        "Corentin Tallec",
        "Pierre Richemond",
        "Elena Buchatskaya",
        "Carl Doersch",
        "Bernardo Avila Pires",
        "Zhaohan Guo",
        "Mohammad Gheshlaghi Azar"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "18",
      "title": "Self-supervised learning with cluster-aware-DINO for high-performance robust speaker verification",
      "authors": [
        "Bing Han",
        "Zhengyang Chen",
        "Yanmin Qian"
      ],
      "year": "2023",
      "venue": "Self-supervised learning with cluster-aware-DINO for high-performance robust speaker verification"
    },
    {
      "citation_id": "19",
      "title": "Masked autoencoders are scalable vision learners",
      "authors": [
        "Kaiming He",
        "Xinlei Chen",
        "Saining Xie",
        "Yanghao Li",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2022",
      "venue": "Proc. CVPR"
    },
    {
      "citation_id": "20",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Kaiming He",
        "Haoqi Fan",
        "Yuxin Wu",
        "Saining Xie",
        "Ross Girshick"
      ],
      "year": "2020",
      "venue": "Proc. TASLP"
    },
    {
      "citation_id": "21",
      "title": "Towards paralinguistic-only speech representations for endto-end speech emotion recognition",
      "authors": [
        "George Ioannides",
        "Michael Owen",
        "Andrew Fletcher",
        "Viktor Rozgic",
        "Chao Wang"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Surrey audiovisual expressed emotion (SAVEE) database",
      "authors": [
        "Philip Jackson",
        "Sjuosg Haq"
      ],
      "year": "2014",
      "venue": "Surrey audiovisual expressed emotion (SAVEE) database"
    },
    {
      "citation_id": "23",
      "title": "Comparison and analysis of deep audio embeddings for music emotion recognition",
      "authors": [
        "Eunjeong Koh",
        "Shlomo Dubnov"
      ],
      "year": "2021",
      "venue": "Proc. CEUR Workshop"
    },
    {
      "citation_id": "24",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "Siddique Latif",
        "Adnan Qayyum",
        "Muhammad Usman",
        "Junaid Qadir"
      ],
      "year": "2018",
      "venue": "Proc. FIT"
    },
    {
      "citation_id": "25",
      "title": "2023a. Context-aware multimodal fusion for emotion recognition",
      "authors": [
        "Jinchao Li",
        "Shuai Wang",
        "Yang Chao",
        "Xunying Liu",
        "Helen Meng"
      ],
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Exploration of a self-supervised speech model: A study on emotional corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "Proc. SLT"
    },
    {
      "citation_id": "27",
      "title": "Yuanbo Fang, Weibin Zhang, and Hengsheng Fan. 2023b. Multi-scale temporal transformer for speech emotion recognition",
      "authors": [
        "Zhipeng Li",
        "Xiaofen Xing"
      ],
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "28",
      "title": "Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Kang Chen",
        "Mngyu Xu",
        "Kexin Wang",
        "Ke Xu",
        "Yu He",
        "Ying Li",
        "Jinming Zhao"
      ],
      "year": "2023",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "29",
      "title": "RoBERTa: A robustly optimized BERT pretraining approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A robustly optimized BERT pretraining approach"
    },
    {
      "citation_id": "30",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "Proc. PloS One"
    },
    {
      "citation_id": "31",
      "title": "",
      "authors": [
        "Ilya Lubenets",
        "Nikita Davidchuk",
        "Artem Amentes",
        "Aniemore"
      ],
      "venue": ""
    },
    {
      "citation_id": "32",
      "title": "2023a. Leveraging speech PTM, text LLM, and emotional TTS for speech emotion recognition",
      "authors": [
        "Ziyang Ma",
        "Wen Wu",
        "Zhisheng Zheng",
        "Yiwei Guo",
        "Qian Chen",
        "Shiliang Zhang",
        "Xie Chen"
      ],
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "33",
      "title": "2023b. MT4SSL: Boosting selfsupervised speech representation learning by integrating multiple targets",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Changli Tang",
        "Yujin Wang",
        "Xie Chen"
      ],
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "34",
      "title": "2023c. Pushing the limits of unsupervised unit discovery for SSL speech representation",
      "authors": [
        "Ziyang Ma",
        "Zhisheng Zheng",
        "Guanrou Yang",
        "Yu Wang",
        "Chao Zhang",
        "Xie Chen"
      ],
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "35",
      "title": "The MSP-conversation corpus",
      "authors": [
        "Luz Martinez-Lucas",
        "Mohammed Abdelwahab",
        "Carlos Busso"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "36",
      "title": "UMAP: Uniform manifold approximation and projection for dimension reduction",
      "authors": [
        "Leland Mcinnes",
        "John Healy",
        "James Melville"
      ],
      "year": "2018",
      "venue": "UMAP: Uniform manifold approximation and projection for dimension reduction"
    },
    {
      "citation_id": "37",
      "title": "ShEMO: a large-scale validated database for Persian speech emotion detection",
      "authors": [
        "Mohamad Omid",
        "Paria Jamshid Nezami",
        "Mansoureh Lou",
        "Karami"
      ],
      "year": "2019",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "38",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "Edmilson Morais",
        "Ron Hoory",
        "Weizhong Zhu",
        "Itai Gat",
        "Matheus Damasceno",
        "Hagai Aronowitz"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "39",
      "title": "Speech-based emotion recognition and next reaction prediction",
      "authors": [
        "Fatemeh Noroozi",
        "Neda Akrami",
        "Gholamreza Anbarjafari"
      ],
      "year": "2017",
      "venue": "Proc. SIU"
    },
    {
      "citation_id": "40",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "41",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "42",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2023",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "43",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "year": "2019",
      "venue": "Ope-nAI blog"
    },
    {
      "citation_id": "44",
      "title": "A vector quantized masked autoencoder for speech emotion recognition",
      "authors": [
        "Samir Sadok",
        "Simon Leglaive",
        "Renaud Séguier"
      ],
      "year": "2023",
      "venue": "A vector quantized masked autoencoder for speech emotion recognition"
    },
    {
      "citation_id": "45",
      "title": "wav2vec: Unsupervised pretraining for speech recognition",
      "authors": [
        "Steffen Schneider",
        "Alexei Baevski",
        "Ronan Collobert",
        "Michael Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "46",
      "title": "Dimensional emotion prediction based on interactive context in conversation",
      "authors": [
        "Xiaohan Shi",
        "Sixia Li",
        "Jianwu Dang"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "47",
      "title": "Emotion awareness in multi-utterance turn for improving emotion prediction in multi-speaker conversation",
      "authors": [
        "Xiaohan Shi",
        "Xingfeng Li",
        "Tomoki Toda"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "48",
      "title": "SUST Bangla emotional speech corpus (SUBESCO): An audio-only emotional speech corpus for Bangla",
      "authors": [
        "Sadia Sultana",
        "M Shahidur Rahman",
        "Reza Selim",
        "M Zafar"
      ],
      "year": "2021",
      "venue": "Proc. PloS One"
    },
    {
      "citation_id": "49",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "Nikolaos Vryzas",
        "Rigas Kotsakis",
        "Aikaterini Liatsou",
        "Charalampos Dimoulas",
        "George Kalliris"
      ],
      "year": "2018",
      "venue": "Proc. AES"
    },
    {
      "citation_id": "50",
      "title": "Supervision-guided codebooks for masked prediction in speech pre-training",
      "authors": [
        "Chengyi Wang",
        "Yiming Wang",
        "Yu Wu",
        "Sanyuan Chen",
        "Jinyu Li",
        "Shujie Liu",
        "Furu Wei"
      ],
      "year": "2022",
      "venue": "Proc. InterSpeech"
    },
    {
      "citation_id": "51",
      "title": "MEAD: A large-scale audio-visual dataset for emotional talking-face generation",
      "authors": [
        "Kaisiyuan Wang",
        "Qianyi Wu",
        "Linsen Song",
        "Zhuoqian Yang",
        "Wayne Wu",
        "Chen Qian",
        "Ran He",
        "Yu Qiao",
        "Chen Loy"
      ],
      "year": "2020",
      "venue": "Proc. ECCV"
    },
    {
      "citation_id": "52",
      "title": "A fine-tuned wav2vec 2.0/Hu-BERT benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Yingzhi Wang",
        "Abdelmoumene Boumadane",
        "Abdelwahab Heba"
      ],
      "year": "2021",
      "venue": "A fine-tuned wav2vec 2.0/Hu-BERT benchmark for speech emotion recognition, speaker verification and spoken language understanding"
    },
    {
      "citation_id": "53",
      "title": "SUPERB: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "54",
      "title": "Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition",
      "authors": [
        "Jiaxin Ye",
        "Xin-Cheng Wen",
        "Yujie Wei",
        "Yong Xu",
        "Kunhong Liu",
        "Hongming Shan"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "55",
      "title": "MOSI: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "MOSI: Multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos"
    },
    {
      "citation_id": "56",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "57",
      "title": "M3ED: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "Jinming Zhao",
        "Tenggan Zhang",
        "Jingwen Hu",
        "Yuchen Liu",
        "Qin Jin",
        "Xinchao Wang",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Proc. ACL"
    }
  ]
}