{
  "paper_id": "2507.13626v2",
  "title": "Unifying Listener Scoring Scales: Comparison Learning Framework For Speech Quality Assessment And Continuous Speech Emotion Recognition",
  "published": "2025-07-18T03:39:48Z",
  "authors": [
    "Cheng-Hung Hu",
    "Yusuke Yasuda",
    "Akifumi Yoshimoto",
    "Tomoki Toda"
  ],
  "keywords": [
    "speech recognition",
    "speech quality assessment",
    "listener modeling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Quality Assessment (SQA) and Continuous Speech Emotion Recognition (CSER) are two key tasks in speech technology, both relying on listener ratings. However, these ratings are inherently biased due to individual listener factors. Previous approaches have introduced a mean listener scoring scale and modeled all listener scoring scales in the training set. However, the mean listener approach is prone to distortion from averaging ordinal data, leading to potential biases. Moreover, learning multiple listener scoring scales while inferring based only on the mean listener scale limits effectiveness. In contrast, our method focuses on modeling a unified listener scoring scale, using comparison scores to correctly capture the scoring relationships between utterances. Experimental results show that our method effectively improves prediction performance in both SQA and CSER tasks, proving its effectiveness and robustness.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the increasing integration of speech technologies into daily life, Speech Quality Assessment (SQA) and Continuous Speech Emotion Recognition (CSER) have become critical research areas. SQA plays an essential role in various applications such as hearing aids  [1] , speech synthesis systems  [2, 3] , and speech coding systems  [4, 5] , etc. On the other hand, CSER is crucial for spoken dialog systems  [6, 7] , mental health monitoring  [8, 9]  , and emotion analysis  [10] . To automate quality prediction and emotion recognition, models are used and typically trained on labeled data, which often originate from direct assessment scores (DAS) provided by various listeners using scoring scales such as the Likert scale.\n\nHowever, inconsistencies in these ratings emerge due to the diverse nature of listeners. Listener ratings can vary significantly based on cultural background, individual perception, and linguistic characteristics, leading to a lack of uniformity among scores. Each listener has their own scoring scale and may exhibit listener bias when compared to others. This inconsistency poses a significant challenge in building models that accurately represent a global consensus. Consequently, developing a consistent and reliable scoring scale has become a key objective in speech-related research.\n\nIn SQA, existing methods often use listener embeddings to associate corresponding listener's scoring scale for training or inference. To infer the score of an utterance in this scheme, the mean listener approach is usually used to introduce a virtual listener, which assigns each utterance an average score across all listeners for both training and inference  [11, 12, 13, 14] . During training, this virtual listener is included in the dataset, and evaluations during testing rely on this virtual listener's scoring scale. However, learning individual scoring scales for each listener not only complicates the process but also limits the model's effectiveness, especially under a fixed model size, where handling multiple scoring scales adds complexity and hinders learning compared to using a single unified scale. Additionally, because listener ratings are typically represented on an ordinal scale, a prior study  [15]  have noted that averaging ordinal scores is mathematically meaningless because ordinal data only represents order, not precise numerical differences, and averages can be misleading due to the arbitrary nature of the numerical values assigned. Using average ordinal scores for training can introduce biases to the model, ultimately undermining the model's ability to generalize effectively.\n\nTo address the challenges of averaging ordinal scores and eliminate the need to learn across multiple listener scoring scales, we propose an improvement to the method introduced in UTP  [16] . UTP explored converting DAS into comparison scores (referred to as \"preference scores\" in their paper) for model training, aiming to enhance prediction accuracy by explicitly providing reference utterances for comparison. However, their approach still relies on the mean listener approach and listener embeddings, since the DAS prediction model (UT-MOS  [12] ) is used as a basic component of the UTP framework.\n\nIn this paper, we propose a method eliminating the use of listener embeddings during both training and testing. We believe that, unlike DAS, which is influenced by listener biases and cannot be directly merged across different scoring scales, comparison scores-leveraging the ordinal nature of score data-can generalize effectively on a global unified listener scoring scale. By fully utilizing all comparison scores within a unified scoring scale, our approach can further improve model performance. Furthermore, we analyze the conditions under which using the mean score for model training is effective by examining the datasets used in our experiments, particularly in terms of the distribution of unique listener sets evaluating each utterance.\n\nTo validate the effectiveness of our proposed method, we conduct experiments on both SQA and CSER tasks: • SQA Task: We select UTMOS and UTP as baseline models. Our experiments include configurations with and without listener-related approaches. Notably, the UTP model improves performance when trained on a unified listener scoring scale, highlighting the effectiveness of our approach.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "Speech Emotion Recognition (SER) can generally be divided into two categories: discrete SER (DSER) and continuous SER (CSER). In DSER, the task is typically focused on classifying emotions into predefined categories, such as happy, sad, angry, etc.  [18]  introduced the concept that each listener has a different assessing standard. They proposed an \"all listener\" method, which learns the scoring scales of all listeners and aggregates their individual predictions through a voting mechanism to infer the final result. In CSER, there has been relatively little focus on incorporating listener embeddings. However, several studies have explored different methods to enhance performance. For example,  [19]  proposed a multi-task learning framework that jointly predicts emotional attributes, such as arousal, valence, speaker ID, and gender. Additionally,  [17]  investigates the use of speech-based SSL features for CSER, achieving state-of-theart results in valence domain on the IEMOCAP dataset without linguistic input. Our experiments are based on the model from  [17] , with an extension to incorporate text information in an effort to improve prediction performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Speech Quality Assessment",
      "text": "LDNet  [11]  employs two testing methods: the mean listener approach and the all listener approach. The mean listener approach augments the training dataset by introducing a virtual listener, but it complicates the learning process and may obscure true scoring patterns. Additionally, directly averaging scores can introduce biases, as ratings on Likert scales are ordinal and not suited for simple averaging. The all listener approach, on the other hand, uses embeddings for all listeners during testing and averages their predictions. However, this method heavily depends on the distribution of listeners in the training set and suffers from the same limitations of averaging scores after inferring them using each listener's scoring scale. On the other hand, MBNet  [20]  addresses listener variability by combining two components: a MeanNet, designed to predict the average scores, and a BiasNet, which accounts for listener-specific biases. While this architecture explicitly models individual differences, it inherits the fundamental problem of averaging listener scores, which, as noted earlier, is considered meaningless for ordinal data. The UTMOS system  [12]  employs the mean listener approach, learns every listener's scoring scales and demonstrated top performance in the VoiceMOS Challenge 2022. Building on this foundation, the UTP model leverages a comparison learning framework to model listener ratings effectively. Due to its robust performance and relevance, the UTP model serves as the baseline for our proposed approach.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Das Prediction Model",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Quality Assessment",
      "text": "As shown in Fig.  1 , UTMOS  [12]  incorporates five inputs: the SSL feature, the data-domain ID, the phoneme sequence, the reference sequence, and the listener ID. The SSL feature is extracted from a pretrained wav2vec2 model  [21] . The phoneme sequence is recognized using a pretrained ASR model  [22]  and subsequently clustered using the DBSCAN algorithm  [23]  to generate the reference sequence. These inputs are concatenated and fed into a BLSTM layer followed by linear layers to produce frame-wise scores, which are then averaged to obtain the utterance-level score. UTMOS calculates its loss using a combination of contrastive loss and clipped MSE loss. During testing, we follow the UTP paper, which focuses on predicting systemlevel quality scores. As a result, we average all scores from the same system to obtain the system-level quality score.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Continuous Speech Emotional Recognition",
      "text": "As shown in Fig.  2 , the CSER model was originally designed to take SSL features as input. Inspired by the top-performing model in  [17] , we adopt wav2vec-robust  [24]  as the SSL model for feature extraction. To enhance the prediction of arousal and valence, we incorporate textual information into the model. Specifically, we use Whisper  [25]  to transcribe audio recordings into text. The transcriptions are then converted into embeddings using BERT  [26] , which are subsequently processed by an BLSTM to generate utterance-level embeddings. Additionally, we introduce listener embeddings as utterance-level inputs during training. All utterance-level embeddings, including those derived from SSL features, textual information, and listener representations, are concatenated and fed into two dense layers. The final output consists of two nodes corresponding to arousal and valence. The model is trained with the concordance correlation coefficient loss.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Effect Of Listener Embeddings On The Scoring Scale",
      "text": "Incorporating listener embeddings into the model as input ensures that training and evaluation are conducted using each listener's respective scoring scale. Conversely, omitting listener embeddings results in training and testing on a unified listener scoring scale.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Comparison Learning",
      "text": "UTP  [16]  is a model that incorporates comparison learning (CL) into DAS prediction models, as illustrated in Fig.  3 . UTP was originally designed for SQA tasks to predict the quality scores of voice conversion systems. To generalize the framework for the CSER task, we replace the concept of \"preference\" in the original paper with \"comparison\" to better align with the task's objective. The UTP model consists of four key components: pair generation, comparison function, threshold selection, and preferential aggregation. During training for both SQA and CSER tasks, the model utilizes the pair generation and comparison function components. The pair generation module constructs pairs of utterances that have been evaluated by the same listener. Notably, even when the model is not trained with listener embeddings as input, the pair generation module still leverages listener information to ensure that utterance pairs are assessed by the same listener. The comparison function then computes a derived comparison score Comp dv for each pair, defined as Comp dv = α(sc1, sc2) = 2 • sigmoid(sc1 -sc2) -1, where sc1 and sc2 represent the pre-  dicted attribute scores of the two input utterances, respectively. The ground-truth comparison score is defined as Comp gt = sgn(scgt1 -scgt2), where scgt1 and scgt2 are the ground-truth scores for the two input utterances. sgn is the sign function. The training loss is computed using the mean squared error (MSE) between the predicted and ground-truth comparison scores: LX = MSE(Comp dv , Comp gt ), where X represents the type of the target attribute to be predicted. For SQA, the goal is to predict quality, so the loss is L = L quality . For CSER, the aim is to predict both arousal and valence, and the loss is L = (L arousal + L valence )/2. Importantly, the training process is entirely driven by the comparison scores and does not involve DAS explicitly.\n\nDuring testing, the UTP model supports two evaluation modes: (1) DAS evaluation mode: after training, we detach the DAS model from the UTP framework and use it directly for DAS prediction. Notably, this evaluation does not rely on the four components of the UTP framework. We use this mode for the CSER task to predict arousal and valence for each utterance.\n\n(2) Comparison score evaluation mode: This evaluation mode applies all four components: pair generation to create utterance pairs for comparison, the comparison function to compute comparison scores, threshold selection to filter out pairs with small quality differences, and preferential aggregation to integrate the valid comparison scores into system scores. We use this mode for the SQA task, as the original UTP paper focused on score prediction at the system level. We follow the UTP paper, using Balanced System Pair Selection for pair generation, ensuring that pairs are balanced across all system pairs. For threshold selection, we apply No Draw, which does not filter out preference scores but uses their sign to determine the comparison direction. Lastly, we use Differential Count for comparison aggregation, calculating system scores based on the difference between the win count and the loss count.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mean Listener Approach",
      "text": "A virtual listener is introduced and added to the dataset, assigning the average score across all listeners. This creates an additional scoring scale in the dataset, representing the mean listener. If listener embedding is incorporated, the model is trained on the dataset augmented with this virtual listener. During inference, the mean listener's scoring scale is used for score prediction. If listener embedding is not used, the model is trained on a unified listener scoring scale where the dataset is augmented with average scores. During inference, this unified listener scoring scale is used for score prediction. Specifically, for the pair generation method in the CL model, the mean listener is treated as an individual listener, and utterance pairs can also be generated from the mean listener for model training. We followed the setting of UTP and used BVCC  [27]  for the experiment. In the training set, there are 4,973 unique utterances, each evaluated 8 times, yielding a total of 39,784 utterancescore pairs. The set comprises 175 systems, and a total of 288 listeners participated in the evaluation. The development set contains 1,066 unique utterances, each also evaluated 8 times, leading to a total of 8,528 utterance-score pairs. This set includes 181 systems, and 296 listeners participated in the evaluation, with each listener scoring between 16 and 177 utterances. For the test set, there are 1,066 unique utterances, each assigned an average quality score. The dataset consisted of 187 systems, each containing 1 to 38 unique utterances. It is worth noting that in the design of the listening test, the 288 listeners in the training set were divided into 36 unique groups of 8 listeners, with no overlap between groups. Each group evaluated between 126 and 152 utterances. As a result, using the mean listener approach, each utterance received a mean score, averaged over 8 ratings, resulting in 36 actually different mean listener score scales combined as the overall mean listener scale.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iemocap",
      "text": "We used IEMOCAP  [28]  dataset for CSER experiment. IEMO-CAP consists of 5 sessions, each containing 2 speakers. The dataset includes a total of 10,039 unique utterances and 23,887 scores. A total of 12 listeners were gathered to evaluate the arousal, valence, and dominance of each utterance. Each utterance was evaluated by 2 to 4 listeners. We found that 29 unique listener sets, with significant listener overlap, evaluated each of the 10,039 utterances. This resulted in 29 different mean listener score scales, which were combined to form the overall mean listener scale when the mean listener approach was applied. Among all the listener sets, the listener set {A-E1,A-E2} evaluated 4,941 utterances, covering approximately half utterances of the full dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiment Details",
      "text": "In training DAS prediction models, we used the same settings as the original model, including the optimizer, scheduler, batch size, and other hyperparameters. For the CL model, we followed the same settings as the DAS prediction models, except that we used the MSE loss. To evaluate model performance, we employed Spearman's rank correlation coefficient (SRCC) and the linear correlation coefficient (LCC). For SQA, we used SRCC and LCC to measure the correlation between predicted scores and ground-truth scores at the system level. For CSER, we used SRCC and LCC to measure the correlation between predicted scores and ground-truth scores at the utterance level. In the SQA experiment, we ran the experiment 20 times and reported the average performance in the table. In the CSER experiment, we followed Chen et al.  [29]  and adopted a 5-fold cross-validation approach, where each IEMOCAP session was held out as the test set. We then reported the average performance over the 5-fold cross-validation results. when the mean listener approach was not applied and listener embeddings were not used. However, when the mean listener approach was introduced, the performance improved, and it further increased when listener embeddings were incorporated. On the other hand, for the UTP model, we observed an inverse trend. The model performed the worst when listener embeddings were used, and its performance further degraded when the mean listener approach was not applied. This suggests that, unlike the UTMOS model, the UTP model does not benefit from either the mean listener approach or the incorporation of listener embeddings, which decreases the model's predictive capability.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Main Experiment Results On Sqa",
      "text": "When comparing the model performance between UTMOS and UTP, we found that UTP always outperformed UTMOS model using every setting. This suggests that comparison scores help modeling scoring scales for predicting true attribute scores.\n\nFrom these results, we conclude that for the SQA task, incorporating these methods allows UTMOS to better learn multiple listeners' scoring scales while simultaneously improving its performance. In contrast, for the UTP model, learning on a unified listener scoring scale without the use of listener embeddings improves model performance. Furthermore, learning from pairs with comparison scores generated from mean scores of 36 different listener sets degrades performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Main Experiment Results On Cser",
      "text": "The experiment results on CSER are shown in Table  2 . We first examined the effect of incorporating text information in our models. In arousal prediction, we observed that the DAS model exhibited a decline in performance, whereas the CL model remained stable or improved slightly. We believe this is due to the fact that the networks are jointly trained for arousal and valence values, leading to mutual influence between the two predictions. Compared to the more stable comparison scores, the inherently more unstable DAS scores hinder the model's ability to effectively improve arousal prediction performance. For valence prediction, since the positive or negative aspects of speech are correlated with textual information, both DAS and CL models achieved significant improvements in SRCC and LCC. This indicates that textual features effectively enhance the model's ability to predict valence.\n\nNext, we compared the effect of incorporating the mean listener approach in our models. In both DAS and CL models, we observed that using mean listener either maintained or improved performance. For the DAS model, this aligns with the findings in the SQA task. However, for the CL model, the results contradict the SQA task findings. We hypothesize that this discrepancy is due to the differences in dataset properties, specifically:\n\n(1) In IEMOCAP, there are relatively few listeners--12 listeners appear across 29 listener sets, resulting in a high degree of overlap. As a result, different sets are influenced by the same listeners to varying extents, making the comparison scores more As a result, in IEMOCAP, the utterance pairs obtained using the mean listener method are more likely to be assessed from the same listener set, making their comparison more reliable. This contributes to the differences observed in the CL models' performance across datasets. Next, we investigate the impact of listener embedding on model performance. For the DAS model, we find that listener embedding generally improves performance, except in valence prediction, where using only the mean listener approach outperforms the version trained with both the mean listener and listener embedding. This observation is consistent with findings from the SQA task. However, in the CL model, as in the SQA task, incorporating listener embedding leads to a decline in performance. We hypothesize that this occurs because learning the comparative relationships between utterances on a unified listener scoring scale enables the model to better capture the true scores.\n\nFinally, when comparing the model performance between DAS prediction model and CL model, we also observe that CL model always outperformed DAS prediction model using every setting as on SQA task, suggesting that comparison scores help modeling scoring scales for predicting true attribute scores.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we explored listener scoring scale modeling for the SQA and CSER tasks. Specifically, we proposed a method that enables the CL model to learn on a unified listener scoring scale through comparison scores and extend the use of CL model from SQA task to CSER task. Our experimental results demonstrated that the performance of the DAS model improves when applying both the mean listener approach and listener embedding. In contrast, the effectiveness of the mean listener approach in the CL model depends on the distribution of mean listener scales. Furthermore, the experiment results showed that employing comparison scores without listener embedding allows the model to learn on a unified listener scoring scale, leading to performance improvements and achieving the best performance among all models. For future work, we suggest investigating the feasibility of establishing a unified listener scoring scale across different dataset domains.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , UTMOS [12] incorporates five inputs: the",
      "page": 2
    },
    {
      "caption": "Figure 1: DAS prediction (UTMOS) model for SQA task.",
      "page": 2
    },
    {
      "caption": "Figure 2: DAS prediction model for CSER task.",
      "page": 2
    },
    {
      "caption": "Figure 2: , the CSER model was originally designed",
      "page": 2
    },
    {
      "caption": "Figure 3: The UTP framework: Line styles indicate training conditions, shapes indicate testing conditions for each module.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": ""
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "cantly based on cultural background, individual perception, and"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": ""
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "linguistic characteristics, leading to a lack of uniformity among"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": ""
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "scores. Each listener has their own scoring scale and may ex-"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": ""
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "hibit listener bias when compared to others. This inconsistency"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "poses a significant challenge in building models that accurately"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "represent a global consensus. Consequently, developing a con-"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "sistent and reliable scoring scale has become a key objective in"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "speech-related research."
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "In SQA, existing methods often use listener embeddings to"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "associate corresponding listener’s scoring scale for training or"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "inference.\nTo infer\nthe score of an utterance in this scheme,"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "the mean listener approach is usually used to introduce a virtual"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "listener, which assigns each utterance an average score across"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "all\nlisteners\nfor both training and inference [11, 12, 13, 14]."
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "During training,\nthis virtual\nlistener is included in the dataset,"
        },
        {
          "diverse nature of\nlisteners.\nListener\nratings can vary signifi-": "and evaluations during testing rely on this virtual\nlistener’s"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "Content and Media Sciences Research Division, National Institute of Informatics, Japan"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "tomoki@icts.nagoya-u.ac.jp"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "Abstract"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "Speech Quality Assessment\n(SQA)\nand Continuous Speech"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "Emotion Recognition (CSER) are two key tasks in speech tech-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "nology, both relying on listener ratings. However, these ratings"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "are inherently biased due to individual listener factors. Previous"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "approaches have introduced a mean listener scoring scale and"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "modeled all listener scoring scales in the training set. However,"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "the mean listener approach is prone to distortion from averag-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "ing ordinal data,\nleading to potential biases. Moreover,\nlearn-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "ing multiple listener scoring scales while inferring based only"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "on the mean listener scale limits effectiveness.\nIn contrast, our"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "method focuses on modeling a unified listener\nscoring scale,"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "using comparison scores to correctly capture the scoring rela-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "tionships between utterances.\nExperimental\nresults show that"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "our method effectively improves prediction performance in both"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "SQA and CSER tasks, proving its effectiveness and robustness."
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "Index Terms:\nspeech recognition, speech quality assessment,"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "listener modeling"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "1.\nIntroduction"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "With\nthe\nincreasing\nintegration\nof\nspeech\ntechnologies\ninto"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "daily life, Speech Quality Assessment\n(SQA) and Continuous"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "Speech Emotion Recognition (CSER) have become critical re-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "search areas.\nSQA plays an essential\nrole in various applica-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "tions such as hearing aids [1], speech synthesis systems [2, 3],"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "and speech coding systems [4, 5], etc. On the other hand, CSER"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "is crucial for spoken dialog systems [6, 7], mental health mon-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "itoring [8, 9] , and emotion analysis [10]. To automate quality"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "prediction and emotion recognition, models are used and typi-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "cally trained on labeled data, which often originate from direct"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "assessment\nscores\n(DAS) provided by various\nlisteners using"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "scoring scales such as the Likert scale."
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "However, inconsistencies in these ratings emerge due to the"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "diverse nature of\nlisteners.\nListener\nratings can vary signifi-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "cantly based on cultural background, individual perception, and"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "linguistic characteristics, leading to a lack of uniformity among"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "scores. Each listener has their own scoring scale and may ex-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": ""
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "hibit listener bias when compared to others. This inconsistency"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "poses a significant challenge in building models that accurately"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "represent a global consensus. Consequently, developing a con-"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "sistent and reliable scoring scale has become a key objective in"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "speech-related research."
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "In SQA, existing methods often use listener embeddings to"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "associate corresponding listener’s scoring scale for training or"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "inference.\nTo infer\nthe score of an utterance in this scheme,"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "the mean listener approach is usually used to introduce a virtual"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "listener, which assigns each utterance an average score across"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "all\nlisteners\nfor both training and inference [11, 12, 13, 14]."
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "During training,\nthis virtual\nlistener is included in the dataset,"
        },
        {
          "1Department of Intelligent Systems, Nagoya University, Japan; 2CyberAgent, Japan; 3Digital": "and evaluations during testing rely on this virtual\nlistener’s"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "Domain ID"
        },
        {
          "SSL feature": "Text Sequence\nUTMOS\nQuality Score"
        },
        {
          "SSL feature": "Reference Sequence"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "Listener ID"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "Figure 1: DAS prediction (UTMOS) model for SQA task."
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "SSL feature\nSSL model"
        },
        {
          "SSL feature": "Arousal"
        },
        {
          "SSL feature": "Text Sequence\nBERT\nBLSTM\nDense * 2"
        },
        {
          "SSL feature": "Valence\nListener ID"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "Figure 2: DAS prediction model for CSER task."
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "utterance-level score. UTMOS calculates its loss using a combi-"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "nation of contrastive loss and clipped MSE loss. During testing,"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "we follow the UTP paper, which focuses on predicting system-"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "level quality scores. As a result, we average all scores from the"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "same system to obtain the system-level quality score."
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "3.1.2. Continuous Speech Emotional Recognition"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "As shown in Fig. 2,\nthe CSER model was originally designed"
        },
        {
          "SSL feature": "to take SSL features as input.\nInspired by the top-performing"
        },
        {
          "SSL feature": "model in [17], we adopt wav2vec-robust [24] as the SSL model"
        },
        {
          "SSL feature": "for\nfeature extraction.\nTo enhance the prediction of arousal"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "and valence, we incorporate textual information into the model."
        },
        {
          "SSL feature": "Specifically, we use Whisper\n[25]\nto transcribe audio record-"
        },
        {
          "SSL feature": "ings into text. The transcriptions are then converted into em-"
        },
        {
          "SSL feature": "beddings using BERT [26], which are subsequently processed"
        },
        {
          "SSL feature": "by an BLSTM to generate utterance-level embeddings. Addi-"
        },
        {
          "SSL feature": "tionally, we introduce listener embeddings as utterance-level in-"
        },
        {
          "SSL feature": "puts during training. All utterance-level embeddings, including"
        },
        {
          "SSL feature": "those derived from SSL features,\ntextual\ninformation, and lis-"
        },
        {
          "SSL feature": "tener representations, are concatenated and fed into two dense"
        },
        {
          "SSL feature": "layers. The final output consists of two nodes corresponding to"
        },
        {
          "SSL feature": "arousal and valence. The model is trained with the concordance"
        },
        {
          "SSL feature": "correlation coefficient loss."
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "3.1.3. Effect of Listener Embeddings on the Scoring Scale"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "Incorporating listener embeddings into the model as input en-"
        },
        {
          "SSL feature": "sures that training and evaluation are conducted using each lis-"
        },
        {
          "SSL feature": "tener’s respective scoring scale. Conversely, omitting listener"
        },
        {
          "SSL feature": "embeddings results in training and testing on a unified listener"
        },
        {
          "SSL feature": "scoring scale."
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "3.2. Comparison Learning"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "UTP [16] is a model that incorporates comparison learning (CL)"
        },
        {
          "SSL feature": "into DAS prediction models,\nas\nillustrated in Fig.\n3.\nUTP"
        },
        {
          "SSL feature": "was originally designed for SQA tasks\nto predict\nthe quality"
        },
        {
          "SSL feature": "scores of voice conversion systems. To generalize the frame-"
        },
        {
          "SSL feature": "work for\nthe CSER task, we replace the concept of “prefer-"
        },
        {
          "SSL feature": "ence” in the original paper with “comparison” to better align"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "with the\ntask’s objective.\nThe UTP model\nconsists of\nfour"
        },
        {
          "SSL feature": "key components: pair generation, comparison function, thresh-"
        },
        {
          "SSL feature": "old selection, and preferential aggregation. During training for"
        },
        {
          "SSL feature": ""
        },
        {
          "SSL feature": "both SQA and CSER tasks,\nthe model utilizes the pair gener-"
        },
        {
          "SSL feature": "ation and comparison function components. The pair genera-"
        },
        {
          "SSL feature": "tion module constructs pairs of utterances that have been eval-"
        },
        {
          "SSL feature": "uated by the same listener. Notably, even when the model\nis"
        },
        {
          "SSL feature": "not\ntrained with listener embeddings as input,\nthe pair genera-"
        },
        {
          "SSL feature": "tion module still\nleverages listener\ninformation to ensure that"
        },
        {
          "SSL feature": "utterance pairs are assessed by the same listener.\nThe com-"
        },
        {
          "SSL feature": "parison function then computes\na derived comparison score"
        },
        {
          "SSL feature": "Compdv\nfor each pair, defined as Compdv = α(sc1, sc2) ="
        },
        {
          "SSL feature": "2 · sigmoid(sc1 − sc2) − 1, where sc1 and sc2 represent the pre-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "SQA/CSER\nAttribute": "model\nScore",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "Function\nScores\nSelection\nAggregation\nScores"
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "Figure 3: The UTP framework: Line styles indicate training conditions, shapes indicate testing conditions for each module."
        },
        {
          "SQA/CSER\nAttribute": "dicted attribute scores of the two input utterances, respectively.",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "each evaluated 8 times, yielding a total of 39,784 utterance-"
        },
        {
          "SQA/CSER\nAttribute": "The ground-truth comparison score is defined as Compgt =",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "score pairs. The set comprises 175 systems, and a total of 288"
        },
        {
          "SQA/CSER\nAttribute": "are the ground-truth\nsgn(scgt1 − scgt2), where scgt1\nand scgt2",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "listeners participated in the evaluation.\nThe development\nset"
        },
        {
          "SQA/CSER\nAttribute": "scores\nfor\nthe\ntwo input utterances.\nsgn is\nthe\nsign func-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "contains 1,066 unique utterances, each also evaluated 8 times,"
        },
        {
          "SQA/CSER\nAttribute": "tion. The training loss is computed using the mean squared er-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "leading to a total of 8,528 utterance-score pairs.\nThis set\nin-"
        },
        {
          "SQA/CSER\nAttribute": "ror (MSE) between the predicted and ground-truth comparison",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "cludes 181 systems, and 296 listeners participated in the evalu-"
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "ation, with each listener scoring between 16 and 177 utterances."
        },
        {
          "SQA/CSER\nAttribute": "scores: LX = MSE(Compdv, Compgt), where X represents the",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "type of the target attribute to be predicted. For SQA,\nthe goal",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "For the test set, there are 1,066 unique utterances, each assigned"
        },
        {
          "SQA/CSER\nAttribute": "For CSER,\nis to predict quality, so the loss is L = Lquality.",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "an average quality score. The dataset consisted of 187 systems,"
        },
        {
          "SQA/CSER\nAttribute": "the aim is to predict both arousal and valence, and the loss is",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "each containing 1 to 38 unique utterances.\nIt\nis worth noting"
        },
        {
          "SQA/CSER\nAttribute": "Importantly,\nthe training pro-\nL = (Larousal + Lvalence)/2.",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "that\nin the design of the listening test,\nthe 288 listeners in the"
        },
        {
          "SQA/CSER\nAttribute": "cess is entirely driven by the comparison scores and does not",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "training set were divided into 36 unique groups of 8 listeners,"
        },
        {
          "SQA/CSER\nAttribute": "involve DAS explicitly.",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "with no overlap between groups. Each group evaluated between"
        },
        {
          "SQA/CSER\nAttribute": "During testing,\nthe UTP model\nsupports\ntwo evaluation",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "126 and 152 utterances. As a result, using the mean listener ap-"
        },
        {
          "SQA/CSER\nAttribute": "modes: (1) DAS evaluation mode: after training, we detach the",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "proach, each utterance received a mean score, averaged over"
        },
        {
          "SQA/CSER\nAttribute": "DAS model\nfrom the UTP framework and use it directly for",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "8 ratings, resulting in 36 actually different mean listener score"
        },
        {
          "SQA/CSER\nAttribute": "DAS prediction. Notably,\nthis evaluation does not rely on the",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "scales combined as the overall mean listener scale."
        },
        {
          "SQA/CSER\nAttribute": "four components of the UTP framework. We use this mode for",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "the CSER task to predict arousal and valence for each utterance.",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "4.1.2.\nIEMOCAP"
        },
        {
          "SQA/CSER\nAttribute": "(2) Comparison score evaluation mode: This evaluation mode",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "We used IEMOCAP [28] dataset for CSER experiment. IEMO-"
        },
        {
          "SQA/CSER\nAttribute": "applies all four components: pair generation to create utterance",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "CAP consists of 5 sessions, each containing 2 speakers.\nThe"
        },
        {
          "SQA/CSER\nAttribute": "pairs for comparison, the comparison function to compute com-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "dataset includes a total of 10,039 unique utterances and 23,887"
        },
        {
          "SQA/CSER\nAttribute": "parison scores,\nthreshold selection to filter out pairs with small",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "scores.\nA total of 12 listeners were gathered to evaluate the"
        },
        {
          "SQA/CSER\nAttribute": "quality differences, and preferential aggregation to integrate the",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "arousal, valence, and dominance of each utterance. Each utter-"
        },
        {
          "SQA/CSER\nAttribute": "valid comparison scores into system scores. We use this mode",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "ance was evaluated by 2 to 4 listeners. We found that 29 unique"
        },
        {
          "SQA/CSER\nAttribute": "for the SQA task, as the original UTP paper focused on score",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "listener sets, with significant listener overlap, evaluated each of"
        },
        {
          "SQA/CSER\nAttribute": "prediction at the system level. We follow the UTP paper, using",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "the 10,039 utterances.\nThis resulted in 29 different mean lis-"
        },
        {
          "SQA/CSER\nAttribute": "Balanced System Pair Selection for pair generation, ensuring",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "tener score scales, which were combined to form the overall"
        },
        {
          "SQA/CSER\nAttribute": "that pairs are balanced across all system pairs. For threshold se-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "mean listener scale when the mean listener approach was ap-"
        },
        {
          "SQA/CSER\nAttribute": "lection, we apply No Draw, which does not filter out preference",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "plied. Among all the listener sets, the listener set {A-E1,A-E2}"
        },
        {
          "SQA/CSER\nAttribute": "scores but uses their sign to determine the comparison direction.",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "evaluated 4,941 utterances, covering approximately half utter-"
        },
        {
          "SQA/CSER\nAttribute": "Lastly, we use Differential Count for comparison aggregation,",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "ances of the full dataset."
        },
        {
          "SQA/CSER\nAttribute": "calculating system scores based on the difference between the",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "win count and the loss count.",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "4.2. Experiment Details"
        },
        {
          "SQA/CSER\nAttribute": "3.3. Mean Listener Approach",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "In training DAS prediction models, we used the same settings"
        },
        {
          "SQA/CSER\nAttribute": "A virtual listener is introduced and added to the dataset, assign-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "as the original model, including the optimizer, scheduler, batch"
        },
        {
          "SQA/CSER\nAttribute": "ing the average score across all\nlisteners. This creates an ad-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "size, and other hyperparameters.\nFor\nthe CL model, we fol-"
        },
        {
          "SQA/CSER\nAttribute": "ditional scoring scale in the dataset, representing the mean lis-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "lowed the same settings as the DAS prediction models, except"
        },
        {
          "SQA/CSER\nAttribute": "tener. If listener embedding is incorporated, the model is trained",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "that we used the MSE loss.\nTo evaluate model performance,"
        },
        {
          "SQA/CSER\nAttribute": "on the dataset augmented with this virtual listener. During infer-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "we employed Spearman’s rank correlation coefficient\n(SRCC)"
        },
        {
          "SQA/CSER\nAttribute": "ence, the mean listener’s scoring scale is used for score predic-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "and the linear correlation coefficient (LCC). For SQA, we used"
        },
        {
          "SQA/CSER\nAttribute": "tion.\nIf listener embedding is not used,\nthe model\nis trained on",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "SRCC and LCC to measure the correlation between predicted"
        },
        {
          "SQA/CSER\nAttribute": "a unified listener scoring scale where the dataset\nis augmented",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "scores and ground-truth scores at\nthe system level. For CSER,"
        },
        {
          "SQA/CSER\nAttribute": "with average scores. During inference, this unified listener scor-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "we used SRCC and LCC to measure the correlation between"
        },
        {
          "SQA/CSER\nAttribute": "ing scale is used for score prediction. Specifically, for the pair",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "predicted scores and ground-truth scores at\nthe utterance level."
        },
        {
          "SQA/CSER\nAttribute": "generation method in the CL model, the mean listener is treated",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "In the SQA experiment, we ran the experiment 20 times and"
        },
        {
          "SQA/CSER\nAttribute": "as an individual\nlistener, and utterance pairs can also be gener-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "reported the average performance in the table. In the CSER ex-"
        },
        {
          "SQA/CSER\nAttribute": "ated from the mean listener for model training.",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "periment, we followed Chen et al.\n[29] and adopted a 5-fold"
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "cross-validation approach, where each IEMOCAP session was"
        },
        {
          "SQA/CSER\nAttribute": "4. Experiment",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "held out as the test set. We then reported the average perfor-"
        },
        {
          "SQA/CSER\nAttribute": "",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "mance over the 5-fold cross-validation results."
        },
        {
          "SQA/CSER\nAttribute": "4.1. Datasets",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": ""
        },
        {
          "SQA/CSER\nAttribute": "4.1.1. BVCC",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "4.3. Main Experiment Results on SQA"
        },
        {
          "SQA/CSER\nAttribute": "We followed the setting of UTP and used BVCC [27] for the ex-",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "Table 1 presents the experiment results on SQA. We observed"
        },
        {
          "SQA/CSER\nAttribute": "periment.\nIn the training set, there are 4,973 unique utterances,",
          "Comparison\nComparison\nThreshold\nComparison\nSystem": "that\nfor\nthe UTMOS model,\nthe model performed the worst"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: Arousal and valenceprediction results for the CSER",
      "data": [
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "task. The bolded cells indicate the best performance among all",
          "Table 2: Arousal and valence prediction results for the CSER": "task across all configurations of the DAS prediction model and"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "configurations of UTMOS and UTP.",
          "Table 2: Arousal and valence prediction results for the CSER": "CL model.\nThe bolded cells\nindicate\nthe best performance"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "",
          "Table 2: Arousal and valence prediction results for the CSER": "among all configurations of DAS prediction models and CL"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "Mean\nListener",
          "Table 2: Arousal and valence prediction results for the CSER": ""
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "Model\nSRCC\nLCC",
          "Table 2: Arousal and valence prediction results for the CSER": "models."
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "Listener\nEmbedding",
          "Table 2: Arousal and valence prediction results for the CSER": ""
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "0.9291\n0.9317",
          "Table 2: Arousal and valence prediction results for the CSER": "Mean\nListener\nArousal\nValence"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "✓",
          "Table 2: Arousal and valence prediction results for the CSER": "Model\nText"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "UTMOS\n0.9296\n0.9322",
          "Table 2: Arousal and valence prediction results for the CSER": "Listener\nSRCC\nLCC\nSRCC\nLCC\nEmbedding"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "✓\n✓\n0.9316\n0.9350",
          "Table 2: Arousal and valence prediction results for the CSER": ""
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "",
          "Table 2: Arousal and valence prediction results for the CSER": "0.6983\n0.7029\n0.6034\n0.6168"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "0.9420\n0.9376",
          "Table 2: Arousal and valence prediction results for the CSER": "✓\n0.6725\n0.6858\n0.6645\n0.6722"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "✓\n0.9376\nUTP\n0.9410",
          "Table 2: Arousal and valence prediction results for the CSER": "✓"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "",
          "Table 2: Arousal and valence prediction results for the CSER": "0.6981\n0.7043\n0.6116\n0.6210"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "✓\n✓",
          "Table 2: Arousal and valence prediction results for the CSER": "DAS"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "0.9408\n0.9370",
          "Table 2: Arousal and valence prediction results for the CSER": "✓\n✓\n0.6797\n0.6826\n0.6636\n0.6726"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "",
          "Table 2: Arousal and valence prediction results for the CSER": "✓\n✓\n0.7099\n0.7139\n0.5986\n0.6174"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "",
          "Table 2: Arousal and valence prediction results for the CSER": "✓\n✓\n✓\n0.6760\n0.6891\n0.6847\n0.6907"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "when the mean listener approach was not applied and listener",
          "Table 2: Arousal and valence prediction results for the CSER": "0.7375\n0.7480\n0.7240\n0.7470"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "",
          "Table 2: Arousal and valence prediction results for the CSER": "✓"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "embeddings were not used. However, when the mean listener",
          "Table 2: Arousal and valence prediction results for the CSER": "0.7362\n0.7435\n0.7878\n0.7956"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "",
          "Table 2: Arousal and valence prediction results for the CSER": "✓\n0.7505\n0.7577\n0.7418\n0.7602"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "approach was introduced, the performance improved, and it fur-",
          "Table 2: Arousal and valence prediction results for the CSER": "CL"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "",
          "Table 2: Arousal and valence prediction results for the CSER": "✓\n✓\n0.7703\n0.7774\n0.8004\n0.8023"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "ther increased when listener embeddings were incorporated. On",
          "Table 2: Arousal and valence prediction results for the CSER": "✓\n✓\n0.7396\n0.7492\n0.7347\n0.7580"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "",
          "Table 2: Arousal and valence prediction results for the CSER": "✓\n✓\n✓"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "the other hand,\nfor\nthe UTP model, we observed an inverse",
          "Table 2: Arousal and valence prediction results for the CSER": "0.7586\n0.7652\n0.7956\n0.7989"
        },
        {
          "Table 1: Experimental results of UTMOS and UTP on the SQA": "trend.\nThe model performed the worst when listener embed-",
          "Table 2: Arousal and valence prediction results for the CSER": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "framework for automatic speech quality assessment using deep"
        },
        {
          "6. Acknowledgement": "This work was partly supported by JST AIP Acceleration Re-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "neural network,” in INTERSPEECH 2023, 2023, pp. 546–550."
        },
        {
          "6. Acknowledgement": "search JPMJCR25U5, Japan.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[17]\nJ. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt,"
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "F. Burkhardt, F. Eyben, and B. W. Schuller, “Dawn of the trans-"
        },
        {
          "6. Acknowledgement": "7. References",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "former era in speech emotion recognition:\nclosing the valence"
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "gap,” IEEE Transactions on Pattern Analysis and Machine Intel-"
        },
        {
          "6. Acknowledgement": "[1] A. Hines and N. Harte, “Speech intelligibility prediction using",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "ligence, vol. 45, no. 9, pp. 10 745–10 759, 2023."
        },
        {
          "6. Acknowledgement": "a neurogram similarity index measure,” Speech Communication,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "vol. 54, no. 2, pp. 306–320, 2012.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[18] A. Ando, T. Mori, S. Kobashikawa, and T. Toda, “Speech emotion"
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "recognition based on listener-dependent emotion perception mod-"
        },
        {
          "6. Acknowledgement": "[2]\nJ. Kim, S. Kim, J. Kong, and S. Yoon, “Glow-tts: A generative",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "els,” APSIPA Transactions on Signal and Information Processing,"
        },
        {
          "6. Acknowledgement": "flow for\ntext-to-speech via monotonic alignment search,” in Ad-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "vol. 10, p. e6, 2021."
        },
        {
          "6. Acknowledgement": "vances in Neural Information Processing Systems, H. Larochelle,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[19]\nJ. Seo and B. Lee, “Multi-task conformer with multi-feature com-"
        },
        {
          "6. Acknowledgement": "Curran Associates, Inc., 2020, pp. 8067–8077.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "bination for\nspeech emotion recognition,”\nSymmetry,\nvol. 14,"
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "no. 7, p. 1428, 2022."
        },
        {
          "6. Acknowledgement": "[3] V. Popov,\nI. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "“Grad-tts: A diffusion probabilistic model for text-to-speech,” in",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[20] Y. Leng, X. Tan, S. Zhao, F. Soong, X.-Y. Li, and T. Qin, “Mbnet:"
        },
        {
          "6. Acknowledgement": "International Conference on Machine Learning.\nPMLR, 2021,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "Mos prediction for synthesized speech with mean-bias network,”"
        },
        {
          "6. Acknowledgement": "pp. 8599–8608.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "in ICASSP 2021-2021 IEEE International Conference on Acous-"
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "tics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp."
        },
        {
          "6. Acknowledgement": "[4] C. Gˆarbacea, A. van den Oord, Y. Li, F. S. Lim, A. Luebs,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "391–395."
        },
        {
          "6. Acknowledgement": "O. Vinyals, and T. C. Walters, “Low bit-rate speech coding with",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "vq-vae and a wavenet decoder,” in ICASSP 2019-2019 IEEE Inter-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[21] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:"
        },
        {
          "6. Acknowledgement": "national Conference on Acoustics, Speech and Signal Processing",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "A framework for self-supervised learning of speech representa-"
        },
        {
          "6. Acknowledgement": "(ICASSP).\nIEEE, 2019, pp. 735–739.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "in Neural\ntions,” in Advances\nInformation Processing Systems,"
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin,"
        },
        {
          "6. Acknowledgement": "[5]\nJ.-M. Valin and J. Skoglund, “Lpcnet:\nImproving neural speech",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "Eds., vol. 33.\nCurran Associates, Inc., 2020, pp. 12 449–12 460."
        },
        {
          "6. Acknowledgement": "synthesis through linear prediction,” in ICASSP 2019-2019 IEEE",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "International Conference on Acoustics, Speech and Signal Pro-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[22] Q. Xu,\nA.\nBaevski,\nand M. Auli,\n“Simple\nand\neffective"
        },
        {
          "6. Acknowledgement": "cessing (ICASSP).\nIEEE, 2019, pp. 5891–5895.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "arXiv\npreprint\nzero-shot\ncross-lingual\nphoneme\nrecognition,”"
        },
        {
          "6. Acknowledgement": "[6]\nJ. Llanes-Jurado, L. G´omez-Zaragoz´a, M. E. Minissi, M. Alca˜niz,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "arXiv:2109.11680, 2021."
        },
        {
          "6. Acknowledgement": "and J. Mar´ın-Morales, “Developing conversational virtual humans",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[23] M. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., “A density-based"
        },
        {
          "6. Acknowledgement": "for social emotion elicitation based on large language models,”",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "algorithm for discovering clusters in large spatial databases with"
        },
        {
          "6. Acknowledgement": "Expert Systems with Applications, vol. 246, p. 123261, 2024.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "noise.” in kdd, vol. 96, no. 34, 1996, pp. 226–231."
        },
        {
          "6. Acknowledgement": "[7] K. Ochi, K. Inoue, D. Lala, T. Kawahara, and H. Kumazaki, “Ef-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[24] W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu,"
        },
        {
          "6. Acknowledgement": "fect of attentive listening robot on pleasure and arousal change",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "V\n. Pratap,\nJ. Kahn, A. Lee, R. Collobert, G. Synnaeve,\nand"
        },
        {
          "6. Acknowledgement": "in psychiatric daycare,” Advanced Robotics, vol. 37, no. 21, pp.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "M. Auli, “Robust wav2vec 2.0: Analyzing domain shift\nin self-"
        },
        {
          "6. Acknowledgement": "1382–1391, 2023.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "supervised pre-training,” in Interspeech 2021, 2021, pp. 721–725."
        },
        {
          "6. Acknowledgement": "[8] A. Baird, A. Triantafyllopoulos, S. Z¨ankert, S. Ottl, L. Christ,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[25] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and"
        },
        {
          "6. Acknowledgement": "L. Stappen,\nJ. Konzok, S. Sturmbauer, E.-M. Meßner, B. M.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "I. Sutskever,\n“Robust\nspeech recognition via\nlarge-scale weak"
        },
        {
          "6. Acknowledgement": "Kudielka et al., “An evaluation of\nspeech-based recognition of",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "supervision,”\nin International conference on machine learning."
        },
        {
          "6. Acknowledgement": "emotional and physiological markers of stress,” Frontiers in Com-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "PMLR, 2023, pp. 28 492–28 518."
        },
        {
          "6. Acknowledgement": "puter Science, vol. 3, p. 750284, 2021.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[26]\nJ. Devlin, “Bert: Pre-training of deep bidirectional\ntransformers"
        },
        {
          "6. Acknowledgement": "[9]\nI. Zubiaga,\nI. Menchaca, M. de Velasco, and R. Justo, “Mental",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "for\nlanguage understanding,” arXiv preprint arXiv:1810.04805,"
        },
        {
          "6. Acknowledgement": "health monitoring from speech and language,” Depression, vol. 3,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "2018."
        },
        {
          "6. Acknowledgement": "p. 276, 2022.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[27] W. C. Huang, E. Cooper, Y. Tsao, H.-M. Wang, T. Toda, and J. Ya-"
        },
        {
          "6. Acknowledgement": "[10] M. M¨antyl¨a, B. Adams, G. Destefanis, D. Graziotin, and M. Ortu,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "magishi, “The VoiceMOS Challenge 2022,” in Proc. Interspeech"
        },
        {
          "6. Acknowledgement": "“Mining valence, arousal, and dominance:\npossibilities\nfor de-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "2022, 2022, pp. 4536–4540."
        },
        {
          "6. Acknowledgement": "tecting burnout and productivity?” in Proceedings of the 13th in-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "ternational conference on mining software repositories, 2016, pp.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[28] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower,"
        },
        {
          "6. Acknowledgement": "247–258.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "S. Kim,\nJ. N. Chang, S. Lee, and S. S. Narayanan, “Iemocap:"
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "Interactive emotional dyadic motion capture database,” Language"
        },
        {
          "6. Acknowledgement": "[11] W.-C. Huang, E. Cooper, J. Yamagishi, and T. Toda, “Ldnet: Uni-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "resources and evaluation, vol. 42, pp. 335–359, 2008."
        },
        {
          "6. Acknowledgement": "fied listener dependent modeling in mos prediction for synthetic",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "speech,” in ICASSP 2022-2022 IEEE International Conference on",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "[29]\nL.-W. Chen and A. Rudnicky, “Exploring wav2vec 2.0 fine tuning"
        },
        {
          "6. Acknowledgement": "Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2022,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "for improved speech emotion recognition,” in ICASSP 2023-2023"
        },
        {
          "6. Acknowledgement": "pp. 896–900.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "IEEE International Conference on Acoustics, Speech and Signal"
        },
        {
          "6. Acknowledgement": "[12]\nT. Saeki, D. Xin, W. Nakata, T. Koriyama, S. Takamichi,\nand",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": "Processing (ICASSP).\nIEEE, 2023, pp. 1–5."
        },
        {
          "6. Acknowledgement": "H. Saruwatari,\n“UTMOS: UTokyo-SaruLab System for Voice-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "MOS Challenge 2022,”\nin Proc.\nInterspeech 2022,\n2022,\npp.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "4521–4525.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "[13] K. Shen, D. Yan, L. Dong, Y. Ren, X. Wu, and J. Hu, “Sqat-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "ld: Speech quality assessment\ntransformer utilizing listener de-",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "pendent modeling for zero-shot out-of-domain mos prediction,”",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "in 2023 IEEE Automatic Speech Recognition and Understanding",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "Workshop (ASRU).\nIEEE, 2023, pp. 1–6.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "[14]\nZ. Li and W. Li, “Moslight: A lightweight data-efficient system",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "for non-intrusive speech quality assessment,” in Proc. Interspeech",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "2023, 2023, pp. 5386–5390.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "[15] H. M. Marcus-Roberts and F. S. Roberts, “Meaningless statistics,”",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "Journal of Educational Statistics, vol. 12, no. 4, pp. 383–394,",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        },
        {
          "6. Acknowledgement": "1987.",
          "[16] C.-H. Hu, Y. Yasuda,\nand T. Toda,\n“Preference-based training": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Speech intelligibility prediction using a neurogram similarity index measure",
      "authors": [
        "A Hines",
        "N Harte"
      ],
      "year": "2012",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "3",
      "title": "Glow-tts: A generative flow for text-to-speech via monotonic alignment search",
      "authors": [
        "J Kim",
        "S Kim",
        "J Kong",
        "S Yoon"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "Grad-tts: A diffusion probabilistic model for text-to-speech",
      "authors": [
        "V Popov",
        "I Vovk",
        "V Gogoryan",
        "T Sadekova",
        "M Kudinov"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "5",
      "title": "Low bit-rate speech coding with vq-vae and a wavenet decoder",
      "authors": [
        "C Gârbacea",
        "A Van Den Oord",
        "Y Li",
        "F Lim",
        "A Luebs",
        "O Vinyals",
        "T Walters"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "6",
      "title": "Lpcnet: Improving neural speech synthesis through linear prediction",
      "authors": [
        "J.-M Valin",
        "J Skoglund"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "7",
      "title": "Developing conversational virtual humans for social emotion elicitation based on large language models",
      "authors": [
        "J Llanes-Jurado",
        "L Gómez-Zaragozá",
        "M Minissi",
        "M Alcañiz",
        "J Marín-Morales"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "8",
      "title": "Effect of attentive listening robot on pleasure and arousal change in psychiatric daycare",
      "authors": [
        "K Ochi",
        "K Inoue",
        "D Lala",
        "T Kawahara",
        "H Kumazaki"
      ],
      "year": "2023",
      "venue": "Advanced Robotics"
    },
    {
      "citation_id": "9",
      "title": "An evaluation of speech-based recognition of emotional and physiological markers of stress",
      "authors": [
        "A Baird",
        "A Triantafyllopoulos",
        "S Zänkert",
        "S Ottl",
        "L Christ",
        "L Stappen",
        "J Konzok",
        "S Sturmbauer",
        "E.-M Meßner",
        "B Kudielka"
      ],
      "year": "2021",
      "venue": "Frontiers in Computer Science"
    },
    {
      "citation_id": "10",
      "title": "Mental health monitoring from speech and language",
      "authors": [
        "I Zubiaga",
        "I Menchaca",
        "M Velasco",
        "R Justo"
      ],
      "year": "2022",
      "venue": "Depression"
    },
    {
      "citation_id": "11",
      "title": "Mining valence, arousal, and dominance: possibilities for detecting burnout and productivity",
      "authors": [
        "M Mäntylä",
        "B Adams",
        "G Destefanis",
        "D Graziotin",
        "M Ortu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 13th international conference on mining software repositories"
    },
    {
      "citation_id": "12",
      "title": "Ldnet: Unified listener dependent modeling in mos prediction for synthetic speech",
      "authors": [
        "W.-C Huang",
        "E Cooper",
        "J Yamagishi",
        "T Toda"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "UTMOS: UTokyo-SaruLab System for Voice-MOS Challenge 2022",
      "authors": [
        "T Saeki",
        "D Xin",
        "W Nakata",
        "T Koriyama",
        "S Takamichi",
        "H Saruwatari"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "Sqatld: Speech quality assessment transformer utilizing listener dependent modeling for zero-shot out-of-domain mos prediction",
      "authors": [
        "K Shen",
        "D Yan",
        "L Dong",
        "Y Ren",
        "X Wu",
        "J Hu"
      ],
      "year": "2023",
      "venue": "2023 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "15",
      "title": "Moslight: A lightweight data-efficient system for non-intrusive speech quality assessment",
      "authors": [
        "Z Li",
        "W Li"
      ],
      "year": "2023",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Meaningless statistics",
      "authors": [
        "H Marcus-Roberts",
        "F Roberts"
      ],
      "year": "1987",
      "venue": "Journal of Educational Statistics"
    },
    {
      "citation_id": "17",
      "title": "Preference-based training framework for automatic speech quality assessment using deep neural network",
      "authors": [
        "C.-H Hu",
        "Y Yasuda",
        "T Toda"
      ],
      "venue": "Preference-based training framework for automatic speech quality assessment using deep neural network"
    },
    {
      "citation_id": "18",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition based on listener-dependent emotion perception models",
      "authors": [
        "A Ando",
        "T Mori",
        "S Kobashikawa",
        "T Toda"
      ],
      "year": "2021",
      "venue": "APSIPA Transactions on Signal and Information Processing"
    },
    {
      "citation_id": "20",
      "title": "Multi-task conformer with multi-feature combination for speech emotion recognition",
      "authors": [
        "J Seo",
        "B Lee"
      ],
      "year": "2022",
      "venue": "Symmetry"
    },
    {
      "citation_id": "21",
      "title": "Mbnet: Mos prediction for synthesized speech with mean-bias network",
      "authors": [
        "Y Leng",
        "X Tan",
        "S Zhao",
        "F Soong",
        "X.-Y Li",
        "T Qin"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato"
    },
    {
      "citation_id": "23",
      "title": "Simple and effective zero-shot cross-lingual phoneme recognition",
      "authors": [
        "Q Xu",
        "A Baevski",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Simple and effective zero-shot cross-lingual phoneme recognition",
      "arxiv": "arXiv:2109.11680"
    },
    {
      "citation_id": "24",
      "title": "A density-based algorithm for discovering clusters in large spatial databases with noise",
      "authors": [
        "M Ester",
        "H.-P Kriegel",
        "J Sander",
        "X Xu"
      ],
      "year": "1996",
      "venue": "kdd"
    },
    {
      "citation_id": "25",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in selfsupervised pre-training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve",
        "M Auli"
      ],
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in selfsupervised pre-training"
    },
    {
      "citation_id": "26",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "27",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "28",
      "title": "The VoiceMOS Challenge 2022",
      "authors": [
        "W Huang",
        "E Cooper",
        "Y Tsao",
        "H.-M Wang",
        "T Toda",
        "J Yamagishi"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "29",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "30",
      "title": "Exploring wav2vec 2.0 fine tuning for improved speech emotion recognition",
      "authors": [
        "L.-W Chen",
        "A Rudnicky"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}