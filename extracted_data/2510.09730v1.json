{
  "paper_id": "2510.09730v1",
  "title": "Adaptive Fusion Network With Temporal-Ranked And Motion-Intensity Dynamic Images For Mer",
  "published": "2025-10-10T11:03:20Z",
  "authors": [
    "Thi Bich Phuong Man",
    "Luu Tu Nguyen",
    "Vu Tram Anh Khuong",
    "Thanh Ha Le",
    "Thi Duyen Ngo"
  ],
  "keywords": [
    "Micro-expression recognition",
    "Temporal-ranked dynamic image",
    "Motion-intensity dynamic image",
    "adaptive fusion network",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Micro-expressions (MEs) are subtle, transient facial changes with very low intensity, almost imperceptible to the naked eye, yet they reveal a person's genuine emotions. They are of great value in lie detection, behavioral analysis, and psychological assessment. However, micro-expression recognition (MER) remains a major challenge due to its short duration, low intensity, and the difficulty of learning discriminative features. Recent studies have exploited deep learning with dynamic images (DIs) and optical flow (OF), but OF is highly sensitive to illumination noise, while traditional DIs -originally designed for action recognition -mainly capture temporal progression without encoding motion intensity. To address these limitations, this paper proposes a novel MER method with two main contributions. First, we propose two complementary representations: Temporal-ranked dynamic image, which emphasizes temporal progression, and Motion-intensity dynamic image, which highlights subtle motions through a frame reordering mechanism incorporating motion intensity. Second, we propose an Adaptive fusion network, which automatically learns to optimally integrate these two representations, thereby enhancing discriminative ME features while suppressing noise. Experiments on three benchmark datasets (CASME-II, SAMM and MMEW) demonstrate the superiority of the proposed method. Specifically, TRDI + MIDI + AFN achieves 93.95% Accuracy and 89.72% UF1 on CASME-II, setting a new state-of-the-art benchmark. On SAMM, the method attains 82.47% Accuracy and 66.52% UF1, demonstrating more balanced recognition across classes. On MMEW, the model achieves 76.00%",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions play a vital role in daily life, shaping people's thoughts, decisions, and behaviors  [1] . Psychological studies have shown that facial expressions are among the most powerful non-verbal channels for conveying emotions, accounting for up to 55% of emotional communication in social interactions  [2]   [3] . Facial expressions can be categorized into two types: macro-expressions and micro-expressions. Macro-expressions are more apparent, typically lasting a few seconds, and reflect strong, easily recognizable emotions. In contrast, MEs are unconscious, extremely brief facial movements, usually lasting less than half a second, that reveal genuine emotions which may be suppressed or deliberately concealed. These subtle and fleeting expressions are difficult to perceive with the naked eye but carry valuable information about a person's internal emotional state. Consequently, MER has become an important research area with applications in lie detection, behavior analysis, and the study of emotional responses in sensitive contexts such as criminal investigations, psychological assessments, and social interactions  [4]   [5]   [6] . However, MER has remained a major challenge for many years. The difficulties primarily arise from the intrinsic characteristics of MEs: (1) their extremely short duration makes it challenging to capture and model temporal dynamics; (2) their subtle motion intensity renders the signals highly susceptible to noise and difficult to distinguish from non-relevant facial movements; and (3) the limited availability of annotated datasets restricts the ability of deep learning models to fully exploit their potential. Therefore, achieving high performance in MER requires not only designing an effective input representation but also developing an optimal feature extraction strategy tailored to this type of representation. In MER, research typically focuses on transforming video sequences into more manageable input forms, primarily categorized into two modalities: static and dynamic  [7] . The static modality often relies on the apex frame-the frame that captures the peak intensity of a ME to classify emotions. While this approach reduces the temporal complexity of video analysis, it fails to fully capture the temporal dynamics of MEs, which limits its effectiveness in distinguishing between different emotions. The dynamic modality addresses the limitations of the static modality by capturing motion changes across video frames and encoding them into image-based representations. Two commonly used inputs in this category are OF and DIs. OF estimates motion vectors (direction and magnitude) between consecutive frames, thereby directly characterizing dynamic information. However, most optical flow-based studies rely on a limited set of representative frames, such as the onset, apex, and offset frames (corresponding to the start, peak, and end of the expression). While this approach can reveal motion changes in MEs, it remains incomplete when restricted to a few representative frames. Moreover, the computational cost increases significantly with the number of optical flow images, and the method is highly sensitive to noise.\n\nUnlike OF, a DI compresses the entire video sequence into a single image using a ranking function  [8][7] . This representation not only preserves the temporal order of frames but also reduces computational cost by summarizing dynamic information in a compact form. DIs have demonstrated effectiveness in various action recognition tasks, highlighting their potential for MER. However, when applied to MER, DIs face a critical limitation: they primarily encode the temporal order of frames without explicitly representing motion intensity (MI). Since MEs are characterized by extremely subtle and low-intensity movements, this omission reduces the ability of DIs to capture the most discriminative features. Several extensions, such as active imaging  [9] , affective-motion imaging  [10] , and attention-based models, have been proposed to address this limitation. While these methods achieve modest improvements, their accuracy in MER remains relatively low, typically ranging between 35% and 60%. Moreover, approaches like active and affective imaging still rely on the assumption that later frames carry more importance than earlier ones-a premise that may not always hold true in MER scenarios.\n\nGiven the limitations of current input modalities-namely, the insufficient ability of static modality to capture temporal kinematic information, the susceptibility of optical flow to noise and high computational cost, and the inadequate design of dynamic images for the MER task-there is a need for a new representation that can simultaneously preserve temporal progression and highlight subtle motion intensity. Such a representation would provide more discriminative features and improve the overall performance of MER. The choice of feature extraction and emotion recognition method in MER is also a key factor in effectively capturing ME motion. Existing studies are generally divided into two main approaches: hand-crafted methods and deep learning-based methods. For hand-crafted approaches, techniques such as Local Binary Pattern on Three Orthogonal Planes (LBP-TOP), LBP-SIX, Histogram of Oriented Optical Flow (HOOF), and Histogram of Oriented Gradients (HOG) are commonly used to extract ME features  [11][12] [13]  [11] [14]  [15] . These methods describe facial muscle movements based on pixel motion directions or spatio-temporal texture features. However, they are not powerful enough and remain limited in their ability to capture the subtle and finegrained dynamic information of MEs. For deep learning-based approaches, Convolutional neural networks (CNNs) are commonly used for feature extraction and MER. Deep CNN models have been widely applied in MER with the aim of jointly learning both spatial and temporal features. Among them, spatially oriented CNN architectures such as ResNet, VGG, and Inception mainly focus on extracting fine-grained facial features from static frames, especially apex frames  [16] [17]  [18] . However, a key limitation of this approach is its inability to capture temporal dynamics. To address this, temporal modeling techniques such as Long short-term memory (LSTM) , Gated recurrent unit (GRU), and 3D-CNN have been employed to analyze frame sequences and capture motion information  [19]   [20] . More advanced methods, including 3D-FCNN, CNNCapsNet, and MSCNN, incorporate OF into spatiotemporal deep learning frameworks to enhance performance  [21] [22]  [23] . Nevertheless, these models often require large-scale datasets and are computationally expensive. Hybrid architectures, such as Inception-LSTM and 3D-ResNet, combine spatial and temporal feature learning but at the cost of increased complexity  [24]   [25] . Overall, although deep CNNs have achieved impressive results in many domains, their dependence on large amounts of training data makes them prone to overfitting in MER, where datasets are inherently limited.\n\nBuilding on the potential yet notable limitations of DIs as input, and considering the challenges faced by deep learning models due to the limited data available for MER, this study proposes a novel MER method. Specifically, we introduce an Adaptive Fusion Network that takes as input two types of DIs: Temporal-ranked dynamic images, which capture temporal progression, and Motion-intensity dynamic images, which emphasize subtle motion intensity.\n\nThe main contributions of this research are as follows:\n\n• We propose novel image representations for MER: Temporal-ranked dynamic image and Motion-Intensity dynamic image. These enable the model to capture both temporal variations and the subtle motion intensity of MEs. Specifically:\n\n-Temporal-ranked dynamic image (TRDI): frames containing MEs are rearranged based on the assumption that the frame ranks decrease symmetrically from the apex frame outward. This approach allows for better modeling of the subtle and transient motions of MEs.\n\n-Motion-intensity dynamic image (MIDI): frames are weighted by MI, calculated through OF between each frame and the apex frame. The DI is then generated based on the MI coefficient of each frame.\n\n• We propose the Adaptive fusion network (AFN), a novel deep learning architecture designed to maximize the complementary information from the two novel image representations. The core idea of AFN is not to treat the two information sources as independent, but to enable them to interact and complement each other through an adaptive fusion mechanism. Specifically:\n\n-Representation fusion block (RFB): proposed to automatically learn fusion weights using attention, enabling the model to flexibly emphasize the more important information source at each spatial location, instead of relying on fixed aggregation or simple pairing strategies as in previous studies.\n\n-  By combining these two components, AFN not only enhances feature representation capability but also exhibits adaptive dynamic behavior -a crucial factor in processing MEs, which are inherently short-lived and difficult to detect.\n\nExperiments were conducted to validate the effectiveness of the proposed method, including: evaluating the novel input representations, assessing the performance of the AFN architecture, and analyzing the combined effectiveness of both proposed components.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "To overcome the limitations of traditional dynamic imaging, this paper proposes an improved MER method by integrating an AFN with two new representations: TRDI and MIDI. Unlike conventional dynamic imaging, which is mainly designed for action recognition, TRDI rearranges frames based on temporal ranking to emphasize subtle and transient changes, while MIDI captures the local motion intensity between frames. These two complementary representations provide richer and more discriminative information for the MER task. Furthermore, the AFN model is designed to effectively fuse the features extracted from TRDI and MIDI, thereby improving the overall recognition performance.\n\nFigure  1  illustrates the workflow of the proposed method, where TRDI and MIDI refine the input representation, while AFN performs multi-stream feature fusion and optimizes the MER process. The following subsections will detail each component.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Dynamic Image Construction",
      "text": "Dynamic image was developed by Bilen et al.  [8]  based on the concepts of ranking function and rank pooling proposed by Fernando et al.  [26]   [27] . The main idea is to represent a video by a single feature vector through learning a ranking function over its frames.\n\nSpecifically, for a sequence of frames I 1 , I 2 , ..., I T , Fernando defined the ranking function in Equation  1 :\n\nwhere d ∈ R n is a learnable parameter vector, and ψ(I τ ) denotes the feature vector of frame τ . The objective is to ensure that later frames receive higher scores in Equation  2 :\n\n(\n\nThe optimal vector d * is estimated by Equation  3 4:\n\nThis process is referred to as rank pooling, and the resulting d * can be interpreted as an RGB image representing the whole video.\n\nBilen et al. later proposed Approximate Rank Pooling (ARP) to reduce computational cost. ARP approximates d * as a weighted sum of frame features in Equation  5 :\n\nIn this study, we adopt ARP to generate dynamic images from ME frame sequences, which are then used as input for the recognition process.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Temporal-Ranked Dynamic Image",
      "text": "In traditional DI construction, the basic assumption is that frames appearing later in the sequence usually carry more important information, so this method tends to assign higher weights to the ending frames of the video. This assumption is suitable for action recognition tasks, where motion often accumulates towards the end of a segment. However, when applied to MER, empirical observations show that this assumption no longer holds.\n\nThe distinctive characteristic of MEs is that their motion intensity is very small, short-lived, and mainly concentrated around the apex frame. If depicted graphically, the MI trajectory often resembles a bell shape: gradually increasing from the onset, reaching a peak at the apex, and then gradually decreasing towards the offset (Figure  2a ). This indicates that the frames around the apex actually contain the most important information for recognition, rather than the final frames as assumed in traditional DIs.\n\nBased on this observation, we propose a Temporal-ranked method, in which DI construction is performed according to temporal ranking. Specifically, instead of summing or averaging the entire frame sequence as in the traditional approach, the frames in the TRDI are rearranged according to a temporal ranking, which typically decreases symmetrically from the central frame (apex) towards both the onset and the offset. This idea reflects the assumption that the central frame (apex) carries the most ME information, while frames further away from the center are less important.\n\nDuring the construction process, each frame is assigned a weight based on its temporal ranking. As a result, the final TRDI emphasizes the most informative frames around the apex, while reducing noise from less relevant frames. This approach ensures that the representation is not only consistent with the bell-shaped nature of MEs, but also more optimal for the recognition task.\n\nAlgorithm 1 details the Temporal-ranked mechanism:\n\n• First, the motion coefficients are sorted in descending order to prioritize the values that represent the strongest motion.\n\n• Next, a new coefficient array is initialized with all values set to 0, retaining only the important coefficients to reduce noise from irrelevant data. The largest weight is assigned directly to the apex frame, reflecting the critical importance of this moment.\n\n• Finally, the remaining coefficients are distributed symmetrically around the apex, modeling the propagation of motion from the peak intensity towards both the onset and offset frames.\n\nThe result is a symmetrical bell-shaped weight distribution, consistent with the empirical characteristics of MEs (Figure  2b, 2c ). Comparisons show that this representation better emphasizes meaningful frames and smooths the transitions, resulting in a more informative and less noisy DI. As shown in Figure  3 , we compare apex frame, the original DI and the TRDI. It can be seen that TRDI highlights the details around the eye and forehead regions more clearly, while the original DI still contains a lot of irrelevant noise.  if apex_frame -i ≥ 0 then 6:\n\nend if 11: end for 12: return new_coefficients",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Motion-Intensity Di Construction",
      "text": "One of the major challenges in MER is how to extract the subtle and transient features of facial motion. Although TRDI is capable of emphasizing the importance of frames based on their temporal positions, it does not capture the differences in motion intensity between frames. In practice, not all frames contribute equally: some frames exhibit distinct motion associated with the ME apex, while many others are mostly neutral or contain substantial noise. Based on this observation, we propose the MIDI method, which incorporates motion intensity into the DI generation process to emphasize the most discriminative frames.\n\nThe core idea of MIDI is based on the assumption that MEs are typically characterized by localized and brief facial muscle activations, with MI varying over time. Therefore, frames with larger motion magnitudes should be assigned higher weights during the synthesis process, while frames with weaker or irrelevant motion should be down-weighted. Incorporating motion magnitudes into the DI generation helps to highlight important signals while minimizing the impact of background noise or irrelevant variations, thus providing a clean yet informative representation for the recognition model.\n\nTo compute MIDI, we estimate the OF between each frame and the apex, assuming that the apex contains the maximum motion of the ME. The DIS optical flow algorithm is employed due to its ability to balance computational efficiency with sensitivity in detecting small displacements. For each frame pair, we obtain a two-dimensional OF field (dx, dy), from which we calculate the average motion amplitude over the entire image.\n\nFor each frame i, the motion magnitude is computed as follows Equation  6 :\n\nTo ensure that the apex frame always receives the highest weight, the magnitude values are normalized by inverting them with respect to the maximum value follow in Equation  7 :\n\nThe final MIDI dynamic image is constructed as a weighted sum of all frames, as shown in Equation  8 :\n\nThis approach incorporates motion intensity directly into the DI construction process, allowing the model to focus more on frames that contain key expressive information while reducing the influence of neutral or noisy frames. As shown in Figure  4 , the MIDI compared with the apex frame and onset frame reveals more clearly the subtle motion cues on the face. This indicates that MIDI not only inherits the critical information from the apex frame but also highlights fine-grained expressive details, making it easier for the model to extract discriminative features. Consequently, MIDI becomes an effective representation that captures the spatio-temporal dynamics of MEs, and is particularly useful in distinguishing extremely subtle expressions where critical signals are often obscured by irrelevant variations.",
      "page_start": 10,
      "page_end": 12
    },
    {
      "section_name": "Adative Fusion Network",
      "text": "In MER, DI-based methods face inherent limitations. Using only one type of DI often results in information imbalance, while static fusion methods such as addition or averaging are inflexible because they apply a fixed fusion rule to all instances, regardless of the individual characteristics of each ME. To address these issues, we propose the AFN as shown in Figure  5 , an architecture designed to intelligently handle information fusion and feature extraction.\n\nAFN achieves this through two core components: first, the Representation fusion block employs an attention mechanism to adaptively adjust the weights of each type of representation according to the context, producing an optimal fused representation. Next, the Multi-scale channel attention block leverages this representation by extracting multi-level features and rebalancing the importance of information channels. Through this combination, AFN not only generates a more informative input but also ensures that the most discriminative features are effectively exploited, thereby significantly",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Representation Fusion Block",
      "text": "This paper introduces the RFB as shown in Figure  6 , a core component designed to address the challenge of adaptively and efficiently fusing information from multiple DIs. This module is particularly well-suited for integrating our two main input formats: the TRDI and the MIDI. Instead of relying on static fusion methods such as concatenation or pooling, the module employs a spatial attention mechanism to dynamically assign weights to each information source at every spatial location in the image.\n\nSpecifically, the module begins by extracting high-level features from each input image (coef_img and motion_img) through the coef_encoder and mo-tion_encoder branches. Each branch consists of a convolutional layer to capture local spatial patterns, followed by GroupNorm and ReLU to ensure training stability and enable nonlinear representation learning. The extracted features from these two branches are then concatenated and passed through a lightweight neural network (attention_generator). This network, composed of 1×1 convolutional layers, learns to generate spatial weight maps (atten-tion_weights). These weights, normalized via a softmax function, determine the contribution of each feature source (coef_features and motion_features) at every spatial location.\n\nFinally, adaptive fusion is performed by element-wise multiplying the features from each source with their corresponding attention weights, followed by summation to produce fused_features. A 1×1 convolutional layer (final_conv) is then applied to adjust the output channel dimension. In addition, a skip connection is introduced by adding the average of the two original input images. This skip connection not only facilitates gradient propagation through the deep network but also ensures that essential information from the original inputs is preserved, thereby improving training stability.\n\nThrough the combination of these mechanisms, the RFB produces an optimal, informative, and context-adaptive fused representation, which serves as a rich input for the subsequent stages of the MER model.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Multi-Scale Channel Attention Block",
      "text": "MSCAB, as shown in Figure  7  is the second core component of AFN, responsible for extracting discriminative features from the adaptively fused DIs and optimizing their representations for the MER task. While the RFB ensures that the input image is both contextual and informative, MSCAB further enhances this representation by performing deep feature extraction, multi-level abstraction, and channel rebalancing. The module begins with a backbone feature extractor that processes the fused image to capture hierarchical features, ranging from low-level spatial textures to high-level semantic motion patterns. A lightweight yet powerful convolutional neural network is employed to balance computational efficiency with representational capacity, ensuring that subtle motion cues are preserved. To further refine the extracted features, MSCAB incorporates a channel reweighting mechanism inspired by the attention principle. This mechanism adaptively emphasizes the most informative channels while suppressing redundant or noisy ones. This is particularly important in MER, where discriminative cues are extremely subtle and sparse. By dynamically adjusting the importance of different feature channels, the model ensures that the core ME signals dominate the final representation. Finally, the refined features are aggregated and passed through a classification head, which maps them into the ME label space. This head typically consists of a global pooling layer to condense spatial information, followed by fully connected layers to perform classification. With this design, MSCAB not only maximizes the utility of the fused DIs but also directly addresses the core challenge of MER: distinguishing subtle, low-intensity, and transient expressions. By complementing the adaptive fusion stage, MSCAB ensures that the most discriminative patterns are effectively captured and leveraged for accurate recognition.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Experiment And Results",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Experiment",
      "text": "To evaluate the effectiveness of the proposed method, a series of experiments were conducted to analyze the contributions of each component in-  • Second, we examine the performance of the proposed AFN architecture, which is tailored for feature extraction in MER and addresses data limitations. Comparisons with pre-trained DCNN models show that AFN achieves more meaningful feature representations.\n\n• Finally, we evaluate the overall performance of the MER framework by integrating TRDI, MIDI, and AFN. The combination of enhanced motion representation from TRDI and MIDI with optimized feature extraction from AFN results in significant performance improvement, confirming the complementary nature of these components.",
      "page_start": 14,
      "page_end": 16
    },
    {
      "section_name": "Dataset",
      "text": "This study utilizes three widely used MER benchmarks: CASME-II  [28] , SAMM  [29] , and MMEW  [30] .\n\nCASME-II: Consists of 247 videos from 26 participants recorded at 200 fps. Videos were annotated using FACS, and only emotion labels with suffi-cient samples were retained, resulting in five categories: surprise, happiness, disgust, other, and repression.\n\nSAMM: Contains 159 videos from 32 participants across 13 ethnicities, recorded at 200 fps with high spatial resolution. Five emotion categories were annotated: anger, contempt, happiness, surprise, and other.\n\nMMEW: Provides both macro-and micro-expressions from the same participants, with 300 micro-expression and 900 macro-expression samples at 1920×1080 resolution and 90 fps. Four emotion categories were used for experiments: happiness, surprise, disgust, and others.\n\nData augmentation: To improve generalization, images were horizontally flipped and rotated at small (±5°) and larger (±10°) angles. All images were resized to 224×224, converted to three-channel grayscale, and normalized. Each original image was transformed into multiple variants to expand the dataset and reduce overfitting.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Experiment Settings",
      "text": "All experiments follow the Leave-One-Subject-Out (LOSO) protocol. In each iteration, samples from one subject are used for testing, while the remaining subjects are used for training. Final performance is reported as the mean accuracy over all subjects. Training configuration includes Adam optimizer with a learning rate of 0.001, weight decay of 0.0001, batch size of 16, 50 epochs, and cross-entropy loss. Two main experiments are conducted:\n\nExperiment 1: Effectiveness of the proposed dynamic image inputs. This experiment investigates the contribution of the two proposed dynamic image representations, TRDI and MIDI, for MER. These inputs are designed to capture subtle facial movements and are compared against other input types, including Apex frames and original DIs. Both MSCAB (a component of AFN) and ResNet are used as backbone architectures to evaluate and demonstrate the superior performance of TRDI and MIDI in representing micro-expressions.\n\nExperiment 2: Effectiveness of the complete MER framework. This experiment evaluates the overall performance of the proposed method, AFN. TRDI and MIDI are used as the primary inputs. Additionally, a combination of MIDI with the original dynamic image is evaluated to investigate whether integrating complementary motion information can further improve performance. To highlight the advantages of the proposed architecture, AFN is tested both with its original MSCAB component and with ResNet as a replacement backbone, allowing a direct comparison of feature extraction and overall MER performance.\n\nThis experimental setup ensures reliability, reproducibility, and robustness of the proposed method.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Results 1) Visual Representation",
      "text": "The figure  8  illustrates the visualization of DIs and TRDIs extracted from facial video sequences. The DIs on the left represent the aggregated motion information over time, while the TRDIs on the right emphasize the residual features corresponding to frame-to-frame changes. Compared with DIs, TRDIs highlight more detailed facial dynamics such as wrinkles, micro muscle contractions, and fine motion cues around key regions of the face.\n\nFor instance, in Disgust, TRDIs show clearer activations around the nose and upper lip area, corresponding to AU9 (nose wrinkler) and AU10 (upper lip raiser). In Happiness, the eye corners and cheek areas become more pronounced, which are related to AU6 (cheek raiser) and AU12 (lip corner puller). Similarly, Repression and Surprise expressions show distinct movements around the mouth and eyebrows, emphasizing AU1 (inner brow raiser), AU2 (outer brow raiser), and AU5 (upper lid raiser) that are crucial for distinguishing subtle expressions.\n\nOverall, TRDIs effectively preserve these fine-grained and transient expression cues, providing a richer representation of subtle facial motion patterns. This enhanced visualization confirms that TRDIs are better suited for capturing micro-expressions, where small and short-lived facial actions play a vital role in classification.\n\nThis figure  9  presents the visualization of MIDI inputs, TRDI, and the fused features extracted from the RFB. The first column shows MIDI, which captures global facial movements; the second column illustrates TRDI, which focuses on local variation information; and the third column displays the post-fusion features, which highlight the most important motion patterns for classification. The differences among the three representations demonstrate that the feature extraction and fusion process enables the model to learn both global and fine-grained dynamic information.\n\n1) Experiment 1 result Table  1  present the experimental results comparing the performance of MER on the CASME-II, SAMM and MMEW datasets using different input modalities and backbones in two classify models: MSCAB (ours) and ResNet. The results clearly show that both TRDI and MIDI significantly   Notably, ResNet performs worse than MSCAB in most cases, suggesting that using a generic deep CNN model for MER is not necessarily the optimal choice. These results confirm that TRDI and MIDI, when combined with the classification model in the AFN architecture, provide superior and stable performance across diverse datasets.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "2) Experiment 2 Result",
      "text": "The results in Table  2  clearly show that combining modality approaches provides significant improvements over using each modality individually. In particular, the MIDI + TRDI combination achieves superior performance across all datasets. On CASME-II, MSCAB with MIDI + TRDI achieves 93.95%, significantly higher than MIDI + DI (62.65%) and surpassing the results reported in Experiment 1. On SAMM, this combination also attains the highest performance at 80.16%, outperforming MIDI + DI by 16%. On MMEW, the MIDI + TRDI combination achieves 76%, which is 14.28 higher than MIDI + DI. This demonstrates that TRDI and MIDI provide complementary information, which, when combined, creates a more effective dynamic representation for micro-expression recognition. Furthermore, in all cases, MSCAB outperforms ResNet, especially in the MIDI + TRDI combination, suggesting that a MER-specific backbone architecture such as MSCAB can exploit the complex features of the data more effectively. These results confirm that integrating multiple dynamic modalities, especially MIDI and TRDI, with a suitable classification model such as MSCAB within the AFN architecture is a powerful approach to improving the performance of MER.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Comparison With State-Of-The-Art (Sota) Methods",
      "text": "Table  3  presents the performance comparison of MER methods on three datasets: CASME-II, SAMM, and MMEW, based on two main metrics: Accuracy (Acc) and Unweighted F1-score (UF1). In this comparison, the MMEW dataset uses six classes, excluding the Others label.\n\nOn CASME-II, the proposed method MIDI + TRDI + AFN achieves 93.95% Accuracy and 89.72% UF1, outperforming all other SOTA methods. In particular, the MMNet method -one of the recent strong baselines -achieves 88.35% Accuracy and 86.76% UF1, which are 5.17% and 2.71% lower than the proposed method, respectively. Other methods such as TSMicro and AUGCN yield significantly lower results, further demonstrating the effectiveness of our proposed architecture in extracting fine-grained features from ME data.\n\nFor SAMM, the proposed method achieves 82.47% Accuracy and 66.52% UF1. In terms of Accuracy, this result is higher than most other methods, only slightly behind DSSTDN with 85.68% and MMNet with 85.21%. However, in terms of UF1 -an important metric reflecting the balance across classes -our method achieves 66.52%, which is higher than many SOTA methods such as AMAN (65.82%) and MERsiamC3D (64.00%). This indicates that the proposed method not only achieves high accuracy but also improves the ability to recognize expressions more evenly across different classes.\n\nFor the MMEW dataset, which includes six primary emotion categories, the proposed method also achieves impressive performance with 74.31% Accuracy and 55.79% UF1. Although these values are slightly lower than those obtained on CASME-II and SAMM, they are still higher than most other state-of-the-art (SOTA) methods listed in the table. This indicates that the model maintains stable recognition performance even under challenging conditions with high inter-class variation and complex facial dynamics.\n\nThese results demonstrate that the combination of MIDI, TRDI, and AFN modules effectively captures subtle and diverse facial movements, while enabling the model to generalize well across different datasets. The consistent and superior performance across multiple benchmarks confirms the robustness, adaptability, and reliability of the proposed method.",
      "page_start": 21,
      "page_end": 23
    },
    {
      "section_name": "Ablation Study",
      "text": "To further verify the validity and effectiveness of the proposed method, we design three ablation studies to address the following questions:\n\n• Is the RFB really necessary?\n\nWe compare it with a manual fusion strategy to verify whether the original design provides a clear advantage.\n\n• Are the components within the RFB important?\n\nWe remove or fix each component individually to analyze their contribution and evaluate their impact on the overall performance.\n\n• Do the component removal and fixation strategies work similarly on the MER model?\n\nWe apply the same ablation procedure to the MER model to examine whether the trend remains consistent, thereby confirming the generality of the proposed framework.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Necessity Of Rfb",
      "text": "In this section, we design a manual fusion strategy for two inputs, TRDI and MIDI, based on the formula as shown in Equation  9 :\n\nwhere λ ∈ [0, 1] is a coefficient that adjusts the contribution of each feature to the fused representation before being fed into the MSCAB for classification.\n\nWe choose this linear formulation because of its simplicity, intuitiveness, and ease of control. It allows direct verification of the relative contributions of TRDI and MIDI to the fused feature and provides a reasonable baseline for comparison with the RFB. Through this, we can assess whether the original design of the fusion module offers a superior advantage.\n\nTo verify the necessity of the RFB, we conduct two experimental steps. First, we design a manual fusion strategy by varying the coefficient α ∈ [0, 1] on three datasets, CASME II, SAMM, and MEWW, following the LOSO protocol. The results in Table  4  show that the highest performance is achieved when α is in the range of 0.5 ∼ 0.6. This indicates that TRDI and MIDI contribute almost equally, and that a balanced combination of these two feature sources yields the optimal performance. Next, we compare the results of the manual fusion strategy with those of the proposed RFB. To ensure objectivity, we also include ResNet as a baseline, trained on the best fusion features. The results in Table ?? show that although manual fusion achieved competitive results, the RFB consistently outperformed it on all three datasets. In particular, MSCAB consistently surpassed ResNet in every case, demonstrating that MSCAB is more effective at exploiting dynamic information and is better suited for the micro-expression recognition task. In summary, these results demonstrate that the RFB is not only necessary but also more effective than both manual fusion and the baseline ResNet, due to its ability to automatically learn complex interactions between TRDI and MIDI rather than relying solely on fixed linear combinations.",
      "page_start": 24,
      "page_end": 26
    },
    {
      "section_name": "Effect Of Rfb Components",
      "text": "To evaluate the contribution of each component in the RFB in detail, we conducted an ablation experiment by removing the residual connection and the attention mechanism individually. The results are presented in Table  6 . When only the attention mechanism (Attn + no-residual ) is retained, the accuracy slightly improves compared to the previous case but is still lower than the full model (CASME-II: 76.71%, SAMM: 66.56%, MMEW: 67.19%).\n\nWhen both components are removed (No-attn + no-residual ), the model suffers severe performance degradation across all three datasets (CASME-II: 70.09%, SAMM: 57.96%, MMEW: 60.41%).\n\nIn contrast, the Full model with both residual connection and attention mechanism achieves the best results (CASME-II: 93.95%, SAMM: 80.16%, MMEW: 75.99%), demonstrating the effectiveness of combining both components simultaneously.\n\nThese results confirm that both the residual connection and the attention mechanism play essential roles in enhancing the representational capacity of the RFB, and their combination brings a significant improvement in MER performance.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Effect Of Mscab Module Components",
      "text": "To evaluate the role of the components in MSCAB in detail, we conducted a series of experiments by removing or fixing each component in turn, including the learned weights α and the SE block. The results are presented in Table  7 . When α is fixed to a uniform value of 1/4 for each branch (a_fixed ), the performance drops significantly compared to the full model (CASME-II: 72.51%, SAMM: 63.85%, MMEW: 62.56%).\n\nWhen the SE block is removed (no se), the model slightly improves compared to the a_fixed case but still falls short of the full model (CASME-II: 75.56%, SAMM: 63.97%, MMEW: 63.69%).\n\nWhen simultaneously fixing α and removing the SE block (a_fixed + no se), the model's performance deteriorates drastically, especially on the SAMM dataset (CASME-II: 73.36%, SAMM: 51.25%, MMEW: 58.56%).\n\nIn contrast, the full model with both the learned α weights and the SE block achieves the best performance across all three datasets (CASME-II: 93.95%, SAMM: 80.16%, MMEW: 75.99%). This confirms that both components play an essential role, and combining them optimizes the representational capacity, thereby significantly improving the MER performance.",
      "page_start": 26,
      "page_end": 27
    },
    {
      "section_name": "Conclusion",
      "text": "This paper presents a novel MER method that combines two specially designed dynamic image representations -TRDI and MIDI -with an AFN. Unlike traditional dynamic images, TRDI emphasizes the temporal progression around the apex frame, while MIDI highlights subtle motion intensity cues. By adaptively fusing these two complementary representations via AFN, our method enhances discriminative feature learning while reducing irrelevant noise.\n\nExperiments on three benchmark datasets (CASME-II, SAMM, and MMEW) demonstrate the superiority of the proposed method. In particular, the combination of TRDI and MIDI with AFN achieves 93.95% Accuracy and 89.72% UF1 on CASME-II, setting a new state-of-the-art benchmark. On SAMM and MMEW, the method also delivers competitive results, with significant improvements in UF1, confirming more balanced recognition across expres-sion classes. The ablation studies further demonstrate the necessity of the RFB and the components of MSCAB, thereby emphasizing the effectiveness of the adaptive fusion mechanism and attention.\n\nOverall, the proposed framework not only improves the performance of MER but also provides a robust solution to the challenges of subtle intensity, short duration, and limited data in MEs. In the future, this research can be extended by integrating additional multi-modal signals such as physiological data, and applied to real-world scenarios such as lie detection, affective computing, and human-computer interaction. Ultimately, our contribution not only advances the development of the MER field but also establishes a solid foundation for further innovations and practical applications in affective computing.",
      "page_start": 27,
      "page_end": 28
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Pipeline for MER with AFN via TRDI and MIDI",
      "page": 6
    },
    {
      "caption": "Figure 1: illustrates the workflow of the proposed method, where TRDI",
      "page": 6
    },
    {
      "caption": "Figure 2: a). This indicates that the frames",
      "page": 8
    },
    {
      "caption": "Figure 2: b, 2c). Comparisons show",
      "page": 8
    },
    {
      "caption": "Figure 3: , we compare apex frame, the original DI and the TRDI. It can be",
      "page": 9
    },
    {
      "caption": "Figure 2: Comparison of ME motion intensity with DI and Temporal-ranked function",
      "page": 9
    },
    {
      "caption": "Figure 3: Comparison between the original DI and the proposed TRDI.",
      "page": 10
    },
    {
      "caption": "Figure 4: Comparison between Onset frame, Apex frame and the proposed MIDI.",
      "page": 12
    },
    {
      "caption": "Figure 4: , the MIDI compared with the apex frame and",
      "page": 12
    },
    {
      "caption": "Figure 5: The architecture of AFN",
      "page": 13
    },
    {
      "caption": "Figure 6: , a core compo-",
      "page": 13
    },
    {
      "caption": "Figure 7: is the second core component of AFN,",
      "page": 14
    },
    {
      "caption": "Figure 6: The architecture of Representation fusion block",
      "page": 15
    },
    {
      "caption": "Figure 7: The architecture of MSCAB",
      "page": 16
    },
    {
      "caption": "Figure 8: illustrates the visualization of DIs",
      "page": 18
    },
    {
      "caption": "Figure 9: presents the visualization of MIDI inputs, TRDI, and the",
      "page": 18
    },
    {
      "caption": "Figure 8: The visualization of DIs and TRDIs",
      "page": 19
    },
    {
      "caption": "Figure 9: The visualization of MIDI, TRDI, and feature",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table 1: present the experimental results comparing the performance of",
      "page": 18
    },
    {
      "caption": "Table 1: Comparison of MER performance in terms of Accuracy (%) on the CASME-II,",
      "page": 21
    },
    {
      "caption": "Table 2: MER performance comparison with combined input modalities across CASME-",
      "page": 21
    },
    {
      "caption": "Table 2: clearly show that combining modality approaches",
      "page": 21
    },
    {
      "caption": "Table 3: presents the performance comparison of MER methods on three",
      "page": 22
    },
    {
      "caption": "Table 3: Comparison of MER performance in terms of Accuracy (%) on the CASME-II,",
      "page": 23
    },
    {
      "caption": "Table 4: show that the highest performance is",
      "page": 24
    },
    {
      "caption": "Table 4: MER performance (Accuracy %) on the CASME-II, SAMM, and MMEW datasets",
      "page": 25
    },
    {
      "caption": "Table 5: Comparison of manual fusion, RFB (ours), and ResNet baseline in terms of",
      "page": 25
    },
    {
      "caption": "Table 6: Table 6: Performance comparison under different component configurations of RFB.",
      "page": 26
    },
    {
      "caption": "Table 7: Performance comparison under different component configurations of MSCAB",
      "page": 27
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Facial micro-expressions: An overview",
      "authors": [
        "G Zhao",
        "X Li",
        "Y Li",
        "M Pietikäinen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE",
      "doi": "10.1109/JPROC.2023.3275192"
    },
    {
      "citation_id": "2",
      "title": "Decoding of inconsistent communications",
      "authors": [
        "A Mehrabian",
        "M Wiener"
      ],
      "year": "1967",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "3",
      "title": "An Urban Legend Called: 'The 7/38/55 Ratio Rule",
      "authors": [
        "T Amsel"
      ],
      "year": "2019",
      "venue": "An Urban Legend Called: 'The 7/38/55 Ratio Rule"
    },
    {
      "citation_id": "4",
      "title": "Study of Facial Micro-expressions in Psychology",
      "authors": [
        "B Bhushan"
      ],
      "year": "2015",
      "venue": "Study of Facial Micro-expressions in Psychology",
      "doi": "10.1007/978-81-322-1934-7_13"
    },
    {
      "citation_id": "5",
      "title": "How fast are the leaked facial expressions: The duration of micro-expressions",
      "authors": [
        "W.-J Yan",
        "Q Wu",
        "J Liang",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2013",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "6",
      "title": "Detection and measurement of facial micro-expression characteristics for psychological analysis",
      "authors": [
        "S Polikovsky",
        "Y Kameda",
        "Y Ohta"
      ],
      "year": "2010",
      "venue": "Kameda's Publication"
    },
    {
      "citation_id": "7",
      "title": "Deep learning for microexpression recognition: A survey",
      "authors": [
        "Y Li",
        "J Wei",
        "Y Liu",
        "J Kauttonen",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "Deep learning for microexpression recognition: A survey",
      "arxiv": "arXiv:2107.02823"
    },
    {
      "citation_id": "8",
      "title": "Action recognition with dynamic image networks",
      "authors": [
        "H Bilen",
        "B Fernando",
        "E Gavves",
        "A Vedaldi"
      ],
      "year": "2017",
      "venue": "Action recognition with dynamic image networks",
      "arxiv": "arXiv:1612.00738"
    },
    {
      "citation_id": "9",
      "title": "Non-linearities improve originet based on active imaging for micro expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN48605.2020.9207718"
    },
    {
      "citation_id": "10",
      "title": "Affectivenet: Affective-motion feature learningfor micro expression recognition",
      "authors": [
        "M Verma",
        "S Vipparthi",
        "G Singh"
      ],
      "year": "2021",
      "venue": "Affectivenet: Affective-motion feature learningfor micro expression recognition",
      "arxiv": "arXiv:2104.07569"
    },
    {
      "citation_id": "11",
      "title": "Dynamic texture recognition using local binary patterns with an application to facial expressions",
      "authors": [
        "G Zhao",
        "M Pietikäinen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Lbp with six intersection points: Reducing redundant information in lbp-top for micro-expression recognition",
      "authors": [
        "Y Wang",
        "J See",
        "R -W. Phan",
        "Y.-H Oh"
      ],
      "year": "2014",
      "venue": "Computer Vision -ACCV"
    },
    {
      "citation_id": "13",
      "title": "Efficient spatio-temporal local binary patterns for spontaneous facial micro-expression recognition",
      "authors": [
        "Y Wang",
        "J See",
        "R -W. Phan",
        "Y.-H Oh"
      ],
      "year": "2015",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "14",
      "title": "Spontaneous facial micro-expression detection based on deep learning",
      "authors": [
        "X Li",
        "J Yu",
        "S Zhan"
      ],
      "year": "2016",
      "venue": "IEEE 13th International Conference on Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Micro-expression recognition model based on tv-l1 optical flow method and improved shufflenet",
      "authors": [
        "Y Liu",
        "Y Li",
        "X Yi",
        "Z Hu",
        "H Zhang",
        "Y Liu"
      ],
      "year": "2022",
      "venue": "Scientific Reports",
      "doi": "10.1038/s41598-022-21738-8"
    },
    {
      "citation_id": "16",
      "title": "Microexpression recognition using vgg19 convolutional neural network architecture and random forest",
      "authors": [
        "A Ibrahim",
        "S Ristiawanto",
        "C Setianingsih",
        "B Irawan"
      ],
      "year": "2021",
      "venue": "2021 4th International Symposium on Agents, Multi-Agent Systems and Robotics (ISAMSR)",
      "doi": "10.1109/ISAMSR53229.2021.9567872"
    },
    {
      "citation_id": "17",
      "title": "Face micro-expression recognition algorithm based on resnet depth model",
      "authors": [
        "L Wang",
        "S Zhan"
      ],
      "year": "2023",
      "venue": "2023 8th International Conference on Intelligent Computing and Signal Processing",
      "doi": "10.1109/ICSP58490.2023.10248754"
    },
    {
      "citation_id": "18",
      "title": "Inceptr: micro-expression recognition integrating inception-cbam and vision transformer",
      "authors": [
        "H Zhou",
        "S Huang",
        "Y Xu"
      ],
      "year": "2023",
      "venue": "Multimedia Systems",
      "doi": "10.1007/s00530-023-01164-0"
    },
    {
      "citation_id": "19",
      "title": "Investigating lstm for micro-expression recognition",
      "authors": [
        "M Bai",
        "R Goecke"
      ],
      "year": "2020",
      "venue": "Investigating lstm for micro-expression recognition",
      "doi": "10.1145/3395035.3425248"
    },
    {
      "citation_id": "20",
      "title": "Micro-expression recognition based on 3d flow convolutional neural network",
      "authors": [
        "J Li",
        "Y Wang",
        "J See",
        "W Liu"
      ],
      "year": "2019",
      "venue": "Pattern Analysis and Applications",
      "doi": "10.1007/s10044-018-0757-5"
    },
    {
      "citation_id": "21",
      "title": "3dfcnn: Real-time action recognition using 3d deep neural networks with raw depth information",
      "authors": [
        "A Sanchez-Caballero",
        "S De López-Diz",
        "D Fuentes-Jimenez",
        "C Losada-Gutiérrez",
        "M Marrón-Romera",
        "D Casillas-Perez",
        "M Sarker"
      ],
      "year": "2020",
      "venue": "3dfcnn: Real-time action recognition using 3d deep neural networks with raw depth information",
      "arxiv": "arXiv:2006.07743"
    },
    {
      "citation_id": "22",
      "title": "Micro-expression recognition based on capsule network",
      "authors": [
        "L Qing",
        "F Huang"
      ],
      "year": "2024",
      "venue": "Micro-expression recognition based on capsule network",
      "doi": "10.1109/CSIS-IAC63491.2024.10919257"
    },
    {
      "citation_id": "23",
      "title": "Micro-expression recognition via cnn and multi-path vision transformer integrated with spatiotemporal separated self-attention",
      "authors": [
        "Y Guo",
        "T Xie",
        "W Jia",
        "S Xu",
        "X Ben"
      ],
      "year": "2025",
      "venue": "Biometric Recognition"
    },
    {
      "citation_id": "24",
      "title": "Investigating lstm for micro-expression recognition",
      "authors": [
        "M Bai",
        "R Goecke"
      ],
      "year": "2020",
      "venue": "Investigating lstm for micro-expression recognition",
      "doi": "10.1145/3395035.3425248"
    },
    {
      "citation_id": "25",
      "title": "Micro-expression recognition based on multi-scale 3d residual convolutional neural network",
      "authors": [
        "H Jin",
        "N He",
        "Z Li",
        "P Yang"
      ],
      "year": "2024",
      "venue": "Mathematical Biosciences and Engineering",
      "doi": "10.3934/mbe.2024221"
    },
    {
      "citation_id": "26",
      "title": "Modeling video evolution for action recognition",
      "authors": [
        "B Fernando",
        "E Gavves",
        "J Oramas",
        "A Ghodrati",
        "T Tuytelaars"
      ],
      "year": "2015",
      "venue": "Modeling video evolution for action recognition"
    },
    {
      "citation_id": "27",
      "title": "Rank pooling for action recognition",
      "authors": [
        "B Fernando",
        "E Gavves",
        "J Oramas",
        "A Ghodrati",
        "T Tuytelaars"
      ],
      "year": "2016",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "28",
      "title": "CASME II: an improved spontaneous micro-expression database and the baseline evaluation",
      "authors": [
        "W.-J Yan",
        "X Li",
        "S.-J Wang",
        "G Zhao",
        "Y.-J Liu",
        "Y.-H Chen",
        "X Fu"
      ],
      "year": "2014",
      "venue": "PLoS One"
    },
    {
      "citation_id": "29",
      "title": "Samm: A spontaneous micro-facial movement dataset",
      "authors": [
        "A Davison",
        "C Lansley",
        "N Costen",
        "K Tan",
        "M Yap"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2016.2573832"
    },
    {
      "citation_id": "30",
      "title": "Video-based facial micro-expression analysis: A survey of datasets, features and algorithms",
      "authors": [
        "X Ben",
        "Y Ren",
        "J Zhang",
        "S.-J Wang",
        "K Kpalma",
        "W Meng",
        "Y.-J Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2021.3067464"
    },
    {
      "citation_id": "31",
      "title": "Dynamic stereotype theory induced micro-expression recognition with oriented deformation",
      "authors": [
        "B Zhang",
        "X Wang",
        "C Wang",
        "G He"
      ],
      "year": "2025",
      "venue": "2025 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR52734.2025.01000"
    },
    {
      "citation_id": "32",
      "title": "Micro-expression recognition by fusing action unit detection and spatio-temporal features",
      "authors": [
        "L Wang",
        "P Huang",
        "W Cai",
        "X Liu"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP48485.2024.10446702"
    },
    {
      "citation_id": "33",
      "title": "Nonlinear deep subspace network for micro-expression recognition",
      "authors": [
        "W Feng",
        "M Xu",
        "Y Chen",
        "X Wang",
        "J Guo",
        "L Dai",
        "N Wang",
        "X Zuo",
        "X Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 3rd Workshop on Facial Micro-Expression: Advanced Techniques for Multi-Modal Facial Expression Analysis, FME '23",
      "doi": "10.1145/3607829.3616444"
    },
    {
      "citation_id": "34",
      "title": "Muscle motion-guided network for micro-expression recognition",
      "authors": [
        "H Li",
        "M Sui",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2022",
      "venue": "Muscle motion-guided network for micro-expression recognition",
      "arxiv": "arXiv:2201.05297"
    },
    {
      "citation_id": "35",
      "title": "Key facial components guided microexpression recognition based on first second-order motion",
      "authors": [
        "Y Su",
        "J Zhang",
        "J Liu",
        "G Zhai"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Multimedia and Expo (ICME)",
      "doi": "10.1109/ICME51207.2021.9428407"
    },
    {
      "citation_id": "36",
      "title": "A novel graph-tcn with a graph structured representation for micro-expression recognition",
      "authors": [
        "L Lei",
        "J Li",
        "T Chen",
        "S Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, MM '20",
      "doi": "10.1145/3394171.3413714"
    },
    {
      "citation_id": "37",
      "title": "Temporal and spatial feature fusion framework for dynamic micro expression recognition",
      "authors": [
        "F Liu",
        "B Nan",
        "X Qian",
        "X Fu"
      ],
      "year": "2025",
      "venue": "Temporal and spatial feature fusion framework for dynamic micro expression recognition",
      "doi": "10.48550/arXiv.2505.16372"
    },
    {
      "citation_id": "38",
      "title": "A dual stream spatio-temporal deep network for micro-expression recognition using upper facial features",
      "authors": [
        "N Matharaarachchi",
        "M Pasha"
      ],
      "year": "2025",
      "venue": "Neural Computing and Applications",
      "doi": "10.1007/s00521-024-10374-0"
    },
    {
      "citation_id": "39",
      "title": "C3dbed: Facial micro-expression recognition with three-dimensional convolutional neural network embedding in transformer model",
      "authors": [
        "H Pan",
        "L Xie",
        "Z Wang"
      ],
      "year": "2023",
      "venue": "Engineering Applications of Artificial Intelligence",
      "doi": "10.1016/j.engappai.2023.106258"
    },
    {
      "citation_id": "40",
      "title": "A novel microexpression recognition approach using attention-based magnificationadaptive networks",
      "authors": [
        "M Wei",
        "W Zheng",
        "Y Zong",
        "X Jiang",
        "C Lu",
        "J Liu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP43922.2022.9747232"
    },
    {
      "citation_id": "41",
      "title": "A two-stage 3d cnn based learning method for spontaneous micro-expression recognition",
      "authors": [
        "S Zhao",
        "H Tao",
        "Y Zhang",
        "T Xu",
        "K Zhang",
        "Z Hao",
        "E Chen"
      ],
      "year": "2021",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2021.03.058"
    },
    {
      "citation_id": "42",
      "title": "Joint estimation of micro-expression, optical flow, and landmark via transformergraph-style convolution",
      "authors": [
        "Z Shao",
        "Y Cheng",
        "F Li",
        "Y Zhou",
        "X Lu",
        "Y Xie",
        "L Ma"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2025.3581162"
    },
    {
      "citation_id": "43",
      "title": "Multi-task mid-level feature learning for micro-expression recognition",
      "authors": [
        "J He",
        "J.-F Hu",
        "X Lu",
        "W.-S Zheng"
      ],
      "year": "2017",
      "venue": "Pattern Recognition",
      "doi": "10.1016/j.patcog.2016.11.029"
    },
    {
      "citation_id": "44",
      "title": "Learning from hierarchical spatiotemporal descriptors for micro-expression recognition",
      "authors": [
        "Y Zong",
        "X Huang",
        "W Zheng",
        "Z Cui",
        "G Zhao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2018.2820321"
    },
    {
      "citation_id": "45",
      "title": "Sparse mdmo: Learning a discriminative feature for micro-expression recognition",
      "authors": [
        "Y.-J Liu",
        "B.-J Li",
        "Y.-K Lai"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2018.2854166"
    },
    {
      "citation_id": "46",
      "title": "A main directional mean optical flow feature for spontaneous micro-expression recognition",
      "authors": [
        "Y.-J Liu",
        "J.-K Zhang",
        "W.-J Yan",
        "S.-J Wang",
        "G Zhao",
        "X Fu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2485205"
    },
    {
      "citation_id": "47",
      "title": "Dual temporal scale convolutional neural network for micro-expression recognition",
      "authors": [
        "M Peng",
        "C Wang",
        "T Chen",
        "G Liu",
        "X Fu"
      ],
      "year": "2017",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "48",
      "title": "Micro-expression recognition with small sample size by transferring long-term convolutional neural network",
      "authors": [
        "S.-J Wang",
        "B.-J Li",
        "Y.-J Liu",
        "W.-J Yan",
        "X Ou",
        "X Huang",
        "F Xu",
        "X Fu"
      ],
      "year": "2018",
      "venue": "Neurocomputing",
      "doi": "10.1016/j.neucom.2018.05.107"
    }
  ]
}