{
  "paper_id": "2310.17194v1",
  "title": "Privacy-Preserving Representation Learning For Speech Understanding",
  "published": "2023-10-26T07:20:23Z",
  "authors": [
    "Minh Tran",
    "Mohammad Soleymani"
  ],
  "keywords": [
    "privacy and security in speech communication",
    "privacy-preserving representation learning",
    "transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Existing privacy-preserving speech representation learning methods target a single application domain. In this paper, we present a novel framework to anonymize utterance-level speech embeddings generated by pre-trained encoders and show its effectiveness for a range of speech classification tasks. Specifically, given the representations from a pre-trained encoder, we train a Transformer to estimate the representations for the same utterances spoken by other speakers. During inference, the extracted representations can be converted into different identities to preserve privacy. We compare the results with the voice anonymization baselines from the VoicePrivacy 2022 challenge. We evaluate our framework on speaker identification for privacy and emotion recognition, depression classification, and intent classification for utility. Our method outperforms the baselines on privacy and utility in paralinguistic tasks and achieves comparable performance for intent classification.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Privacy-preserving data processing is an actively researched topic due to its societal significance that motivated the introduction of privacy protection legislation such as the General Data Protection Regulation (GDPR). Since speech data can disclose personally identifiable information such as age  [1] , gender  [1] , and race  [2] , it is important to develop privacy-preserving technologies for speech processing.\n\nDistributed learning, encryption, and anonymization are the well-known approaches for privacy preservation. While distributed training solutions such as federated learning preserve privacy by only sharing models' updates, the shared gradients can be vulnerable to privacy attacks  [3] . Encryption methods lock the data in such a way that makes it unusable until decrypted  [4, 5]  but are computationally expensive. With lower computational requirements, anonymization solutions aim at removing sensitive information from speech signals while preserving everything else. Generally, existing anonymization studies focus on voice anonymization, which alters the voice of the original speaker to hide the speaker's identity while leaving the remaining information intact  [6, 7, 8, 9, 10] . However, for supervised machine learning purposes, an alternative approach of anonymizing speech at the feature level is under-explored. Because the speech utterance has to be converted to feature representations at some point during the training process, it is possible to locally extract and sanitize the representations and only release the privacy-aware representations instead of a speech utterance. As the features extracted from pre-trained encoders are useful for a wide range of downstream tasks, the anonymized representations are expected to be equally universal.\n\nIn this paper, we propose a novel framework to anonymize (remove identifiable information) features extracted using a pretrained speech encoder (HuBERT  [11] ) while preserving other information useful to a wide range of downstream tasks. As an initial step towards feature-level anonymization, we only focus on sanitizing the mean-pooled vector representations from pretrained encoders, which are commonly used for utterance-level classification tasks. Our problem formulation is as follows: Given a vector z extracted for an utterance u via a speech encoder E, we want to develop a method M such that z = M (z) remains useful for different types of downstream tasks while reducing speaker identifiable information. Our method first trains a Transformer-based  [12]  representation learning model that learns to convert the mean-pooled embedding extracted from a source utterance to the embedding of the same utterance spoken by a different speaker, using the VCTK voice cloning database  [13] . During inference, it converts the input embeddings into randomly selected speakers to preserve the speakers' identities. We evaluate our method on a speaker identification task with the VoxCeleb1 dataset  [14]  for privacy. For utility, we evaluate our method on emotion recognition (paralinguistic), depression detection (paralinguistic) and intent classification (semantic). Our method is able to outperform the voice anonymization baselines from the VoicePrivacy 2022 Challenge  [6]  on the privacy metric and paralinguistic tasks, while achieving comparable performance for intent classification. We further show that our ap-  A privacy Transformer is trained to make the features identity-conditioned while preserving their utility for downstream tasks. During inference the Privacy Transformer can be used to sanitize features from identifiable information, thus, preserving privacy.\n\nproach is computationally efficient compared to the baselines in terms of runtime and memory usage.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "Self-supervised representation learning Self-supervised representation is effective for extracting high-level features from speech signals by learning the underlying structure of the inputs in a self-supervised manner. There are three lines of selfsupervised representation learning for speech: generative, contrastive, and predictive. Generative methods generally try to reconstruct masked frames  [15]  or predict future frames  [16] . Contrastive methods compare pairs of sampled frames with contrastive losses  [17, 18] . In this work, we focus on sanitizing features produced by HuBERT  [11] , which is an instance of predictive methods that predict the pseudo-labels for masked frames. The discrete pseudo-labels for HuBERT are created by applying K-means clustering on extracted MFCC features. Voice anonymization Voice anonymization methods tend to follow two lines of work, namely voice conversion and voice modification. While voice conversion-based solutions deal with changing the x-vector  [19]  for an input utterance via speech synthesis , voice modification-based solutions directly perturb the waveforms using speech processing techniques. Yoo et al.  [8]  propose using many-to-many voice conversion technique based on Variational Auto-encoders (VAE) to extract and modify speaker identity vectors. Qian et al.  [9]  combine sensitive keyword substitution with voice conversion to reduce speaker recognition accuracy. Recently, the VoicePrivacy 2022 Challenge releases its first baseline based on x-vector modification, in combination with the fundamental frequency, bottleneck features, and a speech synthesis component, to produce anonymized speech  [6] . For voice modification, Kai et al.  [7]  explore various lightweight speech processing methods such as vocal length normalizing, clipping, and resampling to achieve anonymization. The second baseline of the VoicePrivacy 2022 Challenge uses McAdams coefficients  [20]  to shift the pole positions extracted from linear predictive coding analysis for voice anonymization. Privacy-aware representation learning Prior work on privacy-preserving representation learning for speech signals generally focus on a single target application. Srivastava et al. use adversarial training to learn representations that is useful for automatic speech recognition (ASR) without disclosing speaker identity  [21] . Jaiswal et al.  [22]  use Gradient Reversal Layer  [23]  to learn gender-invariant features that perform well for emotion recognition. To the best of our knowledge, there has been no prior work exploring privacy-preserving representations that perform well on multiple applications.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "Figure  2  shows an overview of our proposed method. At the core of our approach is a Privacy Transformer that converts the identity of the extracted representations from a pre-trained encoder to achieve privacy. In particular, we train our Privacy Transformer with a Voice Cloning dataset that provides speech utterances of different speakers reading a fixed set of sentences. Given a pair of utterances with the same content, our Transformer learns to predict the embedding of the target utterance (speaker 2) given the embedding of the source utterance (speaker 1) and the target speaker. During inference, we use the trained Privacy Transformer to convert extracted embeddings to randomly selected identities to produce privacy-preserving representations that are suitable for different downstream tasks. Pretrained encoder. Although our method is compatible to different speech encoders, we focus our analysis on the HuBERTbase representation learning model  [11]  in this work. The architecture of HuBERT consists of a Convolutional-based feature extractor to convert raw waveforms into low-level features, followed by a Transformer encoder  [12]  to generate high-level representations. During training, the model first generates pseudolabels from input audio signals with k-means clustering on extracted MFCC features. The model is then optimized in a selfsupervised manner by predicting the masked pseudo-labels.\n\nThe Transformer encoder within the HuBERT-base model has L = 12 layers with an embedding dimension of d = 768. Because features from different layers of HuBERT contain different information  [24] , we extract features and temporally mean-pool them from all 12 layers of HuBERT. Hence, each utterance can be represented as a matrix of size L × d. Privacy Transformer. The Privacy Transformer P is fed by the mean-pooled features from all 12-layers of HuBERT for the input utterances in addition to the target speakers (discrete IDs). It consists of three main components, namely, embedding layers P emb , a Transformer encoder Pte and a fully-connected layer P f c . We use two trainable embedding layers, namely P spk emb and P layer emb , to convert discrete target speakers and layer information (layer 1 to 12) into vectors of size d spk and dL, respectively. The speakers and layer embeddings are then concatenated with the HuBERT embeddings to form inputs of size bsz × L × (d + d spk + dL) for the Transformer encoder. Our Transformer encoder follows closely the original architecture from  [12]  that consists of LP stacks of self-attention layers followed by a feed-forward neural network. However, we do not use the positional encoding as our inputs are permutationinvariant. The goal of the Transformer encoder is to model the relationship between embeddings of different layers (but the same content) to produce high-level layer-aware contextualized embeddings. Finally, we use a fully-connected layer to map the output dimension back to d. Although having a Transformer encoder to post-process the extracted representations might seem computationally expensive at first, P can efficiently process its inputs due to the lack of the temporal dimension, which we later demonstrate via experimental results in the discussion section. Learning scheme. In each learning step, N pairs of utterances from different speakers with the same content, i.e., {(u i 1 , spk i 1 )} N i=1 and {(u i 2 , spk i 2 )} N i=1 , are sampled from the Voice Cloning dataset (with speakers pool S), and the HuBERT encoder E extracts the feature embeddings for each utterance\n\nThe Privacy Transformer P takes as inputs zsrc along with the target speakers spktgt = {spk i 2 } N i=1 and generate estimations ztgt for the target representations ztgt.\n\n(2) where ⊕ denotes the concatenation operation with z spk = P spk emb (spktgt) ; z layer = P layer emb (layers) (3) for layers = 1..L . To achieve this goal, the Privacy Transformer is trained with a mean-squared error loss\n\nInference scheme. During inference, we want to anonymize a set of arbitrary utterances (from unseen speakers), i.e., {u i } N i=1 , without any information about the speaker identities. We use E to extract z = {E(u i )} N i=1 features from these utterances. We randomly select target speakers for the input utterances from the speakers pool S. Here, we allow different layers to have different target speakers because there is no ground-truth target speaker as in the learning process.\n\nThe outputs z from the Privacy Transformer is the sanitized representations produced by our method.\n\nWe then use z for different downstream tasks for privacy and utility evaluations.\n\nChoice of the Pretrained encoder and Privacy module. Although the experiments presented in this paper are limited to one encoder architecture (HuBERT  [11] ) and one Privacy model architecture (Transformer  [12] ), the method can be applied to different combinations of pre-trained encoder and privacy module architectures because the roles of the modules are architecture-independent. Specifically, the encoder needs to extract a robust representation of the input speech for a wide range of downstream tasks, and the Privacy module needs to produce speaker-conditioned vector representations, given the extracted features and the target speaker IDs. We chose HuBERT as our feature encoder due to its superior performance and a Transformer as our privacy module due to its ubiquitous usage. To demonstrate the usefulness of our method to Transformer-free encoders, we provide additional experimental results with the APC pre-trained model  [25]  in the supplementary materials.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets Cstr Vctk",
      "text": "The CSTR VCTK dataset  [13]  includes speech data utters by 110 English speakers of various accents. Each speaker reads from a fixed set of about 400 sentences collected from different sources such as a newspaper, a rainbow passage and an elicitation paragraph. For preprocessing, we downsample the 48kHz audio recordings provided by the dataset to 16kHz to be compatible with HuBERT feature extraction.\n\nVoxCeleb1 We use the VoxCeleb1 dataset  [14]  to report the speaker identification (SID) performance, our privacy metric. The dataset contains over 145K utterances from 1541 speakers, collected from more than 21K YouTube videos. We follow the standard train/test splits provided by the dataset with 10% of the train set randomly selected for validation.\n\nIt is important to note that prior studies generally use (semiinformed) speaker verification models to demonstrate robustness to attacks  [26, 6] . However, it is rather difficult for us to construct a robust speaker verification model given that our inputs are 1D vector representations of speech. Therefore, we report the SID performance in this work, as a speaker verificationbased attack is more appropriate for future work on anonymizing the entire speech representations with a temporal dimension. IEMOCAP The IEMOCAP dataset  [27]  is a commonly used dataset for multi-modal emotion recognition with approximately 12 hours of recordings from 10 speakers. The dataset is collected in 5 sessions, in which participants are asked to act according to scripts that are designed to invoke emotions. Following prior work  [28] , We focus on the four basic emotions (happy, sad, angry, and neutral) to avoid severe class imbalance, which contains 5,536 utterances. MSP-IMPROV The MSP-IMPROV dataset  [29]  is an acted audio-visual emotional dataset that contains acted and improvised emotional dyadic interactions from 12 speakers over 6 sessions. In total, the dataset contains around 8.5K utterances (over 9 hours). Similar to IEMOCAP, we drop the unbalanced emotion classes and focus on the four basic emotion, which reduce the number of utterances to 7,735. Extended-DAIC The E-DAIC dataset  [30]  contains 219 video recordings of clinical interviews probing for symptoms of psychological distress. It was used as part of the 2019 Audio-Visual Emotion Challenge Workshop (AVEC2019)  [31]  in the Detecting Depression sub-challenge. Each interview, ranging from 15 to 25 minutes, is rated according to the eight-item Patient Health Questionaire (PHQ-8) along with a binary label for depression conditions. We use the provided transcripts with the dataset to extract 18,846 utterances, from which around 33% are labeled as having depression. Fluent Speech Commands The Fluent Speech Commands dataset is a widely used dataset for the semantic task of intent classification. It contains around 30,000 utterances from 97 speakers with 31 unique intents used for controlling smarthome appliances and virtual assistants.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baselines",
      "text": "We use the two baselines provided by the VoicePrivacy 2022 Challenge  [3] . Baseline 1 is based on the voice anonymization architecture proposed in  [32] . The method first extracts relevant features from the input speech signal, including the xvector  [19] , the fundamental frequency, and bottleneck features. The extracted x-vector is then anonymized with an external pool of x-vectors, and the anonymized speech is synthesized with a speech synthesis model based on the anonymized x-vector. We use a variant of the first baseline, in which the bottleneck features are replaced with representations produced by a finetuned wav2vec 2.0 model  [17]  and the speech synthesis module is HifiGAN-based  [33] . Baseline 2 is based on McAdams transformation  [10]  with a uniformly sampled McAadam coefficient. We add Baseline 3 based on Differential Privacy  [34]  with the mean-pooled embeddings. In particular, the embeddings extracted from HuBERT are clipped to [-1, 1] and Laplacian noise ∼ Lap(  2 ϵ ) is added independently to all dimensions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Implementation Details",
      "text": "For the Privacy Transformer, we set the size of the speaker embeddings d spk = 256 and size of the layer embedding dL = 128. Our Privacy Transformer contains of LP = 5 stacks of Transformer encoder layers with h = 8 self-attention heads and an intermediate size of 4608. We train our Privacy Transformer with an SGD optimizer with a learning rate of 0.001 for 50 epochs. During the evaluation, our classification model consists of a featurizer (12 learnable scalars) to fuse information from all 12 layers of the extracted features from HuBERT, followed by 2 fully-connected layers of sizes {256, 128}. For a fair comparison, all methods are trained using the Adam optimizer with a learning rate of 1e -3 for 50 epochs with early stopping. For baselines 1 and 2, we extract HuBERT features after the original utterances are converted to privacy-preserving utterances.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "Table  1  shows the performance comparisons between our method with the baselines. Since Baseline 3 allows flexibility in terms of privacy and utility trade-offs, we adjust the amount of added noise (ϵ = 15) such that the Speaker Identification accuracy is similar to our method. We also report the original performance of HuBERT (without any privacy-preserving processing) in the first row of Table  1 . Surprisingly, Baseline 3 achieves a very competitive performance on both privacy and utility metrics compared to the voice anonymization approaches despite involving minimal post-processing. Our method achieves similar speaker identification accuracy compared to the most secure baseline (11.64% vs. 11.72%) while outperforming all of the baselines on paralinguistic tasks. Specifically, for emotion recognition, our method achieves F1-scores of 64.97% (IEMO-CAP) and 54.33% (MSP-IMPROV) compared to 63.35% and 49.54% for Baseline 2. For depression detection, our approach also outperforms Baseline 2 with a margin of 1.23% on the F1 score. For intent classification (a semantics task), the proposed approach achieves comparable classification accuracy with the best-performing baseline (92.91% vs. 93.37%). The results indicate feature-level anonymization's potential compared to voice anonymization for supervised ML tasks.\n\nBecause anonymization techniques are developed to run locally with limited computation, we compare our method with the baselines in terms of runtime and memory usage. In particular, we select a fixed set of 500 utterances from the Vox-Celeb1 dataset  [14]  and perform anonymization on four CPUs with a batch size of 1. Table  2  shows the efficiency comparisons between our method and the baselines. We also report the computational resources for independently extracting HuBERT features from the selected utterances in the first row. Adding the Privacy Transformer post-processing increases the computation costs by 18% in runtime and 51% in memory usage compared to HuBERT Extr (extract HuBERT features from speech without any post-processing). Our method achieves superior efficiency performance compared to Baseline 1 (the more secured voice anonymization baseline) and comparable runtime to Baseline 2. Although Baseline 2 achieves superior efficiency in terms of memory usage, the method is the weakest privacy-preserving baseline and also suffers from poor performance on the semantic task. It is worth noting that the performance of our method is highly dependent on the efficiency of the speech encoders, and hence, there is still room to further boost the efficiency with light encoders such as the Distill-HuBERT  [24] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We present a novel framework to anonymize mean-pooled vector speech embeddings extracted from pre-trained HuBERT. We train a Privacy Transformer to convert the extracted embeddings into different identities while preserving the content using a voice cloning dataset. We compare our method with the VoicePrivacy 2022 Challenge anonymization baseline methods on the following tasks: speaker identification (privacy), emotion recognition (utility), depression recognition (utility), and intent classification (utility). Our method achieves superior privacy preservation performance and outperforms the baselines on paralinguistic tasks. We further show that our method is computationally efficient, compared to the baselines.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgement",
      "text": "Research was sponsored by the Army Research Office and was accomplished under Cooperative Agreement Number W911NF-20-2-0053. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Existing anonymization approaches focus on voice",
      "page": 1
    },
    {
      "caption": "Figure 2: An overview of the proposed method. A privacy Transformer is trained to make the features identity-conditioned while",
      "page": 2
    },
    {
      "caption": "Figure 2: shows an overview of our proposed method. At the",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Existing\nprivacy-preserving\nspeech\nrepresentation\nlearning"
        },
        {
          "Abstract": "methods target a single application domain.\nIn this paper, we"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "present a novel framework to anonymize utterance-level speech"
        },
        {
          "Abstract": "embeddings generated by pre-trained encoders and show its ef-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "fectiveness for a range of speech classification tasks.\nSpecifi-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "cally, given the representations from a pre-trained encoder, we"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "train a Transformer to estimate the representations for the same"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "utterances spoken by other speakers. During inference,\nthe ex-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "tracted representations can be converted into different\nidenti-"
        },
        {
          "Abstract": "ties to preserve privacy. We compare the results with the voice"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "anonymization baselines from the VoicePrivacy 2022 challenge."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "We evaluate our framework on speaker identification for privacy"
        },
        {
          "Abstract": "and emotion recognition, depression classification, and intent"
        },
        {
          "Abstract": "classification for utility. Our method outperforms the baselines"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "on privacy and utility in paralinguistic tasks and achieves com-"
        },
        {
          "Abstract": "parable performance for intent classification."
        },
        {
          "Abstract": "Index Terms: privacy and security in speech communication,"
        },
        {
          "Abstract": "privacy-preserving representation learning, transformer"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "1.\nIntroduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Privacy-preserving data processing is\nan actively researched"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "topic due to its societal significance that motivated the introduc-"
        },
        {
          "Abstract": "tion of privacy protection legislation such as the General Data"
        },
        {
          "Abstract": "Protection Regulation (GDPR). Since speech data can disclose"
        },
        {
          "Abstract": "personally identifiable information such as age [1], gender [1],"
        },
        {
          "Abstract": "and race [2], it is important to develop privacy-preserving tech-"
        },
        {
          "Abstract": "nologies for speech processing."
        },
        {
          "Abstract": "Distributed learning, encryption, and anonymization are the"
        },
        {
          "Abstract": "well-known approaches\nfor privacy preservation. While dis-"
        },
        {
          "Abstract": "tributed training solutions such as federated learning preserve"
        },
        {
          "Abstract": "privacy by only sharing models’ updates,\nthe shared gradients"
        },
        {
          "Abstract": "can be vulnerable to privacy attacks [3]. Encryption methods"
        },
        {
          "Abstract": "lock the data in such a way that makes\nit unusable until de-"
        },
        {
          "Abstract": "crypted [4, 5] but are computationally expensive. With lower"
        },
        {
          "Abstract": "computational requirements, anonymization solutions aim at re-"
        },
        {
          "Abstract": "moving sensitive information from speech signals while pre-"
        },
        {
          "Abstract": "serving everything else.\nGenerally,\nexisting anonymization"
        },
        {
          "Abstract": "studies focus on voice anonymization, which alters the voice of"
        },
        {
          "Abstract": "the original speaker to hide the speaker’s identity while leaving"
        },
        {
          "Abstract": "the remaining information intact [6, 7, 8, 9, 10]. However, for"
        },
        {
          "Abstract": "supervised machine learning purposes, an alternative approach"
        },
        {
          "Abstract": "of anonymizing speech at\nthe feature level\nis under-explored."
        },
        {
          "Abstract": "Because the speech utterance has to be converted to feature rep-"
        },
        {
          "Abstract": "resentations at some point during the training process, it is pos-"
        },
        {
          "Abstract": "sible to locally extract and sanitize the representations and only"
        },
        {
          "Abstract": "release the privacy-aware representations instead of a speech ut-"
        },
        {
          "Abstract": "terance. As the features extracted from pre-trained encoders are"
        },
        {
          "Abstract": "useful\nfor a wide range of downstream tasks,\nthe anonymized"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the proposed method.\nFigure 2: An overview of": "preserving their utility for downstream tasks. During inference the Privacy Transformer can be used to sanitize features from identifiable",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "information, thus, preserving privacy.",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "proach is computationally efficient compared to the baselines in",
          "A privacy Transformer is trained to make the features identity-conditioned while": "has been no prior work exploring privacy-preserving represen-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "terms of runtime and memory usage.",
          "A privacy Transformer is trained to make the features identity-conditioned while": "tations that perform well on multiple applications."
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "3. Proposed Method"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "2. Related Work",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "Figure 2 shows an overview of our proposed method. At\nthe"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "Self-supervised representation learning Self-supervised rep-",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "core of our approach is a Privacy Transformer\nthat converts"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "resentation is effective for extracting high-level\nfeatures from",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "the identity of the extracted representations from a pre-trained"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "speech signals by learning the underlying structure of\nthe in-",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "encoder\nto achieve privacy.\nIn particular, we\ntrain our Pri-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "puts in a self-supervised manner. There are three lines of self-",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "vacy Transformer with a Voice Cloning dataset\nthat provides"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "supervised representation learning for speech: generative, con-",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "speech utterances of different speakers reading a fixed set of"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "trastive, and predictive.\nGenerative methods generally try to",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "sentences. Given a pair of utterances with the same content, our"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "reconstruct masked frames [15] or predict\nfuture frames [16].",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "Transformer learns to predict the embedding of the target utter-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "Contrastive methods\ncompare pairs of\nsampled frames with",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "ance (speaker 2) given the embedding of\nthe source utterance"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "contrastive losses [17, 18].\nIn this work, we focus on sanitiz-",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "(speaker 1) and the target speaker. During inference, we use the"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "ing features produced by HuBERT [11], which is an instance",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "trained Privacy Transformer to convert extracted embeddings to"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "of predictive methods that predict the pseudo-labels for masked",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "randomly selected identities to produce privacy-preserving rep-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "frames. The discrete pseudo-labels for HuBERT are created by",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "resentations that are suitable for different downstream tasks."
        },
        {
          "the proposed method.\nFigure 2: An overview of": "applying K-means clustering on extracted MFCC features.",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "Pretrained encoder. Although our method is compatible to dif-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "Voice anonymization Voice anonymization methods\ntend to",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "ferent speech encoders, we focus our analysis on the HuBERT-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "follow two lines of work, namely voice conversion and voice",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "base representation learning model [11] in this work. The archi-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "modification. While voice conversion-based solutions deal with",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "tecture of HuBERT consists of a Convolutional-based feature"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "changing the x-vector\n[19]\nfor an input utterance via speech",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "extractor to convert raw waveforms into low-level features, fol-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "synthesis , voice modification-based solutions directly perturb",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "lowed by a Transformer encoder [12] to generate high-level rep-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "the waveforms using speech processing techniques. Yoo et al.",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "resentations. During training, the model first generates pseudo-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "[8] propose using many-to-many voice conversion technique",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "labels from input audio signals with k-means clustering on ex-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "based on Variational Auto-encoders (VAE) to extract and mod-",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "tracted MFCC features. The model\nis then optimized in a self-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "ify speaker identity vectors. Qian et al.\n[9] combine sensitive",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "",
          "A privacy Transformer is trained to make the features identity-conditioned while": "supervised manner by predicting the masked pseudo-labels."
        },
        {
          "the proposed method.\nFigure 2: An overview of": "keyword substitution with voice conversion to reduce speaker",
          "A privacy Transformer is trained to make the features identity-conditioned while": ""
        },
        {
          "the proposed method.\nFigure 2: An overview of": "recognition accuracy.\nRecently,\nthe VoicePrivacy 2022 Chal-",
          "A privacy Transformer is trained to make the features identity-conditioned while": "The Transformer encoder within the HuBERT-base model"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "lenge\nreleases\nits first baseline based on x-vector modifica-",
          "A privacy Transformer is trained to make the features identity-conditioned while": "has L = 12 layers with an embedding dimension of d = 768."
        },
        {
          "the proposed method.\nFigure 2: An overview of": "tion,\nin combination with the fundamental\nfrequency, bottle-",
          "A privacy Transformer is trained to make the features identity-conditioned while": "Because features from different layers of HuBERT contain dif-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "neck features, and a speech synthesis component,\nto produce",
          "A privacy Transformer is trained to make the features identity-conditioned while": "ferent\ninformation [24], we\nextract\nfeatures\nand temporally"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "anonymized speech [6]. For voice modification, Kai et al.\n[7]",
          "A privacy Transformer is trained to make the features identity-conditioned while": "mean-pool\nthem from all 12 layers of HuBERT. Hence, each"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "explore various lightweight speech processing methods such as",
          "A privacy Transformer is trained to make the features identity-conditioned while": "utterance can be represented as a matrix of size L × d."
        },
        {
          "the proposed method.\nFigure 2: An overview of": "vocal\nlength normalizing, clipping, and resampling to achieve",
          "A privacy Transformer is trained to make the features identity-conditioned while": "Privacy Transformer. The Privacy Transformer P is fed by the"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "anonymization. The second baseline of the VoicePrivacy 2022",
          "A privacy Transformer is trained to make the features identity-conditioned while": "mean-pooled features from all 12-layers of HuBERT for the in-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "Challenge uses McAdams coefficients [20] to shift the pole po-",
          "A privacy Transformer is trained to make the features identity-conditioned while": "put utterances in addition to the target speakers (discrete IDs). It"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "sitions extracted from linear predictive coding analysis for voice",
          "A privacy Transformer is trained to make the features identity-conditioned while": "consists of three main components, namely, embedding layers"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "anonymization.",
          "A privacy Transformer is trained to make the features identity-conditioned while": "Pemb, a Transformer encoder Pte and a fully-connected layer"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "Privacy-aware\nrepresentation\nlearning\nPrior\nwork\non",
          "A privacy Transformer is trained to make the features identity-conditioned while": "Pf c. We use two trainable embedding layers, namely P spk\nemb"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "privacy-preserving representation learning for\nspeech signals",
          "A privacy Transformer is trained to make the features identity-conditioned while": "and P layer\n,\nto convert discrete target\nspeakers and layer\nin-\nemb"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "generally focus on a single target application.\nSrivastava et",
          "A privacy Transformer is trained to make the features identity-conditioned while": "formation (layer 1 to 12) into vectors of size dspk and dL, re-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "al. use adversarial\ntraining to learn representations that\nis use-",
          "A privacy Transformer is trained to make the features identity-conditioned while": "spectively.\nThe speakers and layer embeddings are then con-"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "ful for automatic speech recognition (ASR) without disclosing",
          "A privacy Transformer is trained to make the features identity-conditioned while": "catenated with the HuBERT embeddings to form inputs of size"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "speaker identity [21]. Jaiswal et al.\n[22] use Gradient Reversal",
          "A privacy Transformer is trained to make the features identity-conditioned while": "bsz × L × (d + dspk + dL) for the Transformer encoder. Our"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "Layer [23] to learn gender-invariant features that perform well",
          "A privacy Transformer is trained to make the features identity-conditioned while": "Transformer encoder\nfollows closely the original architecture"
        },
        {
          "the proposed method.\nFigure 2: An overview of": "for emotion recognition. To the best of our knowledge,\nthere",
          "A privacy Transformer is trained to make the features identity-conditioned while": "from [12]\nstacks of\nself-attention layers\nthat consists of LP"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "followed by a feed-forward neural network. However, we do": "not use the positional encoding as our inputs are permutation-",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "4.1. Datasets"
        },
        {
          "followed by a feed-forward neural network. However, we do": "invariant.\nThe goal of\nthe Transformer encoder\nis\nto model",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "the relationship between embeddings of different layers (but the",
          "4. Experiments": "CSTR VCTK The CSTR VCTK dataset [13] includes speech"
        },
        {
          "followed by a feed-forward neural network. However, we do": "same content) to produce high-level layer-aware contextualized",
          "4. Experiments": "data utters by 110 English speakers of various accents.\nEach"
        },
        {
          "followed by a feed-forward neural network. However, we do": "embeddings. Finally, we use a fully-connected layer to map the",
          "4. Experiments": "speaker reads from a fixed set of about 400 sentences collected"
        },
        {
          "followed by a feed-forward neural network. However, we do": "output dimension back to d. Although having a Transformer en-",
          "4. Experiments": "from different\nsources\nsuch as a newspaper,\na rainbow pas-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "coder to post-process the extracted representations might seem",
          "4. Experiments": "sage and an elicitation paragraph. For preprocessing, we down-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "computationally expensive at first, P can efficiently process its",
          "4. Experiments": "sample the 48kHz audio recordings provided by the dataset\nto"
        },
        {
          "followed by a feed-forward neural network. However, we do": "inputs due to the lack of the temporal dimension, which we later",
          "4. Experiments": "16kHz to be compatible with HuBERT feature extraction."
        },
        {
          "followed by a feed-forward neural network. However, we do": "demonstrate via experimental results in the discussion section.",
          "4. Experiments": "VoxCeleb1 We use the VoxCeleb1 dataset\n[14]\nto report\nthe"
        },
        {
          "followed by a feed-forward neural network. However, we do": "Learning\nscheme.\nIn\neach\nlearning\nstep, N pairs\nof\nut-",
          "4. Experiments": "speaker\nidentification (SID) performance, our privacy metric."
        },
        {
          "followed by a feed-forward neural network. However, we do": "terances\nfrom different\nspeakers with the same content,\ni.e.,",
          "4. Experiments": "The dataset contains over 145K utterances from 1541 speakers,"
        },
        {
          "followed by a feed-forward neural network. However, we do": "{(ui\nand {(ui\n1, spki\n1)}N\n2, spki\n2)}N\ni=1\ni=1, are sampled from the",
          "4. Experiments": "collected from more than 21K YouTube videos. We follow the"
        },
        {
          "followed by a feed-forward neural network. However, we do": "Voice Cloning dataset (with speakers pool S), and the HuBERT",
          "4. Experiments": "standard train/test splits provided by the dataset with 10% of the"
        },
        {
          "followed by a feed-forward neural network. However, we do": "encoder E extracts the feature embeddings for each utterance",
          "4. Experiments": "train set randomly selected for validation."
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "It is important to note that prior studies generally use (semi-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "(1)\n;\nzsrc = {E(ui\nztgt = {E(ui\n1)}N\ni=1\n2)}N\ni=1",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "informed)\nspeaker verification models\nto demonstrate robust-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "The Privacy Transformer P takes as inputs zsrc along with the",
          "4. Experiments": "ness to attacks [26, 6]. However,\nit\nis rather difficult for us to"
        },
        {
          "followed by a feed-forward neural network. However, we do": "target speakers spktgt = {spki\n2}N\ni=1 and generate estimations",
          "4. Experiments": "construct a robust speaker verification model given that our in-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "ztgt for the target representations ztgt.",
          "4. Experiments": "puts are 1D vector representations of speech. Therefore, we re-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "port the SID performance in this work, as a speaker verification-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "ztgt = P (zsrc|spktgt) = Pf c(Pte(zsrc ⊕ zspk ⊕ zlayer))",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "based attack is more appropriate for future work on anonymiz-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "(2)",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "ing the entire speech representations with a temporal dimension."
        },
        {
          "followed by a feed-forward neural network. However, we do": "where ⊕ denotes the concatenation operation with",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "IEMOCAP The IEMOCAP dataset\n[27]\nis a commonly used"
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "dataset\nfor multi-modal\nemotion\nrecognition with\napproxi-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "(3)\n;\n(layers)\nzspk = P spk\nzlayer = P layer",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "emb(spktgt)\nemb",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "mately 12 hours of\nrecordings from 10 speakers. The dataset"
        },
        {
          "followed by a feed-forward neural network. However, we do": "1..L\nfor layers =\n. To achieve this goal,\nthe Privacy Trans-",
          "4. Experiments": "is collected in 5 sessions, in which participants are asked to act"
        },
        {
          "followed by a feed-forward neural network. However, we do": "(cid:74)\n(cid:75)\nformer is trained with a mean-squared error loss",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "according to scripts that are designed to invoke emotions. Fol-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "lowing prior work [28], We focus on the four basic emotions"
        },
        {
          "followed by a feed-forward neural network. However, we do": "(4)\nLP = ||˜ztgt − ztgt||2",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "(happy, sad, angry, and neutral) to avoid severe class imbalance,"
        },
        {
          "followed by a feed-forward neural network. However, we do": "Inference scheme. During inference, we want\nto anonymize a",
          "4. Experiments": "which contains 5,536 utterances."
        },
        {
          "followed by a feed-forward neural network. However, we do": "set of arbitrary utterances (from unseen speakers), i.e., {ui}N\ni=1,",
          "4. Experiments": "MSP-IMPROV The MSP-IMPROV dataset\n[29]\nis an acted"
        },
        {
          "followed by a feed-forward neural network. However, we do": "without any information about the speaker identities. We use E",
          "4. Experiments": "audio-visual emotional dataset\nthat contains acted and impro-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "to extract z = {E(ui)}N\ni=1 features from these utterances. We",
          "4. Experiments": "vised emotional dyadic interactions\nfrom 12 speakers over 6"
        },
        {
          "followed by a feed-forward neural network. However, we do": "randomly select\ntarget speakers for\nthe input utterances from",
          "4. Experiments": "sessions.\nIn total,\nthe dataset contains around 8.5K utterances"
        },
        {
          "followed by a feed-forward neural network. However, we do": "the speakers pool S. Here, we allow different\nlayers to have",
          "4. Experiments": "(over 9 hours). Similar to IEMOCAP, we drop the unbalanced"
        },
        {
          "followed by a feed-forward neural network. However, we do": "different target speakers because there is no ground-truth target",
          "4. Experiments": "emotion classes and focus on the four basic emotion, which re-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "speaker as in the learning process.",
          "4. Experiments": "duce the number of utterances to 7,735."
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "Extended-DAIC The E-DAIC dataset [30] contains 219 video"
        },
        {
          "followed by a feed-forward neural network. However, we do": "spkinf\n;\nspki\ntgt(layers)}N\ni=1\ntgt(layerj) iid∼ S\ntgt = {spki",
          "4. Experiments": "recordings of clinical interviews probing for symptoms of psy-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "(5)",
          "4. Experiments": "chological distress. It was used as part of the 2019 Audio-Visual"
        },
        {
          "followed by a feed-forward neural network. However, we do": "The outputs ˜z from the Privacy Transformer is the sanitized rep-",
          "4. Experiments": "Emotion Challenge Workshop (AVEC2019) [31] in the Detect-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "resentations produced by our method.",
          "4. Experiments": "ing Depression sub-challenge. Each interview, ranging from 15"
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "to 25 minutes, is rated according to the eight-item Patient Health"
        },
        {
          "followed by a feed-forward neural network. However, we do": "(6)\nz = P (zsrc|spkinf\ntgt )",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "Questionaire (PHQ-8) along with a binary label for depression"
        },
        {
          "followed by a feed-forward neural network. However, we do": "We then use ˜z for different downstream tasks for privacy and",
          "4. Experiments": "conditions. We use the provided transcripts with the dataset\nto"
        },
        {
          "followed by a feed-forward neural network. However, we do": "utility evaluations.",
          "4. Experiments": "extract 18,846 utterances, from which around 33% are labeled"
        },
        {
          "followed by a feed-forward neural network. However, we do": "Choice of the Pretrained encoder and Privacy module. Al-",
          "4. Experiments": "as having depression."
        },
        {
          "followed by a feed-forward neural network. However, we do": "though\nthe\nexperiments\npresented\nin\nthis\npaper\nare\nlimited",
          "4. Experiments": "Fluent Speech Commands The Fluent Speech Commands"
        },
        {
          "followed by a feed-forward neural network. However, we do": "to one encoder architecture (HuBERT [11]) and one Privacy",
          "4. Experiments": "dataset\nis a widely used dataset\nfor\nthe semantic task of\nin-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "model architecture (Transformer\n[12]),\nthe method can be ap-",
          "4. Experiments": "tent classification.\nIt contains around 30,000 utterances from"
        },
        {
          "followed by a feed-forward neural network. However, we do": "plied to different combinations of pre-trained encoder and pri-",
          "4. Experiments": "97 speakers with 31 unique intents used for controlling smart-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "vacy module architectures because the roles of the modules are",
          "4. Experiments": "home appliances and virtual assistants."
        },
        {
          "followed by a feed-forward neural network. However, we do": "architecture-independent. Specifically, the encoder needs to ex-",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "",
          "4. Experiments": "4.2. Baselines"
        },
        {
          "followed by a feed-forward neural network. However, we do": "tract a robust representation of the input speech for a wide range",
          "4. Experiments": ""
        },
        {
          "followed by a feed-forward neural network. However, we do": "of downstream tasks, and the Privacy module needs to produce",
          "4. Experiments": "We use the two baselines provided by the VoicePrivacy 2022"
        },
        {
          "followed by a feed-forward neural network. However, we do": "speaker-conditioned vector representations, given the extracted",
          "4. Experiments": "Challenge [3].\nBaseline 1 is based on the voice anonymiza-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "features and the target speaker IDs. We chose HuBERT as our",
          "4. Experiments": "tion architecture proposed in [32].\nThe method first extracts"
        },
        {
          "followed by a feed-forward neural network. However, we do": "feature encoder due to its superior performance and a Trans-",
          "4. Experiments": "relevant features from the input speech signal,\nincluding the x-"
        },
        {
          "followed by a feed-forward neural network. However, we do": "former as our privacy module due to its ubiquitous usage. To",
          "4. Experiments": "vector [19], the fundamental frequency, and bottleneck features."
        },
        {
          "followed by a feed-forward neural network. However, we do": "demonstrate the usefulness of our method to Transformer-free",
          "4. Experiments": "The extracted x-vector is then anonymized with an external pool"
        },
        {
          "followed by a feed-forward neural network. However, we do": "encoders, we provide additional experimental\nresults with the",
          "4. Experiments": "of x-vectors, and the anonymized speech is synthesized with"
        },
        {
          "followed by a feed-forward neural network. However, we do": "APC pre-trained model [25] in the supplementary materials.",
          "4. Experiments": "a speech synthesis model based on the anonymized x-vector."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: shows the efficiency compar-",
      "data": [
        {
          "SID Acc. ↓": "72.79",
          "IEM. Acc.": "67.01",
          "IEM. F1": "67.97",
          "MSP. Acc.": "61.29",
          "MSP. F1": "58.10",
          "AVEC Acc.": "60.65",
          "AVEC F1": "56.12",
          "IC Acc.": "95.86"
        },
        {
          "SID Acc. ↓": "14.94",
          "IEM. Acc.": "60.01",
          "IEM. F1": "61.24",
          "MSP. Acc.": "52.65",
          "MSP. F1": "46.24",
          "AVEC Acc.": "53.35",
          "AVEC F1": "52.55",
          "IC Acc.": "93.37"
        },
        {
          "SID Acc. ↓": "29.92",
          "IEM. Acc.": "62.50",
          "IEM. F1": "63.35",
          "MSP. Acc.": "54.63",
          "MSP. F1": "49.54",
          "AVEC Acc.": "57.56",
          "AVEC F1": "54.75",
          "IC Acc.": "86.46"
        },
        {
          "SID Acc. ↓": "11.72",
          "IEM. Acc.": "60.18",
          "IEM. F1": "60.96",
          "MSP. Acc.": "54.14",
          "MSP. F1": "46.45",
          "AVEC Acc.": "57.88",
          "AVEC F1": "54.22",
          "IC Acc.": "89.94"
        },
        {
          "SID Acc. ↓": "11.64",
          "IEM. Acc.": "63.87",
          "IEM. F1": "64.97",
          "MSP. Acc.": "58.17",
          "MSP. F1": "54.33",
          "AVEC Acc.": "58.13",
          "AVEC F1": "55.98",
          "IC Acc.": "92.91"
        },
        {
          "SID Acc. ↓": "",
          "IEM. Acc.": "Table 1: Comparison results between our proposed method and the baselines.",
          "IEM. F1": "",
          "MSP. Acc.": "",
          "MSP. F1": "",
          "AVEC Acc.": "",
          "AVEC F1": "",
          "IC Acc.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: shows the efficiency compar-",
      "data": [
        {
          "Baseline 3\n11.72\n60.18\n60.96": "11.64\n63.87\n64.97\nPropose method",
          "54.14\n46.45\n57.88\n54.22\n89.94": "58.17\n54.33\n58.13\n55.98\n92.91"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "Table 1: Comparison results between our proposed method and the baselines."
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "Runtime (s)\nMemory Usage (GB)",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "also outperforms Baseline 2 with a margin of 1.23% on the F1"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "HuBERT Extr.\n439\n1.13",
          "54.14\n46.45\n57.88\n54.22\n89.94": "score. For intent classification (a semantics task), the proposed"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "Baseline 1\n2022\n3.50",
          "54.14\n46.45\n57.88\n54.22\n89.94": "approach achieves comparable classification accuracy with the"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "Baseline 2\n494\n0.18",
          "54.14\n46.45\n57.88\n54.22\n89.94": "best-performing baseline (92.91% vs.\n93.37%).\nThe results"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "Baseline 3\n457\n1.13",
          "54.14\n46.45\n57.88\n54.22\n89.94": "indicate\nfeature-level\nanonymization’s potential\ncompared to"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "Proposed method\n518\n1.71",
          "54.14\n46.45\n57.88\n54.22\n89.94": "voice anonymization for supervised ML tasks."
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "Table 2: Efficiency comparison between our proposed method",
          "54.14\n46.45\n57.88\n54.22\n89.94": "Because anonymization techniques are developed to run lo-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "and the baselines.",
          "54.14\n46.45\n57.88\n54.22\n89.94": "cally with limited computation, we compare our method with"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "the baselines in terms of\nruntime and memory usage.\nIn par-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "ticular, we select a fixed set of 500 utterances from the Vox-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "We use a variant of\nthe first baseline,\nin which the bottleneck",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "Celeb1 dataset [14] and perform anonymization on four CPUs"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "features are replaced with representations produced by a fine-",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "with a batch size of 1.\nTable 2 shows the efficiency compar-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "tuned wav2vec 2.0 model [17] and the speech synthesis mod-",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "isons between our method and the baselines. We also report the"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "ule is HifiGAN-based [33]. Baseline 2 is based on McAdams",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "computational resources for independently extracting HuBERT"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "transformation [10] with a uniformly sampled McAadam coef-",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "features from the selected utterances in the first row. Adding the"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "ficient. We add Baseline 3 based on Differential Privacy [34]",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "Privacy Transformer post-processing increases the computation"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "with the mean-pooled embeddings.\nIn particular,\nthe embed-",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "costs by 18% in runtime and 51% in memory usage compared to"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "dings extracted from HuBERT are clipped to [−1, 1] and Lapla-",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "HuBERT Extr (extract HuBERT features from speech without"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "cian noise ∼ Lap( 2",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "ϵ ) is added independently to all dimensions.",
          "54.14\n46.45\n57.88\n54.22\n89.94": "any post-processing). Our method achieves superior efficiency"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "performance compared to Baseline 1 (the more secured voice"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "4.3.\nImplementation Details",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "anonymization baseline) and comparable runtime to Baseline"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "2. Although Baseline 2 achieves superior efficiency in terms"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "For\nthe Privacy Transformer, we set\nthe size of\nthe speaker",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "of memory usage, the method is the weakest privacy-preserving"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "the\nlayer\nembedding\nembeddings dspk = 256 and size of",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "baseline and also suffers from poor performance on the seman-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "dL = 128. Our Privacy Transformer contains of LP = 5 stacks",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "tic task. It is worth noting that the performance of our method is"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "of Transformer encoder layers with h = 8 self-attention heads",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "highly dependent on the efficiency of the speech encoders, and"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "and an intermediate size of 4608. We train our Privacy Trans-",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "hence,\nthere is still\nroom to further boost\nthe efficiency with"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "former with an SGD optimizer with a learning rate of 0.001",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "light encoders such as the Distill-HuBERT [24]."
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "for 50 epochs. During the evaluation, our classification model",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "consists of a featurizer\n(12 learnable scalars)\nto fuse informa-",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "tion from all 12 layers of the extracted features from HuBERT,",
          "54.14\n46.45\n57.88\n54.22\n89.94": "6. Conclusion"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "followed by 2 fully-connected layers of sizes {256, 128}. For",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "We present a novel framework to anonymize mean-pooled vec-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "a fair comparison, all methods are trained using the Adam op-",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "tor speech embeddings extracted from pre-trained HuBERT. We"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "timizer with a learning rate of 1e−3 for 50 epochs with early",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "train a Privacy Transformer\nto convert\nthe extracted embed-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "stopping. For baselines 1 and 2, we extract HuBERT features",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "dings into different\nidentities while preserving the content us-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "after the original utterances are converted to privacy-preserving",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "ing a voice cloning dataset. We compare our method with the"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "utterances.",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "VoicePrivacy 2022 Challenge anonymization baseline methods"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "on the following tasks:\nspeaker\nidentification (privacy), emo-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "5. Results and Discussion",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "tion recognition (utility), depression recognition (utility), and"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "Table\n1\nshows\nthe\nperformance\ncomparisons\nbetween\nour",
          "54.14\n46.45\n57.88\n54.22\n89.94": "intent classification (utility). Our method achieves superior pri-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "method with the baselines. Since Baseline 3 allows flexibility in",
          "54.14\n46.45\n57.88\n54.22\n89.94": "vacy preservation performance and outperforms the baselines"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "terms of privacy and utility trade-offs, we adjust the amount of",
          "54.14\n46.45\n57.88\n54.22\n89.94": "on paralinguistic tasks. We further\nshow that our method is"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "added noise (ϵ = 15) such that the Speaker Identification accu-",
          "54.14\n46.45\n57.88\n54.22\n89.94": "computationally efficient, compared to the baselines."
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "racy is similar to our method. We also report the original perfor-",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "mance of HuBERT (without any privacy-preserving processing)",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "",
          "54.14\n46.45\n57.88\n54.22\n89.94": "7. Acknowledgement"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "in the first row of Table 1. Surprisingly, Baseline 3 achieves a",
          "54.14\n46.45\n57.88\n54.22\n89.94": ""
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "very competitive performance on both privacy and utility met-",
          "54.14\n46.45\n57.88\n54.22\n89.94": "Research was\nsponsored by the Army Research Office\nand"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "rics compared to the voice anonymization approaches despite",
          "54.14\n46.45\n57.88\n54.22\n89.94": "was\naccomplished\nunder\nCooperative Agreement Number"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "involving minimal post-processing. Our method achieves sim-",
          "54.14\n46.45\n57.88\n54.22\n89.94": "W911NF-20-2-0053. The views and conclusions contained in"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "ilar speaker\nidentification accuracy compared to the most se-",
          "54.14\n46.45\n57.88\n54.22\n89.94": "this document are those of\nthe authors and should not be in-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "cure baseline (11.64% vs. 11.72%) while outperforming all of",
          "54.14\n46.45\n57.88\n54.22\n89.94": "terpreted as representing the official policies, either expressed"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "the baselines on paralinguistic tasks. Specifically, for emotion",
          "54.14\n46.45\n57.88\n54.22\n89.94": "or\nimplied, of\nthe Army Research Office or\nthe U.S. Govern-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "recognition, our method achieves F1-scores of 64.97% (IEMO-",
          "54.14\n46.45\n57.88\n54.22\n89.94": "ment. The U.S. Government is authorized to reproduce and dis-"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "CAP) and 54.33% (MSP-IMPROV) compared to 63.35% and",
          "54.14\n46.45\n57.88\n54.22\n89.94": "tribute reprints for Government purposes notwithstanding any"
        },
        {
          "Baseline 3\n11.72\n60.18\n60.96": "49.54% for Baseline 2. For depression detection, our approach",
          "54.14\n46.45\n57.88\n54.22\n89.94": "copyright notation herein."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "autoregressive model\nfor speech representation learning,” Proc."
        },
        {
          "8. References": "[1]\nF. Metze et al., “Comparison of four approaches to age and gender",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "Interspeech 2019, pp. 146–150, 2019."
        },
        {
          "8. References": "recognition for telephone applications,” in ICASSP, vol. 4.\nIEEE,",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "2007, pp. IV–1089.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "[26] N. Tomashenko et al., “The voiceprivacy 2020 challenge: Results"
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "and findings,” Computer Speech & Language, vol. 74, p. 101362,"
        },
        {
          "8. References": "[2]\nJ. H. Walton et al., “Speaker race identification from acoustic cues",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "2022."
        },
        {
          "8. References": "in the vocal signal,” Journal of Speech, Language, and Hearing",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "Research, vol. 37, no. 4, pp. 738–745, 1994.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "[27] C. Busso et al., “Iemocap:\nInteractive emotional dyadic motion"
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "capture database,” Language resources and evaluation, vol. 42,"
        },
        {
          "8. References": "et\n[3] N. Tomashenko\nal.,\n“Privacy\nattacks\nfor\nautomatic\nspeech",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "no. 4, pp. 335–359, 2008."
        },
        {
          "8. References": "recognition acoustic models in a federated learning framework,”",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "[28]\nS.-w. Yang, P.-H. Chi, Y.-S. Chuang, C.-I.\nJ. Lai, K. Lakho-"
        },
        {
          "8. References": "in ICASSP.\nIEEE, 2022, pp. 6972–6976.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "tia, Y. Y. Lin, A. T. Liu,\nJ. Shi, X. Chang, G.-T. Lin et al.,"
        },
        {
          "8. References": "[4]\nS.-X. Zhang et al.,\n“Encrypted speech recognition using deep",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "“Superb: Speech processing universal performance benchmark,”"
        },
        {
          "8. References": "polynomial networks,” in ICASSP.\nIEEE, 2019, pp. 5691–5695.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "arXiv preprint arXiv:2105.01051, 2021."
        },
        {
          "8. References": "[5]\nF. Brasser et al., “Voiceguard: Secure and private speech process-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "[29] C. Busso et al., “Msp-improv: An acted corpus of dyadic interac-"
        },
        {
          "8. References": "ing.” in Interspeech, vol. 18, 2018, pp. 1303–1307.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "tions to study emotion perception,” IEEE Transactions on Affec-"
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "tive Computing, vol. 8, no. 1, pp. 67–80, 2016."
        },
        {
          "8. References": "[6] N. Tomashenko et al., “The voiceprivacy 2022 challenge evalua-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "tion plan,” arXiv preprint arXiv:2203.12468, 2022.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "[30] D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast, A. Gainer,"
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "K. Georgila, J. Gratch, A. Hartholt, M. Lhommet et al., “Simsen-"
        },
        {
          "8. References": "[7] H. Kai et al., “Lightweight voice anonymization based on data-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "sei kiosk: a virtual human interviewer for healthcare decision sup-"
        },
        {
          "8. References": "driven optimization of cascaded voice modification modules,” in",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "port,” in Proceedings of the 2014 international conference on Au-"
        },
        {
          "8. References": "SLT.\nIEEE, 2021, pp. 560–566.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "tonomous agents and multi-agent systems, 2014, pp. 1061–1068."
        },
        {
          "8. References": "[8]\nI.-C. Yoo et al., “Speaker anonymization for personal\ninforma-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "[31]\nF. Ringeval et al., “Avec 2019 workshop and challenge:\nstate-"
        },
        {
          "8. References": "tion protection using voice conversion techniques,” IEEE Access,",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "of-mind, detecting depression with ai, and cross-cultural affect"
        },
        {
          "8. References": "vol. 8, pp. 198 637–198 645, 2020.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "of\nthe\n9th\nInternational\non Au-\nrecognition,”\nin Proceedings"
        },
        {
          "8. References": "[9]\nJ. Qian et al., “Speech sanitizer: Speech content desensitization",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "dio/visual Emotion Challenge and Workshop, 2019, pp. 3–12."
        },
        {
          "8. References": "and voice anonymization,” IEEE Transactions on Dependable and",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "[32]\nF. Fang et al., “Speaker anonymization using x-vector and neural"
        },
        {
          "8. References": "Secure Computing, vol. 18, no. 6, pp. 2631–2642, 2019.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "waveform models,” in Proc. 10th ISCA Speech Synthesis Work-"
        },
        {
          "8. References": "[10]\nJ. Patino et al., “Speaker anonymisation using the mcadams coef-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "shop, 2019, pp. 155–160."
        },
        {
          "8. References": "ficient,” in Interspeech.\nISCA, 2021, pp. 1099–1103.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "[33]\nJ. Kong et al., “Hifi-gan: Generative adversarial networks for ef-"
        },
        {
          "8. References": "[11] W.-N. Hsu et al., “Hubert: Self-supervised speech representation",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "ficient and high fidelity speech synthesis,” NeurIPS, vol. 33, pp."
        },
        {
          "8. References": "learning by masked prediction of hidden units,” TASLP, vol. 29,",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "17 022–17 033, 2020."
        },
        {
          "8. References": "pp. 3451–3460, 2021.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "[34] C. Dwork, “Differential privacy: A survey of results,” in Interna-"
        },
        {
          "8. References": "[12] A. Vaswani et al., “Attention is all you need,” NeurIPS, vol. 30,",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "tional conference on theory and applications of models of compu-"
        },
        {
          "8. References": "2017.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": "tation.\nSpringer, 2008, pp. 1–19."
        },
        {
          "8. References": "[13] C. Veaux et al., “Cstr vctk corpus: English multi-speaker corpus",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "for cstr voice cloning toolkit,” University of Edinburgh. The Cen-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "tre for Speech Technology Research (CSTR), 2017.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[14] A. Nagrani et al., “Voxceleb: a large-scale speaker identification",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "dataset,” Telephony, pp. 33–039, 2017.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[15] A. T. Liu et al., “Mockingjay: Unsupervised speech representa-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "tion learning with deep bidirectional\ntransformer encoders,”\nin",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "ICASSP.\nIEEE, 2020, pp. 6419–6423.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[16] Y.-A. Chung et al., “Vector-quantized autoregressive predictive",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "coding,” Interspeech, pp. 3760–3764, 2020.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[17] A. Baevski et al., “wav2vec 2.0: A framework for self-supervised",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "learning of speech representations,” NeurIPS, vol. 33, pp. 12 449–",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "12 460, 2020.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[18] ——, “vq-wav2vec: Self-supervised learning of discrete speech",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "representations,” in ICLR, 2019.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[19] D. Snyder et al., “X-vectors: Robust dnn embeddings for speaker",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "recognition,” in ICASSP.\nIEEE, 2018, pp. 5329–5333.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[20]\nS. E. McAdams, Spectral fusion, spectral parsing and the forma-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "tion of auditory images.\nStanford university, 1984.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[21] B. M. L. Srivastava et al., “Privacy-preserving adversarial\nrep-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "resentation learning in asr: Reality or illusion?”\nin Interspeech,",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "2019.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[22] M.\nJaiswal et al.,\n“Privacy enhanced multimodal neural\nrepre-",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "sentations for emotion recognition,” in AAAI, vol. 34, 2020, pp.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "7985–7993.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[23] Y. Ganin et al., “Domain-adversarial training of neural networks,”",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "JMLR, vol. 17, no. 1, pp. 2096–2030, 2016.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "[24] H.-J. Chang et al., “Distilhubert: Speech representation learning",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "by layer-wise distillation of hidden-unit bert,” in ICASSP.\nIEEE,",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        },
        {
          "8. References": "2022, pp. 7087–7091.",
          "[25] Y.-A. Chung, W.-N. Hsu, H. Tang, and J. Glass, “An unsupervised": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Comparison of four approaches to age and gender recognition for telephone applications",
      "authors": [
        "F Metze"
      ],
      "year": "2007",
      "venue": "ICASSP"
    },
    {
      "citation_id": "3",
      "title": "Speaker race identification from acoustic cues in the vocal signal",
      "authors": [
        "J Walton"
      ],
      "year": "1994",
      "venue": "Journal of Speech, Language, and Hearing Research"
    },
    {
      "citation_id": "4",
      "title": "Privacy attacks for automatic speech recognition acoustic models in a federated learning framework",
      "authors": [
        "N Tomashenko"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "5",
      "title": "Encrypted speech recognition using deep polynomial networks",
      "authors": [
        "S.-X Zhang"
      ],
      "year": "2019",
      "venue": "ICASSP"
    },
    {
      "citation_id": "6",
      "title": "Voiceguard: Secure and private speech processing",
      "authors": [
        "F Brasser"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "The voiceprivacy 2022 challenge evaluation plan",
      "authors": [
        "N Tomashenko"
      ],
      "year": "2022",
      "venue": "The voiceprivacy 2022 challenge evaluation plan",
      "arxiv": "arXiv:2203.12468"
    },
    {
      "citation_id": "8",
      "title": "Lightweight voice anonymization based on datadriven optimization of cascaded voice modification modules",
      "authors": [
        "H Kai"
      ],
      "year": "2021",
      "venue": "SLT"
    },
    {
      "citation_id": "9",
      "title": "Speaker anonymization for personal information protection using voice conversion techniques",
      "authors": [
        "I.-C Yoo"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "10",
      "title": "Speech sanitizer: Speech content desensitization and voice anonymization",
      "authors": [
        "J Qian"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Dependable and Secure Computing"
    },
    {
      "citation_id": "11",
      "title": "Speaker anonymisation using the mcadams coefficient",
      "authors": [
        "J Patino"
      ],
      "venue": "Speaker anonymisation using the mcadams coefficient"
    },
    {
      "citation_id": "12",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu"
      ],
      "year": "2021",
      "venue": "TASLP"
    },
    {
      "citation_id": "13",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani"
      ],
      "year": "2017",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "14",
      "title": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit",
      "authors": [
        "C Veaux"
      ],
      "year": "2017",
      "venue": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit"
    },
    {
      "citation_id": "15",
      "title": "Voxceleb: a large-scale speaker identification dataset",
      "authors": [
        "A Nagrani"
      ],
      "year": "2017",
      "venue": "Telephony"
    },
    {
      "citation_id": "16",
      "title": "Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders",
      "authors": [
        "A Liu"
      ],
      "year": "2020",
      "venue": "ICASSP"
    },
    {
      "citation_id": "17",
      "title": "Vector-quantized autoregressive predictive coding",
      "authors": [
        "Y.-A Chung"
      ],
      "year": "2020",
      "venue": "Vector-quantized autoregressive predictive coding"
    },
    {
      "citation_id": "18",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "19",
      "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "year": "2019",
      "venue": "ICLR"
    },
    {
      "citation_id": "20",
      "title": "X-vectors: Robust dnn embeddings for speaker recognition",
      "authors": [
        "D Snyder"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "21",
      "title": "Spectral fusion, spectral parsing and the formation of auditory images",
      "authors": [
        "S Mcadams"
      ],
      "year": "1984",
      "venue": "Spectral fusion, spectral parsing and the formation of auditory images"
    },
    {
      "citation_id": "22",
      "title": "Privacy-preserving adversarial representation learning in asr: Reality or illusion",
      "authors": [
        "B Srivastava"
      ],
      "year": "2019",
      "venue": "Privacy-preserving adversarial representation learning in asr: Reality or illusion"
    },
    {
      "citation_id": "23",
      "title": "Privacy enhanced multimodal neural representations for emotion recognition",
      "year": "2020",
      "venue": "AAAI"
    },
    {
      "citation_id": "24",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin"
      ],
      "year": "2016",
      "venue": "JMLR"
    },
    {
      "citation_id": "25",
      "title": "Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert",
      "authors": [
        "H.-J Chang"
      ],
      "year": "2022",
      "venue": "ICASSP"
    },
    {
      "citation_id": "26",
      "title": "An unsupervised autoregressive model for speech representation learning",
      "authors": [
        "Y.-A Chung",
        "W.-N Hsu",
        "H Tang",
        "J Glass"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "27",
      "title": "The voiceprivacy 2020 challenge: Results and findings",
      "authors": [
        "N Tomashenko"
      ],
      "year": "2022",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "28",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "29",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "30",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Simsensei kiosk: a virtual human interviewer for healthcare decision support",
      "authors": [
        "D Devault",
        "R Artstein",
        "G Benn",
        "T Dey",
        "E Fast",
        "A Gainer",
        "K Georgila",
        "J Gratch",
        "A Hartholt",
        "M Lhommet"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems"
    },
    {
      "citation_id": "32",
      "title": "Avec 2019 workshop and challenge: stateof-mind, detecting depression with ai, and cross-cultural affect recognition",
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/visual Emotion Challenge and Workshop"
    },
    {
      "citation_id": "33",
      "title": "Speaker anonymization using x-vector and neural waveform models",
      "authors": [
        "F Fang"
      ],
      "year": "2019",
      "venue": "Proc. 10th ISCA Speech Synthesis Workshop"
    },
    {
      "citation_id": "34",
      "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis",
      "authors": [
        "J Kong"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "35",
      "title": "Differential privacy: A survey of results",
      "authors": [
        "C Dwork"
      ],
      "year": "2008",
      "venue": "International conference on theory and applications of models of computation"
    }
  ]
}