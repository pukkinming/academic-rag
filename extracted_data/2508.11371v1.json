{
  "paper_id": "2508.11371v1",
  "title": "Speech Emotion Recognition Using Fine-Tuned Dwformer: A Study On Track 1 Of The Ierp Challenge 2024",
  "published": "2025-08-15T10:13:43Z",
  "authors": [
    "Honghong Wang",
    "Xupeng Jia",
    "Jing Deng",
    "Rong Zheng"
  ],
  "keywords": [
    "speech emotion recognition",
    "fine-tuning",
    "data augmentation",
    "score fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The field of artificial intelligence has a strong interest in the topic of emotion recognition. The majority of extant emotion recognition models are oriented towards enhancing the precision of discrete emotion label prediction. Given the direct relationship between human personality and emotion, as well as the significant inter-individual differences in subjective emotional expression, the IERP Challenge 2024 incorporates personality traits into emotion recognition research. This paper presents the Fosafer's submissions to the Track 1 of the IERP Challenge 2024. This task primarily concerns the recognition of emotions in audio, while also providing text and audio features. In Track 1, we utilized exclusively audio-based features and fine-tuned a pre-trained speech emotion recognition model, DWFormer, through the integration of data augmentation and score fusion strategies, thereby achieving the first place among the participating teams.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) represents a pivotal technology for attaining human-computer interaction  [1] . To enhance the efficacy of human-computer interaction, it is imperative to optimize the performance of SER. The conventional approach to SER entails two principal stages: feature extraction and statistical modelling. The former encompasses the extraction of paralinguistic information pertinent to emotional states from the speech signal. This may include prosodic and energy features  [2] , spectral features  [3] , and other pertinent characteristics. The latter stage employs a range of general classification models to model and predict these extracted speech features. Common statistical modelling methods  [4]  include Gaussian Mixture Model (GMM), Hidden Markov Model (HMM), and Support Vector Machine (SVM). Deep learning-based SER models have significantly improved recognition performance. Leveraging the powerful learning capabilities of neural networks, deep learning not only achieves more complex data classification but also extracts paralinguistic information related to emotions from raw speech signal. By using self-supervised pre-trained speech representation models such as Wav2vec2.0  [5] , HuBERT  [6] , and WavLM  [7] , emotion-related speech features can be extracted and combined with neural network classifiers to achieve remarkable results. The current state of SER methods presents a challenge in that they are only capable of predicting discrete emotion labels. This limitation prevents the intensity of the emotion in speech from being accurately reflected  [8] . Furthermore, the coexistence of multiple emotions in a single utterance makes it difficult to fully capture the true sentiment by classifying the speech into a single emotion category. In order to address these issues, the IERP Challenge 2024 considers the direct relationship between human personality and emotional expression, incorporating personality traits into emotion recognition. In Track 1, the organizers employed the use of acoustic and textual features, which were extracted through the pre-trained models such as Wav2vec2.0, to predict scores for eight distinct emotional states. The emotional states included in the study were sadness, happiness, relaxation, surprise, anger, fear, disgust and neutrality. Each emotion score is on a scale of 1 to 5, with 1 representing no emotion and 5 representing the highest level of emotion. The evaluation is based on the root mean squared error (RMSE) of the scores assigned to the eight aforementioned emotions. A pre-trained SER model, dynamic window transformer (DWFormer)  [9] , was fine-tuned to predict scores for eight different emotions. Our approach comprises the following stages: Model Fine-Tuning: Building on the pre-trained SER model DWFormer, we froze all parameters except for the last fully connected layer of the classifier, then fine-tuned this fully connected layer using the official dataset. Data Augmentation: To enhance the model's robustness, we augmented the official data by introducing noise from the MUSAN dataset  [10] . We established three sets of additive noise probabilities to enhance the robustness of data augmentation strategy. Score Fusion: We combined scores predicted by models trained on various speech features, as well as those obtained from the same feature with different weight parameters, to further enhance model performance. For the above two scores, we used three methods for score fusion: simple averaging, weighted averaging, and maximum value, fusing a total of three scores. In the weighted average method, the weights assigned to the scores of models trained with different features were based on the three smallest RMSE values from the validation set. Similarly, when selecting different training weights for the model, we followed the same RMSE-based selection method. We fine-tuned the pre-trained SER model DWFormer to adapt with the Track 1 of the IERP Challenge 2024, using WavLM-Large 1 , Chinese-HuBERT-Large 2 , and Chinese-Wav2vec2-Large 3 as training and testing audio features. The model's performance was significantly improved through noise data augmentation with varying noise addition probabilities and model fusion strategies, leading to a first place ranking in the final submission.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "System Description",
      "text": "DWFormer is a state-of-the-art model in the field of SER. DWFormer is designed to capture the emotional features of speech across different temporal regions and scales. The results on the IEMOCAP  [11]  and MELD  [12]  datasets demonstrate that DWFormer outperforms other dominant methods, achieving superior results. Our system is constructed using the DWFormer framework, with all layers except for the final fully connected layer of the classifier being frozen. The classifier is then fine-tuned using the official training data. In contrast to the conventional discrete emotional labeling dataset, the official dataset incorporates a rating for each speech sample across the eight primary emotional categories. The original pre-trained DWFormer is a five-class model designed to recognize happiness, sadness, neutrality, anger, and fear. Its classifier consists of three fully connected layers with rectified linear unit (ReLU) activation functions. In order to accommodate the new emotion classification task, the output of the last fully connected layer was modified to 8 for finetuning. Given the considerable number of parameters in DWFormer and the limited training data provided for the competition, we employed data augmentation on the speech features using the noise subset of the MUSAN dataset to enhance the model's capability. Ultimately, a score fusion module was devised to integrate the outputs of models trained on disparate speech features, in addition to those trained on a single speech feature with varying weight parameters, with the objective of further enhancing the model's performance.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Model Architecture",
      "text": "where n denotes the number of samples, i y represents the true label of the i-th sample, and i y ˆdenotes the predicted value of the i-th sample, respectively.\n\nFigure  1 : The flowchat of system architecture .IC represents Importance Calculation module.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Audio Feature",
      "text": "The audio features utilized in this work, as illustrated in Table  1 , encompass three distinct types: WavLM-Large, Chinese-HuBERT-Large and Chinese-Wav2vec2-Large speech representations. These features derived from large pre-trained models, possess enhanced representational capacities, allowing for the capture of finer details in speech, including tone, emotion and semantic information. They are employed across a variety of speech processing tasks, such as speech recognition, speech emotion recognition, and speech synthesis. The specific selection of the noise probability value will be detailed in the experimental section.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Score Fusion",
      "text": "To address discrepancies in performance across models trained with different audio features and the same audio feature under varying training model parameters, we employ score fusion strategy. This approach leverages the strengths of different models to enhance prediction accuracy. Model parameter files are selected based on the RMSE scores obtained from the validation set. We combine the scores using three methods: simple averaging, weighted averaging, and maximum value.\n\nSimple Averaging: This method combines scores from models trained with different audio features on the test set, as well as scores from models trained with the same audio feature but different parameters on the test set. Simple averaging can be expressed by equation (  2 ):\n\nwhere n represents the number of features or model parameter files. i s is the score of the model trained using the i-th audio feature or the i-th model parameter on the test set.\n\nWeighted Averaging: This method uses a weighting system based on the RMSE scores of models trained with different audio features and models trained with the same audio feature but different parameters on the validation set to fuse scores on the test set. Weighted averaging is defined by the following equation (  3 ):\n\nwhere n and i s are consistent with equation (  2 ), i w is the weight of the model trained using the i-th audio feature or the i-th model parameter.\n\nMaximum Value: In this method, the final score is determined by selecting the highest score from models trained with different audio features and models trained with the same audio feature but different parameters on the test set.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We used the official dataset of 2608 audio features for training and validation, and 480 audio features for testing. The dataset was obtained from 163 subjects who watched 16 emotioninducing videos (including sadness, happiness, relaxation, surprise, fear, disgust, anger, and neutral, with each emotion being randomly presented in two videos). During training, we set the batch size to 4, the initial learning rate to 0.0001, and used the Adam optimizer  [14]  with a weight decay value of 1e-4. Furthermore, the ReduceLROnPlateau learning rate scheduler  [15]  was utilized to dynamically adjust the learning rate based on the model's performance during training. In both the training and testing, WavLM-Large, Chinese-HuBERT-Large and Chinese-Wav2vec2-Large representations were employed as audio features. The RMSE of the eight emotion scores serving as the evaluation metric, which is a widely used metric for evaluating the accuracy of a model's predictions. It measures the square root of the average of the squared differences between predicted and actual values. A smaller RMSE value indicates better model performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dwformer Fine-Tuning",
      "text": "The pre-trained SER model DWFormer was optimized using various audio features. The results are presented in Table  2 .\n\nFor brief representation, we denote the fine-tuned pre-trained DWFormer as Model_1. Compared to models utilizing WavLM-Large and Chinese-HuBERT-Large features, the model trained with Chinese-Wav2vec2-Large features exhibited the lowest average RMSE on the test set, which indicates that the model using the Chinese-Wav2vec2-Large representation as the audio feature achieves the highest performance. Chinese-Wav2vec2-Large 1.79058",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Data Augmentation",
      "text": "To enhance randomness and diversity in data augmentation, we set random noise addition probabilities of 0.3, 0.5 and 0.8. As illustrated in Table  3 , Model_2 represents the fine-tuned DWFormer with noise data augmentation, the experimental results demonstrate that the noise augmentation strategy effectively reduces the RMSE on the test set across three distinct audio features when random noise addition probability is 0.3 and 0.5. Model performance degrades with a random noise addition probability of 0.8. The most favorable outcomes were observed with the Chinese-Wav2vec2-Large feature. Notably, the models trained with all three different audio features achieved the lowest RMSE scores when a random noise addition probability of 0.3 was employed, indicating that a lower random noise addition probability can significantly enhance model performance. Chinese-Wav2vec2-Large 1.77790",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Score Fusion",
      "text": "Based on the use of noisy data augmentation, a fusion of the scores from models trained with three different audio features and three different model parameters for the same audio feature on the test set was conducted. Three methods mentioned above were employed for score fusion: simple averaging, weighted averaging, and maximum value. In the weighted averaging method, weights were assigned based on the performance of the models with different features on the validation set. The weights were 0.6, 0.2, and 0.2 for the models trained with Chinese-Wav2vec2-Large, Chinese-HuBERT-Large and WavLM-Large, respectively. The same weighting configuration was used for models trained with the same audio feature but different parameters.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions And Discussions",
      "text": "This paper presents the system description submitted for Track 1 of the IERP Challenge 2024, which ranked first among all participants. We utilized audio features exclusively, finetuning the pre-trained speech emotion recognition model DWFormer, and employed noise data augmentation and score fusion strategies. The experimental results indicate that the model using Chinese-Wav2vec2-Large as the audio feature outperforms those using Chinese-HuBERT-Large and WavLM-Large. Applying noise data augmentation with a randomized noise addition probability of 0.3 further improves the RMSE score compared to probabilities of 0.5 and 0.8. Lastly, among the three different score fusion methods based on two fusion modes, the Maximum Value score fusion method achieved the best performance. Due to submission limits, the final leaderboard result is 1.77157, rather than the system's optimal result of 1.77042. The aforementioned experiments demonstrated the efficacy and superiority of our system. However, for Track 1, there are still some outstanding issues that require attention. In particular, the text features provided by Track 1 have not been fully exploited, which limits performance. The future of emotion recognition research will likely focus on multimodal emotion recognition, as multimodal approaches have been shown to outperform unimodal ones. The other system, which employs alternative open-source datasets to train an eightclassification SER model and subsequently utilizes official data for fine-tuning, was not pursued due to time constraints. Furthermore, since the official data is labeled with sentiment intensity scores ranging from 1 to 5, the method of using open-source sentiment data to fine-tune the sentiment pretraining model to produce official-like labels and extend the data was not employed. This method could be investigated in future work.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: illustrates the overall structure of the system. Initially,",
      "page": 2
    },
    {
      "caption": "Figure 1: The flowchat of system architecture .IC",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "{wanghonghong,jiaxupeng,dengjing,zhengrong}@fosafer.com"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "Abstract"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "The field of artificial\nintelligence has a strong interest\nin the"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "topic of emotion recognition. The majority of extant emotion"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "recognition\nmodels\nare\noriented\ntowards\nenhancing\nthe"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "precision of discrete emotion label prediction. Given the direct"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "relationship between human personality and emotion, as well"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "as\nthe\nsignificant\ninter-individual\ndifferences\nin\nsubjective"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "emotional expression,\nthe IERP Challenge 2024 incorporates"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "personality traits into emotion recognition research. This paper"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "presents the Fosafer’s submissions to the Track 1 of the IERP"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "Challenge 2024. This task primarily concerns the recognition"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "of\nemotions\nin\naudio, while\nalso\nproviding\ntext\nand\naudio"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "features.\nIn Track\n1, we\nutilized\nexclusively\naudio-based"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "features\nand\nfine-tuned\na\npre-trained\nspeech\nemotion"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "recognition model, DWFormer,\nthrough the integration of data"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "augmentation and score\nfusion strategies,\nthereby achieving"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "the first place among the participating teams."
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "speech emotion recognition,\nfine-tuning, data\nIndex Terms:"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "augmentation, score fusion"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "1.\nIntroduction"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "Speech\nemotion\nrecognition\n(SER)\nrepresents\na\npivotal"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "technology for attaining human-computer\ninteraction [1]. To"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "enhance\nthe\nefficacy\nof\nhuman-computer\ninteraction,\nit\nis"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "imperative\nto\noptimize\nthe\nperformance\nof\nSER.\nThe"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "conventional\napproach to SER entails\ntwo principal\nstages:"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "feature\nextraction\nand\nstatistical modelling.\nThe\nformer"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "encompasses\nthe\nextraction\nof\nparalinguistic\ninformation"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "pertinent\nto emotional states from the speech signal. This may"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "include prosodic and energy features [2], spectral features [3],"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "and other pertinent characteristics. The latter stage employs a"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "range of general\nclassification models\nto model\nand predict"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "these extracted speech features. Common statistical modelling"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "methods [4] include Gaussian Mixture Model (GMM), Hidden"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "Markov Model (HMM), and Support Vector Machine (SVM)."
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "Deep learning-based SER models have significantly improved"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "recognition\nperformance. Leveraging\nthe\npowerful\nlearning"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "capabilities\nof\nneural\nnetworks,\ndeep\nlearning\nnot\nonly"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "achieves more\ncomplex data\nclassification but\nalso extracts"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "paralinguistic\ninformation\nrelated\nto\nemotions\nfrom raw"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "speech\nsignal. By\nusing\nself-supervised\npre-trained\nspeech"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "representation models such as Wav2vec2.0 [5], HuBERT [6],"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "and WavLM [7],\nemotion-related\nspeech\nfeatures\ncan\nbe"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "extracted\nand\ncombined with\nneural\nnetwork\nclassifiers\nto"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "achieve remarkable results."
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": ""
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "Corresponding authors: Rong Zheng"
        },
        {
          "Beijing Fosafer Information Technology Co., Ltd., Beijing": "This work is supported by the National Key Research and"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "performance was\nsignificantly improved through noise data",
          "in speech. The output from the DWFormer block is employed": "by the\nclassifier\nto\npredict\nemotion\nscores. Ultimately,\nthe"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "augmentation with\nvarying\nnoise\naddition\nprobabilities\nand",
          "in speech. The output from the DWFormer block is employed": "model’s inferred scores are integrated through the score fusion"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "model fusion strategies,\nleading to a first place ranking in the",
          "in speech. The output from the DWFormer block is employed": "module."
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "final submission.",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "During\nthe\ntraining\nphase,\nthe\nparameters\nof\nthe Vanilla"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "Transformer\nBlock,\nIC Module,\nDWFormer\nBlock,\nand"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "2.\nSystem Description",
          "in speech. The output from the DWFormer block is employed": "Classifier, with\nthe\nexception\nof\nthe\nfinal\nlayer,\nare\nkept"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "unchanged,\nindicating that\nthe gradients are not updated. The"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "DWFormer\nis\na\nstate-of-the-art model\nin the\nfield of SER.",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "final\nfully\nconnected\nlayer\nof\nthe\nclassifier\nis\nthe\nsole"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "DWFormer\nis designed to capture the emotional\nfeatures of",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "component subjected to training. The model\nis trained using a"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "speech\nacross\ndifferent\ntemporal\nregions\nand\nscales.\nThe",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "mean squared error\n(MSE)\nloss\nfunction,\nas defined by the"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "results\non\nthe\nIEMOCAP\n[11]\nand MELD [12]\ndatasets",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "following equation (1):"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "demonstrate\nthat DWFormer\noutperforms\nother\ndominant",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "methods, achieving superior results. Our system is constructed",
          "in speech. The output from the DWFormer block is employed": "2"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "(1)"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "using the DWFormer framework, with all layers except for the",
          "in speech. The output from the DWFormer block is employed": "\n\nMSE\ny(\ny\n)\ni\ni\n1 "
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "\nn"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "final\nfully connected layer of\nthe classifier being frozen. The",
          "in speech. The output from the DWFormer block is employed": "i\n1"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "classifier\nis then fine-tuned using the official\ntraining data.\nIn",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "contrast to the conventional discrete emotional labeling dataset,",
          "in speech. The output from the DWFormer block is employed": "where n denotes the number of samples,\nrepresents the\niy"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "the\nofficial\ndataset\nincorporates\na\nrating\nfor\neach\nspeech",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "true label of the i-th sample, and\ndenotes the predicted\niyˆ"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "sample\nacross\nthe\neight\nprimary\nemotional\ncategories. The",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "original pre-trained DWFormer is a five-class model designed",
          "in speech. The output from the DWFormer block is employed": "value of the i-th sample, respectively."
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "to recognize happiness, sadness, neutrality, anger, and fear. Its",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "classifier consists of three fully connected layers with rectified",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "linear\nunit\n(ReLU)\nactivation\nfunctions.\nIn\norder\nto",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "accommodate the new emotion classification task,\nthe output",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "of\nthe last\nfully connected layer was modified to 8 for\nfine-",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "tuning.",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "Given the considerable number of parameters\nin DWFormer",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "and the limited training data provided for the competition, we",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "employed data augmentation on the speech features using the",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "noise subset of\nthe MUSAN dataset\nto enhance the model’s",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "capability. Ultimately, a score fusion module was devised to",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "integrate\nthe outputs of models\ntrained on disparate\nspeech",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "features,\nin addition to those trained on a single speech feature",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "with varying weight parameters, with the objective of\nfurther",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "enhancing the model’s performance.",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "2.1. Model Architecture",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "Figure 1 illustrates the overall structure of the system. Initially,",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "the\nspeech\nfeatures\nundergo\nprocessing\nthrough\na\ndata",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "augmentation module\nfor\nthe\naddition\nof\nnoise,\nand\nare",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "subsequently fed into the Vanilla Transformer\n[13] Encoder",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "Block.\nThe\noutput\ncomprises\nthe\nencoded\nfeature",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "representations\nand\nthe\ncorresponding\nattention\nweight",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "matrices. The attention weight matrices are then processed by",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "the\nImportance Calculation (IC) Module\nto obtain temporal",
          "in speech. The output from the DWFormer block is employed": "Figure 1: The flowchat of system architecture .IC"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "estimates of\nthe importance of\nspeech emotion information.",
          "in speech. The output from the DWFormer block is employed": "represents Importance Calculation module."
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "Subsequently,\nthe importance scores,\nin conjunction with the",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "encoded output of\nthe original speech features, are conveyed",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "2.2. Audio Feature"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "to N stacked DWFormer blocks\nfor\nthe extraction of critical",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "The audio features utilized in this work, as illustrated in Table"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "emotional information from the speech. The DWFormer Block",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "1,\nencompass\nthree distinct\ntypes: WavLM-Large, Chinese-"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "consists of\nthe dynamic\nlocal window transformer\n(DLWT)",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "HuBERT-Large\nand\nChinese-Wav2vec2-Large\nspeech"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "and\ndynamic\nglobal\nwindow\ntransformer\n(DGWT).\nThe",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "representations. These features derived from large pre-trained"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "DLWT dynamically segments speech features into windows of",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "models,\npossess\nenhanced\nrepresentational\ncapacities,"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "varying scales, capturing local emotional\ninformation within",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "allowing for\nthe capture of\nfiner details in speech,\nincluding"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "each window, while the DGWT recalculates\nthe significance",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "tone, emotion and semantic information. They are employed"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "of\neach\nwindow\nfor\nglobal\ninformation\nfusion.\nThe",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "across\na variety of\nspeech processing tasks,\nsuch as\nspeech"
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "combination\nof DLWT and DGWT enhances\nthe model’s",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "",
          "in speech. The output from the DWFormer block is employed": "recognition, speech emotion recognition, and speech synthesis."
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "ability to uncover emotion-related paralinguistic information",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "2https://huggingface.co/TencentGameMate/chinese-",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "hubert-large",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "3https://huggingface.co/TencentGameMate/chinese-",
          "in speech. The output from the DWFormer block is employed": ""
        },
        {
          "Large3\nas\ntraining\nand\ntesting\naudio\nfeatures. The model’s": "wav2vec2-large",
          "in speech. The output from the DWFormer block is employed": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: three methods: simple averaging, weighted averaging, and For brief representation, we denote the fine-tuned pre-trained",
      "data": [
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "the dimensionality of frame-level features, ‘Params’"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "denotes the number of parameters, ‘Hours’ denotes"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "the amount of speech data during pretraining."
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "Feature\nDim\nParams\nHours"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "WavLM-Large\n1024\n300M\n94K"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "Chinese-HuBERT-Large\n1024\n317M\n60K"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "Chinese-Wav2vec2-Large\n1024\n317M\n10K"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "2.3. Data Augmentation"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "To enhance the model’s performance,\nthis study employs data"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "augmentation strategy. Specifically,\nthe noise subset\nfrom the"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "MUSAN dataset\nis\nused. Given\nthat\nthe\naudio\nfeatures"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "encoded\nby\npre-trained models\nare\ntwo-dimensional,\nthe"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "length of\nthe original data is determined by calculating the"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "product of\nthe 0th and 1st dimensions of\nthe feature. Noise"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "samples are then randomly selected from MUSAN dataset.\nIf"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "the length of\nthe noise data exceeds that of\nthe original data,"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "the noise is truncated to match the original length. Conversely,"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "if\nthe noise data is\nshorter,\nit\nis replicated to fit\nthe original"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "length. A noise probability value is established to regulate the"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "random incorporation of noise.\nIf\nthis probability exceeds a"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "randomly initialized value between 0 and 1, noise is added."
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "The specific selection of\nthe noise probability value will be"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "detailed in the experimental section."
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "2.4. Score Fusion"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "To\naddress\ndiscrepancies\nin\nperformance\nacross models"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "trained with\ndifferent\naudio\nfeatures\nand\nthe\nsame\naudio"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "feature under varying training model parameters, we employ"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "score fusion strategy. This approach leverages the strengths of"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "different models\nto\nenhance\nprediction\naccuracy. Model"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "parameter\nfiles\nare\nselected\nbased\non\nthe RMSE\nscores"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "obtained from the validation set. We combine the scores using"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "three methods:\nsimple\naveraging, weighted\naveraging,\nand"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "maximum value."
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "Simple Averaging: This method combines scores from models"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "trained with different audio features on the test set, as well as"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "scores\nfrom models\ntrained with the same audio feature but"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "different parameters on the test set. Simple averaging can be"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "expressed by equation (2):"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "n"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "1"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "(2)"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "\nS\nS"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "\navg\ni"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "n\ni\n1"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "where n represents the number of features or model parameter"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "files.\nis the score of the model trained using the i-th audio\nis"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "feature or the i-th model parameter on the test set."
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "Weighted Averaging: This method uses\na weighting system"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "based on the RMSE scores of models\ntrained with different"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "audio features and models trained with the same audio feature"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "but different parameters on the validation set\nto fuse scores on"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "the test\nset. Weighted averaging is defined by the following"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "equation (3):"
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": ""
        },
        {
          "Table 1: Different audio feature. where ‘Dim’ denotes": "n"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: , Model_3 represents the fine- [2] C. Williams and K. Stevens. “Vocal correlates of emotional",
      "data": [
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "improvement\nin RMSE on the\ntest\nset. Multi-feature model"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "score\nfusion\napproaches\nreduce\nperformance Additionally,"
        },
        {
          "Table 3: The results on the test set using different": "Prob.",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "compared to the simple averaging and the weighted averaging"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "score\nfusion methods,\nthe maximum value\nscore\nfusion"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": ""
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "method\nyields\nbetter\nresults.\nLimited\nby\nthe\nnumber\nof"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "submissions, the final ranking result, presented in Table 4, row"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": ""
        },
        {
          "Table 3: The results on the test set using different": "0.8",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "5, shows that Model_4 achieved a score of 1.77157 using the"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "average score fusion method,\nsecuring the first place among"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "all entries."
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": ""
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "4.\nConclusions and Discussions"
        },
        {
          "Table 3: The results on the test set using different": "0.5",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": ""
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "This paper presents the system description submitted for Track"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": ""
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "1 of\nthe IERP Challenge 2024, which ranked first among all"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "participants. We\nutilized\naudio\nfeatures\nexclusively,\nfine-"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "tuning\nthe\npre-trained\nspeech\nemotion\nrecognition model"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": ""
        },
        {
          "Table 3: The results on the test set using different": "0.3",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "DWFormer, and employed noise data augmentation and score"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "fusion strategies. The\nexperimental\nresults\nindicate\nthat\nthe"
        },
        {
          "Table 3: The results on the test set using different": "",
          "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable": "model using Chinese-Wav2vec2-Large\nas\nthe\naudio feature"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: , Model_3 represents the fine- [2] C. Williams and K. Stevens. “Vocal correlates of emotional",
      "data": [
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "classification SER model\nand\nsubsequently\nutilizes\nofficial"
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "data for\nfine-tuning, was not pursued due to time constraints."
        },
        {
          "multi-feature models and single-feature models with": "Model",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "Furthermore, since the official data is labeled with sentiment"
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "intensity\nscores\nranging from 1\nto 5,\nthe method of using"
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": ""
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "open-source\nsentiment\ndata\nto\nfine-tune\nthe\nsentiment\npre-"
        },
        {
          "multi-feature models and single-feature models with": "Model_3",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "training model\nto produce official-like labels and extend the"
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "data was not employed. This method could be investigated in"
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": ""
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "future work."
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": ""
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "5.\nReferences"
        },
        {
          "multi-feature models and single-feature models with": "Model_4",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": ""
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "[1]\nT. M. Wani, T. S. Gunawan, S. A. A. Qadri, M. Kartiwi and E."
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "Ambikairajah, “A Comprehensive Review of Speech Emotion"
        },
        {
          "multi-feature models and single-feature models with": "",
          "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-": "Recognition Systems,” in IEEE Access, vol. 9, pp. 47795-47814,"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: , Model_3 represents the fine- [2] C. Williams and K. Stevens. “Vocal correlates of emotional",
      "data": [
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "noise\naddition\nprobability\nof\n0.8.\nThe\nmost\nfavorable",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "the\nChinese-Wav2vec2-Large\nfeature\nand\ntraining\nwith"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "outcomes were observed with the Chinese-Wav2vec2-Large",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "distinct model\nparameters\nfor\nscore\nfusion.\nThis\nfeature-"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "feature. Notably,\nthe models\ntrained with all\nthree different",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "trained model was\nselected\nfor\nscore\nfusion because of\nits"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "audio\nfeatures\nachieved\nthe\nlowest RMSE scores when\na",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "superior\nperformance\nin\nprevious\nexperiments.\n'Average'"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "random noise\naddition\nprobability\nof\n0.3 was\nemployed,",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "refers to the simple averaging score fusion method,\n'Weighted"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "indicating that a lower\nrandom noise addition probability can",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "Average' denotes the weighted averaging score fusion method"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "significantly enhance model performance.",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "and 'Max' represents the maximum value score fusion method."
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "Compared to single-feature models,\nthe score fusion methods"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Table 3: The results on the test set using different",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "for\ndifferent\nmodel\nparameters\ndemonstrated\na\nnotable"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "random noise addition probabilities.",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "improvement\nin RMSE on the\ntest\nset. Multi-feature model"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "score\nfusion\napproaches\nreduce\nperformance Additionally,"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Model\nProb.\nFeature\nRMSE ↓",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "compared to the simple averaging and the weighted averaging"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "score\nfusion methods,\nthe maximum value\nscore\nfusion"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "WavLM-Large\n1.98074",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "method\nyields\nbetter\nresults.\nLimited\nby\nthe\nnumber\nof"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "submissions, the final ranking result, presented in Table 4, row"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Chinese-HuBERT-Large\n1.93404",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "0.8",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "5, shows that Model_4 achieved a score of 1.77157 using the"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Chinese-Wav2vec2-Large\n1.86812",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "average score fusion method,\nsecuring the first place among"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "all entries."
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "WavLM-Large\n1.82412",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Chinese-HuBERT-Large\n1.81857",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "4.\nConclusions and Discussions"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Model_2\n0.5",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "This paper presents the system description submitted for Track"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Chinese-Wav2vec2-Large\n1.78335",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "1 of\nthe IERP Challenge 2024, which ranked first among all"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "WavLM-Large\n1.80782",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "participants. We\nutilized\naudio\nfeatures\nexclusively,\nfine-"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "tuning\nthe\npre-trained\nspeech\nemotion\nrecognition model"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Chinese-HuBERT-Large\n1.77779",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "0.3",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "DWFormer, and employed noise data augmentation and score"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Chinese-Wav2vec2-Large\n1.77790",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "fusion strategies. The\nexperimental\nresults\nindicate\nthat\nthe"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "model using Chinese-Wav2vec2-Large\nas\nthe\naudio feature"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "outperforms\nthose\nusing\nChinese-HuBERT-Large\nand"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "3.4. Score Fusion",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "WavLM-Large. Applying\nnoise\ndata\naugmentation with\na"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Based on the use of noisy data augmentation, a fusion of\nthe",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "randomized noise addition probability of 0.3 further improves"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "scores from models trained with three different audio features",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "the RMSE score\ncompared\nto probabilities of 0.5 and 0.8."
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "and\nthree\ndifferent model\nparameters\nfor\nthe\nsame\naudio",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "Lastly, among the three different score fusion methods based"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "feature\non\nthe\ntest\nset\nwas\nconducted.\nThree methods",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "on\ntwo\nfusion modes,\nthe Maximum Value\nscore\nfusion"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "mentioned\nabove were\nemployed\nfor\nscore\nfusion:\nsimple",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "method\nachieved\nthe\nbest\nperformance. Due\nto\nsubmission"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "averaging, weighted averaging,\nand maximum value.\nIn the",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "limits,\nthe final\nleaderboard result\nis 1.77157,\nrather\nthan the"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "weighted averaging method, weights were assigned based on",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "system’s optimal result of 1.77042."
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "the performance of\nthe models with different\nfeatures on the",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "The\naforementioned\nexperiments\ndemonstrated\nthe\nefficacy"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "validation\nset. The weights were\n0.6,\n0.2,\nand\n0.2\nfor\nthe",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "and superiority of our system. However, for Track 1,\nthere are"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "models\ntrained\nwith\nChinese-Wav2vec2-Large,\nChinese-",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "still\nsome\noutstanding\nissues\nthat\nrequire\nattention.\nIn"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "HuBERT-Large\nand WavLM-Large,\nrespectively. The\nsame",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "particular, the text features provided by Track 1 have not been"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "weighting configuration was used for models trained with the",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "fully\nexploited, which\nlimits\nperformance.\nThe\nfuture\nof"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "same audio feature but different parameters.",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "emotion recognition research will\nlikely focus on multimodal"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "emotion\nrecognition,\nas multimodal\napproaches\nhave\nbeen"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Table 4: The results of score fusion methods using",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "shown to outperform unimodal ones. The other system, which"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "multi-feature models and single-feature models with",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "employs\nalternative\nopen-source\ndatasets\nto\ntrain\nan\neight-"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "different parameters on the test set.",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "classification SER model\nand\nsubsequently\nutilizes\nofficial"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "data for\nfine-tuning, was not pursued due to time constraints."
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Model\nFusion Method\nRMSE ↓",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "Furthermore, since the official data is labeled with sentiment"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "intensity\nscores\nranging from 1\nto 5,\nthe method of using"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Average\n1.78673",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "open-source\nsentiment\ndata\nto\nfine-tune\nthe\nsentiment\npre-"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Model_3\nWeighted Average\n1.80780",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "training model\nto produce official-like labels and extend the"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "data was not employed. This method could be investigated in"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Max\n1.78492",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "future work."
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Average\n1.77157",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "5.\nReferences"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Model_4\nWeighted Average\n1.77192",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "[1]\nT. M. Wani, T. S. Gunawan, S. A. A. Qadri, M. Kartiwi and E."
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "Max\n1.77042",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "Ambikairajah, “A Comprehensive Review of Speech Emotion"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "Recognition Systems,” in IEEE Access, vol. 9, pp. 47795-47814,"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "2021."
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "As\nillustrated in Table 4, Model_3 represents\nthe\nfine-",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "[2]\nC. Williams\nand K. Stevens.\n“Vocal\ncorrelates\nof\nemotional"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "state”.\nIn:\nSpeech\nEvaluation\nin\nPsychiatry,\nGrune\nand"
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "tuned DWFormer with\nnoise\ndata\naugmentation,\nutilizing",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "",
          "fine-tuned DWFormer with noise data augmentation, utilizing": "Stratton ,pages 189–220, 1981."
        },
        {
          "is 0.3 and 0.5. Model performance degrades with a random": "diverse audio features for score fusion. Model_4 represents the",
          "fine-tuned DWFormer with noise data augmentation, utilizing": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "in spoken dialogs,” in IEEE Transactions on Speech and Audio"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "Processing, vol. 13, no. 2, pp. 293-303, March, 2005."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[4]\nBreazeal, Cynthia, and Li Aryananda. \"Recognition of affective"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "communicative\nintent\nin\nrobot-directed\nspeech.\" Autonomous"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "robots 12 : 83-104, 2002."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[5]\nA. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0:"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "A\nframework\nfor\nself-supervised\nlearning\nof\nspeech"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "representations,” in Advances in Neural Information Processing"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "Systems (NeurIPS), 2020."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[6]\nW. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov,"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "and\nA.\nMohamed,\n“HuBERT:\nSelf-supervised\nspeech"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "representation learning by masked prediction of hidden units,”"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "arXiv preprint arXiv:2106.07447, 2021."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[7]\nS. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, and F. Wei,"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "“ WavLM:\nLarge-scale\nself-supervised\npre-training\nfor\nfull"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "stack speech processing,” IEEE J. Sel. Topics Signal Process.,"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "vol. 16, no. 6, pp. 1505–1518, 2022."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[8]\nZ. Ma\net\nal.,\n“Leveraging\nSpeech\nPTM,\nText\nLLM,\nand"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "Emotional TTS for\nspeech\nemotion\nrecognition,”\nin\nICASSP"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "2024\n-\n2024\nIEEE\nInternational Conference\non\nAcoustics,"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "Speech and Signal Processing (ICASSP), Seoul, Korea, Republic"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "of, 2024, pp. 11146–11150."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[9]\nS. Chen, X. Xing, W. Zhang, W. Chen, and X. Xu, “DWFormer:"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "Dynamic Window Transformer for speech emotion recognition,”"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "in\nICASSP 2023\n-\n2023\nIEEE International Conference\non"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "Acoustics,\nSpeech\nand\nSignal Processing\n(ICASSP), Rhodes"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "Island, Greece, 2023, pp. 1–5."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[10] D. Snyder, G. Chen, and D. Povey, “MUSAN: A music, speech,"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "and noise corpus,” arXiv preprint arXiv:1510.08484, 2015."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[11] C. Busso\net\nal.,\n“IEMOCAP:\nInteractive\nemotional\ndyadic"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "motion capture database,” Lang. Resour. Evalu., vol. 42, pp."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "335–359, 2008."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[12]\nS. Poria et al., “MELD: A multimodal multi-party dataset\nfor"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "emotion\nrecognition\nin\nconversations,”\narXiv\npreprint"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "arXiv:1810.02508, 2018."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[13] T. Lin et al., “A survey of\ntransformers,” AI Open, vol. 3, pp."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "111–132, 2022."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[14] Z. Zhang, “Improved Adam optimizer for deep neural networks,”"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "in 2018 IEEE/ACM 26th International Symposium on Quality of"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "Service (IWQoS), 2018, pp. 1–6."
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "[15] A.\nLewkowycz,\n“How to\ndecay\nyour\nlearning\nrate,”\narXiv"
        },
        {
          "[3]\nChul Min Lee and S. S. Narayanan, “Toward detecting emotions": "preprint arXiv:2103.12682, 2021."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "A Comprehensive Review of Speech Emotion Recognition Systems",
      "authors": [
        "T Wani",
        "T Gunawan",
        "S Qadri",
        "M Kartiwi",
        "E Ambikairajah"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Vocal correlates of emotional state",
      "authors": [
        "C Williams",
        "K Stevens"
      ],
      "year": "1981",
      "venue": "Speech Evaluation in Psychiatry, Grune and Stratton"
    },
    {
      "citation_id": "4",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "Chul Min",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Speech and Audio Processing"
    },
    {
      "citation_id": "5",
      "title": "Recognition of affective communicative intent in robot-directed speech",
      "authors": [
        "Cynthia Breazeal",
        "Li Aryananda"
      ],
      "year": "2002",
      "venue": "Autonomous robots"
    },
    {
      "citation_id": "6",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems (NeurIPS)"
    },
    {
      "citation_id": "7",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "arxiv": "arXiv:2106.07447"
    },
    {
      "citation_id": "8",
      "title": "WavLM: Large-scale self-supervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "F Wei"
      ],
      "year": "2022",
      "venue": "IEEE J. Sel. Topics Signal Process"
    },
    {
      "citation_id": "9",
      "title": "Leveraging Speech PTM, Text LLM, and Emotional TTS for speech emotion recognition",
      "authors": [
        "Z Ma"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "DWFormer: Dynamic Window Transformer for speech emotion recognition",
      "authors": [
        "S Chen",
        "X Xing",
        "W Zhang",
        "W Chen",
        "X Xu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "MUSAN: A music, speech, and noise corpus",
      "authors": [
        "D Snyder",
        "G Chen",
        "D Povey"
      ],
      "year": "2015",
      "venue": "MUSAN: A music, speech, and noise corpus",
      "arxiv": "arXiv:1510.08484"
    },
    {
      "citation_id": "12",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evalu"
    },
    {
      "citation_id": "13",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria"
      ],
      "year": "2018",
      "venue": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "14",
      "title": "A survey of transformers",
      "authors": [
        "T Lin"
      ],
      "year": "2022",
      "venue": "AI Open"
    },
    {
      "citation_id": "15",
      "title": "Improved Adam optimizer for deep neural networks",
      "authors": [
        "Z Zhang"
      ],
      "year": "2018",
      "venue": "2018 IEEE/ACM 26th International Symposium on Quality of Service (IWQoS)"
    },
    {
      "citation_id": "16",
      "title": "How to decay your learning rate",
      "authors": [
        "A Lewkowycz"
      ],
      "year": "2021",
      "venue": "How to decay your learning rate",
      "arxiv": "arXiv:2103.12682"
    }
  ]
}