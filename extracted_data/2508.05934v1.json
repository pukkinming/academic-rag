{
  "paper_id": "2508.05934v1",
  "title": "Aslsl: Adaptive Shared Latent Structure Learning With Incomplete Multi-Modal Physiological Data For Multi-Dimensional Emotional Feature Selection",
  "published": "2025-08-08T01:54:02Z",
  "authors": [
    "Xueyuan Xu",
    "Tianze Yu",
    "Wenjia Dong",
    "Fulin Wei",
    "Li Zhuo"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recently, multi-modal physiological signals based emotion recognition has garnered increasing attention in the field of brain-computer interfaces. Nevertheness, the associated multi-modal physiological features are often highdimensional and inevitably include irrelevant, redundant, and noisy representation, which can easily lead to overfitting, poor performance, and high computational complexity in emotion classifiers. Feature selection has been widely applied to address these challenges. However, previous studies generally assumed that multi-modal physiological data are complete, whereas in reality, the data are often incomplete due to the openness of the acquisition and operational environment. For example, a part of samples are available in several modalities but not in others. To address this issue, we propose a novel method for incomplete multi-modal physiological signal feature selection called adaptive shared latent structure learning (ASLSL). Based on the property that similar features share similar emotional labels, ASLSL employs adaptive shared latent structure learning to explore a common latent space shared for incomplete multi-modal physiological signals and multi-dimensional emotional labels, thereby mitigating the impact of missing information and mining consensus information. Two most popular multi-modal physiological emotion datasets (DEAP and DREAMER) with multidimensional emotional labels were utilized to compare the performance between compare ASLSL and seventeen feature selection methods. Comprehensive experimental results on these datasets demonstrate the effectiveness of ASLSL.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Existing affective computing researches predominantly utilize discrete or multi-dimensional emotion models to represent distinct emotional states. Compared to discrete emotion models, multi-dimensional emotional label models offer broader characterization and are capable of describing the variations in emotional states  (Khare et al. 2023) . Currently, emotion recognition approaches predominantly encompass facial expressions, speech, body gestures, text, and neurophysiological signals  (Ezzameli and Mahersia 2023) . Emotions arise alongside physiological and psychological activities, and neurophysiological signals offer the advantage of being difficult to fake compared to other approaches  (Wu et al. 2023 ).\n\nAmong multiple physiological signals, electroencephalogram (EEG) has garnered considerable attention in emotion recognition research due to its high temporal resolution, non-invasiveness, and ease of implementation  (Wang, Zhang, and Di 2024) . Nevertheless, due to the nonlinear, non-stationary nature and individual variability of EEG signals  (Li et al. 2022) , emotion recognition systems based on unimodal EEG often face challenges related to low accuracy in practical real-world applications  (Geetha et al. 2024 ).\n\nTo address these issues, several studies have attempted to analyze emotion states using multi-modal neurophysiological signals  (Zhang et al. 2020b) . Each modality reveals specific aspects of the processes and dynamics of emotions, and complementary multi-modal neurophysiological signals provide multi-level emotional representations that enhance the precision of emotion recognition  (Geetha et al. 2024) .\n\nThe proliferation of sensors for affective computing has accelerated due to advancements in multi-modal physiological signal acquisition technologies, resulting in a substantial volume of features extracted from these sensors  (Becker et al. 2017) . However, the limited availability of high-quality multi-modal physiological signal samples means that the resultant features are often high-dimensional and may contain redundant, irrelevant, or noisy data  (Xu et al. 2023) . This can lead to challenges such as overfitting, diminished performance, and increased computational complexity  (Wang et al. 2020a) . Feature selection has emerged as a crucial technique for identifying and retaining significant features while removing irrelevant and noisy ones from the data. This approach helps maintain the core physiological and psychological representations of physiological signal features, thereby enhancing the transparency and interpretability of affective computing models  (Jenke, Peer, and Buss 2014; Liu et al. 2018b) .\n\nFeature selection methods can be broadly categorized into three main types based on their feature evaluation and search mechanisms: filter methods, wrapper methods, and embedded methods  (Li et al. 2017) . Filter methods assess the importance of neurophysiological signal features in emotion recognition using statistical metrics or information entropy measures. However, these methods often yield suboptimal feature selection results, regardless of the performance of classifiers  (Zhang et al. 2019b ). To address these limitations, several studies have investigated wrapper methods, which generally offer superior classification performance compared to filter methods. This is because wrapper methods evaluate feature subsets based on the performance of a specific classifier  (Hou et al. 2011) . Nonetheless, wrapper methods can be computationally intensive due to the extensive number of trials required  (Zhang et al. 2019b) . Recently, embedded methods have gained attention as a viable alternative to overcome the drawbacks of filter methods. These methods integrate the process of feature selection directly into the model optimization framework. Experimental results in  (Torres-Valencia, Álvarez-López, and Orozco-Gutiérrez 2017; Xu et al. 2023)  shows the effectiveness of the embedded approaches in emotion recognition.\n\nExisting studies on physiological feature selection typically assume that multi-modal physiological signal data are complete. However, due to the inherent variability of the acquisition environment, practical applications of emotion recognition based on multi-modal neurophysiological signals often encounter incomplete sample data. For example, some samples may be available in certain modalities but not in others. This lack of complete information directly impedes the accurate modeling of the relationship between multi-modal neurophysiological signals and multidimensional emotional labels  (Wang, Li, and Cui 2024) .\n\nTo address this issue, as illustrated in Fig.  1 , we propose a novel feature selection model for incomplete multi-modal physiological feature selection, termed adaptive shared latent structure learning (ASLSL). This model utilizes adaptive shared latent structure learning to explore a common la-tent space for multi-modal physiological signals and multidimensional emotions, even in the presence of incomplete multi-modal information. Additionally, it employs adaptive learning of modality weights to assess the significance of different modalities in emotion recognition. By the above means, ASLSL could effectively uncovers the relationships between incomplete multi-modal neurophysiological data and multi-dimensional emotional representation, thereby facilitating the construction of informative emotional feature subsets.\n\nFurthermore, the contributions of our work are as follows:\n\n• We propose a novel incomplete multi-modal physiological feature selection method for the multi-dimensional affective computing task. ASLSL employs adaptive shared latent structure learning to explore a latent space shared for incomplete multi-modal signal data and multidimensional emotional representation, thereby mitigating the impact of missing information and mining consensus information. With the above strategy, ASLSL could adaptively adjust the discriminative analysis results for various feature types within the incomplete multi-modal data, thus realizing efficient and accurate multi-dimensional affective computing. To the best of our knowledge, this is the first study to construct an incomplete multi-view feature selection framework in a complete multi-label data scenario. • To address the optimization challenges associated with ASLSL, an efficient alternative approach is proposed to ensure convergence and attain an optimal solution. • To validate the efficacy of ASLSL, we utilized two publicly available datasets: DREAMER and DEAP. Experimental results show that ASLSL outperforms seventeen advanced feature selection methods, achieving superior emotion recognition performance across six metrics.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Problem Formulation",
      "text": "The ASLSL framework is formulated as follows:\n\nmin\n\n(1) where γ and λ are both regularization parameters. X = X 1) , S (2) , ..., S (v) , ..., S (m) where S (v) ∈ R n×n is an indicator matrix that marks the location of the missing instances. U ∈ R n×k , Q (v) ∈ R dv×k , and M ∈ R k×k are shared space of all modalities, projection matrix of X (v) , and coefficient matrix of Y , respectively. d (v) , n, m, and k correspond to the number of features of each modal, instances, modalities, and dimensions of labeling, respectively.\n\nThe adaptive shared latent structure term, graph-based manifold regularization term, and l 2,1 -norm sparsity restriction term are represented by the symbols F , C, and Ω, re-",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Adaptive Shared Latent Structure Term",
      "text": "Shared latent structure learning can be obtained as follows:\n\nwhere the feature matrix X can be represented by two nonnegative matrices U and Q. Similarly, the multi-dimensional emotional label matrix Y can be decomposed to a latent label structure matrix U and corresponding coefficient matrix M . Based on the property that similar features share similar emotional labels, a common matrix U is utilized as the latent structure space for X and Y , which could share dependence between physiological signal features and multidimensional emotional labels.\n\nInspired by the previous research findings that every modal have different contributions to the emotion recognition task  (Koelstra et al. 2011; Zhang et al. 2020b ), an adaptive modal weight evaluation strategy is adopted to assign the modal-weight α (v) (v = 1, . . . , m) for each physiological signal modal. Additionally, the same latent feature structure U is shared by each physiological signal to mine the consensus information across various modalities. Finally, to represent the instance-missing information, a diagonal weighted matrix S (v) in the v-th modal is included. In con-clusion, F can be formulated as follows: min\n\nwhere γ and λ are both regularization parameters.\n\nThe diagonal element S (v) j,j of the diagonal weighted matrix S (v) in the v-th modal is defined as follows:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Graph-Based Manifold Regularization Term",
      "text": "Based on the spectral graph theory  (Jian et al. 2016 ), a graphbased manifold regularizer Tr U T L Y U is employed to maintain consistency of the local geometric structures between the shared latent structure space U and the multidimensional emotional label space Y . The graph-based manifold regularization term C can be formulated as follows:\n\nwhere a graph Laplacian matrix\n\nA heat kernel has been employed to generate the affinity graph S Y . The similarity value of two labels, y .i and y .j , is represented by the element S Y (i, j). S Y (i, j) is defined as follows:\n\ny .i ∈ N q (y .j ) or y .j ∈ N q (y .i ) 0 otherwise (6) where the symbol σ represents the graph construction parameter, and N p (y .j ) denotes the set of the top q closest neighbours of the label y .j . Referring to the study  (Jian et al. 2016) , the value of σ is set to 1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "The Final Objective Function Of Aslsl",
      "text": "By combining Eq. (  3 ) and Eq. (  5 ) and introducing l 2,1 -norm sparsity on the projection matrix Q (v) , we can obtain the final cost function of ASLSL:\n\nwhere λ, η, γ, and δ are regularization parameters. The flowchart of ASLSL is shown in Fig.  2 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Optimization Strategy",
      "text": "The alternatively iterative update algorithm is adopted to derive solutions for the four variables (Q (v) , U , M , and α (v) ) in Eq. (  7 ). The ASLSL algorithm is presented as follows:\n\nUpdate Q (v) by fixing other variables When U , M , and α (v) are fixed, we remove the irrelevant terms and introduce a Lagrange multiplier Ψ for Q (v) ≥ 0. Then, we obtain the following function about\n\nBy taking the partial derivative w.r.t.Q (v) , we could get\n\nAccording to Karush-Kuhn-Tucker (KKT) complementary condition, Q (v) can be updated as follows:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Update U By Fixing Other Variables",
      "text": "When Q (v) , M , and α (v) are fixed, by introducing a Lagrange multiplier Θ for U ≥ 0, we have:\n\nThen, the partial derivative w.r.t.U is calculated as:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Update M By Fixing Other Variables",
      "text": "When Q (v) , U , and α (v) are fixed, we have the following Lagrange function by removing irrelevant terms and introducing a Lagrange multiplier Φ for M ≥ 0:\n\nThen, the partial derivative w.r.t.M is calculated as:\n\nVia KKT condition Φ ij M ij = 0, M can be updated as:\n\nUpdate α (v) by fixing other variables\n\nWhen other variables are fixed, we have\n\nThen, the optimization problem of α (v) is changed to\n\n(18) By introducing a Lagrange multiplier φ for m v=1 α (v) = 1, we have the following Lagrange function:\n\nThe partial derivative w.r.t.α (v) is calculated as:\n\n= 0, we have Update Q (v) via Eq. (  10 ); 4:\n\nUpdate U via Eq. (  13 );\n\n5:\n\nUpdate M via Eq. (  16 ); 6:\n\nUpdate α (v) via Eq. (  21 ); 7: until Convergence; 8: return Q (v) for physiological feature selection. 9: Sort the physiological features by ∥q\n\nAlgorithm 1 provides the specific optimization steps for Eq. (  7 ). The importance of each physiological feature in the multi-dimensional emotion recognition can be evaluated by Q (v) . Ultimately, the multi-modal physiological feature subset with informative features is obtained.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments Dataset Description",
      "text": "A comprehensive evaluation was conducted using two multidimensional emotion datasets, DREAMER  (Katsigiannis and Ramzan 2018)  and DEAP  (Koelstra et al. 2011) , which encompass multi-modal physiological signal data, to assess the performance of ASLSL. Both datasets adopted the valence-arousal-dominance framework to represent the subject's emotional state. Detailed descriptions of the experimental setup can be found in  (Katsigiannis and Ramzan 2018)  and  (Koelstra et al. 2011 ). The experimental process involved applying a band-pass filter with a frequency range from 1 to 50 Hz to remove noise from the EEG recordings. Following this, independent component analysis was employed to mitigate artifacts from the multi-modal physiological signal data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Feature Extraction",
      "text": "Based on previous research on physiological signal features for emotion recognition  (Jenke, Peer, and Buss 2014; Xu et al. 2020; Katsigiannis and Ramzan 2018; Koelstra et al. 2011) , thirteen distinct EEG features were extracted. These features encompass the absolute power, the beta-to-theta absolute power ratio, C0 complexity, higher-order crossing, non-stationary index, Shannon entropy, spectral entropy, differential entropy, differential asymmetry, rational asymmetry, the instantaneous phase of the Hilbert-transformed intrinsic mode functions, and the amplitude of the Hilberttransformed intrinsic mode functions.\n\nFor ECG data, we extracted thirteen distinct features related to heart rate and variability. These features encompass heart rate, mean, median, standard deviation, minimum, maximum, range of each segment of the PQRST complexes, as well as the difference between consecutive RR intervals, power spectral density for low frequency (LF) and high frequency (HF), the LF to HF ratio, SHE, and total power (Katsigiannis and Ramzan 2018).\n\nFinally, four distinct EOG features were extracted: eye blinking rate, signal energy, mean of the signal, and signal variance  (Koelstra et al. 2011) . A comprehensive description of these physiological features is available in  (Duan, Zhu, and Lu 2013; Jenke, Peer, and Buss 2014; Xu et al. 2023; Katsigiannis and Ramzan 2018; Koelstra et al. 2011) . The total dimensionality of these multi-modal physiological signal features are 676 for DREAMER and 1764 for DEAP.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "Comparative methods: To thoroughly assess the performance of ASLSL in multi-dimensional affective computing, seventeen advanced feature selection methods were compared. The comparative methods include:\n\n(1) Four feature selection methods commonly used in the BCI applications: mRMR  (Peng, Long, and Ding 2005) , RFS  (Nie et al. 2010) , FSOR  (Xu et al. 2020) , and GRMOR  (Xu et al. 2023) .\n\n(2) Nine popular multi-label feature selection approaches: RPMFS  (Cai, Nie, and Huang 2013) , SCLS (Lee and Kim 2017), MDFS  (Zhang et al. 2019a) , MGFS  (Hashemi, Dowlatshahi, and Nezamabadi-pour 2020b) , MFS-MCDM  (Hashemi, Dowlatshahi, and Nezamabadi-pour 2020a) , GRRO  (Zhang et al. 2020a) , SDFS  (Wang et al. 2020b ), SLMDS  (Li, Hu, and Gao 2023b) , and RFSFS  (Li, Hu, and Gao 2023a) .\n\n(3) Four advanced multi-view multi-label feature selection methods: MSFS  (Zhang et al. 2020c) , DHLI  (Hao, Liu, and Gao 2024) , UGRFS  (Hao, Liu, and Gao 2025) , and EF 2 FS  (Hao, Gao, and Hu 2025) .\n\nExperimental details: The multi-modal physiological data were categorized into low and high values based on self-assessed scores for each affective dimension, with a threshold value of five. Multi-label k-nearest neighbor (ML-KNN)  (Zhang and Zhou 2007)  was employed as the base classifier, with the number of nearest neighbors set to 10 and the smoothing parameter set to 1. Seventy percent of the participants were randomly selected for the training set, while the remaining thirty percent constituted the test set. A crosssubject experimental design was utilized to ensure unbiased results. To minimize bias, 50 separate and independent experiments were conducted, and the average results were used as the final measure. It is noteworthy that physiological feature extraction was performed on the entire trial as a single sample, without subdividing trials into smaller segments to augment the sample size for the experiments.\n\nThe strategy described in  (Liu et al. 2018a ) was adopted to simulate an incomplete multi-modal physiological data scenario by removing specific percentages of instances from each modality to represent partial data absence. The missing data ratio ranged from 10% to 50%, in increments of 10%. Approximately 10% of all features were selected using the feature selection methods. The parameters (λ, η, and δ) were adjusted within the range of 10 -3 to 10 3 with a step size of 10 1 . The parameter γ varied within the set {2, 3, 4, 5, 6, 7, 8, 9}. The parameters of comparative methods were set as described in the corresponding references.\n\nPerformance metrics: To evaluate the effectiveness of multi-dimensional affect computing, six metrics were utilized. These metrics included two label-based measures (micro-F1 (MI) and macro-F1 (MA)) and four examplebased metrics: Hamming loss (HL), ranking loss (RL), average precision (AP), and coverage (CV). For CV, HL, and RL, lower values indicate superior performance, ideally approaching zero. For MA, MI, and AP, higher values correspond to better classification outcomes, theoretically nearing the maximum value of 1. These metrics collectively quantify model performance across dimensions such as label correlation, classification accuracy, and generalization capability, forming the core evaluation framework for multi-label tasks. For further details, refer to  (Zhang et al. 2019a) .    axis shows the performance of each method according to the given indicator. In all the subgraphs, ASLSL method is denoted by the red line. As depicted in Fig.  4  and Fig.  3 , ASLSL consistently outperforms the other seventeen methods across all indicators and various levels of missing data.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Emotion Recognition Performance With Incomplete Physiological Data",
      "text": "To further evaluate the performance of the eighteen feature selection methods, a Friedman test was conducted. The significance level (α) was set to 0.05. The results of this statistical significance test are summarized in Table  1 , which indicates that the null hypothesis is rejected, signifying significant differences in performance among the eighteen feature selection methods for multi-dimensional emotion recognition with incomplete multi-modal physiological signal data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Experiments",
      "text": "To evaluate the contributions of each component within the proposed EEG feature selection model, we conducted abla-  tion experiments. The ASLSL framework comprises three essential modules, and we systematically removed each module to assess its impact. Table  2  reveals that adaptive shared latent structure learning is pivotal for addressing the challenges posed by incomplete data, as it facilitates the exploration of a common space for multi-modal physiological signals and multi-dimensional emotions. The other modules are instrumental in preserving local geometric structures and evaluating the significance of each model.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Convergence Analysis",
      "text": "Additionally, we performed an analysis to assess the convergence speed of the iterative optimization algorithm proposed. Fig.  5  illustrates the convergence trajectories of the objective function for both the DREAMER and DEAP datasets. The tradeoff parameters (λ, η, and δ) were all set to 1 and r was set to 2. As shown in Fig.  5 , the ASLSL algorithm exhibits rapid convergence, highlighting the effectiveness of our optimization algorithm.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Parameter Sensitivity",
      "text": "This section presents a sensitivity analysis of ASLSL concerning these four parameters. For each analysis, one parameter was held constant at 0.1 (γ = 2), while the others were varied within the specified range. Due to space limitations, we provide only the sensitivity analysis results for the DEAP dataset. The 3-D histograms, depicting the AP values, are presented in Fig.  6 . As shown in Fig.  6 , the AP values remain relatively stable across variations in two parameters, indicating that the performance of ASLSL is not sensitive to these balance parameters.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Computational Cost",
      "text": "The computational time required by each method was assessed. The implementation was carried out using MATLAB (MathWorks Inc., Novi, MI, USA) and was executed on a computer running Microsoft Windows 11 × 64, featuring an Intel i5-13500H 2.60 GHz CPU and 16.00 GB of RAM. The results for average computational time are summarized in Table  3 . As illustrated in Table  3 , ASLSL has superior performance in recognizing emotions from incomplete data while maintaining a relatively low computational cost.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusions",
      "text": "We introduce a novel approach for multi-modal physiological signal feature selection specifically designed to handle incomplete physiological signal data in emotion recognition tasks. This method utilizes adaptive shared latent structure learning to investigate a unified space for multi-modal physiological signals and multi-dimensional emotions, effectively mitigating the effects of missing information and enabling the selection of informative features. Furthermore, we propose an efficient iterative algorithm to address the ASLSL optimization problem. Experimental results demonstrate that ASLSL markedly surpasses seventeen advanced methods in performance.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An illustration of the multi-modal affective BCI",
      "page": 1
    },
    {
      "caption": "Figure 2: The proposed ASLSL framework includes the following processes: (a) adaptive shared latent structure learning; (b)",
      "page": 3
    },
    {
      "caption": "Figure 2: Optimization Strategy",
      "page": 4
    },
    {
      "caption": "Figure 3: Multi-dimensional emotion recognition perfor-",
      "page": 6
    },
    {
      "caption": "Figure 4: and Fig. 3 illustrate the comparative performance re-",
      "page": 6
    },
    {
      "caption": "Figure 4: Multi-dimensional emotion recognition perfor-",
      "page": 6
    },
    {
      "caption": "Figure 4: and Fig. 3,",
      "page": 6
    },
    {
      "caption": "Figure 5: Convergence verification of ASLSL.",
      "page": 7
    },
    {
      "caption": "Figure 6: The parameter sensitivity results on DEAP.",
      "page": 7
    },
    {
      "caption": "Figure 5: illustrates the convergence trajectories of the",
      "page": 7
    },
    {
      "caption": "Figure 5: , the ASLSL algo-",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Recently, multi-modal\nphysiological\nsignals\nbased\nemo-"
        },
        {
          "Abstract": "tion\nrecognition\nhas\ngarnered\nincreasing\nattention\nin\nthe"
        },
        {
          "Abstract": "field of brain-computer\ninterfaces. Nevertheness,\nthe asso-"
        },
        {
          "Abstract": "ciated multi-modal\nphysiological\nfeatures\nare\noften\nhigh-"
        },
        {
          "Abstract": "dimensional and inevitably include irrelevant, redundant, and"
        },
        {
          "Abstract": "noisy representation, which can easily lead to overfitting,"
        },
        {
          "Abstract": "poor\nperformance,\nand\nhigh\ncomputational\ncomplexity\nin"
        },
        {
          "Abstract": "emotion classifiers. Feature selection has been widely applied"
        },
        {
          "Abstract": "to address these challenges. However, previous studies gen-"
        },
        {
          "Abstract": "erally assumed that multi-modal physiological data are com-"
        },
        {
          "Abstract": "plete, whereas in reality,\nthe data are often incomplete due"
        },
        {
          "Abstract": "to the openness of\nthe acquisition and operational environ-"
        },
        {
          "Abstract": "ment. For example, a part of samples are available in sev-"
        },
        {
          "Abstract": "eral modalities but not\nin others. To address this issue, we"
        },
        {
          "Abstract": "propose a novel method for incomplete multi-modal physio-"
        },
        {
          "Abstract": "logical signal\nfeature selection called adaptive shared latent"
        },
        {
          "Abstract": "structure learning (ASLSL). Based on the property that sim-"
        },
        {
          "Abstract": "ilar features share similar emotional\nlabels, ASLSL employs"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "adaptive shared latent structure learning to explore a common"
        },
        {
          "Abstract": "latent space shared for incomplete multi-modal physiological"
        },
        {
          "Abstract": "signals and multi-dimensional emotional labels, thereby miti-"
        },
        {
          "Abstract": "gating the impact of missing information and mining consen-"
        },
        {
          "Abstract": "sus information. Two most popular multi-modal physiolog-"
        },
        {
          "Abstract": "ical emotion datasets (DEAP and DREAMER) with multi-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "dimensional emotional\nlabels were utilized to compare the"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "performance between compare ASLSL and seventeen fea-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ture selection methods. Comprehensive experimental results"
        },
        {
          "Abstract": "on these datasets demonstrate the effectiveness of ASLSL."
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Introduction"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Existing affective computing researches predominantly uti-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "lize discrete or multi-dimensional emotion models to rep-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "resent distinct emotional states. Compared to discrete emo-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "tion models, multi-dimensional emotional label models offer"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "broader characterization and are capable of describing the"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "variations in emotional states (Khare et al. 2023). Currently,"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "emotion recognition approaches predominantly encompass"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "facial expressions, speech, body gestures,\ntext, and neuro-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "physiological signals(Ezzameli and Mahersia 2023). Emo-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "tions arise alongside physiological and psychological activi-"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "ties, and neurophysiological signals offer\nthe advantage of"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "Copyright © 2026, Association for the Advancement of Artificial"
        },
        {
          "Abstract": "Intelligence (www.aaai.org). All rights reserved."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "sultant features are often high-dimensional and may contain": "redundant,\nirrelevant, or noisy data(Xu et al. 2023). This",
          "cilitating the construction of informative emotional feature": "subsets."
        },
        {
          "sultant features are often high-dimensional and may contain": "can lead to challenges such as overfitting, diminished per-",
          "cilitating the construction of informative emotional feature": "Furthermore, the contributions of our work are as follows:"
        },
        {
          "sultant features are often high-dimensional and may contain": "formance, and increased computational complexity (Wang",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "• We propose a novel\nincomplete multi-modal physiolog-"
        },
        {
          "sultant features are often high-dimensional and may contain": "et al. 2020a). Feature selection has emerged as a crucial tech-",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "ical\nfeature selection method for\nthe multi-dimensional"
        },
        {
          "sultant features are often high-dimensional and may contain": "nique for identifying and retaining significant features while",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "affective\ncomputing\ntask. ASLSL\nemploys\nadaptive"
        },
        {
          "sultant features are often high-dimensional and may contain": "removing irrelevant and noisy ones from the data. This ap-",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "shared latent structure learning to explore a latent space"
        },
        {
          "sultant features are often high-dimensional and may contain": "proach helps maintain the core physiological and psycholog-",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "shared for incomplete multi-modal signal data and multi-"
        },
        {
          "sultant features are often high-dimensional and may contain": "ical representations of physiological signal features, thereby",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "dimensional emotional\nrepresentation,\nthereby mitigat-"
        },
        {
          "sultant features are often high-dimensional and may contain": "enhancing the transparency and interpretability of affective",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "ing the impact of missing information and mining con-"
        },
        {
          "sultant features are often high-dimensional and may contain": "computing models (Jenke, Peer, and Buss 2014; Liu et al.",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "sensus\ninformation. With\nthe\nabove\nstrategy, ASLSL"
        },
        {
          "sultant features are often high-dimensional and may contain": "2018b).",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "could adaptively adjust\nthe discriminative\nanalysis\nre-"
        },
        {
          "sultant features are often high-dimensional and may contain": "Feature selection methods can be broadly categorized into",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "sults\nfor\nvarious\nfeature\ntypes within\nthe\nincomplete"
        },
        {
          "sultant features are often high-dimensional and may contain": "three main types based on their feature evaluation and search",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "multi-modal data,\nthus\nrealizing efficient and accurate"
        },
        {
          "sultant features are often high-dimensional and may contain": "mechanisms: filter methods, wrapper methods, and embed-",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "multi-dimensional affective computing. To the best of"
        },
        {
          "sultant features are often high-dimensional and may contain": "ded methods\n(Li\net\nal. 2017). Filter methods\nassess\nthe",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "our knowledge,\nthis is the first study to construct an in-"
        },
        {
          "sultant features are often high-dimensional and may contain": "importance of neurophysiological\nsignal\nfeatures\nin emo-",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "complete multi-view feature\nselection framework in a"
        },
        {
          "sultant features are often high-dimensional and may contain": "tion recognition using statistical metrics or information en-",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "",
          "cilitating the construction of informative emotional feature": "complete multi-label data scenario."
        },
        {
          "sultant features are often high-dimensional and may contain": "tropy measures. However, these methods often yield subop-",
          "cilitating the construction of informative emotional feature": ""
        },
        {
          "sultant features are often high-dimensional and may contain": "timal feature selection results, regardless of the performance",
          "cilitating the construction of informative emotional feature": "• To address the optimization challenges associated with"
        },
        {
          "sultant features are often high-dimensional and may contain": "of classifiers(Zhang et al. 2019b). To address these limita-",
          "cilitating the construction of informative emotional feature": "ASLSL, an efficient alternative approach is proposed to"
        },
        {
          "sultant features are often high-dimensional and may contain": "tions,\nseveral\nstudies have investigated wrapper methods,",
          "cilitating the construction of informative emotional feature": "ensure convergence and attain an optimal solution."
        },
        {
          "sultant features are often high-dimensional and may contain": "which generally offer\nsuperior\nclassification performance",
          "cilitating the construction of informative emotional feature": "• To validate the efficacy of ASLSL, we utilized two pub-"
        },
        {
          "sultant features are often high-dimensional and may contain": "compared to filter methods. This is because wrapper meth-",
          "cilitating the construction of informative emotional feature": "licly available datasets: DREAMER and DEAP. Experi-"
        },
        {
          "sultant features are often high-dimensional and may contain": "ods evaluate feature subsets based on the performance of",
          "cilitating the construction of informative emotional feature": "mental results show that ASLSL outperforms seventeen"
        },
        {
          "sultant features are often high-dimensional and may contain": "a specific classifier\n(Hou et al. 2011). Nonetheless, wrap-",
          "cilitating the construction of informative emotional feature": "advanced feature selection methods, achieving superior"
        },
        {
          "sultant features are often high-dimensional and may contain": "per methods can be computationally intensive due to the",
          "cilitating the construction of informative emotional feature": "emotion recognition performance across six metrics."
        },
        {
          "sultant features are often high-dimensional and may contain": "extensive number of\ntrials\nrequired (Zhang et al. 2019b).",
          "cilitating the construction of informative emotional feature": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "provide multi-level emotional\nrepresentations that enhance": "the precision of emotion recognition(Geetha et al. 2024).",
          "tent space for multi-modal physiological signals and multi-": "dimensional emotions, even in the presence of\nincomplete"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "The proliferation of sensors for affective computing has",
          "tent space for multi-modal physiological signals and multi-": "multi-modal\ninformation. Additionally,\nit employs adaptive"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "accelerated due to advancements in multi-modal physiolog-",
          "tent space for multi-modal physiological signals and multi-": "learning of modality weights to assess the significance of"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "ical signal acquisition technologies,\nresulting in a substan-",
          "tent space for multi-modal physiological signals and multi-": "different modalities\nin emotion recognition. By the above"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "tial volume of features extracted from these sensors (Becker",
          "tent space for multi-modal physiological signals and multi-": "means, ASLSL could effectively uncovers the relationships"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "et al. 2017). However, the limited availability of high-quality",
          "tent space for multi-modal physiological signals and multi-": "between incomplete multi-modal neurophysiological data"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "multi-modal physiological signal samples means that the re-",
          "tent space for multi-modal physiological signals and multi-": "and multi-dimensional emotional representation, thereby fa-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "sultant features are often high-dimensional and may contain",
          "tent space for multi-modal physiological signals and multi-": "cilitating the construction of informative emotional feature"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "redundant,\nirrelevant, or noisy data(Xu et al. 2023). This",
          "tent space for multi-modal physiological signals and multi-": "subsets."
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "can lead to challenges such as overfitting, diminished per-",
          "tent space for multi-modal physiological signals and multi-": "Furthermore, the contributions of our work are as follows:"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "formance, and increased computational complexity (Wang",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "• We propose a novel\nincomplete multi-modal physiolog-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "et al. 2020a). Feature selection has emerged as a crucial tech-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "ical\nfeature selection method for\nthe multi-dimensional"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "nique for identifying and retaining significant features while",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "affective\ncomputing\ntask. ASLSL\nemploys\nadaptive"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "removing irrelevant and noisy ones from the data. This ap-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "shared latent structure learning to explore a latent space"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "proach helps maintain the core physiological and psycholog-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "shared for incomplete multi-modal signal data and multi-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "ical representations of physiological signal features, thereby",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "dimensional emotional\nrepresentation,\nthereby mitigat-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "enhancing the transparency and interpretability of affective",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "ing the impact of missing information and mining con-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "computing models (Jenke, Peer, and Buss 2014; Liu et al.",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "sensus\ninformation. With\nthe\nabove\nstrategy, ASLSL"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "2018b).",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "could adaptively adjust\nthe discriminative\nanalysis\nre-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "Feature selection methods can be broadly categorized into",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "sults\nfor\nvarious\nfeature\ntypes within\nthe\nincomplete"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "three main types based on their feature evaluation and search",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "multi-modal data,\nthus\nrealizing efficient and accurate"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "mechanisms: filter methods, wrapper methods, and embed-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "multi-dimensional affective computing. To the best of"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "ded methods\n(Li\net\nal. 2017). Filter methods\nassess\nthe",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "our knowledge,\nthis is the first study to construct an in-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "importance of neurophysiological\nsignal\nfeatures\nin emo-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "complete multi-view feature\nselection framework in a"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "tion recognition using statistical metrics or information en-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "complete multi-label data scenario."
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "tropy measures. However, these methods often yield subop-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "timal feature selection results, regardless of the performance",
          "tent space for multi-modal physiological signals and multi-": "• To address the optimization challenges associated with"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "of classifiers(Zhang et al. 2019b). To address these limita-",
          "tent space for multi-modal physiological signals and multi-": "ASLSL, an efficient alternative approach is proposed to"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "tions,\nseveral\nstudies have investigated wrapper methods,",
          "tent space for multi-modal physiological signals and multi-": "ensure convergence and attain an optimal solution."
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "which generally offer\nsuperior\nclassification performance",
          "tent space for multi-modal physiological signals and multi-": "• To validate the efficacy of ASLSL, we utilized two pub-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "compared to filter methods. This is because wrapper meth-",
          "tent space for multi-modal physiological signals and multi-": "licly available datasets: DREAMER and DEAP. Experi-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "ods evaluate feature subsets based on the performance of",
          "tent space for multi-modal physiological signals and multi-": "mental results show that ASLSL outperforms seventeen"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "a specific classifier\n(Hou et al. 2011). Nonetheless, wrap-",
          "tent space for multi-modal physiological signals and multi-": "advanced feature selection methods, achieving superior"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "per methods can be computationally intensive due to the",
          "tent space for multi-modal physiological signals and multi-": "emotion recognition performance across six metrics."
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "extensive number of\ntrials\nrequired (Zhang et al. 2019b).",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "Recently, embedded methods have gained attention as a vi-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "able alternative to overcome the drawbacks of filter methods.",
          "tent space for multi-modal physiological signals and multi-": "Problem formulation"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "These methods integrate the process of feature selection di-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "The ASLSL framework is formulated as follows:"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "rectly into the model optimization framework. Experimen-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "´",
          "tent space for multi-modal physiological signals and multi-": "min\nF (X (v), Q(v), S(v), M, U, Y )+λC(U )+γΩ(Q(v))"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "tal results in (Torres-Valencia,\nAlvarez-L´opez, and Orozco-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "Q(v),M,U"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "Guti´errez 2017; Xu et al. 2023) shows the effectiveness of",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "(1)"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "the embedded approaches in emotion recognition.",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "where γ and λ are both regularization parameters. X ="
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "Existing studies on physiological\nfeature selection typi-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "(cid:8)X (1), X (2), · · ·\n, X (v), · · ·\n, X (m)(cid:9) is multi-modal phys-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "cally assume that multi-modal physiological signal data are",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "iological\nsignal\nfeature data,\nand each element X (v) ="
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "complete. However, due to the inherent variability of\nthe",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "∈\nY\n∈\n{0, 1}k×n\nRdv×n.\n[x1, x2, ..., xdv ]T . X (v)"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "acquisition environment, practical applications of emotion",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "S\n=\nis\na multi-dimensional\nemotional\nlabel matrix."
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "recognition based on multi-modal neurophysiological\nsig-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "(cid:8)S(1), S(2), ..., S(v), ..., S(m)(cid:9) where S(v) ∈ Rn×n is an"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "nals often encounter\nincomplete\nsample data. For\nexam-",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "",
          "tent space for multi-modal physiological signals and multi-": "indicator matrix that marks the location of\nthe missing in-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "ple,\nsome samples may be available in certain modalities",
          "tent space for multi-modal physiological signals and multi-": ""
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "but not\nin others. This\nlack of\ncomplete\ninformation di-",
          "tent space for multi-modal physiological signals and multi-": "stances. U ∈ Rn×k, Q(v) ∈ Rdv×k, and M ∈ Rk×k are"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "rectly impedes\nthe\naccurate modeling of\nthe\nrelationship",
          "tent space for multi-modal physiological signals and multi-": "shared space of all modalities, projection matrix of X (v),"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "between multi-modal neurophysiological signals and multi-",
          "tent space for multi-modal physiological signals and multi-": "and coefficient matrix of Y ,\nrespectively. d(v), n, m, and"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "dimensional emotional labels(Wang, Li, and Cui 2024).",
          "tent space for multi-modal physiological signals and multi-": "k\ncorrespond to the number of\nfeatures of\neach modal,"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "To address this issue, as illustrated in Fig.1, we propose",
          "tent space for multi-modal physiological signals and multi-": "instances, modalities, and dimensions of\nlabeling,\nrespec-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "a novel feature selection model for incomplete multi-modal",
          "tent space for multi-modal physiological signals and multi-": "tively."
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "physiological\nfeature selection,\ntermed adaptive shared la-",
          "tent space for multi-modal physiological signals and multi-": "The\nadaptive\nshared latent\nstructure\nterm, graph-based"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "tent structure learning (ASLSL). This model utilizes adap-",
          "tent space for multi-modal physiological signals and multi-": "manifold regularization term, and l2,1-norm sparsity restric-"
        },
        {
          "provide multi-level emotional\nrepresentations that enhance": "tive shared latent structure learning to explore a common la-",
          "tent space for multi-modal physiological signals and multi-": "tion term are represented by the symbols F , C, and Ω, re-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Q(v) ≥ 0,": "U ≥ 0,\ns.t.M ≥ 0,"
        },
        {
          "Q(v) ≥ 0,": ""
        },
        {
          "Q(v) ≥ 0,": ""
        },
        {
          "Q(v) ≥ 0,": "m(cid:88) v\n0 ≤ α(v) ≤ 1,\nα(v) = 1,\nv = 1, . . . , m"
        },
        {
          "Q(v) ≥ 0,": "=1"
        },
        {
          "Q(v) ≥ 0,": "(3)"
        },
        {
          "Q(v) ≥ 0,": "where γ and λ are both regularization parameters."
        },
        {
          "Q(v) ≥ 0,": "The diagonal element S(v)\nof the diagonal weighted ma-\nj,j"
        },
        {
          "Q(v) ≥ 0,": ""
        },
        {
          "Q(v) ≥ 0,": "trix S(v) in the v-th modal is defined as follows:"
        },
        {
          "Q(v) ≥ 0,": ""
        },
        {
          "Q(v) ≥ 0,": "(cid:26) 1\nif j-th instance exists in v-th modal;"
        },
        {
          "Q(v) ≥ 0,": "S(v)"
        },
        {
          "Q(v) ≥ 0,": "(4)\nj,j ="
        },
        {
          "Q(v) ≥ 0,": "0\notherwise."
        },
        {
          "Q(v) ≥ 0,": ""
        },
        {
          "Q(v) ≥ 0,": "Graph-based manifold regularization term"
        },
        {
          "Q(v) ≥ 0,": "Based on the spectral graph theory(Jian et al. 2016), a graph-"
        },
        {
          "Q(v) ≥ 0,": "based manifold regularizer Tr (cid:0)U T LY U (cid:1) is employed to"
        },
        {
          "Q(v) ≥ 0,": "maintain consistency of\nthe local geometric structures be-"
        },
        {
          "Q(v) ≥ 0,": "tween the shared latent\nstructure space U and the multi-"
        },
        {
          "Q(v) ≥ 0,": "dimensional emotional label space Y . The graph-based man-"
        },
        {
          "Q(v) ≥ 0,": "ifold regularization term C can be formulated as follows:"
        },
        {
          "Q(v) ≥ 0,": ""
        },
        {
          "Q(v) ≥ 0,": "(5)\nC(U ) = Tr (cid:0)U T LY U (cid:1)"
        },
        {
          "Q(v) ≥ 0,": ""
        },
        {
          "Q(v) ≥ 0,": "where a graph Laplacian matrix LY ∈ Rn×n for Y is indi-"
        },
        {
          "Q(v) ≥ 0,": "is the affinity\ncated by the notation LY = GY − SY . SY"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "graph for Y , and GY is the diagonal matrix with GY (i, i) =": "(cid:80)n",
          "According to Karush-Kuhn-Tucker": "",
          "(KKT) complemen-": ""
        },
        {
          "graph for Y , and GY is the diagonal matrix with GY (i, i) =": "j=1 SY (i, j). A heat kernel has been employed to gener-",
          "According to Karush-Kuhn-Tucker": "tary condition, Q(v) can be updated as follows:",
          "(KKT) complemen-": ""
        },
        {
          "graph for Y , and GY is the diagonal matrix with GY (i, i) =": "ate the affinity graph SY . The similarity value of two labels,",
          "According to Karush-Kuhn-Tucker": "",
          "(KKT) complemen-": ""
        },
        {
          "graph for Y , and GY is the diagonal matrix with GY (i, i) =": "",
          "According to Karush-Kuhn-Tucker": "X (v)S(v)S(v)⊤\nU",
          "(KKT) complemen-": ""
        },
        {
          "graph for Y , and GY is the diagonal matrix with GY (i, i) =": "y.i and y.j, is represented by the element SY (i, j). SY (i, j)",
          "According to Karush-Kuhn-Tucker": "",
          "(KKT) complemen-": ""
        },
        {
          "graph for Y , and GY is the diagonal matrix with GY (i, i) =": "",
          "According to Karush-Kuhn-Tucker": "Q(v) ← Q(v) ⊙",
          "(KKT) complemen-": "(10)"
        },
        {
          "graph for Y , and GY is the diagonal matrix with GY (i, i) =": "is defined as follows:",
          "According to Karush-Kuhn-Tucker": "Q(v)U ⊤S(v)S(v)U + γD(v)Q(v)",
          "(KKT) complemen-": ""
        },
        {
          "graph for Y , and GY is the diagonal matrix with GY (i, i) =": "(cid:16)\n(cid:17)",
          "According to Karush-Kuhn-Tucker": "",
          "(KKT) complemen-": ""
        },
        {
          "graph for Y , and GY is the diagonal matrix with GY (i, i) =": "− ∥y.i−y.j ∥2",
          "According to Karush-Kuhn-Tucker": "",
          "(KKT) complemen-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Then, the optimization problem of α(v) is changed to": "(cid:16)",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "m(cid:88) v\nm(cid:88) v\nmin\na(v)(cid:17)γ\nd(v)\nα(v) = 1, 0 ≤ α(v) ≤ 1\ns.t.",
          "Feature extraction": "Based on previous research on physiological signal features"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "a(v)",
          "Feature extraction": "for emotion recognition (Jenke, Peer, and Buss 2014; Xu"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "=1\n=1",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "et al. 2020; Katsigiannis and Ramzan 2018; Koelstra et al."
        },
        {
          "Then, the optimization problem of α(v) is changed to": "(18)",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "2011),\nthirteen distinct EEG features were extracted. These"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "By introducing a Lagrange multiplier φ for (cid:80)m",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "v=1 α(v) =",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "features encompass the absolute power, the beta-to-theta ab-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "1, we have the following Lagrange function:",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "solute power\nratio, C0 complexity, higher-order crossing,"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "(cid:34) m\n(cid:35)",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "(cid:16)\n(cid:16)",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "m(cid:88) v\n(cid:88) v\n=\nd(v) − φ\na(v) − 1\nL\na(v)(cid:17)\na(v)(cid:17)γ\n(19)",
          "Feature extraction": "non-stationary index, Shannon entropy, spectral entropy, dif-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "ferential entropy, differential asymmetry, rational asymme-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "=1\n=1",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "try,\nthe instantaneous phase of\nthe Hilbert-transformed in-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "The partial derivative w.r.t.α(v) is calculated as:",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "trinsic mode functions, and the amplitude of\nthe Hilbert-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "∂L (cid:0)α(v)(cid:1)",
          "Feature extraction": "transformed intrinsic mode functions."
        },
        {
          "Then, the optimization problem of α(v) is changed to": "= γ\nα(v)(cid:17)γ−1\nd(v) − φ\n(20)",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "For ECG data, we extracted thirteen distinct features re-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "∂α(v)",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "lated to heart\nrate and variability. These features encom-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "∂L(α(v))",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "= 0, we have\nSet",
          "Feature extraction": "pass heart rate, mean, median, standard deviation, minimum,"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "∂α(v)",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "1",
          "Feature extraction": "maximum, range of each segment of the PQRST complexes,"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "1−γ\n(cid:0)d(v)(cid:1)",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "as well as the difference between consecutive RR intervals,"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "α(v) =\n(21)\n1",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "(cid:80)m",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "1−γ\n(cid:0)d(v)(cid:1)",
          "Feature extraction": "power spectral density for low frequency (LF) and high fre-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "v=1",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "quency (HF), the LF to HF ratio, SHE, and total power (Kat-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "sigiannis and Ramzan 2018)."
        },
        {
          "Then, the optimization problem of α(v) is changed to": "Algorithm 1: Adaptive Shared Latent Structure Learning",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "Finally,\nfour distinct EOG features were extracted: eye"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "Input: Multi-modal physiological\nsignal\nfeature data X,",
          "Feature extraction": "blinking rate, signal energy, mean of the signal, and signal"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "multi-dimensional\nemotional\nlabel matrix\nY ,\nand",
          "Feature extraction": "variance (Koelstra et al. 2011). A comprehensive descrip-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "modal-missing indicator S.",
          "Feature extraction": "tion of\nthese physiological\nfeatures\nis available in (Duan,"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "Output: Return ranked physiological features.",
          "Feature extraction": "Zhu, and Lu 2013; Jenke, Peer, and Buss 2014; Xu et al."
        },
        {
          "Then, the optimization problem of α(v) is changed to": "1:\nInitial Q(v) , U , M , and α(v) randomly.",
          "Feature extraction": "2023; Katsigiannis and Ramzan 2018; Koelstra et al. 2011)."
        },
        {
          "Then, the optimization problem of α(v) is changed to": "2:\nrepeat",
          "Feature extraction": "The total dimensionality of these multi-modal physiological"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "3:\nUpdate Q(v) via Eq. (10);",
          "Feature extraction": "signal features are 676 for DREAMER and 1764 for DEAP."
        },
        {
          "Then, the optimization problem of α(v) is changed to": "4:\nUpdate U via Eq. (13);",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "5:\nUpdate M via Eq. (16);",
          "Feature extraction": "Experimental setup"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "6:\nUpdate α(v) via Eq. (21);",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "Comparative methods: To thoroughly assess\nthe perfor-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "7: until Convergence;",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "mance of ASLSL in multi-dimensional affective computing,"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "8:\nreturn Q(v) for physiological feature selection.",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "seventeen advanced feature selection methods were com-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "9: Sort the physiological features by ∥q(v)\n∥2;",
          "Feature extraction": "pared. The comparative methods include:"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "i",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "(1) Four feature selection methods commonly used in the"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "BCI\napplications: mRMR (Peng, Long,\nand Ding 2005),"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "Algorithm 1 provides the specific optimization steps for",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "RFS (Nie et al. 2010), FSOR (Xu et al. 2020), and GRMOR"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "Eq. (7). The importance of each physiological feature in the",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "(Xu et al. 2023)."
        },
        {
          "Then, the optimization problem of α(v) is changed to": "multi-dimensional emotion recognition can be evaluated by",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "(2) Nine popular multi-label feature selection approaches:"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "Q(v). Ultimately, the multi-modal physiological feature sub-",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "RPMFS\n(Cai, Nie,\nand Huang\n2013),\nSCLS\n(Lee\nand"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "set with informative features is obtained.",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "Kim 2017), MDFS(Zhang et al. 2019a), MGFS (Hashemi,"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "Dowlatshahi, and Nezamabadi-pour 2020b), MFS-MCDM"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "Experiments",
          "Feature extraction": ""
        },
        {
          "Then, the optimization problem of α(v) is changed to": "",
          "Feature extraction": "(Hashemi,\nDowlatshahi,\nand\nNezamabadi-pour\n2020a),"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "Dataset description",
          "Feature extraction": "GRRO (Zhang et al. 2020a), SDFS (Wang et al. 2020b),"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "A comprehensive evaluation was conducted using two multi-",
          "Feature extraction": "SLMDS (Li, Hu, and Gao 2023b), and RFSFS (Li, Hu, and"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "dimensional\nemotion\ndatasets, DREAMER (Katsigiannis",
          "Feature extraction": "Gao 2023a)."
        },
        {
          "Then, the optimization problem of α(v) is changed to": "and Ramzan 2018) and DEAP (Koelstra et al. 2011), which",
          "Feature extraction": "(3) Four advanced multi-view multi-label\nfeature selec-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "encompass multi-modal physiological\nsignal data,\nto as-",
          "Feature extraction": "tion methods: MSFS (Zhang et al. 2020c), DHLI (Hao, Liu,"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "sess the performance of ASLSL. Both datasets adopted the",
          "Feature extraction": "and Gao 2024), UGRFS (Hao, Liu,\nand Gao 2025),\nand"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "valence-arousal-dominance framework to represent the sub-",
          "Feature extraction": "EF2FS (Hao, Gao, and Hu 2025)."
        },
        {
          "Then, the optimization problem of α(v) is changed to": "ject’s emotional\nstate. Detailed descriptions of\nthe exper-",
          "Feature extraction": "Experimental details: The multi-modal\nphysiological"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "imental\nsetup can be found in (Katsigiannis and Ramzan",
          "Feature extraction": "data were categorized into low and high values based on"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "2018) and (Koelstra et al. 2011). The experimental process",
          "Feature extraction": "self-assessed scores\nfor\neach affective dimension, with a"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "involved applying a band-pass filter with a frequency range",
          "Feature extraction": "threshold value of five. Multi-label k-nearest neighbor (ML-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "from 1 to 50 Hz to remove noise from the EEG record-",
          "Feature extraction": "KNN)\n(Zhang and Zhou 2007) was employed as the base"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "ings. Following this,\nindependent component analysis was",
          "Feature extraction": "classifier, with the number of nearest neighbors set to 10 and"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "employed to mitigate artifacts from the multi-modal physio-",
          "Feature extraction": "the smoothing parameter set to 1. Seventy percent of the par-"
        },
        {
          "Then, the optimization problem of α(v) is changed to": "logical signal data.",
          "Feature extraction": "ticipants were randomly selected for the training set, while"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: The Friedman test results (significance level α =",
      "data": [
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "mance of various missing ratios (MR) on DREAMER.",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "mance of various missing ratios (MR) on DEAP."
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Evaluation metric\nCritical value\nFF"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "the remaining thirty percent constituted the test set. A cross-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Ranking loss\n13.282"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "subject experimental design was utilized to ensure unbiased",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Coverage\n16.986"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "results. To minimize bias, 50 separate and independent ex-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Hamming loss\n10.098"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "periments were conducted, and the average results were used",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Average precision\n14.954\n≈ 2.272"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "as the final measure. It is noteworthy that physiological fea-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Macro-F1\n4.673"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Micro-F1\n4.694"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "ture extraction was performed on the entire trial as a single",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "sample, without subdividing trials into smaller segments to",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Table 1: The Friedman test\nresults (significance level α ="
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "augment the sample size for the experiments.",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "0.05)."
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "The strategy described in (Liu et al. 2018a) was adopted",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "to simulate an incomplete multi-modal physiological data",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Average precision"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "scenario by removing specific percentages of instances from",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Conditions"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "DEAP\nDREAMER"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "each modality to represent partial data absence. The miss-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "w/o ASLSL\n0.72\n0.76"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "ing data ratio ranged from 10% to 50%,\nin increments of",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "w/o GMR\n0.76\n0.79"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "10%. Approximately 10% of all features were selected us-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "w/o AMWE\n0.75\n0.77"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "ing the\nfeature\nselection methods. The parameters\n(λ, η,",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Our method\n0.82\n0.84"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "and δ) were adjusted within the range of 10−3 to 103 with",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "a step size of 101. The parameter γ varied within the set",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Table 2: The results of ablation experiments (w/o, ASLSL,"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "{2, 3, 4, 5, 6, 7, 8, 9}. The parameters of comparative meth-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "GMR, and AMWE denote without, adaptive shared latent"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "ods were set as described in the corresponding references.",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "structure\nlearning, graph-based manifold regularizer,\nand"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "Performance metrics: To evaluate the effectiveness of",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "adaptive modal weight evaluation, respectively)."
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "multi-dimensional affect computing,\nsix metrics were uti-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "lized. These metrics\nincluded\ntwo\nlabel-based measures",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "(micro-F1 (MI)\nand macro-F1 (MA))\nand four\nexample-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "axis\nshows\nthe performance of each method according to"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "based metrics: Hamming loss (HL), ranking loss (RL), av-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "the given indicator. In all\nthe subgraphs, ASLSL method is"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "erage precision (AP), and coverage (CV). For CV, HL, and",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "denoted by the red line. As depicted in Fig. 4 and Fig. 3,"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "RL, lower values indicate superior performance, ideally ap-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "ASLSL consistently outperforms the other seventeen meth-"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "proaching zero. For MA, MI, and AP, higher values corre-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "ods across all indicators and various levels of missing data."
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "spond to better classification outcomes, theoretically nearing",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "the maximum value of 1. These metrics collectively quantify",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "To further evaluate the performance of the eighteen fea-"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "model performance across dimensions such as label corre-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "ture selection methods, a Friedman test was conducted. The"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "lation, classification accuracy, and generalization capability,",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "significance level (α) was set to 0.05. The results of this sta-"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "forming the core evaluation framework for multi-label tasks.",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "tistical significance test are summarized in Table 1, which in-"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "For further details, refer to (Zhang et al. 2019a).",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "dicates that the null hypothesis is rejected, signifying signif-"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "icant differences in performance among the eighteen feature"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "Emotion recognition performance with incomplete",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "selection methods for multi-dimensional emotion recogni-"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "tion with incomplete multi-modal physiological signal data."
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "physiological data",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "Fig. 4 and Fig. 3 illustrate the comparative performance re-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "Ablation experiments"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "sults\nfor\nthe DEAP and DREAMER dataset,\nrespectively.",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": ""
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "The horizontal axis in each figure represents the missing ra-",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "To evaluate the contributions of each component within the"
        },
        {
          "Figure\n3: Multi-dimensional\nemotion\nrecognition\nperfor-": "tio of multi-modal physiological signals, while the vertical",
          "Figure\n4: Multi-dimensional\nemotion\nrecognition\nperfor-": "proposed EEG feature selection model, we conducted abla-"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: reveals that adaptive Theresultsforaveragecomputationaltimearesummarized",
      "data": [
        {
          "Methods": "MGFS",
          "DEAP": "0.23",
          "DREAMER": "0.35"
        },
        {
          "Methods": "MFS-MCDM",
          "DEAP": "0.21",
          "DREAMER": "0.19"
        },
        {
          "Methods": "SCLS",
          "DEAP": "15.93",
          "DREAMER": "1.27"
        },
        {
          "Methods": "mRMR",
          "DEAP": "0.63",
          "DREAMER": "0.09"
        },
        {
          "Methods": "RFS",
          "DEAP": "8.22",
          "DREAMER": "0.59"
        },
        {
          "Methods": "SDFS",
          "DEAP": "1.01",
          "DREAMER": "0.16"
        },
        {
          "Methods": "RPMFS",
          "DEAP": "4.94",
          "DREAMER": "0.98"
        },
        {
          "Methods": "FSOR",
          "DEAP": "359.77",
          "DREAMER": "54.07"
        },
        {
          "Methods": "GRMOR",
          "DEAP": "35.91",
          "DREAMER": "7.22"
        },
        {
          "Methods": "",
          "DEAP": "",
          "DREAMER": ""
        },
        {
          "Methods": "GRRO",
          "DEAP": "15.62",
          "DREAMER": "1.47"
        },
        {
          "Methods": "MDFS",
          "DEAP": "23.80",
          "DREAMER": "2.07"
        },
        {
          "Methods": "SLMDS",
          "DEAP": "0.71",
          "DREAMER": "0.08"
        },
        {
          "Methods": "RFSFS",
          "DEAP": "1.62",
          "DREAMER": "0.41"
        },
        {
          "Methods": "MSFS",
          "DEAP": "1.67",
          "DREAMER": "1.13"
        },
        {
          "Methods": "DHLI",
          "DEAP": "0.60",
          "DREAMER": "0.17"
        },
        {
          "Methods": "UGRFS",
          "DEAP": "104.23",
          "DREAMER": "4.78"
        },
        {
          "Methods": "EF2FS",
          "DEAP": "1.42",
          "DREAMER": "0.34"
        },
        {
          "Methods": "ASLSL",
          "DEAP": "7.13",
          "DREAMER": "1.07"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: reveals that adaptive Theresultsforaveragecomputationaltimearesummarized",
      "data": [
        {
          "Table 3: The computational time results (seconds).": "Parameter sensitivity"
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "This section presents a sensitivity analysis of ASLSL con-"
        },
        {
          "Table 3: The computational time results (seconds).": "cerning these four parameters. For each analysis, one pa-"
        },
        {
          "Table 3: The computational time results (seconds).": "rameter was held constant at 0.1 (γ = 2), while the others"
        },
        {
          "Table 3: The computational time results (seconds).": "were varied within the specified range. Due to space limita-"
        },
        {
          "Table 3: The computational time results (seconds).": "tions, we provide only the sensitivity analysis results for the"
        },
        {
          "Table 3: The computational time results (seconds).": "DEAP dataset. The 3-D histograms, depicting the AP values,"
        },
        {
          "Table 3: The computational time results (seconds).": "are presented in Fig.6. As shown in Fig.6, the AP values re-"
        },
        {
          "Table 3: The computational time results (seconds).": "main relatively stable across variations in two parameters,"
        },
        {
          "Table 3: The computational time results (seconds).": "indicating that the performance of ASLSL is not sensitive to"
        },
        {
          "Table 3: The computational time results (seconds).": "these balance parameters."
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "Computational cost"
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "The computational\ntime required by each method was as-"
        },
        {
          "Table 3: The computational time results (seconds).": "sessed. The implementation was carried out using MATLAB"
        },
        {
          "Table 3: The computational time results (seconds).": "(MathWorks Inc., Novi, MI, USA) and was executed on a"
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "computer\nrunning Microsoft Windows 11 × 64,\nfeaturing"
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "an Intel\ni5-13500H 2.60 GHz CPU and 16.00 GB of RAM."
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "The results for average computational time are summarized"
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "in Table 3. As illustrated in Table 3, ASLSL has superior"
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "performance in recognizing emotions from incomplete data"
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "while maintaining a relatively low computational cost."
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "Conclusions"
        },
        {
          "Table 3: The computational time results (seconds).": ""
        },
        {
          "Table 3: The computational time results (seconds).": "We introduce a novel approach for multi-modal physiologi-"
        },
        {
          "Table 3: The computational time results (seconds).": "cal signal\nfeature selection specifically designed to handle"
        },
        {
          "Table 3: The computational time results (seconds).": "incomplete physiological\nsignal data\nin emotion recogni-"
        },
        {
          "Table 3: The computational time results (seconds).": "tion tasks. This method utilizes adaptive shared latent struc-"
        },
        {
          "Table 3: The computational time results (seconds).": "ture learning to investigate a unified space for multi-modal"
        },
        {
          "Table 3: The computational time results (seconds).": "physiological\nsignals and multi-dimensional emotions, ef-"
        },
        {
          "Table 3: The computational time results (seconds).": "fectively mitigating the effects of missing information and"
        },
        {
          "Table 3: The computational time results (seconds).": "enabling the selection of informative features. Furthermore,"
        },
        {
          "Table 3: The computational time results (seconds).": "we propose an efficient\niterative algorithm to address\nthe"
        },
        {
          "Table 3: The computational time results (seconds).": "ASLSL optimization problem. Experimental results demon-"
        },
        {
          "Table 3: The computational time results (seconds).": "strate that ASLSL markedly surpasses seventeen advanced"
        },
        {
          "Table 3: The computational time results (seconds).": "methods in performance."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "dations.\nInformation Fusion, 102019."
        },
        {
          "References": "Becker, H.; Fleureau, J.; Guillotel, P.; Wendling, F.; Merlet,",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "I.; and Albera, L. 2017. Emotion recognition based on high-",
          "A systematic review (2014–2023) and research recommen-": "Koelstra, S.; Muhl, C.; Soleymani, M.; Lee, J.-S.; Yazdani,"
        },
        {
          "References": "resolution EEG recordings and reconstructed brain sources.",
          "A systematic review (2014–2023) and research recommen-": "A.; Ebrahimi, T.; Pun, T.; Nijholt, A.; and Patras,\nI. 2011."
        },
        {
          "References": "IEEE Transactions on Affective Computing, 11(2): 244–257.",
          "A systematic review (2014–2023) and research recommen-": "Deap: A database for emotion analysis; using physiological"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "signals. IEEE transactions on affective computing, 3(1): 18–"
        },
        {
          "References": "Cai, X.; Nie, F.; and Huang, H. 2013.\nExact\ntop-k feature",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "31."
        },
        {
          "References": "In Twenty-third interna-\nselection via l2,1-norm constraint.",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "tional joint conference on artificial intelligence.",
          "A systematic review (2014–2023) and research recommen-": "Lee, J.; and Kim, D.-W. 2017. SCLS: Multi-label feature se-"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "lection based on scalable criterion for large label set. Pattern"
        },
        {
          "References": "Duan, R.; Zhu, J.; and Lu, B. 2013.\nDifferential entropy",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "Recognition, 66: 342–352."
        },
        {
          "References": "feature for EEG-based emotion classification.\nIn 2013 6th",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "international IEEE/EMBS conference on neural engineering",
          "A systematic review (2014–2023) and research recommen-": "Li, J.; Cheng, K.; Wang, S.; Morstatter, F.; Trevino, R. P.;"
        },
        {
          "References": "(NER), 81–84. IEEE.",
          "A systematic review (2014–2023) and research recommen-": "Tang, J.; and Liu, H. 2017.\nFeature selection: A data per-"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "spective. ACM computing surveys (CSUR), 50(6): 1–45."
        },
        {
          "References": "Ezzameli, K.; and Mahersia, H. 2023. Emotion recognition",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "Informa-\nfrom unimodal\nto multimodal analysis: A review.",
          "A systematic review (2014–2023) and research recommen-": "Li, X.; Zhang, Y.; Tiwari, P.; Song, D.; Hu, B.; Yang, M.;"
        },
        {
          "References": "tion Fusion, 101847.",
          "A systematic review (2014–2023) and research recommen-": "Zhao, Z.; Kumar, N.; and Marttinen, P. 2022.\nEEG based"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "emotion recognition: A tutorial and review. ACM Computing"
        },
        {
          "References": "Geetha, A.; Mala, T.; Priyanka, D.; and Uma, E. 2024. Mul-",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "Surveys, 55(4): 1–57."
        },
        {
          "References": "timodal Emotion Recognition with deep learning: advance-",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "Information Fu-\nments, challenges, and future directions.",
          "A systematic review (2014–2023) and research recommen-": "Li, Y.; Hu, L.; and Gao, W. 2023a. Multi-label feature selec-"
        },
        {
          "References": "sion, 105: 102218.",
          "A systematic review (2014–2023) and research recommen-": "tion via robust flexible sparse regularization. Pattern Recog-"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "nition, 134: 109074."
        },
        {
          "References": "Hao, P.; Gao, W.; and Hu, L. 2025. Embedded feature fusion",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "for multi-view multi-label feature selection. Pattern Recog-",
          "A systematic review (2014–2023) and research recommen-": "Li, Y.; Hu, L.; and Gao, W. 2023b. Robust sparse and low-"
        },
        {
          "References": "nition, 157: 110888.",
          "A systematic review (2014–2023) and research recommen-": "redundancy multi-label feature selection with dynamic local"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "and global structure preservation. Pattern Recognition, 134:"
        },
        {
          "References": "Hao, P.; Liu, K.; and Gao, W. 2024. Double-layer hybrid-",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "109120."
        },
        {
          "References": "label\nidentification feature selection for multi-view multi-",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "the AAAI Conference on\nlabel\nlearning.\nIn Proceedings of",
          "A systematic review (2014–2023) and research recommen-": "Liu, X.; Zhu, X.; Li, M.; Wang, L.; Tang, C.; Yin, J.; Shen,"
        },
        {
          "References": "Artificial Intelligence, volume 38, 12295–12303.",
          "A systematic review (2014–2023) and research recommen-": "D.; Wang, H.; and Gao, W. 2018a. Late fusion incomplete"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "IEEE transactions on pattern analy-\nmulti-view clustering."
        },
        {
          "References": "Hao, P.; Liu, K.; and Gao, W. 2025.\nUncertainty-Aware",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "sis and machine intelligence, 41(10): 2410–2423."
        },
        {
          "References": "Global-View Reconstruction\nfor Multi-View Multi-Label",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "the AAAI Conference\nFeature Selection.\nIn Proceedings of",
          "A systematic review (2014–2023) and research recommen-": "Liu, Z.-T.; Xie, Q.; Wu, M.; Cao, W.-H.; Li, D.-Y.; and Li,"
        },
        {
          "References": "on Artificial Intelligence, volume 39, 17068–17076.",
          "A systematic review (2014–2023) and research recommen-": "S.-H. 2018b.\nElectroencephalogram emotion recognition"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "based on empirical mode decomposition and optimal\nfea-"
        },
        {
          "References": "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "IEEE Transactions on Cognitive and Devel-\nture selection."
        },
        {
          "References": "2020a. MFS-MCDM: Multi-label\nfeature selection using",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "opmental Systems, 11(4): 517–526."
        },
        {
          "References": "multi-criteria decision making. Knowledge-Based Systems,",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "206: 106365.",
          "A systematic review (2014–2023) and research recommen-": "Nie, F.; Huang, H.; Cai, X.; and Ding, C. H. 2010. Efficient"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "and robust feature selection via joint\nl2,1-norms minimiza-"
        },
        {
          "References": "Hashemi, A.; Dowlatshahi, M. B.; and Nezamabadi-pour, H.",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "tion.\nIn Advances in neural information processing systems,"
        },
        {
          "References": "2020b. MGFS: A multi-label graph-based feature selection",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "1813–1821."
        },
        {
          "References": "algorithm via PageRank centrality. Expert Systems with Ap-",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "plications, 142: 113024.",
          "A systematic review (2014–2023) and research recommen-": "Peng, H.; Long, F.; and Ding, C. 2005.\nFeature selection"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "based on mutual\ninformation: criteria of max-dependency,"
        },
        {
          "References": "Hou, C.; Nie, F.; Yi, D.; and Wu, Y. 2011.\nFeature selec-",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "IEEE Transactions on\nmax-relevance, and min-redundancy."
        },
        {
          "References": "tion via joint embedding learning and sparse regression.\nIn",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "Pattern Analysis & Machine Intelligence, (8): 1226–1238."
        },
        {
          "References": "Twenty-Second international\njoint conference on Artificial",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "Intelligence, 1324–1329.",
          "A systematic review (2014–2023) and research recommen-": "´"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "Torres-Valencia,\nC.;\nAlvarez-L´opez, M.;\nand\nOrozco-"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "´"
        },
        {
          "References": "Jenke, R.; Peer, A.; and Buss, M. 2014. Feature Extraction",
          "A systematic review (2014–2023) and research recommen-": "Guti´errez,\nA. 2017.\nSVM-based feature selection methods"
        },
        {
          "References": "IEEE\nand Selection for Emotion Recognition from EEG.",
          "A systematic review (2014–2023) and research recommen-": "Journal on\nfor emotion recognition from multimodal data."
        },
        {
          "References": "Transactions on Affective Computing, 5(3): 327–339.",
          "A systematic review (2014–2023) and research recommen-": "Multimodal User Interfaces, 11: 9–23."
        },
        {
          "References": "Jian, L.; Li, J.; Shu, K.; and Liu, H. 2016. Multi-label\nin-",
          "A systematic review (2014–2023) and research recommen-": "Wang, F.; Wu, S.; Zhang, W.; Xu, Z.; Zhang, Y.; Wu, C.; and"
        },
        {
          "References": "formed feature selection.\nIn The twenty-fifth International",
          "A systematic review (2014–2023) and research recommen-": "Coleman, S. 2020a. Emotion recognition with convolutional"
        },
        {
          "References": "Joint Conference on Artificial Intelligence, 1627–1633.",
          "A systematic review (2014–2023) and research recommen-": "neural network and EEG-based EFDMs. Neuropsychologia,"
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "107506."
        },
        {
          "References": "Katsigiannis, S.; and Ramzan, N. 2018.\nDREAMER: A",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "Database for Emotion Recognition Through EEG and ECG",
          "A systematic review (2014–2023) and research recommen-": "Wang,\nY\n.;\nLi,\nY\n.;\nand\nCui,\nZ.\n2024.\nIncomplete"
        },
        {
          "References": "Signals\nFrom Wireless Low-cost Off-the-Shelf Devices.",
          "A systematic review (2014–2023) and research recommen-": "Advances\nin\nmultimodality-diffused emotion recognition."
        },
        {
          "References": "IEEE Journal of Biomedical and Health Informatics, 22(1):",
          "A systematic review (2014–2023) and research recommen-": "Neural Information Processing Systems, 36."
        },
        {
          "References": "98–107.",
          "A systematic review (2014–2023) and research recommen-": ""
        },
        {
          "References": "",
          "A systematic review (2014–2023) and research recommen-": "Wang, Y.; Zhang, B.; and Di, L. 2024. Research Progress"
        },
        {
          "References": "Khare, S. K.; Blanes-Vidal, V.; Nadimi, E. S.; and Acharya,",
          "A systematic review (2014–2023) and research recommen-": "of EEG-Based Emotion Recognition: A Survey. ACM Com-"
        },
        {
          "References": "U. R. 2023. Emotion recognition and artificial intelligence:",
          "A systematic review (2014–2023) and research recommen-": "puting Surveys, 56(11): 1–49."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Discriminative feature selection via a structured sparse sub-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "space learning module.\nIn Proc. Twenty-Ninth Int. Joint"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Conf. Artif. Intell., 3009–3015."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Wu, D.; Lu, B.-L.; Hu, B.; and Zeng, Z. 2023.\nAffective"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "brain–computer interfaces (abcis): A tutorial. Proceedings"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "of the IEEE, 111(10): 1314–1332."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Xu, X.; Jia, T.; Li, Q.; Wei, F.; Ye, L.; and Wu, X. 2023. EEG"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Feature Selection via Global Redundancy Minimization for"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Emotion Recognition. IEEE Transactions on Affective Com-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "puting, 14(1): 421–435."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Xu, X.; Wei, F.; Zhu, Z.; Liu, J.; and Wu, X. 2020. Eeg Fea-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "ture Selection Using Orthogonal Regression: Application to"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Emotion Recognition.\nIn ICASSP 2020 - 2020 IEEE Inter-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "national Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "cessing (ICASSP), 1239–1243."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Zhang, J.; Lin, Y.; Jiang, M.; Li, S.; Tang, Y.; and Tan, K. C."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "2020a. Multi-label Feature Selection via Global Relevance"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "and Redundancy Optimization.\nIn the twenty-ninth Inter-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "national Joint Conference on Artificial\nIntelligence, 2512–"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "2518."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Zhang, J.; Luo, Z.; Li, C.; Zhou, C.; and Li, S. 2019a. Man-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "ifold regularized discriminative feature selection for multi-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "label learning. Pattern Recognition, 95: 136–150."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Zhang, J.; Yin, Z.; Chen, P.; and Nichele, S. 2020b. Emo-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "tion recognition using multi-modal data and machine learn-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "ing techniques: A tutorial and review.\nInformation Fusion,"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "59: 103–126."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Zhang, M.; and Zhou, Z. 2007. ML-KNN: A lazy learn-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "ing approach to multi-label\nlearning.\nPattern Recognition,"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "40(7): 2038 – 2048."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Zhang, R.; Nie, F.; Li, X.; and Wei, X. 2019b.\nFeature se-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "lection with multi-view data: A survey.\nInformation Fusion,"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "50: 158–167."
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "Zhang, Y.; Wu, J.; Cai, Z.; and Yu, P. S. 2020c. Multi-view"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "multi-label\nlearning with sparse\nfeature\nselection for\nim-"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "age annotation.\nIEEE Transactions on Multimedia, 22(11):"
        },
        {
          "Wang, Z.; Nie, F.; Tian, L.; Wang, R.; and Li, X. 2020b.": "2844–2857."
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition based on highresolution EEG recordings and reconstructed brain sources",
      "authors": [
        "H Becker",
        "J Fleureau",
        "P Guillotel",
        "F Wendling",
        "I Merlet",
        "L Albera"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Exact top-k feature selection via l 2,1 -norm constraint",
      "authors": [
        "X Cai",
        "F Nie",
        "H Huang"
      ],
      "year": "2013",
      "venue": "Twenty-third international joint conference on artificial intelligence"
    },
    {
      "citation_id": "3",
      "title": "Differential entropy feature for EEG-based emotion classification",
      "authors": [
        "R Duan",
        "J Zhu",
        "B Lu"
      ],
      "year": "2013",
      "venue": "Differential entropy feature for EEG-based emotion classification"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "5",
      "title": "Multimodal Emotion Recognition with deep learning: advancements, challenges, and future directions",
      "authors": [
        "A Geetha",
        "T Mala",
        "D Priyanka",
        "E Uma"
      ],
      "year": "2024",
      "venue": "Multimodal Emotion Recognition with deep learning: advancements, challenges, and future directions"
    },
    {
      "citation_id": "6",
      "title": "Embedded feature fusion for multi-view multi-label feature selection",
      "authors": [
        "P Hao",
        "W Gao",
        "L Hu"
      ],
      "year": "2025",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Double-layer hybridlabel identification feature selection for multi-view multilabel learning",
      "authors": [
        "P Hao",
        "K Liu",
        "W Gao"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Uncertainty-Aware Global-View Reconstruction for Multi-View Multi-Label Feature Selection",
      "authors": [
        "P Hao",
        "K Liu",
        "W Gao"
      ],
      "year": "2025",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "9",
      "title": "MFS-MCDM: Multi-label feature selection using multi-criteria decision making",
      "authors": [
        "A Hashemi",
        "M Dowlatshahi",
        "H Nezamabadi-Pour"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "10",
      "title": "MGFS: A multi-label graph-based feature selection algorithm via PageRank centrality. Expert Systems with Applications",
      "authors": [
        "A Hashemi",
        "M Dowlatshahi",
        "H Nezamabadi-Pour"
      ],
      "year": "2020",
      "venue": "MGFS: A multi-label graph-based feature selection algorithm via PageRank centrality. Expert Systems with Applications"
    },
    {
      "citation_id": "11",
      "title": "Feature selection via joint embedding learning and sparse regression",
      "authors": [
        "C Hou",
        "F Nie",
        "D Yi",
        "Y Wu"
      ],
      "year": "2011",
      "venue": "Twenty-Second international joint conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Feature Extraction and Selection for Emotion Recognition from EEG",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Multi-label informed feature selection",
      "authors": [
        "L Jian",
        "J Li",
        "K Shu",
        "H Liu"
      ],
      "year": "2016",
      "venue": "The twenty-fifth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "DREAMER: A Database for Emotion Recognition Through EEG and ECG Signals From Wireless Low-cost Off-the-Shelf Devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition and artificial intelligence: A systematic review (2014-2023) and research recommendations",
      "authors": [
        "S Khare",
        "V Blanes-Vidal",
        "E Nadimi",
        "U Acharya"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "16",
      "title": "SCLS: Multi-label feature selection based on scalable criterion for large label set",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "17",
      "title": "Feature selection: A data perspective",
      "authors": [
        "J Li",
        "K Cheng",
        "S Wang",
        "F Morstatter",
        "R Trevino",
        "J Tang",
        "H Liu"
      ],
      "year": "2017",
      "venue": "ACM computing surveys (CSUR)"
    },
    {
      "citation_id": "18",
      "title": "EEG based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "19",
      "title": "Multi-label feature selection via robust flexible sparse regularization",
      "authors": [
        "Y Li",
        "L Hu",
        "W Gao"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "Robust sparse and lowredundancy multi-label feature selection with dynamic local and global structure preservation",
      "authors": [
        "Y Li",
        "L Hu",
        "W Gao"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "Late fusion incomplete multi-view clustering",
      "authors": [
        "X Liu",
        "X Zhu",
        "M Li",
        "L Wang",
        "C Tang",
        "J Yin",
        "D Shen",
        "H Wang",
        "W Gao"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "22",
      "title": "Electroencephalogram emotion recognition based on empirical mode decomposition and optimal feature selection",
      "authors": [
        "Z.-T Liu",
        "Q Xie",
        "M Wu",
        "W.-H Cao",
        "D.-Y Li",
        "S.-H Li"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "23",
      "title": "Efficient and robust feature selection via joint l 2,1 -norms minimization",
      "authors": [
        "F Nie",
        "H Huang",
        "X Cai",
        "C Ding"
      ],
      "year": "2010",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy",
      "authors": [
        "H Peng",
        "F Long",
        "C Ding"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Pattern Analysis & Machine Intelligence"
    },
    {
      "citation_id": "25",
      "title": "SVM-based feature selection methods for emotion recognition from multimodal data",
      "authors": [
        "C Torres-Valencia",
        "M Álvarez-López",
        "Á Orozco-Gutiérrez"
      ],
      "year": "2017",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "26",
      "title": "Emotion recognition with convolutional neural network and EEG-based EFDMs",
      "authors": [
        "F Wang",
        "S Wu",
        "W Zhang",
        "Z Xu",
        "Y Zhang",
        "C Wu",
        "S Coleman",
        "Y Wang",
        "Y Li",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "27",
      "title": "Research Progress of EEG-Based Emotion Recognition: A Survey",
      "authors": [
        "Y Wang",
        "B Zhang",
        "L Di"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "28",
      "title": "Discriminative feature selection via a structured sparse subspace learning module",
      "authors": [
        "Z Wang",
        "F Nie",
        "L Tian",
        "R Wang",
        "X Li"
      ],
      "year": "2020",
      "venue": "Proc. Twenty-Ninth Int. Joint Conf"
    },
    {
      "citation_id": "29",
      "title": "Affective brain-computer interfaces (abcis): A tutorial",
      "authors": [
        "D Wu",
        "B.-L Lu",
        "B Hu",
        "Z Zeng"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "30",
      "title": "EEG Feature Selection via Global Redundancy Minimization for Emotion Recognition",
      "authors": [
        "X Xu",
        "T Jia",
        "Q Li",
        "F Wei",
        "L Ye",
        "X Wu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "31",
      "title": "Eeg Feature Selection Using Orthogonal Regression: Application to Emotion Recognition",
      "authors": [
        "X Xu",
        "F Wei",
        "Z Zhu",
        "J Liu",
        "X Wu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "32",
      "title": "Multi-label Feature Selection via Global Relevance and Redundancy Optimization",
      "authors": [
        "J Zhang",
        "Y Lin",
        "M Jiang",
        "S Li",
        "Y Tang",
        "K Tan"
      ],
      "year": "2020",
      "venue": "the twenty-ninth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Manifold regularized discriminative feature selection for multilabel learning",
      "authors": [
        "J Zhang",
        "Z Luo",
        "C Li",
        "C Zhou",
        "S Li"
      ],
      "year": "2019",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "35",
      "title": "ML-KNN: A lazy learning approach to multi-label learning",
      "authors": [
        "M Zhang",
        "Z Zhou"
      ],
      "year": "2007",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "36",
      "title": "Feature selection with multi-view data: A survey",
      "authors": [
        "R Zhang",
        "F Nie",
        "X Li",
        "X Wei"
      ],
      "year": "2019",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "37",
      "title": "Multi-view multi-label learning with sparse feature selection for image annotation",
      "authors": [
        "Y Zhang",
        "J Wu",
        "Z Cai",
        "P Yu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    }
  ]
}