{
  "paper_id": "2507.21015v1",
  "title": "Learning Transferable Facial Emotion Representations From Large-Scale Semantically Rich Captions",
  "published": "2025-07-28T17:28:08Z",
  "authors": [
    "Licai Sun",
    "Xingxun Jiang",
    "Haoyu Chen",
    "Yante Li",
    "Zheng Lian",
    "Biu Liu",
    "Yuan Zong",
    "Wenming Zheng",
    "Jukka M. Leppänen",
    "Guoying Zhao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Current facial emotion recognition systems are predominately trained to predict a fixed set of predefined categories or abstract dimensional values. This constrained form of supervision hinders generalization and applicability, as it reduces the rich and nuanced spectrum of emotions into oversimplified labels or scales. In contrast, natural language provides a more flexible, expressive, and interpretable way to represent emotions, offering a much broader source of supervision. Yet, leveraging semantically rich natural language captions as supervisory signals for facial emotion representation learning remains relatively underexplored, primarily due to two key challenges: 1) the lack of large-scale caption datasets with rich emotional semantics, and 2) the absence of effective frameworks tailored to harness such rich supervision. To this end, we introduce EmoCap100K, a large-scale facial emotion caption dataset comprising over 100,000 samples, featuring rich and structured semantic descriptions that capture both global affective states and fine-grained local facial behaviors. Building upon this dataset, we further propose EmoCapCLIP, which incorporates a joint global-local contrastive learning framework enhanced by a cross-modal guided positive mining module. This design facilitates the comprehensive exploitation of multi-level caption information while accommodating semantic similarities between closely related expressions. Extensive evaluations on over 20 benchmarks covering five tasks demonstrate the superior performance of our method, highlighting the promise of learning facial emotion representations from large-scale semantically rich captions. The code and data will be available at https://github.com/sunlicai/EmoCapCLIP.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Emotion plays a crucial role in the advancement of artificial intelligence, as it empowers machines to interpret and respond to human affective states, facilitating more natural and harmonious humanmachine interactions  [1, 2, 3] . With the proliferation of deep learning, facial emotion recognition (FER) has emerged as a pivotal research area within the affective computing community, aiming to equip machines with the capability to decode human emotions from facial expressions  [4, 5, 6, 7] .\n\nA fundamental challenge in developing FER systems lies in how emotions are represented or annotated. Traditionally, two principal paradigms have dominated this landscape: categorical method and dimensional method (Figure  1a )  [8, 9, 10] . The categorical method frames emotions as a limited set of categories, such as Ekman's six basic emotions  [11] . Although this way offers simplicity, it suffers from limited expressiveness, failing to capture emotion intensity or mixed emotions. The dimensional method, on the other hand, characterizes emotions as numerical coordinates in a We used GPT-4o  [14]  to convert the original image into Ghibli style.\n\nmultidimensional space  [12] , allowing for granular modeling of emotional polarity and intensity. Despite this capability, it lacks interpretability  [13]  and also struggles to represent compound emotions. These limitations inherent in two paradigms constrain FER systems from capturing the rich and nuanced spectrum of human emotions, undermining their generalization and practical applicability.\n\nIn contrast, language intrinsically resonates with the complex and nuanced nature of emotional expressions  [15, 16, 17, 18, 19] , naturally offering a more expressive, flexible, and interpretable alternative for representing and annotating facial emotions. Meanwhile, recent advances in visionlanguage pre-training (e.g., CLIP  [20] ) have showcased the remarkable power of leveraging language as supervisory signals in general visual understanding. Nevertheless, the potential of harnessing semantically rich natural language captions as a broader source of supervision to learn facial emotion representations has remained relatively untapped in the existing literature (Figure  1b )  [21, 22, 23] , mainly due to two major obstacles: (1) From the data perspective, existing methods either rely on relatively small, manually annotated datasets with limited diversity and expressiveness  [21] , or utilize generic face captions or latent information that lack explicit emotional semantics  [22, 23] . Other available datasets  [19]  are similarly constrained in scale and fail to provide comprehensive, well-structured facial emotion caption that integrates global expressive configurations (e.g., an overall happily surprised expression) and corresponding local facial cues (e.g., highly raised eyebrows and wide open mouth).\n\n(2) From the model perspective, previous efforts predominantly adopt a vanilla global contrastive framework  [21, 23, 22] , neglecting the modeling of multi-level information embedded in rich emotion captions. Furthermore, semantically related expressions (e.g., joyful grin versus broad smile) are indiscriminately pushed apart within this framework, which undermines the semantic consistency and generalization ability of the learned emotion representations.\n\nTo address the above challenges, we first introduce EmoCap100K, a large-scale semantically rich facial emotion caption dataset comprising more than 100,000 samples. Given the prohibitive cost and effort of manual annotation, and motivated by the promising performance of multimodal large language models (MLLMs) in emotion-related tasks  [24, 25, 26] , we employ Gemini-1.5-Flash  [27] , a leading proprietary MLLM, for scalable automated caption generation. We leverage carefully designed prompts to elicit comprehensive interpretation of emotional expressions covering both global affective gist and local facial behaviors in a well-structured manner (Figure  1d ). Building upon the large-scale EmoCap100K dataset, we further propose EmoCapCLIP to learn emotion-aware facial representations from expressive caption supervision (Figure  1c ). Benefiting from the well-structured multi-level information within captions, EmoCapCLIP moves beyond simple global contrast by introducing a joint global-local contrastive learning framework to facilitate more comprehensive and nuanced emotion representation learning. Moreover, EmoCapCLIP develops a cross-modal guided positive mining module to account for the inherent semantic similarity between closely related emotional expressions, further improving the overall quality of the learned representations. To verify the effectiveness of our method, we conduct extensive experiments on more than 20 benchmarks covering five FER-related tasks. The results show that EmoCapCLIP outperforms the state-of-the-art (SOTA) methods by large margins (Figure  1e ).\n\nIn summary, our main contributions include: (1) We construct EmoCap100K, a large-scale facial emotion caption dataset with comprehensive, well-structured descriptions of global affective gist and local facial behaviors, providing a valuable resource for advancing facial emotion understanding.\n\n(   [28, 29, 30, 31, 32, 33, 34, 35, 36] ) and loss function refinements  [37, 38, 39, 40] , aiming to enhance the discriminative power of emotion classifiers. However, the supervisory signals employed by these models remain largely constrained to a fixed set of predefined discrete emotion categories (e.g., happiness, sadness, anger, fear, disgust, surprise, and neutral)  [41, 42, 37, 43, 44, 45, 46]  or abstract dimensional labels (e.g., valence, arousal, and dominance)  [43, 47, 48] . While these traditional annotation schemes offer advantages in simplicity or granularity, they inherently lack expressiveness, flexibility, and interpretability, thereby constraining the generalization capability and practical applicability of FER systems.\n\nNatural Language Supervision. Natural language, with its inherent semantic depth, provides a more expressive, flexible, and interpretable way to annotate the rich spectrum of emotional expressions. This paradigm aligns deeply with the intrinsic nature of human emotion perception, which is widely acknowledged as continuous (varying in intensity, e.g., slightly annoyed and completely enraged)  [12] , compound (involving blended affective states, e.g., happily surprised and anxiously excited)  [49] , and dependent on the synergistic integration of global configurations and local facial cues  [50, 51, 52, 53] . Motivated by this advantage and the remarkable success of vision-language pre-training, several recent studies have attempted to leverage language supervision for emotion representations learning  [40, 21, 23] . Specifically, EmotionCLIP  [40]  learns emotion representations by contrasting verbal and nonverbal cues in daily communications. However, it does not specifically target facial expressions.\n\nTo tackle this issue, EmoCLIP  [21]  uses short human-annotated emotion captions from MAFW  [45]  to perform global contrastive alignment between video and language representations. Subsequently, Exp-CLIP  [23]  adopts an implicit alignment strategy, matching CLIP's visual representations to textual features from BLIP2  [54] . While these methods have shown decent results, they typically depend on small-scale datasets with limited emotional semantics and are constrained by a simplistic global contrast framework, thus failing to fully exploit the potential of semantically rich captions. In contrast, this paper introduces a large-scale facial emotion caption dataset with rich emotional semantics, and leverages it as the foundation to develop a novel vision-language model that effectively unlocks the power of natural language supervision for facial emotion representation learning.",
      "page_start": 1,
      "page_end": 11
    },
    {
      "section_name": "Vision-Language Models",
      "text": "CLIP Models. Large-scale vision-language pre-training has emerged as a powerful paradigm for learning transferable visual representations  [20, 55, 56, 57, 58, 59, 60, 61, 62] . Pioneering visionlanguage models such as CLIP  [20]  and ALIGN  [55]  have demonstrated impressive performance across a broad range of computer vision tasks such as image classification, image-text retrieval, and open-vocabulary detection and segmentation. These models typically employ contrastive learning to align visual and textual modalities on large-scale image-text pairs, resulting in strong zero-shot generalization in downstream tasks. Building upon this foundation, recent efforts such as FaRL  [63]  and FLIP  [22]  have extended vision-language pre-training to the facial domain, aiming to learn general-purpose facial representations. Despite demonstrating strong performance in various facial analysis tasks (e.g., face alignment and face attribute recognition), they primarily rely on web-crawled generic face captions, which often lack explicit and fine-grained affective semantics, thereby limiting their effectiveness in emotion-centric applications.\n\nMLLMs. Beyond CLIP models, recent advances in MLLMs have garnered significant interest, owing to their capacity to jointly understand and reason over diverse modalities  [64, 65, 66, 67, 68] .\n\nBuilding on this trend, several studies have explored the development of emotion-centric MLLMs  [19, 69, 70, 71, 72, 73] , such as EmoLA  [19] , Emotion-LLaMA  [71] , and R1-Omni  [73] . These models are primarily designed to generate emotion-related captions from image, video, or multimodal inputs, typically through visual instruction tuning. In contrast, our work takes a fundamentally different perspective: rather than treating caption generation as the end goal, we leverage semantically rich captions as supervisory signals to facilitate the learning of robust facial emotion representations. Moreover, our approach does not rely on heavy language models and thus has significantly fewer parameters. Nevertheless, it achieves competitive or even superior performance on several benchmarks compared to these substantially larger MLLMs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Emocap100K",
      "text": "As shown in Table  1 , our EmoCap100K is distinct from existing face caption datasets. First, most existing datasets (e.g., LAION-Face  [63]  and FLIP-80M  [22] ) are constructed for generic facial perception tasks (e.g., facial attributes recognition) rather than being tailored for facial emotion understanding. Although they typically contain a large volume of facial images, emotional semantics are largely absent or underrepresented in the associated captions. Second, MAFW  [45]  provides a manually annotated caption dataset dedicated to facial emotions, however, its scale is highly limited, containing less than one-tenth of the number of samples in EmoCap100K. Moreover, due to the inherent challenges of manual annotation, the captions are notably short (18 vs. 267 words on average) and exhibit limited linguistic diversity in terms of unique emotion words (312 vs. 703).\n\nThird, the closest counterpart to our dataset is the recently introduced FABA-Instruct  [19] , which also leverages MLLMs to construct two instruction-tuning datasets focused on facial emotion and action units. Nevertheless, its dataset size is still relatively small, only about one-fifth of EmoCap100K.\n\nAdditionally, the use of simple prompting strategies leads to captions that lack clear structural organization across different semantic components (i.e., global emotional configurations and local facial behaviors), making it difficult to utilize multi-level affective information.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Collection",
      "text": "From a data-centric viewpoint, one promising approach to improve the model's generalization ability is to collect large-scale affective facial data that exhibits spontaneous and diverse emotions in unconstrained, real-world settings  [37] . As a typical type of screen media, movies have been considered as a reliable data source to collect required facial expressions because actors perform in naturalistic conditions and vividly mimic spontaneous facial expressions  [44, 45] . Following this rationale, we curated a large-scale dataset by downloading over 1,000 movies spanning various genres on open-source platform and extracting human faces from them. To ensure diversity in the collected data, we focused on three key aspects: head poses, scene contexts, and emotion categories.\n\nIn particular, for emotional diversity, we aimed to go beyond the six basic emotions and include compound and atypical emotional expressions  [78] . To maximize the collection of desired facial samples, we established a set of collection rules and recruited multiple volunteers to manually select the data accordingly. Detailed collection rules and data statistics are provided in Appendix C.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Caption Generation",
      "text": "Research in psychology indicates that human perceiving facial expressions of emotion is a complex process involving the synergistic interplay of global and local information processing  [50, 51, 79, 52, 53] . Specifically, local processing allows for detailed analysis of individual facial components, while global processing enables the rapid perception of the overall configuration and integration of features into a meaningful whole. Following this insight, our aim is to generate captions that offer a comprehensive and well-structured interpretation of facial expressions by integrating both the global expressive configuration and specific local facial behaviors that contribute to this global impression.\n\nGenerally speaking, manual annotation is considered one of the most reliable labeling methods. However, as mentioned above, manually annotated captions are often short and lack diversity. Moreover, the process is time-consuming and labor-intensive, making it expensive to scale data size. Fortunately, several recent studies have demonstrated the strong performance of proprietary MLLMs in facial emotion-related tasks  [24, 25, 26] . Considering that Gemini-1.5-Flash  [27]  was a highly capable yet cost-effective LLM at annotating time, we employed it to generate large-scale captions in an automated manner. Specifically, to effectively elicit the desired captions, we carefully designed a prompt that emphasizes multi-level emotional information across three components:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emocapclip",
      "text": "As illustrated in Figure  3 (a), EmoCapCLIP primarily features: (i) a joint global-local contrastive learning framework (Section 4.1) that fosters comprehensive emotion understanding by fully exploiting multi-level semantic information embedded in rich, structured captions, and (ii) a cross-modal guided positive mining module (Section 4.2) aimed at promoting semantic consistency and enhancing contrastive representation learning through the identification of more informative positive samples.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Joint Global-Local Contrastive Learning",
      "text": "EmoCapCLIP follows CLIP  [20]  to employ a dual-encoder architecture, utilizing a Vision Transformer (ViT  [80] ) for image encoding and a Transformer  [81]  for text encoding. For convenience in the following discussion, we assume that B = {I i , T i } N i=1 is a batch of N image-caption pairs, where the caption includes three parts, i.e.,",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Intra-Sample Local Contrast",
      "text": "EOS token Global Contrastive Learning. As shown in Figure  3 (a), the image I i is passed through the image encoder to produce the global image representation g i I ∈ R D (derived from the [CLS] token  [80] ). For the caption T i , since both T i Global and T i Summary contain the overall description of affective states, we feed one of them into the text encoder to obtain the global text representation g i T ∈ R D (derived from the [EOS] token  [20] ). Following CLIP, especially during the early training stage, for each image (or caption), EmoCapCLIP treats the paired caption (or image) as the positive sample and all other captions (or images) in the batch as negative samples. This leads to the formulation of the global contrastive loss:",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Global Contrastive Learning",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Inter-Sample Local Contrast",
      "text": "where S⟨•, •⟩ denotes the cosine similarity function and τ is a learnable temperature parameter.\n\nLocal Contrastive Learning. Local facial behaviors are the building blocks of emotional expressions, encapsulating critical details that characterize specific emotional nuances  [53, 82] . Hence, supplementing global contrastive learning with local contrastive learning is essential to achieve a more comprehensive and nuanced understanding of facial emotions. To do so, as depicted in Figure  3 (a), we first randomly sample M sentences from T i Local without replacement, each of which details one of local behaviors in specific regions. Then we pass them through the text encoder to obtain their sentence representations R i T = {r i,j T } M j=1 ∈ R M ×D . To facilitate local contrast, a cross-attention layer, whose query is from R i T and key/value comes from the dense image patch embeddings R i I ∈ R P ×D (P is the number of patch tokens), is employed to obtain the pooled image representations Ri\n\nand W ∈ R D×D are projection matrices. As shown in Figure  3 (b), the local contrastive loss operates on the sets of local sentence representations R i T = {r i,j T } M j=1 and the associated pooled image representations Ri I = {r i,j I } M j=1 . Since the elements in them correspond to distinct facial behaviors in specific regions, it is intuitive that we can treat representations from other local regions of the same sample as negatives and the paired one as the positive  [62, 60] . This formulates the intra-sample local contrastive loss:\n\nAlthough minimizing L intra r promotes learning discriminative features for different local facial behaviors, the contrastive signal derived solely from within a single sample can be weak due to the limited number of distinct local descriptions (e.g., M = 3 in our experiments). To address this, we also introduce inter-sample local contrastive loss to incorporate more diverse negative samples. Following global contrastive loss L g , particularly during early training, this loss leverages other samples in the batch as negatives to improve local contrastive representation learning. It is formulated as:",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Cross-Modal Guided Positive Mining (Cmgpm)",
      "text": "A strong assumption in the above formulation of L g and L inter r is that only the explicitly paired representations represent a true positive match, while all other N -1 non-paired ones within the batch (i.e., (g i I , g n T ) or (r i,j I , r n,j T ) for n ̸ = i) are complete mismatches serving as negative samples. However, this assumption often fails to hold, especially considering the continuous spectrum of emotions (e.g., non-paired two faces exhibiting joyful grin versus broad smile)  [15, 83, 84] . To this end, CMGPM is introduced to identify these inappropriate negatives and treat them as positives, thereby enhancing semantic consistency and improving the overall quality of learned representations.\n\nContrastive Loss with CMGPM. As illustrated in Figure  3 (c), the core idea of CMGPM is to leverage similarities within one modality as guidance to identify potential positive samples for the other modality. Without loss of generality, let us consider the global image representation g i I as the anchor. The default positive is the paired global text representation g i T . We posit that if another text representation g p T in the batch is similar to g i T , then the caption of sample p is likely to be semantically close to that of sample i, thus making g p T a potential positive for g i I . Therefore, we first calculate the cosine similarity of g i T with all global text representations (i.e., {g p T } N p=1 ). We then identify a positive set P i T for the anchor g i I . This set comprises the sample whose cosine similarity with g i T is above a predefined threshold σ and is among the top K most similar samples in the batch. Formally, the positive set P i T for g i I is defined as:\n\nNote that this set P i T includes the index i itself, as S⟨g i T , g i T ⟩ = 1. Besides, P i T degenerates into {i} (i.e., without positive mining) when σ → 1 or K = 1. The positive set P i I for g i T can be defined similarly, which is omitted it here. Thus, the global contrastive loss with CMGPM is formulated as:\n\nwhere λ i,p T = S⟨g i T , g p T ⟩ and λ i,p I = S⟨g i I , g p I ⟩ are weights assigned to the mined positive pairs. Finally, the inter-sample local loss with CMGPM (i.e., L inter ′ r ) is similarly defined and omitted.\n\nOverall Training Loss. In the early training stage, we adopt the standard formulation of global and inter-sample local contrastive losses without CMGPM, as the representation spaces of both modalities are still immature and unable to capture reliable sample relationships. As training progresses and representations become more stable, we activate the CMGPM module to leverage cross-modal complementary cues for mining high-quality positives, thereby facilitating mutual refinement of representations across modalities. Formally, the overall training loss is defined as:\n\nwhere α is the weight for local contrastive loss and t is the epoch at which CMGPM starts.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Main Results",
      "text": "To comprehensively evaluate EmoCapCLIP, we conduct experiments across five tasks, including zero-shot static facial expression recognition (SFER), zero-shot dynamic facial expression recognition (DFER), zero-shot facial action unit detection (FAUD), zero-shot expression-caption retrieval (ECR), and few-shot SFER. The main evaluation metrics are the unweighted average recall (UAR) and the weighted average recall (WAR, i.e., accuracy). Due to space constraints, we only report a subset of the results for zero-shot SFER/DFER and few SFER. The implementation & evaluation details and more results for other tasks are provided in Appendix E,F.\n\nZero-shot SFER. We first evaluate EmoCapCLIP on 9 popular SFER benchmarks. To ensure a fair comparison, we follow Exp-CLIP  [23]  to use the prompt \"a photo of a face with an expression of  [CLASS] .\" to evaluate all CLIP models. As shown in Table  2 , EmoCapCLIP outperforms both generalpurpose and domain-specific CLIP models by significant margins on all benchmarks. For instance, when using ViT-B/32, EmoCapCLIP beats Exp-CLIP  [23] -a method specifically designed for zeroshot SFER-by over 20% on RAF-DB and ∼10% on RAF-DB (Compound). It also substantially outperforms FLIP  [22] , a strong generic face CLIP model trained on FLIP-80M-800× larger than EmoCap100K, thereby underscoring the value of semantically rich captions and task-tailored contrastive learning. Besides, EmoCapCLIP can push performance to even higher levels by scaling up the backbone capacity or compute. For example, EmoCapCLIP (ViT-L/14) improves EmoCapCLIP (ViT-B/32) by more than 6%, 9%, 15% in terms of UAR on RAF-DB, RAF-DB (Compound), and CK+, respectively. Furthermore, compared to much larger MLLMs that have billion-level parameters, EmoCapCLIP also demonstrates comparable or even better performance. Specifically, EmoCapCLIP (ViT-L/14) surpasses EmoLA  [19]  by 13% UAR on FABA-Bench (Emotion) and EMO-LLaMA  [70]  by 5% UAR on AffectNet-8. More surprisingly, it slightly outperforms proprietary Gemini-1.5-Flash  [27]  and GPT-4V  [64]  on several benchmarks (e.g., AffectNet-8 and CK+). To sum up, the above encouraging results indicate that EmoCapCLIP can learn powerful facial emotion representations from large-scale semantically rich captions.\n\nTo further examine the generalization capability of EmoCapCLIP, we compare its zero-shot performance against the cross-dataset performance of traditional supervised baselines. Following the previous method  [85] , we report the results of WAR using ViT-B/32 in Table  3 . The zero-shot performance of EmoCapCLIP is better than the cross-dataset evaluation results of multiple supervised methods (e.g., CAFE  [85] , FaRL  [63] , and ExpLLM  [69] ) on AffectNet-7 and SFEW2.0, even though they are trained with manually annotated labels. This suggests that our approach has stronger generalization capabilities, highlighting the benefit of leveraging natural language with rich semantics as a much broader source of supervision rather than relying solely on a fixed set of predefined categories.\n\nAlthough the performance on RAF-DB is relatively lower, it is worth noting that these supervised models are trained on AffectNet-7, which is over 2× larger than EmoCap100K.\n\nZero-shot DFER. To investigate the transferability of the learned representations in the video domain, we further evaluate EmoCapCLIP on 8 DFER benchmarks. For image CLIP models, we sample 16 frames per video and apply temporal pooling to obtain the video representation  [23] . Besides, the same prompt as SFER is used for evaluation. As shown in Table  4 , our method achieves much better performance than all image CLIP baselines. Compared to general-purpose and domain-specific video CLIP models with complex temporal information modeling, EmoCapCLIP also shows competitive or superior performance on all benchmarks. Notably, although EmoCLIP  [21]  utilizes human-annotated captions from MAFW for supervision, it still lags behind EmoCapCLIP in terms of UAR on several in-the-wild benchmarks (e.g., 25.86% vs. 30.85% on MAFW) and lab-controlled benchmarks (e.g., 33.13% vs. 48.80% on CREMA-D). This finding suggests that large-scale, semantically rich captions from MLLMs can be effectively leveraged within a well-designed contrastive learning framework to learn transferable facial emotion representations, without relying on costly manual annotations.\n\nFew-shot SFER. The above parts focus on the zero-shot evaluation of the learned representations. We are also interested in the adaptation of EmoCapCLIP within the few-shot setting. To investigate this, we follow CoOp  [94]  to employ two prominent few-shot adaptation methods: linear probing and prompt tuning. The results on RAF-DB are shown in Figure  4 . Each experiment was run five times with different seeds to ensure the reliability of the evaluation. As can be seen in the figure, EmoCapCLIP significantly outperforms Exp-CLIP and FLIP across different backbones and     evaluation methods. Notably, this performance advantage is more pronounced when the number of shots is smaller, demonstrating the superior quality and high transferability of the representations learned by EmoCapCLIP, particularly in low-data regimes.",
      "page_start": 8,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study",
      "text": "In this study, we compare the zero-shot SFER performance on RAF-DB and FERPlus using ViT-B/32. and L inter r ), and the cross-modal guided positive mining (CMGPM) module. As shown in Table  5 , we start with the simple baseline using only L g . It achieves reasonable performance on two datasets, which serves as a good starting point. Subsequently, the incorporation of L intra r or L inter r boosts the performance over the baseline. Besides, combining them yields additional gains. These results indicate that the local contrastive learning enforced by L intra r and L inter r effectively supplements the global contrastive learning, facilitating a more comprehensive and nuanced representation of facial emotions. Furthermore, the inclusion of the CMGPM module yields consistent performance across different configurations. This outcome underscores the critical importance of mining high-quality positive samples with similar emotional states to enhance the contrastive learning process. The final configuration integrating all components achieves the highest scores, validating the contribution of each element to the effectiveness of EmoCapCLIP.\n\nDataset Size. We investigate how the amount of training data affects the quality of the learned representations. As shown in Figure  5 (a), the model performance is limited when only a small amount of data is available. However, substantial performance improvement is observed as the data volume increases, culminating in the best results achieved at the full 100K scale. This finding highlights the critical importance of sufficient data volume for learning strong emotion-aware facial representations from semantically rich captions.\n\nSeveral Factors in CMGPM. We then investigate the influence of the activation timing of the CMGPM mechanism, the similarity threshold σ, and the number of top K similar samples used for positive set construction. The results shown in Figure  5 (b) first reveal that a suitable activation timing for CMGPM is crucial to achieve good performance. Initiating CMGPM prematurely, during the early stage of model training, is detrimental. This is intuitive, as the representation space is not yet sufficiently well-formed, rendering the positive samples mined by CMGPM at this stage less reliable. Conversely, delaying CMGPM activation until the final epoch is also suboptimal, limiting its potential impact on the learning process. Regarding the ablation of K and σ shown in Figure  5 (c) and (d), we find that the model performance shows relatively low sensitivity to their specific choices.\n\nCaption Selection for L g . There are two caption choices for global contrastive learning, i.e., the global part (T i Global ) and the summary part (T i Summary ) in the full caption.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we have embraced large-scale semantically rich captions to unleash the power of natural language supervision for facial emotion representation learning. To overcome the data bottleneck, we established EmoCap100K, a large-scale facial emotion caption dataset featuring comprehensive and structured affective descriptions. We also proposed EmoCapCLIP, a novel framework tailored to fully harness the rich supervisory signals in captions and facilitate better emotion understanding. Extensive evaluations on diverse benchmarks and tasks demonstrate the strong transferability of the learned representations and open new research avenues for emotion-centric vision-language pre-training.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: a) [8, 9, 10]. The categorical method frames emotions as a limited",
      "page": 1
    },
    {
      "caption": "Figure 1: Conceptual overview of EmoCapCLIP. For the performance comparison, we report results",
      "page": 2
    },
    {
      "caption": "Figure 1: b) [21, 22, 23],",
      "page": 2
    },
    {
      "caption": "Figure 1: d). Building upon the",
      "page": 2
    },
    {
      "caption": "Figure 1: c). Benefiting from the well-structured",
      "page": 2
    },
    {
      "caption": "Figure 1: (d), the generated",
      "page": 5
    },
    {
      "caption": "Figure 2: highlight the semantic richness",
      "page": 5
    },
    {
      "caption": "Figure 2: Word clouds of EmoCap100K dataset, showing rich words to describe facial emotions.",
      "page": 5
    },
    {
      "caption": "Figure 3: (a), EmoCapCLIP primarily features: (i) a joint global-local contrastive",
      "page": 5
    },
    {
      "caption": "Figure 3: The illustration of EmoCapCLIP, featuring a joint global-local contrastive framework (a-b)",
      "page": 6
    },
    {
      "caption": "Figure 3: (a), the image Ii is passed through the image",
      "page": 6
    },
    {
      "caption": "Figure 3: (a), we first randomly sample M sentences from T i",
      "page": 6
    },
    {
      "caption": "Figure 3: (b), the local contrastive loss operates on the sets of local sentence representations",
      "page": 6
    },
    {
      "caption": "Figure 3: (c), the core idea of CMGPM is to",
      "page": 7
    },
    {
      "caption": "Figure 4: Each experiment was run",
      "page": 8
    },
    {
      "caption": "Figure 4: Few-shot linear probing (Upper) and prompt tuning (Bottom) results on RAF-DB.",
      "page": 10
    },
    {
      "caption": "Figure 5: Ablation studies on miscellaneous design choices.",
      "page": 10
    },
    {
      "caption": "Figure 5: (a), the model performance is limited when only a small amount",
      "page": 10
    },
    {
      "caption": "Figure 5: (b) first reveal that a suitable activation timing",
      "page": 11
    },
    {
      "caption": "Figure 5: (c) and (d), we",
      "page": 11
    },
    {
      "caption": "Figure 5: (e) shows that",
      "page": 11
    },
    {
      "caption": "Figure 5: (f). Results show that larger M generally leads to slight performance improvement.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "0",
          "0": "0"
        },
        {
          "1": "0",
          "0": "0"
        },
        {
          "1": "0",
          "0": "0"
        },
        {
          "1": "0",
          "0": "1"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.0": "0.8",
          "0.8": "1.0",
          "0.1": "0.6",
          "0.9": "0.3",
          "0.2": "0.5"
        },
        {
          "1.0": "0.1",
          "0.8": "0.6",
          "0.1": "1.0",
          "0.9": "0.2",
          "0.2": "0.7"
        },
        {
          "1.0": "0.9",
          "0.8": "0.3",
          "0.1": "0.2",
          "0.9": "1.0",
          "0.2": "0.4"
        },
        {
          "1.0": "0.2",
          "0.8": "0.5",
          "0.1": "0.7",
          "0.9": "0.4",
          "0.2": "1.0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.0": "0.8",
          "0.8": "1.0",
          "0.0": "0.0",
          "0.9": "0.0"
        },
        {
          "1.0": "0.0",
          "0.8": "0.0",
          "0.0": "0.7",
          "0.9": "0.0"
        },
        {
          "1.0": "0.9",
          "0.8": "0.0",
          "0.0": "0.0",
          "0.9": "1.0"
        },
        {
          "1.0": "0.0",
          "0.8": "0.0",
          "0.0": "1.0",
          "0.9": "0.0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.0": "0.0",
          "0.0": "0.8",
          "0.9": "0.0"
        },
        {
          "1.0": "0.0",
          "0.0": "0.0",
          "0.9": "0.0"
        },
        {
          "1.0": "0.0",
          "0.0": "1.0",
          "0.9": "0.9"
        },
        {
          "1.0": "0.9",
          "0.0": "0.9",
          "0.9": "1.0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "0",
          "0": "0"
        },
        {
          "1": "0",
          "0": "0"
        },
        {
          "1": "0",
          "0": "0"
        },
        {
          "1": "0",
          "0": "1"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1.0": "0.2",
          "0.2": "1.0",
          "0.5": "0.3",
          "0.6": "0.8",
          "0.9": "0.6"
        },
        {
          "1.0": "0.5",
          "0.2": "0.3",
          "0.5": "1.0",
          "0.6": "0.4",
          "0.9": "0.1"
        },
        {
          "1.0": "0.6",
          "0.2": "0.8",
          "0.5": "0.4",
          "0.6": "1.0",
          "0.9": "0.9"
        },
        {
          "1.0": "0.9",
          "0.2": "0.6",
          "0.5": "0.1",
          "0.6": "0.9",
          "0.9": "1.0"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Society of mind. Simon and Schuster",
      "authors": [
        "Marvin Minsky"
      ],
      "year": "1986",
      "venue": "Society of mind. Simon and Schuster"
    },
    {
      "citation_id": "2",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "3",
      "title": "An overview of emotion in artificial intelligence",
      "authors": [
        "Gustavo Assunção",
        "Bruno Patrão",
        "Miguel Castelo-Branco",
        "Paulo Menezes"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Artificial Intelligence"
    },
    {
      "citation_id": "4",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "5",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "Felipe Zago Canal",
        "Tobias Rossi Müller",
        "Cristine Jhennifer",
        "Gustavo Matias",
        "Antonio Gino Scotton",
        "Junior Reis De Sa",
        "Eliane Pozzebon",
        "Antonio Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "6",
      "title": "Understanding deep learning techniques for recognition of human emotions using facial expressions: A comprehensive survey",
      "authors": [
        "Mohan Karnati",
        "Ayan Seal",
        "Debotosh Bhattacharjee",
        "Anis Yazidi",
        "Ondrej Krejcar"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "7",
      "title": "Facial micro-expressions: An overview",
      "authors": [
        "Guoying Zhao",
        "Xiaobai Li",
        "Yante Li",
        "Matti Pietikäinen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "8",
      "title": "A model of the perception of facial expressions of emotion by humans: Research overview and perspectives",
      "authors": [
        "Aleix Martinez",
        "Shichuan Du"
      ],
      "year": "2012",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "9",
      "title": "Deep learning for human affect recognition: Insights and new developments",
      "authors": [
        "Marc Tp Philipp V Rouast",
        "Raymond Adam",
        "Chiong"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information fusion"
    },
    {
      "citation_id": "11",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "12",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "13",
      "title": "Open-vocabulary multimodal emotion recognition: Dataset, metric, and benchmark",
      "authors": [
        "Zheng Lian",
        "Haiyang Sun",
        "Licai Sun",
        "Lan Chen",
        "Haoyu Chen",
        "Hao Gu",
        "Zhuofan Wen",
        "Shun Chen",
        "Siyuan Zhang",
        "Hailiang Yao"
      ],
      "year": "2024",
      "venue": "Open-vocabulary multimodal emotion recognition: Dataset, metric, and benchmark",
      "arxiv": "arXiv:2410.01495"
    },
    {
      "citation_id": "14",
      "title": "Gpt-4o system card",
      "authors": [
        "Aaron Hurst",
        "Adam Lerer",
        "Adam Goucher",
        "Adam Perelman",
        "Aditya Ramesh",
        "Aidan Clark",
        "Akila Ostrow",
        "Alan Welihinda",
        "Alec Hayes",
        "Radford"
      ],
      "year": "2024",
      "venue": "Gpt-4o system card",
      "arxiv": "arXiv:2410.21276"
    },
    {
      "citation_id": "15",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "Alan Cowen",
        "Dacher Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "16",
      "title": "The language of emotion",
      "authors": [
        "Joel R Davitz"
      ],
      "year": "2013",
      "venue": "The language of emotion"
    },
    {
      "citation_id": "17",
      "title": "The role of language in emotion: Predictions from psychological constructionism",
      "authors": [
        "Kristen Lindquist",
        "Jennifer Maccormack",
        "Holly Shablack"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "18",
      "title": "Describe your facial expressions by linking image encoders and large language models",
      "authors": [
        "Yujian Yuan",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2023",
      "venue": "BMVC"
    },
    {
      "citation_id": "19",
      "title": "Facial affective behavior analysis with instruction tuning",
      "authors": [
        "Yifan Li",
        "Anh Dao",
        "Wentao Bao",
        "Zhen Tan",
        "Tianlong Chen",
        "Huan Liu",
        "Yu Kong"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "21",
      "title": "Emoclip: A vision-language method for zero-shot video facial expression recognition",
      "authors": [
        "Niki Maria",
        "Ioannis Patras"
      ],
      "year": "2024",
      "venue": "2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "22",
      "title": "Flip-80m: 80 million visual-linguistic pairs for facial language-image pre-training",
      "authors": [
        "Yudong Li",
        "Xianxu Hou",
        "Zheng Dezhi",
        "Linlin Shen",
        "Zhe Zhao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Enhancing zero-shot facial expression recognition by llm knowledge transfer",
      "authors": [
        "Zengqun Zhao",
        "Yu Cao"
      ],
      "year": "2025",
      "venue": "2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "24",
      "title": "Evaluating multimodal llms on face understanding",
      "authors": [
        "Kartik Narayan",
        "V Vibashan",
        "M Vishal",
        "Patel",
        "Facexbench"
      ],
      "year": "2025",
      "venue": "Evaluating multimodal llms on face understanding",
      "arxiv": "arXiv:2501.10360"
    },
    {
      "citation_id": "25",
      "title": "Face-human-bench: A comprehensive benchmark of face and human understanding for multi-modal assistants",
      "authors": [
        "Lixiong Qin",
        "Shilong Ou",
        "Miaoxuan Zhang",
        "Jiangning Wei",
        "Yuhang Zhang",
        "Xiaoshuai Song",
        "Yuchen Liu",
        "Mei Wang",
        "Weiran Xu"
      ],
      "year": "2025",
      "venue": "Face-human-bench: A comprehensive benchmark of face and human understanding for multi-modal assistants",
      "arxiv": "arXiv:2501.01243"
    },
    {
      "citation_id": "26",
      "title": "Gpt-4v with emotion: A zero-shot benchmark for generalized emotion recognition",
      "authors": [
        "Zheng Lian",
        "Licai Sun",
        "Haiyang Sun",
        "Kang Chen",
        "Zhuofan Wen",
        "Hao Gu",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "27",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "Gemini Team",
        "Petko Georgiev",
        "Ian Ving",
        "Ryan Lei",
        "Libin Burnell",
        "Anmol Bai",
        "Garrett Gulati",
        "Damien Tanzer",
        "Zhufeng Vincent",
        "Shibo Pan",
        "Wang"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "arxiv": "arXiv:2403.05530"
    },
    {
      "citation_id": "28",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Yong Li",
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "IEEE transactions on image processing"
    },
    {
      "citation_id": "29",
      "title": "Region attention networks for pose and occlusion robust facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Debin Meng",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "30",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu",
        "Shanmin Wang"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "31",
      "title": "Robust lightweight facial expression recognition network with label distribution training",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu",
        "Feng Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "32",
      "title": "Facial expression recognition with visual transformers and attentional selective fusion",
      "authors": [
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "Transfer: Learning relation-aware facial expression representations with transformers",
      "authors": [
        "Fanglei Xue",
        "Qiangchang Wang",
        "Guodong Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International conference on computer vision"
    },
    {
      "citation_id": "34",
      "title": "Poster: A pyramid cross-fusion transformer network for facial expression recognition",
      "authors": [
        "Ce Zheng",
        "Matias Mendieta",
        "Chen Chen"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "35",
      "title": "Svfap: Self-supervised video facial affect perceiver",
      "authors": [
        "Licai Sun",
        "Zheng Lian",
        "Kexin Wang",
        "Yu He",
        "Mingyu Xu",
        "Haiyang Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Poster++: A simpler and stronger facial expression recognition network",
      "authors": [
        "Jiawei Mao",
        "Rui Xu",
        "Xuesong Yin",
        "Yuanqi Chang",
        "Binling Nie",
        "Aibin Huang",
        "Yigang Wang"
      ],
      "year": "2024",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "38",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Shijian Lu",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "39",
      "title": "Relative uncertainty learning for facial expression recognition",
      "authors": [
        "Yuhang Zhang",
        "Chengrui Wang",
        "Weihong Deng"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "40",
      "title": "Learning emotion representations from verbal and nonverbal communication",
      "authors": [
        "Sitao Zhang",
        "Yimu Pan",
        "James Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "41",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "42",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "43",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "44",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "45",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Yuanyuan Liu",
        "Wei Dai",
        "Chuanxu Feng",
        "Wenbin Wang",
        "Guanghao Yin",
        "Jiabei Zeng",
        "Shiguang Shan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM international conference on multimedia"
    },
    {
      "citation_id": "46",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Yan Wang",
        "Yixuan Sun",
        "Yiwen Huang",
        "Zhongying Liu",
        "Shuyong Gao",
        "Wei Zhang",
        "Weifeng Ge",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "47",
      "title": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "Aff-wild2: Extending the aff-wild database for affect recognition",
      "arxiv": "arXiv:1811.07770"
    },
    {
      "citation_id": "48",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "49",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "Shichuan Du",
        "Yong Tao",
        "Aleix Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "50",
      "title": "Configural information in facial expression perception",
      "authors": [
        "J Andrew",
        "Andrew Calder",
        "Jill Young",
        "Michael Keane",
        "Dean"
      ],
      "year": "2000",
      "venue": "Journal of Experimental Psychology: Human perception and performance"
    },
    {
      "citation_id": "51",
      "title": "Parts and wholes in expression recognition",
      "authors": [
        "Murray White"
      ],
      "year": "2000",
      "venue": "Cognition & Emotion"
    },
    {
      "citation_id": "52",
      "title": "Mixed emotions: Holistic and analytic perception of facial expressions",
      "authors": [
        "Martha James W Tanaka",
        "Sean Kaiser",
        "Richard Butler",
        "Grand"
      ],
      "year": "2012",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "53",
      "title": "The role of facial movements in emotion recognition",
      "authors": [
        "Lina Eva G Krumhuber",
        "Harold Ch Skora",
        "Karen Hill",
        "Lander"
      ],
      "year": "2023",
      "venue": "Nature Reviews Psychology"
    },
    {
      "citation_id": "54",
      "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models",
      "authors": [
        "Junnan Li",
        "Dongxu Li",
        "Silvio Savarese",
        "Steven Hoi"
      ],
      "year": "2023",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "55",
      "title": "Scaling up visual and vision-language representation learning with noisy text supervision",
      "authors": [
        "Chao Jia",
        "Yinfei Yang",
        "Ye Xia",
        "Yi-Ting Chen",
        "Zarana Parekh",
        "Hieu Pham",
        "Quoc Le",
        "Yun-Hsuan Sung",
        "Zhen Li",
        "Tom Duerig"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "56",
      "title": "Reproducible scaling laws for contrastive language-image learning",
      "authors": [
        "Mehdi Cherti",
        "Romain Beaumont",
        "Ross Wightman",
        "Mitchell Wortsman",
        "Gabriel Ilharco",
        "Cade Gordon",
        "Christoph Schuhmann",
        "Ludwig Schmidt",
        "Jenia Jitsev"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "57",
      "title": "Sigmoid loss for language image pre-training",
      "authors": [
        "Xiaohua Zhai",
        "Basil Mustafa",
        "Alexander Kolesnikov",
        "Lucas Beyer"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "58",
      "title": "Eva-clip: Improved training techniques for clip at scale",
      "authors": [
        "Quan Sun",
        "Yuxin Fang",
        "Ledell Wu",
        "Xinlong Wang",
        "Yue Cao"
      ],
      "year": "2023",
      "venue": "Eva-clip: Improved training techniques for clip at scale",
      "arxiv": "arXiv:2303.15389"
    },
    {
      "citation_id": "59",
      "title": "Demystifying clip data",
      "authors": [
        "Hu Xu",
        "Saining Xie",
        "Xiaoqing Tan",
        "Po-Yao Huang",
        "Russell Howes",
        "Vasu Sharma",
        "Shang-Wen Li",
        "Gargi Ghosh",
        "Luke Zettlemoyer",
        "Christoph Feichtenhofer"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "60",
      "title": "Dreamlip: Language-image pre-training with long captions",
      "authors": [
        "Kecheng Zheng",
        "Yifei Zhang",
        "Wei Wu",
        "Fan Lu",
        "Shuailei Ma",
        "Xin Jin",
        "Wei Chen",
        "Yujun Shen"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "61",
      "title": "Modeling caption diversity in contrastive vision-language pretraining",
      "authors": [
        "Samuel Lavoie",
        "Polina Kirichenko",
        "Mark Ibrahim",
        "Mahmoud Assran",
        "Andrew Gordon Wilson",
        "Aaron Courville",
        "Nicolas Ballas"
      ],
      "year": "2024",
      "venue": "Modeling caption diversity in contrastive vision-language pretraining",
      "arxiv": "arXiv:2405.00740"
    },
    {
      "citation_id": "62",
      "title": "Improving fine-grained understanding in image-text pre-training",
      "authors": [
        "Ioana Bica",
        "Anastasija Ilić",
        "Matthias Bauer",
        "Goker Erdogan",
        "Matko Bošnjak",
        "Christos Kaplanis",
        "Alexey Gritsenko",
        "Matthias Minderer",
        "Charles Blundell",
        "Razvan Paşcanu"
      ],
      "year": "2024",
      "venue": "Proceedings of the 41st International Conference on Machine Learning"
    },
    {
      "citation_id": "63",
      "title": "General facial representation learning in a visual-linguistic manner",
      "authors": [
        "Yinglin Zheng",
        "Hao Yang",
        "Ting Zhang",
        "Jianmin Bao",
        "Dongdong Chen",
        "Yangyu Huang",
        "Lu Yuan",
        "Dong Chen",
        "Ming Zeng",
        "Fang Wen"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "64",
      "title": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "authors": [
        "Josh Achiam",
        "Steven Adler",
        "Sandhini Agarwal",
        "Lama Ahmad",
        "Ilge Akkaya",
        "Florencia Leoni Aleman",
        "Diogo Almeida",
        "Janko Altenschmidt",
        "Sam Altman"
      ],
      "year": "2023",
      "venue": "Shyamal Anadkat, et al. Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "65",
      "title": "Visual instruction tuning",
      "authors": [
        "Haotian Liu",
        "Chunyuan Li",
        "Qingyang Wu",
        "Yong Jae Lee"
      ],
      "year": "2023",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "66",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "Deyao Zhu",
        "Jun Chen",
        "Xiaoqian Shen",
        "Xiang Li",
        "Mohamed Elhoseiny"
      ],
      "year": "2024",
      "venue": "ICLR"
    },
    {
      "citation_id": "67",
      "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks",
      "authors": [
        "Zhe Chen",
        "Jiannan Wu",
        "Wenhai Wang",
        "Weijie Su",
        "Guo Chen",
        "Sen Xing",
        "Muyan Zhong",
        "Qinglong Zhang",
        "Xizhou Zhu",
        "Lewei Lu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "68",
      "title": "Qwen technical report",
      "authors": [
        "Jinze Bai",
        "Shuai Bai",
        "Yunfei Chu",
        "Zeyu Cui",
        "Kai Dang",
        "Xiaodong Deng",
        "Yang Fan",
        "Wenbin Ge",
        "Yu Han",
        "Fei Huang"
      ],
      "year": "2023",
      "venue": "Qwen technical report",
      "arxiv": "arXiv:2309.16609"
    },
    {
      "citation_id": "69",
      "title": "Expllm: Towards chain of thought for facial expression recognition",
      "authors": [
        "Xing Lan",
        "Jian Xue",
        "Ji Qi",
        "Dongmei Jiang",
        "Ke Lu",
        "Tat-Seng Chua"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "70",
      "title": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "authors": [
        "Bohao Xing",
        "Zitong Yu",
        "Xin Liu",
        "Kaishen Yuan",
        "Qilang Ye",
        "Weicheng Xie",
        "Huanjing Yue",
        "Jingyu Yang",
        "Heikki Kälviäinen"
      ],
      "year": "2024",
      "venue": "Emo-llama: Enhancing facial emotion understanding with instruction tuning",
      "arxiv": "arXiv:2408.11424"
    },
    {
      "citation_id": "71",
      "title": "Emotion-llama: Multimodal emotion recognition and reasoning with instruction tuning",
      "authors": [
        "Zebang Cheng",
        "Zhi-Qi Cheng",
        "Jun-Yan He",
        "Kai Wang",
        "Yuxiang Lin",
        "Zheng Lian",
        "Xiaojiang Peng",
        "Alexander Hauptmann"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "72",
      "title": "Affectgpt: A new dataset, model, and benchmark for emotion understanding with multimodal large language models",
      "authors": [
        "Zheng Lian",
        "Haoyu Chen",
        "Lan Chen",
        "Haiyang Sun",
        "Licai Sun",
        "Yong Ren",
        "Zebang Cheng",
        "Bin Liu",
        "Rui Liu",
        "Xiaojiang Peng"
      ],
      "year": "2025",
      "venue": "Affectgpt: A new dataset, model, and benchmark for emotion understanding with multimodal large language models",
      "arxiv": "arXiv:2501.16566"
    },
    {
      "citation_id": "73",
      "title": "R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning",
      "authors": [
        "Jiaxing Zhao",
        "Xihan Wei",
        "Liefeng Bo"
      ],
      "year": "2025",
      "venue": "R1-omni: Explainable omni-multimodal emotion recognition with reinforcement learning",
      "arxiv": "arXiv:2503.05379"
    },
    {
      "citation_id": "74",
      "title": "Generative adversarial network for text-to-face synthesis and manipulation with pretrained bert model",
      "authors": [
        "Yutong Zhou",
        "Nobutaka Shimada"
      ],
      "year": "2021",
      "venue": "2021 16th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2021)"
    },
    {
      "citation_id": "75",
      "title": "Tedigan: Text-guided diverse face image generation and manipulation",
      "authors": [
        "Weihao Xia",
        "Yujiu Yang",
        "Jing-Hao Xue",
        "Baoyuan Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "76",
      "title": "Talk-to-edit: Fine-grained facial editing via dialog",
      "authors": [
        "Yuming Jiang",
        "Ziqi Huang",
        "Xingang Pan",
        "Chen Loy",
        "Ziwei Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "77",
      "title": "15m multimodal facial image-text dataset",
      "authors": [
        "Dawei Dai",
        "Yutang Li",
        "Yingge Liu",
        "Mingming Jia",
        "Zhang Yuanhui",
        "Guoyin Wang"
      ],
      "year": "2024",
      "venue": "15m multimodal facial image-text dataset",
      "arxiv": "arXiv:2407.08515"
    },
    {
      "citation_id": "78",
      "title": "Measuring non-typical emotions for mental health: A survey of computational approaches",
      "authors": [
        "Puneet Kumar",
        "Alexander Vedernikov",
        "Xiaobai Li"
      ],
      "year": "2024",
      "venue": "Measuring non-typical emotions for mental health: A survey of computational approaches",
      "arxiv": "arXiv:2403.08824"
    },
    {
      "citation_id": "79",
      "title": "Recognizing emotion from facial expressions: psychological and neurological mechanisms",
      "authors": [
        "Ralph Adolphs"
      ],
      "year": "2002",
      "venue": "Behavioral and cognitive neuroscience reviews"
    },
    {
      "citation_id": "80",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "81",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "82",
      "title": "Facial action coding system",
      "authors": [
        "Paul Ekman",
        "Wallace Friesen"
      ],
      "year": "1978",
      "venue": "Environmental Psychology & Nonverbal Behavior"
    },
    {
      "citation_id": "83",
      "title": "Softclip: Softer cross-modal alignment makes clip stronger",
      "authors": [
        "Yuting Gao",
        "Jinfeng Liu",
        "Zihan Xu",
        "Tong Wu",
        "Enwei Zhang",
        "Ke Li",
        "Jie Yang",
        "Wei Liu",
        "Xing Sun"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "84",
      "title": "Cwcl: Cross-modal transfer with continuously weighted contrastive loss",
      "authors": [
        "Rakshith Sharma Srinivasa",
        "Jaejin Cho",
        "Chouchang Yang",
        "Yashas Malur Saidutta",
        "Ching-Hua Lee",
        "Yilin Shen",
        "Hongxia Jin"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "85",
      "title": "Generalizable facial expression recognition",
      "authors": [
        "Yuhang Zhang",
        "Xiuqi Zheng",
        "Chenyi Liang",
        "Jiani Hu",
        "Weihong Deng"
      ],
      "year": "2024",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "86",
      "title": "Flava: A foundational language and vision alignment model",
      "authors": [
        "Amanpreet Singh",
        "Ronghang Hu",
        "Vedanuj Goswami",
        "Guillaume Couairon",
        "Wojciech Galuba",
        "Marcus Rohrbach",
        "Douwe Kiela"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "87",
      "title": "Face-mllm: A large face perception model",
      "authors": [
        "Haomiao Sun",
        "Mingjie He",
        "Tianheng Lian",
        "Hu Han",
        "Shiguang Shan"
      ],
      "year": "2024",
      "venue": "Face-mllm: A large face perception model",
      "arxiv": "arXiv:2410.20717"
    },
    {
      "citation_id": "88",
      "title": "Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features",
      "authors": [
        "Alexey Michael Tschannen",
        "Xiao Gritsenko",
        "Muhammad Wang",
        "Ibrahim Ferjad Naeem",
        "Nikhil Alabdulmohsin",
        "Talfan Parthasarathy",
        "Lucas Evans",
        "Ye Beyer",
        "Basil Xia",
        "Mustafa"
      ],
      "year": "2025",
      "venue": "Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features",
      "arxiv": "arXiv:2502.14786"
    },
    {
      "citation_id": "89",
      "title": "Learn from all: Erasing attention consistency for noisy label facial expression recognition",
      "authors": [
        "Yuhang Zhang",
        "Chengrui Wang",
        "Xu Ling",
        "Weihong Deng"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "90",
      "title": "Latent-ofer: Detect, mask, and reconstruct with latent vectors for occluded facial expression recognition",
      "authors": [
        "Isack Lee",
        "Eungi Lee",
        "Bong Seok",
        "Yoo"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "91",
      "title": "From static to dynamic: Adapting landmark-aware image models for facial expression recognition in videos",
      "authors": [
        "Yin Chen",
        "Jia Li",
        "Shiguang Shan",
        "Meng Wang",
        "Richang Hong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "92",
      "title": "Videoclip: Contrastive pre-training for zero-shot video-text understanding",
      "authors": [
        "Hu Xu",
        "Gargi Ghosh",
        "Po-Yao Huang",
        "Dmytro Okhonko",
        "Armen Aghajanyan",
        "Florian Metze",
        "Luke Zettlemoyer",
        "Christoph Feichtenhofer"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "93",
      "title": "Expanding language-image pretrained models for general video recognition",
      "authors": [
        "Bolin Ni",
        "Houwen Peng",
        "Minghao Chen",
        "Songyang Zhang",
        "Gaofeng Meng",
        "Jianlong Fu",
        "Shiming Xiang",
        "Haibin Ling"
      ],
      "year": "2022",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "94",
      "title": "Learning to prompt for vision-language models",
      "authors": [
        "Kaiyang Zhou",
        "Jingkang Yang",
        "Chen Loy",
        "Ziwei Liu"
      ],
      "year": "2022",
      "venue": "International Journal of Computer Vision"
    }
  ]
}