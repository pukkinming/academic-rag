{
  "paper_id": "2204.05571v1",
  "title": "Speech Emotion Recognition With Global-Aware Fusion On Multi-Scale Feature Representation",
  "published": "2022-04-12T07:03:04Z",
  "authors": [
    "Wenjing Zhu",
    "Xiang Li"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Attention Mechanism",
    "Multi-scale Features",
    "Feature Fusion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech Emotion Recognition (SER) is a fundamental task to predict the emotion label from speech data. Recent works mostly focus on using convolutional neural networks (CNNs) to learn local attention map on fixed-scale feature representation by viewing time-varied spectral features as images. However, rich emotional feature at different scales and important global information are not able to be well captured due to the limits of existing CNNs for SER. In this paper, we propose a novel GLobal-Aware Multi-scale (GLAM) neural network 1 to learn multi-scale feature representation with global-aware fusion module to attend emotional information. Specifically, GLAM iteratively utilizes multiple convolutional kernels with different scales to learn multiple feature representation. Then, instead of using attention-based methods, a simple but effective global-aware fusion module is applied to grab most important emotional information globally. Experiments on the benchmark corpus IEMOCAP over four emotions demonstrates the superiority of our proposed model with 2.5% to 4.5% improvements on four common metrics compared to previous state-of-the-art approaches.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech is the key medium of communication between people and plays an important role in everyone's daily life. Speech Emotion Recognition (SER) aims to predict the emotion reflected in speech. SER is a fundamental task for various intelligent applications. For example, SER brings better user experience for intelligent robots by better understanding user's intents and states. Thus, interests from research communities have been increased for SER task  [1] .\n\nRecently, inspired by the uplifting progress in computer vision, existing studies  [2, 3, 4, 5, 6, 7, 8, 9]  have achieved great improvements on SER by viewing spectral features as images. The standard convolutional neural network (CNN) architecture mainly consists of feature representation and attention map. Generally, feature representation usually learns fixed-scale features for capturing all informative features. Besides, attention map is utilized to attend most emotional information among local fixed-scale features  [6, 7, 8, 9] . However, emotions are characterized by articulation variations with different scales in time-varied spectral feature.\n\nDue to the limits of existing CNNs in SER, we propose a novel GLobal-Aware Multi-scale (GLAM) neural network. In our work, we utilize multi-scale convolutional blocks to extract features at varying scales due to different temporal spans and intonation strength in emotions. Furthermore, a global-aware fusion module is introduced to select important information reflecting emotional patterns in different scales. Experiments show both multi-scale feature representation and global-aware fusion can bring great improvements compared to the state-of-the-art CNN model  [9] .\n\nOur contributions can be summarized as follows:\n\n• To the best of our knowledge, we are the first to apply multi-scale feature representation to SER. Besides, this representation also enhances existing CNNs greatly.\n\n• We introduce a new global-aware fusion module to grab emotional information across multiple scales. Our study displays the great potential of information fusion among different granularities.\n\n• Experiments demonstrate the superiority of our proposed model on four common metrics for SER. We achieve 2.5% to 4.5% improvements on the IEMOCAP corpus compared to state-of-the-art methods.\n\nThe rest of this paper is organized as follows. We review related methods in Section 2. Section 3 presents the proposed GLAM neural network. Finally, Section 4 and Section 5 are the experiment results and conclusion.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Typical works on SER focus on feature representation of the emotion information in speech. The first deep learning method was utilized to learn feature representation for predicting emotion label of each feature segment  [10] . Then, by viewing feature segments as images, convolutional networks show the great effects on feature representation for SER  [2, 4, 6, 8, 9, 11, 12] . Finally, various methods such as extreme learning machine (ELM)  [10] , capsule networks  [12] , and LSTM  [2, 11]  have been applied to fuse all feature representations from all feature segments. However, most existing CNNs focus on fixed-scale local feature and ignore the effects of varied scales, which are fully considered in our proposed model.\n\nRecently, attention-based models have made significant progress on SER  [6, 7, 8, 9, 13] . For example, multi-head attention maps are learned by convolutional operations to select important information according to surrounding information  [8] . Moreover, area attention is further introduced to compute the importance from different ranges of convolutions  [9] . Though local information is useful for describing subtle information, the lack of perceiving global information limits the advance of existing attention-based methods. Hence, we propose a global-aware fusion module to compute global attention map and fuse emotional information across different scales.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "In this section, we present our GLAM model in detail. As shown in Fig.  1 , GLAM contains two main modules: multiscale module and global-aware fusion module. We first introduce these two modules and then combine a mixup method to enhance the generalization of GLAM.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Scale Feature Representation",
      "text": "We propose a multi-scale block to parallelize multi-scale convolutional layers on the same level. The multi-scale block, which receive different sized receptive fields, can capture multi-scale feature representation. The structure is shown in right down panel of Fig.  1 . We keep the channel size unchanged. For better capturing spatial and temporal features, we implement kernel size of 1 × 3 for spatial convolution and kernel size of 3 × 1 for temporal convolution with batch normalization put after on. The out-channels both are 16. For the first layer, we concatenate spatial and temporal convolved outputs alongside spatial dimension. For the rest, the outputs are concatenated alongside channel dimension. In the last convolutional layer with batch normalization followed, we use a large kernel size of 5 × 5. Same paddings are applied to all of convolutional layers.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Global-Aware Fusion",
      "text": "As the multi-scale architecture extracts local features, we consider correlating features and enhance feature communica-Fig.  1 . Overview of the model architecture. tions globally. We implement a gMLP block  [14] , named as a global-aware block in this paper, shown in the right up panel of Fig.  1 , to enhance cross-scale feature communication. For a feature map X with a size of C × d f , where C is the channel size and d f represents size of flattened feature map. The output dimension of first and last fully connected layer are 4d f and d f respectively. After a projection of GeLU, the output is split along feature dimension. And the convolutional layer keeps channel size unchanged. The operations of multiplication enhance feature mixing across channel dimension. The output of global-aware fusion module is then reshaped to feed into fully connected (FC) network for classification.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Augment",
      "text": "To improve generalization capability of the system, we use a mixup method  [15]  for training, which combines pairs of examples and their labels. Mixup mixes two pair training examples (x i , y i ) and (x j , y j ) as a new constructed example\n\nwhere (x i , y i ) and (x j , y j ) are randomly drawn from training data and λ ∼ Beta(α, α) with α ∈ (0, ∞). Mixup regularizes the neural network to favor simple linear behavior in-between training examples. This method effectively smoothens discrete data space into continuous space.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments",
      "text": "In this section, we first introduce the experimental settings. Then, we compare our proposed model with state-of-the-art methods. Finally, we conduct ablation study on two modules.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Settings",
      "text": "Datasets. We conduct our experiments on the benchmark corpus IEMOCAP  [16] . IEMOCAP contains 12 hours of emotional speech performed by 10 actors. We select four types of emotions angry, happy, sad, and neutral as same as previous studies  [6, 7, 8, 9, 13]  for balancing the category distribution.\n\nIn agreement with previous works, we label excited as happy category due to similarity of two categories. According to whether the actors perform a fixed script, we have three types of datasets for experiments: Improvisation, Script, and Full. Compared Methods and Evaluation Metrics. To study our proposed model deeply, we choose the best model of Area Attention CNN (AACNN)  [9]  with recent work Multi-Head Attention CNN (MHCNN)  [8]  and Attention Pooling CNN (APCNN)  [6]  for comparison. Moreover, to comprehensively understand prediction performance, we utilize four common metrics Weighted Accuracy (WA), Unweighted Accuracy (UA), Micro F1 score, and Macro F1 score. The distinction between WA and UA ( micro and macro F1 score) is that the former takes label imbalance into account, while the latter does not. We report the mean value and standard deviation of above metrics in experiments. In addition, we will also show confusion matrix for concrete analysis. Implementation Details. Since there is no unified way to split dataset, we randomly divide dataset into 80% of data for training and the rest for test with 100 times to ensure the reliability of experiment results. We use mel-frequency cepstral coefficients (MFCCs)  [17, 18]  as the feature input. Each utterance is divided into 2-second segments with 1.6 second overlap between segments. We average the prediction results of all segments in same utterance as the prediction result. Crossentropy criterion is used as an objective function. Adam optimizer is adopted with weight decay rate as 10 -6 . The learning rate is initialized to 10 -4 and exponentially decayed with multiplicative factor 0.95 until the value reaches 10 -6 . The models are all trained for 50 epochs and the batch size is set to 32. In our proposed model, we evaluate the performance of different mixup parameter α and set it to 0.5 for comparison.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "Experiments are conducted on three types of datasets with four common metrics, shown in Table  1 . Compared to recent the state-of-the-art approaches, the proposed architecture significantly improves performances on all listed metrics, which demonstrates the effectiveness of global-aware fusion and multi-scale technique.\n\nMeanwhile, GLAM has smaller values of standard deviation for four common metrics which indicates a robust and stable performance. In detail, we compute a confusion matrix to exploit accuracies for each emotional category, see in Fig.  2 . GLAM eliminates confusion sets of neutral-angry and sad-angry and superiorly increase 6.95% on prediction of sad label and 4.52% on prediction of angry label. With t-distributed stochastic neighbor embeddings (t-SNE) method  [19] , we show 2D projections of high-level features in Fig.  3  by using the last second layer output for APCNN, MHCNN and GLAM and applying an additional dense layer to resize large sized output for AACNN. The visualization of APCNN in Fig.  3 a ) shows a drastic overlap among four categories.\n\nFor a MHCNN model shown in Fig.  3 b ), the happy category is unobviously departed from other categories. For a AACNN model shown in Fig.  3 c ), the areas of four categories are successively joint together. In our model, see in Fig.  3 d ), the happy category is apparently separated from other categories. We see a few angry points located in area of neutral. There is a better distinction between sad and angry categories compared to AACNN model. From above results, we see effectiveness of the multiscale technique with a global-aware perceiving. Previous works only scale features with fixed length. And current attention-based SER methods are lack of perceiving global information. While, GLAM exploits features at different scales for multiple spanned patterns and attend global information across different features.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "We conduct ablation experiments on the model by removing and replacing modules used in our proposed module. Results are shown in Table  2 . By removing the global-aware fusion module, the multi-scale module universally outperforms over all existing methods. By replacing such module with Multi-Head Attention (MHA)  [8]  or Area Attention (AA)  [9] , the performance even degrades compared to pure multi-scale module, which indicates effective attention on high dimensional representation globally rather than local fusion. Selection of α. Without the mixup method (i.e. α = 0), our model surpasses previous works. Further improvement is realized by using mixup method with the optimal result for α = 0.5 in Table  3 , which indicates the effectiveness of enhancing generalization capability.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a novel architecture, named as GLAM network, which combines a multi-scale module and a global-aware fusion module. The multi-scale module captures different scaled features for various time-spanned patterns and the global-aware module enhances cross-scale communication and attends feature confusion at different scales. In addition, we use mixup method to enhance the generalization of our model. Experiment results demonstrate that our model achieves the state-of-the-art performance on the benchmark dataset IEMOCAP over 4-class emotion classification.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , GLAM contains two main modules: multi-",
      "page": 2
    },
    {
      "caption": "Figure 1: We keep the channel size un-",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of the model architecture.",
      "page": 2
    },
    {
      "caption": "Figure 1: , to enhance cross-scale feature communication. For",
      "page": 2
    },
    {
      "caption": "Figure 2: Confusion matrix on Improvisation dataset. a) and b)",
      "page": 4
    },
    {
      "caption": "Figure 2: GLAM eliminates confusion sets of neutral-angry",
      "page": 4
    },
    {
      "caption": "Figure 3: by using the last second layer output for APCNN, MHCNN",
      "page": 4
    },
    {
      "caption": "Figure 3: a) shows a drastic overlap among four categories.",
      "page": 4
    },
    {
      "caption": "Figure 3: b), the happy category",
      "page": 4
    },
    {
      "caption": "Figure 3: c), the areas of four categories are suc-",
      "page": 4
    },
    {
      "caption": "Figure 3: Visualizations of hidden states by commonly-used t-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Du Xiaoman, Beijing, China": "{zhuwenjing02,lixiang01}@duxiaoman.com"
        },
        {
          "Du Xiaoman, Beijing, China": "ABSTRACT"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "Speech Emotion Recognition (SER) is a fundamental\ntask to"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "predict\nthe emotion label\nfrom speech data.\nRecent works"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "mostly focus on using convolutional neural networks (CNNs)"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "to learn local attention map on ﬁxed-scale feature representa-"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "tion by viewing time-varied spectral features as images. How-"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "ever, rich emotional feature at different scales and important"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "global information are not able to be well captured due to the"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "limits of existing CNNs for SER. In this paper, we propose a"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "novel GLobal-Aware Multi-scale (GLAM) neural network1"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "to learn multi-scale feature representation with global-aware"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "fusion module to attend emotional\ninformation. Speciﬁcally,"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "GLAM iteratively utilizes multiple convolutional kernels with"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "different scales to learn multiple feature representation. Then,"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "instead of using attention-based methods,\na simple but ef-"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "fective global-aware fusion module is applied to grab most"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "important emotional\ninformation globally.\nExperiments on"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "the benchmark corpus IEMOCAP over four emotions demon-"
        },
        {
          "Du Xiaoman, Beijing, China": "strates the superiority of our proposed model with 2.5% to"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "4.5% improvements on four common metrics compared to"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "previous state-of-the-art approaches."
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "Index Terms— Speech Emotion Recognition, Attention"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "Mechanism, Multi-scale Features, Feature Fusion"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "1.\nINTRODUCTION"
        },
        {
          "Du Xiaoman, Beijing, China": ""
        },
        {
          "Du Xiaoman, Beijing, China": "Speech is the key medium of communication between people"
        },
        {
          "Du Xiaoman, Beijing, China": "and plays an important role in everyone’s daily life. Speech"
        },
        {
          "Du Xiaoman, Beijing, China": "Emotion Recognition (SER) aims to predict\nthe emotion re-"
        },
        {
          "Du Xiaoman, Beijing, China": "ﬂected in speech. SER is a fundamental task for various intel-"
        },
        {
          "Du Xiaoman, Beijing, China": "ligent applications. For example, SER brings better user ex-"
        },
        {
          "Du Xiaoman, Beijing, China": "perience for intelligent robots by better understanding user’s"
        },
        {
          "Du Xiaoman, Beijing, China": "intents and states. Thus, interests from research communities"
        },
        {
          "Du Xiaoman, Beijing, China": "have been increased for SER task [1]."
        },
        {
          "Du Xiaoman, Beijing, China": "Recently,\ninspired by the uplifting progress in computer"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "method was utilized to learn feature representation for pre-": "dicting emotion label of each feature segment\n[10].\nThen,"
        },
        {
          "method was utilized to learn feature representation for pre-": "by viewing feature segments as\nimages,\nconvolutional net-"
        },
        {
          "method was utilized to learn feature representation for pre-": "works\nshow the great effects on feature representation for"
        },
        {
          "method was utilized to learn feature representation for pre-": "SER [2, 4, 6, 8, 9, 11, 12].\nFinally, various methods such"
        },
        {
          "method was utilized to learn feature representation for pre-": "as extreme learning machine (ELM)\n[10], capsule networks"
        },
        {
          "method was utilized to learn feature representation for pre-": "[12], and LSTM [2, 11] have been applied to fuse all feature"
        },
        {
          "method was utilized to learn feature representation for pre-": "representations\nfrom all\nfeature segments.\nHowever, most"
        },
        {
          "method was utilized to learn feature representation for pre-": "existing CNNs focus on ﬁxed-scale local feature and ignore"
        },
        {
          "method was utilized to learn feature representation for pre-": "the effects of varied scales, which are fully considered in our"
        },
        {
          "method was utilized to learn feature representation for pre-": "proposed model."
        },
        {
          "method was utilized to learn feature representation for pre-": "Recently, attention-based models have made signiﬁcant"
        },
        {
          "method was utilized to learn feature representation for pre-": "progress on SER [6, 7, 8, 9, 13].\nFor example, multi-head"
        },
        {
          "method was utilized to learn feature representation for pre-": "attention maps are learned by convolutional operations to se-"
        },
        {
          "method was utilized to learn feature representation for pre-": "lect\nimportant\ninformation according to surrounding infor-"
        },
        {
          "method was utilized to learn feature representation for pre-": "mation [8]. Moreover,\narea attention is\nfurther\nintroduced"
        },
        {
          "method was utilized to learn feature representation for pre-": "to compute the importance from different\nranges of convo-"
        },
        {
          "method was utilized to learn feature representation for pre-": "lutions [9]. Though local\ninformation is useful\nfor describ-"
        },
        {
          "method was utilized to learn feature representation for pre-": "ing subtle information, the lack of perceiving global informa-"
        },
        {
          "method was utilized to learn feature representation for pre-": "tion limits the advance of existing attention-based methods."
        },
        {
          "method was utilized to learn feature representation for pre-": "Hence, we propose a global-aware fusion module to compute"
        },
        {
          "method was utilized to learn feature representation for pre-": "global attention map and fuse emotional\ninformation across"
        },
        {
          "method was utilized to learn feature representation for pre-": "different scales."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "different scales.": ""
        },
        {
          "different scales.": "3. METHODOLOGY"
        },
        {
          "different scales.": "In this section, we present our GLAM model\nin detail. As"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "shown in Fig. 1, GLAM contains two main modules: multi-"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "scale module and global-aware fusion module. We ﬁrst intro-"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "duce these two modules and then combine a mixup method to"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "enhance the generalization of GLAM."
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "3.1. Multi-scale Feature Representation"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "We propose a multi-scale block to parallelize multi-scale con-"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "volutional\nlayers on the same level.\nThe multi-scale block,"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "which receive different\nsized receptive ﬁelds,\ncan capture"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "multi-scale feature representation. The structure is shown in"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "right down panel of Fig. 1. We keep the channel\nsize un-"
        },
        {
          "different scales.": "changed. For better capturing spatial and temporal features,"
        },
        {
          "different scales.": ""
        },
        {
          "different scales.": "we implement kernel\nsize of 1 × 3 for\nspatial convolution"
        },
        {
          "different scales.": "and kernel size of 3 × 1 for temporal convolution with batch"
        },
        {
          "different scales.": "normalization put after on. The out-channels both are 16. For"
        },
        {
          "different scales.": "the ﬁrst layer, we concatenate spatial and temporal convolved"
        },
        {
          "different scales.": "outputs alongside spatial dimension. For the rest, the outputs"
        },
        {
          "different scales.": "are concatenated alongside channel dimension.\nIn the last"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "Model"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "APCNN"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "MHCNN"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "AACNN"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "GLAM"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "APCNN"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "MHCNN"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "AACNN"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "GLAM"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "APCNN"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "MHCNN"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "AACNN"
        },
        {
          "Table 1. Comparison of evaluation metrics on three types of datasets.": "GLAM"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GLAM": "",
          "73.70±1.25\n73.90±1.31\n73.51±1.29\n73.60±1.27": "Performance of multi-scale module and global-aware module on Improvisation dataset."
        },
        {
          "GLAM": "Model",
          "73.70±1.25\n73.90±1.31\n73.51±1.29\n73.60±1.27": "WA\nUA\nmacro-F1\nweighted F1"
        },
        {
          "GLAM": "GLAM",
          "73.70±1.25\n73.90±1.31\n73.51±1.29\n73.60±1.27": "81.18±1.47\n79.25±1.88\n79.87±1.64\n80.99±1.50"
        },
        {
          "GLAM": "Multi-scale",
          "73.70±1.25\n73.90±1.31\n73.51±1.29\n73.60±1.27": "80.89±1.43\n78.85±1.65\n79.47±1.60\n80.67±1.45"
        },
        {
          "GLAM": "Multi-scale + MHA",
          "73.70±1.25\n73.90±1.31\n73.51±1.29\n73.60±1.27": "80.43±1.53\n78.92±1.92\n79.20±1.68\n80.26±1.53"
        },
        {
          "GLAM": "Multi-scale + AA",
          "73.70±1.25\n73.90±1.31\n73.51±1.29\n73.60±1.27": "80.61±1.75\n79.09±1.75\n79.47±1.80\n80.46±1.77"
        },
        {
          "GLAM": "MHCNN",
          "73.70±1.25\n73.90±1.31\n73.51±1.29\n73.60±1.27": "76.09±2.01\n73.87±2.31\n74.27±2.25\n75.91±2.05"
        },
        {
          "GLAM": "AACNN",
          "73.70±1.25\n73.90±1.31\n73.51±1.29\n73.60±1.27": "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "AACNN": "",
          "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58": "Table 3. Result of α on Improvisation dataset."
        },
        {
          "AACNN": "α",
          "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58": "WA\nUA\nmacro-F1\nweighted F1"
        },
        {
          "AACNN": "0",
          "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58": "80.44±1.55\n78.86±1.65\n79.32±1.56\n80.32±1.56"
        },
        {
          "AACNN": "0.3",
          "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58": "81.15±1.73\n79.26±1.98\n79.88±1.85\n80.98±1.76"
        },
        {
          "AACNN": "0.5",
          "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58": "81.18±1.47\n79.25±1.88\n79.87±1.64\n80.99±1.50"
        },
        {
          "AACNN": "0.8",
          "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58": "80.97±1.53\n78.97±1.85\n79.62±1.75\n80.79±1.53"
        },
        {
          "AACNN": "1",
          "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58": "80.96±1.72\n79.19±1.92\n79.65±1.88\n80.78±1.73"
        },
        {
          "AACNN": "2",
          "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58": "80.80±1.39\n79.18±1.66\n79.55±1.50\n80.64±1.40"
        },
        {
          "AACNN": "3",
          "78.47±2.42\n76.68±3.29\n76.69±3.27\n78.29±2.58": "80.61±1.45\n78.95±1.58\n79.26±1.59\n80.44±1.45"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "3\n80.61±1.45\n78.95±1.58": "4. EXPERIMENTS",
          "79.26±1.59\n80.44±1.45": "category due to similarity of\ntwo categories. According to"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "",
          "79.26±1.59\n80.44±1.45": "whether the actors perform a ﬁxed script, we have three types"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "In this section, we ﬁrst\nintroduce the experimental settings.",
          "79.26±1.59\n80.44±1.45": "of datasets for experiments: Improvisation, Script, and Full."
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "Then, we compare our proposed model with state-of-the-art",
          "79.26±1.59\n80.44±1.45": ""
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "",
          "79.26±1.59\n80.44±1.45": "Compared Methods and Evaluation Metrics.\nTo study"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "methods. Finally, we conduct ablation study on two modules.",
          "79.26±1.59\n80.44±1.45": ""
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "",
          "79.26±1.59\n80.44±1.45": "our proposed model deeply, we choose the best model of"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "",
          "79.26±1.59\n80.44±1.45": "Area Attention CNN (AACNN) [9] with recent work Multi-"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "",
          "79.26±1.59\n80.44±1.45": "Head Attention CNN (MHCNN)\n[8] and Attention Pooling"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "4.1. Experimental Settings",
          "79.26±1.59\n80.44±1.45": ""
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "",
          "79.26±1.59\n80.44±1.45": "CNN (APCNN)\n[6]\nfor comparison. Moreover,\nto compre-"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "Datasets. We conduct our experiments on the benchmark cor-",
          "79.26±1.59\n80.44±1.45": "hensively understand prediction performance, we utilize four"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "pus IEMOCAP [16].\nIEMOCAP contains 12 hours of emo-",
          "79.26±1.59\n80.44±1.45": "common metrics Weighted Accuracy (WA), Unweighted Ac-"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "tional speech performed by 10 actors. We select four types of",
          "79.26±1.59\n80.44±1.45": "curacy (UA), Micro F1 score,\nand Macro F1 score.\nThe"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "emotions angry, happy, sad, and neutral as same as previous",
          "79.26±1.59\n80.44±1.45": "distinction between WA and UA ( micro and macro F1 score)"
        },
        {
          "3\n80.61±1.45\n78.95±1.58": "studies [6, 7, 8, 9, 13] for balancing the category distribution.",
          "79.26±1.59\n80.44±1.45": "is that\nthe former\ntakes label\nimbalance into account, while"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Compared to re- attention-based SER methods are lack of perceiving global",
      "data": [
        {
          "deviation of above metrics in experiments.\nIn addition, we": "will also show confusion matrix for concrete analysis.",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "is unobviously departed from other categories. For a AACNN"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "Implementation Details.\nSince there is no uniﬁed way to",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "model shown in Fig. 3 c), the areas of four categories are suc-"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "split dataset, we randomly divide dataset into 80% of data for",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "cessively joint\ntogether.\nIn our model, see in Fig.\n3 d),\nthe"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "training and the rest for test with 100 times to ensure the reli-",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "happy category is apparently separated from other categories."
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "ability of experiment results. We use mel-frequency cepstral",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "We see a few angry points located in area of neutral. There"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "coefﬁcients (MFCCs) [17, 18] as the feature input. Each utter-",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "is a better distinction between sad and angry categories com-"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "ance is divided into 2-second segments with 1.6 second over-",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "pared to AACNN model."
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "lap between segments. We average the prediction results of",
          "For a MHCNN model shown in Fig. 3 b), the happy category": ""
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "all segments in same utterance as the prediction result. Cross-",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "20"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "entropy criterion is used as an objective function. Adam opti-",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "15\n10"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "mizer is adopted with weight decay rate as 10−6. The learn-",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "10"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "5"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "ing rate is initialized to 10−4 and exponentially decayed with",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "5"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "multiplicative factor 0.95 until\nthe value reaches 10−6. The",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "0\n0"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "5"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "models are all\ntrained for 50 epochs and the batch size is set",
          "For a MHCNN model shown in Fig. 3 b), the happy category": ""
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "10"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "to 32. In our proposed model, we evaluate the performance of",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "5"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "15"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "different mixup parameter α and set it to 0.5 for comparison.",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "10"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "20"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "8\n6\n4\n2\n0\n2\n4\n6\n8\n10\n0\n10\n20"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "a) APCNN\nb) MHCNN"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "Prediction\nPrediction",
          "For a MHCNN model shown in Fig. 3 b), the happy category": ""
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "Neutral"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "20\n20\nHappy"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "Neutral\nNeutral\n78.24\n7.06\n2.35\n12.35\n85.57\n4.98\n3.98\n5.47",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "Sad"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "15\nAngry"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "10"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "10"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "Happy\nHappy\n6.93\n89.11\n0.00\n3.96\n6.96\n92.17\n0.87\n0.00",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "5"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "0"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "Actual\nActual",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "0"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "Sad\nSad\n4.17\n4.17\n70.83\n20.83\n16.67\n2.78\n77.78\n2.78",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "10\n5"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "10"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "20"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "15"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "Angry\nAngry\n16.19\n2.86\n10.48\n70.48\n18.75\n3.13\n3.13\n75.00",
          "For a MHCNN model shown in Fig. 3 b), the happy category": ""
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "20"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "20\n10\n0\n10\n20\n30\n20\n10\n0\n10\n20"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "Neutral\nHappy\nSad\nAngry\nNeutral\nHappy\nSad\nAngry",
          "For a MHCNN model shown in Fig. 3 b), the happy category": ""
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "",
          "For a MHCNN model shown in Fig. 3 b), the happy category": "c) AACNN\nd) GLAM"
        },
        {
          "deviation of above metrics in experiments.\nIn addition, we": "a) AACNN\nb) GLAM",
          "For a MHCNN model shown in Fig. 3 b), the happy category": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: , which indicates the effectiveness of en- accuracy of speech emotion recognition with attention",
      "data": [
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "nal Processing Letters, vol. 25, no. 10, pp. 1440–1444,"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "2018."
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "[12] Xixin Wu, Songxiang Liu, Yuewen Cao, Xu Li,\nJian-"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "wei Yu, Dongyang Dai, Xi Ma, Shoukang Hu, Zhiyong"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "Wu, Xunying Liu, and Helen Meng,\n“Speech emotion"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "recognition using capsule networks,”\nin 2019 IEEE In-"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "ternational Conference on Acoustics, Speech and Signal"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "Processing, 2019."
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "[13] Lorenzo Tarantino, Philip N. Garner,\nand Alexandros"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "Lazaridis,\n“Self-attention for speech emotion recogni-"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "the International\ntion,”\nin 2019 Annual Conference of"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "Speech Communication Association, 2019."
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "[14] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le,"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "“Pay attention to mlps,”\nCoRR, vol. abs/2105.08050,"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "2021."
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "[15] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "David Lopez-Paz, “mixup: Beyond empirical risk min-"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "imization,” in 2018 International Conference on Learn-"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "ing Representations, 2018."
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "[16] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "“Iemocap:\nInteractive emotional dyadic motion capture"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "database,” Language resources and evaluation, vol. 42,"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "no. 4, pp. 335–359, 2008."
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "[17] Moataz M. H. El Ayadi, Mohamed S. Kamel, and Fakhri"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "Karray,\n“Speech emotion recognition using gaussian"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "mixture vector autoregressive models,”\nin 2007 IEEE"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "International Conference on Acoustics, Speech and Sig-"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "nal Processing, 2007."
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": ""
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "[18] Weihui Dai, Dongmei Han, Yonghui Dai, and Dongrong"
        },
        {
          "IEEE Sig-\ntion model for speech emotion recognition,”": "Xu,\n“Emotion recognition and affective computing on"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: , which indicates the effectiveness of en- accuracy of speech emotion recognition with attention",
      "data": [
        {
          "realized by using mixup method with the optimal\nresult\nfor": "α = 0.5 in Table 3, which indicates the effectiveness of en-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "accuracy of speech emotion recognition with attention"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "hancing generalization capability.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "head fusion,”\nin 2020 Annual Computing and Commu-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "nication Workshop and Conference, 2020."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "5. CONCLUSION",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[9] Mingke Xu, Fan Zhang, Xiaodong Cui, and Wei Zhang,"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "“Speech emotion recognition with multiscale area atten-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "In this paper, we propose\na novel\narchitecture,\nnamed as",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "tion and data augmentation,”\nin 2021 IEEE Interna-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "GLAM network, which combines a multi-scale module and",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "tional Conference on Acoustics, Speech and Signal Pro-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "a global-aware fusion module. The multi-scale module cap-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "cessing, 2021."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "tures different scaled features for various time-spanned pat-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "terns and the global-aware module enhances cross-scale com-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[10] Kun Han, Dong Yu, and Ivan Tashev,\n“Speech emo-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "munication and attends feature confusion at different scales.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "tion recognition using deep neural network and extreme"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "In addition, we use mixup method to enhance the general-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "learning machine,” in 2014 Annual Conference of the In-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "ization of our model.\nExperiment\nresults demonstrate that",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "ternational Speech Communication Association, 2014."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "our model achieves\nthe state-of-the-art performance on the",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[11] Mingyi Chen, Xuanji He, Jing Yang, and Han Zhang,"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "benchmark dataset IEMOCAP over 4-class emotion classiﬁ-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "“3-d convolutional recurrent neural networks with atten-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "cation.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "IEEE Sig-\ntion model for speech emotion recognition,”"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "nal Processing Letters, vol. 25, no. 10, pp. 1440–1444,"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "6. REFERENCES",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "2018."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "[1] Moataz El Ayadi, Mohamed S. Kamel, and Fakhri Kar-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[12] Xixin Wu, Songxiang Liu, Yuewen Cao, Xu Li,\nJian-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "ray,\n“Survey on speech emotion recognition: Features,",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "wei Yu, Dongyang Dai, Xi Ma, Shoukang Hu, Zhiyong"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "classiﬁcation schemes, and databases,” Pattern Recog-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "Wu, Xunying Liu, and Helen Meng,\n“Speech emotion"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "nition, vol. 44, no. 3, pp. 572–587, 2011.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "recognition using capsule networks,”\nin 2019 IEEE In-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "ternational Conference on Acoustics, Speech and Signal"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "[2] Aharon Satt, Shai Rozenberg, and Ron Hoory, “Efﬁcient",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "Processing, 2019."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "emotion recognition from speech using deep learning on",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "spectrograms,” in 2017 Annual Conference of the Inter-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[13] Lorenzo Tarantino, Philip N. Garner,\nand Alexandros"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "national Speech Communication Association, 2017.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "Lazaridis,\n“Self-attention for speech emotion recogni-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "[3] Abdul Malik Badshah, Jamil Ahmad, Nasir Rahim, and",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "the International\ntion,”\nin 2019 Annual Conference of"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "Sung Wook Baik,\n“Speech emotion recognition from",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "Speech Communication Association, 2019."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "spectrograms with deep convolutional neural network,”",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[14] Hanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le,"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "in 2017 International Conference on Platform Technol-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "“Pay attention to mlps,”\nCoRR, vol. abs/2105.08050,"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "ogy and Service, 2017.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "2021."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "[4] Promod Yenigalla, Abhay Kumar, Suraj Tripathi, Chi-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[15] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "rag Singh, Sibsambhu Kar, and Jithendra Vepa, “Speech",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "David Lopez-Paz, “mixup: Beyond empirical risk min-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "emotion recognition using spectrogram & phoneme em-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "imization,” in 2018 International Conference on Learn-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "the Interna-\nbedding,”\nin 2018 Annual Conference of",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "ing Representations, 2018."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "tional Speech Communication Association, 2018.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[16] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "[5] Mustaqeem and Soonil Kwon,\n“A cnn-assisted en-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "hanced\naudio\nsignal\nprocessing\nfor\nspeech\nemotion",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "Chang,\nSungbok Lee,\nand\nShrikanth\nS Narayanan,"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "recognition,” Sensors, vol. 20, no. 1, 2020.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "“Iemocap:\nInteractive emotional dyadic motion capture"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "[6] Pengcheng\nLi,\nYan\nSong,\nIan Vince McLoughlin,",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "database,” Language resources and evaluation, vol. 42,"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "Wu Guo,\nand Li-Rong Dai,\n“An attention pooling",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "no. 4, pp. 335–359, 2008."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "based representation learning method for speech emo-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "tion recognition,” 2018 Annual Conference of the Inter-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[17] Moataz M. H. El Ayadi, Mohamed S. Kamel, and Fakhri"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "national Speech Communication Association, 2018.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "Karray,\n“Speech emotion recognition using gaussian"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "mixture vector autoregressive models,”\nin 2007 IEEE"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "[7] Yuanyuan Zhang, Jun Du, Zirui Wang, Jianshu Zhang,",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "International Conference on Acoustics, Speech and Sig-"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "and Yanhui Tu,\n“Attention based fully convolutional",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "nal Processing, 2007."
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "network for speech emotion recognition,” in 2018 Asia-",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": ""
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "Paciﬁc Signal and Information Processing Association",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "[18] Weihui Dai, Dongmei Han, Yonghui Dai, and Dongrong"
        },
        {
          "realized by using mixup method with the optimal\nresult\nfor": "Annual Summit and Conference, 2018.",
          "[8] Mingke Xu, Fan Zhang, and Samee U. Khan, “Improve": "Xu,\n“Emotion recognition and affective computing on"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A.\nSUPPLEMENTARY": "We stress\nthat\nthere is no uniﬁed and regular way to split"
        },
        {
          "A.\nSUPPLEMENTARY": "dataset. And it\nis also unstable to for training if we set dif-"
        },
        {
          "A.\nSUPPLEMENTARY": "ferent random seeds. Furthermore, the high accuracy may be"
        },
        {
          "A.\nSUPPLEMENTARY": "obtained by choosing one optimal partition way of dataset. To"
        },
        {
          "A.\nSUPPLEMENTARY": "ensure the reliability and strictness of experiment results, we"
        },
        {
          "A.\nSUPPLEMENTARY": "randomly split dataset 100 times for easily reproducing this"
        },
        {
          "A.\nSUPPLEMENTARY": "work. We sincerely apologize for leaving out\nthe validation"
        },
        {
          "A.\nSUPPLEMENTARY": "set.\nIndeed, the validation set is truly necessary. Since recent"
        },
        {
          "A.\nSUPPLEMENTARY": "works [8, 9] omitted it due to scarcity of dataset, we merely"
        },
        {
          "A.\nSUPPLEMENTARY": "follow the split strategy for better comparison. According to"
        },
        {
          "A.\nSUPPLEMENTARY": "reviewers’ comments, also for the sake of strictness, we ran-"
        },
        {
          "A.\nSUPPLEMENTARY": "domly divide the dataset into training, validation, and test sets"
        },
        {
          "A.\nSUPPLEMENTARY": "at a ratio of 8:1:1 for comparison. During training, we save"
        },
        {
          "A.\nSUPPLEMENTARY": "the optimal result according to the best mean value of UA and"
        },
        {
          "A.\nSUPPLEMENTARY": "WA metrics on the validation set.\nResults are listed in the"
        },
        {
          "A.\nSUPPLEMENTARY": "Table. 4. We see our model also achieve the state-of-the-art"
        },
        {
          "A.\nSUPPLEMENTARY": "performance with a robust stability."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "Model"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "APCNN"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "MHCNN"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "AACNN"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "GLAM"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "APCNN"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "MHCNN"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "AACNN"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "GLAM"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "APCNN"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "MHCNN"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "AACNN"
        },
        {
          "Table 4. Comparison of evaluation metrics on three types of datasets with a validation set.": "GLAM"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "Moataz Ayadi",
        "Mohamed Kamel",
        "Fakhri Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "2017 Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "Abdul Malik Badshah",
        "Jamil Ahmad",
        "Nasir Rahim",
        "Sung Baik"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Platform Technology and Service"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "Promod Yenigalla",
        "Abhay Kumar",
        "Suraj Tripathi",
        "Chirag Singh",
        "Sibsambhu Kar",
        "Jithendra Vepa"
      ],
      "year": "2018",
      "venue": "2018 Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "6",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "Soonil Mustaqeem",
        "Kwon"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "7",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Vince Mcloughlin",
        "Wu Guo",
        "Li-Rong Dai"
      ],
      "year": "2018",
      "venue": "2018 Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "8",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Yuanyuan Zhang",
        "Jun Du",
        "Zirui Wang",
        "Jianshu Zhang",
        "Yanhui Tu"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "9",
      "title": "Improve accuracy of speech emotion recognition with attention head fusion",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Samee Khan"
      ],
      "year": "2020",
      "venue": "2020 Annual Computing and Communication Workshop and Conference"
    },
    {
      "citation_id": "10",
      "title": "Speech emotion recognition with multiscale area attention and data augmentation",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Xiaodong Cui",
        "Wei Zhang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "2014 Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "12",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "Xixin Wu",
        "Songxiang Liu",
        "Yuewen Cao",
        "Xu Li",
        "Jianwei Yu",
        "Dongyang Dai",
        "Xi Ma",
        "Shoukang Hu",
        "Zhiyong Wu",
        "Xunying Liu",
        "Helen Meng"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Self-attention for speech emotion recognition",
      "authors": [
        "Lorenzo Tarantino",
        "Philip Garner",
        "Alexandros Lazaridis"
      ],
      "year": "2019",
      "venue": "2019 Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "15",
      "title": "Pay attention to mlps",
      "authors": [
        "Hanxiao Liu",
        "Zihang Dai",
        "David So",
        "Quoc Le"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "16",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang",
        "Moustapha Cisse",
        "Yann Dauphin",
        "David Lopez-Paz"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Learning Representations"
    },
    {
      "citation_id": "17",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "18",
      "title": "Speech emotion recognition using gaussian mixture vector autoregressive models",
      "authors": [
        "M Moataz",
        "Mohamed Ayadi",
        "Fakhri Kamel",
        "Karray"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition and affective computing on vocal social media",
      "authors": [
        "Weihui Dai",
        "Dongmei Han",
        "Yonghui Dai",
        "Dongrong Xu"
      ],
      "year": "2015",
      "venue": "Information & Management"
    },
    {
      "citation_id": "20",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    }
  ]
}