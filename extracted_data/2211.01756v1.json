{
  "paper_id": "2211.01756v1",
  "title": "Speech-Based Emotion Recognition With Self-Supervised Models Using Attentive Channel-Wise Correlations And Label Smoothing",
  "published": "2022-11-03T12:37:59Z",
  "authors": [
    "Sofoklis Kakouros",
    "Themos Stafylakis",
    "Ladislav Mosner",
    "Lukas Burget"
  ],
  "keywords": [
    "emotion recognition",
    "self-supervised features",
    "iemocap",
    "hubert",
    "wavlm",
    "wav2vec 2.0"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "When recognizing emotions from speech, we encounter two common problems: how to optimally capture emotionrelevant information from the speech signal and how to best quantify or categorize the noisy subjective emotion labels. Self-supervised pre-trained representations can robustly capture information from speech enabling state-of-the-art results in many downstream tasks including emotion recognition. However, better ways of aggregating the information across time need to be considered as the relevant emotion information is likely to appear piecewise and not uniformly across the signal. For the labels, we need to take into account that there is a substantial degree of noise that comes from the subjective human annotations. In this paper, we propose a novel approach to attentive pooling based on correlations between the representations' coefficients combined with label smoothing, a method aiming to reduce the confidence of the classifier on the training labels. We evaluate our proposed approach on the benchmark dataset IEMOCAP, and demonstrate high performance surpassing that in the literature. The code to reproduce the results is available at github.com/ skakouros/s3prl_attentive_correlation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotional expressions are a fundamental component of spoken interaction. When we communicate with other people, we are implicitly monitoring their emotional state and respond based on that emotional state  [1] . Emotions fall in the realm of prosodic function. In recent years, the importance of prosodic qualities in speech has attracted increasing attention. This is also the case for speech emotion recognition (SER) which has seen a growing interest with the increasing role of spoken language interfaces in human-computer interaction (HCI) applications  [2] . However, recognizing emotions in speech remains a challenging problem complicated by numerous factors including fundamental issues of how emotion is defined, elicited, expressed, and communicated  [3]  and extending to how we can capture this information from speech.\n\nThe challenges in SER can be split into three distinct problems. First, we encounter the issue of developing engineered representations that can robustly capture the acoustic information in speech that best describe the variation found across different emotions. This has been traditionally done using features such as mel-frequency cepstral coefficients (MFCCs), filterbanks, fundamental frequency, energy, zero-crossing rate, chroma-based features and their feature functionals  [4]  or through standardized feature collections and their functionals such as eGeMAPS  [5] . More recently, self-supervised learning (SSL) has shown its effectiveness in various domains, including SER, and is becoming the new principle for extracting representations from speech. HuBERT  [6] , Wav2vec 2.0  [7] , WavLM  [8] , are some of the self-supervised approaches for speech representation learning that have been used in the context of SER  [9, 5] .\n\nThe second issue that we face in SER is the effective modeling of the long temporal context over which emotions take place. Emotion specific information lies beyond segmental productions and in longer time scales. These may include parts of an utterance but can also span across one or more utterances. To appropriate model long-term dependencies by capturing and connecting the relevant cues across time suitable methods are necessary. These vary from approaches that simply take the first and second order statistics of selfsupervised representations across time  [9]  to approaches that focus on complex sequence modeling tasks  [10] . For example Sarma et al.  [10]  used a TDNN architecture combined with LSTM and self-attention to model the long-term temporal context and to capture the emotionally relevant portions of speech. In a recent work, Liu et al.  [11]  used a cascaded attention network to locate the relevant emotion regions from the input features. Other approaches also use different types of recurrent neural networks (RNNs) to explain the long temporal contexts of emotions in speech  [12] .\n\nThe third and final issue comes from the observation that human emotional expressions are often unclear and ambiguous, leading to disagreement and confusion among human evaluators  [13, 14] . This confusion might be partly attributed to the multimodal nature of emotion expression. Facial expressions, hand gestures, and speech with its prosodic and linguistic content all work together in eliciting different emotions. Perhaps the absence of multimodality may be one source of confusion that leads to overlaps in the clusters of the different emotion classes. However, speech alone holds much relevant information in its prosodic content that can be used for robust SER. Different ways to tackle the problem of noisy labels and consequently the uncertainty in predicting emotions have been suggested in the literature. These typically include custom loss functions  [15, 11]  and modifications to the target hard labels  [16, 17] . For example Liu and Wang used a triplet loss to make anchor utterances more similar to all other positive utterances  [15]  while Tarantino et al. used regression targets instead of hard categorical targets by taking the proportion of the classes within the annotations  [17] .\n\nThis paper presents a framework for SER that uses pretrained speech models with a novel approach to attentive pooling based on channel-wise correlations on soft targets. We evaluate the framework with HuBERT [6], Wav2vec 2.0  [7] , and WavLM  [8]  upstream models. We use the SUPERB  [9]  evaluation setup throughout our experiments. The effectiveness of our proposed framework is evaluated on the interactive emotional dyadic motion capture (IEMOCAP) dataset  [18]  and shows state-of-the-art performance in SER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Relation To Prior Work",
      "text": "The idea of taking the correlations between different filter responses over the spatial extent of the feature maps to obtain a representation of the style of an input image was introduced by Gatys et al.  [19] . Their method was later adapted to speech where it has found applications in speech generation and voice conversion  [20] , pooling to obtain speaker embeddings  [21] , and sentence-level tasks such as speaker identification, speaker verification, and SER  [22] .\n\nIn this work, we extend the method for correlation pooling presented in  [22] . In  [22] , it was shown that channel-wise correlations provide an alternative way of extracting speaker and emotion information from self-supervised models, providing also improvements over the standard mean and meanstd pooling (std stands for standard deviation). In the present work, we add an attention mechanism to pool representations before estimating the correlation matrix, while reducing the confidence on the target labels with label smoothing.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "We construct our SER framework based on the pipeline and principles of SUPERB  [9] . As finetuning pretrained models has a high resource demand in terms of the computational power needed, we use a simple framework with a frozen pretrained model and lightweight classification heads. An overview of our framework is presented in Fig.  1 . In this section we describe details of the proposed approach.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Layer-Wise Pooling",
      "text": "We extract information relevant for our downstream task as in SUPERB (  [9] ), by taking representations from all transformer layers in the model and collapsing them to one via a weighted average (see Fig.  1 ). There is one weight for each layer (a total of L + 1) and all weights are trained jointly with the classification network. The weighted average representation is expressed as follows\n\nwhere the weights L l=0 γ l = 1, γ l ≥ 0 are implemented with a learnable vector of size L+1, followed by the Softmax function, and h t,l is the representation of the lth layer at time t (h t,0 is the output of the ConvNet).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Frame-Wise Pooling",
      "text": "Tasks requiring sentence-level classification typically employ a pooling method, such as mean, max or attentive pooling. Mean pooling, which is employed in SUPERB is defined as\n\nwhere T is the number of acoustic features of an utterance extracted by the ConvNet, r is the resulting pooled representation, while h t are the representations at time t after layerwise pooling. Concatenating the pooled representations with std features is in general helpful in speaker recognition  [23] ,\n\nand is implemented as\n\nwhere [•; •] denotes vector concatenation and the exponents should be considered as element-wise operators.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Correlation Pooling",
      "text": "Correlation pooling (introduced in  [21] ) is an alternative pooling method which has shown improvements in speaker recognition. The embedding dimension of SSL models (typically 768 or 1024) is too high to estimate correlations. We therefore project h onto a lower d v -dimensional space v via a linear layer (d v = 256). We then calculate the mean vector µ and the covariance matrix Σ of v as follows\n\nwhere x denotes the transpose of x. Finally, the correlation matrix is derived by normalizing with respect to the variances as follows\n\nwhere S = ss + 1, s = diag(Σ) 1/2 (i.e. the vector of std), denotes element-wise division, while = 10 -8 . Since C is a symmetric matrix and its diagonal elements are equal to 1, we vectorize the elements above the diagonal, yielding a (d v ×(d v -1)/2)-sized vector, which we project onto a linear layer followed by the Softmax over the emotion classes. For regularization, dropout is applied to v, where whole channels are dropped with probability p d = 0.25.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Attentive Correlation Pooling",
      "text": "We introduce here the attentive correlation pooling, by inserting weights in the estimates of the statistics, i.e.\n\nand we calculate C as in eq. (  5 ). The weights {w t } T t=1 (where t w t = 1 and w t ≥ 0) are estimated using a new flavor of attention. Similarly to the single-head attention a single set of weights is estimated. Similarly to the multi-head attention, multiple heads are employed, however their similarities with v t are aggregated prior to the Softmax function via the logsum-exp function, as follows\n\nwhere\n\nthe heads {q h , b h } H h=1 are trainable d v -dimensional vectors and biases, o t = ReLU(W att v t ) and W att is a square matrix (d v × d v ). Note that an equivalent implementation is to use as input to the Softmax the (H ×T )-sized vector of dot-products q h o t + b h and sum the outputs over heads to obtain {w t } T t=1 . As we observe, the proposed attention resembles a mixture model with heads parametrizing the mixture components. The log-sum-exp function is a soft version of the max operator, meaning that a t is high when at least one of the H headspecific dot-products {q h o t + b h } H h=1 is high. The rationale for proposing this kind of attention is twohold. We desire to keep the multi-modality of multi-head attention since a single head is too weak to capture the phonetic, speaker, emotion and channel variability. On the other hand, the standard multi-head attention results in H context-vectors (in our case H correlation matrices), which can be hard to estimate robustly, especially when the utterances are short and the estimation involves second-order statistics.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Label Smoothing",
      "text": "With label smoothing we soften the hard (one-hot) targets vectors y of the training set as follows\n\nwhere p l is the label smoothing parameter (i.e. the probability mass equally distributed to all classes) and y LS is the smoothed target vector  [24] . Cross-entropy is still employed as loss function, but with soft targets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "The IEMOCAP database consists of multi-modal recordings (speech, video) by 10 actors in dyadic sessions in English (≈ 12 hours)  [18] . The dataset is split in 5 dialogue sessions (one female-male speaker pair per session). The emotions conveyed are happiness, anger, excitement, sadness, surprise, fear, frustration, and neutral state. As in other studies on IEMOCAP, we relabel excitement as happiness and use 4 balanced emotion classes, namely: anger, happiness, sadness, and neutral  [9, 5, 25] . All other classes are discarded.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "We use a 5-fold cross-validation setup where at each fold we leave out one session from the dataset. Each held-out session consists of two speakers that are not present in the train and validation sets. This approach leaves approximately 19% of the data for testing. Mean and standard deviation across folds is computed and presented as the aggregated result. Our SER framework is evaluated with WavLM, Wav2vec 2.0, and Hu-BERT speech representations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "An overview of the results for the most common pooling methods and our proposed approach is shown in Table  1 .\n\nThe results are presented for the best configuration of our framework with p d and p l both equal to 0.25 and H = 4.\n\nThe best overall performance was achieved for our proposed approach using WavLM (75.60%; see also Fig.  2 ) which is higher compared to other SSL approaches in the literature on the same data -67.20%  [5] , 67.62%  [9] , 73.01% with fine-tuned model  [15] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Attention",
      "text": "To investigate the performance of the proposed attentive correlation pooling method we experimented with different numbers of attention heads (H). Specifically, we tested for H = 1, 4, 16, 32 heads. We obtained the best result with H = 4. Note that we did not observe any correlation between heads and emotion classes, meaning that there is no direct mapping between heads and emotions. In particular, for Session 1, Hu-BERT, p d = 0.25, p l = 0.25 accuracy was 70.32% (H = 1), 73.18% (H = 4), 72.53% (H = 16), and 71.34% (H = 32).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Label Smoothing And Dropout",
      "text": "To better understand the performance of our system, we probed our setup by varying the parameters for label smoothing and dropout rate. For a set number of attention heads (H = 4), label smoothing was varied for p l = 0, 0.   and dropout rate for p d = 0...0.5. The effect was evaluated on Session 1 of the setup for attentive correlation pooling using HuBERT and the results can be seen in Fig.  3 . Both dropout and label smoothing have an impact on the performance with label smoothing having a greater positive effect; increasing performance from 67.93% (p d = 0, p l = 0) to 70, 41% (p d = 0, p l = 0.25) and even further with increasing dropout to 73.18% (p d = 0.25, p l = 0.25).\n\nThe impact of dropout rate and label smoothing was also investigated for mean, mean-std, and correlation pooling. The impact in all was small to negligible. For example, for p d = 0, p l = 0.25 the performance remained unchanged compared to p d = 0, p l = 0 for mean, mean-std, and correlation pooling while for p d = 0.25, p l = 0.25 there was a small improvement for correlation pooling (from 68.20% to 70.60%).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this work we presented an SER framework that uses selfsupervised representations and is based on label smoothing and a novel approach to attention, attentive correlation pooling. Notably, our method does not require fine-tuning of the pre-trained SSL models but rather uses a light-weight classification head that attempts to capture all relevant emotion information from the pre-trained representations. We run several experiments using a 5-fold cross-validation setup and we have clearly demonstrated that our method reaches high performance in all pre-trained models tested surpassing that of the literature in similar tasks. In future work, we will extend the evaluation setup and validate the performance of our method on more datasets.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed architecture.",
      "page": 2
    },
    {
      "caption": "Figure 1: ). There is one weight for each layer (a",
      "page": 2
    },
    {
      "caption": "Figure 2: ) which is",
      "page": 4
    },
    {
      "caption": "Figure 2: Confusion matrix for WavLM using attentive correla-",
      "page": 4
    },
    {
      "caption": "Figure 3: Dropout-label smoothing interaction for Session 1 -",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "3Brno University of Technology, Faculty of Information Technology, Speech@FIT, Czechia"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "and extending to how we can capture this information from"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "speech."
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "The\nchallenges\nin SER can be\nsplit\ninto three distinct"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "problems. First, we encounter the issue of developing engi-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "neered representations\nthat can robustly capture the acous-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "tic\ninformation\nin\nspeech\nthat\nbest\ndescribe\nthe\nvariation"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "found across different emotions. This has been traditionally"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "done using features\nsuch as mel-frequency cepstral coefﬁ-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "cients (MFCCs), ﬁlterbanks, fundamental frequency, energy,"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "zero-crossing rate,\nchroma-based features and their\nfeature"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "functionals\n[4] or\nthrough standardized feature\ncollections"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "and their functionals such as eGeMAPS [5]. More recently,"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "self-supervised learning (SSL) has\nshown its\neffectiveness"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "in various domains,\nincluding SER,\nand is becoming the"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "new principle\nfor\nextracting\nrepresentations\nfrom speech."
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "HuBERT [6], Wav2vec 2.0 [7], WavLM [8], are some of the"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "self-supervised approaches for speech representation learning"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "that have been used in the context of SER [9, 5]."
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "The second issue that we face in SER is\nthe effective"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "modeling of the long temporal context over which emotions"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "take place.\nEmotion speciﬁc information lies beyond seg-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "mental productions and in longer time scales. These may in-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "clude parts of an utterance but can also span across one or"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "more utterances. To appropriate model\nlong-term dependen-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "cies by capturing and connecting the relevant cues across time"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "suitable methods are necessary. These vary from approaches"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "that simply take the ﬁrst and second order statistics of self-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "supervised representations across time [9] to approaches that"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "focus on complex sequence modeling tasks [10]. For exam-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "ple Sarma et al.\n[10] used a TDNN architecture combined"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "with LSTM and self-attention to model the long-term tempo-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "ral context and to capture the emotionally relevant portions"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "of speech.\nIn a recent work, Liu et al.\n[11] used a cascaded"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "attention network to locate the relevant emotion regions from"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "the input features. Other approaches also use different\ntypes"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "of recurrent neural networks (RNNs) to explain the long tem-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "poral contexts of emotions in speech [12]."
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": ""
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "The third and ﬁnal issue comes from the observation that"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "human emotional expressions are often unclear and ambigu-"
        },
        {
          "2Omilia - Conversational Intelligence, Athens, Greece": "ous,\nleading to disagreement and confusion among human"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "h=1 are trainable dv-dimensional vectors"
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "and biases, ot = ReLU(Wattvt) and Watt is a square matrix"
        },
        {
          "and is implemented as": "(cid:32)\n(cid:33)1/2",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": " ",
          "the heads {qh, bh}H": "(dv × dv). Note that an equivalent implementation is to use as"
        },
        {
          "and is implemented as": "1 T\nT(cid:88) t\nr =\nh;\n(3)\n(ht − ¯h)2\n ,",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "input to the Softmax the (H ×T )-sized vector of dot-products"
        },
        {
          "and is implemented as": "=1",
          "the heads {qh, bh}H": "q(cid:48)"
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "t=1.\nhot + bh and sum the outputs over heads to obtain {wt}T"
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "As we observe,\nthe proposed attention resembles a mix-"
        },
        {
          "and is implemented as": "where [·; ·] denotes vector concatenation and the exponents",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "ture model with heads parametrizing the mixture components."
        },
        {
          "and is implemented as": "should be considered as element-wise operators.",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "The log-sum-exp function is a soft version of the max opera-"
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "is high when at least one of the H head-\ntor, meaning that at"
        },
        {
          "and is implemented as": "3.3. Correlation pooling",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "hot + bh}H\nh=1 is high."
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "The rationale for proposing this kind of attention is two-"
        },
        {
          "and is implemented as": "Correlation pooling (introduced in [21]) is an alternative pool-",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "hold. We desire to keep the multi-modality of multi-head at-"
        },
        {
          "and is implemented as": "ing method which has shown improvements in speaker recog-",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "tention since a single head is too weak to capture the phonetic,"
        },
        {
          "and is implemented as": "nition. The embedding dimension of SSL models (typically",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "speaker, emotion and channel variability. On the other hand,"
        },
        {
          "and is implemented as": "768 or 1024) is too high to estimate correlations. We there-",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "the standard multi-head attention results in H context-vectors"
        },
        {
          "and is implemented as": "fore project h onto a lower dv-dimensional space v via a lin-",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "(in our case H correlation matrices), which can be hard to es-"
        },
        {
          "and is implemented as": "ear layer (dv = 256). We then calculate the mean vector µ",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "timate robustly, especially when the utterances are short and"
        },
        {
          "and is implemented as": "and the covariance matrix Σ of v as follows",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "the estimation involves second-order statistics."
        },
        {
          "and is implemented as": "1 T\nT(cid:88) t\n1 T\nT(cid:88) t\nµ =\n(4)\nvt, Σ =\n(vt − µ)(vt − µ)(cid:48),",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "=1\n=1",
          "the heads {qh, bh}H": "3.5. Label smoothing"
        },
        {
          "and is implemented as": "where x(cid:48) denotes the transpose of x. Finally,\nthe correlation",
          "the heads {qh, bh}H": "With label\nsmoothing we soften the hard (one-hot)\ntargets"
        },
        {
          "and is implemented as": "matrix is derived by normalizing with respect to the variances",
          "the heads {qh, bh}H": "vectors y of the training set as follows"
        },
        {
          "and is implemented as": "as follows",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "(9)\nyLS = y(1 − pl) + pl/K,"
        },
        {
          "and is implemented as": "C = Σ (cid:11) S,\n(5)",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "where S = ss(cid:48) + (cid:15)1, s = diag(Σ)1/2 (i.e.\nthe vector of std),",
          "the heads {qh, bh}H": ""
        },
        {
          "and is implemented as": "",
          "the heads {qh, bh}H": "is the label smoothing parameter\n(i.e.\nthe proba-\nwhere pl"
        },
        {
          "and is implemented as": "(cid:11) denotes element-wise division, while (cid:15) = 10−8. Since C",
          "the heads {qh, bh}H": "bility mass equally distributed to all classes) and yLS is the"
        },
        {
          "and is implemented as": "is a symmetric matrix and its diagonal elements are equal\nto",
          "the heads {qh, bh}H": "smoothed target vector [24]. Cross-entropy is still employed"
        },
        {
          "and is implemented as": "1, we vectorize the elements above the diagonal, yielding a",
          "the heads {qh, bh}H": "as loss function, but with soft targets."
        },
        {
          "and is implemented as": "(dv ×(dv −1)/2)-sized vector, which we project onto a linear",
          "the heads {qh, bh}H": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: The results are presented for the best configuration of our",
      "data": [
        {
          "74%": "73%"
        },
        {
          "74%": "72%"
        },
        {
          "74%": ""
        },
        {
          "74%": "71%"
        },
        {
          "74%": "accuracy\n70%"
        },
        {
          "74%": "69%"
        },
        {
          "74%": ""
        },
        {
          "74%": "68%"
        },
        {
          "74%": "67%"
        },
        {
          "74%": "66%"
        },
        {
          "74%": "65%"
        },
        {
          "74%": ""
        },
        {
          "74%": ""
        },
        {
          "74%": ""
        },
        {
          "74%": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 1: The results are presented for the best configuration of our",
      "data": [
        {
          "65%": ""
        },
        {
          "65%": "0\n0.1\n0.2\n0.25\n0.3\n0.4\n0.5"
        },
        {
          "65%": "dropout rate"
        },
        {
          "65%": "0\n0.15\n0.2\n0.25\n0.3"
        },
        {
          "65%": "Fig. 3. Dropout-label smoothing interaction for Session 1 -"
        },
        {
          "65%": "HuBERT. Lines represent different label smoothing values."
        },
        {
          "65%": ""
        },
        {
          "65%": "and dropout rate for pd = 0...0.5. The effect was evaluated"
        },
        {
          "65%": ""
        },
        {
          "65%": "on Session 1 of\nthe setup for attentive correlation pooling"
        },
        {
          "65%": ""
        },
        {
          "65%": "using HuBERT and the results can be seen in Fig.\n3. Both"
        },
        {
          "65%": ""
        },
        {
          "65%": "dropout and label smoothing have an impact on the perfor-"
        },
        {
          "65%": ""
        },
        {
          "65%": "mance with label smoothing having a greater positive effect;"
        },
        {
          "65%": ""
        },
        {
          "65%": "to\nincreasing performance from 67.93% (pd = 0, pl = 0)"
        },
        {
          "65%": ""
        },
        {
          "65%": "70, 41% (pd = 0, pl = 0.25) and even further with increasing"
        },
        {
          "65%": ""
        },
        {
          "65%": "dropout to 73.18% (pd = 0.25, pl = 0.25)."
        },
        {
          "65%": ""
        },
        {
          "65%": "The impact of dropout rate and label smoothing was also"
        },
        {
          "65%": "investigated for mean, mean-std, and correlation pooling. The"
        },
        {
          "65%": ""
        },
        {
          "65%": "impact\nin all was small\nto negligible. For example, for pd ="
        },
        {
          "65%": "0, pl = 0.25 the performance remained unchanged compared"
        },
        {
          "65%": ""
        },
        {
          "65%": "to pd = 0, pl = 0 for mean, mean-std, and correlation pooling"
        },
        {
          "65%": ""
        },
        {
          "65%": "improve-\nwhile for pd = 0.25, pl = 0.25 there was a small"
        },
        {
          "65%": ""
        },
        {
          "65%": "ment for correlation pooling (from 68.20% to 70.60%)."
        },
        {
          "65%": ""
        },
        {
          "65%": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "ambiguous emotional expressions,”\nin 2009 3rd Inter-"
        },
        {
          "8. REFERENCES": "[1] Scott Brave\nand Cliff Nass,\n“Emotion\nin\nhuman-",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "national Conference on Affective Computing and Intel-"
        },
        {
          "8. REFERENCES": "computer\ninteraction,”\nin The human-computer inter-",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "ligent Interaction and Workshops. IEEE, 2009, pp. 1–8."
        },
        {
          "8. REFERENCES": "action handbook, pp. 103–118. CRC Press, 2007.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[15]\nJiawang Liu and Haoxiang Wang,\n“A speech emotion"
        },
        {
          "8. REFERENCES": "[2] Chul Min Lee and Shrikanth S Narayanan, “Toward de-",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "recognition framework for better discrimination of con-"
        },
        {
          "8. REFERENCES": "tecting emotions in spoken dialogs,” IEEE transactions",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "fusions.,” in Interspeech, 2021, pp. 4483–4487."
        },
        {
          "8. REFERENCES": "on speech and audio processing, vol. 13, no. 2, pp. 293–",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[16] Haytham M Fayek, Margaret Lech, and Lawrence Cave-"
        },
        {
          "8. REFERENCES": "303, 2005.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "don,\n“Modeling subjectiveness in emotion recognition"
        },
        {
          "8. REFERENCES": "[3] Rosalind W Picard,\nAffective computing, MIT press,",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "with deep neural networks: Ensembles vs soft labels,” in"
        },
        {
          "8. REFERENCES": "2000.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "2016 international\njoint conference on neural networks"
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "(IJCNN). IEEE, 2016, pp. 566–570."
        },
        {
          "8. REFERENCES": "[4] Dimitrios\nVerveridis\nand\nConstantine\nKotropoulos,",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "“Emotional\nspeech recognition:\nResources,\nfeatures,",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[17] Lorenzo Tarantino,\nPhilip N Garner,\net\nal.,\n“Self-"
        },
        {
          "8. REFERENCES": "and methods,”\nSpeech communication, vol. 48, no. 9,",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "attention for\nspeech emotion recognition.,”\nin Inter-"
        },
        {
          "8. REFERENCES": "pp. 1162–1181, 2006.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "speech, 2019, pp. 2578–2582."
        },
        {
          "8. REFERENCES": "[5] Leonardo Pepino,\nPablo Riera,\nand Luciana Ferrer,",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[18] Carlos Busso, Murtaza Bulut, et al., “IEMOCAP: Inter-"
        },
        {
          "8. REFERENCES": "“Emotion recognition from speech using wav2vec 2.0",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "active emotional dyadic motion capture database,” Lan-"
        },
        {
          "8. REFERENCES": "embeddings,” arXiv preprint arXiv:2104.03502, 2021.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "guage resources and evaluation, vol. 42, no. 4, pp. 335–"
        },
        {
          "8. REFERENCES": "[6] Wei-Ning Hsu, Benjamin Bolte, et al., “HuBERT: Self-",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "359, 2008."
        },
        {
          "8. REFERENCES": "supervised speech representation learning by masked",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[19] Leon A Gatys, Alexander S Ecker, and Matthias Bethge,"
        },
        {
          "8. REFERENCES": "prediction of hidden units,” IEEE/ACM Transactions on",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "arXiv preprint\n“A neural algorithm of artistic style,”"
        },
        {
          "8. REFERENCES": "Audio, Speech, and Language Processing, vol. 29, pp.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "arXiv:1508.06576, 2015."
        },
        {
          "8. REFERENCES": "3451–3460, 2021.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[20]\nJan Chorowski, Ron J Weiss, et al., “On using backprop-"
        },
        {
          "8. REFERENCES": "[7] Alexei Baevski, Yuhao Zhou, et al.,\n“wav2vec 2.0: A",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "agation for speech texture generation and voice conver-"
        },
        {
          "8. REFERENCES": "framework for self-supervised learning of speech repre-",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "sion,” in 2018 IEEE International Conference on Acous-"
        },
        {
          "8. REFERENCES": "sentations,” Advances in Neural Information Processing",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "tics,\nSpeech and Signal Processing (ICASSP).\nIEEE,"
        },
        {
          "8. REFERENCES": "Systems, vol. 33, pp. 12449–12460, 2020.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "2018, pp. 2256–2260."
        },
        {
          "8. REFERENCES": "[8] Sanyuan Chen, Chengyi Wang, et al., “WavLM: Large-",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[21] Themos Stafylakis,\nJohan Rohdin, and Lukas Burget,"
        },
        {
          "8. REFERENCES": "scale self-supervised pre-training for\nfull stack speech",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "“Speaker embeddings by modeling channel-wise corre-"
        },
        {
          "8. REFERENCES": "processing,” arXiv preprint arXiv:2110.13900, 2021.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "lations,” in Interspeech, 2021."
        },
        {
          "8. REFERENCES": "[9] Shu-wen Yang, Po-Han Chi, et al.,\n“SUPERB: Speech",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[22] Themos Stafylakis, Ladislav Moˇsner, et al., “Extracting"
        },
        {
          "8. REFERENCES": "processing universal performance benchmark,”\nin Pro-",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "speaker and emotion information from self-supervised"
        },
        {
          "8. REFERENCES": "ceedings of Interspeech, 2021.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "speech models via channel-wise correlations,”\nin 2023"
        },
        {
          "8. REFERENCES": "[10] Mousmita Sarma, Pegah Ghahremani, et al.,\n“Emotion",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "IEEE Workshop on Spoken Language Technology (SLT),"
        },
        {
          "8. REFERENCES": "identiﬁcation from raw speech signals using dnns.,”\nin",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "to appear, pp. 1–8."
        },
        {
          "8. REFERENCES": "Interspeech, 2018, pp. 3097–3101.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[23] Shuai Wang, Yexin Yang, et al., “Revisiting the statistics"
        },
        {
          "8. REFERENCES": "[11] Yang Liu, Haoqin Sun, et al.,\n“Discriminative feature",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "pooling layer in deep speaker embedding learning,”\nin"
        },
        {
          "8. REFERENCES": "representation based on cascaded attention network with",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "2021 12th International Symposium on Chinese Spoken"
        },
        {
          "8. REFERENCES": "adversarial\njoint\nloss for speech emotion recognition,”",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "Language Processing (ISCSLP), 2021, pp. 1–5."
        },
        {
          "8. REFERENCES": "Proc. Interspeech 2022, pp. 4750–4754, 2022.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[24] Christian Szegedy, Vincent Vanhoucke, et al., “Rethink-"
        },
        {
          "8. REFERENCES": "[12]\nJinkyu Lee and Ivan Tashev,\n“High-level\nfeature rep-",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "ing the inception architecture for computer vision,”\nin"
        },
        {
          "8. REFERENCES": "resentation using recurrent neural network for\nspeech",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "Proceedings of the IEEE conference on computer vision"
        },
        {
          "8. REFERENCES": "emotion recognition,” in Interspeech 2015, 2015.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "and pattern recognition, 2016, pp. 2818–2826."
        },
        {
          "8. REFERENCES": "[13] Yelin Kim and Emily Mower Provost,\n“Leveraging",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "[25] Haytham M Fayek, Margaret Lech, and Lawrence Cave-"
        },
        {
          "8. REFERENCES": "inter-rater agreement for audio-visual emotion recogni-",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "don, “Evaluating deep learning architectures for speech"
        },
        {
          "8. REFERENCES": "tion,”\nin 2015 International Conference on Affective",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "emotion recognition,” Neural Networks, vol. 92, pp. 60–"
        },
        {
          "8. REFERENCES": "Computing\nand\nIntelligent\nInteraction\n(ACII).\nIEEE,",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        },
        {
          "8. REFERENCES": "",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": "68, 2017."
        },
        {
          "8. REFERENCES": "2015, pp. 553–559.",
          "[14] Emily Mower, Angeliki Metallinou, et al., “Interpreting": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Emotion in humancomputer interaction",
      "authors": [
        "Scott Brave",
        "Cliff Nass"
      ],
      "year": "2007",
      "venue": "The human-computer interaction handbook"
    },
    {
      "citation_id": "3",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "Min Chul",
        "Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "4",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "5",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "Dimitrios Ververidis",
        "Constantine Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech communication"
    },
    {
      "citation_id": "6",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "Leonardo Pepino",
        "Pablo Riera",
        "Luciana Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "arxiv": "arXiv:2104.03502"
    },
    {
      "citation_id": "7",
      "title": "HuBERT: Selfsupervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "9",
      "title": "WavLM: Largescale self-supervised pre-training for full stack speech processing",
      "authors": [
        "Sanyuan Chen",
        "Chengyi Wang"
      ],
      "year": "2021",
      "venue": "WavLM: Largescale self-supervised pre-training for full stack speech processing",
      "arxiv": "arXiv:2110.13900"
    },
    {
      "citation_id": "10",
      "title": "SUPERB: Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi"
      ],
      "year": "2021",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "11",
      "title": "Emotion identification from raw speech signals using dnns",
      "authors": [
        "Mousmita Sarma",
        "Pegah Ghahremani"
      ],
      "year": "2018",
      "venue": "Emotion identification from raw speech signals using dnns"
    },
    {
      "citation_id": "12",
      "title": "Discriminative feature representation based on cascaded attention network with adversarial joint loss for speech emotion recognition",
      "authors": [
        "Yang Liu",
        "Haoqin Sun"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech 2022"
    },
    {
      "citation_id": "13",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "Jinkyu Lee",
        "Ivan Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "14",
      "title": "Leveraging inter-rater agreement for audio-visual emotion recognition",
      "authors": [
        "Yelin Kim",
        "Emily Provost"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "15",
      "title": "Interpreting ambiguous emotional expressions",
      "authors": [
        "Emily Mower",
        "Angeliki Metallinou"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "16",
      "title": "A speech emotion recognition framework for better discrimination of confusions",
      "authors": [
        "Jiawang Liu",
        "Haoxiang Wang"
      ],
      "year": "2021",
      "venue": "A speech emotion recognition framework for better discrimination of confusions"
    },
    {
      "citation_id": "17",
      "title": "Modeling subjectiveness in emotion recognition with deep neural networks: Ensembles vs soft labels",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2016",
      "venue": "2016 international joint conference on neural networks (IJCNN)"
    },
    {
      "citation_id": "18",
      "title": "Selfattention for speech emotion recognition",
      "authors": [
        "Lorenzo Tarantino",
        "Philip Garner"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "19",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "20",
      "title": "A neural algorithm of artistic style",
      "authors": [
        "Leon Gatys",
        "Alexander Ecker",
        "Matthias Bethge"
      ],
      "year": "2015",
      "venue": "A neural algorithm of artistic style",
      "arxiv": "arXiv:1508.06576"
    },
    {
      "citation_id": "21",
      "title": "On using backpropagation for speech texture generation and voice conversion",
      "authors": [
        "Jan Chorowski",
        "Ron Weiss"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Speaker embeddings by modeling channel-wise correlations",
      "authors": [
        "Themos Stafylakis",
        "Johan Rohdin",
        "Lukas Burget"
      ],
      "year": "2021",
      "venue": "Speaker embeddings by modeling channel-wise correlations"
    },
    {
      "citation_id": "23",
      "title": "Extracting speaker and emotion information from self-supervised speech models via channel-wise correlations",
      "authors": [
        "Themos Stafylakis",
        "Ladislav Mošner"
      ],
      "venue": "2023 IEEE Workshop on Spoken Language Technology (SLT)"
    },
    {
      "citation_id": "24",
      "title": "Revisiting the statistics pooling layer in deep speaker embedding learning",
      "authors": [
        "Shuai Wang",
        "Yexin Yang"
      ],
      "year": "2021",
      "venue": "2021 12th International Symposium on Chinese Spoken Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    }
  ]
}