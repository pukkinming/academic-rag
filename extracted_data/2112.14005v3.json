{
  "paper_id": "2112.14005v3",
  "title": "Towards Relatable Explainable Ai With The Perceptual Process",
  "published": "2021-12-28T05:48:53Z",
  "authors": [
    "Wencan Zhang",
    "Brian Y. Lim"
  ],
  "keywords": [
    "Explainable AI",
    "contrastive explanations",
    "audio",
    "vocal emotion"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Machine learning models need to provide contrastive explanations, since people often seek to understand why a puzzling prediction occurred instead of some expected outcome. Current contrastive explanations are rudimentary comparisons between examples or raw features, which remain difficult to interpret, since they lack semantic meaning. We argue that explanations must be more relatable to other concepts, hypotheticals, and associations. Inspired by the perceptual process from cognitive psychology, we propose the XAI Perceptual Processing Framework and RexNet model for relatable explainable AI with Contrastive Saliency, Counterfactual Synthetic, and Contrastive Cues explanations. We investigated the application of vocal emotion recognition, and implemented a modular multi-task deep neural network to predict and explain emotions from speech. From think-aloud and controlled studies, we found that counterfactual explanations were useful and further enhanced with semantic cues, but not saliency explanations. This work provides insights into providing and evaluating relatable contrastive explainable AI for perception applications.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "We introduce various explainable AI techniques, argue how they lack human-centeredness, and describe the background on speech emotion recognition and highlight their lack of explainability.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Explainable Ai Techniques",
      "text": "Much research has been done to develop explainable AI (XAI) for improving model transparency and trustworthiness. An intuitive approach is to point out which features are most important. Attribution explanations do this by identifying importance using gradients  [99] , ablation  [92] , activations  [97] , or decompositions  [8, 75, 90] . In computer vision, attributions take the form of saliency maps (e.g.,  [97] ). Explaining by referring to key examples is another popular approach. This includes simply providing arbitrary samples of specific classes, cluster prototypes or criticisms  [46] , or influential training set instances  [49] . However, users typically have expectations and goals when asking for explanations.\n\nUsers ask for contrastive explanations when expected outcomes do not happen. A simple answer would find the attribution differences between the actual (fact) and expected (foil) outcomes  [84]  However, this is naive because users are truly asking for what differences in feature values, not attributions, would lead to the alternative outcome. That is a counterfactual explanation. Furthermore, to anticipate a future outcome or prevent an undesirable one, users could ask for counterfactual explanations. Indeed, contrastive explanations are often conflated with counterfactual explanations in the research literature. Such explanations suggest the minimum changes in the current case to achieve the desired outcome  [103] . Trained decision structures, such as local foil trees  [102] , Bayesian rule lists  [55] , or structural causal models  [73]  can also serve as counterfactual explanations. Though typically explained in terms of feature values  [20, 54, 103]  or anchor rules  [91] , techniques have been developed to synthesize counterfactuals of unstructured data (e.g., images  [29]  and text  [33] ). In this work, we employ the synthesis approach to generate counterfactuals of audio data.\n\nThere are many explanation types and Lim and Dey have framed them in an intelligibility taxonomy as Why (Attribution), Why Not (Contrastive), and How To (Counterfactual)  [59] . Many of these XAI techniques have been independently developed or tested, so their usage is disparate. In this work, we unify them in a common framework and integrate them in a single machine learning model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Human-Centered Explainable Ai",
      "text": "Abdul et al.  [1]  found a large gap between XAI algorithms and human-centered research. To close this gap, HCI researchers have been active in evaluating the various benefits of XAI or lack thereof, including understanding and trust  [64] , uncertainty  [62, 105, 110] , cognitive load  [2] , types of examples  [11] , etc. Studies have sought to determine the \"best\" explanation type  [64, 100] , but others have revealed the benefit of reasoning with multiple explanations  [6, 61, 63] . Hence, we propose a unified framework to provide multiple relatable explanations together. We determined our human-centered explanation requirements by studying literature on human cognition, which is epistemologically similar to works grounded in philosophy and psychology  [72, 104] , and unlike empirical approaches to elicit user requirements  [22, 58, 59] . Furthermore, current works focus on explaining higher-level reasoning tasks, but not perception tasks that are commonplace. This has implications on the depth of explanations to provide, which we investigate in this work.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Speech Emotion Recognition",
      "text": "Deep learning approaches proliferate research on automatic speech emotion recognition (SER). Leveraging the intrinsic time-series structure of speech data, recurrent neural network (RNN) models with attention mechanism have been developed to capture transient acoustic features to understand contextual information  [74] . Employing popular techniques from the computer vision domain, audio data can be treated as 1D arrays or converted to a spectrogram as a 2D image. Convolutional neural networks (CNNs) can then extract spatial features from these audiograms or spectrograms  [37] . Current approaches improve performance by combining CNN and RNN  [101, 114] , or modeling with multiple modalities  [111] . Our RexNet model starts with a base CNN model to leverage many more XAI techniques available to CNNs than RNNs. Since our approach is modular, it can be generalized to state-of-the-art SER models.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Explanations Of Audio Predictions",
      "text": "Due to the availability of image data and intuitiveness of vision, much XAI research has focused on image prediction tasks; in contrast, few techniques have been developed for audio prediction tasks. Many techniques exploit CNN explanations by generating a saliency map on the audio spectrogram  [4, 52] . Other explanations focus on model debugging by visualizing neuron activations  [51] , or as feature visualizing  [56]  (like  [78]  for image kernels). We also leverage saliency maps as one explanation, due to its intuitive pointing, but augment it with relatable explanations. Other than explaining the model behavior post-hoc, another approach is to make the model more interpretable and trustworthy by constraining the trained model with domain knowledge, such as with voice-specific parametric convolutional filters  [67, 88] . Our approach with modular explanations of specific types follows a similar objective. For visual clarity, we present the use case for visually recognizing a cat instead of a dog, although we use vocal emotion recognition for our prediction task and evaluation. Image credits: \"dog face\" and \"cat face\" by \"irfan al haq\", \"Dog\" by Maxim Kulikov, \"cat mouth\" by needumee from the Noun Project.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Intuition And Background",
      "text": "To improve trust, models should provide explanations that are relatable and human-like. Thus, we propose to use theories of human perception and cognition to define our explainable AI techniques. We discuss how the framework supports relatable explanations, and apply it to vocal emotion recognition. Next, we describe background theories from cognitive psychology and vocal emotion prosody, and define requirements for relatable explanations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Perceptual Processing",
      "text": "The perceptual process defines three stages for how humans perceive and understand stimuli: selection, organization, and interpretation  [13] . Fig.  1  illustrates these stages for the case of visually perceiving a cat and relates them to our technical approach. When sensory stimuli (e.g., light rays or audio vibrations) reach the senses, 1) our brain first selects only a subset of the information to focus attention. This is equivalent to highlighting salient regions in an image.\n\n2) The next stage organizes the salient regions into meaningful cues. For a face, these would include recognizing the ears, eyes, and nose. 3) Finally, the brain interprets these lower-level cues towards higher-level concepts. In our example, the face cues are used to recognize the animal by: a) recalling from long-term memory the concepts of cat and dog, and their respective cues, b) compare whether each element is closer to the cat or dog version (Fig.  1  uses a slider paradigm for illustration), and c) categorize the concept with the smallest difference. For our application in vocal emotion recognition, the perceptual processing framework aligns with Kotz et al's model for processing emotion prosody  [79, 95]  that describes stages for \"extracting sensory/acoustic features, detecting meaningful relations, conceptual processing of the acoustic patterns in relation to emotion-related knowledge held in long-term memory. \"\n\nIn particular, people categorize concepts by mentally recalling examples and comparing their similarities  [27] . These examples may be prototypes or exemplars. With Prototype Theory, people summarize and recall average examples, but these may be quite different from the observed case being compared. With Exemplar Theory, people memorize and recall specific examples, but this does not scale with inexperienced cases. Instead, people can imagine new cases that they have never experienced  [10] . Moreover, rather than tacitly comparing some ill-defined difference between the examples, people make comparisons by judging similarities or differences along dimensions (cues)  [77] . Categorization can then be done systematically with proposition rules or intuitively  [40] , with either sometimes being more effective  [69] .\n\nWe apply this framework and propose a unified technical approach with contrastive explanation types to align with each stage of perceptual processing: 1) highlight saliency, 2) recognize cues, 3a) synthesize counterfactual, 3b) compare cues, and 3c) classify concept. We further present cue differences as rules and leverage an embedding for emotions to represent intuition (described later).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Desiderata For Relatable Explanations",
      "text": "Informed by the Perceptual Process, Prototype and Exemplar Theories, we identified requirements that AI explanations of the prediction of an instance should be made more relatable towards:\n\nâ€¢ Concepts by relating the predicted concept to other concepts.\n\nContrastive explanations  [59, 72]     [85]  to describe how two products have similar attributes (e.g., both are red in color), but they are used to explain similarity, rather than contrast. Finally, we propose an integrated architecture, RexNet, which is relatable to human reasoning by mimicking parts of human perceptual processing. Together the individual capabilities and overall architecture can improve trust, understanding, and performance by being more relatable.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Vocal Emotion Prosody",
      "text": "People recognize vocal emotions based on various vocal stimulus types and prosodic attributes  [53] , such as verbal  [39]  and nonverbal expressions  [94]  (e.g., laughs, sobs, screams), and lexical  [71]  information. In this work, we focus on vocal cues (prosody) identified by Juslin et al. (e.g., see Table  1 ). These cues are about how words are spoken, rather than the words themselves (lexical information). We leverage people's ability to index vocal emotion categories by the pattern of cues  [39]  to identify cue differences between different emotions, which we present in our model explanation. Although people may be able to perceive various vocal cues, they may be unable to relate to them conceptually (e.g., \"formant frequency\" is technically complex), therefore, we limit cues to familiar everyday concepts. In our user study, we further verified their understandability in a screening test. For our prediction application, the concept to predict is emotion, cues are vocal cues for emotion prosody, cue differences support dimensional comparisons, and saliency is in terms of phonemes or pauses between them.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Technical Approach",
      "text": "We propose an interpretable deep neural network to predict vocal emotions and provide relatable explanations. We first describe the base prediction model, then specific explanation modules.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Base Prediction Model",
      "text": "We trained a vocal emotion classifier on the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset  [66]  with 7356 audio clips of 24 voice actors (50% female) reading fixed sentences with 8 emotions (neutral, calm, happy, fearful, surprised, sad, disgust, angry). Each audio clip was 2.5-3.5 seconds long, and we padded or cropped them to a fixed 3.0s. We parsed each audio file to a time-series array of 48k readings (i.e., 16 kHz sampling rate), and preprocessed it to obtain a mel-frequency spectrogram with 128 frequency bins, 0.04s window size, and 0.01s overlap. Treating the spectrogram as a 2D image, we can train a convolutional neural network (CNN)  [34] . Specifically, we trained a CNN with 3 convolutional blocks, and 2 fully connected layers. We used cross-entropy loss for multi-class classification. In sum, the base CNN model ð‘€ 0 takes audio input ð’™ to predict an emotion Å·0 (lower left in Fig.  2 ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Rexnet: Relatable Explanation Network",
      "text": "We introduce RexNet -Relatable Explanation Network 1  -to provide relatable explanations for contrastive explainable AI. We extended the base model with multiple modules to provide three relatable contrastive explanations (Fig.  2 ). The whole architecture can be understood in terms of a chain of dependencies. We describe this in reverse starting with the goal. Ultimately, we want to explain the prediction with descriptive contrastive cues. This requires a counterfactual \"foil\" to compare the target \"fact\" with, therefore, we need to obtain an example for comparison. When making a comparison, not all stimuli are relevant for interpretation, hence, we need to select salient segments. For example, noticing a flower in a photo of a pet is irrelevant to identifying whether an animal is a dog or cat. In summary, our approach has steps: 1. Highlight salient segments i. Predict emotion concept as initial estimation ii. Keep embedding vector of estimation for final classification iii. Explain contrastive saliency using discounted Grad-CAM 2. Describe segments i. Infer associated cues 3a. Generate counterfactual exemplar for each contrast concept i. Generate counterfactual synthetic using StarGAN-VC  [41]  3b. Compare cue differences between target case and each exemplar i. Calculate cue differences weighted by saliency ii. Classify cue difference relations with cue differences and embedding for target and contrast concepts. 3c. Classify concept fully i. Predict concept using inputs: cue differences of all counterfactuals + embedding (initial estimation) ii. Explain final concept with attributions for cue differences using Layer-wise Relevance Propagation (LRP)  [8]  We next describe each module for specific contrastive explanations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Contrastive Saliency.",
      "text": "Saliency maps are very popular to explain predictions on images, since they intuitively highlight which pixels the model considers important for the predicted outcome  [97, 98, 115] . For spectrograms, they can identify which frequencies or time periods are most salient  [34] . However, they have limited interpretability, since they merely point to raw pixels but do not further elaborate on why those pixels were important. For timeseries data, highlighting on a spectrogram remains uninterpretable to non-technical users, since many are not trained to read spectrograms. Furthermore, some salient pixels may be important across all prediction classes, and thus be less uniquely relevant to the specific class of interest. For example, a saliency map to predict emotions from faces may always highlight the eyes regardless of emotion. To address the issue of saliency lacking semantic meaningfulness, we introduce associative cues, which we describe later. Here, we address the need for more specific saliency with a discounted saliency map to produce contrastive saliency. This retains some importance of globally important pixels, unlike current methods that simply subtract a saliency map of one class from that of another class  [84] . Dhurandhar et al.  [20]  identified pertinent positives and negatives for more precise contrastive explanations by perturbing features, but our approach calculates based on feature activations.\n\nWe define two forms of contrastive saliency: pairwise and total. Pairwise contrastive saliency highlights pixels that are important for predicting target class ð‘¦ but discounts pixels that are also important for foil class ð›¾. We implemented the saliency map with Grad-CAM  [97] , and define the class activation map for class ð‘¦ as s ð‘¦ . The pairwise contrastive saliency between classes ð‘¦ and ð›¾ is thus:\n\nwhere ð€ ð‘¦ð›¾ = (1s ð›¾ ) indicates the discount factors for all pixels due to their attributions to class ð›¾, 1 is a matrix of all ones, and âŠ™ is the Hadamard operator for pixel-wise multiplication. To identify important pixels for class ð‘¦ but not any other class, we define total contrastive saliency as:\n\nwhere ð€ ð‘¦ = ð›¾ âˆˆð¶\\ð‘¦ (1s ð›¾ )/|ð¶ -1| indicates the discount factors across all alternative classes, and ð¶ is the number of classes.\n\nIn RexNet, the saliency explanation is calculated from the initial emotion classifier ð‘€ 0 predicting an initial emotion concept Å·0 . We present contrastive saliency for audio using a 1D saliency bar aligned to words in the speech (see Fig.  5 ), which aggregates saliency in the spectrogram across frequencies per time bin. This is more accessible for lay people to understand since it avoids using technical spectrograms or audiograms (audio waveforms).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Counterfactual Synthetic.",
      "text": "Due to the open-ended variability in unstructured data, counterfactual samples drawn from a training set are likely to be quite different from the target instance. Counterfactual samples will have extraneous differences that may be distracting to interpret and less meaningful for comparison. Instead, counterfactual synthetics are generated to be similar to the target instance, except sufficient differences to achieve the contrastive outcome. Fig.  3  illustrates the benefit of using counterfactual synthetics for comparison. When deciding whether a target item is more similar to a first or second reference, one would measure the target's distance to each reference. Counterfactual synthesis produces comparison references that are closer to the target item being classified, because it minimizes the differences between the target item and reference example. These counterfactual synthetics will be closer to other model samples that the model knows (prior instances in the training set), model prototypes (centroids or medoids of class clusters), or human mental exemplars (from the user's memory), since the model may not have a similar example or the human may never have seen or heard a very similar case to the target item. This amplifies the ratio between the reference distances larger, and makes the difference more perceptible. Formally, the ratio of differences for counterfactual synthetics are larger than for other examples (prototypes, or samples of prior items), i.e., |ð‘™ð‘œð‘”(ð›¿ 1 /ð›¿ 2 )| > |ð‘™ð‘œð‘”(ð‘‘ 1 /ð‘‘ 2 )|. Therefore, counterfactual synthetics help make comparison between references more easy.\n\nWe aim to create a counterfactual that is similar to the target instance ð’™ which is classified as class ð‘¦, but with sufficient differences to be classified as another class ð›¾. Current counterfactual methods focus on structured (tabular) data by minimizing changes to the target instance  [76, 103] , or identifying anchor rules  [91] , but this is not possible for unstructured data (e.g., images, sounds). Instead, inspired by data synthesis with Generative Adversarial Networks (GANs)  [44, 86]  and style transfer  [16, 117] , we propose  For vocal emotion recognition, we aim to change the emotion of the speech audio while retaining the original words and identity by using an extension of StarGAN  [16]  for voice data, StarGAN-VC  [41]  (Fig.  4 ). As a generative adversarial model, StarGAN trains three models -a generator ðº, discriminator ð·, and domain classifier ð‘€. ðº inputs the target instance ð’™ that is of class ð‘¦ and the objective class ð›¾ to generate a similar instance ð’™ ð›¾ . The training objectives are to make xð›¾ â‰ˆ ð’™ and ð‘€ (ð’™ ð›¾ ) â‰ˆ ð›¾. Next, xð›¾ and ð‘¦ are input into ðº to get xð‘¦ as output. ðº is trained to minimize the cycle consistency reconstruction loss between ð’™ ð‘¦ and ð’™, which also improves ð’™ ð›¾ . ð’™ ð›¾ is also input into the ð‘€ to output class Î³, which is trained to minimize the loss between Î³ and ð›¾. Finally, ð· is trained to ensure that the generated instances are more realistic d. Together, this semisupervised method trains ðº to generate style-transferred instances.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Contrastive Cues.",
      "text": "The final contrastive explanation involves first inferring cues from the target and counterfactual instances and comparing them. We define the individual cue as absolute cues (Ä‰ ð‘¦ and Ä‰ð›¾ ), and the difference as contrastive cues Ä‰ð‘¦ð›¾ . We report on 6 vocal cues identified by Juslin and Laukka  [39]  for vocal emotions (Table  1 ). Absolute cues can be inferred with machine learning predictions or heuristically. For vocal emotions, since cues can be deterministically measured from the input data, we use heuristic methods M c to infer the cues c. For example, pitch range is calculated as follows: a) calculate fundamental frequency (modal frequency bin) for each CAM-salient time window in the spectrogram, b) calculate their standard deviation. For semantically abstract cues, such as sounding \"melodic\", \"questioning\", or \"nasally\", they should be annotated by humans and inferred using supervised learning.  We calculated contrastive cues as ordinal cue difference relations r ð‘¦ð›¾ ð‘¤ from numeric cue differences Ä‰ð‘¦ð›¾ based on the instances in the RAVDESS dataset  [66] . To determine differences between emotions for each cue, we fit the data to a linear mixed effects model with emotion as the main fixed effect and voice actors as random effect (see Supplementary Fig.  1 ), and performed a Tukey HSD test with significance level ð›¼ = .005 to account for the multiple comparison effect. For each cue, if an emotion is not significantly higher than the other, then we label the cue difference as \"similar\"; otherwise, we label it as \"higher\" or \"lower\" depending on the direction. Table  2  describes the vocal cue patterns of each emotion compared to average levels, which is in close agreement with  [39]  except for the fearful emotion. Table  3  describes the pairwise cue difference relations between each emotion and an emotion (happy).\n\nPredicting the cue difference relations r ð‘¦ð›¾ ð‘¤ requires deciding the decision threshold at which to split the cue difference Ä‰ð‘¦ð›¾ to categorize the relation, and this can contextually depend on initially estimating which emotion concepts Å·0 and Î³0 to compare, and which cues are more relevant. We define this as a multi-task model with two sub-models with fully connected neural network layers ð‘€ ð‘Ÿ and ð‘€ ð‘¦ . ð‘€ ð‘¦ takes in the numeric cue differences Ä‰ð‘¦ð›¾ and embedding representations (from the penultimate fully connected layer) of the emotion concepts áº‘ð‘¦ 0 and áº‘ð›¾ 0 to predict the emotion Å· heard in ð‘¥. We determine which cues were more important by calculating an attribution explanation Åµð‘¦ð›¾ ð‘ with layer-wise relevance propagation (LRP)  [8] . These attributions are then concatenated on Ä‰ð‘¦ð›¾ to determine the weighted cue differences Åµð‘¦ð›¾ ð‘ . ð‘€ ð‘Ÿ takes in Åµð‘¦ð›¾ ð‘ , áº‘ð‘¦ 0 and áº‘ð›¾ 0 to predict the cue difference relations r ð‘¦ð›¾ ð‘¤ . With the ground truth references, cue difference relations prediction can be trained using supervised learning. Since the cue difference relations (lower, similar, higher) are ordinal, we employed the NNRank ordinal encoding  [15]  with 2 classes, such that lower = (0, 0) ð‘‡ , similar = (1, 0) ð‘‡ , higher = (1, 1) ð‘‡ , sigmoid activation, and binary cross-entropy loss for multi-label classification.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Relatable Explanation User Interface",
      "text": "Fig.  5  shows the user interface with all relatable explanations. After listening to a voice clip (Input), the user can read the model's recognition of the emotion (Prediction), listen to the voice as an alternative emotion (Counterfactual Synthetic), compare the cues between the target and counterfactual voice clips (Contrastive Cues), and see the salient moments in the heatmap (Contrastive Saliency).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Evaluations",
      "text": "We first evaluated the performance of our interpretable model, then conducted two user studies to evaluate the usage and usefulness of the contrastive explanations. The first user study was formative to qualitatively understand usage, and the second was summative to measure the effectiveness of each explanation type.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Modeling Study",
      "text": "5.1.1 Method. We evaluated the model prediction performance and explanation correctness with several metrics (Table  4 ). We measured the accuracies of the initial and final predictions of emotion, and compared them against that of the baseline CNN model. Each explanation type was evaluated with different metrics due to their different forms. We evaluated saliency maps by the relevance of important features to the model prediction, and compared absolute and contrastive saliency. We employed the ablation approach of  [57]  that identifies more important features as those that cause larger decreases in model performance when that feature is ablated. We evaluated the faithfulness of counterfactual synthetics with these metrics: 1) reconstruction similarity ð‘’ð‘¥ð‘ (-ð‘€ð‘†ð¸ (ð’™, xð›¾ )) between the input ð’™ and synthesized xð›¾ , calculated with mean square error ð‘€ð‘†ð¸, to determine how similar they are; 2) the identity classification accuracy to indicate whether the counterfactual voice sounds like the same actor portraying the original emotion; and 3) the emotion classification accuracy with respect to the contrast emotion. We evaluated the correctness of cue difference relations r ð‘¦ð›¾ ð‘¤ by comparing the inferred relations (i.e., higher, lower, similar) to the ground truth relations calculated from the dataset (e.g., see Table  3 ). All multi-class metrics are reported with their macro-averages.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results",
      "text": ". We split the dataset into 80% training and 20% test. Table  4  reports the test results. Training with the explainable modules helped RexNet to achieve higher emotion accuracy than the base CNN (79.5% vs. 75.7%). Though the final emotion accuracy is slightly lower than the initial emotion prediction (78.5% vs. 79.5%), this is expected since interpretability typically trades-off accuracy  [31] . The ablated accuracy decrease indicates that the saliency pixels are somewhat important. Contrastive Saliency has slightly less importance than Absolute Saliency (13.7% vs. 14.9%), because the former excludes pixels that are commonly important for all classes. Counterfactual synthesis was moderately successful, achieving reasonable reconstruction similarity (reconstruction error MSE = 0.680), good speaker re-identification (60.2% compared to 4.2% random chance), and somewhat recognizable emotion which is significantly better than random chance (30.7% vs. 12.5%). The predictions of cue difference relations were good (71.9%).\n\nAlthough the counterfactual synthesis accuracy was better than chance, it is still too low to be used by people. Hence, we evaluated instead using Counterfactual Samples (C.Samples), which uses actual voice clips corresponding to the same voice actor (identity), same speech words, but different portrayed contrast emotion. As expected, the identity and emotion accuracies are higher for Samples than Synthetics, but the other performances were comparable.\n\nIn the next step, we investigate the usage and usefulness of each explanation type. The focus is on the interactions and interface, rather than investigating whether each explanation as implemented is good enough. Therefore, we select instances with correct predictions and coherent explanations for the user studies. Since the Counterfactual Synthesis performance is limited, we use Counterfactual Samples to represent counterfactual examples instead.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Think-Aloud User Study",
      "text": "We conducted a formative study with the think-aloud protocol to understand how people 1) naturally infer emotions without AI assistance, and 2) use or misunderstand various explanations.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiment",
      "text": "Method and Procedure. We recruited 14 participants from a university mailing list. They were 3 males, 11 females, with ages between 21-40 years old. We conducted the study via an online Zoom audio call. The experiment took 40-50 minutes and each participant was compensated with a $7.43 USD coffee gift card. The user task is a human-AI collaborative task for vocal emotion recognition. Given a voice clip, the participant infers the portrayed emotion with or without AI prediction and explanation. We provided 16 voice clips of 2 neutral sentences 2  intoned to portray 8 emotions. We selected only correct system predictions and explanations, since we were not investigating the impact of erroneous predictions or misleading explanations. The study contains 4 explanation interface conditions: Contrastive Saliency only, Counterfactual Sample voice examples only, Counterfactual Sample and Contrastive Cues, and all 3 explanations together (Fig.  5 ).\n\nThe procedure is: read an introduction, consent to the study, complete a guided tutorial of all explanations (regardless of condition), and start the main study with multiple trials of a vocal emotion recognition task. To limit the participation duration, each participant completes three trials, each trial randomly assigned to an explanation interface condition. For each trial, the participant listened to a voice clip, and gave an initial label of the emotion. On the next page, the participant was shown the system's prediction with (or without) explanation based on the assigned condition. She could then revise her emotion label if she changed her mind. We used the think-aloud protocol to ask participants to articulate their thoughts as they examined the audio clip, prediction and explanations. We also asked them about their perceptions using the interface, and any suggestions for improvement. We describe our findings next.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Findings.",
      "text": "We performed thematic analysis on the recorded audio to determine key themes (bolded). We describe our findings in terms of our research questions of how users innately infer vocal emotions, and how they used each explanation type. When inferring on their own (without XAI), participants would focus on specific cues to \"check the intonations [pitch variation] for decision\" [Participant P12], infer a Sad emotion based on the \"flatness of the voice\" [P04], or \"use shrillness to distinguish between fearful and surprise\" [P01]. Participants also relied on changes in tone, which we had not modeled. For example, a rising tone \"sounds like the man is asking a question\" [P02], \"the last word has a questioning tone\" [P03] helped participants to infer Surprise. The latter case identified the most relevant segment. In contrast, a \"tone going down at the end of sentence\" helped P01 infer Sad. Some participants mentally generated their own examples to \"imagine what neutral sound like and compare against it\" [P05]. These unprompted behaviors suggest the relevance of saliency, counterfactual, and cue explanations.\n\nThe usage of explanations was mixed with some benefits and some issues. In general, participants could understand the Saliency maps. P09 saw that \"the highlight parts are consistent with my judgment for important words\", referring to 'talking' being highlighted. However, several participants had issues with saliency maps. There were some cases with highlights that spanned across multiple words and included highlighting spaces. P08 felt that saliency \"should highlight all words\", and P14 \"would prefer the color highlighted on text\". This lack of focus made P13 feel that \"the color bar is not necessary\". Regularizing the explanation to prioritize highlighting words and penalize highlighting spaces can help align the explanations with user expectations and improve trust  [93]  Next, P11 thought that \"the color bar reflects the fluctuation of tone\". While plausible, this indicates the risk of misinterpreting technical visualizations for explanations. Finally, P12 \"used the saliency bar by listening to the highlighted part of the words, and try to infer based on intonation. But I think the highlighting in this example is not accurate\". This demonstrates causal oversimplification by reasoning with one factor rather than multiple factors  [19, 61] .\n\nMany participants found Counterfactual samples \"intuitive\". P11 could \"check whether it's consistent with my intuition\" by mentally comparing the similarity of the target audio clip (sad) with clips for other suspected emotions (neutral, sad, happy). Unfortunately, her intuition was somewhat flawed, since she inferred Neutral which was wrong. P12 found counterfactuals \"helpful to have a reference state, then I will also check the intonations for my decision. \" Conversely, some participants felt counterfactual samples were not helpful. P06 felt that the \"clips [neutral and calm] are too similar\". Had she received deeper explanations with saliency map or cue differences, she would have had more information about where and what the differences were, respectively.\n\nCues were used to check semantic consistency. P04 used cues to \"confirm my judgment\" and found that the \"low shrillness [of Sad] is consistent with my understanding. \" However, some participants perceived inconsistencies. P13 thought that \"some cue descriptions were not consistent with my perception\", and disagreed with the system that Speaking Rate was similar for the Happy and Surprised audio clips. Along with the earlier case of P06, this suggests differences in perceptual acuity of cues between the user and system.\n\nFinally, some participants felt that Counterfactual samples were more useful than Contrastive Cues. P11 found that \"the comparison voice part is more helpful than the text part, though the text part is also helpful to reinforce my decision. \" This could be due to cognitive load and differences between mental dual processing  [40] . Many participants considered the audio samples \"quite intuitive\" [P04]. They used System 1 thinking which is fast, though they did not articulate why this was simple. In contrast, they found that \"it's hard to describe or understand the voice cue patterns\" [P04]. P10 felt that \"compared with [audio] clips, cue pattern is too abstract to use for comparison.\" This requires slower System 2 thinking. Another possible reason is that the audio clip has higher information bandwidth than the 6 verbally presented semantic cues. Participants can perceive the gestalt  [48]  of the audio to make their inferences.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Controlled User Study",
      "text": "Having identified various benefits and usages of contrastive explanation, we next conducted a summative controlled study to understand: 1) how well participants could infer vocal emotions on their own, and with model predictions and explanations, and 2) how various explanations affect perceived helpfulness.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "5.",
      "text": "3.1 Experiment Design and Apparatus. We conducted a betweensubjects experiment with XAI Type as the independent variable with 5 levels of explanations (None, Contrastive Saliency, Counterfactual Sample, Counterfactual + Contrastive Cues, and Saliency + Counterfactual + Cues). The user task is to label the portrayed emotion in a voice clip with feedback from the AI in one of the XAI Types. We included emotion as a random variable with 8 levels . Having many emotions helps to make the task more challenging to test. Fig.  5  shows the UI with all explanations together, and others are shown in Supplementary Figs.  7-11.  For dependent variables, we measured decision quality (emotion label correctness and confidence), understanding of cue differences, task times, decision confidence, and perceived system helpfulness. Labeling correctness was measured with a \"balls and bins\" question  [26]  that elicits the probability of multiple labels. Cue difference understanding was measured per cue with a multiple choice question for the cue difference relation between a randomly selected contrast emotion label and the target voice clip. Task times were logged for different pages. Perceptions were measured as ratings on a 7-point Likert scale (-3 = Strongly Disagree, +3 = Strongly Agree). We asked two text questions about the rationale for perceived helpfulness and how the explanation was used. This was posed only twice to limit fatigue. See Supplementary Figs. 7-12 for the survey.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Experiment",
      "text": "Procedure. The participant reads an introduction, consents to the study, reads a short tutorial about the explanation interfaces, and completes a screening test of audio equipment, auditory acuity, and UI understanding (Supplementary Figs.  2 3 4 ), where she: a) listens to a voice clip and chooses the correct spoken words, b) reads a saliency map and identifies important words, and c) identifies easy cue differences between two voice clips.\n\nAfter passing screening (with all questions correct), the participant is randomly assigned to an XAI Type and commences a practice session. Similar to  [64] , we conducted the practice session to enable the participant to learn from any model explanations how the system predicts the emotion. She is encouraged to study these cases carefully, since she will not see the correct predictions later in the main study. The practice session comprises 8 trials, where each trial has three pages: i) Pre-AI to listen to a voice clip, label the emotion without AI assistance. We assess the labeling correctness here to estimate the Participant Unaided Skill, i.e., whether the participant has above-or below-average skill in recognizing vocal emotions. ii) Post-XAI to read any explanation feedback (without seeing the system prediction, label the emotion (again), and answer questions about cue difference understanding, and perceived ratings. iii) Review to examine the correct emotion label (same as the system prediction) with any AI explanations, and the participant's previous answer, and write any free-form notes (open text).\n\nAfter the practice session, the participant engages in the main study with the same XAI Type in two sessions with 8 trials each and a break in-between. Each trial is presented on one page where the participant: i) listens to the voice clip, ii) views any explanation feedback, iii) labels the emotion, and iv) rates perceptions. This evaluates human-simulatability  [64, 65]  by deeply testing the participant's understanding to apply explanations to new instances.\n\nTo control any fatigue effects due to the moderate number of trials, we randomized the order of instances. We asked the rationale questions randomly in one trial per main session. The participant is incentivized to be fast and correct with a maximum $0.50 USD bonus for completing all trials within 8 minutes. The bonus is prorated by the number of correct emotion labels. Maximum bonus is $1.00 for two sessions over a base compensation of $3.00 USD. The participant ends with answering demographic questions.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Statistical Analysis And Quantitative",
      "text": "Results. We recruited 175 participants from Amazon Mechanical Turk with high qualifications (â‰¥ 5000 completed HITs with >97% approval rate). They were 52.0% male, with ages 21-70 (Median = 36). Participants took 27.4 minutes (median) to complete the survey. We excluded 14 participants who completed the survey without playing any voice clips.\n\nFor each dependent variable, we fit a linear mixed-effects model with XAI Type, Emotion, Voice Clip, Participant Unaided Skill and Trial Number as main fixed effects, and Participant as random effect. We did not find any significant interaction effects. See Supplementary Table  1  for details. We report significant results at a stricter significance level (p<.005) to account for multiple comparisons.\n\nRegarding emotion labeling in the Pre-AI Practice Trials, participants recognized some emotions better than others (Fig.  6a ) and perceived different cues with varying accuracies (Fig.  6b ), indicating that speaking rate and shrillness could be most verifiable in explanations, while pause proportion and pitch variation may be least. Furthermore, there was a wide range of average correctness among participants (M=49.9%, SD=18.3%), so we divided participants by whether they had above-or below-average unaided skill.\n\nAnalyzing the Main Trials, we found varying performances and perceptions due to different XAI Types (Fig.  7 ). Although participants may select a wrong emotion label as most likely, they may still select the correct label with low confidence in the balls and bins question. Hence, we analyzed the Confidence on Correct Label to determine the participant's decision quality. Results were similar when analyzing with labeling correctness. Fig.  7  shows that providing Counterfactual Sample voices with Cues (C.Sample + Cues) were most effective and significantly better than not providing any explanation (None), p=.0007. Omitting the cues (C.Sample) led to a decrease in decision quality such that the difference from None was marginal, p=.0160. Providing Contrastive Saliency explanations did not help to improve decision quality, and, surprisingly, neither did providing all explanations combined together. All XAI Types were rated as more helpful than None, though Saliency was only marginally so (p=.0443). All participants were equally confident (p=n.s.) in their emotion labels across XAI Types (Median=2 on -3 to 3 Likert scale). There was no difference in task time to label the emotions, though the most complex explanations (Saliency + C.Sample + Cues) was only 4.1 sec longer than None (26.1 vs. 21.0s).",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Qualitative Results",
      "text": ". We report why participants found specific XAI Types helpful or unhelpful and how they used them. Some participants depended on their own ability than rely on explanations, e.g., \"I don't think that the [Saliency] explanation information is helpful. I think that the voice is all you really need to be able to determine an emotion. Some participants struggled to use the Contrastive Saliency map. P26 found it \"difficult to parse ... hard to analyze it by eye)\". Errors in the Saliency explanations also led to distrust, as described by P8 that \"the highlighted moments for Fearful don't match well with [the] voice\". Conversely, the sophistication of the explanation led to overtrusting, with P146 mentioning that it was \"helpful to view the color bar to determine which part has the most importance\", yet, this shallow interpretation led to him labeling wrongly. P117 commented that she was \"unable to listen to different ratings the system has given to each emotion\", indicating her desire to hear other samples.\n\nCounterfactual Sample explanations were more appreciated and marginally effective in improving decision quality. P38 felt that the \"emotion in the clip is very clearly anger and it helped to hear the system show me what this voice would sound like when angry\"; thus, she was matching samples by their perceived similarity. Similarly, P132 \"first made my own judgment to narrow down the possible emotions, then listen to those emotions. I rate the one that matches the highest. \" In contrast, P14 felt that C.Sample was \"helpful to tell the difference between the neutral and calm voice\" and \"tried to see if there was a change in inflection or speed\". P103 felt that \"it is slightly far away from the sample clip, every single one of them\", suggesting that he would appreciate Counterfactual Synthetics which would be generated to be more similar. Finally, P54 demurred that \"the explanation information doesn't elaborate at all why it's giving that determination, so it's mostly not helpful\"; this indicates the need for deeper semantic explanations which C.Sample + Cues provides.\n\nInstead of manually perceiving similarities or differences in voice clips, participants could read the cue differences in the Counterfactual Sample + Cues explanation. Their analytical understanding improved, as demonstrated in the vocabulary of their rationalization; e.g., P119 had a \"better sense of the speaker's pitch, loudness\". The semantic knowledge provided by cues also helped to reduce cognitive burden, e.g., P168 \"used the information to confirm something I feel ambiguous about or just to make a guess and not have to spend so much effort deciding between guesses. \" Specifically, cues helped to focus participants' analyses, e.g., P90 found the explanation \"helpful in letting you figure out what qualities to try to isolate in the voice clip to decide on where it learns in terms of emotion. \"\n\nFinally, although participants perceived Saliency + Counterfactual Sample + Cues as helpful, it did not improve decision quality. Participants rationalized the explanations by describing its various components separately, e.g., \"it helps pinpoint what parts to listen to\" [P31, Saliency], \"the sample clips for each emotion are [helpful]\" [P163, C.Sample], \"compare the voices and the levels (like shrillness and pitch)\" [P15, Cues]. However, no one explicitly described multiple components together, and there were few explicit descriptions about the saliency map. Perhaps, participants could not focus on specific explanation details. P167 was \"not sure how to apply cross the broad\", suggesting an issue with information overload.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Summary Of Results",
      "text": "We summarize the results from our three evaluation studies. The modeling study showed that RexNet provides relevant Saliency explanations, accurate Contrastive Cues explanations, and promising Counterfactual Synthetics. These explanations helped to improve RexNet's performance over the base CNN. The think-aloud user study showed that RexNet explanations align with how users innately perceive and infer vocal emotions, validating the XAI Perceptual Processing framework. We identified limitations in user perception and reasoning that led to some interpretation issues. The controlled user study showed that some relatable explanations can improve decision quality without sacrificing task time, especially Counterfactual Samples with semantic Cues. Saliency visualization is too technically sophisticated to be useful, and combining it with Counterfactual Samples and Cues could improve the perceived helpfulness, but also confuse or distract participants to decide poorly.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Discussion",
      "text": "Having evaluated our framework for relatable explainable AI, we discuss their usefulness, improvements to our approach and experiment, implications for human-centric XAI, and generalization.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Usefulness Of Relatable Explanations",
      "text": "Our proposed XAI Perceptual Processing Framework and RexNet architecture unifies different explanations towards relatability. We have rationalized their relevance based on cognitive theories, demonstrated their benefit to improving model prediction performance, and partially validated their usefulness in user studies. We discuss takeaways for XAI developers to design relatable explanations.\n\nThe effectiveness of Counterfactual + Cues explanations indicates the value of augmenting example-based explanations with semantic information. However, we found that saliency explanations had limited usefulness. Furthermore, adding Saliency to Counterfactual + Cues nullifies any benefits of the latter. Our findings contradict those by Wang et al.  [107]  that attribution explanations were more useful than counterfactuals, possibly due to the difference of interpreting structured or unstructured data. Despite many XAI techniques being developed as saliency maps (e.g.,  [112] ), there have been recent calls to develop more meaningful explanations of image prediction tasks  [25] . Thus, saliency maps should not be used or need to be made more precisely correct (e.g., through model training or regularizations) and more semantically meaningful.\n\nTo address the weaknesses of some relatable explanations, we discuss ways to further improve their effectiveness. Using counterfactual synthetics, instead of counterfactual samples would refine the difference between the example and target, so this may focus the user's attention to more meaningful differences and improve discriminating between concepts. Moreover, our current approach identifies one set of cue differences across multiple salient locations. Instead, different cue sets can be associated with specific highlights in the saliency map. This can provide more semantics to various parts of a saliency map, to indicate why particular regions were important, and improve the usefulness of saliency maps.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "User Evaluation Of Relatable Explanations",
      "text": "We chose to evaluate with vocal emotion recognition since it is an everyday task that is feasible to test with lay users. However, most people are already innately skilled in this, so this diminishes their need for AI or XAI to help them. Conversely, relatable explanations may be more useful for more analytical tasks and applications with more explicit domain knowledge (e.g., engine noise diagnosis).\n\nWe had identified several potential confounds -fatigue, skill at recognizing emotions, participants copying system predictions, learning effects from exposure to prior XAI versions -and discuss how we mitigated them. 1) We controlled for fatigue by: a) providing breaks between sessions, b) randomizing instances across trial numbers. We checked for fatigue by measuring: a) repeatedly identical responses (no participants were disqualified), b) decreases in labeling correctness over trials (no significant difference). 2) We controlled for recognition skills by measuring labeling performance without XAI (Pre-AI) and analyzed our results with that as a factor. 3) A more realistic use of AI is for it to make predictions and the user would verify its decision. However, in a pilot study evaluating with this task, we found that participants may copy the prediction rather than study the explanation, thus leading to over-trusting  [110]  and diminishing the usefulness of explanations to improve decision quality. We mitigated copying by evaluating with a human-simulatability task, instead of a predicted label verification task, though this trades-off some ecological validity. 4) We mitigated learning effects by designing the experiment as betweensubjects, otherwise, participants may exploit new knowledge in subsequent experiment conditions (with weaker explanations).",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "More Relatable Vocal Emotion Explanations",
      "text": "This work is the first to explore relatable explanations for vocal emotion prediction, with an initial set of cues and adequate explanation accuracy. Future work can leverage other vocal stimulus types and prosodic attributes  [53] , such as non-verbal expressions, affect bursts, and lexical information. In particular, we learned that participants focus on the change in tone in voices to infer emotion, so this should be included as a vocal cue. Counterfactual synthesis accuracy can be improved by using newer generators, such as Sequence-to-Sequence Voice Conversion  [42] , StarGAN-VC v2  [43] . Though generated from a unified architecture, the explanations still had some inconsistencies. Annotating and debiasing explanations  [57, 113]  could help to align explanations with user expectations  [93]  and improve the coherence between explanation types. Contrastive Cue relations were encoded as a table, but they could be represented as another data structure (e.g., decision trees or causal graphs) to better fit human mental models. Finally, further testing could evaluate the usage and usefulness of predictions and explanations in in-the-wild applications  [63] , such as with smart speakers (e.g., Amazon Echo)  [70] , smartphone digital assistants for mental health or emotion monitoring  [9, 106] , or AI coaching for call center employees  [28, 68, 80] .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Relatability For Human-Centric Xai",
      "text": "Although many XAI techniques have been recently developed, many remain too technical, or focus on supporting data scientists and machine learning model developers. Instead, there is a growing call to support different stakeholders and less technical users  [14, 21, 58]  Towards this end, we have studied human perception and cognition to determine new requirements for XAI. Miller had argued for contrastive and counterfactual explanations based on philosophical and psychological principles  [72] . This was extended by Wang et al. to identify human reasoning pathways that can be supported by specific XAI techniques  [107] . We extend these perspectives by identifying a broader requirement that explanations need to be relatable and contextualized to be more meaningfully interpreted. Specifically, we supported four criteria for relatability: contrastive concepts, saliency, counterfactuals, and associated cues. Extending our work, explanations can be made more relatable by providing for other criteria such as: social proof  [17, 72] , narrative stories  [96]  or rationalizations  [21] , analogies  [24] , user-defined concepts  [25, 47, 116] , and plausible explanations  [93] . Moreover, human cognition has natural flaws, like cognitive biases and limited working memory. XAI should include designs and capabilities to mitigate cognitive biases  [104] , moderate cognitive load  [2] , and accommodate information handling preferences  [105] . Relatable explanations may need to account for these human factors to communicate why they may deviate from human reasoning.\n\nThe XAI Perceptual Processing Framework was inspired by human perceptual reasoning, rather than higher-level cognition. The latter is relevant for complex decision-making tasks, such as doctors' reasoning with disease models, which are specific cases of causal structural models. Wang et al. proposed the XAI Reasoning Framework based on human reasoning processes  [104] , but this was not explicitly implemented in a single machine learning architecture. The Intelligibility Toolkit  [60]  provided an API to automatically generate explanations to a taxonomy of questions  [58, 59] , but this was not implemented for deep learning. Future work can explore a meta-model that combines perceptual and reasoning faculties for more complex, human-like model explanations.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Generalization Of Relatable Xai",
      "text": "Although we implemented RexNet for the application of vocal emotion recognition, the XAI Perceptual Processing Framework is generalizable to other audio and visual prediction applications. Other audio applications include equipment monitoring via vibrations  [108] , and heart murmur diagnosis  [89] . 1) Saliency can be highlighted on a spectrogram of vibration signals for trained engineers to interpret, or highlighted on auscultation diagrams for clinicians. 2) Counterfactual samples can be archetypal sounds of engine failure, specific heart disease (e.g., crescendo-decrescendo murmur in aortic stenosis), etc. 3) Cues can be the sound profiles, such as engine pinging, or a seagull cry sound in heart murmurs.\n\nFor vision perception (e.g., image recognition) tasks, relatable explanations can be as follows. 1) Saliency can be presented, as is common, as a heatmap to identify important pixels for a decision, such as highlighting the eyes and mouth of a happy face  [38] ), or papillary, sclerotic, solid, and hemorrhagic growth patterns for cancer in a histology image  [87] . 2) Counterfactual samples can be based on canonical (prototype) or critical (almost ambiguous) examples  [46] , such as feminizing male faces  [32, 109]  or changing a scene from day to night  [117] . 3) Cues can include visual cues, such as depth, motion, color, and contrast  [82] .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Conclusion",
      "text": "We presented the XAI Perceptual Processing Framework to unify a set of contrastive, saliency, counterfactual and cues explanations towards relatable explainable AI. The framework was implemented with RexNet, a modular multi-task deep neural network with multiple explanations, trained to predict vocal emotions. From qualitative think-aloud and quantitative controlled studies, we found varying usage and usefulness across the relatable contrastive explanations. This work gives insights into providing and evaluating relatable contrastive explainable AI for perception applications, and contributes a new basis towards human-centered XAI.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: XAI Perceptual Processing Framework for relatable explainable AI. Inspired by the human perceptual process to",
      "page": 3
    },
    {
      "caption": "Figure 1: illustrates these stages for the case of visually",
      "page": 3
    },
    {
      "caption": "Figure 2: ). The whole architecture",
      "page": 4
    },
    {
      "caption": "Figure 2: Modular architecture of RexNet with relatable explanations for the prediction of emotion ð‘¦from input voice ð’™. Each",
      "page": 5
    },
    {
      "caption": "Figure 1: ). Black arrows indicate feedforward activations.",
      "page": 5
    },
    {
      "caption": "Figure 5: ), which aggregates",
      "page": 5
    },
    {
      "caption": "Figure 3: illustrates the benefit of using counterfactual syn-",
      "page": 5
    },
    {
      "caption": "Figure 3: Conceptual illustration of the benefit of using",
      "page": 6
    },
    {
      "caption": "Figure 4: StarGAN-VC [16] architecture to generate counter-",
      "page": 6
    },
    {
      "caption": "Figure 4: ). As a generative adversarial model, StarGAN trains three",
      "page": 6
    },
    {
      "caption": "Figure 1: ), and performed a Tukey HSD test with",
      "page": 6
    },
    {
      "caption": "Figure 5: User interface of voice clip (RexNet step 0), pre-",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the user interface with all relatable explanations. After",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the UI with all explanations together, and",
      "page": 9
    },
    {
      "caption": "Figure 6: Participant audio perception skill across emotions",
      "page": 9
    },
    {
      "caption": "Figure 6: b), indicating",
      "page": 9
    },
    {
      "caption": "Figure 7: ). Although partici-",
      "page": 9
    },
    {
      "caption": "Figure 7: Results of relatable explanations on a) labeling correctness, b) perceived helpfulness, and c) task time for AI-assisted",
      "page": 10
    },
    {
      "caption": "Figure 7: shows that pro-",
      "page": 10
    },
    {
      "caption": "Figure 1: Distribution of cue values for different emotions and the average across all voice clips. Values calculated from the",
      "page": 15
    },
    {
      "caption": "Figure 2: Tutorial to clarify usersâ€™ tasks, to interpret the â€œballs and binsâ€ question [26] and screening question to check usersâ€™",
      "page": 16
    },
    {
      "caption": "Figure 3: Tutorial on the contrastive saliency explanation and screening question to check usersâ€™ interpretation.",
      "page": 17
    },
    {
      "caption": "Figure 4: Tutorial on the counterfactual sample explanation.",
      "page": 17
    },
    {
      "caption": "Figure 5: Tutorial on the contrastive cue explanation and screening question to check usersâ€™ understanding about vocal cues.",
      "page": 18
    },
    {
      "caption": "Figure 6: Example practice session per-voice trial before revealing the systemâ€™s XAI information (Pre-XAI).",
      "page": 19
    },
    {
      "caption": "Figure 7: Example main study per-voice trial without the systemâ€™s explanation.",
      "page": 20
    },
    {
      "caption": "Figure 8: Example main study per-voice trial with the contrastive saliency explanation.",
      "page": 20
    },
    {
      "caption": "Figure 9: Example main study per-voice trial with the counterfactual sample explanation.",
      "page": 20
    },
    {
      "caption": "Figure 10: Example main study per-voice trial with the counterfactual sample and contrastive cue explanations.",
      "page": 21
    },
    {
      "caption": "Figure 11: Example main study per-voice trial with the contrastive saliency, counterfactual sample and contrastive cue explana-",
      "page": 21
    },
    {
      "caption": "Figure 12: Example main study per-voice trial with the questionnaire after revealing the systemâ€™s XAI information (Post-XAI).",
      "page": 22
    },
    {
      "caption": "Figure 13: Example practice session per-voice trial to show the correct answer and review usersâ€™ choices.",
      "page": 23
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "ð’™"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a)": "80% ecnedfinoC lebaL\n60%\ntcerroC\n40%\nIA-erP\n20% no\n0%\nSad Happy Calm Disgust NeutralSurprised Angry Fearful\nEmoGon\nb)"
        },
        {
          "a)": "50% ssentcerroC\necnereffiD\n40%\nno4aleR\n30% euC\n20%\nPause Pitch Average Loudness Shrillness Speaking\nPropor4on Varia4on Pitch Rate\nCue"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a)\np=.0\np=.0160": "",
          "Column_2": "p=.0\np=.0160",
          "Column_3": "007"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ")\np=.0003p=.0\np=.0443": "",
          "Column_2": "p=.0003p=.0\np=.0443",
          "p<.0001": "006"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda",
      "authors": [
        "Ashraf Abdul",
        "Jo Vermeulen",
        "Danding Wang",
        "Brian Lim",
        "Mohan Kankanhalli"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 CHI conference on human factors in computing systems"
    },
    {
      "citation_id": "2",
      "title": "COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations",
      "authors": [
        "Ashraf Abdul",
        "Christian Von Der Weth",
        "Mohan Kankanhalli",
        "Brian Lim"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "3",
      "title": "Peeking inside the black-box: a survey on explainable artificial intelligence (XAI)",
      "authors": [
        "Amina Adadi",
        "Mohammed Berrada"
      ],
      "year": "2018",
      "venue": "IEEE access"
    },
    {
      "citation_id": "4",
      "title": "Interpretable representation learning for speech and audio signals based on relevance weighting",
      "authors": [
        "Purvi Agrawal",
        "Sriram Ganapathy"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "5",
      "title": "Deep speech 2: End-to-end speech recognition in english and mandarin",
      "authors": [
        "Dario Amodei",
        "Rishita Sundaram Ananthanarayanan",
        "Jingliang Anubhai",
        "Eric Bai",
        "Carl Battenberg",
        "Jared Case",
        "Bryan Casper",
        "Qiang Catanzaro",
        "Guoliang Cheng",
        "Chen"
      ],
      "year": "2016",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "6",
      "title": "Mental models of mere mortals with explanations of reinforcement learning",
      "authors": [
        "Andrew Anderson",
        "Jonathan Dodge",
        "Amrita Sadarangani",
        "Zoe Juozapaitis",
        "Evan Newman",
        "Jed Irvine",
        "Souti Chattopadhyay",
        "Matthew Olson",
        "Alan Fern",
        "Margaret Burnett"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS)"
    },
    {
      "citation_id": "7",
      "title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
      "authors": [
        "Alejandro Barredo Arrieta",
        "Natalia DÃ­az-RodrÃ­guez",
        "Javier Del Ser",
        "Adrien Bennetot",
        "Siham Tabik",
        "Alberto Barbado",
        "Salvador GarcÃ­a",
        "Sergio Gil-LÃ³pez",
        "Daniel Molina",
        "Richard Benjamins"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "8",
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "authors": [
        "Sebastian Bach",
        "Alexander Binder",
        "GrÃ©goire Montavon",
        "Frederick Klauschen",
        "Klaus-Robert MÃ¼ller",
        "Wojciech Samek"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "9",
      "title": "Next-generation psychiatric assessment: Using smartphone sensors to monitor behavior and mental health",
      "authors": [
        "Dror Ben-Zeev",
        "Emily Scherer",
        "Rui Wang",
        "Haiyi Xie",
        "Andrew Campbell"
      ],
      "year": "2015",
      "venue": "Psychiatric rehabilitation journal"
    },
    {
      "citation_id": "10",
      "title": "The rational imagination: How people create alternatives to reality",
      "authors": [
        "M Ruth",
        "Byrne"
      ],
      "year": "2007",
      "venue": "The rational imagination: How people create alternatives to reality"
    },
    {
      "citation_id": "11",
      "title": "The effects of examplebased explanations in a machine learning interface",
      "authors": [
        "Carrie Cai",
        "Jonas Jongejan",
        "Jess Holbrook"
      ],
      "year": "2019",
      "venue": "Proceedings of the 24th international conference on intelligent user interfaces"
    },
    {
      "citation_id": "12",
      "title": "Human-centered tools for coping with imperfect algorithms during medical decision-making",
      "authors": [
        "Carrie Cai",
        "Emily Reif",
        "Narayan Hegde",
        "Jason Hipp",
        "Been Kim",
        "Daniel Smilkov",
        "Martin Wattenberg",
        "Fernanda Viegas",
        "Greg Corrado",
        "Martin Stumpe"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "13",
      "title": "Perceptual processing",
      "year": "1978",
      "venue": "Perceptual processing"
    },
    {
      "citation_id": "14",
      "title": "Explaining decision-making algorithms through UI: Strategies to help non-expert stakeholders",
      "authors": [
        "Hao-Fei Cheng",
        "Ruotong Wang",
        "Zheng Zhang",
        "Fiona O' Connell",
        "Terrance Gray",
        "Maxwell Harper",
        "Haiyi Zhu"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 chi conference on human factors in computing systems"
    },
    {
      "citation_id": "15",
      "title": "A neural network approach to ordinal regression",
      "authors": [
        "Jianlin Cheng",
        "Zheng Wang",
        "Gianluca Pollastri"
      ],
      "year": "2008",
      "venue": "2008 IEEE international joint conference on neural networks (IEEE world congress on computational intelligence)"
    },
    {
      "citation_id": "16",
      "title": "Stargan: Unified generative adversarial networks for multidomain image-to-image translation",
      "authors": [
        "Yunjey Choi",
        "Minje Choi",
        "Munyoung Kim",
        "Jung-Woo Ha",
        "Sunghun Kim",
        "Jaegul Choo"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Compliance with a request in two cultures: The differential influence of social proof and commitment/consistency on collectivists and individualists",
      "authors": [
        "Wilhelmina Robert B Cialdini",
        "Daniel Wosinska",
        "Jonathan Barrett",
        "Malgorzata Butner",
        "Gornik-Durose"
      ],
      "year": "1999",
      "venue": "Personality and Social Psychology Bulletin"
    },
    {
      "citation_id": "18",
      "title": "Intellingo: an intelligible translation environment",
      "authors": [
        "Sven Coppers",
        "Jan Van Den",
        "Kris Bergh",
        "Karin Luyten",
        "Iulianna Coninx",
        "Tom Van Der Lek-Ciudin",
        "Vincent Vanallemeersch",
        "Vandeghinste"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "19",
      "title": "Attacking faulty reasoning",
      "authors": [
        "Damer Edward"
      ],
      "year": "2012",
      "venue": "Attacking faulty reasoning"
    },
    {
      "citation_id": "20",
      "title": "Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives",
      "authors": [
        "Amit Dhurandhar",
        "Pin-Yu Chen",
        "Ronny Luss",
        "Chun-Chen Tu",
        "Paishun Ting",
        "Karthikeyan Shanmugam",
        "Payel Das"
      ],
      "venue": "Ann Arbor"
    },
    {
      "citation_id": "21",
      "title": "Rationalization: A neural machine translation approach to generating natural language explanations",
      "authors": [
        "Upol Ehsan",
        "Brent Harrison",
        "Larry Chan",
        "Mark Riedl"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society"
    },
    {
      "citation_id": "22",
      "title": "Expanding explainability: Towards social transparency in ai systems",
      "authors": [
        "Q Upol Ehsan",
        "Michael Vera Liao",
        "Mark Muller",
        "Justin Riedl",
        "Weisz"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "23",
      "title": "Bringing transparency design into practice",
      "authors": [
        "Malin Eiband",
        "Hanna Schneider",
        "Mark Bilandzic",
        "Julian Fazekas-Con",
        "Mareike Haug",
        "Heinrich Hussmann"
      ],
      "year": "2018",
      "venue": "23rd international conference on intelligent user interfaces"
    },
    {
      "citation_id": "24",
      "title": "Analogical reasoning",
      "authors": [
        "Dedre Gentner",
        "Linsey Smith"
      ],
      "year": "2012",
      "venue": "Encyclopedia of human behavior"
    },
    {
      "citation_id": "25",
      "title": "Towards Automatic Concept-based Explanations",
      "authors": [
        "Amirata Ghorbani",
        "James Wexler",
        "James Zou",
        "Been Kim"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "26",
      "title": "Lay understanding of probability distributions",
      "authors": [
        "G Daniel",
        "David Goldstein",
        "Rothschild"
      ],
      "year": "2014",
      "venue": "Judgment & Decision Making"
    },
    {
      "citation_id": "27",
      "title": "Cognitive psychology: Connecting mind, research and everyday experience",
      "authors": [
        "Goldstein Bruce"
      ],
      "year": "2014",
      "venue": "Cognitive psychology: Connecting mind, research and everyday experience"
    },
    {
      "citation_id": "28",
      "title": "Attention-based Sequence Classification for Affect Detection",
      "authors": [
        "Cristina Gorrostieta",
        "Richard Brutti",
        "Kye Taylor",
        "Avi Shapiro",
        "Joseph Moran",
        "Ali Azarbayejani",
        "John Kane"
      ],
      "year": "2018",
      "venue": "Attention-based Sequence Classification for Affect Detection"
    },
    {
      "citation_id": "29",
      "title": "Counterfactual visual explanations",
      "authors": [
        "Yash Goyal",
        "Ziyan Wu",
        "Jan Ernst",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "A survey of methods for explaining black box models",
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Salvatore Ruggieri",
        "Franco Turini",
        "Fosca Giannotti",
        "Dino Pedreschi"
      ],
      "year": "2018",
      "venue": "ACM computing surveys (CSUR)"
    },
    {
      "citation_id": "31",
      "title": "DARPA's explainable artificial intelligence (XAI) program",
      "authors": [
        "David Gunning",
        "David Aha"
      ],
      "year": "2019",
      "venue": "AI Magazine"
    },
    {
      "citation_id": "32",
      "title": "Attgan: Facial attribute editing by only changing what you want",
      "authors": [
        "Zhenliang He",
        "Wangmeng Zuo",
        "Meina Kan",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "IEEE transactions on image processing"
    },
    {
      "citation_id": "33",
      "title": "Generating Counterfactual Explanations with Natural Language",
      "authors": [
        "Anne Lisa",
        "Ronghang Hendricks",
        "Trevor Hu",
        "Zeynep Darrell",
        "Akata"
      ],
      "year": "2018",
      "venue": "ICML Workshop on Human Interpretability in Machine Learning"
    },
    {
      "citation_id": "34",
      "title": "CNN architectures for large-scale audio classification",
      "authors": [
        "Shawn Hershey",
        "Sourish Chaudhuri",
        "P Daniel",
        "Ellis",
        "Aren Jort F Gemmeke",
        "R Channing Jansen",
        "Manoj Moore",
        "Devin Plakal",
        "Rif Platt",
        "Bryan Saurous",
        "Seybold"
      ],
      "year": "2017",
      "venue": "2017 ieee international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "35",
      "title": "Conversational processes and causal explanation",
      "authors": [
        "Hilton Denis"
      ],
      "year": "1990",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "36",
      "title": "Visual analytics in deep learning: An interrogative survey for the next frontiers",
      "authors": [
        "Fred Hohman",
        "Minsuk Kahng",
        "Robert Pienta",
        "Duen Horng"
      ],
      "year": "2018",
      "venue": "IEEE transactions on visualization and computer graphics"
    },
    {
      "citation_id": "37",
      "title": "Speech emotion recognition using CNN",
      "authors": [
        "Zhengwei Huang",
        "Ming Dong",
        "Qirong Mao",
        "Yongzhao Zhan"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "38",
      "title": "Hybrid deep neural networks for face emotion recognition",
      "authors": [
        "Neha Jain",
        "Shishir Kumar",
        "Amit Kumar",
        "Pourya Shamsolmoali",
        "Masoumeh Zareapoor"
      ],
      "year": "2018",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "39",
      "title": "Impact of intended emotion intensity on cue utilization and decoding accuracy in vocal expression of emotion",
      "authors": [
        "N Patrik",
        "Petri Juslin",
        "Laukka"
      ],
      "year": "2001",
      "venue": "Emotion"
    },
    {
      "citation_id": "40",
      "title": "Thinking, fast and slow",
      "authors": [
        "Daniel Kahneman"
      ],
      "year": "2011",
      "venue": "Thinking, fast and slow"
    },
    {
      "citation_id": "41",
      "title": "Stargan-vc: Non-parallel many-to-many voice conversion using star generative adversarial networks",
      "authors": [
        "Hirokazu Kameoka",
        "Takuhiro Kaneko",
        "Kou Tanaka",
        "Nobukatsu Hojo"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "42",
      "title": "ConvS2S-VC: Fully convolutional sequence-to-sequence voice conversion",
      "authors": [
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Damian KwaÅ›ny",
        "Takuhiro Kaneko",
        "Nobukatsu Hojo"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "43",
      "title": "StarGAN-VC2: Rethinking conditional methods for StarGAN-based voice conversion",
      "authors": [
        "Takuhiro Kaneko",
        "Hirokazu Kameoka",
        "Kou Tanaka",
        "Nobukatsu Hojo"
      ],
      "year": "2019",
      "venue": "StarGAN-VC2: Rethinking conditional methods for StarGAN-based voice conversion",
      "arxiv": "arXiv:1907.12279"
    },
    {
      "citation_id": "44",
      "title": "A style-based generator architecture for generative adversarial networks",
      "authors": [
        "Tero Karras",
        "Samuli Laine",
        "Timo Aila"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Lee Kristina"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "46",
      "title": "Examples are not enough, learn to criticize! criticism for interpretability",
      "authors": [
        "Been Kim",
        "Rajiv Khanna",
        "Oluwasanmi Koyejo"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "47",
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "authors": [
        "Been Kim",
        "Martin Wattenberg",
        "Justin Gilmer",
        "Carrie Cai",
        "James Wexler",
        "Fernanda Viegas"
      ],
      "year": "2018",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "48",
      "title": "Principles of Gestalt psychology",
      "authors": [
        "Kurt Koffka"
      ],
      "year": "2013",
      "venue": "Principles of Gestalt psychology"
    },
    {
      "citation_id": "49",
      "title": "Understanding black-box predictions via influence functions",
      "authors": [
        "Pang Wei",
        "Percy Liang"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "50",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "51",
      "title": "Neuron activation profiles for interpreting convolutional speech recognition models",
      "authors": [
        "Andreas Krug",
        "RenÃ© Knaebel",
        "Sebastian Stober"
      ],
      "year": "2018",
      "venue": "NeurIPS Workshop on Interpretability and Robustness in Audio, Speech, and Language (IRASL)"
    },
    {
      "citation_id": "52",
      "title": "Introspection for convolutional automatic speech recognition",
      "authors": [
        "Andreas Krug",
        "Sebastian Stober"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP"
    },
    {
      "citation_id": "53",
      "title": "Emotion recognition and confidence ratings predicted by vocal stimulus type and prosodic parameters",
      "authors": [
        "Adi Lausen",
        "Kurt Hammerschmidt"
      ],
      "year": "2020",
      "venue": "Humanities and Social Sciences Communications"
    },
    {
      "citation_id": "54",
      "title": "GRACE: Generating Concise and Informative Contrastive Sample to Explain Neural Network Model's Prediction",
      "authors": [
        "Thai Le",
        "Suhang Wang",
        "Dongwon Lee"
      ],
      "year": "2020",
      "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "55",
      "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model",
      "authors": [
        "Benjamin Letham",
        "Cynthia Rudin",
        "Tyler Mccormick",
        "David Madigan"
      ],
      "year": "2015",
      "venue": "The Annals of Applied Statistics"
    },
    {
      "citation_id": "56",
      "title": "What does a network layer hear? analyzing hidden representations of end-to-end asr through speech synthesis",
      "authors": [
        "Chung-Yi Li",
        "Pei-Chieh Yuan",
        "Hung-Yi Lee"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "57",
      "title": "Tell me where to look: Guided attention inference network",
      "authors": [
        "Kunpeng Li",
        "Ziyan Wu",
        "Kuan-Chuan Peng",
        "Jan Ernst",
        "Yun Fu"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "58",
      "title": "Questioning the AI: informing design practices for explainable AI user experiences",
      "authors": [
        "Daniel Vera Liao",
        "Sarah Gruen",
        "Miller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "59",
      "title": "Assessing demand for intelligibility in context-aware applications",
      "authors": [
        "Y Brian",
        "Anind Lim",
        "Dey"
      ],
      "year": "2009",
      "venue": "Proceedings of the 11th international conference on Ubiquitous computing"
    },
    {
      "citation_id": "60",
      "title": "Toolkit to support intelligibility in contextaware applications",
      "authors": [
        "Y Brian",
        "Anind Lim",
        "Dey"
      ],
      "year": "2010",
      "venue": "Proceedings of the 12th ACM international conference on Ubiquitous computing"
    },
    {
      "citation_id": "61",
      "title": "Design of an intelligible mobile contextaware application",
      "authors": [
        "Y Brian",
        "Anind Lim",
        "Dey"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on human computer interaction with mobile devices and services"
    },
    {
      "citation_id": "62",
      "title": "Investigating intelligibility for uncertain context-aware applications",
      "authors": [
        "Y Brian",
        "Anind Lim",
        "Dey"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th international conference on Ubiquitous computing"
    },
    {
      "citation_id": "63",
      "title": "Evaluating intelligibility usage and usefulness in a context-aware application",
      "authors": [
        "Y Brian",
        "Anind Lim",
        "Dey"
      ],
      "year": "2013",
      "venue": "International Conference on Human-Computer Interaction"
    },
    {
      "citation_id": "64",
      "title": "Why and why not explanations improve the intelligibility of context-aware intelligent systems",
      "authors": [
        "Brian Lim",
        "Anind Dey",
        "Daniel Avrahami"
      ],
      "year": "2009",
      "venue": "Proceedings of the SIGCHI conference on human factors in computing systems"
    },
    {
      "citation_id": "65",
      "title": "The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery",
      "authors": [
        "Zachary C Lipton"
      ],
      "year": "2018",
      "venue": "Queue"
    },
    {
      "citation_id": "66",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "67",
      "title": "On Learning Interpretable CNNs with Parametric Modulated Kernel-Based Filters",
      "authors": [
        "Erfan Loweimi",
        "Peter Bell",
        "Steve Renals"
      ],
      "year": "2019",
      "venue": "INTERSPEECH"
    },
    {
      "citation_id": "68",
      "title": "Artificial Intelligence Coaches for Sales Agents: Caveats and Solutions",
      "authors": [
        "Xueming Luo",
        "Marco Shaojun Qin",
        "Zheng Fang",
        "Zhe Qu"
      ],
      "year": "2021",
      "venue": "Journal of Marketing"
    },
    {
      "citation_id": "69",
      "title": "Trust your gut or think carefully? Examining whether an intuitive, versus a systematic, mode of thought produces greater empathic accuracy",
      "authors": [
        "Christine Ma",
        "Jennifer Lerner"
      ],
      "year": "2016",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "70",
      "title": "Hear me out\" smart speaker based conversational agent to monitor symptoms in mental health",
      "authors": [
        "Raju Maharjan",
        "Per Baekgaard",
        "Jakob Bardram"
      ],
      "year": "2019",
      "venue": "Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers"
    },
    {
      "citation_id": "71",
      "title": "Compensating for speaker or lexical variabilities in speech for emotion recognition",
      "authors": [
        "Soroosh Mariooryad",
        "Carlos Busso"
      ],
      "year": "2014",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "72",
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "authors": [
        "Tim Miller"
      ],
      "year": "2019",
      "venue": "Artificial intelligence"
    },
    {
      "citation_id": "73",
      "title": "Contrastive explanation: A structural-model approach",
      "authors": [
        "Tim Miller"
      ],
      "year": "2021",
      "venue": "The Knowledge Engineering Review"
    },
    {
      "citation_id": "74",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "Seyedmahdad Mirsamadi",
        "Emad Barsoum",
        "Cha Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "75",
      "title": "Explaining nonlinear classification decisions with deep taylor decomposition",
      "authors": [
        "GrÃ©goire Montavon",
        "Sebastian Lapuschkin",
        "Alexander Binder",
        "Wojciech Samek",
        "Klaus-Robert MÃ¼ller"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "76",
      "title": "Explaining machine learning classifiers through diverse counterfactual explanations",
      "authors": [
        "Amit Ramaravind K Mothilal",
        "Chenhao Sharma",
        "Tan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency"
    },
    {
      "citation_id": "77",
      "title": "Mental comparison and the symbolic distance effect",
      "authors": [
        "S Robert",
        "Richard Moyer",
        "Bayer"
      ],
      "year": "1976",
      "venue": "Cognitive Psychology"
    },
    {
      "citation_id": "78",
      "title": "Feature visualization",
      "authors": [
        "Chris Olah",
        "Alexander Mordvintsev",
        "Ludwig Schubert"
      ],
      "year": "2017",
      "venue": "Distill"
    },
    {
      "citation_id": "79",
      "title": "On the time course of vocal emotion recognition",
      "authors": [
        "D Marc",
        "Sonja Pell",
        "Kotz"
      ],
      "year": "2011",
      "venue": "PLoS One"
    },
    {
      "citation_id": "80",
      "title": "Emotion in speech: Recognition and application to call centers",
      "authors": [
        "Valery Petrushin"
      ],
      "year": "1999",
      "venue": "Proceedings of artificial neural networks in engineering"
    },
    {
      "citation_id": "81",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "82",
      "title": "Visual dominance: an information-processing account of its origins and significance",
      "authors": [
        "Mary Michael I Posner",
        "Raymond Nissen",
        "Klein"
      ],
      "year": "1976",
      "venue": "Psychological review"
    },
    {
      "citation_id": "83",
      "title": "Manipulating and measuring model interpretability",
      "authors": [
        "Forough Poursabzi-Sangdeh",
        "Jake Daniel G Goldstein",
        "Jennifer Hofman",
        "Wortman Wortman",
        "Hanna Vaughan",
        "Wallach"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "84",
      "title": "Contrastive explanations in neural networks",
      "authors": [
        "Mohit Prabhushankar",
        "Gukyeong Kwon"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "85",
      "title": "Trust-inspiring explanation interfaces for recommender systems",
      "authors": [
        "Pearl Pu",
        "Li Chen"
      ],
      "year": "2007",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "86",
      "title": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "authors": [
        "Alec Radford",
        "Luke Metz",
        "Soumith Chintala"
      ],
      "year": "2015",
      "venue": "Unsupervised representation learning with deep convolutional generative adversarial networks",
      "arxiv": "arXiv:1511.06434"
    },
    {
      "citation_id": "87",
      "title": "Deep convolutional neural networks for breast cancer histology image analysis",
      "authors": [
        "Alexander Rakhlin",
        "Alexey Shvets",
        "Vladimir Iglovikov",
        "Alexandr Kalinin"
      ],
      "year": "2018",
      "venue": "International conference image analysis and recognition"
    },
    {
      "citation_id": "88",
      "title": "Speaker recognition from raw waveform with sincnet",
      "authors": [
        "Mirco Ravanelli",
        "Yoshua Bengio"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "89",
      "title": "Heart sound analysis for symptom detection and computer-aided diagnosis",
      "authors": [
        "Todd Reed",
        "Nancy Reed",
        "Peter Fritzson"
      ],
      "year": "2004",
      "venue": "Simul. Model. Pract. Theory"
    },
    {
      "citation_id": "90",
      "title": "Explaining the predictions of any classifier",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining"
    },
    {
      "citation_id": "91",
      "title": "Anchors: Highprecision model-agnostic explanations",
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "92",
      "title": "Beyond PageRank: machine learning for static ranking",
      "authors": [
        "Matthew Richardson",
        "Amit Prakash",
        "Eric Brill"
      ],
      "year": "2006",
      "venue": "Proceedings of the 15th international conference on World Wide Web"
    },
    {
      "citation_id": "93",
      "title": "Right for the right reasons: training differentiable models by constraining their explanations",
      "authors": [
        "Andrew Slavin",
        "Michael Hughes",
        "Finale Doshi-Velez"
      ],
      "year": "2017",
      "venue": "Proceedings of the 26th International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "94",
      "title": "Perceptual cues in nonverbal vocal expressions of emotion",
      "authors": [
        "A Disa",
        "Frank Sauter",
        "Andrew Eisner",
        "Sophie Calder",
        "Scott"
      ],
      "year": "2010",
      "venue": "Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "95",
      "title": "Beyond the right hemisphere: brain mechanisms mediating vocal emotional processing",
      "authors": [
        "Annett Schirmer",
        "Sonja Kotz"
      ],
      "year": "2006",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "96",
      "title": "Narrative visualization: Telling stories with data",
      "authors": [
        "Edward Segel",
        "Jeffrey Heer"
      ],
      "year": "2010",
      "venue": "IEEE transactions on visualization and computer graphics"
    },
    {
      "citation_id": "97",
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "authors": [
        "Michael Ramprasaath R Selvaraju",
        "Abhishek Cogswell",
        "Ramakrishna Das",
        "Devi Vedantam",
        "Dhruv Parikh",
        "Batra"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "98",
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "authors": [
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "year": "2014",
      "venue": "Deep inside convolutional networks: Visualising image classification models and saliency maps"
    },
    {
      "citation_id": "99",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "100",
      "title": "Exploring and Promoting Diagnostic Transparency and Explainability in Online Symptom Checkers",
      "authors": [
        "Chun-Hua Tsai",
        "Yue You",
        "Xinning Gui",
        "Yubo Kou",
        "John Carroll"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "101",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "Jiehao Zhang",
        "Bjorn Schuller"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "102",
      "title": "Contrastive Explanations with Local Foil Trees",
      "authors": [
        "J Van Der Waa",
        "M Robeer",
        "J Van Diggelen",
        "M Brinkhuis",
        "M Neerincx"
      ],
      "year": "2018",
      "venue": "Proceedings of the ICML Workshop on Human Interpretability in Machine Learning"
    },
    {
      "citation_id": "103",
      "title": "Counterfactual explanations without opening the black box: Automated decisions and the GDPR",
      "authors": [
        "Sandra Wachter",
        "Brent Mittelstadt",
        "Chris Russell"
      ],
      "year": "2017",
      "venue": "Harv. JL & Tech"
    },
    {
      "citation_id": "104",
      "title": "Designing theory-driven user-centric explainable AI",
      "authors": [
        "Danding Wang",
        "Qian Yang",
        "Ashraf Abdul",
        "Brian Lim"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 CHI conference on human factors in computing systems"
    },
    {
      "citation_id": "105",
      "title": "Show or suppress? Managing input uncertainty in machine learning model explanations",
      "authors": [
        "Danding Wang",
        "Wencan Zhang",
        "Brian Lim"
      ],
      "year": "2021",
      "venue": "Artificial Intelligence"
    },
    {
      "citation_id": "106",
      "title": "StudentLife: assessing mental health, academic performance and behavioral trends of college students using smartphones",
      "authors": [
        "Rui Wang",
        "Fanglin Chen",
        "Zhenyu Chen",
        "Tianxing Li",
        "Gabriella Harari",
        "Stefanie Tignor",
        "Xia Zhou",
        "Dror Ben-Zeev",
        "Andrew Campbell"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing"
    },
    {
      "citation_id": "107",
      "title": "Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making",
      "authors": [
        "Xinru Wang",
        "Ming Yin"
      ],
      "year": "2021",
      "venue": "26th International Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "108",
      "title": "Nonparametric modeling of vibration signal features for equipment health monitoring",
      "authors": [
        "Stephan Wegerich",
        "Alan Wilks",
        "R Matthew Pipke"
      ],
      "year": "2003",
      "venue": "IEEE Aerospace Conference Proceedings"
    },
    {
      "citation_id": "109",
      "title": "Elegant: Exchanging latent encodings with gan for transferring multiple face attributes",
      "authors": [
        "Taihong Xiao",
        "Jiapeng Hong",
        "Jinwen Ma"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "110",
      "title": "Understanding the effect of accuracy on trust in machine learning models",
      "authors": [
        "Ming Yin",
        "Jennifer Vaughan",
        "Hanna Wallach"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 chi conference on human factors in computing systems"
    },
    {
      "citation_id": "111",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "Seunghyun Yoon",
        "Seokhyun Byun",
        "Kyomin Jung"
      ],
      "year": "2018",
      "venue": "IEEE Spoken Language Technology Workshop"
    },
    {
      "citation_id": "112",
      "title": "Visual interpretability for deep learning: a survey",
      "authors": [
        "Quanshi Zhang",
        "Song-Chun Zhu"
      ],
      "year": "2018",
      "venue": "Frontiers of Information Technology & Electronic Engineering"
    },
    {
      "citation_id": "113",
      "title": "Debiased-CAM for bias-agnostic faithful visual explanations of deep convolutional networks",
      "authors": [
        "Wencan Zhang",
        "Mariella Dimiccoli",
        "Brian Lim"
      ],
      "year": "2020",
      "venue": "Debiased-CAM for bias-agnostic faithful visual explanations of deep convolutional networks",
      "arxiv": "arXiv:2012.05567"
    },
    {
      "citation_id": "114",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Lijiang Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "115",
      "title": "Learning deep features for discriminative localization",
      "authors": [
        "Bolei Zhou",
        "Aditya Khosla",
        "Agata Lapedriza",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "116",
      "title": "Interpretable basis decomposition for visual explanation",
      "authors": [
        "Bolei Zhou",
        "Yiyou Sun",
        "David Bau",
        "Antonio Torralba"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "117",
      "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks",
      "authors": [
        "Jun-Yan Zhu",
        "Taesung Park",
        "Phillip Isola",
        "Alexei Efros"
      ],
      "year": "2017",
      "venue": "Proceedings"
    }
  ]
}