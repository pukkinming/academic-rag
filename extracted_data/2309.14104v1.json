{
  "paper_id": "2309.14104v1",
  "title": "Affective Game Computing: A Survey",
  "published": "2023-09-25T12:52:48Z",
  "authors": [
    "Georgios N. Yannakakis",
    "David Melhart"
  ],
  "keywords": [
    "Affective computing",
    "games",
    "player modelling",
    "affective loop",
    "survey",
    "taxonomy"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper surveys the current state of the art in affective computing principles, methods and tools as applied to games. We review this emerging field, namely affective game computing, through the lens of the four core phases of the affective loop: game affect elicitation, game affect sensing, game affect detection and game affect adaptation. In addition, we provide a taxonomy of terms, methods and approaches used across the four phases of the affective game loop and situate the field within this taxonomy. We continue with a comprehensive review of available affect data collection methods with regards to gaming interfaces, sensors, annotation protocols, and available corpora. The paper concludes with a discussion on the current limitations of affective game computing and our vision for the most promising future research directions in the field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "O VER a third of the Earth's population is playing games by now-with the projected number of gamers rising up to 3.3 billion by 2024  [1] . We could argue that game playing at this massive scale is probably the largest ongoing experiment of human behaviour and experience. The emotional patterns that a player goes through are deeply interwoven in the design of any game. That central role of affect interaction in this domain make games an ideal test-bed for the study of affective computing (AC)  [2] . Games, however, are not merely an important domain for AC. As a matter of fact, games have shaped and advanced the AC field in numerous ways given the unique challenges they pose and opportunities they bring to affective interaction.\n\nLooking at digital games through the lens of the affective loop  [3] , one can only observe benefits for AC research and innovation. When it comes to emotion elicitation, games define one of the richest forms of human-computer interaction and thus offer highly multimodal and dynamic ways to elicit affect. Moving on to affect sensing, the availability of game engine and sensor technology brings AC researchers unprecedented opportunities for measuring manifestations of affect way beyond physiology, verbal and non-verbal communication (e.g. game analytics and in-game social activity). Affect detection benefits from the massive gameplay corpora available in the wild e.g. over streaming services  [4] . Finally, affect adaptation in games can be achieved via highly diverse stimuli that vary from AI-controlled expressive agents to content generators of various types  [5] ,  [6] .\n\nIn this paper, we survey the emerging research area at the intersection of affective computing and games, namely affective game computing. In particular, we build on the affective loop paradigm  [3] ,  [7]  (see Section II) and survey core contributions in the areas of affect elicitation, sensing, detection and adaptation in games (Sections III-VI). We use indicative examples from both the game industry and academic research showcasing the advancements of AC through games but also the benefits AC offers to games and their development. Throughout our survey, we identify core terms, methods and approaches that we, later on, use to situate the affective game computing field as a whole (Section VII). Moreover, our survey puts an emphasis on state-of-the-art data collection methods in games relating to interfaces, sensing devices, and annotation protocols (Section VIII) and the available affect corpora (Section IX). The paper concludes with a detailed list of current limitations of the available methods and technologies (Section X) and outlines a number of promising future research directions for affective game computing (Section XI). We feel (and hope) that all aforementioned studies, methods, resources and tools contained in this survey paper will serve as a guide for affective game computing researchers and will also lower the entry bar for any newcomer to this emerging research and innovation field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Contributions Of This Paper",
      "text": "The field of affective computing in the domain of games has been studied extensively over the last 15 years. The literature is rich in this application area-as one can observe through the volume of references in this paper. Despite the variety and breadth of the studies covered, however, only a few papers have reviewed this research field in a comprehensive and detailed manner to the degree this paper does. Indicatively, an early short survey of the field focused on the relationship between emotion and games  [8]  and introduced the concept of the affective loop in games. The edited volume Emotion In Games  [9]  provides broad coverage of several aspects of affective computing research in games but it does not survey the field comprehensively and in a systematic fashion. In  [10]  Yannakakis and Togelius offer an entire chapter on player modelling-the use of computational means to capture aspects of playing behaviour and experience-which touches upon some of the aspects covered here. Two more recent relevant surveys include the work of Robinson and Clore  [11]  who examined the use of physiological sensors in the field of human-computer interaction and Navarro et al.  [12]  who surveyed biofeedback interaction specifically in videogames for general entertainment.\n\nIn contrast to all the aforementioned attempts, this paper introduces the affective game computing field and surveys it in a holistic, systematic, and comprehensive manner. In particular, the paper covers both industry and academic examples, and it offers a taxonomy of terms and methods through which we can map the critical studies in the field. Furthermore, the paper surveys existing corpora, methods and annotation protocols, provides guidelines for the newcomer in this field and discusses the next most promising research steps forward.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Affective Game Loop",
      "text": "The affective loop first introduced by Sundström  [7]  and Höök  [3]  has become a dominant AC paradigm that is able to represent any affective interaction in a general fashion. The affective loop comprises four core sequential phases that enable an affective interaction: affect elicitation, affect sensing, affect detection and affect adaptation. When the affective loop principle is applied to games the resulting paradigm has been defined as the affective game loop  [8]  (see Fig.  1 ). Before delving into the details of the AC and games survey, in this section, we outline the key elements of each phase of the affective game loop as follows.\n\n1) Elicitation: In this initial phase of the loop the game yields affective responses to players via a multitude of available affect elicitors such as game agents and game content. We detail those elicitors and survey the corresponding literature on game affect elicitation in Section III. 2) Sensing: Once affect is elicited, players manifest it in numerous ways. The second phase of the affective game loop is responsible for sensing those manifestations via sensor and tracking technology as detailed in Section IV. 3) Detection: Given appropriate signals obtained from the sensed multimodal player input and human demonstrations of affect (such as player experience annotations) one can build mathematical formulations (e.g. via statistical machine learning) that are capable of inferring the annotated affect accurately based on the user signals.\n\nDetails on game affect detection methods are provided in Section V. 4) Adaptation: In the last phase of the affective game loop the game is required to offer the next sequence of ingame stimuli so that the experience of the player is set within predetermined bounds. We survey methods and studies on game affect adaptation in Section VI.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Game Affect Elicitation",
      "text": "As mentioned earlier, games are equipped with a rather diverse set of stimuli that are capable of eliciting a wide spectrum of emotions in players. In this section, we provide a taxonomy of such stimuli independently of the affective states they might be able to elicit. Importantly, the context of the game environment, the game genre, the form of interfacing, the number of players, potential social aspects of the game, and the overall objective of the game are foundational and they impact any other in-game elicitor covered here. For our taxonomy of elicitors we largely adopt and build upon the taxonomies introduced by Yannakakis and Togelius  [10] . In particular, as summarised in Table  I  we identify three categories of affect elicitors in games namely game context, game agent and game content. We discuss these categories in detail below followed by an indicative example of affect elicitation in games in Section III-A.\n\nGame context refers to the game's genre, the platform used, and the game's characteristics that collectively define the momentaneous state of the game. A player can obviously affect the dynamic aspects of the game context (i.e. the game state) and vice versa, the game context can affect the gameplay and elicit affect patterns. The importance of game context is critical for player affect modelling as the context of the game needs to be considered for reliable affect detection. Simply put, any player's reactions cannot be dissociated from the stimulus that elicited them. Following the taxonomy introduced in  [10]  the core game characteristics that fall under the game context group include: the number of players, the observability of play, the stochasticity of the game, the time granularity, and the action space for the player (see Fig.  2 ). Under the game agent category, we fit any affect elicitor related to AI-controlled agents that might be available in the game. Examples include agent facial expression, verbal, and non-verbal agent behaviour, social agent behaviour, and individual agent behavioural patterns that can affect a player's experience. Finally, game content refers to any content type existent in a game that is not related to AI agent behaviours as those are covered in the game agent category (i.e. the virtual environment  [13] ). Building on the categorisation of Liapis et al.  [5]  each game is viewed as a synthesis of creative facets available which include the design of the games (i.e. rules and mechanics), the game level, and the creative ways a human plays the game (i.e. gameplay). Depending on the game available content types may include visuals, audio, narrative, and even novel control modalities  [14] .  All the above-mentioned elicitors can affect the experience of play and are met in various configurations in games. It is rarely the case-as in most other domains of AC-that only one elicitor type (e.g. an image, or a sound) is active at a time. In games, instead, affect elicitors are orchestrated  [6] ,  [15]  in groups thereby offering rich and multifaceted affective interactions  [14] -  [19] . As an example of such an orchestration process think of a scary story that is told by an expressive agent who is placed in a dungeon level with the corresponding visual and audio effects, and the appropriate virtual camera placement, and lighting.\n\nIt is important to note that the game context is static meaning that no aspect of it can be altered by the player or the game. As a result, the game context cannot act as a dynamic affect stimulus when the affective loop reaches its adaptation phase. One could think of games that change their genre, their mode of interaction (e.g. from VR to desktop)  [20]  and the number of players but those alterations are rare. Therefore in this section, instead of surveying the existing literature with respect to all possible affect elicitors in games, we will survey only the game context category. We will then survey AI agents and content in Section VI as these two categories include dynamic stimuli that can be altered during the game affective interaction.\n\nAspects of the game context have been predominantly included as a modality of input for affect detection in games (e.g.  [21] -  [30]  among others). In most of these studies (e.g.  [23] ,  [24] ,  [30] ) the game context does not refer to any of the aspects included in Table I but rather to aspects of the game environment such as level features. We could argue that the game context aspects covered in Table I become highly relevant for affect detection once one performs studies on general affect modelling  [31] -  [34] . Very few studies have explored modalities of user input in isolation of the game context. For example, Makantasis et al.  [35]  built models of affect based solely on physiology for the purpose of comparison against models that fused aspects of in-game video and audio.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "A. Indicative Examples",
      "text": "In this section, we will provide an indicative example of affect elicitors that have been used in a game that realises the game affection loop. In particular, we will outline the StartleMart Post-Traumatic Stress Disorder (PTSD) game (see Fig.  3 ) which was designed and developed as a form of virtual exposure therapy  [36] ,  [37] . The game adapts to the level of stress of PTSD patients-as measured via their skin conductance-by triggering certain auditory and visual stimuli including war sounds, stressful social settings and war flashback moments. When it comes to the context of the game, StartleMart can be characterised as a training game with a health purpose (i.e. PTSD treatment) (genre) that takes place in a supermarket, played in desktop (platform) by a single player (number of players). The game is partially observable (observability) as it features a first-person view over a 3D level, it is deterministic (stochasticity) and continuous (time granularity), and the player actions are limited to moving around in a continuous environment and picking objects (action space). The game context as described above acts as a mild stressor for patients suffering from PTSD. A pre-selected number of in-game stimuli-provided in the form of audio, visuals and video cut-scenes-act as personalised intense stress elicitors. More details about StartleMart can be found in  [36] -  [38] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Game Affect Sensing",
      "text": "Moving on to phase 2 of the affective game loop in this section we will first survey the multiple ways we can sense how a player feels based on their manifested emotions (see Section IV-A) and we will then survey the available methods Figure  3 . Screenshots of StartleMart  [36] ; a biofeedback game designed as a virtual stress inoculation and exposure therapy tool.\n\nwe can obtain affect annotations and labels in games (see Section IV-B). From an affect modelling perspective, we will first focus on the input of the model and then cover its output. Similarly to Section III, we start by introducing our taxonomy, then survey the corresponding literature and end with an indicative example that is presented in more detail.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "A. Sensing The Input",
      "text": "Player emotional manifestations may be sensed through variations of gameplay patterns, alterations in a player's attention, level of focus, and changes in the player's physiology, facial expression, posture, and speech. Monitoring such alterations may assist in recognising and constructing the player's model.\n\nAny affect model of players relies on data of the manifested affective experience. Such data define the input of the affect model and can be obtained directly through the game engine or with the help of additional sensors. These sensors are either available through the various platforms (e.g. eyetracking featured in a VR headset) or they are integrated into the game (e.g. a skin conductance sensor that interfaces with the game)-for a recent survey of physiological sensors in affective game research see also  [11] . Following the player modelling taxonomies of  [8] ,  [10]  we argue that sensing affect manifestations in games can be of the two main categories:\n\ngameplay and objective sensing. The two categories are detailed in the remainder of this section and summarised in Table  II .\n\n1) Gameplay: As games affect the player's cognitive processing patterns and cognitive focus our core assumption is that players' in-game behaviour is linked directly to their experience. As a result players' affect can be derived through the analysis of their in-game interaction patterns considering game context variables  [39] ,  [40] . Gameplay refers to anything a player does in a game environment which is collected via in-game logs of any type such as user interface selections, preferences, or in-game actions. In particular, gameplay includes any aspect that can be derived from the interaction between the player and the game directly. Such aspects-also broadly known as player metrics  [41] ,  [42] -include detailed attributes of the player's behaviour which are based on interactions with game elements such as objects, non-player characters, and levels. Popular examples of attributes that are directly linked with gameplay include spatial locations of players and key events viewed as heat maps  [43]  trajectories or aggregated descriptive data  [32] , communication with other players or an audience  [4] , all the way to pixel colours of the game's footage  [33] ,  [34] .\n\nA key limitation of gameplay is that the player affect is only observed indirectly. For example, a player that shows limited interaction through their logged gameplay data could be either planning their next quest, talking to their friend over the phone or even feeling bored with the game. It is also important to note that affect models that are derived from gameplay data do not necessarily generalise across all players and games. Therefore, it is crucial that affect models are fine-tuned to the needs of players, and manifest gameplay experiences of player personas  [44]  and ultimately individual players. One might even argue that gameplay data is not even relevant for particular games or players as the ad-hoc design of the gameplay attributes might not be useful for capturing certain aspects of player affect.\n\n2) Objective: This category refers to any signals available as a response to in-game stimuli. In particular, objective ways of sensing affect include physiological signals-electrodermal activity (EDA), electrocardiogram (ECG), electromyogram (EMG), electroencephalogram (EEG)  [45] -  [47] -camerabased signals including facial expression, head pose, gestures and body movement  [48] ,  [49] , and verbal signals including speech and body movements.\n\nThe analysis of physiological manifestations of psychology (i.e. psychophysiology) is well studied by now; see  [11] ,  [28] ,  [50] -  [52]  among many. It is widely evidenced that arousing or tense events cause dynamic changes in both sympathetic (increase) and parasympathetic (decrease) nervous systems whereas low arousal (e.g. relaxing or resting) states increase the activity on the parasympathetic nervous system. Such activity may cause observable alterations, for instance in a player's facial expression, head pose, and EDA  [53] ,  [54] . A significant body of literature has focused on the relationship between a player's physiology in response to their gameplay patterns  [21] ,  [55] -  [61] , relying on ECG  [62] , photoplethysmography  [60] ,  [62] ,  [63] , EDA  [36] ,  [38] ,  [57] ,  [64] , respiration  [60] , EEG  [45] ,  [65] -  [67] , and eye movement   [68] ,  [69] . In addition to physiology, the player's bodily expressions can reveal real-time affective responses from the gameplay stimuli. Such input modalities, have been explored extensively in games and include facial expressions  [70] -  [74] , muscle activation  [22] ,  [75] , body movement and posture  [68] ,  [70] ,  [76] -  [78] , haptics  [79] , and gestures  [80] , On a higher level, objective input can be viewed largely as either verbal or non-verbal. Verbal input includes speech-based modalities  [81] -  [85] , while no-verbal input may rely on text  [86] -  [88]  or any of the above-mentioned modalities. The limitations of objective inputs are several. First, most of the sensors are not available in a player's natural habitat (i.e. in the wild); second, most sensors are intrusive, thereby, affecting gameplay at large; third, the signals obtained are usually noisy due to environmental conditions and hardware limitations. We discuss these limitations in more detail in Section X.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Sensing The Output",
      "text": "The output of any affect model is usually a set of particular states (e.g. happy), a scalar (e.g. the emotional dimensions of arousal and valence), or an ordinal relationship (e.g. tension is higher now than before). Sensing the output is predominately achieved through a subjective annotation process by the player themselves (first person) or by others (third person) such as game experience designers, peers or external observers. Those annotated labels ideally need to be as close to the ground truth of playing experience as possible  [10] . Sensing the most reliable affect labels for players is a tedious and laborious task which defines a challenge in its own right. The area of affect annotation is long studied in the literature, yet there are still many open research questions left for the design of the ideal annotation collection protocol. First, who provides the labels (first or third person); second, is player experience represented as states or instead as intensity/magnitude; third, should annotations be provided in discrete time periods or continuously; fourth, should the annotators be asked to give a magnitude label (e.g. frustration is 0.8) or an ordinal relationship (e.g. frustration is higher in this level segment). Some of these questions are addressed in the remainder of the paper; particularly Sections V and VIII.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Indicative Examples",
      "text": "At the moment of writing, there are a number of examples of commercial games that utilise physiological input from players. One particularly interesting example is Nevermind (Flying Mollusk, 2015), a biofeedback-based adventure horror game that adapts to the player's stress levels by altering the game's content including visuals and sounds (see Fig.  4 )  [89] . A number of sensors which monitor the player's heart rate variability, skin conductance, facial expressions and gestures are available for affective interaction with the game. Our second affect sensing example comes from a collaborative project between academic and industrial partners named Apex of Fear (2022)  1  . Apex of Fear is a VR horror game experience that adapts to its players' fear levels in real-time based on their psycho-physiological measures. The multimodal sensing capacities of Apex of Fear include electrodermal activity, respiration, electrocardiography, electromyography, eye tracking, and in-game events (see Fig.  5 ). The game is currently in its beta testing phase through which user data is collected for constructing reliable models of in-game fear based on the physiological manifestations of players.\n\nBoth of these examples focus on sensing the input rather than the output. While surveys and continuous annotation tools to measure player experience are used extensively in games research  [90]  and game industry user research  [91] , they rarely show up in commercial games. A possible explanation of why this happens is that most commercial games are presented as black boxes to the players. Game designers value immersive experiences highly  [92] , therefore, pointed surveys that measure the output of emotions directly can uncover the inner workings of game systems, revealing the smokeand-mirror nature of games. Recent advancements, however, within large-scale language models (LLMs) point towards a promising direction incorporating human affect labels for shaping and defining large-scale affect models in games. As shown by Lambert et al.  [93] , reinforcement learning from human feedback can greatly enhance the performance and   personalise the output of large foundation models such as GPT-3  [94]  and GPT-4  [95] . Human feedback may take the form of like/dislike labels or preferences among options (i.e. generated texts and/or images).\n\nEven though general like/dislike labels already encode a form of affective feedback, detailed survey methods used in the games industry could inform future affect models in games. A good example of such a survey is the Ubisoft Player Experience Questionnaire (UPEQ)  [96]  designed to assess the motivational drives of players. The UPEQ questionnaire has been used successfully to model player's motivation based on simple behavioral patterns of players in the role-playing thirdperson shooter game Tom Clancy's The Division (Ubisoft, 2016)  [97]  (see Fig.  6 ).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Game Affect Detection",
      "text": "In the third phase of the game affective loop, a computational model is requested to detect player affect based on the player modalities available. Because this affect model is trained to predict labels, the task of affect detection is largely viewed as a supervised learning paradigm  [51]  through which measurable attributes of the player (model's input) are mapped to the player's affect state (model's output). Any supervised learning method can be used for inferring such a mapping, including decision trees and random forests, support vector machines, and shallow or deep neural network architectures. The data type of the affect label available determines the output type of the model and, in turn, the machine learning approach that is applicable. Numerical data can be modelled using regression methods whereas nominal variables, such as emotion categories or arbitrary bins of numerical data (e.g. high vs low values based on a split criterion) are modelled via classification methods. Finally, ordinal observations (e.g. pairwise preferences or forced choices) can be trained via preference learning methods. It is also possible to create ordinal observations from numerical values (e.g. change in score) or even from nominal values in some cases (e.g. arousal intensity of labelled emotions)  [98] . We detail the three supervised learning types below (see Table  III ).\n\nWhen the affect labels that need to be predicted are interval, affect modelling can be achieved via regression algorithms including linear or polynomial regression, artificial neural networks and support vector machines. Modelling player affect via regression has certain limitations and will most likely yield unreliable models as regression assumes that the target value to be predicted follows a numerical scale. A number of comprehensive studies  [98] -  [104]  provide sufficient evidence against the use of regression for player experience modelling. Values obtained via magnitude-based annotation (e.g. ratings) should instead be converted to ordinal values via the secondorder process described in  [98] .\n\nIf instead of numerical values player affect is represented as a set of classes (e.g. high vs low engagement) any classification method is applicable for learning to predict affect. Classes can represent both aspects of player experience-e.g. excited or frustrated player-and aspects of playing behaviour such as quest completion times (e.g. low vs. high completion time). Classification is ideal for modelling player experience if discrete annotations of experience are available as target outputs  [105] -  [108] . Converting numerical values to classes (e.g. convert arousal annotations between 0 and 1 to low vs. high arousal) might introduce data biases which, in turn, might prove to be detrimental for modelling player affect  [98] ,  [99] ,  [109] .\n\nAlternatively to regression and classification, preference learning  [98] ,  [110]  methods can learn to predict affect from ordinal data such as affect ranks or preferences. The target values in the preference learning paradigm provide information for the relative relation between instances of the label we attempt to learn. For instance, labels are obtained by comparing two game levels in terms of engagement  [111]  or they can be retrieved via ordinal processing of an arousal trace  [90] ,  [112] . Largely speaking, the data analysis of ordinal labels follows the first-order process described in  [98] . A large palette of algorithms is available for the task of preference learning including linear statistical models and non-linear approaches such as Gaussian processes  [113] , deep and shallow artificial neural networks  [114] -  [117] , neuro-evolutionary methods  [118] , and support vector machines  [119] . Many of those methods are available in online accessible tools  [120] .\n\nGrounded in extensive studies available in the literature  [98] ,  [99]  and supported by contemporary research  [117] ,  [118]  the selection of a supervised learning approach for modelling player affect becomes obvious. We argue that preference learning is a superior supervised learning method for player affect modelling, classification provides a good balance between simplicity and approximation of the ground truth of player experience whereas regression is based on numerical affect annotations which, in turn, yield models of questionable validity and reliability.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Affect Detection As Reinforcement Learning",
      "text": "The affect detection task is traditionally viewed through the lens of supervised learning. However, alternative approaches have emerged beyond this paradigm recently. In particular, methods from reinforcement learning have shown great potential for modelling player affect (see Table  III ). The key motivation for the use of reinforcement learning (RL) for modelling player affect is that it can capture the relative valuation of affective states as encoded internally by humans during play  [121] . According to the RL perspective for modelling players, the derived RL policy can capture internal player states with no corresponding absolute target values such as decision-making, learnability, cognitive skills or style  [122] . The player model that is built via an RL process is expected to offer a psychometrically-valid, abstract simulation of a human player's internal cognitive and/or affective processes. An agent equipped with such a model can be used to interpret human play, or featured in AI agents which can be used as playtesting bots or as believable human-like opponents  [123] -  [125] .\n\nThese non-traditional ways of detecting players' affect are still in their infancy with only a few studies existent in racing and Atari-like 2D games  [123] ,  [124] ,  [126] , racing games  [125] , serious games  [127]  and first-person shooters  [121] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Indicative Examples",
      "text": "We use racing games as the genre of the indicative examples we discuss in more detail here. Focusing on the game industry, the Drivatar imitation learning system of the Forza Motorsport series (2005-2022, Microsoft Game Studios) is the longeststanding AI within a game title. Building on a set of simple rules and behavioural cloning techniques back in 2005 Drivatar has evolved to feature deep neural network approaches (in 2022) that imitate the way any player drives a car and Figure  7 . A screenshot of the Solid Rally racing game  [128]  (top) and the generated behavioural and arousal traces of an RL agent that learns how to play like and \"feel\" like a human expert player (bottom)  [124] ,  [128] .\n\nsimulate the player's driving style 2  . Even though modelling in Drivatar relies on behavioural human demonstrations it is indicative of what player affect modelling can achieve in commercial-standard games if human affect demonstrations can be provided.\n\nMoving from the game industry to a recent research example in the area of affect modelling the work of Barthet et al.  [124] ,  [125] ,  [128]  is worth outlining. The authors in that series of papers import the highly successful reinforcement learning algorithm Go-Explore  [129]  to the field of affect modelling in their attempt to model both behavioural and affective patterns of play. The resulting algorithm, namely Go-Blend, is able to blend human demonstrations of behaviour and affect in a common representation thereby allowing for believable playtesting. The generative Go-Blend agents are able to play and \"feel\" like human players of racing games by imitating their play and arousal traces (see Fig.  7 ).",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Game Affect Adaptation",
      "text": "Computer games-as opposed to traditional media content such as images and videos-are interactive media that continuously react to the users' input. This interactivity can naturally accommodate mechanisms for real-time adaptation of game content aimed at adjusting player experience and realising affective interaction  [130] . One of the main reasons we can achieve meaningful affect-driven adaptation in games is because players are prepared for personalised experiences more than in any other form of human-computer interaction  [8] . The relationship of players to adaptation mechanisms in games is highly dependent on their playing style, mood, experience, personality, and on the efficiency of the adaptation with regards to player needs.\n\nThe last phase of the game affective loop involves the adaptation of in-game elements for eliciting particular affective patterns. We refer to such elements as actionable as they are linked and can directly affect player experience. Games may evolve and adapt to the player in many different ways and convey emotions through a variety of techniques and effects. Any adaptation process that will eventually close the affective interaction between the player and the game should be able to decide which stimulus (or playful experience) will be presented next, when it should be presented, which actionable game elements should be adjusted, and how  [8] ,  [131] ,  [132] -for a recent survey on biofeedback interactions see  [12] . Viewing affect-based adaptation from a high-level perspective it appears that the game can adjust its agents-(or non-player characters) if those are available-or adjust its content to the affective needs of its player(s). Both of these actionable in-game element types can be manipulated in ways that lead the player to become more emotionally involved with the game. We review these categories in the remainder of this section.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Agents",
      "text": "Several games feature agents or non-player characters that may act as opponents, collaborators, or assistants  [10] . Independently of the type of agent and its role such agents might be required to express emotion during their interaction with the player(s) and elicit specific emotional patterns. Emotion expression can be achieved in a completely scripted manner (e.g. behaviour trees) all the way to machine-learned behavioural generation approaches (e.g. procedural personas). Approaches may rely upon popular emotion agent architectures  [133] -  [135] , underlying cognitive models  [136] , or personality trait models  [137] .\n\nIt is important to note that emotion modelling plays a dual role when it comes to game agents: emotions both guide an agent's decision-making capacities but they also affect the expressions of different emotional states (e.g. fear or sadness). Procedural animation is key for the latter with several impressive breakthroughs achieved for real-time animated characters  [138]  via generative systems  [139] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Content",
      "text": "Not all games feature non-player characters but all games have some form of content such as visuals, audio, game rules, game levels, and narrative; those content types can be defined as the creative facets available in games  [5] . An affect-driven adaptive process could in principle alter those creative facets independently or in an orchestrated manner  [6] ,  [10] . The area widely known as procedural content generation (PCG) has offered several methods varying from simple search-based methods  [140]  all the way to deep learning-based approaches  [141]  for the task. Of particular importance for the scope of this paper is the experience-driven PCG framework  [130] ,  [142] , which views game content as an indirect building block of player affect and proposes mechanisms for synthesising personalised game experiences. Game content adaptation that may affect the emotional patterns of players can take the form of game rules  [143] , difficulty  [144] , lighting  [145] , camera profiles  [62] , maps  [146] , levels  [147] , tracks  [148] , narrative structures  [149] , and music  [131] -among many other content types.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Indicative Examples",
      "text": "Affect adaptation in the games industry usually takes the form of Dynamic Difficulty Adjustment (DDA)  [10] ,  [151] ,  [152] . In DDA, certain agent properties (e.g. enemy's health, speed and position) are predominately altered to match the skill of the player. Rubber-banding in racing games (e.g. in the Forza series mentioned earlier) and difficulty scaling in games like the Resident Evil series (Capcom, 1996-2023) are among the most popular methods for DDA in the games industry. While adaptation in games considers primarily a player's behaviour, there are games such as Left 4 Dead (Valve, 2008) that follow the experience-driven PCG  [130]  paradigm and, thereby, consider aspects of in-game tension and player's emotional intensity to adapt the game  [153] . The game's AI Director observes the players' performance, follows a predetermined tension curve and modifies the ingame experience by altering a number of content types: the number and location of opponents (zombies), the pacing of the game and audiovisual effects  3   As an indicative research example in affect-driven adaptation, we will cover Experience-Driven (ED) PCG via Reinforcement Learning (RL) or EDRL for short  [154] ,  [155] . EDRL is able to generate various facets of game content (e.g. game levels and gameplay patterns) that follow particular experience patterns via RL methods; see Fig.  8 . More specifically, the method was applied for the online and endless generation of game levels and gameplay patterns in the game of Super Mario Bros. Inspired by Koster's theory of fun  [150] , reward functions were formulated as moderate degrees of level or gameplay divergence. The resulting multifaceted EDRL is not only capable of generating fun levels efficiently, but it is also robust with respect to dissimilar playing styles and initial game level conditions.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Vii. A Holistic View Of Affective Game Computing",
      "text": "This section provides a holistic overview of the field of affective game computing, focusing on state-of-the-art research, active areas, and research gaps in the field. Beyond the survey above, here we aim to show key examples of research across each phase of the affective loop. While our survey gave an overview in light of the larger field of affective computing and user modelling, our holistic overview highlights papers with a videogame focus. First, we outline the methodology we used (Section VII-A) and then we analyse our findings (Section VII-B).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Methodology",
      "text": "Sections III-VI pull knowledge from the last two decades of research in affective game computing, This section, instead, complements our literature review with a survey of recent papers (i.e. Table  IV  shows the outcome of this survey complemented with seminal works over the last 20 years of affective game computing. The table is organised based on the core aspects of the affective game loop-elicitation, sensing, detection, and adaptation. While many of the presented papers utilise multiple-if not all-aspects of the affective loop, we have selected papers that focus on a specific phase of the loop with the corresponding high-level categories defined in earlier sections of the paper.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Analysis Of Findings",
      "text": "In this section we discuss overall observations regarding the papers identified under each one of the four main phases of the affective loop (Table  IV .) Elicitation encompasses papers that focus on the emotional impact of games and their measurement. We divided these papers based on the game facet they consider. While many studies seem to put an emphasis on the game context, most of them focus on specific genres and platforms. In particular, the horror genre  [15] ,  [17] ,  [156]  provides a popular research test-bed, possibly due to the visceral emotions that emerge during consuming horror media. VR and AR games are also popular for studying the impact of gameplay context on player experience  [20] ,  [157] ,  [158] . Even though there are some studies dedicated to the ways the action space of the game affects the player experience  [14] ,  [28] , there are no studies found that focus on the impact of the number of players, observability, stochasticity, and time granularity on player affect. When it comes to AI agents, there is surprisingly little research dedicated on how these agents can be used for emotion elicitation despite the field of believable non-player characters  [167] ,  [168]  being very active. Unsurprisingly, most studies with a primary focus on affect elicitation are investigating the impact of the game content and the game environment  [13] , virtual objects  [19] , sounds  [18] , game events  [28]  or multiple facets of content  [6]  on affect elicitation. Summing it up, it appears that there are two major research gaps identified in game affect elicitation. First, there is lack of fundamental research into how components of the game context affect the player experience. Second, there is an untapped opportunity to utilise the research that has gone into creating more emotive and human-like agents for emotion elicitation in affective game computing studies.\n\nSensing appears to be a well-researched phase, with a special focus on physiology and peripheral signals (e.g.  [11] ,  [12] ). Similarly, thanks to the ubiquity of web cameras, camera-based methods are also popular  [48] ,  [69] . While gameplay on its own can be a strong predictor of the player experience-and it is often the only modality available in the wild  [42] -most studies in affective game computing aim for a multimodal approach instead. Finally, the field of voice-based sentiment and emotion analysis is overshadowed by more traditional user modalities such as physiological signals. This latter finding is surprising as in the world of eSports  [162]  and streaming services, access to player voice and commentary are easier than ever. Regarding subjective measures, game affect sensing appears to adapt traditional affective computing labelling techniques to games-from annotation tools  [90] ,  [112]  to Likert-like surveys  [96] -but there is relatively little research effort put on the design of game-specific annotation tools-with VR and AR platforms being the exception  [169]  due to the involvement required from players.\n\nAlmost all studies presented involve some form of detection as it is the machine learning aspect that often aids the evaluation of other components of the affective loop. Most studies with a primary focus on detection traditionally involve supervised learning. While regression is still used from time to time for predicting aspects of player behaviour (e.g. purchase decisions  [170] ), when it comes to affect modelling, the vast majority of the surveyed research studies have a preference for classification methods. This is despite a long line of research advocating for the ordinal processing and modelling of emotions  [98] ,  [99] . More interestingly, we observe a contemporary increase of interest in affect-driven reinforcement learning  [122] ,  [124] ,  [125] . Seeing how imitation learning algorithms are already being used in the games industry (see Section V-B about Forza Motorsport), we expect a rapid growth of interest in RL-based affect detection.\n\nFinally, we take a look at studies mostly linked to the phase of adaptation (bottom row of Table  IV ). One could argue that most commercial games that use some type of player-based adaptation employ a form of the affective loop. Similarly in academia, the research area of game affect adaptation is getting some serious traction in recent years. As Table  IV  shows, there are plenty of research projects that focus on affect adaptation and intelligent interaction based on sensing and emotion detection. Unsurprisingly, similarly to studies focusing on elicitation, most studies involving game affect adaptation focus on the game content instead of interactive agents  [139] . While the holistic approach of experience-driven procedural content generation allows for more in-game adaptation opportunities  [130] , the lack of emotive agents in affect adaptation reveals a notable gap in the research field. Despite the successful affectadaptive methods showcased in the literature, the widespread adoption of these techniques in commercial-standard games is still in its infancy. One of the biggest obstacles in this regard appears to be the field's reliance on often intrusive biosensors. While most physiological sensors can provide valuable multimodal data, such data are often very hard to reliably capture in the wild. We expect that future research avenues will investigate methods that would allow rapid deployment in games via camera-based technology or via the use of useragnostic models-e.g. learning from in-game footage pixels or via privileged information  [34] ,  [35] .\n\nThis section completes the survey and taxonomy of affective game computing. In the second part of this paper (as initiated in Section VIII) we survey the tools available for reliable data collection in games and review the various game affect corpora that are currently available (Section IX).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Viii. Collecting Affective Data In Games",
      "text": "Many tools for affective computing research can support games user research applications; however, games impose a number of unique challenges. Indicatively, different gaming interfaces provide different affordances both for play and data collection. Therefore, any dissimilarities across game controls and platform configurations would likely yield discrepancies in the collected telemetry and peripheral signals. Keeping a focus on affective data collection the remainder of this section provides an overview of popular gaming interfaces (Section VIII-A), and available tools for sensor data collection (Section VIII-B) and annotation (Section VIII-C).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "A. Gaming Interfaces And Telemetry",
      "text": "When it comes to selecting a game interface for an affective game computing experiment there are several aspects that need to be considered. Desktop computers provide the most straightforward way of data collection. Since players have to be seated in front of a computer-operating with a keyboard and a mouse-they are generally less obstructed. This makes desktop setups ideal for laboratory studies. Designing and running experiments from a desktop computer also implies an easier integration between the experimental games and the research software. Due to the generally similar setup, crowd-sourcing methods for data collection can be relatively reliable. On the flip side, desktop computers come in many different hardware configurations, which can affect the game performance, user experience, and quality of the data. Some of the available consoles, instead, such as the Microsoft XBox 4  , the Sony PlayStation 5  , and the Nintendo Switch 6  include specialised gaming hardware. Due to the standardised hardware and software, the collection of data related to player experience on these platforms can be more consistent. These specialised systems, however, also pose a major limitation as it is generally hard to integrate console software with researchbased hardware such as biosensors; the latter has often to be operated from a separate desktop computer.\n\nAs both desktop and console games are generally played by sitting in front of a monitor, the collection of traditional in-game behavioural telemetry, facial features, eye tracking information, and biosignals is relatively easier. In contrast, games that require full body movement are harder to integrate with more intrusive biosensors-especially the ones that involve electrodes and wires. Nevertheless, these games (and the platforms they are played on) provide unique affordances. Thanks to advances in computer vision and wearable technology, several exer-, VR-, and mobile games already integrate peripheral sensors into their systems.\n\nMobile games use regular smartphones as their platform. Due to the interactions afforded by these phones, the collected telemetry can cover an input space different to traditional consoles and desktop computers-such as tapping patterns and gestures. Many smart accessories are readily interfacing with smartphones, making it easy to collect peripheral data from different biosensors-such as photoplethysmography obtained via a smartwatch. The drawback of the platform is the uncertainty and obfuscation of the gameplay context. Notably, mobile games usually are played across many different environments for very short periods of time or with irregular breaks during the experience.\n\nExer(cise)-games exist on both console and mobile platforms. They usually rely on peripheral sensors that collect body movement and biosignals  [20] . The particularity of such games with regards to affective game computing is that they often employ a number of sensors to capture various modalities of the player. As players of exergames tend to (and have to) move their body throughout the game, the placement and arrangement of any physiological sensor poses a major challenge. Beyond the obstructed play, the motion and experimental artefacts that are embedded in the collected data make data cleaning and processing a non-trivial task  [171] .\n\nBeyond console, desktop and mobile platforms, VR and VR platforms-which become increasingly popular over the last few years-open a promising avenue for rich multimodal data collection in games; the Apex of Fear (2022) project covered earlier is one such example. VR Headsets such as the Meta Quest 7  , Valve Index 8  , HTC Vive 9  , and HP Omnicept headset 10  may provide a unique 360 • immersive experience  [158] . The VR headsets available generally block almost all obstructions coming from the physical environment and many can be used for multimodal data collection including gaze and physiology (see more discussion in Section X).\n\nGenerally speaking, affect data collection based on passive affect elicitors, such as images, is usually combined with signals obtained via physiological and other peripheral sensors. In games, however, it is a dominant practice to collect data from in-game telemetry in addition to any other sensor data and independently of the game platform used. This type of behavioural data can be very powerful, especially in multimodal applications. The form of such data, however, highly depends on the given game and purpose of collection. While some game engines offer standardised methods to collect various types of user data, these methods often focus on data relating to monetisation (see for example the Unity Analytics 11  ).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "B. Sensing Devices And Tools",
      "text": "One of the most comprehensive surveys of affective computing research and development tools to date shows that even though there is a continuous development of new tools, most studies end up using custom-made solutions  [172] . This is also largely true for data collection, labelling, data processing, and machine learning platforms and tools, and it makes intuitive sense, especially when it comes to collecting data from games.\n\nCollecting physiological sensor data is typically achieved through a vendor-based solution. The market is filled with reliable solutions for EEG, EDA, HR, and eye-tracking sensors. Some of the popular vendors include Emotiv 12  , Empatica  13  , Intel Realsense 14  , Plux 15  , Polar  16  , Shimmer  17  , and Tobii  18  , among many others. Most of these vendors offer processing software as well, although open-source solutions do exist. A recent survey on the field of affective game research showed that the most popular modalities employed include heart activity, followed by facial recognition and EDA  [11] . Beyond specialised sensors, conventional cameras are also often used to collect recordings of subjects. Vendor-based solutions exist to process such multimodal data (i.e. Affedex  [173]  for facial recognition); researchers, however, may prefer to use open source tools such as OpenCV  19  for general computer vision tasks or OpenFace  [174]  for facial recognition.\n\nWhile most affective computing methods can be transferred to game research without any major hiccups, sensing technology, in particular, brings two core challenges to the table. First, playing is usually physically active-even if the user plays on traditional platforms using a controller-which means that any sensor used for data collection has to afford a degree of comfort and free movement. Second, because games generally impose a high level of cognitive load on the user, there is an expected loss of affect expressivity. Due to this limitation, some more popular detection methods-such as face-based affect recognition-might be less relevant when applied to games  [175] ,  [176] .",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "C. Annotation Tools",
      "text": "In this section, we review the available annotation tools that could be used for collecting affect labels in games. We also discuss the appropriateness of these tools for affective game computing. Table V summarises our survey on popular annotation tools that were introduced during the last two decades 22 . One of the major trends of the affective computing field is the shift from discrete and more complex annotation methods towards simpler, continuous labelling techniques. Early annotation tools such as ANVIL  [178]  or ELAN  [179]  (released in 2001 and 2006, respectively) focused on measuring discrete categorical emotions. While the Natural Language Processing field still uses such tools for labelling speech and text and certain annotation software still offers categorical labelling options (i.e. NOVA  [187] ) the field of affective computing started to shift away from such tools relatively early. With the advent of machine learning, moment-to-moment affect modelling became feasible-and with it, continuous annotation tools started to be used more widely. Traditional annotation tools such as FeelTrace  [177]  (released in 2000), AffectButton  [181]  (first released in 2009), and AffectRank  [163]  (released in 2015) measure multiple dimensions of affect at once-in some cases up to three dimensions. Annotation tools of that period were inspired by the Circumplex Model of Emotions  [189]  and generally used to label the arousalvalence-dominance spectrum.\n\nWhile the two-dimensional annotation scheme is still popular nowadays we note a shift towards one-dimensional labelling tools (from 2013 onwards) including GTrace  [183]  (released in 2013), ANNEMO  [182]  (released in 2013), CARMA  [184]  (released in 2014), RankTrace  [185]  (released in 2017), and PAGAN  [112]  (released in 2019). Similarly to FeelTrace in multi-dimensional labelling, GTrace became the new foundation for one-dimensional annotation tools. The bounded, continuous annotation method quickly spread and became popular in human-computer interaction and affective computing studies  [190] -  [193] . The first new annotation method to break the mould was RankTrace, with an unbounded protocol aimed to collect data specifically for preference learning  [185] . Nevertheless, GTrace and its derivatives remain one of the most popular annotation tools to this day.\n\nThe core strength of one-dimensional labelling tools is the reduced cognitive load they cause to annotators compared to multi-dimensional labelling in which annotators are requested to split their attention  [183]  across multiple dimensions and labels. Focusing on one affect dimension at a time reduces noise and provides a higher face validity. Multi-dimensional tools, however, can produce annotations at a higher rate. Moreover, as labels across different dimensions are collected simultaneously, the labels are less susceptible to recency effects  [194]  compared to those obtained from repeated one-dimensional protocols. For all aforementioned reasons, multi-dimensional annotation methods are still popular today. Meanwhile a series of new such tools have being developed recently including DARMA  [186]  (released in 2018) and two versions of RCEA  [169] ,  [188]  (the original was released in 2020 and the VR version was released in 2021).\n\nAnother, even more recent, trend is the shift from annotation tools which are usable in a lab setting towards those that are usable in the wild. Many traditional and popular tools require researcher oversight, which limits the dataset size collected. In addition to this issue, the new social reality brought on by the COVID-19 pandemic pushed the field even more towards crowd-sourcing affect labels. PAGAN  [112]  is one of the first frameworks developed with crowd-sourcing capacities in mind. Compared to earlier annotation tools, PAGAN is highly configurable and supports multiple different annotation techniques including a GTrace and a RankTrace variant. While PAGAN tackles the issue of crowd-sourcing by using an online platform, other annotation tools offer mobile integration (RCEA  [188] ), VR integration (RCEA-360VR  [169] ) or aim to alleviate the need for annotators through automatic labelling (NOVA  [187] ). We can observe this shift towards crowdsourcing and mobile integration in custom, yet-to-be-released annotation tools as well  [195] ,  [196] , and we expect more reliable crowd-sourcing methods to appear as the focus will continuously shift from the lab to real-world (in the wild) experiences.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ix. Game Affective Corpora",
      "text": "Instead of building a new affect corpus from scratch one may apply affective computing methods directly to existing corpora. Unlike traditional affective computing datasets, how-ever, player modeling research often focuses on player experience aspects that are not directly linked to affect. Many player experience datasets for instance are annotated with high-level game-related concepts, such as frustration, perceived challenge  [197] ,  [198] , engagement  [198] , and fun  [197] ,  [199] . Nevertheless, affective computing methods are still applicable; and conversely, such datasets can offer interesting new test-beds for affective computing applications. This section reviews a number of available affect corpora that are build using games as the underlying context of interaction.\n\nWhile, traditionally, affective computing datasets are collected through induced emotions and posed expressions, in recent years we witness a notable shift towards spontaneous emotion elicitation and naturalistic settings. During the last decade, a new wave of datasets has started to employ popular multimedia as elicitors of affect  [200] -  [202] . Using artefacts such as clips and still images from popular movies has proven to be a reliable and cost-effective way to elicit emotions in a natural way. The resulting datasets, however, only focus on a specific type of passive elicitation that comes with consuming traditional media. The latter half of the decade, instead, has seen a rise in affect corpora that employ interactive elicitation methods including dyads  [203] ,  [204] , group tasks  [205] , board games  [206] , and videogames  [32] ,  [198] ,  [199] .\n\nThere are two major differences one can identify between traditional affective computing and game-based corpora. The first difference is that traditional affective computing corpora use third-person annotation (i.e. RELOCA  [203] , LIRIS-ACCEDE  [207] , Aff-Wild  [201] , AffectNet  [202] , SEWA DB  [204] ) whereas game-based datasets, instead, use primarily self-reports (i.e. MazeBall  [197] , PED  [198] , FUNii  [199] , MUMBAI  [206] , AGAIN  [32] ). The second major difference between affective computing and player experience datasets is the wider focus of the latter on experience aspects that often cover behavioural or user states. In fact, most player experience datasets do not consider affect labels or affect manifestations at all. While there is definitely a mapping between affect and higher-level concepts such as fun and engagement, revealing such a relationship might not be as trivial. With this in mind, the survey of datasets presented in this section focuses on game-based datasets that have some connection to affective computing; in particular, they either consider physiological signals and/or are annotated using traditional affective dimensions such as arousal or valence. For the sake of clarity, we also omit datasets that might have been influential in the past but are not available anymoresuch as the Tower Game  [208]  or the GeMo  [209]  datasetsand popular game-based datasets that do not include affective labels-such as the Obstacle Tower dataset  [210] .\n\nTable  VI  presents our survey of 11 datasets which can be used for affective game computing research. Most of the datasets presented here are quite recent, (i.e. 8 of the 11 are released in 2019 or later) showing the potential and the emerging nature of the field. Moreover, most of the examined datasets focus on one specific context; i.e. the genre of the game. While the AGAIN dataset  [32]  was designed explicitly to offer a wide array of different games, the Atari-HEAD  [214]  and MUMBAI  [206]  datasets also provide more varied inputs in their own niche (arcade-and board-games, respectively). Many game datasets contain video data of the gameplay footage, which is ideal for deep learning applications and for mapping pixels to affect directly  [33] ,  [34] . While pixel-based affect detection might be a harder task on datasets collected on board or social games (i.e. MUMBAI  [206]  and GAME-ON  [205] ), datasets that provide a large amount of gameplay footage are ideal for computer vision methods (i.e. Atari-HEAD  [214]  and AGAIN  [32] ). Five out of the eleven affective game computing corpora feature physiological signals that are more commonly used in traditional affective computing research, while only 3 of them feature eye-tracking data. In addition to traditional features, many game datasets (5 of the 11 surveyed) contain behavioural and contextual data in the form of game telemetry and player input. This type of data has proven to be robust as a predictor of game-related emotional states  [4] ,  [164] ,  [216] .\n\nEven though all datasets surveyed contain multiple modalities of user signals, not all of them offer affect labels. Specifically, 3 of the surveyed datasets (i.e. GSET Somi  [211] , eSports Sensors Dataset  [213] , and Atari-HEAD  [214] ) do not provide affect labels of any sort. The rest of the corpora provide a wide array of experience labels, mostly related to high-level game-related outcomes such as fun, challenge, and engagement. Two of the surveyed datasets feature emotional labels (GAME-ON  [205]  and IRAFFE2  [215] ) whereas 3 of them contain at least one type of affective label (RAGA  [212] , MUMBAI  [206] , and AGAIN  [32] ). As mentioned earlier, affect labels in games are predominately self-reported and not usually annotated by a third person.\n\nReflecting over Table VI one can observe a substantial increase in interest and availability of game-based datasets over the last few years; there are still, however, many aspects of player experience that have not made it to an affect corpus as of yet. Moreover the design of the various data collection protocols comes with limitations. Most notably the wide spectrum of labels used across the corpora makes it hard to compare datasets and transfer any knowledge gained across corpora. While it is understandable that game-related outcomes are more important than basic affective dimensions for game user researchers, some level of standardisation would go a long way towards making these datasets more accessible and reusable. Nevertheless, affective corpora-as the ones surveyed in Table VI-that are based on interactive elicitation bring a new perspective to affective computing at large, as such elicitation methods were largely unsupported by traditional datasets up until recently.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "X. Considerations And Discussion",
      "text": "After surveying the affective game computing field as a whole, the available tools for data collection and finally the datasets available, in this section we will discuss a number of considerations that are linked to this field. Specifically, we will start by outlining a number of ethical considerations and we will then briefly touch upon the current hardware limitations that can be detrimental to research advancements and breakthroughs in the field.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A. Ethical Challenges",
      "text": "With the acceleration of widespread AI adoption, various ethical considerations around the field have become more important than ever. While, on the one side, we can observe an unprecedented increase of AI use in both academia and industry, on the other side, we can see the growing public anxiety towards the very same systems  [217] ,  [218] . Although many ethical frameworks exist to address these anxieties, the industry as a whole has been slow to react  [219] . This section highlights some of the challenges in Responsibility, Transparency, Auditability, Incorruptibility, and Predictability  [220]  that future game affective applications will have to face  [221] ,  [222] . The interested reader may refer to  [223]  for a recent in-depth discussion of ethical considerations related to the various uses of AI in and for games.\n\nThe first challenge is to establish a well-defined chain of responsibility and ownership over affect models, the data they train on, and their output. A major issue in this topic is the decoupling of the data, the model, and its output. Because the results provided by an affect model are thought to be \"inferred\" instead of \"observed\", the chain of responsibility becomes opaque  [224] . Even though the responsibility of transparent data handling is well-defined in documents such as European Union's General Data Protection Regulation (GDPR)  [225] , the research community is still lagging behind  [226] . The issue is not trivial both on the academic and industrial levels by the fact that affective corpora-despite being deeply personalis not protected under the current legal frameworks  [219] . We already note proposals for a change in regulations  [227]  addressing some of these issues, and expect future affective interaction applications to be upheld to more scrutiny.\n\nTransparency is a core challenge of AI as a whole. While seemingly a clear-cut issue which can be tackled by explainable AI systems  [228] ,  [229] , in reality, the issue is complicated by the transparency-efficiency trade-off  [230] . This phenomenon describes the detrimental effect of bias against AI  [231]  to human-machine cooperation. Resolving transparency in games is even more challenging due to the \"smoke and mirrors\" nature of the medium.\n\nAuditability is more of an industry-specific challenge rather than an academic one, as recent research studies already strive for external validity and reliability. Industrial applications of affective game computing should consider the audit process during their development cycle. As the field moves more and more towards large-scale foundation models  [232] , the role of planned audits to maintain transparency will become more and more a necessity.\n\nThe challenge of Incorruptibility refers to the robustness of the system against any kind of manipulation. While short-lived research-focused projects are generally not expected to receive many adversarial attacks-if any; industry applications and even large-scale crowd-sourced studies have to face external attackers. However, beyond malicious users, computer models can also be corrupted by internal forces. Biased affect models can encode and perpetuate socioeconomic and sociopolitical biases that lead to direct or indirect harm to the users  [233] -  [235] . Unfortunately, incorruptibility is a wicked problem in the field as often there is no apparent way of ascertaining algorithmic bias before the system is deployed.\n\nThe final challenge for ethical AC applications is Predictability. Similarly to auditability, academic studies in general fare well on this front because predictability is somewhat analogous to the internal reliability and validity of the system. Beyond this, predictability can help increase transparency and incorruptibility by ensuring the reliability and fairness of the application in question  [236] .\n\nThe framework presented here as adapted from  [223]  describes the universally understood challenges when it comes to AC applications in games. These issues have served as a bedrock for discussions and proposals surrounding ethical player modelling  [221] , ethically aligned design  [237] , ethics in human-AI interaction  [236] , AI trustworthiness  [238] , and general ethical AI guidelines  [239] .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B. Hardware Limitations",
      "text": "As already mentioned in Section VIII sensing affect via objective measurements offers rich information about the player's experience; a major limitation, however, is that several of these sensors can be invasive, impractical or even impossible in the wild. Pupillometry and gaze tracking for instance, are sensitive to variations in light and screen luminance. Camera-based sensing (e.g. facial expressions and body posture) requires a well-lit environment which is often not available when we play videogames in our living rooms for instance. However, the recent rapid advancement of mobile, smartwatch and VR hardware with integrated eye tracking and physiology sensors (e.g. see the recent HP Omnicept headset gives such sensing technologies entirely new opportunities and use within games  [240] . Speech and text may offer some alternative unobtrusive and highly accessible modalities which are only applicable, however, to games that feature those modalities. This includes games in which a) speech is a control modality  [241] ,  [242] , b) text is used as means of communication across the audience of a streamed game  [4] , c) speech or chat is used for multiplayer coordination, d) natural language processing is used as a game control in text-based adventure games or interactive fiction. Finally, existing hardware for EEG, respiration and EMG (if not embedded within a VR headset) requires the placement of sensors on the player's body thereby making those physiological signals intrusive and rather impractical, to say the least. Recent sensor technology advancements however, especially via state-of-the-art VR headsets (see Section VIII) have revived the use of physiological signals for commercial standard applications  [52] .",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Xi. The Road Ahead",
      "text": "In this section, we will cover the two areas we think will be the most challenging for future research in affective game computing. In particular, we first discuss current and future research in the area of artificial general intelligence (AGI) and emotion in games, and we move on to discuss particular computer vision research areas that appear to be of high value for affective game computing research. We end with a small section dedicated to large language models (LLMs) and their potential impact on affective game computing.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "A. Agi And Affective Game Computing",
      "text": "Damasio's work  [243]  suggests that our ability to recognise human emotion across contexts and people of dissimilar moods, cultural backgrounds and personalities, acts as a facilitator of decision-making and general (emotional) intelligence  [244] . While the research reviewed has reached important milestones, all key findings suggest that any success of affective (game) computing is heavily dependent on the domain, the task at hand, and the context in general. This limitation of specificity is also present in games  [8] . The vast majority of the studies presented focus on modelling player experience within a particular game and a narrow player base, and under wellcontrolled conditions (e.g.  [130] ,  [176] ,  [245] ,  [246]  among many).\n\nAssuming that the game affective loop can be successfully realised within a particular game, the next long-term and ambitious goal for affective game computing is to achieve a good level of generality across games of the same genre of other genres. For affective game computing to become general, models are required to recognise general emotional and cognitive-behavioural patterns across contexts of players and games. So far the literature is rather sparse in this area with only a few available preliminary studies. Early work focused on the ad-hoc design of general metrics of player interest for prey-predator games  [247] ,  [248]  followed by player experience models that can operate to a satisfactory degree across dissimilar games  [31] ,  [249] ,  [250] . Recent work in the area is driven by the AGAIN dataset  [90]  through which a number of promising studies have been performed to test for the generality of player arousal models across games  [32] ,  [251] . Discovering entirely new representations of player emotive manifestations across games, modalities, and player types appears to be a critical step towards achieving general player affect models. Methods from representation learning and transfer learning could be of great assistance to this cause  [251] . In the next section, we discuss how such computer vision methods can further advance the study of affective game computing.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "B. Computer Vision For Affective Game Computing",
      "text": "Computer vision research brings methods with great potential for the future of affective game computing. Recent work in the use of convolutional neural networks has shown that it is possible to solely rely on gameplay footage and in-game audio for predicting player affect with high levels of accuracy  [33] ,  [34] ,  [117] . What is fascinating about pixel-based affect modelling is that it is general and user-agnostic: it is general as it can eventually represent any affective patterns by observing the game footage pixels; it is user agnostic as it does not rely on any other personal information about players beyond their gameplay (e.g. manifestations of affect via the physiology of facial expressions). These two properties make pixel-based affect models operational in the wild. Making players affect models usable in the wild is key for the models to be usable in games. A recent direction with great promise addressing the operation of affect models in the wild is the use of privileged information  [35] . Privileged information allows models to be trained with game lab data (e.g. including physiology, speech and facial expression) and operate without these modalities which are not available in a player's living room (i.e. in the wild).\n\nComplementary to the above computer vision methods, self-supervised learning (SSL) methods such as contrastive learning define a recent machine learning paradigm which has been widely and successfully employed for learning general representations of data  [252] . SSL methods attempt to learn by processing different views of the same input that have similar representations. Although contrastive learning is gaining traction for computer vision tasks such methods have found applications in games  [253] ,  [254]  and affective computing  [255] ,  [256]  only recently. SSL-based affect modelling assumes that affect information is an inherent property of the manifestations of affect and thus can be fused and learned in a contrastive learning manner. Early results of Pinitas et al.  [256]  suggest that when affect is used as a label to contrast multimodal data arousal models achieve supreme classification performance compared to end-to-end classification.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "C. Llms For Affective Game Computing",
      "text": "Large scale language models such as GPT-3  [94]  and GPT-4  [95]  appear to offer promising, yet largely unexplored, methods for affective game computing. We envision the use of LLMs for learning and interweaving game context and affect demonstrations as e.g. in early experiments reported in  [257] . In principle, an LLM that is hosted in a game engine could predict any affective state transition including states where \"the game is more engaging\" and thereby change the surrounding environment or spawn a number of enemies to get the player to a supposedly more engaging state. Learning to generate affect transitions via foundation models builds on the experience-driven procedural content generation paradigm  [130]  but with a contemporary touch. Generative affect-based AI methods including GPT variants and diffusion models  [258]  will likely expand beyond text-to-(affect)-image applications  [259] , to text-to-video, text-to-3D models and finally textto-games with prescribed affect patterns  [257] ; text can be replaced (or complemented) by other modalities including images and sound.\n\nCompared to the very specific game state transitions that an LLM could use to play a game the relationship between game context and player affect appears to be more general  [32] . One can thus argue that such relationships could potentially be learned from many games rather than one. We could then imagine the future development (or fine tuning) of large foundation models that are capable of representing and inferring affect transitions based on in-game observations and affect demonstrations. Referring to our earlier discussion about AGI, it remains to be seen to which degree we can build a generalised computational game experience designer, and how.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Xii. Conclusions",
      "text": "The domain of games has advanced the ways we represent, model, and annotate emotion over the last decade. It is their highly interactive nature, the complex spatiotemporal and rich behaviour of their users, and the availability of multiple sensor data that have helped such advancements collectively. Affective computing has also advanced game design and development. For instance, affect-driven adaptation and automatic player experience design are becoming gradually the norm across a number of game genres (e.g. horror and racing). This paper surveyed this exciting intersection of AC and games and introduced the field of affective game computing through the phases of the affective loop. The paper also introduced a taxonomy of terms and methods used largely in affective game computing and placed exemplar studies on this taxonomy. Finally, this survey offers the interested reader a comprehensive list of data collection tools and datasets that are directly accessible for research in this field.\n\nWith this paper we survey the past, capture the present and offer a vision about the future of this field. In our attempt we tried to be as comprehensive and inclusive as possible (as indicated by the reference list of this article). Despite the best of our intentions, however, we are aware that some studies were omitted or not discussed thoroughly due to space considerations. Evidently, while a lot has been achieved in affective game computing until nowadays, there are still many open research questions left to be addressed. We hope that this paper will act as a key driver of groundbreaking research and innovation in this emerging field.",
      "page_start": 16,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A high-level illustration of the affective game loop. The orange",
      "page": 2
    },
    {
      "caption": "Figure 2: Characteristics of Games: Examples across the dimensions of",
      "page": 3
    },
    {
      "caption": "Figure 3: ) which was designed and developed as a form",
      "page": 3
    },
    {
      "caption": "Figure 3: Screenshots of StartleMart [36]; a biofeedback game designed as",
      "page": 4
    },
    {
      "caption": "Figure 4: A screenshot of Nevermind (Flying Mollusk, 2015) showcasing",
      "page": 5
    },
    {
      "caption": "Figure 5: ). The game is",
      "page": 5
    },
    {
      "caption": "Figure 5: A screenshot of Apex of Fear (2022). The dashboard showcases",
      "page": 6
    },
    {
      "caption": "Figure 6: Game footage from Tom Clancy’s The Division (Ubisoft, 2016).",
      "page": 6
    },
    {
      "caption": "Figure 7: A screenshot of the Solid Rally racing game [128] (top) and the",
      "page": 7
    },
    {
      "caption": "Figure 8: The ERDL framework (top) and an example of a Super Mario",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Context": "AI Agent",
          "Genre\nPlatform\nNumber of players\nObservability\nStochasticity\nTime granularity\nAction Space": "",
          "Shooter, platformer,\nracing, strategy, adventure, etc.\nMobile, AR, VR, console, desktop.\nsingle, one-and-a-half,\ntwo-player, multi-player\nFully observable vs. partially observable game\nDeterministic vs. non-deterministic game\nReal-time vs. continuous\nPlayer actions available: 2 to many": "Navigation, expression, exploration, verbal/non-verbal\ninteraction"
        },
        {
          "Context": "Content",
          "Genre\nPlatform\nNumber of players\nObservability\nStochasticity\nTime granularity\nAction Space": "",
          "Shooter, platformer,\nracing, strategy, adventure, etc.\nMobile, AR, VR, console, desktop.\nsingle, one-and-a-half,\ntwo-player, multi-player\nFully observable vs. partially observable game\nDeterministic vs. non-deterministic game\nReal-time vs. continuous\nPlayer actions available: 2 to many": "Visuals, Audio, Narrative, Game Design, Levels, Gameplay [5]"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "FeelTrace [177]": "ANVIL [178]",
          "2 dimensions\n(arousal-valence)": "Categorical\nlabels",
          "bounded continuous\ncircumplex": "discrete labels",
          "N/A": "Standalone installer"
        },
        {
          "FeelTrace [177]": "ELAN [179]",
          "2 dimensions\n(arousal-valence)": "Categorical\nlabels",
          "bounded continuous\ncircumplex": "discrete labels",
          "N/A": "Standalone installer"
        },
        {
          "FeelTrace [177]": "EmuJoy [180]",
          "2 dimensions\n(arousal-valence)": "2 dimensions\n(arousal-valence)",
          "bounded continuous\ncircumplex": "bounded continuous",
          "N/A": "Standalone installer"
        },
        {
          "FeelTrace [177]": "AffectButton\n[181]",
          "2 dimensions\n(arousal-valence)": "3 dimensions (pleasure-\narousal-dominance)",
          "bounded continuous\ncircumplex": "bounded continuous",
          "N/A": "Standalone or online"
        },
        {
          "FeelTrace [177]": "ANNEMO [182]",
          "2 dimensions\n(arousal-valence)": "1 dimension\n(configurable)",
          "bounded continuous\ncircumplex": "bounded continuous",
          "N/A": "Node.js package"
        },
        {
          "FeelTrace [177]": "GTrace [183]",
          "2 dimensions\n(arousal-valence)": "1 dimension (negative to\npositive)",
          "bounded continuous\ncircumplex": "bounded continuous",
          "N/A": "N/A"
        },
        {
          "FeelTrace [177]": "CARMA [184]",
          "2 dimensions\n(arousal-valence)": "1 dimension (negative to\npositive)",
          "bounded continuous\ncircumplex": "bounded discrete\n(configurable)",
          "N/A": "Installer\n(requires MATLAB20)"
        },
        {
          "FeelTrace [177]": "AffectRank [163]",
          "2 dimensions\n(arousal-valence)": "2 dimensions\n(arousal-valence)",
          "bounded continuous\ncircumplex": "discrete circumplex",
          "N/A": "Adaptation through PHP and\nJavaScript"
        },
        {
          "FeelTrace [177]": "RankTrace [185]",
          "2 dimensions\n(arousal-valence)": "1 dimension (tension)",
          "bounded continuous\ncircumplex": "unbounded continuous",
          "N/A": "C# source (pre-built version\navailable;\nrequires VLC21)"
        },
        {
          "FeelTrace [177]": "DARMA [186]",
          "2 dimensions\n(arousal-valence)": "2 dimensions\n(configurable)",
          "bounded continuous\ncircumplex": "bounded continuous\n(optional circumplex)",
          "N/A": "Installer\n(requires MATLAB,\nVLC, and a joystick)"
        },
        {
          "FeelTrace [177]": "NOVA [187]",
          "2 dimensions\n(arousal-valence)": "1 dimension or\ncategorical\n(configurable)",
          "bounded continuous\ncircumplex": "bounded continuous or\ndiscrete labels",
          "N/A": "Standalone installer"
        },
        {
          "FeelTrace [177]": "PAGAN [112]",
          "2 dimensions\n(arousal-valence)": "1 dimension\n(configurable)",
          "bounded continuous\ncircumplex": "unbounded and bounded\ncontinuous and discrete\nbinary (configurable)",
          "N/A": "No installation (online)"
        },
        {
          "FeelTrace [177]": "RCEA [188]",
          "2 dimensions\n(arousal-valence)": "2 dimensions\n(arousal-valence)",
          "bounded continuous\ncircumplex": "bounded continuous\ncircumplex",
          "N/A": "N/A"
        },
        {
          "FeelTrace [177]": "RCEA-360VR\n[169]",
          "2 dimensions\n(arousal-valence)": "2 dimensions\n(arousal-valence)",
          "bounded continuous\ncircumplex": "bounded continuous",
          "N/A": "Python package"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MazeBall\n[197]": "PED [198]",
          "Navigation": "Platformer",
          "1": "1",
          "N/A": "6",
          "36": "58",
          "BVP (HRV),\nEDA, game\ntelemetry": "Gaze, head\nposition, game\ntelemetry",
          "Pairwise": "Discrete (5-step),\npairwise",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "Engagement,\nfrustration,\nchallenge",
          "self-report": "self-report"
        },
        {
          "MazeBall\n[197]": "GSET Somi\n[211]",
          "Navigation": "Rail Shooter",
          "1": "1-3",
          "N/A": "6.75",
          "36": "84",
          "BVP (HRV),\nEDA, game\ntelemetry": "Eyetracking,\ngameplay video",
          "Pairwise": "N/A",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "N/A",
          "self-report": "N/A"
        },
        {
          "MazeBall\n[197]": "FUNii\n[199]",
          "Navigation": "Action",
          "1": "2",
          "N/A": "N/A",
          "36": "190",
          "BVP (HRV),\nEDA, game\ntelemetry": "ECG, EDA, gaze\nand head\nposition,\ncontroller\ninput",
          "Pairwise": "Continuous,\ndiscrete",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "Fun (cont.),\nfun,\ndifficulty,\nworkload,\nimmersion, UX",
          "self-report": "self-report"
        },
        {
          "MazeBall\n[197]": "RAGA [212]",
          "Navigation": "Racing\n(VR and PC)",
          "1": "2",
          "N/A": "N/A",
          "36": "33",
          "BVP (HRV),\nEDA, game\ntelemetry": "ECG, EDA,\nEMG,\nresp.",
          "Pairwise": "Continuous\nbounded",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "Arousal, valence",
          "self-report": "self-report"
        },
        {
          "MazeBall\n[197]": "GAME-ON\n[205]",
          "Navigation": "Escape room",
          "1": "5",
          "N/A": "11.5",
          "36": "51",
          "BVP (HRV),\nEDA, game\ntelemetry": "Video, audio,\nand motion\ncapture data",
          "Pairwise": "Discrete\n(5–9-step)",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "Emotions,\ncohesion,\nwarmth,\ncompetence,\ncompetitivity,\nleadership, and\nmotivation",
          "self-report": "self-report"
        },
        {
          "MazeBall\n[197]": "eSports Sensors\nDataset\n[213]",
          "Navigation": "MOBA",
          "1": "11",
          "N/A": "N/A",
          "36": "8",
          "BVP (HRV),\nEDA, game\ntelemetry": "EEG, BVP (HR),\nEDA, EMG,\ntemp., hand and\nhead gestures",
          "Pairwise": "N/A",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "N/A",
          "self-report": "N/A"
        },
        {
          "MazeBall\n[197]": "Atari-HEAD\n[214]",
          "Navigation": "Arcade",
          "1": "20",
          "N/A": "117",
          "36": "4",
          "BVP (HRV),\nEDA, game\ntelemetry": "Eyetracking,\ngameplay video,\ngame telemetry",
          "Pairwise": "N/A",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "N/A",
          "self-report": "N/A"
        },
        {
          "MazeBall\n[197]": "MUMBAI\n[206]",
          "Navigation": "Board-game",
          "1": "6",
          "N/A": "46",
          "36": "58",
          "BVP (HRV),\nEDA, game\ntelemetry": "Gameplay,\nfacial\nvideo, and facial\naction units",
          "Pairwise": "Discrete labels",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "Valence,\nattention,\ngameplay\nexperience,\npersonality",
          "self-report": "56 (3rd-person)\n58 (1st-person)"
        },
        {
          "MazeBall\n[197]": "BIRAFFE2 [215]",
          "Navigation": "Platformer,\nnavigation",
          "1": "3",
          "N/A": "23",
          "36": "103",
          "BVP (HRV),\nEDA, game\ntelemetry": "EEG, EDA,\ngame video,\ngame telemetry,\nfacial\nrecognition",
          "Pairwise": "Continous\n(automated) and\ndiscrete (survey)",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "Emotions,\nNEO-FFI and\nGEQ survey",
          "self-report": "automated\n(emotions),\nself-report\n(survey)"
        },
        {
          "MazeBall\n[197]": "AGAIN [32]",
          "Navigation": "Racing,\nshooter,\nplatformer",
          "1": "9",
          "N/A": "37",
          "36": "124",
          "BVP (HRV),\nEDA, game\ntelemetry": "Game video,\ngame telemetry",
          "Pairwise": "Continuous\nunbounded",
          "Fun, challenge,\nfrustration,\nanxiety,\nboredom,\nexcitement,\nrelaxation": "Arousal",
          "self-report": "self-report"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Number of gamers worldwide from 2015 to 2024",
      "authors": [
        "J Clement"
      ],
      "year": "2022",
      "venue": "Number of gamers worldwide from 2015 to 2024"
    },
    {
      "citation_id": "2",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "1995",
      "venue": "MIT Media Laboratory Perceptual Computing, Tech. Rep"
    },
    {
      "citation_id": "3",
      "title": "Affective loop experiences-what are they",
      "authors": [
        "K Höök"
      ],
      "year": "2008",
      "venue": "Proceedings of the International Conference on Persuasive Technology"
    },
    {
      "citation_id": "4",
      "title": "Moment-to-moment engagement prediction through the eyes of the observer: Pubg streaming on twitch",
      "authors": [
        "D Melhart",
        "D Gravina",
        "G Yannakakis"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on the Foundations of Digital Games (FDG)"
    },
    {
      "citation_id": "5",
      "title": "Computational game creativity",
      "authors": [
        "A Liapis",
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2014",
      "venue": "Proceedings of the fifth International Conference on Computational Creativity"
    },
    {
      "citation_id": "6",
      "title": "Orchestrating game generation",
      "authors": [
        "A Liapis",
        "G Yannakakis",
        "M Nelson",
        "M Preuss",
        "R Bidarra"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Games"
    },
    {
      "citation_id": "7",
      "title": "Exploring the affective loop",
      "authors": [
        "P Sundström"
      ],
      "year": "2005",
      "venue": "Exploring the affective loop"
    },
    {
      "citation_id": "8",
      "title": "Emotion in games",
      "authors": [
        "G Yannakakis",
        "A Paiva"
      ],
      "year": "2014",
      "venue": "Handbook on affective computing"
    },
    {
      "citation_id": "9",
      "title": "Emotion in Games: Theory and Praxis",
      "authors": [
        "K Karpouzis",
        "G Yannakakis"
      ],
      "year": "2016",
      "venue": "Emotion in Games: Theory and Praxis"
    },
    {
      "citation_id": "10",
      "title": "Artificial Intelligence and Games",
      "authors": [
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2018",
      "venue": "Artificial Intelligence and Games"
    },
    {
      "citation_id": "11",
      "title": "let's get physiological, physiological\" a systematic review of affective gaming",
      "authors": [
        "R Robinson",
        "K Wiley",
        "A Rezaeivahdati",
        "M Klarkowski",
        "R Mandryk"
      ],
      "year": "2020",
      "venue": "Proceedings of the Annual Symposium on Computer-Human Interaction in Play"
    },
    {
      "citation_id": "12",
      "title": "Biofeedback methods in entertainment video games: A review of physiological interaction techniques",
      "authors": [
        "D Navarro",
        "V Sundstedt",
        "V Garro"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Human-Computer Interaction (CHI Play)"
    },
    {
      "citation_id": "13",
      "title": "Nature vs. stress: Investigating the use of biophilia in nonviolent exploration games to reduce stress",
      "authors": [
        "A Reetz",
        "D Valtchanov",
        "M Barnett-Cowan",
        "M Hancock",
        "J Wallace"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Human-Computer Interaction"
    },
    {
      "citation_id": "14",
      "title": "Hold my hand: Impact of intimate controllers on player experience",
      "authors": [
        "A Canossa",
        "A Azadvar",
        "E Andersen"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "15",
      "title": "Impact of visual and sound orchestration on physiological arousal and tension in a horror game",
      "authors": [
        "S Graja",
        "P Lopes",
        "G Chanel"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Games"
    },
    {
      "citation_id": "16",
      "title": "Virtual character facial expressions influence human brain and facial emg activity in a decision-making game",
      "authors": [
        "N Ravaja",
        "G Bente",
        "J Kätsyri",
        "M Salminen",
        "T Takala"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Modelling affect for horror soundscapes",
      "authors": [
        "P Lopes",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Auditory feedback of false heart rate for video game experience improvement",
      "authors": [
        "S Ogawa",
        "K Fujiwara",
        "M Kano"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "19",
      "title": "Psychometric mapping of audio features to perceived physical characteristics of virtual objects",
      "authors": [
        "M Colombo",
        "A Dolhasz",
        "J Hockman",
        "C Harvey"
      ],
      "year": "2021",
      "venue": "2021 Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "20",
      "title": "Exergaming: The impact of virtual reality on cognitive performance and player experience",
      "authors": [
        "F Born",
        "L Graf",
        "M Masuch"
      ],
      "year": "2021",
      "venue": "2021 Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "21",
      "title": "Early prediction of student frustration",
      "authors": [
        "S Mcquiggan",
        "S Lee",
        "J Lester"
      ],
      "year": "2007",
      "venue": "Proceedings of International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "22",
      "title": "Modeling user affect from causes and effects",
      "authors": [
        "C Conati",
        "H Maclaren"
      ],
      "year": "2009",
      "venue": "User Modeling, Adaptation, and Personalization"
    },
    {
      "citation_id": "23",
      "title": "Modeling Player Experience for Content Creation",
      "authors": [
        "C Pedersen",
        "J Togelius",
        "G Yannakakis"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games"
    },
    {
      "citation_id": "24",
      "title": "Mining multimodal sequential patterns: a case study on affect detection",
      "authors": [
        "H Martínez",
        "G Yannakakis"
      ],
      "year": "2011",
      "venue": "Proceedings of the ACM Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "25",
      "title": "Towards Automatic Personalized Content Generation for Platform Games",
      "authors": [
        "N Shaker",
        "J Togelius",
        "G Yannakakis"
      ],
      "year": "2010",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE)"
    },
    {
      "citation_id": "26",
      "title": "Evaluating the consequences of affective feedback in intelligent tutoring systems",
      "authors": [
        "J Robison",
        "S Mcquiggan",
        "J Lester"
      ],
      "year": "2009",
      "venue": "Proceedings of International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "27",
      "title": "Measuring emotional valence during interactive experiences: boys at video game play",
      "authors": [
        "R Hazlett"
      ],
      "year": "2006",
      "venue": "Proceedings of SIGCHI Conference on Human Factors in Computing Systems (CHI)"
    },
    {
      "citation_id": "28",
      "title": "Phasic emotional reactions to video game events: A psychophysiological investigation",
      "authors": [
        "N Ravaja",
        "T Saari",
        "M Salminen",
        "J Laarni",
        "K Kallinen"
      ],
      "year": "2006",
      "venue": "Media Psychology"
    },
    {
      "citation_id": "29",
      "title": "Modeling self-efficacy in intelligent tutoring systems: An inductive approach",
      "authors": [
        "S Mcquiggan",
        "B Mott",
        "J Lester"
      ],
      "year": "2008",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "30",
      "title": "Genetic search feature selection for affective modeling: a case study on reported preferences",
      "authors": [
        "H Martínez",
        "G Yannakakis"
      ],
      "year": "2010",
      "venue": "Proceedings of the International Workshop on Affective Interaction in Natural Environments"
    },
    {
      "citation_id": "31",
      "title": "Towards general models of player affect",
      "authors": [
        "E Camilleri",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "32",
      "title": "The arousal video game annotation (again) dataset",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "From pixels to affect: a study on games and player experience",
      "authors": [
        "K Makantasis",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "34",
      "title": "The pixels and sounds of emotion: General-purpose representations of arousal in games",
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Privileged information for modeling affect in the wild",
      "authors": [
        "K Makantasis",
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "36",
      "title": "Multimodal PTSD characterization via the Startlemart game",
      "authors": [
        "C Holmgård",
        "G Yannakakis",
        "H Martínez",
        "K.-I Karstoft",
        "H Andersen"
      ],
      "year": "2015",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "37",
      "title": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)",
      "authors": [
        "C Holmgård",
        "G Héctor",
        "P Martínez",
        "K.-I Karstoft"
      ],
      "year": "2015",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "38",
      "title": "Stress detection for PTSD via the Startlemart game",
      "authors": [
        "C Holmgård",
        "G Yannakakis",
        "K.-I Karstoft",
        "H Andersen"
      ],
      "year": "2013",
      "venue": "Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "39",
      "title": "Using Bayesian networks to manage uncertainty in student modeling",
      "authors": [
        "C Conati",
        "A Gertner",
        "K Vanlehn"
      ],
      "year": "2002",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "40",
      "title": "Evaluating a computational model of emotion",
      "authors": [
        "J Gratch",
        "S Marsella"
      ],
      "year": "2005",
      "venue": "Autonomous Agents and Multi-Agent Systems"
    },
    {
      "citation_id": "41",
      "title": "Game analytics: Maximizing the value of player data",
      "authors": [
        "M El-Nasr",
        "A Drachen",
        "A Canossa"
      ],
      "year": "2013",
      "venue": "Game analytics: Maximizing the value of player data"
    },
    {
      "citation_id": "42",
      "title": "Game Data Science",
      "authors": [
        "M El-Nasr",
        "T.-H Nguyen",
        "A Canossa",
        "A Drachen"
      ],
      "year": "2021",
      "venue": "Game Data Science"
    },
    {
      "citation_id": "43",
      "title": "Spatial game analytics",
      "authors": [
        "A Drachen",
        "M Schubert"
      ],
      "year": "2013",
      "venue": "Game Analytics"
    },
    {
      "citation_id": "44",
      "title": "Personas versus clones for player decision modeling",
      "authors": [
        "C Holmgård",
        "A Liapis",
        "J Togelius",
        "G Yannakakis"
      ],
      "year": "2014",
      "venue": "Proceedings of the International Conference on Entertainment Computing"
    },
    {
      "citation_id": "45",
      "title": "Classification of video game player experience using consumer-grade electroencephalography",
      "authors": [
        "T Parsons",
        "T Mcmahan",
        "I Parberry"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "An emotion recognition method for game evaluation based on electroencephalogram",
      "authors": [
        "G Du",
        "W Zhou",
        "C Li",
        "D Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "47",
      "title": "Bcimanager: A library for development of brain-computer interfacing applications in unity",
      "authors": [
        "F Škola",
        "F Liarokapis"
      ],
      "year": "2021",
      "venue": "2021 Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "48",
      "title": "Acquiface interface as a device for acquisition of user's facial expressions in game expression hunter",
      "authors": [
        "S Sugiyanto",
        "I Purnama",
        "E Yuniarno",
        "H Haryanto",
        "N Rokhman",
        "M Purnomo"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "49",
      "title": "Facial and bodily expressions of emotional engagement: How dynamic measures reflect the use of game elements and subjective experience of emotions and effort",
      "authors": [
        "S Greipl",
        "K Bernecker",
        "M Ninaus"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM on Human-Computer Interaction"
    },
    {
      "citation_id": "50",
      "title": "Psychophysiology: Human Behavior and Physiological Response",
      "authors": [
        "J Andreassi"
      ],
      "year": "2000",
      "venue": "Psychophysiology: Human Behavior and Physiological Response"
    },
    {
      "citation_id": "51",
      "title": "Effect of experimental factors on the recognition of affective mental states through physiological measures",
      "authors": [
        "R Calvo",
        "I Brown",
        "S Scheding"
      ],
      "year": "2009",
      "venue": "AI 2009: Advances in Artificial Intelligence"
    },
    {
      "citation_id": "52",
      "title": "Psychophysiology in games",
      "authors": [
        "G Yannakakis",
        "H Martínez",
        "M Garbarino"
      ],
      "year": "2016",
      "venue": "Emotion in Games: Theory and Praxis"
    },
    {
      "citation_id": "53",
      "title": "The psychophysiology of emotion",
      "authors": [
        "J Cacioppo",
        "G Berntson",
        "J Larsen",
        "K Poehlmann",
        "T Ito"
      ],
      "year": "2000",
      "venue": "Handbook of emotions"
    },
    {
      "citation_id": "54",
      "title": "Objective measures, sensors and computational techniques for stress recognition and classification: A survey",
      "authors": [
        "N Sharma",
        "T Gedeon"
      ],
      "year": "2012",
      "venue": "Computer methods and programs in biomedicine"
    },
    {
      "citation_id": "55",
      "title": "Dynamic game balancing by recognizing affect",
      "authors": [
        "T Tijs",
        "D Brokken",
        "W Ijsselsteijn"
      ],
      "year": "2008",
      "venue": "Proceedings of International Conference on Fun and Games"
    },
    {
      "citation_id": "56",
      "title": "Flow and immersion in first-person shooters: measuring the player's gameplay experience",
      "authors": [
        "L Nacke",
        "C Lindley"
      ],
      "year": "2008",
      "venue": "Proceedings of the Conference on Future Play: Research, Play, Share"
    },
    {
      "citation_id": "57",
      "title": "Using psychophysiological techniques to measure user experience with entertainment technologies",
      "authors": [
        "R Mandryk",
        "K Inkpen",
        "T Calvert"
      ],
      "year": "2006",
      "venue": "Behaviour & Information Technology"
    },
    {
      "citation_id": "58",
      "title": "A fuzzy physiological approach for continuously modeling emotion during interaction with play technologies",
      "authors": [
        "R Mandryk",
        "M Atkins"
      ],
      "year": "2007",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "59",
      "title": "Maintaining optimal challenge in computer games through real-time physiological feedback",
      "authors": [
        "P Rani",
        "N Sarkar",
        "C Liu"
      ],
      "year": "2005",
      "venue": "Proceedings of the International Conference on Human Computer Interaction"
    },
    {
      "citation_id": "60",
      "title": "Modeling enjoyment preference from physiological responses in a car racing game",
      "authors": [
        "S Tognetti",
        "M Garbarino",
        "A Bonarini",
        "M Matteucci"
      ],
      "year": "2010",
      "venue": "IEEE Symposium on Computational Intelligence and Games (CIG)"
    },
    {
      "citation_id": "61",
      "title": "Correlation between heart rate, electrodermal activity and player experience in first-person shooter games",
      "authors": [
        "A Drachen",
        "L Nacke",
        "G Yannakakis",
        "A Pedersen"
      ],
      "year": "2010",
      "venue": "Proceedings of the SIGGRAPH Symposium on Video Games"
    },
    {
      "citation_id": "62",
      "title": "Towards affective camera control in games",
      "authors": [
        "G Yannakakis",
        "H Martínez",
        "A Jhala"
      ],
      "year": "2010",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "63",
      "title": "Using heart rate and machine learning for vr horror game personalization",
      "authors": [
        "S Zaib",
        "M Yamamura"
      ],
      "year": "2022",
      "venue": "2022 Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "64",
      "title": "To rank or to classify? Annotating stress for reliable PTSD profiling",
      "authors": [
        "C Holmgård",
        "G Yannakakis",
        "H Martínez",
        "K.-I Karstoft"
      ],
      "year": "2015",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "65",
      "title": "BCI for games: A \"state of the art\" survey",
      "authors": [
        "A Nijholt"
      ],
      "year": "2009",
      "venue": "Entertainment Computing-ICEC 2008"
    },
    {
      "citation_id": "66",
      "title": "Assessing Neurosky's usability to detect attention levels in an assessment exercise",
      "authors": [
        "G Rebolledo-Mendez",
        "I Dunwell",
        "E Martínez-Mirón",
        "M Vargas-Cerdán",
        "S Freitas",
        "F Liarokapis",
        "A García-Gaona"
      ],
      "year": "2009",
      "venue": "Human-Computer Interaction. New Trends"
    },
    {
      "citation_id": "67",
      "title": "Classification of EEG for Affect Recognition: An Adaptive Approach",
      "authors": [
        "O Alzoubi",
        "R Calvo",
        "R Stevens"
      ],
      "year": "2009",
      "venue": "AI 2009: Advances in Artificial Intelligence"
    },
    {
      "citation_id": "68",
      "title": "Estimation of behavioral user state based on eye gaze and head pose-application in an e-learning environment",
      "authors": [
        "S Asteriadis",
        "P Tzouveli",
        "K Karpouzis",
        "S Kollias"
      ],
      "year": "2009",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "69",
      "title": "Towards gaze-controlled platform games",
      "authors": [
        "J Munoz",
        "G Yannakakis",
        "F Mulvey",
        "D Hansen",
        "G Gutierrez",
        "A Sanchis"
      ],
      "year": "2011",
      "venue": "Proceedings of the IEEE Conference on Computational Intelligence and Games (CIG)"
    },
    {
      "citation_id": "70",
      "title": "Automatic prediction of frustration",
      "authors": [
        "A Kapoor",
        "W Burleson",
        "R Picard"
      ],
      "year": "2007",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "71",
      "title": "Emotion sensors go to school",
      "authors": [
        "I Arroyo",
        "D Cooper",
        "W Burleson",
        "B Woolf",
        "K Muldner",
        "R Christopherson"
      ],
      "year": "2009",
      "venue": "Proceedings of Conference on Artificial Intelligence in Education (AIED)"
    },
    {
      "citation_id": "72",
      "title": "Predicting facial indicators of confusion with hidden Markov models",
      "authors": [
        "J Grafsgaard",
        "K Boyer",
        "J Lester"
      ],
      "year": "2011",
      "venue": "Proceedings of International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "73",
      "title": "Analysis of emotion recognition using facial expressions, speech and multimodal information",
      "authors": [
        "C Busso",
        "Z Deng",
        "S Yildirim",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "S Lee",
        "U Neumann",
        "S Narayanan"
      ],
      "year": "2004",
      "venue": "Proceedings of the International Conference on Multimodal Interfaces (ICMI)"
    },
    {
      "citation_id": "74",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "75",
      "title": "Frustrating computer users increases exposure to physical factors",
      "authors": [
        "J Dennerlein",
        "T Becker",
        "P Johnson",
        "C Reynolds",
        "R Picard"
      ],
      "year": "2003",
      "venue": "Proceedings of the International Ergonomics Association (IEA)"
    },
    {
      "citation_id": "76",
      "title": "Exploring behavioral expressions of player experience in digital games",
      "authors": [
        "W Van Den Hoogen",
        "W Ijsselsteijn",
        "Y De Kort"
      ],
      "year": "2008",
      "venue": "Proceedings of the Workshop on Facial and Bodily Expression for Control and Adaptation of Games (ECAG)"
    },
    {
      "citation_id": "77",
      "title": "Automatic detection of learner's affect from gross body language",
      "authors": [
        "S Mello",
        "A Graesser"
      ],
      "year": "2009",
      "venue": "Applied Artificial Intelligence"
    },
    {
      "citation_id": "78",
      "title": "Modeling multimodal expression of user's affective subjective experience",
      "authors": [
        "N Bianchi-Berthouze",
        "C Lisetti"
      ],
      "year": "2002",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "79",
      "title": "The role of haptics in games",
      "authors": [
        "M Orozco",
        "J Silva",
        "A Saddik",
        "E Petriu"
      ],
      "year": "2012",
      "venue": "Haptics Rendering and Applications"
    },
    {
      "citation_id": "80",
      "title": "Children's intuitive gestures in vision-based action games",
      "authors": [
        "J Höysniemi",
        "P Hämäläinen",
        "L Turkki",
        "T Rouvi"
      ],
      "year": "2005",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "81",
      "title": "Comparing feature sets for acted and spontaneous speech in view of automatic emotion recognition",
      "authors": [
        "T Vogt",
        "E André"
      ],
      "year": "2005",
      "venue": "Proceedings of IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "82",
      "title": "Towards adapting fantasy, curiosity and challenge in multimodal dialogue systems for preschoolers",
      "authors": [
        "T Kannetis",
        "A Potamianos"
      ],
      "year": "2009",
      "venue": "Proceedings of International Conference on Multimodal Interfaces (ICMI)"
    },
    {
      "citation_id": "83",
      "title": "Vocal expression of affect",
      "authors": [
        "P Juslin",
        "K Scherer"
      ],
      "year": "2005",
      "venue": "Vocal expression of affect"
    },
    {
      "citation_id": "84",
      "title": "Vocal communication of emotion",
      "authors": [
        "T Johnstone",
        "K Scherer"
      ],
      "year": "2000",
      "venue": "Handbook of emotions"
    },
    {
      "citation_id": "85",
      "title": "Acoustic profiles in vocal emotion expression",
      "authors": [
        "R Banse",
        "K Scherer"
      ],
      "year": "1996",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "86",
      "title": "Opinion mining and sentiment analysis",
      "authors": [
        "B Pang",
        "L Lee"
      ],
      "year": "2008",
      "venue": "Foundations and Trends in Information Retrieval"
    },
    {
      "citation_id": "87",
      "title": "Aesthetic Considerations for Automated Platformer Design",
      "authors": [
        "M Cook",
        "S Colton",
        "A Pease"
      ],
      "year": "2012",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE)"
    },
    {
      "citation_id": "88",
      "title": "Towards the automatic generation of fictional ideas for games",
      "authors": [
        "M Llano",
        "M Cook",
        "C Guckelsberger",
        "S Colton",
        "R Hepworth"
      ],
      "year": "2014",
      "venue": "Experimental AI in Games (EXAG'14), a Workshop collocated with the Tenth Annual AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE'14)"
    },
    {
      "citation_id": "89",
      "title": "Designing and utilizing biofeedback games for emotion regulation: The case of nevermind",
      "authors": [
        "A Lobel",
        "M Gotsis",
        "E Reynolds",
        "M Annetta",
        "R Engels",
        "I Granic"
      ],
      "year": "2016",
      "venue": "Extended Abstracts of the CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "90",
      "title": "The Affect Game AnnotatIoN (AGAIN) dataset",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "The Affect Game AnnotatIoN (AGAIN) dataset",
      "arxiv": "arXiv:2104.02643"
    },
    {
      "citation_id": "91",
      "title": "Game analytics",
      "authors": [
        "M El-Nasr",
        "A Drachen",
        "A Canossa"
      ],
      "year": "2016",
      "venue": "Game analytics"
    },
    {
      "citation_id": "92",
      "title": "The challenge of believability in video games: Definitions, agents models and imitation learning",
      "authors": [
        "F Tencé",
        "C Buche",
        "P Loor",
        "O Marc"
      ],
      "year": "2010",
      "venue": "The challenge of believability in video games: Definitions, agents models and imitation learning",
      "arxiv": "arXiv:1009.0451"
    },
    {
      "citation_id": "93",
      "title": "Illustrating reinforcement learning from human feedback (rlhf)",
      "authors": [
        "N Lambert",
        "L Castricato",
        "L Werra",
        "A Havrilla"
      ],
      "year": "2022",
      "venue": "Hugging Face Blog"
    },
    {
      "citation_id": "94",
      "title": "Gpt-3: Its nature, scope, limits, and consequences",
      "authors": [
        "L Floridi",
        "M Chiriatti"
      ],
      "year": "2020",
      "venue": "Minds and Machines"
    },
    {
      "citation_id": "95",
      "title": "GPT-4 technical report",
      "authors": [
        "Openai"
      ],
      "year": "2023",
      "venue": "GPT-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "96",
      "title": "Upeq: ubisoft perceived experience questionnaire: a self-determination evaluation tool for video games",
      "authors": [
        "A Azadvar",
        "A Canossa"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on the Foundations of Digital Games (FDG)"
    },
    {
      "citation_id": "97",
      "title": "Your gameplay says it all: Modelling motivation in Tom Clancy's The Division",
      "authors": [
        "D Melhart",
        "A Azadvar",
        "A Canossa",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proceedings of the Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "98",
      "title": "The ordinal nature of emotions: An emerging approach",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "99",
      "title": "The ordinal nature of emotions",
      "year": "2017",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "100",
      "title": "Ratings and rankings: Reconsidering the structure of values and their measurement",
      "authors": [
        "S Ovadia"
      ],
      "year": "2004",
      "venue": "International Journal of Social Research Methodology"
    },
    {
      "citation_id": "101",
      "title": "Annotation and processing of continuous emotional attributes: Challenges and opportunities",
      "authors": [
        "A Metallinou",
        "S Narayanan"
      ],
      "year": "2013",
      "venue": "Proceedings of the Proceedings of the IEEE Conference and workshops on automatic face and gesture recognition"
    },
    {
      "citation_id": "102",
      "title": "The visual analogue scale: its use in pain measurement",
      "authors": [
        "G Langley",
        "H Sheppeard"
      ],
      "year": "1985",
      "venue": "Rheumatology International"
    },
    {
      "citation_id": "103",
      "title": "Ratings are overrated!",
      "authors": [
        "G Yannakakis",
        "H Martínez"
      ],
      "year": "2015",
      "venue": "Frontiers in ICT"
    },
    {
      "citation_id": "104",
      "title": "Rating vs. preference: A comparative study of self-reporting",
      "authors": [
        "G Yannakakis",
        "J Hallam"
      ],
      "year": "2011",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "105",
      "title": "The Dynamics between Student Affect and Behavior Occurring Outside of Educational Software",
      "authors": [
        "R Baker",
        "G Moore",
        "A Wagner",
        "J Kalka",
        "A Salvi",
        "M Karabinos",
        "C Ashe",
        "D Yaron"
      ],
      "year": "2011",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "106",
      "title": "Form as a cue in the automatic recognition of non-acted affective body expressions",
      "authors": [
        "A Kleinsmith",
        "N Bianchi-Berthouze"
      ],
      "year": "2011",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "107",
      "title": "Automatic recognition of boredom in video games using novel biosignal moment-based features",
      "authors": [
        "D Giakoumis",
        "D Tzovaras",
        "K Moustakas",
        "G Hassapis"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "108",
      "title": "Faceengage: robust estimation of gameplay engagement from user-contributed (youtube) videos",
      "authors": [
        "X Chen",
        "L Niu",
        "A Veeraraghavan",
        "A Sabharwal"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "109",
      "title": "Don't Classify Ratings of Affect; Rank them",
      "authors": [
        "H Martínez",
        "G Yannakakis",
        "J Hallam"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "110",
      "title": "Preference learning",
      "authors": [
        "J Fürnkranz",
        "E Hüllermeier"
      ],
      "year": "2010",
      "venue": "Preference learning"
    },
    {
      "citation_id": "111",
      "title": "Towards automatic personalized content generation for platform games",
      "authors": [
        "N Shaker",
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2010",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment"
    },
    {
      "citation_id": "112",
      "title": "PAGAN: Video affect annotation made easy",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "113",
      "title": "Preference learning with Gaussian processes",
      "authors": [
        "W Chu",
        "Z Ghahramani"
      ],
      "year": "2005",
      "venue": "Proceedings of the International Conference on Machine learning (ICML)"
    },
    {
      "citation_id": "114",
      "title": "Learning deep physiological models of affect",
      "authors": [
        "H Martínez",
        "Y Bengio",
        "G Yannakakis"
      ],
      "year": "2013",
      "venue": "IEEE Computational Intelligence Magazine"
    },
    {
      "citation_id": "115",
      "title": "From ranknet to lambdarank to lambdamart: An overview",
      "authors": [
        "C Burges"
      ],
      "year": "2010",
      "venue": "Learning"
    },
    {
      "citation_id": "116",
      "title": "Emotion recognition from 3d motion capture data using deep cnns",
      "authors": [
        "H Zacharatos",
        "C Gatzoulis",
        "P Charalambous",
        "Y Chrysanthou"
      ],
      "year": "2021",
      "venue": "2021 Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "117",
      "title": "Affranknet+: ranking affect using privileged information",
      "authors": [
        "K Makantasis"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "118",
      "title": "Rankneat: Outperforming stochastic gradient search in preference learning tasks",
      "authors": [
        "K Pinitas",
        "K Makantasis",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2022",
      "venue": "Rankneat: Outperforming stochastic gradient search in preference learning tasks",
      "arxiv": "arXiv:2204.06901"
    },
    {
      "citation_id": "119",
      "title": "Text categorization with support vector machines: Learning with many relevant features",
      "authors": [
        "T Joachims"
      ],
      "year": "1998",
      "venue": "Machine Learning: ECML-98"
    },
    {
      "citation_id": "120",
      "title": "PyPLT: Python preference learning toolbox",
      "authors": [
        "E Camilleri",
        "G Yannakakis",
        "D Melhart",
        "A Liapis"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "121",
      "title": "Learning policies for first person shooter games using inverse reinforcement learning",
      "authors": [
        "B Tastan",
        "G Sukthankar"
      ],
      "year": "2011",
      "venue": "Proceedings of the Artificial Intelligence and Interactive Digital Entertainment Conference (AIIDE)"
    },
    {
      "citation_id": "122",
      "title": "Acting with style: Towards designer centred reinforcement learning for the videogames industry",
      "authors": [
        "B Aytemiz",
        "M Jacob",
        "S Devlin"
      ],
      "year": "2022",
      "venue": "Reinforcement Learning for Humans, Computers and Interaction Workshop at CHI"
    },
    {
      "citation_id": "123",
      "title": "Generative agents for player decision modeling in games",
      "authors": [
        "C Holmgård",
        "A Liapis",
        "J Togelius",
        "G Yannakakis"
      ],
      "year": "2014",
      "venue": "FDG"
    },
    {
      "citation_id": "124",
      "title": "Go-blend behavior and affect",
      "authors": [
        "M Barthet",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "125",
      "title": "Generative personas that behave and experience like humans",
      "authors": [
        "M Barthet",
        "A Khalifa",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2022",
      "venue": "Proceedings of the Conference on the Foundations of Digital Games (FDG)"
    },
    {
      "citation_id": "126",
      "title": "Evolving personas for player decision modeling",
      "authors": [
        "C Holmgård",
        "A Liapis",
        "J Togelius",
        "G Yannakakis"
      ],
      "year": "2014",
      "venue": "Proceedings of the Conference on Computational Intelligence and Games (CIG)"
    },
    {
      "citation_id": "127",
      "title": "Strategy Detection in Wuzzit: A Decision Theoretic Approach",
      "authors": [
        "T.-H Nguyen",
        "S Subramanian",
        "M El-Nasr",
        "A Canossa"
      ],
      "year": "2014",
      "venue": "Proceedings of the International Conference on Learning Science-Workshop on Learning Analytics for Learning and Becoming a Practice"
    },
    {
      "citation_id": "128",
      "title": "Play with emotion: Affect-driven reinforcement learning",
      "authors": [
        "M Barthet",
        "A Khalifa",
        "A Liapis",
        "G Yannakakis"
      ],
      "venue": "Proceedings of the International Conference on Affective Computing and Intelligence Interaction (ACII)"
    },
    {
      "citation_id": "129",
      "title": "First return, then explore",
      "authors": [
        "A Ecoffet",
        "J Huizinga",
        "J Lehman",
        "K Stanley",
        "J Clune"
      ],
      "year": "2021",
      "venue": "Nature"
    },
    {
      "citation_id": "130",
      "title": "Experience-driven procedural content generation",
      "authors": [
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "131",
      "title": "Adaptive music composition for games",
      "authors": [
        "P Hutchings",
        "J Mccormack"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Games"
    },
    {
      "citation_id": "132",
      "title": "Who willed it? decreasing frustration by manipulating perceived control through fabricated input for stroke rehabilitation bci games",
      "authors": [
        "B Hougaard",
        "I Rossau",
        "J Czapla",
        "M Miko",
        "R Skammelsen",
        "H Knoche",
        "M Jochumsen"
      ],
      "year": "2021",
      "venue": "Proceedings of the Annual Symposium on Computer-Human Interaction in Play"
    },
    {
      "citation_id": "133",
      "title": "SenToy in FantasyA: Designing an Affective Sympathetic Interface to a Computer Game",
      "authors": [
        "A Paiva",
        "G Andersson",
        "K Hook",
        "D Costa",
        "C Martinho"
      ],
      "year": "2002",
      "venue": "Personal and Ubiquitous Computing"
    },
    {
      "citation_id": "134",
      "title": "FearNot!an experiment in emergent narrative",
      "authors": [
        "R Aylett",
        "S Louchart",
        "J Dias",
        "A Paiva",
        "M Vala"
      ],
      "year": "2005",
      "venue": "Intelligent Virtual Agents"
    },
    {
      "citation_id": "135",
      "title": "Good moods: outlook, affect and mood in dynemotion and the mind module",
      "authors": [
        "M Eladhari",
        "M Sellers"
      ],
      "year": "2008",
      "venue": "Proceedings of the Conference on Future Play: Research, Play, Share"
    },
    {
      "citation_id": "136",
      "title": "A domain-independent framework for modeling emotion",
      "authors": [
        "J Gratch",
        "S Marsella"
      ],
      "year": "2004",
      "venue": "Cognitive Systems Research"
    },
    {
      "citation_id": "137",
      "title": "Creating individual agents through personality traits",
      "authors": [
        "T Doce",
        "J Dias",
        "R Prada",
        "A Paiva"
      ],
      "year": "2010",
      "venue": "Proceedings of the International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "138",
      "title": "An image synthesizer",
      "authors": [
        "K Perlin"
      ],
      "year": "1985",
      "venue": "ACM SIGGRAPH Computer Graphics"
    },
    {
      "citation_id": "139",
      "title": "Real-time deep dynamic characters",
      "authors": [
        "M Habermann",
        "L Liu",
        "W Xu",
        "M Zollhoefer",
        "G Pons-Moll",
        "C Theobalt"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Graphics (TOG)"
    },
    {
      "citation_id": "140",
      "title": "Searchbased procedural content generation: A taxonomy and survey",
      "authors": [
        "J Togelius",
        "G Yannakakis",
        "K Stanley",
        "C Browne"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games"
    },
    {
      "citation_id": "141",
      "title": "Deep learning for procedural content generation",
      "authors": [
        "J Liu",
        "S Snodgrass",
        "A Khalifa",
        "S Risi",
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "142",
      "title": "Experience-driven procedural content generation",
      "authors": [
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2015",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "143",
      "title": "An experiment in automatic game design",
      "authors": [
        "J Togelius",
        "J Schmidhuber"
      ],
      "year": "2008",
      "venue": "Computational Intelligence and Games, 2008. CIG'08. IEEE Symposium On"
    },
    {
      "citation_id": "144",
      "title": "Dl-dda-deep learning based dynamic difficulty adjustment with ux and gameplay constraints",
      "authors": [
        "D Or",
        "M Kolomenkin",
        "G Shabat"
      ],
      "year": "2021",
      "venue": "2021 Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "145",
      "title": "Architectural form and affect: A spatiotemporal study of arousal",
      "authors": [
        "E Xylakis",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "146",
      "title": "Towards multiobjective procedural map generation",
      "authors": [
        "J Togelius",
        "M Preuss",
        "G Yannakakis"
      ],
      "year": "2010",
      "venue": "Proceedings of the Workshop on Procedural Content Generation in Games"
    },
    {
      "citation_id": "147",
      "title": "Crowdsourcing the aesthetics of platform games",
      "authors": [
        "N Shaker",
        "G Yannakakis",
        "J Togelius"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games"
    },
    {
      "citation_id": "148",
      "title": "Evolving robust and specialized car racing skills",
      "authors": [
        "J Togelius",
        "S Lucas"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE Congress on Evolutionary Computation"
    },
    {
      "citation_id": "149",
      "title": "Tropetwist: Trope-based narrative structure generation",
      "authors": [
        "A Alvarez",
        "J Font"
      ],
      "year": "2022",
      "venue": "Tropetwist: Trope-based narrative structure generation",
      "arxiv": "arXiv:2204.09672"
    },
    {
      "citation_id": "150",
      "title": "Theory of fun for game design",
      "authors": [
        "R Koster"
      ],
      "year": "2013",
      "venue": "Theory of fun for game design"
    },
    {
      "citation_id": "151",
      "title": "Difficulty scaling of game AI",
      "authors": [
        "P Spronck",
        "I Sprinkhuizen-Kuyper",
        "E Postma"
      ],
      "year": "2004",
      "venue": "Proceedings of the International Conference on Intelligent Games and Simulation"
    },
    {
      "citation_id": "152",
      "title": "Extending reinforcement learning to provide dynamic game balancing",
      "authors": [
        "G Andrade",
        "G Ramalho",
        "H Santana",
        "V Corruble"
      ],
      "year": "2005",
      "venue": "Proceedings of the Workshop on Reasoning, Representation, and Learning in Computer Games, International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "153",
      "title": "Biofeedback in gameplay: How Valve measures physiology to enhance gaming experience",
      "authors": [
        "M Ambinder"
      ],
      "year": "2011",
      "venue": "Game Developers Conference"
    },
    {
      "citation_id": "154",
      "title": "Experience-driven pcg via reinforcement learning: A super mario bros study",
      "authors": [
        "T Shu",
        "J Liu",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "2021 Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "155",
      "title": "The fun facets of mario: Multifaceted experience-driven pcg via reinforcement learning",
      "authors": [
        "Z Wang",
        "J Liu",
        "G Yannakakis"
      ],
      "venue": "Proceedings of the Conference on the Foundations of Digital Games (FDG)"
    },
    {
      "citation_id": "156",
      "title": "Effectiveness of virtual reality survival horror games for the emotional elicitation: Preliminary insights using resident evil 7: Biohazard",
      "authors": [
        "F Pallavicini",
        "A Ferrari",
        "A Pepe",
        "G Garcea",
        "A Zanacchi",
        "F Mantovani"
      ],
      "year": "2018",
      "venue": "Proceedings of the International Conference on Universal Access in Human-Computer Interaction"
    },
    {
      "citation_id": "157",
      "title": "Influencing the affective state and attention restoration in vr-supported psychotherapy",
      "authors": [
        "D Pisalski",
        "M Hierhager",
        "C Stein",
        "M Zaudig",
        "C Bichlmeier"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "158",
      "title": "Virtual reality for emotion elicitation-a review",
      "authors": [
        "R Somarathna",
        "T Bednarz",
        "G Mohammadi"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "159",
      "title": "Gaming in virtual reality: What changes in terms of usability, emotional response and sense of presence compared to non-immersive video games?",
      "authors": [
        "F Pallavicini",
        "A Pepe",
        "M Minissi"
      ],
      "year": "2019",
      "venue": "Simulation & Gaming"
    },
    {
      "citation_id": "160",
      "title": "Induction of emotional states in educational video games through a fuzzy control system",
      "authors": [
        "C Lara-Alvarez",
        "H Mitre-Hernandez",
        "J Flores",
        "H Pérez-Espinosa"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "161",
      "title": "Gaming away stress: Using biofeedback games to learn paced breathing",
      "authors": [
        "M Zafar",
        "B Ahmed",
        "R Rihawi",
        "R Gutierrez-Osuna"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "162",
      "title": "Analysis of video game players' emotions and team performance: An esports tournament case study",
      "authors": [
        "S Abramov",
        "A Korotin",
        "A Somov",
        "E Burnaev",
        "A Stepanov",
        "D Nikolaev",
        "M Titova"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "163",
      "title": "Grounding truth via ordinal annotation",
      "authors": [
        "G Yannakakis",
        "H Martinez"
      ],
      "year": "2015",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "164",
      "title": "Towards general models of player experience: A study within genres",
      "authors": [
        "D Melhart",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "2021 Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "165",
      "title": "Visual biofeedback and game adaptation in relaxation skill transfer",
      "authors": [
        "A Parnandi",
        "R Gutierrez-Osuna"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "166",
      "title": "Adaptive virtual reality horror games based on machine learning and player modeling",
      "authors": [
        "E De Lima",
        "B Silva",
        "G Galam"
      ],
      "year": "2022",
      "venue": "Entertainment Computing"
    },
    {
      "citation_id": "167",
      "title": "Studying believability assessment in racing games",
      "authors": [
        "C Pacheco",
        "L Tokarchuk",
        "D Pérez-Liébana"
      ],
      "year": "2018",
      "venue": "Proceedings of the international Conference on the foundations of digital games (FDG)"
    },
    {
      "citation_id": "168",
      "title": "Analysis of the protocols used to assess virtual players in multi-player computer games",
      "authors": [
        "C Even",
        "A.-G Bosser",
        "C Buche"
      ],
      "year": "2017",
      "venue": "Proceedings of the International Work-Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "169",
      "title": "Rcea-360vr: Realtime, continuous emotion annotation in 360°vr videos for collecting precise viewport-dependent ground truth labels",
      "authors": [
        "T Xue",
        "A Ali",
        "T Zhang",
        "G Ding",
        "P Cesar"
      ],
      "year": "2021",
      "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems",
      "doi": "10.1145/3411764.3445487"
    },
    {
      "citation_id": "170",
      "title": "Predicting purchase decisions in mobile free-to-play games",
      "authors": [
        "R Sifa",
        "F Hadiji",
        "J Runge",
        "A Drachen",
        "K Kersting",
        "C Bauckhage"
      ],
      "year": "2015",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment"
    },
    {
      "citation_id": "171",
      "title": "Entertainment modeling through physiology in physical play",
      "authors": [
        "G Yannakakis",
        "J Hallam"
      ],
      "year": "2008",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "172",
      "title": "Research and development tools in affective computing",
      "authors": [
        "M Hussain",
        "S Mello",
        "R Calvo"
      ],
      "year": "2014",
      "venue": "The oxford handbook of affective computing"
    },
    {
      "citation_id": "173",
      "title": "Affdex sdk: a cross-platform real-time multi-face expression recognition toolkit",
      "authors": [
        "D Mcduff",
        "A Mahmoud",
        "M Mavadati",
        "M Amr",
        "J Turcot",
        "R Kaliouby"
      ],
      "year": "2016",
      "venue": "Proceedings of the CHI conference extended abstracts on human factors in computing systems"
    },
    {
      "citation_id": "174",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE international conference on automatic face & gesture recognition (FG)"
    },
    {
      "citation_id": "175",
      "title": "Neural network based facial expression analysis of gameevents: a cautionary tale",
      "authors": [
        "S Roohi",
        "J Takatalo",
        "J Kivikangas",
        "P Hämäläinen"
      ],
      "year": "2018",
      "venue": "Proceedings of the Annual Symposium on Computer-Human Interaction in Play"
    },
    {
      "citation_id": "176",
      "title": "Fusing visual and behavioral cues for modeling user experience in games",
      "authors": [
        "N Shaker",
        "S Asteriadis",
        "G Yannakakis",
        "K Karpouzis"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "177",
      "title": "'feeltrace': An instrument for recording perceived emotion in real time",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "S Savvidou",
        "E Mcmahon",
        "M Sawey",
        "M Schröder"
      ],
      "year": "2000",
      "venue": "ISCA tutorial and research workshop (ITRW) on speech and emotion"
    },
    {
      "citation_id": "178",
      "title": "Anvil-a generic annotation tool for multimodal dialogue",
      "authors": [
        "M Kipp"
      ],
      "year": "2001",
      "venue": "Seventh European Conference on Speech Communication and Technology"
    },
    {
      "citation_id": "179",
      "title": "Elan: a professional framework for multimodality research",
      "authors": [
        "P Wittenburg",
        "H Brugman",
        "A Russel",
        "A Klassmann",
        "H Sloetjes"
      ],
      "year": "2006",
      "venue": "5th International Conference on Language Resources and Evaluation (LREC 2006)"
    },
    {
      "citation_id": "180",
      "title": "Emujoy: Software for continuous measurement of perceived emotions in music",
      "authors": [
        "F Nagel",
        "R Kopiez",
        "O Grewe",
        "E Altenmüller"
      ],
      "year": "2007",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "181",
      "title": "Affectbutton: A method for reliable and valid affective self-report",
      "authors": [
        "J Broekens",
        "W.-P Brinkman"
      ],
      "year": "2013",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "182",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "183",
      "title": "Gtrace: General trace program compatible with EmotionML",
      "authors": [
        "R Cowie",
        "M Sawey",
        "C Doherty",
        "J Jaimovich",
        "C Fyans",
        "P Stapleton"
      ],
      "year": "2013",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "184",
      "title": "Carma: Software for continuous affect rating and media annotation",
      "authors": [
        "J Girard"
      ],
      "year": "2014",
      "venue": "Journal of Open Research Software"
    },
    {
      "citation_id": "185",
      "title": "Ranktrace: Relative and unbounded affect annotation",
      "authors": [
        "P Lopes",
        "G Yannakakis",
        "A Liapis"
      ],
      "year": "2017",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "186",
      "title": "Darma: Software for dual axis rating and media annotation",
      "authors": [
        "J Girard",
        "A Wright"
      ],
      "year": "2018",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "187",
      "title": "Novaa tool for explainable cooperative machine learning",
      "authors": [
        "A Heimerl",
        "T Baur",
        "F Lingenfelser",
        "J Wagner",
        "E André"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "188",
      "title": "Rcea: Realtime, continuous emotion annotation for collecting precise mobile video ground truth labels",
      "authors": [
        "T Zhang",
        "A Ali",
        "C Wang",
        "A Hanjalic",
        "P Cesar"
      ],
      "year": "2020",
      "venue": "Proceedings of the CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "189",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "190",
      "title": "Deep learning vs. kernel methods: Performance for emotion prediction in videos",
      "authors": [
        "Y Baveye",
        "E Dellandréa",
        "C Chamaret",
        "L Chen"
      ],
      "year": "2015",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "191",
      "title": "Emotion recognition from embedded bodily expressions and speech during dyadic interactions",
      "authors": [
        "P Müller",
        "S Amin",
        "P Verma",
        "M Andriluka",
        "A Bulling"
      ],
      "year": "2015",
      "venue": "Proceedings of Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "192",
      "title": "The mediaeval 2016 emotional impact of movies task",
      "authors": [
        "E Dellandréa",
        "L Chen",
        "Y Baveye",
        "M Sjöberg",
        "C Chamaret"
      ],
      "year": "2016",
      "venue": "Proceedings of CEUR Workshop"
    },
    {
      "citation_id": "193",
      "title": "Automated action units vs. expert raters: Face off",
      "authors": [
        "S Dhamija",
        "T Boult"
      ],
      "year": "2018",
      "venue": "Proceedings of IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "194",
      "title": "Emotional context modulates subsequent memory effect",
      "authors": [
        "S Erk",
        "M Kiefer",
        "J Grothe",
        "A Wunderlich",
        "M Spitzer",
        "H Walter"
      ],
      "year": "2003",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "195",
      "title": "Designing micro-intelligences for situated affective computing",
      "authors": [
        "P Lovei",
        "I Nazarchuk",
        "S Aslam",
        "B Yu",
        "C Megens",
        "N Sidorova"
      ],
      "venue": "Proceedings of the Workshops on Computer-Human Interaction in IoT Applications (CHIIoT)"
    },
    {
      "citation_id": "196",
      "title": "Smartphone-based content annotation for ground truth collection in affective computing",
      "authors": [
        "G Salvador",
        "P Bota",
        "V Vinayagamoorthy",
        "H Da Silva",
        "A Fred"
      ],
      "year": "2021",
      "venue": "Proceedings of the ACM International Conference on Interactive Media Experiences"
    },
    {
      "citation_id": "197",
      "title": "Towards affective camera control in games",
      "authors": [
        "G Yannakakis",
        "H Martínez",
        "A Jhala"
      ],
      "year": "2010",
      "venue": "User Modeling and User-Adapted Interaction"
    },
    {
      "citation_id": "198",
      "title": "The platformer experience dataset",
      "authors": [
        "K Karpouzis",
        "G Yannakakis",
        "N Shaker",
        "S Asteriadis"
      ],
      "year": "2015",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "199",
      "title": "The FUNii database: A physiological, behavioral, demographic and subjective video game database for affective gaming and player experience research",
      "authors": [
        "N Beaudoin-Gagnon",
        "A Fortin-Côté",
        "C Chamberland",
        "L Lefebvre",
        "J Bergeron-Boucher",
        "A Campeau-Lecours",
        "S Tremblay",
        "P Jackson"
      ],
      "year": "2019",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "200",
      "title": "DEAP: A database for emotion analysis using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "201",
      "title": "Aff-wild: Valence and arousal 'in-the-wild' challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Proceedings of the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "202",
      "title": "AffectNet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "203",
      "title": "Introducing the RECOLA multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "Proceedings of the Proceedings of the IEEE Conference and workshops on automatic face and gesture recognition"
    },
    {
      "citation_id": "204",
      "title": "SEWA DB: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "205",
      "title": "GAME-ON: a multimodal dataset for cohesion and group analysis",
      "authors": [
        "L Maman",
        "E Ceccaldi",
        "N Lehmann-Willenbrock",
        "L Likforman-Sulem",
        "M Chetouani",
        "G Volpe",
        "G Varni"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "206",
      "title": "MUMBAI: multiperson, multimodal board game affect and interaction analysis dataset",
      "authors": [
        "M Doyran",
        "A Schimmel",
        "P Baki",
        "K Ergin",
        "B Türkmen",
        "A Salah",
        "S Bakkes",
        "H Kaya",
        "R Poppe",
        "A Salah"
      ],
      "year": "2021",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "207",
      "title": "LIRIS-ACCEDE: a video database for affective content analysis",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "L Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "208",
      "title": "The tower game dataset: A multimodal dataset for analyzing social interaction predicates",
      "authors": [
        "D Salter",
        "A Tamrakar",
        "B Siddiquie",
        "M Amer",
        "A Divakaran",
        "B Lande",
        "D Mehri"
      ],
      "year": "2015",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "209",
      "title": "Towards an \"in-thewild\" emotion dataset using a game-based framework",
      "authors": [
        "W Li",
        "F Abtahi",
        "C Tsangouri",
        "Z Zhu"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "210",
      "title": "Obstacle tower: a generalization challenge in vision, control, and planning",
      "authors": [
        "A Juliani",
        "A Khalifa",
        "V.-P Berges",
        "J Harper",
        "E Teng",
        "H Henry",
        "A Crespi",
        "J Togelius",
        "D Lange"
      ],
      "year": "2019",
      "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "211",
      "title": "Gset somi: a game-specific eye tracking dataset for somi",
      "authors": [
        "H Ahmadi",
        "S Tootaghaj",
        "S Mowlaei",
        "M Hashemi",
        "S Shirmohammadi"
      ],
      "year": "2016",
      "venue": "Proceedings of the International Conference on Multimedia Systems"
    },
    {
      "citation_id": "212",
      "title": "An empirical study of players' emotions in vr racing games based on a dataset of physiological data",
      "authors": [
        "M Granato",
        "D Gadia",
        "D Maggiorini",
        "L Ripamonti"
      ],
      "year": "2020",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "213",
      "title": "Collection and validation of psychophysiological data from professional and amateur players: a multimodal esports dataset",
      "authors": [
        "A Smerdov",
        "B Zhou",
        "P Lukowicz",
        "A Somov"
      ],
      "year": "2020",
      "venue": "Collection and validation of psychophysiological data from professional and amateur players: a multimodal esports dataset",
      "arxiv": "arXiv:2011.00958"
    },
    {
      "citation_id": "214",
      "title": "Atari-head: Atari human eye-tracking and demonstration dataset",
      "authors": [
        "R Zhang",
        "C Walshe",
        "Z Liu",
        "L Guan",
        "K Muller",
        "J Whritner",
        "L Zhang",
        "M Hayhoe",
        "D Ballard"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "215",
      "title": "Biraffe2, a multimodal dataset for emotion-based personalization in rich affective game environments",
      "authors": [
        "K Kutt",
        "D Drazyk",
        "L Zuchowska",
        "M Szelazek",
        "S Bobek",
        "G Nalepa"
      ],
      "year": "2022",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "216",
      "title": "Towards a comprehensive model of mediating frustration in videogames",
      "authors": [
        "D Melhart"
      ],
      "year": "2018",
      "venue": "Game Studies"
    },
    {
      "citation_id": "217",
      "title": "Just an artifact: Why machines are perceived as moral agents",
      "authors": [
        "J Bryson",
        "P Kime"
      ],
      "year": "2011",
      "venue": "Proceedings of the International joint conference on artificial intelligence (IJCAI)"
    },
    {
      "citation_id": "218",
      "title": "Building ethics into artificial intelligence",
      "authors": [
        "H Yu",
        "Z Shen",
        "C Miao",
        "C Leung",
        "V Lesser",
        "Q Yang"
      ],
      "year": "2018",
      "venue": "Building ethics into artificial intelligence",
      "arxiv": "arXiv:1812.02953"
    },
    {
      "citation_id": "219",
      "title": "Fit for purpose? affective computing meets eu data protection law",
      "authors": [
        "A Häuselmann"
      ],
      "year": "2021",
      "venue": "Fit for purpose? affective computing meets eu data protection law"
    },
    {
      "citation_id": "220",
      "title": "The Cambridge handbook of artificial intelligence",
      "authors": [
        "N Bostrom",
        "E Yudkowsky"
      ],
      "year": "2014",
      "venue": "The Cambridge handbook of artificial intelligence"
    },
    {
      "citation_id": "221",
      "title": "Ethical considerations for player modeling",
      "authors": [
        "B Mikkelsen",
        "C Holmgard",
        "J Togelius"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence. AI Access Foundation"
    },
    {
      "citation_id": "222",
      "title": "Ethically aligned design of autonomous systems: Industry viewpoint and an empirical study",
      "authors": [
        "V Vakkuri",
        "K.-K Kemell",
        "J Kultanen",
        "M Siponen",
        "P Abrahamsson"
      ],
      "year": "2019",
      "venue": "Ethically aligned design of autonomous systems: Industry viewpoint and an empirical study",
      "arxiv": "arXiv:1906.07946"
    },
    {
      "citation_id": "223",
      "title": "The Ethics of AI in Games",
      "authors": [
        "D Melhart",
        "J Togelius",
        "B Mikkelsen",
        "C Holmgård",
        "G Yannakakis"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "224",
      "title": "A right to reasonable inferences: rethinking data protection law in the age of big data and ai",
      "authors": [
        "S Wachter",
        "B Mittelstadt"
      ],
      "year": "2019",
      "venue": "Columbia Business Law Review"
    },
    {
      "citation_id": "225",
      "title": "The eu general data protection regulation (gdpr)",
      "authors": [
        "P Voigt",
        "A Von",
        "Bussche"
      ],
      "year": "2017",
      "venue": "The eu general data protection regulation (gdpr)"
    },
    {
      "citation_id": "226",
      "title": "Two years after: A scoping review of gdpr effects on serious games research ethics reporting",
      "authors": [
        "P Jost",
        "M Lampert"
      ],
      "year": "2020",
      "venue": "Proceedings of the International Conference on Games and Learning Alliance"
    },
    {
      "citation_id": "227",
      "title": "The brussels effect and artificial intelligence: How eu regulation will impact the global ai market",
      "authors": [
        "C Siegmann",
        "M Anderljung"
      ],
      "year": "2022",
      "venue": "The brussels effect and artificial intelligence: How eu regulation will impact the global ai market",
      "arxiv": "arXiv:2208.12645"
    },
    {
      "citation_id": "228",
      "title": "Opportunities and challenges in explainable artificial intelligence (xai): A survey",
      "authors": [
        "A Das",
        "P Rad"
      ],
      "year": "2020",
      "venue": "Opportunities and challenges in explainable artificial intelligence (xai): A survey",
      "arxiv": "arXiv:2006.11371"
    },
    {
      "citation_id": "229",
      "title": "Open player modeling: Empowering players through data transparency",
      "authors": [
        "J Zhu",
        "M El-Nasr"
      ],
      "year": "2021",
      "venue": "Open player modeling: Empowering players through data transparency",
      "arxiv": "arXiv:2110.05810"
    },
    {
      "citation_id": "230",
      "title": "Behavioural evidence for a transparency-efficiency tradeoff in human-machine cooperation",
      "authors": [
        "F Ishowo-Oloko",
        "J.-F Bonnefon",
        "Z Soroye",
        "J Crandall",
        "I Rahwan",
        "T Rahwan"
      ],
      "year": "2019",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "231",
      "title": "We may not cooperate with friendly machines",
      "authors": [
        "M Rovatsos"
      ],
      "year": "2019",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "232",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "R Bommasani",
        "D Hudson",
        "E Adeli",
        "R Altman",
        "S Arora",
        "S Arx",
        "M Bernstein",
        "J Bohg",
        "A Bosselut",
        "E Brunskill"
      ],
      "year": "2021",
      "venue": "On the opportunities and risks of foundation models",
      "arxiv": "arXiv:2108.07258"
    },
    {
      "citation_id": "233",
      "title": "Ethical implications of bias in machine learning",
      "authors": [
        "A Yapo",
        "J Weiss"
      ],
      "year": "2018",
      "venue": "Proceedings of the Hawaii International Conference on System Sciences"
    },
    {
      "citation_id": "234",
      "title": "The Oxford handbook of ethics of AI",
      "authors": [
        "T Gebru"
      ],
      "year": "2020",
      "venue": "The Oxford handbook of ethics of AI"
    },
    {
      "citation_id": "235",
      "title": "Datasheets for datasets",
      "authors": [
        "T Gebru",
        "J Morgenstern",
        "B Vecchione",
        "J Vaughan",
        "H Wallach",
        "H Iii",
        "K Crawford"
      ],
      "year": "2021",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "236",
      "title": "Toward ai systems that augment and empower humans by understanding us, our society and the world around us",
      "authors": [
        "J Crowley",
        "A Orsullivan",
        "A Nowak",
        "C Jonker",
        "D Pedreschi",
        "F Giannotti",
        "Y Rogers"
      ],
      "year": "2019",
      "venue": "Report of 761758 EU Project HumaneAI"
    },
    {
      "citation_id": "237",
      "title": "Ethically aligned design: A vision for prioritizing human well-being with autonomous and intelligent systems",
      "year": "2019",
      "venue": "The IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems"
    },
    {
      "citation_id": "238",
      "title": "Trustworthy artificial intelligence",
      "authors": [
        "S Thiebes",
        "S Lins",
        "A Sunyaev"
      ],
      "year": "2021",
      "venue": "Electronic Markets"
    },
    {
      "citation_id": "239",
      "title": "The eu approach to ethics guidelines for trustworthy artificial intelligence",
      "authors": [
        "N Smuha"
      ],
      "year": "2019",
      "venue": "Computer Law Review International"
    },
    {
      "citation_id": "240",
      "title": "Binocular eye-tracking for the control of a 3D immersive multimedia user interface",
      "authors": [
        "N Sidorakis",
        "G Koulieris",
        "K Mania"
      ],
      "year": "2015",
      "venue": "IEEE Workshop on Everyday Virtual Reality (WEVR)"
    },
    {
      "citation_id": "241",
      "title": "Fantasy, curiosity and challenge as adaptation indicators in multimodal dialogue systems for preschoolers",
      "authors": [
        "T Kannetis",
        "A Potamianos",
        "G Yannakakis"
      ],
      "year": "2009",
      "venue": "Proceedings of the Workshop on Child, Computer and Interaction"
    },
    {
      "citation_id": "242",
      "title": "Detecting emotional state of a child in a conversational computer game",
      "authors": [
        "S Yildirim",
        "S Narayanan",
        "A Potamianos"
      ],
      "year": "2011",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "243",
      "title": "The somatic marker hypothesis and the possible functions of the prefrontal cortex [and discussion]",
      "authors": [
        "A Damasio",
        "B Everitt",
        "D Bishop"
      ],
      "year": "1996",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "244",
      "title": "The intelligence of emotional intelligence",
      "authors": [
        "J Mayer",
        "P Salovey"
      ],
      "year": "1993",
      "venue": "Intelligence"
    },
    {
      "citation_id": "245",
      "title": "A game-based corpus for analysing the interplay between game context and player experience",
      "authors": [
        "N Shaker",
        "S Asteriadis",
        "G Yannakakis",
        "K Karpouzis"
      ],
      "year": "2011",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "246",
      "title": "Deep multimodal fusion: Combining discrete events and continuous signals",
      "authors": [
        "H Martínez",
        "G Yannakakis"
      ],
      "year": "2014",
      "venue": "Proceedings of the Conference on Multimodal Interaction"
    },
    {
      "citation_id": "247",
      "title": "A generic approach for obtaining higher entertainment in predator/prey computer games",
      "authors": [
        "G Yannakakis",
        "J Hallam"
      ],
      "year": "2005",
      "venue": "Journal of Game Development"
    },
    {
      "citation_id": "248",
      "title": "A Generic Approach for Generating Interesting Interactive Pac-Man Opponents",
      "year": "2005",
      "venue": "Proceedings of the IEEE Symposium on Computational Intelligence and Games"
    },
    {
      "citation_id": "249",
      "title": "Generic physiological features as predictors of player experience",
      "authors": [
        "H Martínez",
        "M Garbarino",
        "G Yannakakis"
      ],
      "year": "2011",
      "venue": "Proceedings of the Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "250",
      "title": "Towards generic models of player experience",
      "authors": [
        "N Shaker",
        "M Shaker",
        "M Abou-Zleikha"
      ],
      "year": "2015",
      "venue": "Proceedings of the Conference on Artificial Intelligence and Interactive Digital Entertainment (AIIDE)"
    },
    {
      "citation_id": "251",
      "title": "The invariant ground truth of affect",
      "authors": [
        "K Makantasis",
        "K Pinitas",
        "A Liapis",
        "G Yannakakis"
      ],
      "venue": "International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)"
    },
    {
      "citation_id": "252",
      "title": "Contrastive representation learning: A framework and review",
      "authors": [
        "P Le-Khac",
        "G Healy",
        "A Smeaton"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "253",
      "title": "Contrastive learning of generalized game representations",
      "authors": [
        "C Trivedi",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "2021 Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "254",
      "title": "Learning task-independent game state representations from unlabeled images",
      "authors": [
        "C Trivedi",
        "K Makantasis",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2022",
      "venue": "Learning task-independent game state representations from unlabeled images",
      "arxiv": "arXiv:2206.06490"
    },
    {
      "citation_id": "255",
      "title": "Contrastive learning of subject-invariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "256",
      "title": "Supervised contrastive learning for affect modelling",
      "authors": [
        "K Pinitas",
        "K Makantasis",
        "A Liapis",
        "G Yannakakis"
      ],
      "venue": "Proceedings of the International Conference on Multimodal Interaction (ICMI)"
    },
    {
      "citation_id": "257",
      "title": "Text-To-Game: Game Engines Powered by Large Language Models",
      "authors": [
        "R Gallotta",
        "M Zammit",
        "K Sfikas",
        "D Baumik",
        "C Trivedi",
        "A Khalifa",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE Conference on Games (CoG)"
    },
    {
      "citation_id": "258",
      "title": "High-resolution image synthesis with latent diffusion models",
      "authors": [
        "R Rombach",
        "A Blattmann",
        "D Lorenz",
        "P Esser",
        "B Ommer"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "259",
      "title": "Affectgan: Affect-based generative art driven by semantics",
      "authors": [
        "T Galanos",
        "A Liapis",
        "G Yannakakis"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    }
  ]
}