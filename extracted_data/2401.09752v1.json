{
  "paper_id": "2401.09752v1",
  "title": "Improving Speaker-Independent Speech Emotion Recognition Using Dynamic Joint Distribution Adaptation",
  "published": "2024-01-18T06:52:52Z",
  "authors": [
    "Cheng Lu",
    "Yuan Zong",
    "Hailun Lian",
    "Yan Zhao",
    "Björn Schuller",
    "Wenming Zheng"
  ],
  "keywords": [
    "speaker-independent",
    "speech emotion recognition",
    "multi-source domain adaptation",
    "joint distribution adaptation"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In speaker-independent speech emotion recognition, the training and testing samples are collected from diverse speakers, leading to a multi-domain shift challenge across the feature distributions of data from different speakers. Consequently, when the trained model is confronted with data from new speakers, its performance tends to degrade. To address the issue, we propose a Dynamic Joint Distribution Adaptation (DJDA) method under the framework of multi-source domain adaptation. DJDA firstly utilizes joint distribution adaptation (JDA), involving marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA), to more precisely measure the multi-domain distribution shifts caused by different speakers. This helps eliminate speaker bias in emotion features, allowing for learning discriminative and speaker-invariant speech emotion features from coarse-level to fine-level. Furthermore, we quantify the adaptation contributions of MDA and CDA within JDA by using a dynamic balance factor based on A-Distance, promoting to effectively handle the unknown distributions encountered in data from new speakers. Experimental results demonstrate the superior performance of our DJDA as compared to other state-of-the-art (SOTA) methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) aims to enable machines to automatically comprehend the emotions conveyed in speech signals, and it has garnered significant attention in affective computing and pattern recognition  [1] ,  [2] ,  [3] . Particularly challenging is speakerindependent SER, which focuses on the training and speech samples collected from different speakers  [4] ,  [5] .\n\nIn this case, the SER model trained on annotated speech samples from known speakers often experiences a decline in performance when dealing with new testing data from unknown speakers  [6] . This degradation is primarily attributed to domain shift in feature distributions caused by speaker bias in the training data (source domain) and testing data (target domain)  [6] . Fortunately, recent SER studies have demonstrated that Domain Adaptation (DA) holds promise in mitigating the domain/speaker bias in SER  [7] ,  [8] ,  [9] . The fundamental concept of DA is to treat the training and testing datasets collected from diverse speakers as signal source and target domains, respectively  [6] . In speech representation learning, DA seeks a latent space of emotion features where the feature distributions of the source and target domains are sufficiently close. Consequently, the disparity in feature distributions betwween two domains can be eliminated, resulting in domain/speaker-invariant speech emotion features.\n\nAlthough DA-based methods have shown promise in SER, they all treat the training and testing datasets as two separate domains and aim to eliminate the discrepancy across them  [3] ,  [6] . However, since each speaker has unique pronunciation and expression habits when conveying emotions, a noticeable gap is existing in the feature distributions among different speakers' speech samples. In other words, the samples of each speaker can be considered as a separate domain. Moreover, the speaker information of the training data is typically known in speaker-independent SER, while that of the testing data is unknown. Therefore, the feature distributions of different speakers' data exhibit a multi-source domain situation  [6] . The issue of multi-domain distribution arising from different speakers poses two challenges in speaker-independent SER: (1) how to effectively and precisely measure the discrepancy across multiple domains, and (2) how to enable the model to adaptively handle unknown distributions of testing data from new speakers.\n\nTo tackle these challenges, we propose a Dynamic Joint Distribution Adaptation (DJDA) method to address the multi-domain distribution adaptation in speaker-independent SER, shown in Figure  1 . Regarding the first issue, DJDA employs a joint distribution adaptation (JDA) under multi-source domain adaptation. JDA includes both marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA), which systematically measures multi-domain distribution shifts caused by speakers. JDA can achieve distribution alignment through several discriminators of domain and speaker. For the second issue, we introduce a dynamic balance factor in JDA to quantify the adaptation contributions of MDA and CDA under multi-source domain adaptation situation. This enables more flexible adaptation to the unknown distributions in data from new speakers.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Method",
      "text": "To facilitate the introduction, we first formalize the source and target data as Ds = {(x s i , y s i , y sp i )} ns i=1 and Dt = {x t j }. Herein,  sents the speaker label of the k th sample, and k is the speaker number in the source dataset. ns and nt correspond to the sample numbers of source and target datasets, respectively. The input features of source and target data x s and x t are fed into the feature extractor G f (•) to obtain their high-level emotion features f s and f t , respectively. This process can be expressed as\n\nwhere θ f represents the parameters of G f (•). Then, we will construct Joint Distribution Adaptation (JDA) through marginal distribution adaptation (MDA) and conditional distribution adaptation (CDA).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Joint Distribution Adaptation Under Multi-Source Domain Adaptation",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Marginal Distribution Adaptation (Mda)",
      "text": "MDA aims to address the distribution discrepancy between the source and target data, as well as among multiple speakers within the source data. To accomplish these goals, we design the source-target domain discriminator Gst(f s , f t ; θst) and the source-domain speaker discriminator Gsp(f s ; θsp). Here, θst and θsp represent the network parameters of the discriminators Gst(•) and Gsp(•), respectively. Through an adversarial training strategy to update the parameters θst and θsp, Gst(•) and Gsp(•) can effectively obscure domain information within speech representations, resulting in the generation of domain/speaker invariant features. Thus, MDA loss L md can be obtained by combining the source-target released MDA loss Lst from Gst(•) and the speaker-related MDA loss Lsp from Gsp(•):\n\nwhere di ∈ {0, 1} represents the domain label of the i th speech sample, and the domain labels of the source and target domains are 0 and 1, respectively. y sp i ∈ [1, 2, ..., k] denotes the speaker label of the i th speech sample. J(•) is the cross-entropy loss function.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conditional Distribution Adaptation (Cda)",
      "text": "MDA focuses on the global distribution adaptation across multiple domains, while ignoring the local multi-class structure of emotional features. Therefore, we consider class-wise CDA to measure the fine-grained discrepancy in local feature distributions. To achieve this goal, we also need to confuse the information both between source and target domains as well as inside speaker domains under each class's features.\n\nSimilar to MDA, we first design a set of class-wise domain discriminators {G m cst } c m=1 to confuse source-target domain information of each class features, where G m cst represents the domain discriminator of the m th class's features. Furthermore, we note that each i th sample can generate emotion prediction probabilities through the emotion classifier G cls (•) with its parameters θy, denoted as ŷi = G cls (f s , f t ; θy). The probabilities provide valuable supervision for assessing the classification of each emotion. Consequently, we assign these probabilities weights to their corresponding features for enabling the domain discriminator to accurately distinguish domain labels bound on correct emotion prediction, thereby facilitating subsequent domain confusion. Through this process, we obtain the domain-related CDA loss function L dcd from the class-wise sourcetarget domain discriminator:\n\nwhere ŷm i represents the predicted probability of the m th class of the i th sample. L m cst is the source-target domain discriminator loss for the m th class features calculated by the cross-entropy loss function.\n\nNote that G cls (•) is trained based on source data, meaning that the emotion labels obtained for target data are pseudo-labels. As the network parameters are continuously optimized, the pseudo-labels will be gradually improved.\n\nThen, we construct a set of class-wise speaker discriminators {G m csp } c m=1 to discriminate the speaker labels of each emotion features in the source domain, where G m csp denotes the speaker discriminator for the m th class's features. Similarly, the predicted probability of each class is also utilized to weight the corresponding class features for ensuring that the speaker domain labels can be more accurately distinguished bound on the correct predicted emotions, which provides the basis for speaker information confusion. Therefore, the speaker-related CDA loss function L scd from the class-wise speaker discriminator can be denoted as follows:\n\nConsequently, we combine the Equation 2 and 3 to produce the overall CDA loss L cd as follows:",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Dynamic Joint Distribution Adaptation Under Multi-Source Domain Adaptation",
      "text": "Due to the unknown and diverse data distributions in the target domain, a practical approach is to empirically set a balance coefficient to adjust the contributions of MDA and CDA for JDA. However, this strategy lacks flexibility. Wang et al.  [10] ,  [11]  firstly introduced a dynamic adversarial factor based on A-distance to balance different distribution adaptation for signal-source DA. According to  [12] , the A-distance is related to minimum errors err(h) of classifiers (e. g., linear classifiers) when distinguishing the source and target domains, which can be defined as dA(Ds, Dt) = 2(1 -2err(h)).\n\nInspired by  [10] ,  [11] , we propose a dynamic balance factor to achieve adaptive JDA under multi-source domain adaptation. To produce the dynamic factor, we first calculate the A-distance of the multi-source domain marginal distribution. According to the definition of A-distance, we can utilize the MDA loss of the source-target domain and the speaker sub-domains of the source data obtained by multiple discriminators as the unbiased estimator of A-distance. Therefore, we define the A-distance of the marginal distribution as d md = 2(1 -2Lm). Similarly, the A-distance of the conditional distribution under m th emotion class d m cd can be denoted as\n\n). Combining the A-distance of MDA and CDA, the dynamic balance factor w of JDA under the multi-source DA can be generated as follows:\n\nFinally, we can obtain the total loss L total for DJDA under multisource domain adaptation as follows:\n\nwhere Lce (θ f , θy) = x i ∈Ds J (G f (x s i ) , y s i ) represents the cross-entropy loss function of the emotion class, and η is the penalty coefficient to balance different items. DNN-HMM  [17]  62.28 58.02 CNN+LSTM Model  [18]  68.80 59.40 CNN GRU-SeqCap  [19]  72.73 59.71 FCN+Attention  [20]  70.40 63.90 Model-3 Fusion  [21]  72.34 58.31 ADARL  [7]  73.02 65.86 ATFNN  [3]  73 GerDA  [16]  81.90 79.10 DNN ELM  [22]  77.01 76.98 ComParE SVM  [23]  N/A 86.00 SDFA  [24]  86.65 N/A DTPM  [25]  87.31 86.30 DANN  [26]  85.98 84.61 DAN  [27]  86.36 85.30 DIFL VGG6  [6]  89.72 88.49 DJDA (ours) 89.91 88.69\n\n3. EXPERIMENTS",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Experimental Dataset",
      "text": "The experiments utilize two public speech emotion datasets: the English Multimodal Emotion Database (IEMOCAP)  [13]  and the Berlin German Emotion Dataset (Emo-DB)  [14] . In detail, we utilize the improvised data in the IEMOCAP dataset, comprising a total of 2280 speech samples with 4 emotions: Angry, Happy, Sad, and Neutral. For the Emo-DB, 535 speech samples consisting of 7 emotions: Anger, Boredom, Disgust, Fear, Happiness, Sadness, and Neutral, are adopted to conduct our experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setting",
      "text": "In our experiments, the log-Mel-spectrograms of all speech samples are extracted as the input features. Moreover, to evaluate the method's performance effectively, the leave-one-speaker-out (LOSO) crossvalidation  [15]  [16] is adopted as the experimental protocol. In terms of evaluation metrics, we employ Weighted Average Recall (WAR) and Unweighted Average Recall (UAR) to assess the recognition accuracy of comparison methods  [15] . For the proposed DJDA, We choose VGGNet as the feature extractor  [6] . The source-target and speaker discriminators in MDA are implemented as two-layer Multilayer Perception (MLP) with dimensions of (1024, 512, 2) and (1024, 512, speaker number), respectively. These discriminators resemble those used in CDA. Our DJDA model is implemented using PyTorch on NVIDIA GTX3090 GPUs and optimized using the Adam optimizer with an initial learning rate of 0.0005 and a batch size of 32.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "For the experiments on IEMOCAP, we selected several state-of-theart (SOTA) models as comparison methods, i. e., DNN-HMM  [17] , CNN+LSTM Model  [18] , CNN GRU-SeqCap  [19] , FCN+Attention  [20] , Model-3 fusion  [21] , ADARL  [7] , and ATFNN  [3] . The comparison results are presented in Table  1 . These results indicate that DJDA achieves the highest accuracy in both WAR (75.26%) and UAR (65.92%). Specifically, DJDA outperforms deep learningbased baseline methods (e. g., DNN-HMM, CNN+LSTM Model, and CNN GRU-SeqCap) by more than 5% in WAR results, and its performance also surpasses DA-based methods (e. g., ADARL).\n\nIn the experiments on Emo-DB, we chose GerDA  [16] , DNN ELM  [22] , ComParE SVM  [23] , SDFA  [24] , DTPM  [25] , DANN  [26] , DAN  [27] , and DIFL VGG6  [6]  as benchmark experimental methods. The comparison results of different methods are shown in Table  2 , where it is evident that DJDA achieves the best recognition performance, with a WAR of 89.91% and UAR of 88.69%. Furthermore, the results in Table  2  also demonstrate that DA-based methods (e. g., DANN, DAN, and DIFL VGG6) generally attain higher accuracy compared to non-DA methods (e. g., GerDA, DNN ELM, and DTPM). This can be attributed to the fact that DANN, DAN, and DIFL VGG6 consider the domain shifts caused by diverse speakers between training and testing data, thereby obtaining more speaker-invariant emotion features than non-DA-based methods. Moreover, our proposed DJDA exhibits better recognition performance than the DA-based method, indicating that dynamic joint distribution adaptation is more effective than single marginal distribution adaptation or conditional distribution adaptation.\n\nConsequently, it is evident that our DJDA achieves SOTA performance on IEMOCAP and Emo-DB, thereby fully demonstrating the effectiveness of DJDA for addressing speaker-independent SER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Experiments",
      "text": "To verify the performance of different modules in DJDA, we also design the corresponding ablation experiments on Emo-DB, and experimental results are presented in The experimental results in Figure  2  show that our proposed DJDA obtains the optimal results, indicating that DJDA can indeed effectively preserve the speaker-invariant robustness of speech emotion features. Specifically, CDA obtains better results than MDA due to its function on distribution discrepancy elimination of finer-grained various emotional features. Furthermore, the results of JDA are better than MDA and CDA because of similar reasons. The speaker-wise domain adaptation (w/o Lst&Lcst) achieves comparable performance  to the source-target-wise domain adaptation (w/o Lsp&Lcsp).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Discussion On The Dynamic Balance Coefficient",
      "text": "This section aims to investigate the performance of the dynamic balance coefficient in DJDA when applied to unknown distributions from different speakers. We visualize the variation of the dynamic balance coefficient values w on IEMOCAP and Emo-DB in Figure  3 , where the results are obtained from test samples of the different speakers in IEMOCAP and Emo-DB.\n\nFrom Figure  3 (a), it can be observed that when the DJDA model's training iterations progressively increase, the value of w from 0.5 to over 0.9. This observation suggests that the contribution of marginal distribution adaptation of emotional features among different speakers in feature learning is weaken, and the conditional distribution adaptation plays a dominant role. While the results shown in",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we proposed a Dynamic Joint Distribution Adaptation (DJDA) for speaker-independent SER. DJDA was designed to address multi-domain unsupervised domain adaptation, aiming to align the global feature distribution and local distribution of emotional features from different speakers more accurately. Experimental results on two publicly speech emotion datasets demonstrated the superior performance of the proposed DJDA. In future work, we will explore the dynamic joint distribution adaptation strategy when the emotion category feature spaces of the source and target domains are different.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Regarding the first issue, DJDA employs a joint distribution adap-",
      "page": 1
    },
    {
      "caption": "Figure 1: Overview of Dynamic Joint Distribution Adaptation (DJDA) method for speaker-independent SER, which primarily consists of JDA",
      "page": 2
    },
    {
      "caption": "Figure 2: Ablation experiments of DJDA on Emo-DB.",
      "page": 4
    },
    {
      "caption": "Figure 2: Among them, w/o",
      "page": 4
    },
    {
      "caption": "Figure 2: show that our proposed",
      "page": 4
    },
    {
      "caption": "Figure 3: Dynamic balance factors for different target samples.",
      "page": 4
    },
    {
      "caption": "Figure 3: (a), it can be observed that when the DJDA model’s",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LOSO\n(10 speakers\nor\n5 sessions)": "",
          "DNN-HMM [17]": "CNN+LSTM Model [18]",
          "62.28": "68.80",
          "58.02": "59.40"
        },
        {
          "LOSO\n(10 speakers\nor\n5 sessions)": "",
          "DNN-HMM [17]": "CNN GRU-SeqCap [19]",
          "62.28": "72.73",
          "58.02": "59.71"
        },
        {
          "LOSO\n(10 speakers\nor\n5 sessions)": "",
          "DNN-HMM [17]": "FCN+Attention [20]",
          "62.28": "70.40",
          "58.02": "63.90"
        },
        {
          "LOSO\n(10 speakers\nor\n5 sessions)": "",
          "DNN-HMM [17]": "Model-3 Fusion [21]",
          "62.28": "72.34",
          "58.02": "58.31"
        },
        {
          "LOSO\n(10 speakers\nor\n5 sessions)": "",
          "DNN-HMM [17]": "ADARL [7]",
          "62.28": "73.02",
          "58.02": "65.86"
        },
        {
          "LOSO\n(10 speakers\nor\n5 sessions)": "",
          "DNN-HMM [17]": "ATFNN [3]",
          "62.28": "73.81",
          "58.02": "64.48"
        },
        {
          "LOSO\n(10 speakers\nor\n5 sessions)": "",
          "DNN-HMM [17]": "DJDA (ours)",
          "62.28": "75.26",
          "58.02": "65.92"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LOSO\n(10 speakers)": "",
          "GerDA [16]": "DNN ELM [22]",
          "81.90": "77.01",
          "79.10": "76.98"
        },
        {
          "LOSO\n(10 speakers)": "",
          "GerDA [16]": "ComParE SVM [23]",
          "81.90": "N/A",
          "79.10": "86.00"
        },
        {
          "LOSO\n(10 speakers)": "",
          "GerDA [16]": "SDFA [24]",
          "81.90": "86.65",
          "79.10": "N/A"
        },
        {
          "LOSO\n(10 speakers)": "",
          "GerDA [16]": "DTPM [25]",
          "81.90": "87.31",
          "79.10": "86.30"
        },
        {
          "LOSO\n(10 speakers)": "",
          "GerDA [16]": "DANN [26]",
          "81.90": "85.98",
          "79.10": "84.61"
        },
        {
          "LOSO\n(10 speakers)": "",
          "GerDA [16]": "DAN [27]",
          "81.90": "86.36",
          "79.10": "85.30"
        },
        {
          "LOSO\n(10 speakers)": "",
          "GerDA [16]": "DIFL VGG6 [6]",
          "81.90": "89.72",
          "79.10": "88.49"
        },
        {
          "LOSO\n(10 speakers)": "",
          "GerDA [16]": "DJDA (ours)",
          "81.90": "89.91",
          "79.10": "88.69"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Computational paralinguistics: emotion, affect and personality in speech and language processing",
      "authors": [
        "Björn Schuller",
        "Anton Batliner"
      ],
      "year": "2013",
      "venue": "Computational paralinguistics: emotion, affect and personality in speech and language processing"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends",
      "authors": [
        "W Björn",
        "Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition via an attentive time-frequency neural network",
      "authors": [
        "Cheng Lu",
        "Wenming Zheng",
        "Hailun Lian",
        "Yuan Zong",
        "Chuangao Tang",
        "Sunan Li",
        "Yan Zhao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "5",
      "title": "Speaker independent emotion recognition by early fusion of acoustic and linguistic features within ensemble",
      "authors": [
        "Schuller",
        "M Müller",
        "G Lang",
        "Rigoll"
      ],
      "year": "2005",
      "venue": "Speaker independent emotion recognition by early fusion of acoustic and linguistic features within ensemble"
    },
    {
      "citation_id": "6",
      "title": "Speaker independent speech emotion recognition by ensemble classification",
      "authors": [
        "Björn Schuller",
        "Stephan Reiter",
        "Ronald Muller",
        "Marc Al-Hames",
        "Manfred Lang",
        "Gerhard Rigoll"
      ],
      "year": "2005",
      "venue": "IEEE international conference on multimedia and expo"
    },
    {
      "citation_id": "7",
      "title": "Domain invariant feature learning for speaker-independent speech emotion recognition",
      "authors": [
        "Cheng Lu",
        "Yuan Zong",
        "Wenming Zheng",
        "Yang Li",
        "Chuangao Tang",
        "Björn Schuller"
      ],
      "year": "2022",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "8",
      "title": "Adaptive domain-aware representation learning for speech emotion recognition.,\" in INTERSPEECH",
      "authors": [
        "Weiquan Fan",
        "Xiangmin Xu",
        "Xiaofen Xing",
        "Dongyan Huang"
      ],
      "year": "2020",
      "venue": "Adaptive domain-aware representation learning for speech emotion recognition.,\" in INTERSPEECH"
    },
    {
      "citation_id": "9",
      "title": "Deep Transductive Transfer Regression Network for Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Yan Zhao",
        "Jincen Wang",
        "Ru Ye",
        "Yuan Zong",
        "Wenming Zheng",
        "Li Zhao"
      ],
      "year": "2022",
      "venue": "Deep Transductive Transfer Regression Network for Cross-Corpus Speech Emotion Recognition"
    },
    {
      "citation_id": "10",
      "title": "Deep implicit distribution alignment networks for cross-corpus speech emotion recognition",
      "authors": [
        "Yan Zhao",
        "Jincen Wang",
        "Yuan Zong",
        "Wenming Zheng",
        "Hailun Lian",
        "Li Zhao"
      ],
      "year": "2023",
      "venue": "Deep implicit distribution alignment networks for cross-corpus speech emotion recognition",
      "arxiv": "arXiv:2302.08921"
    },
    {
      "citation_id": "11",
      "title": "Transfer learning with dynamic adversarial adaptation network",
      "authors": [
        "Chaohui Yu",
        "Jindong Wang",
        "Yiqiang Chen",
        "Meiyu Huang"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Data Mining"
    },
    {
      "citation_id": "12",
      "title": "Transfer learning with dynamic distribution adaptation",
      "authors": [
        "Jindong Wang",
        "Yiqiang Chen",
        "Wenjie Feng",
        "Han Yu",
        "Meiyu Huang",
        "Qiang Yang"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Intelligent Systems and Technology"
    },
    {
      "citation_id": "13",
      "title": "Analysis of representations for domain adaptation",
      "authors": [
        "Shai Ben-David",
        "John Blitzer",
        "Koby Crammer",
        "Fernando Pereira"
      ],
      "year": "2006",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "15",
      "title": "A database of german emotional speech",
      "authors": [
        "Felix Burkhardt",
        "Astrid Paeschke",
        "Miriam Rolfes",
        "Walter Sendlmeier",
        "Benjamin Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "16",
      "title": "Acoustic emotion recognition: A benchmark comparison of performances",
      "authors": [
        "Björn Schuller",
        "Bogdan Vlasenko",
        "Florian Eyben",
        "Gerhard Rigoll",
        "Andreas Wendemuth"
      ],
      "year": "2009",
      "venue": "IEEE Workshop on Automatic Speech Recognition & Understanding"
    },
    {
      "citation_id": "17",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks",
      "authors": [
        "André Stuhlsatz",
        "Christine Meyer",
        "Florian Eyben",
        "Thomas Zielke",
        "Günter Meier",
        "Björn Schuller"
      ],
      "year": "2011",
      "venue": "IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "18",
      "title": "Revisiting hidden markov models for speech emotion recognition",
      "authors": [
        "Shuiyang Mao",
        "Dehua Tao",
        "Guangyan Zhang",
        "Tan Ching",
        "Lee"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion classification using attention-based lstm",
      "authors": [
        "Yue Xie",
        "Ruiyu Liang",
        "Zhenlin Liang",
        "Chengwei Huang",
        "Cairong Zou",
        "Björn Schuller"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition using capsule networks",
      "authors": [
        "Xixin Wu",
        "Songxiang Liu",
        "Yuewen Cao",
        "Xu Li",
        "Jianwei Yu",
        "Dongyang Dai",
        "Xi Ma",
        "Shoukang Hu",
        "Zhiyong Wu",
        "Xunying Liu"
      ],
      "year": "2019",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Yuanyuan Zhang",
        "Jun Du",
        "Zirui Wang",
        "Jianshu Zhang",
        "Yanhui Tu"
      ],
      "year": "2018",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "22",
      "title": "Deep encoded linguistic and acoustic cues for attention based end to end speech emotion recognition",
      "authors": [
        "Swapnil Bhosale",
        "Rupayan Chakraborty",
        "Sunil Kumar"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "24",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "Florian Eyben",
        "Klaus Scherer",
        "Björn Schuller",
        "Johan Sundberg",
        "Elisabeth André",
        "Carlos Busso",
        "Laurence Devillers",
        "Julien Epps",
        "Petri Laukka",
        "Shrikanth S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "25",
      "title": "Speech emotion recognition using deep 1d & 2d cnn lstm networks",
      "authors": [
        "Jianfeng Zhao",
        "Xia Mao",
        "Lijiang Chen"
      ],
      "year": "2019",
      "venue": "Biomedical signal processing and control"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "Shiqing Zhang",
        "Shiliang Zhang",
        "Tiejun Huang",
        "Wen Gao"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "27",
      "title": "Towards adversarial learning of speaker-invariant representation for speech emotion recognition",
      "authors": [
        "Ming Tu",
        "Yun Tang",
        "Jing Huang",
        "Xiaodong He",
        "Bowen Zhou"
      ],
      "year": "2019",
      "venue": "Towards adversarial learning of speaker-invariant representation for speech emotion recognition",
      "arxiv": "arXiv:1903.09606"
    },
    {
      "citation_id": "28",
      "title": "Learning transferable features with deep adaptation networks",
      "authors": [
        "Mingsheng Long",
        "Yue Cao",
        "Jianmin Wang",
        "Michael Jordan"
      ],
      "year": "2015",
      "venue": "International conference on machine learning"
    }
  ]
}