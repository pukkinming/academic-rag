{
  "paper_id": "2111.05890v1",
  "title": "Multimodal End-To-End Group Emotion Recognition Using Cross-Modal Attention",
  "published": "2021-11-10T19:19:26Z",
  "authors": [
    "Lev Evtodienko"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Classifying group-level emotions is a challenging task due to complexity of video, in which not only visual, but also audio information should be taken into consideration. Existing works on multimodal emotion recognition are using bulky approach, where pretrained neural networks are used as a feature extractors and then extracted features are being fused. However, this approach does not consider attributes of multimodal data and feature extractors cannot be fine-tuned for specific task which can be disadvantageous for overall model accuracy. To this end, our impact is twofold: (i) we train model end-to-end, which allows early layers of neural network to be adapted with taking into account later, fusion layers, of two modalities; (ii) all layers of our model was fine-tuned for downstream task of emotion recognition, so there were no need to train neural networks from scratch. Our model achieves best validation accuracy of 60.37% which is approximately 8.5% higher, than VGAF dataset baseline and is competitive with existing works, audio and video modalities. \n CCS Concepts •Computing methodologies∼Artificial intelligence∼ Computer vision; •Human-centered computing∼Human computer interaction (HCI)",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is difficult and important task. Understanding emotions in groups of people is vital not only for every individual of a group, but also for people with different background and cultures. Moreover, knowledge about common emotion of group could be of interest for business, learning, healthcare, surveillance and robotics.\n\nEarly affective computing was focused on individuals, while in recent years more research is done for groups of people from raw, uncontrolled data (i.e. \"in the wild\")  [1] . Results performed on these type of data are easier adoptable to real-world situations such as surveillance cameras or videos from internet.\n\nDataset used in training was presented in EmotiW 2020 Audio-Video Group Emotion Recognition grand challenge  [2]  and the exact challenge is Video level Group AFfect (VGAF). The task of this competition was to classify group emotion into three different categories: negative, neutral and positive. The biggest challenges of this dataset are: different lightning conditions, languages, video quality, frame rate, occlusions and intersection of people. One of the approach to handle such data, is to use only one modality  [3] ,  [4] . Another option are two-stage models, where stages are feature extraction and modality fusion respectively  [5] .\n\nDespite the fact, that unimodal models show decent results, when given poor input information, this problem can not be compensated by another modality, which will hurt the performance. As for two-stage models, fixed feature extractors can not be fine-tuned for a specific task with respect to the information from low-level fusion layers.\n\nTo address these issues we propose our model with following features:\n\n• Model was trained fully end-to-end, compensating the problem of missing information about modalities interaction. Moreover, if one modality does not carry a lot of useful information, another one can mitigate this problem.\n\n• We do not freeze layers throughout our model, which allows it to be fully optimized to a given task and helps model achieve solid results using models from different domains effectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Automatic human emotion recognition is a topic of active research for nearly a decade  [6] ,  [7] ,  [8] . In early works on multimodal learning there were several techniques proposed, such as early fusion  [9]  and late fusion  [10] . While the idea behind these techniques are simple, they show decent results and are still being widely adopted for multimodal tasks. Recently, researchers have been working not only on fusion techniques, but also on multimodal architectures, where pairs of different modalities' are being fed to network. These architectures, for instance, are Tensor Fusion Networks  [11] , LXMERT  [12] , ClipBERT  [13] , VATT  [14] , VILT  [15] . Due to increase in computational power in recent years, a lot of work was done using multimodal learning in different areas, such as question answering  [16] , image captioning  [17] , emotion recognition  [18] , affect recognition  [19] .\n\nPrevious results on VGAF  [2]  dataset were obtained using two-stage models, where at first features were extracted using fixed models and then, late fusion was used for fusion of extracted features  [5] ,  [20] ,  [21] ,  [22] ,  [23] ,  [24] ,  [25] . The best result for this dataset was shown by the winners of Audio-Visual Group Emotion Recognition challenge of EmotiW2020  [20] , where team uses 5 different modalities and achieves 74.28% on validation set.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Dataset",
      "text": "The VGAF dataset  [2]  contains 2661 video clips for training and 766 for validation. Dataset was acquired from YouTube with tags, such as \"fight\", \"holiday\" or \"protest\", videos from which characterize different emotions. Each video was cropped in clip of 5 seconds length. The data is contains 3 classes -Positive, Neutral and Negative corresponding to the 3 emotion categories. Challenges of dataset are different contexts in every video, various resolutions, frame rates, multilingual audio, which is serious abstraction for the vast majority of available models. Moreover, as there are no labels of what exactly language is in the video, it makes it impossible to collect additional text transcription (modality) using automatic tools.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Encoders",
      "text": "We describe the problem as follows. Let X = {(v i , a i )} I i=1 be a sample of data, where I is a number of multimodal samples, v i is a sequence of RGB video frames and a i is a raw audio of a given sample.\n\nFirst, we extract 8 equally distributed frames. Vision encoder accepts a sequence of extracted frames ∀v i ∈ R C×T×H×W , where C, T, H, and W are the number of channels, temporal size, height, and width, respectively. For the vision encoder we use approach, inspired by ClipBERT  [13] , namely we applied mean-pooling ( we will define this operation as M) to aggregate temporal information of a frames sequence, which is inexpensive way to make use of temporal information. We use pretrained 2D ResNet101d  [26]  for feature extraction. During our experiments we tried several backbone architectures described in Table  1 . We decided not to use 3D CNNs, because it can highly increase time of training without advantage in accuracy  [27] ,  [28] . To pass extracted feature maps further, in attention layers, we flatten embedding on last two dimensions and proceed with passing vector to projection layer (defined as PL shown on Fig.  1 ), which projected embeddings of different modalities to a common space. During current research instead of projection layer, layer module was considered, where GeLU  [29]  activation function and additional projection was added, but such a module lead to faster overfitting and shows approximately 3% drop in acccuracy.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Backbone",
      "text": "Accuracy ResNet101d  [26]  60.37% ResNet50d  [26]  58.44% RexNet100  [30]  57.98% ResNet50  [31]  56.83%\n\nTable  1 . Various vision encoder backbones and overall model validation accuracy.\n\nWe extracted raw audio data at sampling rate of 16000 Hz, and pass it to Hubert  [32] . We choose this model, because it was trained in self-supervised manner, which provides more robust representations and can handle multilingual data. To pass embeddings further, in self-attention layer, projection layer (shown on Fig.  1 ) was used.\n\nWe define process of extracting embeddings of audio and video encoder stages as follows:\n\nfor audio embeddings, where v emb , a emb ∈ R S×N , S is a sequence length and N is a number of features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Attention Layers",
      "text": "In this section we will review attention layers used in our model: self-attention and cross-attention. The main purpose of these layers in our model is aligning multimodal information from encoding embedding after encoders.\n\nSelf-attention was initially intorduced in  [33]  and described as layer for extracting information from a set of context vectors to query vector. Formally, it can be written as\n\nwhere K and V are context vectors and Q is a query vector.\n\nFor our model we used multi-head attention (MHA), which was introduced in  [34]  and can be defined as\n\nwhere\n\ni are learnable parameters of query, key and value respectively. Unlike the BERT  [35] , in our model selfattention is not used for text data, but for audio and visual embeddings. In the context of our model input to the selfattention is\n\nCross-modal attention has similar definition, but instead of computing dot product between the same vector for query and key, it makes use of multimodal nature of video, i.e. there are two cross-modal attentions, one takes K = V = a emb and Q = v emb as input; another one takes K = V = v emb and Q = a emb . Such attention enables one modality for receiving information from another modality and helps align them.\n\nFollowing architecture decisions of  [34] , we have added skip connections, which add weights from layer before attention block with weights from layer after it. To prevent exploding gradient problem, Layer Normalization was applied before penultimate projection layers.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Training",
      "text": "We train our model using Adam optimizer  [36] , with learning rate 1e -4 and weight decay 1e -3 . As encoder parts of our models were already pretrained and only being fine-tuned One of the biggest challenge of learning multimodal neural networks is the fact, that they are exposed to severe overfitting. Usual regularization techniques are often ineffective  [37]  for these networks. To mitigate this problem we use label smoothing with ǫ = 0.2, which makes neural network be less \"confident\" about class it predicts.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Models",
      "text": "Modality Accuracy K-injection network  [21]  A+V 63.58% ResNet50 + BiLSTM  [25]  A+V 61.83% Hubert+ResNet101d (ours) A+V 60.37% Inception + LSTM  [2]  (baseline) A+V 52.09%\n\nTable  2 . Results on validation data for two modalities on VGAF dataset. A, V, F states for audio, video, face accordingly.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "We use accuracy as evaluation metric of our model. It achieves an overall accuracy of 60.37% and outperforms baseline by a margin of 8.5%. In Table  2  comparison of all available teams results, who used audio and video modalities for their final predictions, is shown. It can be seen, that our model has a competitive results compared to other works. The best model consists of ResNet101d and Hubert  [32]  as encoders for video and audio. The most challenging class for our model is \"Positive\", it can be explained by the challenging nature of the dataset and hard interpretation of emotions by themselves. Moreover, some videos in dataset have similar context, but different emotions and labels for them. Classifying \"Neutral\" videos as \"Negative\" ones can be a problem, which emerges from the fact, that there are big number of protests in dataset, where some of them are peaceful and others are aggressive.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "Group video emotion recognition is a challenging task, especially for \"in the wild\" data. In this paper we present model for VGAF dataset from Audio-Visual Group Emotion Recognition challenge of EmotiW2020. Two novel approaches is used for our model. Our model was trained end-to-end and optimized fully during training process, which help us achieve noticeable result of 60.37% validation accuracy, which outperforms baseline significantly and can perform practically on par with existing bimodal audio-visual models.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Proposed end-to-end architecture for group emotion recognition.",
      "page": 2
    },
    {
      "caption": "Figure 2: Confusion matrix for predictions of our model.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: We decided",
      "data": [
        {
          "Backbone": "ResNet101d [26]",
          "Accuracy": "60.37%"
        },
        {
          "Backbone": "ResNet50d [26]",
          "Accuracy": "58.44%"
        },
        {
          "Backbone": "RexNet100 [30]",
          "Accuracy": "57.98%"
        },
        {
          "Backbone": "ResNet50 [31]",
          "Accuracy": "56.83%"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 2: Results on validation data for two modalities on",
      "data": [
        {
          "Models": "K-injection network [21]",
          "Modality": "A+V",
          "Accuracy": "63.58%"
        },
        {
          "Models": "ResNet50 + BiLSTM [25]",
          "Modality": "A+V",
          "Accuracy": "61.83%"
        },
        {
          "Models": "Hubert+ResNet101d (ours)",
          "Modality": "A+V",
          "Accuracy": "60.37%"
        },
        {
          "Models": "Inception + LSTM [2] (baseline)",
          "Modality": "A+V",
          "Accuracy": "52.09%"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "From individual to group-level emotion recognition: Emotiw 5.0",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Shreya Ghosh",
        "Jyoti Joshi",
        "Jesse Hoey",
        "Tom Gedeon"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "3",
      "title": "Emotiw 2020: Driver gaze, group emotion, student engagement and physiological signal based challenges",
      "authors": [
        "Abhinav Dhall",
        "Garima Sharma",
        "Roland Goecke",
        "Tom Gedeon"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "4",
      "title": "Group-level speech emotion recognition utilising deep spectrum features",
      "authors": [
        "Sandra Ottl",
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Vincent Karas",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "5",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "Andrey Savchenko"
      ],
      "year": "2021",
      "venue": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "arxiv": "arXiv:2103.17107"
    },
    {
      "citation_id": "6",
      "title": "Fusical: Multimodal fusion for video sentiment",
      "authors": [
        "Boyang Tom",
        "Leila Abdelrahman",
        "Kevin Chen",
        "Amil Khanzada"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "7",
      "title": "Affective computing: From laughter to ieee",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "End-to-end multimodal emotion recognition using deep neural networks",
      "authors": [
        "Panagiotis Tzirakis",
        "George Trigeorgis",
        "Mihalis Nicolaou",
        "Björn Schuller",
        "Stefanos Zafeiriou"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "9",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "10",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th International Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "11",
      "title": "MOSI: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "Amir Zadeh",
        "Rowan Zellers",
        "Eli Pincus",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "12",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "13",
      "title": "LXMERT: learning crossmodality encoder representations from transformers",
      "authors": [
        "Hao Tan",
        "Mohit Bansal"
      ],
      "year": "1908",
      "venue": "CoRR"
    },
    {
      "citation_id": "14",
      "title": "Less is more: Clipbert for video-and-language learning via sparse sampling",
      "authors": [
        "Jie Lei",
        "Linjie Li",
        "Luowei Zhou",
        "Zhe Gan",
        "Tamara Berg",
        "Mohit Bansal",
        "Jingjing Liu"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "15",
      "title": "VATT: transformers for multimodal self-supervised learning from raw video, audio and text",
      "authors": [
        "Hassan Akbari",
        "Liangzhe Yuan",
        "Rui Qian",
        "Wei-Hong Chuang",
        "Shih-Fu Chang",
        "Yin Cui",
        "Boqing Gong"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "16",
      "title": "Vilt: Vision-and-language transformer without convolution or region supervision",
      "authors": [
        "Wonjae Kim",
        "Bokyung Son",
        "Ildoo Kim"
      ],
      "year": "2021",
      "venue": "Vilt: Vision-and-language transformer without convolution or region supervision"
    },
    {
      "citation_id": "17",
      "title": "Multimodalqa: Complex question answering over text, tables and images",
      "authors": [
        "Alon Talmor",
        "Ori Yoran",
        "Amnon Catav",
        "Dan Lahav",
        "Yizhong Wang",
        "Akari Asai",
        "Gabriel Ilharco",
        "Hannaneh Hajishirzi",
        "Jonathan Berant"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "18",
      "title": "Towards unsupervised image captioning with shared multimodal embeddings",
      "authors": [
        "Iro Laina",
        "Christian Rupprecht",
        "Nassir Navab"
      ],
      "year": "1908",
      "venue": "CoRR"
    },
    {
      "citation_id": "19",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "Wenliang Dai",
        "Samuel Cahyawijaya",
        "Zihan Liu",
        "Pascale Fung"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "20",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Group level audio-video emotion recognition using hybrid networks",
      "authors": [
        "Chuanhe Liu",
        "Wenqiang Jiang",
        "Minghao Wang",
        "Tianhao Tang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "22",
      "title": "Implicit knowledge injectable cross attention audiovisual model for group emotion recognition",
      "authors": [
        "Yanan Wang",
        "Jianming Wu",
        "Panikos Heracleous",
        "Shinya Wada",
        "Rui Kimura",
        "Satoshi Kurihara"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "23",
      "title": "Group-level speech emotion recognition utilising deep spectrum features",
      "authors": [
        "Sandra Ottl",
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Vincent Karas",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "24",
      "title": "Recognizing emotion in the wild using multimodal data",
      "authors": [
        "Shivam Srivastava",
        "Saandeep Aathreya Sidhapur Lakshminarayan",
        "Saurabh Hinduja",
        "Rahatul Sk",
        "Hamza Jannat",
        "Shaun Elhamdadi",
        "Canavan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "25",
      "title": "Multi-modal fusion using spatio-temporal and static features for group emotion recognition",
      "authors": [
        "Mo Sun",
        "Jian Li",
        "Hui Feng",
        "Wei Gou",
        "Haifeng Shen",
        "Jian Tang",
        "Yi Yang",
        "Jieping Ye"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "26",
      "title": "Audiovisual classification of group emotion valence using activity recognition networks",
      "authors": [
        "João Ribeiro Pinto",
        "Tiago Gonc ¸alves",
        "Carolina Pinto",
        "Luís Sanhudo",
        "Joaquim Fonseca",
        "Filipe Gonc ¸alves",
        "Pedro Carvalho",
        "Jaime Cardoso"
      ],
      "year": "2020",
      "venue": "2020 IEEE 4th International Conference on Image Processing, Applications and Systems (IPAS)"
    },
    {
      "citation_id": "27",
      "title": "Bag of tricks for image classification with convolutional neural networks",
      "authors": [
        "Tong He",
        "Zhi Zhang",
        "Hang Zhang",
        "Zhongyue Zhang",
        "Junyuan Xie",
        "Mu Li"
      ],
      "year": "2018",
      "venue": "Bag of tricks for image classification with convolutional neural networks"
    },
    {
      "citation_id": "28",
      "title": "Temporal segment networks: Towards good practices for deep action recognition",
      "authors": [
        "Limin Wang",
        "Yuanjun Xiong",
        "Zhe Wang",
        "Yu Qiao",
        "Dahua Lin",
        "Xiaoou Tang",
        "Luc Van Gool"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "29",
      "title": "Learning spatiotemporal representation with pseudo-3d residual networks",
      "authors": [
        "Zhaofan Qiu",
        "Ting Yao",
        "Tao Mei"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "30",
      "title": "Bridging nonlinearities and stochastic regularizers with gaussian error linear units",
      "authors": [
        "Dan Hendrycks",
        "Kevin Gimpel"
      ],
      "year": "2016",
      "venue": "CoRR"
    },
    {
      "citation_id": "31",
      "title": "Rexnet: Diminishing representational bottleneck on convolutional neural network",
      "authors": [
        "Dongyoon Han",
        "Sangdoo Yun",
        "Byeongho Heo",
        "Youngjoon Yoo"
      ],
      "year": "2020",
      "venue": "Rexnet: Diminishing representational bottleneck on convolutional neural network"
    },
    {
      "citation_id": "32",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "33",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "34",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "35",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "CoRR"
    },
    {
      "citation_id": "36",
      "title": "BERT: pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "CoRR"
    },
    {
      "citation_id": "37",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "CoRR"
    },
    {
      "citation_id": "38",
      "title": "What makes training multi-modal networks hard?",
      "authors": [
        "Weiyao Wang",
        "Du Tran",
        "Matt Feiszli"
      ],
      "year": "1905",
      "venue": "CoRR"
    }
  ]
}