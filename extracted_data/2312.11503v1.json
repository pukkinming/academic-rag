{
  "paper_id": "2312.11503v1",
  "title": "Speech And Text-Based Emotion Recognizer",
  "published": "2023-12-10T05:17:39Z",
  "authors": [
    "Varun Sharma"
  ],
  "keywords": [
    "Speech emotion recognition",
    "deep neural networks",
    "convolutional neural networks",
    "HuBERT",
    "Spectrogram",
    "Speech Transcription",
    "CNN"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective computing is a field of study that focuses on developing systems and technologies that can understand, interpret, and respond to human emotions. Speech Emotion Recognition (SER), in particular, has got a lot of attention from researchers in the recent past. However, in many cases, the publicly available datasets, used for training and evaluation, are scarce and imbalanced across the emotion labels. In this work, we focused on building a balanced corpus from these publicly available datasets by combining these datasets as well as employing various speech data augmentation techniques. Furthermore, we experimented with different architectures for speech emotion recognition. Our best system, a multi-modal speech, and text-based model, provides a performance of UA(Unweighed Accuracy) + WA (Weighed Accuracy) of 157.57 compared to the baseline algorithm performance of 119.66",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "A lthough there have been significant advancements in the field of artificial intelligence, natural interaction between humans and machines remains a challenge, partly due to the machine's inability to comprehend our emotional states. When people speak, the sounds they produce convey not only the intended linguistic message but also a wealth of additional information about themselves and their emotional states. Affective computing, a sub-field of Human-Computer Interaction (HCI) systems, aims to facilitate natural interaction with machines by direct voice interaction instead of using traditional devices as input.\n\nSpeech-based Emotion Recognition (SER) is a relatively new field that has emerged in the past few decades. The earliest research in this area can be traced back to the 1990s when scientists began to explore the possibility of using computers to recognize emotions in speech. Early efforts focused on identifying basic emotions such as happiness, sadness, anger, and fear, using simple acoustic features such as pitch and intensity. Over time, researchers developed more sophisticated techniques for extracting emotional information from speech signals, incorporating advanced machine learning algorithms and drawing on knowledge from fields such as psychology and linguistics. There have been significant advances in SER technology in recent years, thanks to the development of deep learning algorithms. Deep learning algorithms can learn complex patterns in data, and they are very effective for SER.\n\nProminent among these have been wav2vec 2.0  [3]  and Hu-BERT  [13]  models developed by Facebook. Both of these models have been pre-trained on large amounts of audio data which enables them to achieve impressive performance on speech processing tasks.\n\nIn the past, the SER domain has been plagued by 2 significant challenges. First has been \"how to model emotional representations from speech signals\". Traditional methods focus on the efficient extraction of hand-crafted features such as intensity, pitch, formants, zero crossing rate, and Mel Frequency Cepstrum Coefficients (MFCCs) to name a few. Although researchers have done a tremendous amount of work in this field, there are still issues of speech feature choice and the correct application of feature engineering that remain to be solved in the domain of SER  [16] .\n\nAnother obstacle has been the \"availability of large, highquality datasets\" for speech emotion recognition (SER). While there are several publicly available SER datasets, they tend to be smaller in size and less diverse than image datasets. This can make it more challenging to train and evaluate SER models, as there may be less data available to learn from. This paper tries to address both of these challenges. The contributions of this paper are chiefly  (1)  the analysis of various machine learning and deep learning architectures for emotion classification.  (2)  combining different publicly available datasets on SER to form a larger corpus. (3) applying speech augmentation techniques to balance and expand the corpus. (4) using transfer learning to finetune the Hubert model on this corpus. (5) evaluating the impact of jointly training audio embeddings from Hubert with the text embeddings obtained from Whisper 1 and BERT models and finally (6) using UA+WA as a metric for optimizing and evaluating the system. The rest of the paper is structured as follows. In Section 2, we review the related work on SER. In Section 3, we describe in detail, the datasets, methods, and evaluation methods that we used. In Section 4, we present the results of our experiments and compare them against each other. In this section, we also present benchmark test results on RAVDESS dataset. In Section 5, we discuss the deployment and web application-based inference and in Section 6, we conclude the paper and suggest future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "The ability to understand and generate emotions in speech has been a key area of research for many decades. Their importance has increased even more in recent times, due to the widespread use of virtual assistants (such as Siri, Alexa, or Google Home) and their applications in the healthcare industry.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "A. Conventional Ser",
      "text": "Early work on SER included using manually-designed features as input, such as audio energy, zero-crossing rate, and Mel-frequency cepstrum coefficients (MFCCs)  [21] . Different types of generative models were used as classifiers, such as Gaussian mixture models (GMMs)  [38] , hidden Markov models (HMMs)  [26] , and support vector machines (SVMs) that use a discriminative approach  [35] .\n\nMany basic acoustic parameters related to prosody and spectrum of speech, such as pitch, formant frequencies, voice quality measures, speech energy, and speech rate, were shown to be associated with emotional intensity and emotional processes  [31]  [32],  [1] . Ververidis and Kotropoulos explored the use of more advanced parameters such as the MFCCs, spectral roll-off  [37] . Furthermore, TEO features  [34]   [12] , spectrograms  [29]  also achieved good SER performance.\n\nBut, finding the most suitable acoustic features that can distinguish different emotions has been a major but also difficult challenge of SER. The research progress was slow and showed some discrepancies among studies. Therefore, the research focus shifted to methods that do not require or reduce the need for prior knowledge of the best features and replace it with automatic feature generation methods provided by neural networks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Deep Learning-Based Ser",
      "text": "Motivated by the remarkable progress in computer vision, recent studies  [30] [2][42]  [27]  have made significant improvements on SER by treating spectral features as images.\n\nFayek et al  [10]  ,investigated SER from short frames of speech spectrograms using a DNN. An average accuracy of 60.53 (six emotions eINTERFACE database) and 59.7 (seven emotions-SAVEEdatabase) was achieved. A similar but improved approach led to 64.78 of average accuracy (IEMOCAP data with five classes)  [9] .\n\nThe simplest DNN systems for emotion recognition are feedforward networks that are built on top of the utterance level feature representations  [11] . Recurrent Neural Networks (RNN)  [17]  are a class of neural networks that have cyclic connections between nodes in the same layer. These net-works capture the inherent temporal context in emotions and have shown improved performance for classification task  [20] . Another class of DNNs, Convolutional Neural Nets-works (CNN)  [19] , capture locally present context, and patterns, working on frame-level features. CNNs enable the training of end-to-end systems where the feature representations and classification are trained together using a single optimization. Few works have analyzed the performance of CNNs for speech emotion classification  [23] ,  [2] . Cummins et al.  [8]  further built image-based CNNs on spectrogram features.\n\nRecently, attention-based models have made significant progress on SER  [22]   [43]  . For example, multi-head attention maps are learned by convolutional operations to select important information according to surrounding information  [41] . Moreover, area attention is further introduced to compute the importance from different ranges of convolutions  [40] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Transfer Learning",
      "text": "Transfer learning is a technique that allows a model to leverage the knowledge learned from a source domain and apply it to a target domain. For example, if you have a model that can recognize cars in images, you can use some of the features learned by that model to help you recognize trucks in images.  2 The artificial intelligence (AI) field is undergoing a major shift in recent years, moving from specialized architectures that are designed for a single task to general-purpose foundation models that can be easily transferred to different use-cases  [4] . These foundation models are often trained on large datasets, using proxy tasks to avoid the need for hard-to-acquire labels, and then fine-tuned on (small) sets of labeled data for their target tasks. This transfer learning technique has been very effective in computer vision  [7] , NLP  [36] , and computer audition  [3]  including SER  [39]   [28] . Among others, wav2vec 2.0  [3]  and HuBERT  [13]  have emerged as promising foundation models for speech-related applications. They have been successfully transferred to (mostly categorical) SER by previous works.\n\nHuBERT shares its architecture with wav2vec 2.0, but it diverges in its training approach. In place of a contrastive loss, HuBERT adopts an offline clustering technique to generate noisy labels for pre-training a Masked Language Model. It processes masked continuous speech features and forecasts designated cluster assignments. The predictive loss is enforced within the masked segments, compelling the model to develop sophisticated representations of unmasked inputs to effectively deduce the intended outcomes of the masked elements.\n\nIn this work, we evaluate several approaches to model the SER problem ranging from baseline machine learning approaches to CNNs, to finally fine-tuning Hubert. We also explore augmenting the speech embeddings with text transcription embeddings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methods",
      "text": "After a thorough literature review, an iterative modeling approach was employed for developing the solution.The following sections discuss each step in detail:\n\nA. Dataset curation Concatenation: The first step was to curate a robust dataset. A common problem seen in the SER domain has been the lack of quality datasets. There are several publicly available datasets for SER, however, they are still limited when compared to other domains like Image and Text. Another issue is the lack of a standard set of labels for human emotions -different datasets use slightly different sets of emotions. Hence, it was decided to combine and standardize 5 publicly available datasets to form a single large corpus.\n\nA brief description of each of these datasets is given below:\n\n1. CERMA-D: The CREMA-D dataset  [6]  is a publicly available dataset for Speech Emotion Recognition (SER). It stands for the Crowd-sourced Emotional Multimodal Actors Dataset and contains audio and video recordings of 91 actors, aged 20 to 74, speaking 12 sentences in six different emotions: anger, disgust, fear, happiness, sadness, and neutral. The actors were recorded in a studio setting and the recordings were annotated by multiple annotators to ensure the accuracy of the emotion labels.\n\n2. SAVEE: The SAVEE dataset  [15] , which stands for Surrey Audio-Visual Expressed Emotion contains recordings from four native English male speakers, who were postgraduate students and researchers at the University of Surrey aged from 27 to 31 years. The emotions in the dataset are described in discrete categories: anger, disgust, fear, happiness, sadness, surprise, and neutral. The text material consisted of 15 TIMIT sentences per emotion: 3 common, 2 emotionspecific, and 10 generic sentences that were different for each emotion and phonetically balanced. This resulted in a total of 120 utterances per speaker.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Tess:",
      "text": "The TESS dataset  [33] , which stands for Toronto Emotional Speech Set contains audio recordings of two actresses, aged 26 and 64 years, speaking 200 target words in the carrier phrase \"Say the word ..\" portraying each of seven emotions: anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral. There are a total of 2800 data points (audio files) in the dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iemocap:",
      "text": "The IEMOCAP dataset  [5] , which stands for Interactive Emotional Dyadic Motion Capture contains approximately 12 hours of audiovisual data, including video, speech, motion capture of face, and text transcriptions. The dataset was collected from 10 actors (5 male and 5 female) who performed scripted and improvised scenarios designed to elicit a range of emotions. The emotions in the dataset are described in both categorical and dimensional terms, with the categorical labels including anger, happiness, excitement, sadness, frustration, fear, surprise, and neutral. The dimensional labels include valence, activation, and dominance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ravdess:",
      "text": "The RAVDESS dataset  [24] , which stands for Ryerson Audio-Visual Database of Emotional Speech and Song, is another dataset for Speech Emotion Recognition (SER). It contains audio and video recordings of 24 professional actors (12 male and 12 female) speaking and singing in a range of emotions. The emotions in the dataset include calm, happy, sad, angry, fearful, surprise, and disgust. Each emotion is expressed at two levels of intensity: normal and strong\n\nFor combining these datasets, the following steps were followed (Figure  1 ):\n\n• From 'IEMOCAP' dataset, 'xxx','neu','oth', and 'fru' labels were removed.\n\n• From 'CREMA-D' dataset, 'Neutral' label was removed.\n\n• From 'RAVDESS' dataset, 'neutral' label was removed.\n\n• From 'SAVEE' dataset, 'n' label was removed.\n\n• From 'TESS' dataset, 'neutral' label was removed.\n\n• From the remaining classes in the datasets, target emotion labels were renamed to below seven classes: calm, happy, sad, angry, fearful, disgust and surprised\n\nAn attentive reader would have noticed that the 'neutral' label has been removed altogether. One reason was that the 'neutral' label is often ambiguous and can be difficult to distinguish from other emotions. For example, often, the neutral speech sample is very similar to a calm speech sample, making it difficult for a machine learning model to accurately classify the two. Another reason for removing the 'neutral' label was that it is not considered an emotion in the traditional sense. A neutral state is often considered to be the 'absence of any particular emotion'.\n\nPreprocessing: When combining multiple datasets, there may be inconsistencies and variations in the data that can affect the performance of the model. Preprocessing techniques can help address these issues and improve the quality of the combined dataset. With that rationale, following steps were carried out:\n\n1. Sample rate adjustment: Since the audio files have different sample rates, we considered resampling them to a common sample rate of 16KHZ. This step ensured that the data is consistent and compatible for further processing. 3. Silence removal: Removed silent or low-energy sections from the audio files. This helped reduce noise and focus on the segments that contain speech and emotional content.\n\n4. Short audio removal: Removed wav files with duration less than 1 second as they may not contain enough information for accurate emotion recognition.\n\nThis resulted in a dataset with 13,236 wav files with 7 target Emotion labels(Figure  2 ).The details about the dataset is listed in Table  4 .\n\nThe dataset consisted of various examples with different target labels. To ensure that the model could learn from a representative sample of the data, the dataset was divided into two parts using a stratified split. A stratified split is a method that preserves the proportion of each target label in both parts. For example, if the original dataset had 50% of examples with label A and 50% with label B, then the stratified split would also have the same proportions in both parts. The stratified split was done with an 80:20 ratio, meaning that 80% of the data was used for training and 20% was used for testing. The training part was used to fit the model to the data, while the testing part was used to measure  Furthermore, since the train dataset was imbalanced, audio augmentation techniques were applied to balance the data points across the classes:\n\n1. Stretching: This technique involved changing the speed of the audio clip without affecting its pitch. Rationale was that by stretching the audio clip, the audio could be made longer or shorter, which can help the model learn to recognize emotions in speech with varying speaking rates.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pitch Shifting:",
      "text": "This technique involved changing the pitch of the audio clip without affecting its duration. Idea was that by shifting the pitch of the audio clip, the audio can be made higher or lower, which can help the model learn to recognize emotions in speech with varying voice pitches.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Audio Gain:",
      "text": "This technique involved changing the volume of the audio clip. By adjusting the gain of the audio clip, one can make it louder or quieter, which can potentially help the model learn to recognize emotions in speech with varying volumes.\n\n4. Background Noise augmentation: Finally, for couple of classes, background noise was added to the audio clip. Idea was to simulate real-world scenarios where speech is often accompanied by background noise.This resulted in additional samples as well as helped the model learn to recognize emotions in speech even when there is background noise present.\n\nAs described earlier , to evaluate the performance of the model, the data was split into two sets: train and test. The train set was used to train the model parameters, while the test set was used to measure how well the model generalizes to unseen data. However, to avoid overfitting the model to the train set, a validation set was also used. The validation set was a subset of the train set that was not used for training, but for tuning the hyperparameters and selecting the best model. The validation set was obtained by splitting the train set into 80% and 20% portions. The 80% portion was used for training, while the 20% portion was used for validation. This way, the model could be evaluated on both the validation set and the test set, which were independent of each other.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Baseline",
      "text": "For baseline we trained a range of machine learning algorithms using a set of hand-crafted features.\n\nFeature Engineering From previous studies, emotion is found to be more correlated with energy features  [14] . Hence, it was decided to use following set of features:   4. ZeroCrossingRate: This is the rate at which a signal changes from positive to negative or vice versa. In the context of speech emotion recognition, zero cross rate can be useful for distinguishing between voiced and unvoiced sounds in an input speech signal.(Figure  3 )\n\n5. SpectralCentriod: The spectral centriod is a measure that indicates where the \"centre of mass\" of the spectrum is located. It is calculated as the weighted mean of the frequencies present in the signal, determined using a Fourier transform, with their magnitudes as the weights. In the context of speech emotion recognition, the spectral centroid can provide useful information about the distribution of energy in the frequency spectrum of a speech signal.(Figure  4 )\n\nSo, a total of 94 features were extracted from the raw audio wav files. After extracting these features, standard scaling was applied to normalize the feature values.\n\nFinally, we fitted the following machine learning models to this data to classify emotions into 7 categories:    With the best performing model (KNN), Grid search CV was fitted to get the optimum set of parameters. With those parameters, the model was again fitted on the same train dataset. However, accuracy did not improve further.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Cnn Models",
      "text": "Next, we experimented with different CNN architectures for solving the emotion classification problem:\n\n1. MobileNetV2 Model with 2D Convolution: Similar to  [18] , another approach that was tried was to redefine the Speech Emotion Recognition problem as an 'image classification task'.\n\nTo achieve this, labeled speech samples were buffered into\n\n• The time-shift between subsequent frames was '4ms' giving 75% overlap between frames.\n\n• The real and imaginary outputs from the short-time Fourier transform were converted to spectral magnitude values and concatenated across the whole frame to form an array.\n\n• Resulting array was of shape 257 × 251 Where 257 is the number of frequency values (rows), and 251 is the number of time values (columns) in the spectrogram array.\n\n• Spectral magnitude arrays of 257 × 251 real-valued numbers were converted into a color RGB image format.\n\n• The transformation into the RGB format was based on the matplotlib \"jet\" colormap as they provided the best visual representation of speech spectrograms  [18] .\n\nAfter the spectrogram images were saved onto the disk(Figure  5 ), a pre-trained CNN model called Mo-bilenetV2.0 was fine-tuned with raw spectrograms images (speech spectrogram arrays) as input.\n\n4 different experiments were carried out by varying the 'overlapping window' while segmenting the audio:\n\n1.1. Image files were created by dividing input 'wav' files into blocks of 1 second with overlapping windows of 10ms and including the Gender of speakers.  For the experiments above where 'overlapping window' was used, downsampling of 'wav' files was needed since otherwise, the number of images created would have been very high requiring substantially more computational power to process.\n\nHowever, none of the experiments yielded satisfactory results, in fact, they could not even surpass the baseline. The result for the best-performing model among the above (1.4) is shown in table  8 .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Cnn With 1D Convolution:",
      "text": "Inspired by a published research 4  , a CNN model was constructed.\n\nTo prevent the model from overfitting, several techniques were employed, including the use of more aggressive dropout rates, L1 and L2 regularization, and batch normalization at different convolutions and fully connected layers.\n\nThe basic idea was The CNN would then learn to identify temporal patterns in the MFCCs and other features that are associated with specific emotions by applying 1D convolution. For instance, a CNN might learn to identify a pattern in the MFCCs that is associated with the emotion of happiness. This pattern might be a sequence of high values that correspond to the frequencies that are associated with happiness.\n\nModel architecture is shown in the table 5. It makes use of 8 Conv1D layers , 3 Dropout layers, 2 batchNormalization layers and finally 3 dense layers. Once the CNN has learned to identify patterns in the MFCCs and other features, FFN (Feed Forward Layer) can be used to classify audio recordings into different emotions. In the architecture just described, the final dense layer acts as a Feed Forward layer.\n\nThe model was trained for 10 epochs using 'categorical cross entropy loss.\n\nModel's performance only slightly improved than the earlier fitted baselines.(Table  8 )",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Transfer Learning (Hubert Finetuning):",
      "text": "Hubert  [13]  employs a convolutional neural network (CNN) as its encoder. The CNN frontend performs feature extraction from the raw audio input.\n\nIt includes a transformer-based acoustic model. The transformer layer(s) in Hubert capture contextual information and allow the model to learn relationships between different parts of the input sequence. It helps in modeling long-range dependencies and generating contextualized representations.\n\nFigure  7  illustrates the distinction between partial and entire fine-tuning on the wav2vec 2.0/HuBERT model. In the case of partial fine-tuning, the model is partitioned into two components: CNN-based feature encoder and a transformer-based contextualized encoder. During this process, the CNN-based encoder remains frozen, with its parameters fixed, while only the transformer blocks are fine-tuned. Partial fine-tuning operates as a form of domain adaptation at the top level, safeguarding the integrity of the lower CNN layers that possess inherent expressive capabilities.\n\nConversely, for complete fine-tuning, depicted on the right side of Figure  1 , both the CNN and Transformer modules undergo fine-tuning throughout downstream training. Through training foundational features at the lower level, complete finetuning enables more comprehensive and targeted higher-level expressions. Assuming the fine-tuned wav2vec 2.0/HuBERT models already possess sufficient information-capturing power, we seamlessly augment them with straightforward downstream adaptors (e.g., classifiers/decoders) without introducing an additional burdensome and redundant encoder.\n\nIn the context of Speech Emotion Recognition (SER), a linear layer is directly appended as a downstream classifier. This linear layer serves as a fundamental component for utterance-level classification, with the overarching goal of minimizing crossentropy loss.\n\nConsidering this recent success of Hubert model in speech recognition tasks, it was decided to finetune it for SER task. Experiments were done with 2 model architectures:\n\n1. Standalone Fine-tuned Hubert Model:\n\nUsing PyTorch as the backend, fine-tuning was performed. Initial experiments were done by unfreezing the top 2 CNN encoder layers of the Hubert model. However, it was observed that the model performs better without unfreezing any of the pre-trained Hubert layers. Hence, finally, weights were updated only for the classifier layer (on top of the Hubert model).\n\nThe training process is described below:\n\n• Audio wav files from the train and validation datasets are resampled to a sampling rate of '16KHZ'. The resulting audio array is stored as a numpy array.\n\n• Using hugging face's feature extractor, specific features -MFCCs and their derivatives were extracted from the audio data stored in the array created in the earlier step.\n\n• Furthermore, these inputs are standardized and converted into a dataset object with key names of in-put_values and labels that can be used for training a model.\n\nTable  7  provides the values of network tuning parameters.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Fine-Tuned Hubert And Bert Multimodal Model:",
      "text": "To improve the accuracy further, a 'Multi-Modal' architecture was developed that combines the strengths of both the Hubert and BERT models.  and text embeddings and concatenates the last hidden states from the Hubert and BERT models. The concatenated output is then passed through a linear classifier to produce the final predictions.\n\nThe Hubert model is used to process the audio input, while the BERT model is used to process the text input. Both models are pre-trained on large amounts of data and fine-tuned for our specific task. The last hidden states from both models are extracted, mean-pooled along the sequence dimension, and passed through a dropout layer for regularization. The resulting outputs are then concatenated along the feature dimension to achieve state-of-the-art performance on our task. The training process is described below:",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Classifier",
      "text": "• The Hubert model which was fine-tuned earlier was utilized again.\n\n• The text transcriptions were extracted for each audio file using Open-AI's Whisper model 5 .\n\n• During training, Input wav files were passed through to the Hubert model. At the same time, tokenized text transcriptions of the audio wav files were forwarded to the BERT model.\n\nThis way the MultiModal model is jointly trained on Text Embeddings and Wav file Embeddings on the complete Train dataset and evaluated on validation set.\n\nFigure  6  illustrates the inference mechanism from this Model.\n\n• When a 'wav' file is given as input to the Model, it is fed both into the Preprocessing pipeline as well as the OpenAI Whisper model. 5 https://openai.com/blog/whisper/ • At the same time audio is pre-processed by applying 'Sam-pleRate Adjustment', 'Noise Reduction', 'Silence Removal'.\n\n• Text transcriptions generated before are tokenized using BERT's tokenizer and simultaneously, the feature array is extracted from an earlier pre-processed audio file using the Wav2VecFeatureExtractor,\n\n• Finally, both of these are given as input to the MultiModal model which gives output probabilities for each emotion class as well as text transcriptions as the final output.\n\nDataset WA UA RAVDESS 78.45 79.31",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Iv. Results",
      "text": "Table  8  presents the overall results for different models. We use the standard performance metrics for accessing different models: sample weighted accuracy, WA, which corresponds to the percentage of correctly classified samples over all samples; unweighed (or balanced) accuracy, UA, which is the average of the individual class accuracies and is not affected by imbalanced classes.\n\nAll models were trained on the same train-validation-test splits and the table 8 captures the accuracies on the 'test set'.\n\nFigure  8  shows the Confusion Matrix for the Multi-model on the 'test dataset'. One can see that the model very accurately identifies most of the 'angry' and 'sad' emotions. All 'calm' emotions are also identified without any error. However, the model does mix up 'sad' and 'happy' , 'angry' and 'happy' sometimes.\n\nFrom the results, it is evident that the 'MultiModal architecture' significantly outperforms others.\n\nBenchmarking and comparison Benchmark test was done with RAVDESS dataset  [24]  and it was found that our model achieved close to 80% accuracy on it. RAVDESS dataset was chosen as it was closest to the actual train dataset on which the model was trained and it requires only removal of the 'neutral' emotion label. Table  9  captures the benchmark test results.\n\nBased on the comparison of our results presented in Table  8 , we can conclude that the emotion recognition task benefits from the integration of different types of data. Our multimodal approach outperforms our speech-only test results and also performs better than other models.\n\nWe attribute our Model's strong performance to curating a robust training dataset (by combining multiple publicly available datasets), preprocessing and augmentation techniques, and the fact that we jointly train the powerful pre-trained BERT and Hubert transformer models.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "V. Web Application",
      "text": "To test the model's performance seamlessly and on real-world speech snippets, a Web-based application was created using Streamlit 6  framework. The web application interface was kept simple and uncluttered. A provision was made to upload the audio files in 'wav' format. Also, a provision to record a 3 seconds clip was provided for someone to record their own short clips.\n\nThe model would then take the speech input, do the preprocessing, and invoke the Whisper model for transcription generation. Text tokenized outputs along with the speech features were passed together to the Multi-modal architecture as shown in the inference diagram (figure  6 ).\n\nFigure  9  shows the graphical user interface along with the model's output consisting of the predicted Emotion probabilities and the transcripts.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Vi. Conclusion And Future Work",
      "text": "In this work, we evaluated different architectures for Speech emotion recognition tasks by training them on a combined dataset specifically curated using different audio augmentation and standardization techniques. We obtained state-of-the-art emotion recognition performance by including implicit linguistic information learned through joint finetuning of Hubert and BERT models. Our research shows that we are entering a new phase in the field of speech emotion recognition. This phase is characterized by the use of pre-trained transformer-based models that provide a strong foundation for integrating the two main sources of information in spoken language: linguistics and paralinguistics. This integration has been long sought after and can be finally achieved.\n\nIn future work, we intend to extend our proposed model to use Hubert LARGE and X-LARGE versions for a more accurate speech model. For text, apart from Bidirectional Encoder Representations from Transformers (BERT), we plan to experiment with Generative Pre-training (GPT) and its variants, Transformer-XL, and Cross-lingual Language Models (XLM).\n\nPersonality identification: Based on the findings from our work on speech emotion recognition, we intend to build a personality detection model to predict personality traits from the Big-Five impressions  [25] . We also intend to explore the relation between emotions and personality traits and the possibility of predicting both of them using a multi-task learning framework.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: ).The details about the dataset is listed",
      "page": 3
    },
    {
      "caption": "Figure 1: Combining different datasets.",
      "page": 4
    },
    {
      "caption": "Figure 2: Combined Preprocessed Dataset Distribution",
      "page": 4
    },
    {
      "caption": "Figure 3: Zero crossing rate (number of times the signal crosses the zero line in a given",
      "page": 5
    },
    {
      "caption": "Figure 4: Spectral Centroid (measure of \"centre of mass\" of the spectrum), is higher for",
      "page": 5
    },
    {
      "caption": "Figure 5: Spectrogram Images created out of blocks of 1-sec wav files.",
      "page": 6
    },
    {
      "caption": "Figure 5: ), a pre-trained CNN model called Mo-",
      "page": 7
    },
    {
      "caption": "Figure 6: Inference from the proposed MultiModal model.",
      "page": 8
    },
    {
      "caption": "Figure 7: Partial fine-tuning (left) and entire fine-tuning (right) of wav2vec 2.0/HuBERT. Source:[39]",
      "page": 8
    },
    {
      "caption": "Figure 8: ConfusionMatrix for the test set",
      "page": 9
    },
    {
      "caption": "Figure 7: illustrates the distinction between partial and entire",
      "page": 9
    },
    {
      "caption": "Figure 1: , both the CNN and Transformer modules un-",
      "page": 9
    },
    {
      "caption": "Figure 6: illustrates the inference mechanism from this Model.",
      "page": 10
    },
    {
      "caption": "Figure 9: Web Application screenshot for the proposed deployed model.",
      "page": 10
    },
    {
      "caption": "Figure 8: shows the Confusion Matrix for the Multi-model on",
      "page": 11
    },
    {
      "caption": "Figure 9: shows the graphical user interface along with the",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotions": "Dataset\nSAVEE\nRAVDESS\nTESS\nIEMOCAP\nCREMA-D",
          "angry\ncalm\ndisgust\nfearful\nhappy\nsad\nsurprised": "38\n0\n52\n54\n38\n37\n108\n122\n1600\n145\n182\n120\n115\n386\n258\n0\n344\n348\n270\n251\n791\n691\n0\n0\n20\n379\n707\n201\n491\n0\n1059\n996\n791\n490\n0",
          "Total": "327\n2670\n2262\n1998\n3827"
        },
        {
          "Emotions": "Column Total",
          "angry\ncalm\ndisgust\nfearful\nhappy\nsad\nsurprised": "1600\n1600\n1600\n1600\n1598\n1600\n1486",
          "Total": "11084"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotions": "Dataset\nSAVEE\nRAVDESS\nTESS\nIEMOCAP\nCREMA-D",
          "angry\ncalm\ndisgust\nfearful\nhappy\nsad\nsurprised": "11\n0\n8\n17\n9\n9\n37\n29\n400\n56\n43\n35\n41\n92\n67\n0\n70\n77\n55\n69\n202\n163\n0\n2\n5\n95\n168\n41\n130\n0\n264\n258\n206\n113\n0",
          "Total": "91\n696\n540\n474\n971"
        },
        {
          "Emotions": "Column Total",
          "angry\ncalm\ndisgust\nfearful\nhappy\nsad\nsurprised": "400\n400\n400\n400\n400\n400\n372",
          "Total": "2772"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Emotions": "Dataset\nSAVEE\nRAVDESS\nTESS\nIEMOCAP\nCREMA-D",
          "angry\ncalm\ndisgust\nfearful\nhappy\nsad\nsurprised": "11\n0\n14\n7\n13\n4\n14\n41\n38\n38\n32\n37\n36\n40\n75\n0\n75\n72\n75\n80\n81\n188\n0\n0\n12\n117\n197\n14\n276\n0\n258\n262\n259\n272\n0",
          "Total": "73\n262\n458\n528\n1327"
        },
        {
          "Emotions": "Column Total",
          "angry\ncalm\ndisgust\nfearful\nhappy\nsad\nsurprised": "591\n38\n385\n385\n501\n599\n1149",
          "Total": "2648"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Layer Type": "Input",
          "Kernels": "-",
          "Kernel Size": "-",
          "Activation": "-",
          "Padding": "-",
          "Batch Norm": "-",
          "Dropout": "-"
        },
        {
          "Layer Type": "Conv1D",
          "Kernels": "256",
          "Kernel Size": "8",
          "Activation": "ReLU",
          "Padding": "Same",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Conv1D",
          "Kernels": "256",
          "Kernel Size": "8",
          "Activation": "ReLU",
          "Padding": "Same",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Batch Norm",
          "Kernels": "-",
          "Kernel Size": "-",
          "Activation": "-",
          "Padding": "-",
          "Batch Norm": "Yes\n(After Conv1D)",
          "Dropout": "No"
        },
        {
          "Layer Type": "Dropout",
          "Kernels": "-",
          "Kernel Size": "-",
          "Activation": "-",
          "Padding": "-",
          "Batch Norm": "No\n(After Batch Norm)",
          "Dropout": "0.6"
        },
        {
          "Layer Type": "Conv1D",
          "Kernels": "128",
          "Kernel Size": "8",
          "Activation": "ReLU",
          "Padding": "Same",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Conv1D",
          "Kernels": "128",
          "Kernel Size": "8",
          "Activation": "ReLU",
          "Padding": "Same",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Conv1D",
          "Kernels": "128",
          "Kernel Size": "8",
          "Activation": "ReLU",
          "Padding": "Same",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Conv1D",
          "Kernels": "128",
          "Kernel Size": "8",
          "Activation": "ReLU",
          "Padding": "Same",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Batch Norm",
          "Kernels": "-",
          "Kernel Size": "-",
          "Activation": "-",
          "Padding": "-",
          "Batch Norm": "Yes\n(After Conv1D)",
          "Dropout": "No"
        },
        {
          "Layer Type": "Dropout",
          "Kernels": "-",
          "Kernel Size": "-",
          "Activation": "-",
          "Padding": "-",
          "Batch Norm": "No\n(After Batch Norm)",
          "Dropout": "0.6"
        },
        {
          "Layer Type": "Conv1D",
          "Kernels": "64",
          "Kernel Size": "8",
          "Activation": "ReLU",
          "Padding": "Same",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Conv1D",
          "Kernels": "64",
          "Kernel Size": "8",
          "Activation": "ReLU",
          "Padding": "Same",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Flatten",
          "Kernels": "-",
          "Kernel Size": "-",
          "Activation": "-",
          "Padding": "-",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Dense",
          "Kernels": "-",
          "Kernel Size": "-",
          "Activation": "ReLU",
          "Padding": "-",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Dense",
          "Kernels": "-",
          "Kernel Size": "-",
          "Activation": "ReLU",
          "Padding": "-",
          "Batch Norm": "No",
          "Dropout": "No"
        },
        {
          "Layer Type": "Dropout",
          "Kernels": "-",
          "Kernel Size": "-",
          "Activation": "-",
          "Padding": "-",
          "Batch Norm": "No\n(After Dense)",
          "Dropout": "0.6"
        },
        {
          "Layer Type": "Dense (Output)",
          "Kernels": "7",
          "Kernel Size": "-",
          "Activation": "Softmax",
          "Padding": "-",
          "Batch Norm": "No",
          "Dropout": "No"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classifier": "KNeighborsClassifier\nMLPClassifier\nXGBClassifier\nLGBClassifier\nLogisticRegression\nRandomForestCLassifier\nGaussianNB\nDecisionTreeClassifier",
          "WA": "62.94\n62.39\n48.61\n33.01\n50.82\n37.52\n23.89\n19.38",
          "UA": "56.72\n54.72\n44.07\n42.30\n41.58\n38.03\n32.74\n22.92"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Classifier": "KNeighborsClassifier\nMLPClassifier\nXGBClassifier\nMobilenetV2.0\nCNN (1D)\nHubert Standalone\nHubert and BERT MultiModal",
          "WA": "62.94\n62.39\n48.61\n38.03\n63.10\n75.52\n79.89",
          "UA": "56.72\n54.72\n44.07\n37.52\n56.70\n74.54\n77.68"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Vocal expression of emotion: Acoustic properties of speech are associated with emotional intensity and context. Psychological",
      "authors": [
        "Jo-Anne Bachorowski",
        "Michael Owren"
      ],
      "year": "1995",
      "venue": "Science"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "Abdul Malik Badshah",
        "Jamil Ahmad",
        "Nasir Rahim",
        "Sung Baik"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Platform Technology and Service (PlatCon)"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "4",
      "title": "On the opportunities and risks of foundation models",
      "authors": [
        "Rishi Bommasani",
        "Drew Hudson",
        "Ehsan Adeli"
      ],
      "year": "2022",
      "venue": "On the opportunities and risks of foundation models"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Emily Ebrahim (abe) Kazemzadeh",
        "Samuel Provost",
        "Jeannette Kim",
        "Sungbok Chang",
        "Shrikanth Lee",
        "Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "6",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "Houwei Cao",
        "David Cooper",
        "Ruben Michael K Keutmann",
        "Ani Gur",
        "Ragini Nenkova",
        "Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "year": "2020",
      "venue": "Proceedings of the 37th International Conference on Machine Learning"
    },
    {
      "citation_id": "8",
      "title": "An image-based deep spectrum feature representation for the recognition of emotional speech",
      "authors": [
        "Nicholas Cummins",
        "Shahin Amiriparian",
        "Gerhard Hagerer",
        "Anton Batliner",
        "Stefan Steidl",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM International Conference on Multimedia, MM '17"
    },
    {
      "citation_id": "9",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "Margaret Haytham M Fayek",
        "Lawrence Lech",
        "Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "10",
      "title": "Towards real-time speech emotion recognition using deep neural networks",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2015",
      "venue": "2015 9th International Conference on Signal Processing and Communication Systems (ICSPCS)"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "Kun Han",
        "Dong Yu",
        "Ivan Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "12",
      "title": "Recognition of stress in speech using wavelet analysis and Teager energy operator",
      "authors": [
        "Ling He",
        "Margaret Lech",
        "Sheeraz Memon",
        "Nicholas Allen"
      ],
      "year": "2008",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "13",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdelrahman Salakhutdinov",
        "Mohamed",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "M Dias Issa",
        "Adnan Demirci",
        "Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "15",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "Philip Jackson",
        "Sana Ul Haq"
      ],
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "16",
      "title": "Speech emotion recognition using clustering based ga-optimized feature set",
      "authors": [
        "Sofia Kanwal",
        "Sohail Asghar"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Convolutional rnn: an enhanced model for extracting features from sequential data",
      "authors": [
        "Gil Keren",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "Convolutional rnn: an enhanced model for extracting features from sequential data"
    },
    {
      "citation_id": "18",
      "title": "Real-time speech emotion recognition using a pre-trained image classification network: Effects of bandwidth reduction and companding",
      "authors": [
        "Margaret Lech",
        "Melissa Stolar",
        "Christopher Best",
        "Robert Bolia"
      ],
      "year": "2020",
      "venue": "Real-time speech emotion recognition using a pre-trained image classification network: Effects of bandwidth reduction and companding"
    },
    {
      "citation_id": "19",
      "title": "Learning methods for generic object recognition with invariance to pose and lighting",
      "authors": [
        "Y Lecun",
        "Jie Fu",
        "L Huang",
        "Bottou"
      ],
      "year": "2004",
      "venue": "Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "20",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "Jinkyu Lee",
        "Ivan Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "21",
      "title": "Classification of general audio data for contentbased retrieval",
      "authors": [
        "Dongge Li",
        "Ishwar Sethi",
        "Nevenka Dimitrova",
        "Tom Mcgee"
      ],
      "year": "2001",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "22",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Vince Mcloughlin",
        "Wu Guo",
        "Li-Rong Dai"
      ],
      "year": "2018",
      "venue": "Interspeech 2018. International Speech Communication Association"
    },
    {
      "citation_id": "23",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "Wootaek Lim",
        "Daeyoung Jang",
        "Taejin Lee"
      ],
      "year": "2016",
      "venue": "2016 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)"
    },
    {
      "citation_id": "24",
      "title": "Funding Information Natural Sciences and Engineering Research Council of Canada: 2012-341583 Hear the world research chair in music and emotional speech from Phonak",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "Funding Information Natural Sciences and Engineering Research Council of Canada: 2012-341583 Hear the world research chair in music and emotional speech from Phonak"
    },
    {
      "citation_id": "25",
      "title": "An introduction to the five-factor model and its applications",
      "authors": [
        "Robert Mccrae",
        "Oliver John"
      ],
      "year": "1992",
      "venue": "Journal of personality"
    },
    {
      "citation_id": "26",
      "title": "Acoustic event detection in real-life recordings",
      "authors": [
        "Annamaria Mesaros",
        "Toni Heittola",
        "Antti Eronen",
        "Tuomas Virtanen"
      ],
      "year": "2014",
      "venue": "Acoustic event detection in real-life recordings"
    },
    {
      "citation_id": "27",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "Soonil Mustaqeem",
        "Kwon"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "28",
      "title": "Group-level speech emotion recognition utilising deep spectrum features",
      "authors": [
        "Sandra Ottl",
        "Shahin Amiriparian",
        "Maurice Gerczuk",
        "Vincent Karas",
        "Björn Schuller"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction, ICMI '20"
    },
    {
      "citation_id": "29",
      "title": "An experiment with evaluation of emotional speech conversion by spectrograms",
      "authors": [
        "Jiří Přibil",
        "Anna Přibilová"
      ],
      "year": "2010",
      "venue": "Measurement Science Review"
    },
    {
      "citation_id": "30",
      "title": "Efficient emotion recognition from speech using deep learning on spectrograms",
      "authors": [
        "Aharon Satt",
        "Shai Rozenberg",
        "Ron Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "31",
      "title": "Vocal affect expression: a review and a model for future research",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "1986",
      "venue": "Psychological Bulletin"
    },
    {
      "citation_id": "32",
      "title": "Vocal communication of emotion: A review of research paradigms",
      "authors": [
        "Klaus Scherer"
      ],
      "year": "2003",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "33",
      "title": "Speech recognition employing mfcc and dynamic time warping algorithm",
      "authors": [
        "Meenakshi Sood",
        "Shruti Jain"
      ],
      "year": "2021",
      "venue": "Innovations in Information and Communication Technologies"
    },
    {
      "citation_id": "34",
      "title": "Investigating glottal parameters for differentiating emotional categories with similar prosodics",
      "authors": [
        "Rui Sun",
        "Elliot Moore",
        "Juan Torres"
      ],
      "year": "2009",
      "venue": "2009 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "35",
      "title": "Non-speech environmental sound classification using svms with a new set of features",
      "authors": [
        "Burak Uzkent",
        "D Buket",
        "Hakan Barkana",
        "Cevikalp"
      ],
      "venue": "Non-speech environmental sound classification using svms with a new set of features"
    },
    {
      "citation_id": "36",
      "title": "",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "37",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "Dimitrios Ververidis",
        "Constantine Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "38",
      "title": "An mfcc-gmm approach for event detection and classification",
      "authors": [
        "Lode Vuegen",
        "Bert Van Den",
        "Peter Broeck",
        "Jort Karsmakers",
        "Bart Gemmeke",
        "Hugo Vanrumste",
        "Van Hamme"
      ],
      "year": "2013",
      "venue": "An mfcc-gmm approach for event detection and classification"
    },
    {
      "citation_id": "39",
      "title": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding",
      "authors": [
        "Yingzhi Wang",
        "Abdelmoumene Boumadane",
        "Abdelwahab Heba"
      ],
      "year": "2022",
      "venue": "A fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding"
    },
    {
      "citation_id": "40",
      "title": "Speech emotion recognition with multiscale area attention and data augmentation",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Xiaodong Cui",
        "Wei Zhang"
      ],
      "year": "2021",
      "venue": "Speech emotion recognition with multiscale area attention and data augmentation"
    },
    {
      "citation_id": "41",
      "title": "Improve accuracy of speech emotion recognition with attention head fusion",
      "authors": [
        "Mingke Xu",
        "Fan Zhang",
        "Samee Khan"
      ],
      "year": "2020",
      "venue": "2020 10th Annual Computing and Communication Workshop and Conference (CCWC)"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition using spectrogram & phoneme embedding",
      "authors": [
        "Promod Yenigalla",
        "Abhay Kumar",
        "Suraj Tripathi",
        "Chirag Singh",
        "Sibsambhu Kar",
        "Jithendra Vepa"
      ],
      "year": "2018",
      "venue": "Interspeech"
    },
    {
      "citation_id": "43",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Yuanyuan Zhang",
        "Jun Du",
        "Zirui Wang",
        "Jianshu Zhang",
        "Yanhui Tu"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    }
  ]
}