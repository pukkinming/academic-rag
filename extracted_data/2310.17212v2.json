{
  "paper_id": "2310.17212v2",
  "title": "Affective Video Content Analysis: Decade Review And New Perspectives",
  "published": "2023-10-26T07:56:17Z",
  "authors": [
    "Junxiao Xue",
    "Jie Wang",
    "Xuecheng Wu",
    "Qian Zhang"
  ],
  "keywords": [
    "Affective computing",
    "Video emotion",
    "Video feature extraction",
    "Machine learning",
    "Emotional intelligence"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Video content is rich in semantics and has the ability to evoke various emotions in viewers. In recent years, with the rapid development of affective computing and the explosive growth of visual data, affective video content analysis (AVCA) as an essential branch of affective computing has become a widely researched topic. In this study, we comprehensively review the development of AVCA over the past decade, particularly focusing on the most advanced methods adopted to address the three major challenges of video feature extraction, expression subjectivity, and multimodal feature fusion. We first introduce the widely used emotion representation models in AVCA and describe commonly used datasets. We summarize and compare representative methods in the following aspects: (1) unimodal AVCA models, including facial expression recognition and posture emotion recognition; (2) multimodal AVCA models, including feature fusion, decision fusion, and attention-based multimodal models; (3) model performance evaluation standards. Finally, we discuss future challenges and promising research directions, such as emotion recognition and public opinion analysis, human-computer interaction, and emotional intelligence.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The ability of machines to possess emotions is a crucial feature of intelligent machines, and addressing emotional issues is a prerequisite for machines to have emotions. Despite the significant role emotions play in machine and artificial intelligence, much less attention has been given to affective computing than objective semantic understanding, such as object classification in computer vision. The rapid development of artificial intelligence has achieved significant advancements in semantic understanding, which has placed higher demands on emotional interaction. For example, a mobile assistant capable of recognizing and expressing emotions could provide services that better meet users' needs, especially the visually impaired. In order to achieve humanlike emotions, machines should first understand how humans express emotions through various channels, such as voice, facial expressions, body posture, and physiological signals. Although physiological signals, unaffected by human will, can provide more reliable information, capturing accurate physiological signals is quite challenging and requires special wearable sensors. On the other hand, the widespread popularity of short video social platforms (such as TikTok, Instagram, and Douyin) allows users to share their daily lives and express their opinions through audio and video. Recognizing the emotional content in this vast amount of video data provides a means to understand users' behavior and emotions.\n\nAs we know, \"a picture is worth a thousand words\", which emphasizes the rich information an image can convey. A video is composed of a sequence of continuous images, making it capable of conveying even richer information than a single image. In addition to images, videos also include audio signals, and sound is the most commonly used way for humans to express emotions  [1] [2] [3] . In contrast to existing research, analyzing video data faces the challenge of extracting video features, making effective video architecture a key focus of Affective Video Content Analysis (AVCA). Compared to objective semantic understanding, emotion recognition focuses on a higher level -the cognitive level, i.e., understanding how videos can evoke emotions in viewers, which is more challenging. Using AVCA to assess human emotional states automatically can help evaluate users' mental health, detect emotional abnormalities, and assist recommendation algorithms in recommending suitable videos to adjust emotions, thereby preventing the deepening of mental issues for users or even the occurrence of extreme behavior in society as a whole  [4, 5] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Main Goals And Challenges",
      "text": "Main Goals. Given an input video, AVCA mainly aims to (1) identify the emotions the video creator intends to convey (Based psychology, the emotions can be represented using different models, such as categorical or dimensional models. For further details, please refer to Section 2.),  (2)  analyze the stimuli within the video that trigger these emotions, such as specific language, actions, or combinations of expressions, and (3) apply the identified emotions to various real-world contexts to enhance emotional intelligence. Challenges. (1) Video Feature Extraction. Compared to static image data, video data possesses more complex characteristics. The temporal nature of video provides  a richer form of emotional expression. For instance, in Fig.  1 , the person in Fig.  1 (a) displays a sense of sadness, while as the video progresses, we can observe the person in Fig.  1 (d) expressing overwhelming happiness. Relying solely on individual frames, it is difficult to accurately discern the emotions conveyed by the creator, whereas the temporal nature of video effectively addresses this issue. Furthermore, the temporal nature of video also presents new challenges for feature extraction. The method of taking the average of the feature vectors of all frames in a video, although based on the intuitive solution of image convolutional neural network (CNN), cannot effectively utilize the feature information of the video. In contrast, NetVLAD  [6]  uses the clustering concept to obtain multiple cluster centers by clustering the frame features of the video. Then concat the average of the feature vectors in each cluster area to form the feature vector of the entire video. This method effectively utilizes the video's feature information and performs excellently in video feature extraction. In response to the temporal information of the video, researchers have utilized recurrent neural networks (RNN) for feature extraction. RNN can effectively extract feature information from temporal data, with Long Short Term Memory (LSTM) being widely applied in video feature extraction due to its ability to capture long-term dependencies. It has been applied in various domains, such as rainfall text emotion recognition  [7] , video summarization  [8] , and action recognition  [9] . Expanding two-dimensional convolutions to obtain three-dimensional convolutional kernels suitable for video data is the most natural solution. Jo√£o et al.  [10]  expanded the pre-trained weights of two-dimensional convolutions to 3D convolutions and incorporated optical flow modal information to address the issue of motion information loss during frame extraction. To date, the use of three-dimensional convolutional kernels in the design of models for video feature extraction is considered the optimal solution, with the extracted features demonstrating significantly higher effectiveness than other methods.\n\nIn addition to video features, incorporating available contextual information also proves helpful for the AVCA task. As illustrated in Fig.  2 , the same video may evoke different emotions under different narratives and voice-overs. For instance, in Fig.  2 (a), if we only see the mother, we might infer fear based on her expression; however,   witnessing the firefighter rescuing a child from a flood is more likely to lead to an inference of surprise. In Fig.  2 (b), upon seeing the disabled person, we might infer profound sadness; yet upon hearing his words, \"I love my life! Nothing's stopped me!\", we are more likely to infer an extremely positive outlook.\n\n(2) Expression Subjectivity. As a result of the differing personal backgrounds of users, including cultural, social, and individual personality factors  [11, 12] , the same emotion may be expressed in completely opposite ways. For example, the gesture in Fig.  3 (a) is commonly used to express affirmation and support in most countries; however, in some countries, this gesture is used to convey offensive behavior, such as in Iran, Greece, and the island of Sardinia in Italy. Additionally, other actions may have different meanings in different regions, such as nodding indicating disagreement in the Mediterranean region. This fact leads to expression subjectivity issues, thus requiring accurate annotation of emotional content by considering the overall background of the content creator. These background differences may impact the recognition of emotions and the interpretation of emotional content, and it is essential to consider these background factors when conducting emotional analysis and annotating emotional content.\n\nTo address the subjectivity of emotional expression, it is crucial to accurately consider the cultural and social backgrounds of content creators during the dataset construction process to achieve precise data annotation. Accurately describing the emotions expressed by users is the primary concern in building the dataset. In psychological research, different models are used to represent emotions, primarily categorized as categorical emotion states (CES)  [13] [14] [15]  and dimensional emotion space (DES)  [16, 17] . These methods of representing emotions have been widely adopted in other areas of affective computing research, and existing video emotion content analysis datasets also employ the same emotional representation methods.\n\n(3) Multimodal Feature Fusion. Videos contain multimodal information such as images, audio, and text. Effectively utilizing the rich information in videos is an important method for improving model performance. In multimodal research, fusion methods are categorized based on different fusion times into feature-level fusion (early fusion), decision-level fusion (late fusion), and hybrid fusion. Feature-level fusion involves using multiple feature extractors to extract features from different modalities and then fusing these features into input features according to a fusion strategy before entering the classifier. This method can fully utilize the feature information from different modalities, but it is also important to consider how to effectively fuse different modality features while maintaining the integrity of the information. In contrast, decision-level fusion involves assigning a separate classifier to each feature extractor to obtain emotional decisions for different modalities and then fusing these decisions into a final decision based on a fusion strategy. This method can independently handle the features and decisions of different modalities, but it also requires careful consideration of how to sensibly fuse the decisions from multiple modalities to obtain the final result. Hybrid fusion combines the advantages of the previous two fusion methods. It involves fusing similar modalities at the feature level, such as images and optical flow, and fusing dissimilar modalities at the decision level, such as images and text. This fusion method simplifies the feature information of similar modalities and effectively utilizes the feature information from different modalities, thereby achieving better performance in multimodal emotion recognition.\n\nFusion strategy is indeed the key factor in determining the performance of multimodal models across the three fusion methods. Taking the average of all modality features or decisions is the most intuitive solution and has been widely used in many models. For example, I3D  [10]  uses this decision fusion for the image and optical flow modalities and achieves excellent performance. However, while averaging is intuitive, it contradicts the logic of emotional decision-making. When dealing with information from different modalities, people assign different weights. For example, phrases such as \"a picture is worth a thousand words,\" \"seeing is believing,\" and \"actions speak louder than words\" indicate the differential weight assigned to different modalities. Therefore, in some studies  [18, 19] , manually designed multimodal fusion weights have resulted in superior performance. In other studies  [20] , researchers have optimized fusion weights as model parameters during the training process, leading to even better performance and higher efficiency compared to manually designed forms.\n\nThe credibility of model classification results is increasingly receiving attention. In single-modal models, the credibility of the model indicates the probability that the model's classification results are worthy of adoption. In multimodal models, the credibility of different modality classification results may affect the overall credibility of the decision. For instance, in the case of the video depicted in Fig.  4 , the multimodal model's classification results, as shown in Fig.  4(a) , indicate that the model classified the input data into incorrect labels in the video modalities. In contrast, the audio modality correctly classified the input data. Consequently, the final decision resulting from decision fusion could be influenced by the erroneous sub-results, leading to an incorrect output. In a study by Han et al.  [21] , this issue was addressed by incorporating confidence as fusion weights, as shown in Fig.  4(b) . The outstanding performance of the credibility of model classification results in multimodal models warrants further research and exploration.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Organization Of This Survey",
      "text": "In this survey, we will first focus on reviewing the latest approaches to AVCA and outline research trends. Specifically, we will provide a brief historical overview of AVCA in section 1.3 and introduce its comparison with other related topics in section 1.4. Furthermore, we will introduce widely used emotion representation models in section 2. Thirdly, we will summarize the available datasets for AVCA evaluation in section 3, comparing the sources of information and the number of information modalities included in the datasets. In section 4, based on the primary objectives and challenges outlined in section 1.1, we will summarize and compare video feature extraction methods, the effectiveness of different modality features, and multimodal fusion methods. Fifthly, we will present representative methods for evaluating emotion recognition results in section 5. Finally, we will discuss potential research directions in section 6. These contents will contribute to a comprehensive understanding of the latest developments and research trends in AVCA.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Brief History",
      "text": "Affective Computing. The origins of affective computing can be traced back to a patent application in 1978  [22] . Subsequently, papers on speech emotion generation  [23]  and facial expressions recognition  [24]  by neural networks introduced affective computing into the research domain in 1990 and 1992, respectively.\n\nSince the inception of the concept of affective computing in the context of intelligent machines, research related to emotions, including the definition and recognition of emotions  [25] , has garnered significant attention. In 1997, the concept of affective computing was first proposed, with Picard providing the following definition  [26] : \"affective computing is computing that relates to, arises from, or deliberately influences emotion or other affective phenomena\". Subsequently, more researchers began to focus on the field of affective computing and organized important academic conference events.  Affective Image Content Analysis. AICA focuses on studying the relationship between images and emotions, with the aim of identifying and understanding the emotions evoked by images. Early emotion recognition methods were primarily based on handcrafted features such as Wiccest, Gabor  [27] , artistic elements  [28] , and Adjective Noun Pairs (ANPs)  [29] . However, with the emergence of CNNs in 2014, their outstanding performance has surpassed that of all handcrafted features. Through pre-training on large-scale data and model transfer learning of parameters, CNNs can extract more representative features, thus overcoming the challenge of manually designed features. In addition, AICA has also begun to focus on the challenge of perception subjectivity, with researchers considering personalized emotional prediction  [30, 31]  and emotional distribution learning  [32, 33] . To address the challenge of missing labels, AICA models have introduced methods such as domain adaptation  [34, 35]  and zero-shot learning  [36] . These methods aim to improve the accuracy and applicability of emotion recognition in order to understand better the emotions evoked by images. Affective Video Content Analysis. AVCA is an extension of AICA's research. With the iteration and evolution of social platforms, the mainstream mode of information dissemination is gradually shifting towards video format. Therefore, the transformation and optimization of existing networks are imperative. As the earliest public' video emotion challenge', EmotiW was held at the ACM International Conference on Multimodal Interaction (ICMI) in 2013. In early research, researchers achieved video feature extraction by combining CNN and RNN  [37, 38] . 3D CNNs have shown outstanding performance in video feature extraction, successfully addressing this challenge. Researchers have also started to focus on the multimodal information contained in videos. They have considered different feature fusion strategies, such as feature-level fusion  [39, 40] , decision-level fusion  [41, 42] , and hybrid fusion  [43] . The milestones in emotional computing and AVCA are summarized in Fig.  5 .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Comparison With Other Related Topics",
      "text": "Comparison with Affective Computing of Other Modalities. Affective content analysis has been widely studied in other research fields, such as text  [44, 45] , audio  [46] , music  [47, 48] , images  [49] , facial expression  [50] [51] [52] , and physiological signals  [53, 54] . Although similar affective models and learning methods are employed, there are significant differences between the affective computing of videos and that of other modalities, especially in the representation of emotional features from multimodal information. Despite the fruitful research conducted on other modalities, the study of AVCA is still not comprehensive enough. Considering the rich emotional information contained in videos, an in-depth analysis of AVCA will propel the development of the field of emotional computation.\n\nComparison with Computer Vision. AVCA generally involves three stages: manual annotation, extraction of visual features, and learning the mapping between emotional labels. Although these steps may seem similar to those in computer vision (CV), there are significant differences between AVCA and CV.  (1)  The subjective nature of emotional expression requires the manual annotation process to consider the personal and social backgrounds of the video presenters. For example, individuals from the Mediterranean region may express opposite emotions with a thumbs-up gesture compared to individuals from other regions. (2) Objects are objective concepts (e.g., gestures such as thumbs-up), while emotions are relatively subjective concepts (related to personal and social backgrounds). (  3 ) Consequently, object classification belongs to the perceptual level of images, while AVCA focuses on the cognitive level. The CV community primarily studies object classification, whereas AVCA is an interdisciplinary task that requires support from psychology, cognitive science, multimedia, and machine learning, among other disciplines.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Emotion Models From Psychology",
      "text": "Emotions play a significant role in our daily lives, exerting profound effects on human consciousness. Generally, emotions are spontaneously generated psychological states or feelings, representing an attitude or experience of individuals toward whether objective things satisfy their own needs  [55] . This experience is subjective, with potential variations among different individuals in response to the same objective stimuli. Simultaneously, it exhibits a certain regularity, and the social nature of humanity results in a consistent tendency among individuals in the same social environment regarding emotions toward the same objective stimuli.\n\nIn affective computing tasks, researchers often use two different methods to describe emotions. One method is the categorical emotion states (CES), which categorizes emotions into discrete categories. In other words, the result of emotion recognition must be selected from a predefined list of word labels, such as the six emotion models proposed by Ekman  [14]  (anger, disgust, fear, happiness, sadness, surprise) and the eight emotion models by Mikels  [15]  (amusement, anger, awe, contentment, disgust, excitement, fear, and sadness). With the development of psychological theories, discrete emotion models have become increasingly diverse and precise. Apart from the eight basic emotion categories, Plutchik  [13]  organized each emotion into three intensities, Fig.  6  V-A emotional space and typical emotional subspaces (as cited in  Plutchik et al., 1980)  as shown in Fig.  6 , thereby providing a richer set. For example, the three intensities of joy and fear are respectively ecstasy, happiness, serenity and terror, fear, unease. Another representative discrete emotion model is the tree hierarchical grouping proposed by Parrott  [56] , which divides emotions into first, second, and third categories. For instance, a three-tier emotion is designed with two basic categories at level 1 (positive and negative), six categories at level 2 (anger, fear, happiness, love, sadness, surprise), and 25 refined emotional categories at level 3, as shown in Fig.  7 .\n\nAnother method is the dimensional emotion space (DES). Observers don't select discrete labels but can display their impressions of each stimulus on several continuous scales, such as pleasant-unpleasant, attention-rejection, simple-complex, and more. Two common scales are valence and arousal. Valence describes the pleasantness of a stimulus, with one end being positive (or pleasant) and the other negative (or unpleasant). For instance, happiness has a positive valence, while disgust has a negative valence. Another dimension is arousal. For example, sadness has a low arousal level, while surprise has a high arousal level. Different emotional labels can be plotted on these two axes, constructing a two-dimensional emotion model  [57] , like the emotion circumplex model proposed by Russell et al.  [58] , as shown in Fig.  8 . It's a circular model divided into quadrants, displaying the valence and arousal levels of emotional states. The x-axis represents a continuum between pleasant and unpleasant emotions, and the y-axis represents high and low arousal emotions, with the circle's center indicating neutral valence and moderate arousal. Subsequently, the American psychologist Engen et al.  [59]  proposed a three-dimensional model describing emotions with three independent dimensions: pleasant-unpleasant, attention-rejection, and high arousal-low arousal, as shown in Fig.  9 . Different emotions can be distinguished in the emotional space based on the distinct characteristics of these three dimensions.\n\nThe CES model and the DES model each have their own merits, as shown in Table  1 . While the CES is easy to understand, the limited number of emotion categories fails to capture the complexity and subtlety of emotions fully. Additionally, psychologists have not reached a consensus on how many discrete emotion categories should be included. In contrast, the DES uses continuous two-dimensional, three-dimensional, or higher-dimensional Cartesian space to represent emotions. In theory, each emotion can be represented as a coordinate point in Cartesian space. However, the representation of continuous values is challenging to comprehend, limiting the use of the DES.",
      "page_start": 9,
      "page_end": 11
    },
    {
      "section_name": "Datasets",
      "text": "The dataset plays a crucial role in training AVCA models and evaluating their effectiveness. Corresponding to the AVCA method, the dataset can be divided into single-modal and unimodal datasets. In unimodal datasets, samples are exclusively from the video modality. In contrast, multimodal datasets include various information beyond the video modality, such as other modal images (e.g., depth maps, infrared images), text, and audio. Refer to Table  2  for a detailed comparison of mainstream datasets.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Unimodal Datasets",
      "text": "The most commonly used datasets in the video unimodal data category are the MMI facial expression database  [74]  and the AFEW database  [75] . The MMI facial expression database comprises data from 61 adults displaying various basic emotions and 25 adults responding to emotional videos. The data are captured from frontal and side angles, resulting in over 2,900 videos and high-resolution still images of 75 subjects. This database encompasses six basic emotions: anger, disgust, fear, happiness, sadness, and surprise. Due to the diverse ways in which subjects express emotions, MMI is considered a challenging database. The AFEW database is currently the most widely used dataset for audio-visual emotion recognition. It is employed in the emotion recognition challenge held by AFEW corporation for the wild challenge. The database comprises video clips extracted from movies that include facial expressions, presenting an environment close to the real world and accurately reflecting human emotional expressions in authentic scenarios. With a total duration of approximately 1.2 hours, it includes a total of 1,809 video segments, covering six basic emotions (anger, disgust, fear, happiness, sadness, surprise) along with neutral emotions, totaling seven types of labels. The specific distribution of data labels is illustrated in Fig.  10 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Multimodal Datasets",
      "text": "Compared with unimodal emotion recognition using speech and facial expressions, research on multimodal emotion recognition is relatively recent. Therefore, the number of multimodal emotion datasets is limited, and most are either semi-open or closed.\n\nMany researchers have established their datasets similar to speech emotion datasets. These datasets are primarily categorized into spontaneous and induced types based on the way emotions are triggered. Spontaneous multimodal emotion datasets mainly originate from interviews or variety shows, while induced multimodal emotion datasets are primarily created through performances in a laboratory environment based on given scripts. Some commonly used multimodal emotion recognition databases include the IEMOCAP dataset and the eNTERFACE'05 dataset.\n\nThe IEMOCAP is a multimodal emotional dataset recorded collaboratively by Busso et al.. The dataset consists of 12 hours of audiovisual material, encompassing various aspects such as video, audio, and text. This dataset is created by 10 actors based on scripts or improvisational performances. Subsequently, each dialogue is segmented into multiple single sentences. Each sentence must encompass at least one of the emotions: joy, sadness, anger, surprise, fear, or disgust. Moreover, a minimum of three annotators is involved in the annotation process. eNTERFACE'05 is a multimodal emotion dataset based on speech and facial expressions. Co-created by  Martin et al. in 2006 , the dataset comprises 1,287 videos involving 42 participants. Testers listened to six English short stories and responded to the specific emotional contexts portrayed in these narratives. Each audio-visual file is labeled with a single emotion, encompassing six basic emotions: happiness, sadness, anger, surprise, fear, and disgust. The emotional label distribution of this dataset is illustrated in Fig.  11 .\n\nFrom the above datasets, it can be observed that the existing datasets are mainly collected through script deduction or laboratory records, lacking genuine emotional records in natural states. Moreover, the types of modalities included in multimodal datasets need to be more comprehensive, with instances of partial modal data loss. Therefore, the trend in future dataset collection should be to increase the number of samples, consider multiple factors (such as region, race, personality, gender, age, etc.), pay attention to environmental changes, and consider adding sources of information. The more factors considered, the more comprehensive the dataset will be, providing researchers with data that will be more beneficial for exploring new methods of video emotion recognition.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Affective Video Content Analysis",
      "text": "In recent years, affective video content analysis has attracted significant attention and has been widely applied in various areas, including human-computer interaction, personalized video retrieval, and emotional video advertising. Generally, AVCA methods can be categorized into two main types: unimodal methods, including those based on facial expressions and action, and multimodal methods. We categorize mainstream AVCA methods into unimodal and multimodal types for discussion. We provide an overview of these methods in Table  3 , introducing representative literature, methodological principles, and the strengths and weaknesses of existing approaches.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Unimodal Avca",
      "text": "Facial Emotion Recognition. The facial emotion recognition (FER) method primarily involves capturing the facial expression features of individuals in videos to identify emotions. Facial expressions are an important means of emotional expression, with each change in the morphology of facial organs and muscles containing crucial information to support affective analysis. According to the research of the American psychologist Mehrabian  [99] , in the process of communication, facial expressions can convey 55% of the information, tone of voice conveys 38%, while the language itself conveys only 7% of the information. Therefore, facial expressions can convey more emotional information compared to other methods. As a result, many researchers prefer to use facial expressions for emotion recognition.\n\nBargal et al.  [37]  adopted the facial detection method proposed by Chen et al.  [100] , using three deep learning networks -VGG13, VGG16, and ResNet -to extract facial expression features from videos. Subsequently, emotion recognition was performed on the features encoded through STST. The results of ablation experiments revealed that the combination of features, specifically using fc5 of VGG13, fc7 of VGG16, and the pool of ResNet, yielded the optimal performance in emotion recognition. Notably, features connected from different networks exhibited superior performance compared to features calculated from the same network. This feature combination achieved an accuracy of 59.42% on the AFEW dataset, marking a 20.61% improvement over the baseline method in the EmotiW'16 challenge. Fan et al.  [76]  proposed an emotion recognition system based on video, which adopts a late fusion approach of RNN and C3D. The RNN takes the appearance features extracted by CNN on individual video frames as input and encodes the motion, while C3D models both the appearance and motion of the video simultaneously. By incorporating an audio module, the system achieved a recognition accuracy of 59.02% without using additional emotional labels for video clips, with an accuracy rate of 53.8% in the EmotiW'15 challenge. Extensive comparative experiments have shown that the combined RNN and C3D model has a significant advantage in video emotion recognition.\n\nXue et al.  [77]  proposed a coarse-to-fine cascaded network with smooth predictions (CFC-SP) for video-based facial emotion recognition. To address the issue of label ambiguity, they initially grouped several similar emotions into rough categories. Subsequently, they employed a cascaded network for preliminary classification, followed by fine classification. Furthermore, they introduced smooth prediction (SP) to enhance performance. The effectiveness of this method was validated through ablation experiments on the Aff-Wild2 dataset  [101] , achieving a final f1-score of 46.15%. Regarding the label ambiguity issue, they identified two possible causes: the inherent ambiguity in expressions, where some expressions are similar and challenging to differentiate, and the diverse interpretations of facial expressions among individuals, leading to label ambiguity and inconsistency. For example, \"sadness\" and \"disgust\" are very similar and difficult to distinguish. Hence, they proposed a coarse-to-fine cascaded network to obtain more reliable predictions gradually.\n\nHu et al.  [38]  focused on the emotion recognition of facial expressions in video sequences. They proposed an integrated framework based on Local Enhanced Motion History Images (LEMHI) and a CNN-LSTM cascaded network, as illustrated in Fig.  12 . In the local network, they employed a novel approach, LEMHI, to aggregate frames from unrecognized videos into a single frame. This method used detected facial landmarks as attention regions to enhance local values in differential image calculations, effectively capturing the movements of key facial units. Subsequently, this frame was input into a CNN network for prediction. On the other hand, they utilized an improved CNN-LSTM model as a global network, serving as a feature extractor and classifier for facial emotion recognition in video. Finally, a random search weighted sum strategy was employed as a late fusion method for the ultimate prediction. Experimental results on AFEW and MMI datasets yielded accuracies of 51.2% and 78.4%, respectively. The findings indicate that the integrated framework of the two networks performs better than the individual networks alone. Actional Emotion Recognition. Emotion recognition in videos based on body postures and movements is an emerging research area. Its principle involves using the movement characteristics of individuals in videos to predict the emotions conveyed in the footage. Psychological studies have found that human perception can identify emotional states expressed through bodily movements  [102] . When expressing emotions, people exhibit habitual bodily actions involving hands, legs, shoulders, etc. For instance, trembling legs when nervous, shrugging shoulders when helpless, or dancing joyfully when happy. Dutch psychologist Beatrice de Gelder  [103]  systematically examined the neural basis of bodily expression processing based on six basic emotion theories, discovering that emotional bodily stimuli significantly activate the amygdala, fusiform gyrus, and superior temporal sulcus, partially overlapping with brain areas involved in facial expression processing. As bodily movements are often subconscious actions, they are seldom deceitful. Hence, there has been a growing interest in recognizing emotions solely through body movements, postures, and gestures in recent years.\n\nGavrilescu et al.  [80]  conducted experiments to verify that incorporating gesture emotion recognition can improve the classification accuracy of facial emotion recognition systems, indicating that gestures and body postures contain emotional information not obtainable from facial expressions. Therefore, studying individuals' gestures and body movements in videos can enhance the accuracy of traditional facial emotion recognition methods. Shen et al.  [81]  conducted research on a selfcollected dataset, which includes six postures: jumping, squatting, throwing, standing, retreating, and turning away. They utilized Temporal Segment Network (TSN) and Spatial-temporal Graph Convolutional Networks (ST-GCN) to extract behavioral and posture features and trained the emotion recognition model using an improved ResNet network. Ultimately, they achieved a recognition accuracy of 53.57% on the self-collected dataset. Specifically, ST-GCN addressed the issue of traditional GCN's inability to model relative position changes between key nodes, proving highly effective in extracting skeletal posture features. The network framework is illustrated in Fig.  13 .\n\nIn AICA studies, basic emotions are insufficient to describe the complex and diverse emotional states. New postures and gestures not included in the collected gesture samples may appear during testing. Based on this, Wu et al.  [82]  proposed a new mechanism to address this issue by considering each emotional category as a set of multiple gesture categories. This approach aims to utilize gesture information for emotion recognition better. They introduced the Generalized Zero-shot Learning (GZSL) framework to identify visible and invisible body gesture categories using semantic information, and made emotion predictions based on the relationship between gestures and emotions. Ultimately, they achieved an accuracy of 67.85% on the publicly available MASR  [104]  dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Multimodal Avca",
      "text": "Feature-level Fusion, also known as early fusion  [105] , is a technique that uses feature concatenation as a fusion method. The design process involves extracting various modal data, constructing corresponding modal features, and designing cross-modal feature mapping rules. Samadian et al.  [40]  proposed a video emotion recognition system, VERMFF, which combines audio and video modalities. They use an SR classifier to classify seven basic emotions and employ a kernel SR based on quality metrics to fuse multiple features. Additionally, they consider challenges such as head poses and lighting variations in outdoor-shot videos and propose a feature extraction method to address them. Ultimately, they achieved an accuracy of 54.39% on the AFEW dataset for the seven-classification task. Adhikari et al.  [39]  introduced a feature-level fusion-based audio-visual multimodal emotion recognition system. They fuse features learned from audio-visual models and connect them to a dense layer meta-classifier. The fusion process involves simple linear, concatenation, or element-wise addition. They achieved a classification accuracy of 71.5% on the eNTERACE'05 dataset for the seven-classification task.\n\nCai et al.  [18]  proposed two new audio-visual fusion methods. One is the featurelevel fusion method, combining audio features with three different visual features: LBP-top-based, CNN-based, and CNN-BLSTM features. These are connected to form a joint feature vector through feature normalization, and each video clip is then inputted into a linear SVM for emotion recognition. The other is the model-level fusion method, explicitly addressing differences in time scale, temporal shift, and measurement from different signals. They independently extract audio and visual information for emotion recognition, and then combine these unimodal recognition results through a probabilistic framework (Bayesian network). In the inference process, the final decision is made by maximizing the posterior probability of all measured values. Experimental results on the AFEW database indicate that both proposed fusion methods significantly outperform baseline unimodal recognition methods and perform comparably or even better than state-of-the-art methods. Fig.  14  illustrates the proposed audio-visual feature-level fusion and model-level fusion frameworks.\n\nIn contrast to the commonly used audio and video fusion frameworks, Nguyen et al.  [85]  proposed a novel emotion recognition framework. This framework integrates facial expressions, postures, body movements, and sound. This framework employed a cascaded 3D Convolutional Network (C3Ds) and Deep Belief Networks (DBNs) to introduce new depth-temporal features, effectively modeling spatiotemporal information in videos and audios for emotion recognition. Subsequently, they introduced a novel feature-level fusion method based on bilinear pooling theory to integrate visual and audio feature vectors. This fusion strategy allows all elements of component vectors to interact effectively, capturing the complexity and inherent correlations between component patterns. Extensive experiments conducted on the eNTERFACE'05 dataset demonstrate that this method can enhance the performance of multimodal emotion recognition, significantly outperforming existing approaches. Decision-level Fusion, also known as late fusion  [106] , is a technique that integrates single-modal decisions through specific fusion rules. The method design process includes inputting signals of different modalities into the corresponding models for feature extraction, using their respective classifiers for emotion recognition, and integrating the predictive results of each modality according to fusion rules. Common fusion methods include maximum value method, minimum value method, product method, sum method, average value method, voting method, Bayesian decision theory, Adaboost algorithm, and DS evidence theory.\n\nAvots et al.  [42]  conducted emotion recognition based on audio-visual information. For the video part, key frames of the video were selected for facial emotion recognition, and MFCC coefficients were extracted for the audio part. The decision-level feature fusion method used is as follows: 6 score values were set for each audio and video prediction, corresponding to the prediction accuracy of a specific category. The sum of all probabilities is 1, and the maximum value represents the predicted label. The probabilities are summed and normalized separately to obtain the final prediction. The system achieved an accuracy of 69.30% in the six-classification task of the RML dataset  [107] . Noroozi et al.  [41]  proposed a multimodal emotion recognition system based on audio-visual clues. For the visual part, they first calculated the geometric relationships of facial landmarks and then summarized each emotional video into a set of simplified keyframes, which can distinguish emotions visually. Finally, the confidence outputs of classifiers for all modalities were used to define a new feature space for decision-level fusion for the final emotion label prediction. This system achieved an accuracy of 63.56% in the six-classification task of the eNTERACE'05 dataset.\n\nSahoo et al.  [19]  proposed a multimodal emotion recognition method using facial images and voice data. The method employs a rule-based decision-level fusion approach, and the decision rules are illustrated in Fig.  15 . The method achieved good results on the eNTERACE'05 dataset, with an accuracy of 81% in situations related to the subjects and 54% in situations independent of the subjects. However, this work was conducted in a laboratory environment without any ambient noise, using an emotion database recorded in the laboratory for validation. The effectiveness of the method in the presence of various environmental anomalies and its applicability to unknown genuine emotions and non-behavioral emotions remain unverified. Additionally, the decision rules set here are based on unimodal performance in the database. Applying the same rules with identical thresholds on different datasets may not be effective.\n\nFor videos, emotional signals exist only in certain segments rather than the entire video signal. Emotion recognition methods based on deep learning directly process input video signals, but they fail to capture the regions containing emotional information in the signal effectively. Moreover, they do not make optimal use of the characteristics of emotional information, significantly limiting the efficiency and performance of the algorithm. In response to this, Zhalehpour et al.  [108]  proposed a fully automatic multimodal emotion recognition system based on three new peak frame selection methods. These methods include MAX DIST, DEND-CLUSTER, and EIFS. Regarding the decision-level fusion of video and audio modalities, they tested several methods for obtaining probabilities and ultimately chose the weighted product rule  [109] , achieving an optimal emotion recognition accuracy of 78.26% on the eNTERACE'05 dataset.\n\nLiu et al.  [91]  proposed a multimodal emotion recognition model based on an attention mechanism, as illustrated in Fig.  16 . Firstly, facial features and audio features are fused to obtain integrated features. Subsequently, attention to facial features is highlighted using the fused features, and facial features are weighted. Finally, through attention analysis, facial and audio features are integrated to obtain the ultimate fused features. This method, through attention analysis of fused features, reveals relationships between features, assigning more weight to noise-free and highly discernible features while reducing the weight of noisy features. Ultimately, an 81.18% recognition accuracy is achieved on the RML dataset.\n\nMultiple modalities convey different valence and arousal information, and their complementary relationships need to be effectively captured. However, most advanced audio-visual fusion methods rely on recurrent networks or traditional attention mechanisms, and cannot effectively utilize the complementarity of audio-visual patterns. Based on this, Praveen et al.  [92]  proposed an audio-visual emotion recognition model with joint cross-modal attention fusion to utilize the complementary relationships between modalities effectively. This model relies on a fusion mechanism based on cross-modal attention to encode inter-modal information while preserving intra-modal features. It calculates cross-modal attention weights based on the correlation between joint feature representation and individual modalities. Deploying the joint A-V feature representation into the cross-modal attention module significantly improves system performance by simultaneously utilizing intra-modal and inter-modal relationships. On the Affwild2 dataset, the consistency correlation coefficients for valence and arousal are 0.374 and 0.363, respectively, showing a significant improvement compared to the baseline of the third challenge in the 2022 Affective Behavior Analysis in-the-Wild competition.\n\nIn previous research on multimodal data fusion, tensor-based representations were predominantly used. When the input is transformed into a tensor, the dimensions and computational complexity grow exponentially. In light of this, Zhu et al.  [93]  proposed a multimodal fusion approach employing a low-rank weight tensor with an attention mechanism. Specifically, in the self-attention module, a novel output vector calculation method, namely, proportionally weighted self-attention, was utilized.\n\nBy obtaining an unimodal representation through an unimodal feature extraction network, this representation was fed into the fusion module. The self-attention mechanism was employed to generate an unimodal representation with new weights, which was then used to output the final classification result. Experimental studies indicate that this approach outperforms methods solely using low-rank tensor representations in multimodal fusion, enhancing both model efficiency and reducing computational complexity. Hybrid Fusion. Feature-level fusion enhances the richness of features, although it can improve recognition performance, it does not consider the differences between features. Some features cannot be integrated, resulting in the inability to model complex relationships, and high-dimensional feature sets are prone to data sparsity issues  [110] . Decision-level fusion, while easy to implement, assumes that the modalities involved in fusion are mutually independent and cannot capture the interrelation between different modalities  [111] . Therefore, both feature-level and decision-level fusion methods struggle to capture deeper cross-modal information and fail to fully exploit the correlations between different modalities, making breakthrough progress challenging. To address these issues, researchers have proposed various more effective multimodal fusion approaches. Based on the attention mechanism of Transformer  [112] , it can adaptively determine which information to extract from the data and generate a robust and effective fusion strategy. Transformer adaptively integrates useful information related to object queries, spatial and contextual relationships for feature fusion. Multihead attention generates multimodal emotional intermediate representations from a common semantic feature space after encoding multimodal information. Additionally, it can effectively learn long-term dependencies with a self-attention mechanism. The network architecture diagram of Transformer is shown in Fig.  17 .\n\nLian et al.  [20]  proposed a model-level multimodal fusion strategy based on Transformer. On the one hand, it captures the temporal dependencies between unimodal features through the Transformer-based unimodal structure. On the other hand, it learns cross-modal interactions on misaligned multimodal features through the Transformer-based cross-modal structure. It effectively models intra-modal and cross-modal interactions. Experiments on the AVEC 2017 database show that the Transformer-based model-level fusion outperforms other fusion strategies. Higher accuracy is achieved on the IEMOCAP and MELD datasets. Multimodal fusion enhances emotion recognition performance due to the complementary nature of different modalities. Compared to decision-level fusion and feature-level fusion, model-level fusion better utilizes the advantages of deep neural networks.\n\nJohn et al.  [97]  proposed adopting a new Transformer-based model to enhance the performance of audio-visual emotion recognition in videos. The model consists of three Transformer branches, referred to as multimodal Transformers. These three branches respectively perform audio self-attention, video self-attention, and audio-video crossmodal attention. The self-attention branches are used to identify the most relevant information in audio and video inputs, while the cross-modal attention branch is employed to recognize the most relevant audio-video interaction information. Through ablation experiments, the researchers verified that the performance benefits from the relevant information provided by these three branches. Additionally, a new time embedding scheme called block embedding was introduced, involving the incorporation of temporal information from multiple frames in the video into visual features.\n\nChumachenko et al.  [98]  proposed three modal fusion methods. The first is Late Transformer Fusion, where features learned from two branches are fused with a Transformer block. Specifically, two Transformers are used at the output of each branch, with one modality being fused into the other. The outputs of these Transformer blocks are further concatenated and passed to the final prediction layer. The second is Mid Transformer Fusion, which involves fusion at the mid-feature layer using Transformer blocks similar to the ones mentioned above. Specifically, after the first stage of feature extraction, i.e., after two convolutional layers, a Transformer block is used for fusion in each branch. The third is based on Attention Fusion, where they propose a fusion method solely based on dot-product similarity, forming the attention mechanism in the Transformer block. This method focuses on modality-independent features, a concept not previously introduced. Experimental validation on the MOSEI dataset shows that, among the three fusion methods, the Mid Attention Fusion method achieves the highest classification accuracy, reaching 68.76%. It is followed by the Mid Transformer Fusion method, with an accuracy of 66.57%.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Experimental Analysis",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Evaluation Criteria",
      "text": "Each dataset has its evaluation method. In the dimensional emotion, annotations are typically divided into the following four groups: High Arousal and High Valence (HAHV), High Arousal and Low Valence (HALV), Low Arousal and High Valence (LAHV), Low Arousal and Low Valence (LALV). Consistency Concordance Correlation (CCC) is a statistical metric used to measure the consistency among multiple observers when assessing the same measured values. It can comprehensively consider the accuracy and consistency of the measured values, thus playing a crucial role in evaluating the reliability of measurement data. The CCC ranges from -1 to 1, with values closer to 1 indicating higher consistency among observers. The calculation of this coefficient takes into account the mean deviation and the linear relationship of the data, providing a more comprehensive reflection of the degree of consistency among observers. The following is the corresponding calculation formula:\n\nIn the discrete dimension, annotations are usually categorized into different types of emotions, generally measuring the model's accuracy in analyzing different emotion labels. The most common evaluation criteria include accuracy (Acc.), recall, precision, and F1-score, as well as confusion matrices and ROC curves. Here are the corresponding calculation formulas:\n\nP recision = T P T P + F P (4)\n\nIn this formula, TP represents the number of samples that are actually true and predicted as true, TN represents the number of samples that are actually false and predicted as false, FP represents the number of samples that are actually false but predicted as true, and FN represents the number of samples that are actually true but predicted as false.",
      "page_start": 20,
      "page_end": 21
    },
    {
      "section_name": "Experimental Comparison",
      "text": "This section introduces recent advancements in both unimodal and multimodal video emotion recognition methods, providing a comparison in Table  4 . The table lists various video emotion recognition methods, the modalities utilized, datasets used for testing, and the experimental results obtained.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Future Directions",
      "text": "Through an extensive survey of published literature on video emotion recognition from 2015 to 2022, this study finds that research on emotion recognition in video scenes is gradually gaining momentum, attracting continuous attention from many researchers. Fig.  18  presents the proportional analysis of publications using different methods.\n\nFrom the figure, it can be observed that researchers currently favor attention-based multimodal fusion methods and are entering a period of rapid development. At the same time, the research focus is shifting gradually from unimodal to multimodal video emotion recognition. As evident from the preceding text, one of the driving factors behind this trend is that multimodal data typically complement each other, addressing their respective shortcomings  [81] , thereby improving the effectiveness of video emotion recognition and becoming a research hotspot. Furthermore, fusion methods based on attention mechanisms can dynamically generate weights, effectively addressing differences in modality quality caused by different samples in practice, thus possessing higher robustness and becoming a focus of current research in multimodal fusion. In summary, the trend in the field of video emotion recognition is moving towards multimodality, and finding superior multimodal fusion methods has become a key focus of current research.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Video Feature Extraction",
      "text": "Due to the emotional information conveyed by video content, accurately analyzing video content can significantly improve the performance of AVCA. In the model introduced in the Section 4, part of the performance improvement comes from a more advanced video feature extraction module. Although in AVCA, temporal features are more valuable than spatial features, how to balance the two for better performance remains unclear. The introduction of an attention mechanism partially addresses this issue, with a general emphasis on temporal features and a stronger focus on noteworthy spatial features. The advanced performance of models incorporating attention mechanisms confirms the importance of judiciously utilizing both temporal and spatial features. Additionally, compared to spatial feature extraction, there is still room for improvement in temporal feature extraction techniques, drawing inspiration from advanced spatial feature extraction methods. For instance, introducing an attention mechanism for temporal features allows the model to automatically focus on time intervals in the video with richer emotional information, achieving a balance of spatiotemporal features. Correspondingly, research on three-dimensional convolution models still lags behind two-dimensional convolution models. In current research on three-dimensional convolutions, modifications to two-dimensional techniques still dominate, and innovative three-dimensional convolution models are still lacking. Taking model input as an example, current video models still require complex preprocessing of videos to receive data, while image models can directly take images as input. Therefore, there is a need for more innovative models specifically designed for videos to propel the rapid development of video feature extraction.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Multimodal Feature Extraction",
      "text": "The expression of human emotions is diverse. In addition to speech intonation and facial expressions, the modalities of emotional expression also include gestures, physiological signals, and text. Therefore, emotional recognition is fundamentally a problem of extracting multimodal emotional features. Currently, mainstream research on multimodal emotion recognition is based on speech and facial modalities, but there is a lack of in-depth study on other modal features. Thus, the integrated utilization of more modal information is an important direction for future emotion recognition, and incorporating additional modalities helps to analyze a person's emotional state more accurately. Various modal features have complementary and different importance in expressing spatial information. They are mutually independent yet interrelated, so how to leverage the complementarity and correlation between multimodal features is a question that needs consideration. By adopting effective fusion methods, multimodal features can be integrated to enhance the accuracy and robustness of emotion recognition. Additionally, exploring methods that combine difficult-to-fake physiological signals and easily collectible non-physiological signals is an area for future development in multimodal video emotion recognition. Furthermore, for most existing video emotion recognition methods, the utilized number of frames is limited, and they cannot fully utilize the video information, making it challenging to capture emotional information regions within the video signal effectively. Therefore, one of the future development directions is to devise methods that can utilize long-term video information and effectively capture the emotional information contained therein.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Multimodal Feature Fusion",
      "text": "Currently, the focus of research on multimodal fusion methods lies in considering how to balance different modalities to utilize the fusion features of various modalities better.\n\nRegarding the advantages and disadvantages of early and late fusion, hybrid fusion achieves relatively high accuracy. However, due to the need for model structure and fusion rules designed for different tasks and data, it lacks generality, posing challenges for widespread application in AVCA. Therefore, there is a need for further research on the trade-off between model optimization and generality. Designing more reasonable and versatile fusion rules while ensuring that the model's performance does not degrade is crucial and is a significant direction for future development. Furthermore, existing late fusion methods often assume stability in the importance of different modalities for all samples, assigning equal or fixed weights to each modality. In practice, the importance of modalities often varies among different samples, and fusion rules should not overlook this issue. Thus, models should adaptively make decisions based on the input of multimodal features, designing appropriate fusion weights for each sample. Introducing attention mechanisms, where attention modules provide higher weights to important modalities, is one method to address this issue. Additionally, Han et al.  [21]  proposed a method using confidence for late fusion, adaptively integrating decisions for different modalities based on uncertainty estimation using evidence theory for each sample. Both approaches have shown significant performance improvements compared to existing methods, simultaneously enhancing the interpretability of fusion rules. Therefore, adaptive fusion rules will be a highly worthwhile direction for future multimodal research.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Trusted Recognition Results",
      "text": "The success of deep learning involves three factors: increased computing power, deep complex models, and sufficient data support. However, these factors all point out a significant drawback of deep learning, which is the lack of interpretability. This deficiency manifests in the interpretation of the learning process of deep learning models and the results of model recognition. This limitation necessitates careful consideration of decisions in practical applications, requiring not only the evaluation of the model's performance but also consideration of whether to use the model's results as the final decision. The evaluation of model results has become a crucial research direction in addressing this issue.\n\nThe study on credibility was first proposed by Guo et al.  [141] , who analyzed the correctness of multiple models in computer vision and natural language processing domains under different datasets. They pointed out an overall overconfidence tendency in existing deep learning models' confidence. Correction methods for the confidence of deep learning models have been widely studied, with main methods including Temperature Scaling, isotonic regression, Mix-n-Match, etc. Han et al.  [21]  combined modal confidence with multimodal decision fusion, using the confidence of each modal decision as fusion weights, achieving adaptive fusion. Introducing the concept of confidence further enhanced the model's performance and decision credibility, making fused decisions more interpretable. Credible research on deep learning decisions will be the most important research direction in the future of the deep learning field, enabling model decisions to be interpretable.",
      "page_start": 24,
      "page_end": 25
    },
    {
      "section_name": "Group Emotion Clustering",
      "text": "Identifying the dominant emotions in videos is too general, while predicting personalized emotions for each user is too specific. Since certain user groups or small communities share similar tastes or interests and have similar backgrounds, they are more likely to react similarly to the same video. Therefore, predicting emotions for these user groups or small communities would be more meaningful. Analyzing user profiles provided by each individual and categorizing users into different groups based on gender, background, taste, interests, etc., may provide a feasible solution.\n\nCurrently, research on group emotions mainly focuses on identifying the emotions conveyed by video creators. Emotional video analysis for creator groups has not been explored. Group emotion recognition plays a crucial role in areas such as public opinion analysis. For instance, for the same group of people, if one person expresses a positive opinion about a certain event, others are more likely to express positive opinions.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Novel And Real-World Avca-Based Applications",
      "text": "With the improvement of multimodal AVCA methods, the model's performance has significantly increased. Therefore, we anticipate the arrival of the era of emotional intelligence, where there will be more applications based on emotion recognition. For example, in voice assistants, having emotion recognition capabilities can provide users with services that better meet their needs. In online shopping recommendations, intelligent consumer services such as customer image interaction can offer a better experience for customers. In sentiment analysis, AVCA models can quickly model public opinions on a large number of videos based on emotional knowledge. The literature  [142]  has preliminarily implemented a video emotion recognition system that captures emotional features from input audio and facial image data, providing emotion results applicable in areas such as call centers, humanoid robots, and robotic pets. Amali et al.  [143]  proposed an emotion-related video recommendation system that identifies emotions from videos watched by users now and in the past, recommending a video queue that better aligns with the user's emotions. In the education field, teaching assistants with rich emotions can help users learn and understand better. Of course, there are more exciting applications on the horizon.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Large-Scale Avca Dataset",
      "text": "The essential foundation of deep learning lies in data. Existing datasets mostly consist of clips from movies or records derived in laboratories, lacking the complexity of the real world. In the application of models, it becomes challenging to handle the intricate factors of the real world. Constructing a dataset that reflects real-world conditions with well-annotated data is advantageous for advancing AVCA research. Publicly available multimodal emotion datasets are relatively scarce compared to unimodal datasets, and some modalities may be missing or damaged  [144] . Existing multimodal emotion databases are predominantly based on Western languages, overlooking regional and racial differences. Establishing a rich, multimodal, linguistically diverse, and openly accessible dataset in the field of multimodal AVCA will contribute to further research in AVCA, and creating a large-scale benchmark dataset is an important trend. With the development of internet technology, the widespread use of smartphones as mobile internet clients has led to the rapid growth of short videos. Most mobile internet users browse short videos due to their low production costs and strong entertainment appeal, catering to modern lifestyles. Establishing a large-scale short video dataset is necessary and crucial, as short videos increasingly dominate public opinion dissemination, leading to a growing demand for AVCA citations and driving the research forward. Please note that the translation is focused on maintaining the academic and technical context while improving clarity and readability.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Conclusion",
      "text": "This article aims to comprehensively investigate the research progress in affective video content analysis (AVCA) over the past decade. Obviously, it cannot cover all the literature on AVCA, and we focus on a representative subset of the latest methods. We summarize and compare widely used emotion representation models, available datasets, and representative work in video feature extraction, model design, and multimodal analysis. Finally, we discuss some open issues and potential research directions in the field of AVCA. Despite the significant progress achieved with multimodal deep learning-based AVCA methods, there is still a need for designing an effective, efficient, robust, trustworthy, and interpretable AVCA algorithm. With the profound understanding of emotional arousal in neuroscience, the establishment of emotional measurement in psychology, and the rapid development of novel deep learning network architectures in machine learning, we believe that AVCA will continue to be an active and promising research topic.",
      "page_start": 25,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The temporal nature plays an important role in AVCA. These images are captured from a",
      "page": 3
    },
    {
      "caption": "Figure 1: , the person in Fig. 1(a)",
      "page": 3
    },
    {
      "caption": "Figure 1: (d) expressing overwhelming happiness. Relying solely on individual frames,",
      "page": 3
    },
    {
      "caption": "Figure 2: , the same video may evoke",
      "page": 3
    },
    {
      "caption": "Figure 2: (a), if we only see the mother, we might infer fear based on her expression; however,",
      "page": 3
    },
    {
      "caption": "Figure 2: The contextual information also proves helpful for the AVCA task. (a) The image without",
      "page": 4
    },
    {
      "caption": "Figure 3: Illustration of the expression subjectivity. The same gestures or actions may elicit significantly",
      "page": 4
    },
    {
      "caption": "Figure 2: (b), upon seeing the disabled person, we might infer",
      "page": 4
    },
    {
      "caption": "Figure 3: (a) is commonly used to express affirmation and support in most countries; how-",
      "page": 4
    },
    {
      "caption": "Figure 4: The credibility of model classification results plays a vital role in multimodal feature fusion.",
      "page": 6
    },
    {
      "caption": "Figure 4: , the multimodal",
      "page": 7
    },
    {
      "caption": "Figure 4: (a), indicate that the model classified",
      "page": 7
    },
    {
      "caption": "Figure 4: (b). The outstanding performance",
      "page": 7
    },
    {
      "caption": "Figure 5: Milestones in both general affective computing (above line, blue) and affective video content",
      "page": 8
    },
    {
      "caption": "Figure 6: V-A emotional space and typical emotional subspaces (as cited in Plutchik et al., 1980)",
      "page": 10
    },
    {
      "caption": "Figure 6: , thereby providing a richer set. For example, the three intensities",
      "page": 10
    },
    {
      "caption": "Figure 7: Another method is the dimensional emotion space (DES). Observers don‚Äôt select",
      "page": 10
    },
    {
      "caption": "Figure 9: Different emotions can be distinguished in the",
      "page": 10
    },
    {
      "caption": "Figure 11: From the above datasets, it can be observed that the existing datasets are mainly",
      "page": 13
    },
    {
      "caption": "Figure 12: In the local network, they employed a novel approach, LEMHI, to aggregate frames",
      "page": 15
    },
    {
      "caption": "Figure 13: In AICA studies, basic emotions are insufficient to describe the complex and diverse",
      "page": 16
    },
    {
      "caption": "Figure 14: illustrates the",
      "page": 16
    },
    {
      "caption": "Figure 15: The method achieved good",
      "page": 17
    },
    {
      "caption": "Figure 16: Firstly, facial features and audio features",
      "page": 18
    },
    {
      "caption": "Figure 17: Lian et al. [20] proposed a model-level multimodal fusion strategy based on",
      "page": 19
    },
    {
      "caption": "Figure 18: presents the proportional analysis of publications using different methods.",
      "page": 21
    },
    {
      "caption": "Figure 8: The Circumflex Model of Russel (as cited",
      "page": 40
    },
    {
      "caption": "Figure 10: Label distribution of AFEW dataset",
      "page": 40
    },
    {
      "caption": "Figure 11: Distribution of emotion labels in the",
      "page": 40
    },
    {
      "caption": "Figure 12: Integration framework diagram (as cited in Hu M et al.,2019)",
      "page": 40
    },
    {
      "caption": "Figure 13: Schematic diagram of the ST-GCN model (as cited in Shen et al.,2019)",
      "page": 41
    },
    {
      "caption": "Figure 14: Flow chart of feature-level fusion and model-level fusion frameworks (as cited in Cai et",
      "page": 41
    },
    {
      "caption": "Figure 15: Decision rules for accepting any of the outcomes between audiovisual outcomes (as cited in",
      "page": 42
    },
    {
      "caption": "Figure 16: Structure diagram of the fusion model proposed by Liu D (as cited in Liu et al.,2022)",
      "page": 42
    },
    {
      "caption": "Figure 17: Transformer network structure diagram (as cited in Vaswani et al.,2017, slightly modified)",
      "page": 43
    },
    {
      "caption": "Figure 18: Percentage stacked histogram of various video emotion recognition methods",
      "page": 44
    }
  ],
  "tables": [
    {
      "caption": "Table 1: WhiletheCESiseasytounderstand,thelimitednumberofemotioncategoriesfails",
      "data": [
        {
          "(cid:36)\n(cid:14)(cid:20)\n(cid:40)\n(cid:97)(cid:20)\n(cid:40)\n(cid:97)(cid:21)\n(cid:55)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)\n(cid:40)(cid:91)(cid:70)(cid:76)(cid:87)(cid:76)(cid:81)(cid:74)\n(cid:40)\n(cid:97)(cid:22)\n(cid:41)(cid:72)(cid:68)(cid:85) (cid:97) (cid:40) (cid:23) (cid:57)\n(cid:16)(cid:20) (cid:49)(cid:72)(cid:88)(cid:87)(cid:85)(cid:68)(cid:79) (cid:40) (cid:97)(cid:24) (cid:14)(cid:20)\n(cid:40)\n(cid:97)(cid:25) (cid:53)(cid:72)(cid:79)(cid:68)(cid:91)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)\n(cid:54)(cid:68)(cid:71)\n(cid:54) (cid:16)(cid:20)": "(cid:36)\n(cid:14)(cid:20)\n(cid:40)\n(cid:97)(cid:20)\n(cid:40)\n(cid:97)(cid:21)\n(cid:55)(cid:72)(cid:81)(cid:86)(cid:76)(cid:82)(cid:81)\n(cid:40)(cid:91)(cid:70)(cid:76)(cid:87)(cid:76)(cid:81)(cid:74)\n(cid:40)\n(cid:97)(cid:22)\n(cid:41)(cid:72)(cid:68)(cid:85) (cid:97) (cid:40) (cid:23) (cid:57)\n(cid:16)(cid:20) (cid:49)(cid:72)(cid:88)(cid:87)(cid:85)(cid:68)(cid:79) (cid:40) (cid:97)(cid:24) (cid:14)(cid:20)\n(cid:40)\n(cid:97)(cid:25) (cid:53)(cid:72)(cid:79)(cid:68)(cid:91)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)\n(cid:54)(cid:68)(cid:71)\n(cid:54) (cid:16)(cid:20)"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "(cid:20)(cid:20)(cid:26)": "(cid:20)(cid:21)(cid:20)\n(cid:20)(cid:20)(cid:25)",
          "(cid:20)(cid:26)(cid:22)": "",
          "Column_5": ""
        }
      ],
      "page": 40
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Embodied Psychoacoustics: Spatial and Multisensory Determinants of Auditory-induced Emotion",
      "authors": [
        "A Tajadura-Jim√©nez"
      ],
      "year": "2008",
      "venue": "Embodied Psychoacoustics: Spatial and Multisensory Determinants of Auditory-induced Emotion"
    },
    {
      "citation_id": "2",
      "title": "Emoacoustics: A study of the psychoacoustical and psychological dimensions of emotional sound design",
      "authors": [
        "E Asutay",
        "D V√§stfj√§ll",
        "A Tajadura-Jimenez",
        "A Genell",
        "P Bergman",
        "M Kleiner"
      ],
      "year": "2012",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "3",
      "title": "Psychoacoustic abilities as predictors of vocal emotion recognition",
      "authors": [
        "E Globerson",
        "N Amir",
        "O Golan",
        "L Kishon-Rabin",
        "M Lavidor"
      ],
      "year": "2013",
      "venue": "Perception, & Psychophysics"
    },
    {
      "citation_id": "4",
      "title": "Emotion regulation: Affective, cognitive, and social consequences",
      "authors": [
        "J Gross"
      ],
      "year": "2002",
      "venue": "Emotion regulation: Affective, cognitive, and social consequences"
    },
    {
      "citation_id": "5",
      "title": "Social functions of emotion and emotion regulation",
      "authors": [
        "A Fischer",
        "A Manstead"
      ],
      "venue": "Social functions of emotion and emotion regulation"
    },
    {
      "citation_id": "6",
      "title": "Netvlad: Cnn architecture for weakly supervised place recognition",
      "authors": [
        "R Arandjelovic",
        "P Gronat",
        "A Torii",
        "T Pajdla",
        "J Sivic"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "7",
      "title": "Emotion classification of indonesian tweets using bidirectional lstm",
      "authors": [
        "A Glenn",
        "P Lacasse",
        "B Cox"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "8",
      "title": "Hsa-rnn: Hierarchical structure-adaptive rnn for video summarization",
      "authors": [
        "B Zhao",
        "X Li",
        "X Lu"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Long-term recurrent convolutional networks for visual recognition and description",
      "authors": [
        "J Donahue",
        "Anne Hendricks",
        "L Guadarrama",
        "S Rohrbach",
        "M Venugopalan",
        "S Saenko",
        "K Darrell"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "10",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Cross-cultural similarities and differences in emotion and its representation",
      "authors": [
        "P Shaver",
        "S Wu",
        "J Schwartz"
      ],
      "year": "1992",
      "venue": "Cross-cultural similarities and differences in emotion and its representation"
    },
    {
      "citation_id": "12",
      "title": "Social influences on the emotion process",
      "authors": [
        "A Fischer",
        "A Manstead",
        "R Zaalberg"
      ],
      "year": "2003",
      "venue": "European review of social psychology"
    },
    {
      "citation_id": "13",
      "title": "A psychoevolutionary theory of emotions",
      "authors": [
        "R Plutchik"
      ],
      "year": "1982",
      "venue": "A psychoevolutionary theory of emotions"
    },
    {
      "citation_id": "14",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "15",
      "title": "Emotional category data on images from the international affective picture system",
      "authors": [
        "J Mikels",
        "B Fredrickson",
        "G Larkin",
        "C Lindberg",
        "S Maglio",
        "P Reuter-Lorenz"
      ],
      "year": "2005",
      "venue": "Behavior research methods"
    },
    {
      "citation_id": "16",
      "title": "Three dimensions of emotion",
      "authors": [
        "H Schlosberg"
      ],
      "year": "1954",
      "venue": "Psychological review"
    },
    {
      "citation_id": "17",
      "title": "Fuzzy similarity-based emotional classification of color images",
      "authors": [
        "J Lee",
        "E Park"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Feature-level and model-level audiovisual fusion for emotion recognition in the wild",
      "authors": [
        "J Cai",
        "Z Meng",
        "A Khan",
        "Z Li",
        "J O'reilly",
        "S Han",
        "P Liu",
        "M Chen",
        "Y Tong"
      ],
      "year": "2019",
      "venue": "2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)"
    },
    {
      "citation_id": "19",
      "title": "Emotion recognition from audio-visual data using rule based decision level fusion",
      "authors": [
        "S Sahoo",
        "A Routray"
      ],
      "year": "2016",
      "venue": "IEEE Students' Technology Symposium (TechSym)"
    },
    {
      "citation_id": "20",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Trusted multi-view classification with dynamic evidential fusion",
      "authors": [
        "Z Han",
        "C Zhang",
        "H Fu",
        "J Zhou"
      ],
      "year": "2022",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "22",
      "title": "Speech analyzer for analyzing frequency perturbations in a speech pattern to determine the emotional state of a person",
      "authors": [
        "J Williamson"
      ],
      "year": "1979",
      "venue": "Google Patents. US Patent"
    },
    {
      "citation_id": "23",
      "title": "The generation of affect in synthesized speech",
      "authors": [
        "J Cahn"
      ],
      "year": "1990",
      "venue": "Journal of the American Voice I/O Society"
    },
    {
      "citation_id": "24",
      "title": "Recognition of six basic facial expression and their strength by neural network",
      "authors": [
        "H Kobayashi",
        "F Hara"
      ],
      "year": "1992",
      "venue": "Proceedings IEEE"
    },
    {
      "citation_id": "25",
      "title": "Emotional intelligence. Imagination, cognition and personality",
      "authors": [
        "P Salovey",
        "J Mayer"
      ],
      "year": "1990",
      "venue": "Emotional intelligence. Imagination, cognition and personality"
    },
    {
      "citation_id": "26",
      "title": "Affective Computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Emotional valence categorization using holistic image features",
      "authors": [
        "V Yanulevskaya",
        "J Gemert",
        "K Roth",
        "A.-K Herbold",
        "N Sebe",
        "J.-M Geusebroek"
      ],
      "year": "2008",
      "venue": "2008 15th IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "28",
      "title": "Affective image classification using features inspired by psychology and art theory",
      "authors": [
        "J Machajdik",
        "A Hanbury"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs",
      "authors": [
        "D Borth",
        "R Ji",
        "T Chen",
        "T Breuel",
        "S.-F Chang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Predicting personalized emotion perceptions of social images",
      "authors": [
        "S Zhao",
        "H Yao",
        "Y Gao",
        "R Ji",
        "W Xie",
        "X Jiang",
        "T.-S Chua"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "User interest and social influence based emotion prediction for individuals",
      "authors": [
        "Y Yang",
        "P Cui",
        "W Zhu",
        "S Yang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "A mixed bag of emotions: Model, predict, and transfer emotion distributions",
      "authors": [
        "K.-C Peng",
        "T Chen",
        "A Sadovnik",
        "A Gallagher"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Predicting discrete probability distribution of image emotions",
      "authors": [
        "S Zhao",
        "H Yao",
        "X Jiang",
        "X Sun"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "34",
      "title": "Emotiongan: Unsupervised domain adaptation for learning discrete probability distributions of image emotions",
      "authors": [
        "S Zhao",
        "X Zhao",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Cycleemotiongan: Emotional semantic consistency preserved cyclegan for adapting image emotions",
      "authors": [
        "S Zhao",
        "C Lin",
        "P Xu",
        "S Zhao",
        "Y Guo",
        "R Krishna",
        "G Ding",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Zero-shot emotion recognition via affective structural embedding",
      "authors": [
        "C Zhan",
        "D She",
        "S Zhao",
        "M.-M Cheng",
        "J Yang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition in the wild from videos using images",
      "authors": [
        "S Bargal",
        "E Barsoum",
        "C Ferrer",
        "C Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "38",
      "title": "Video facial emotion recognition based on local enhanced motion history image and cnn-ctslstm networks",
      "authors": [
        "M Hu",
        "H Wang",
        "X Wang",
        "J Yang",
        "R Wang"
      ],
      "year": "2019",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "39",
      "title": "Emotion recognition using feature-level fusion of facial expressions and body gestures",
      "authors": [
        "T Keshari",
        "S Palaniswamy"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Communication and Electronics Systems (ICCES)"
    },
    {
      "citation_id": "40",
      "title": "A multiple feature fusion framework for video emotion recognition in the wild",
      "authors": [
        "N Samadiani",
        "G Huang",
        "W Luo",
        "C.-H Chi",
        "Y Shu",
        "R Wang",
        "T Kocaturk"
      ],
      "year": "2022",
      "venue": "Concurrency and Computation: Practice and Experience"
    },
    {
      "citation_id": "41",
      "title": "Audiovisual emotion recognition in video clips",
      "authors": [
        "F Noroozi",
        "M Marjanovic",
        "A Njegus",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Audiovisual emotion recognition in wild. Machine Vision and Applications",
      "authors": [
        "E Avots",
        "T Sapi≈Ñski",
        "M Bachmann",
        "D Kami≈Ñska"
      ],
      "year": "2019",
      "venue": "Audiovisual emotion recognition in wild. Machine Vision and Applications"
    },
    {
      "citation_id": "43",
      "title": "Hi-net: hybrid-fusion network for multi-modal mr image synthesis",
      "authors": [
        "T Zhou",
        "H Fu",
        "G Chen",
        "J Shen",
        "L Shao"
      ],
      "year": "2020",
      "venue": "IEEE transactions on medical imaging"
    },
    {
      "citation_id": "44",
      "title": "Like it or not: A survey of twitter sentiment analysis methods",
      "authors": [
        "A Giachanou",
        "F Crestani"
      ],
      "year": "2016",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "45",
      "title": "Deep learning for sentiment analysis: A survey",
      "authors": [
        "L Zhang",
        "S Wang",
        "B Liu"
      ],
      "year": "2018",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery"
    },
    {
      "citation_id": "46",
      "title": "Automatic recognition of emotion evoked by general sound events",
      "authors": [
        "B Schuller",
        "S Hantke",
        "F Weninger",
        "W Han",
        "Z Zhang",
        "S Narayanan"
      ],
      "year": "2012",
      "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "'mister dj, cheer me up!': Musical and textual features for automatic mood classification",
      "authors": [
        "B Schuller",
        "C Hage",
        "D Schuller",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "Journal of New Music Research"
    },
    {
      "citation_id": "48",
      "title": "Machine recognition of music emotion: A review",
      "authors": [
        "Y.-H Yang",
        "H Chen"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Intelligent Systems and Technology (TIST)"
    },
    {
      "citation_id": "49",
      "title": "Affective image content analysis: Two decades review and new perspectives",
      "authors": [
        "S Zhao",
        "X Yao",
        "J Yang",
        "G Jia",
        "G Ding",
        "T.-S Chua",
        "B Schuller",
        "K Keutzer"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "50",
      "title": "Dynamics of facial expression: recognition of facial actions and their temporal segments from face profile image sequences",
      "authors": [
        "M Pantic",
        "I Patras"
      ],
      "year": "2006",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "51",
      "title": "Automatic analysis of facial affect: A survey of registration, representation, and recognition",
      "authors": [
        "E Sariyanidi",
        "H Gunes",
        "A Cavallaro"
      ],
      "year": "2014",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "52",
      "title": "Automatic detection of pain from facial expressions: a survey",
      "authors": [
        "T Hassan",
        "D Seu√ü",
        "J Wollenberg",
        "K Weitz",
        "M Kunz",
        "S Lautenbacher",
        "J.-U Garbas",
        "U Schmid"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "53",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Personalized emotion recognition by personality-aware high-order learning of physiological signals",
      "authors": [
        "S Zhao",
        "A Gholaminejad",
        "G Ding",
        "Y Gao",
        "J Han",
        "K Keutzer"
      ],
      "year": "2019",
      "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)"
    },
    {
      "citation_id": "55",
      "title": "The Society of Mind, Simon and Schuster",
      "authors": [
        "M Minsky"
      ],
      "year": "1988",
      "venue": "The Society of Mind, Simon and Schuster"
    },
    {
      "citation_id": "56",
      "title": "",
      "authors": [
        "W Parrott"
      ],
      "year": "2001",
      "venue": ""
    },
    {
      "citation_id": "57",
      "title": "The emotion probe: Studies of motivation and attention",
      "authors": [
        "P Lang"
      ],
      "year": "1995",
      "venue": "American psychologist"
    },
    {
      "citation_id": "58",
      "title": "A circumplex model of affect",
      "authors": [
        "J Russell"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "59",
      "title": "The dimensional analysis of a new series of facial expressions",
      "authors": [
        "T Engen",
        "N Levy",
        "H Schlosberg"
      ],
      "year": "1958",
      "venue": "Journal of Experimental Psychology"
    },
    {
      "citation_id": "60",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "61",
      "title": "Towards multimodal sentiment analysis: Harvesting opinions from the web",
      "authors": [
        "L.-P Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proc. 13th Int. Conf. Multimodal Interfaces(ICMI)"
    },
    {
      "citation_id": "62",
      "title": "Why we watch the news: a dataset for exploring sentiment in broadcast video news",
      "authors": [
        "J Ellis",
        "B Jou",
        "S.-F Chang"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "63",
      "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "authors": [
        "M W√∂llmer",
        "F Weninger",
        "T Knaup",
        "B Schuller",
        "C Sun",
        "K Sagae",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "64",
      "title": "Utterance-level multimodal sentiment analysis",
      "authors": [
        "V P√©rez-Rosas",
        "R Mihalcea",
        "L.-P Morency"
      ],
      "year": "2013",
      "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "65",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "66",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "67",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "68",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "69",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd International Conference on Data Engineering Workshops (ICDEW'06)"
    },
    {
      "citation_id": "70",
      "title": "Liris-accede: A video database for affective content analysis",
      "authors": [
        "Y Baveye",
        "E Dellandrea",
        "C Chamaret",
        "L Chen"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "71",
      "title": "Predicting emotions in user-generated videos",
      "authors": [
        "Y.-G Jiang",
        "B Xu",
        "X Xue"
      ],
      "year": "2014",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "72",
      "title": "Heterogeneous knowledge transfer in video emotion recognition, attribution and summarization",
      "authors": [
        "B Xu",
        "Y Fu",
        "Y.-G Jiang",
        "B Li",
        "L Sigal"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "73",
      "title": "Sewa db: A rich database for audio-visual emotion and sentiment research in the wild",
      "authors": [
        "J Kossaifi",
        "R Walecki",
        "Y Panagakis",
        "J Shen",
        "M Schmitt",
        "F Ringeval",
        "J Han",
        "V Pandit",
        "A Toisoul",
        "B Schuller"
      ],
      "year": "2019",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "74",
      "title": "Induced disgust, happiness and surprise: an addition to the mmi facial expression database",
      "authors": [
        "M Valstar",
        "M Pantic"
      ],
      "year": "2010",
      "venue": "Proc. 3rd Intern. Workshop on EMOTION (satellite of LREC"
    },
    {
      "citation_id": "75",
      "title": "Facial expression recognition from world wild web",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Salvador",
        "H Abdollahi",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "76",
      "title": "Video-based emotion recognition using cnnrnn and c3d hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "77",
      "title": "Coarse-to-fine cascaded networks with smooth predicting for video facial expression recognition",
      "authors": [
        "F Xue",
        "Z Tan",
        "Y Zhu",
        "Z Ma",
        "G Guo"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "78",
      "title": "Classifying emotions and engagement in online learning based on a single facial expression recognition neural network",
      "authors": [
        "A Savchenko",
        "L Savchenko",
        "I Makarov"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "79",
      "title": "Multi-branch deep radial basis function networks for facial emotion recognition",
      "authors": [
        "F Hernandez-Luquin",
        "H Escalante"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "80",
      "title": "Recognizing emotions from videos by studying facial expressions, body postures and hand gestures",
      "authors": [
        "M Gavrilescu"
      ],
      "year": "2015",
      "venue": "2015 23rd Telecommunications Forum Telfor (TELFOR)"
    },
    {
      "citation_id": "81",
      "title": "Emotion recognition based on multiview body gestures",
      "authors": [
        "Z Shen",
        "J Cheng",
        "X Hu",
        "Q Dong"
      ],
      "year": "2019",
      "venue": "Ieee International Conference on Image Processing"
    },
    {
      "citation_id": "82",
      "title": "Generalized zero-shot emotion recognition from body gestures",
      "authors": [
        "J Wu",
        "Y Zhang",
        "S Sun",
        "Q Li",
        "X Zhao"
      ],
      "year": "2022",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "83",
      "title": "Deep learning approach for emotion recognition from human body movements with feedforward deep convolution neural networks",
      "authors": [
        "R Santhoshkumar",
        "M Geetha"
      ],
      "year": "2019",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "84",
      "title": "imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis",
      "authors": [
        "X Liu",
        "H Shi",
        "H Chen",
        "Z Yu",
        "X Li",
        "G Zhao"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "85",
      "title": "Deep spatiotemporal feature fusion with compact bilinear pooling for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "D Dean",
        "C Fookes"
      ],
      "year": "2018",
      "venue": "Computer vision and image understanding"
    },
    {
      "citation_id": "86",
      "title": "Emotion in context: Deep semantic feature fusion for video emotion recognition",
      "authors": [
        "C Chen",
        "Z Wu",
        "Y.-G Jiang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "87",
      "title": "Multimodal fusion based on information gain for emotion recognition in the wild",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "E Hortal",
        "S Asteriadis"
      ],
      "year": "2017",
      "venue": "2017 Intelligent Systems Conference (IntelliSys)"
    },
    {
      "citation_id": "88",
      "title": "Dvc-net: a new dual-view context-aware network for emotion recognition in the wild",
      "authors": [
        "L Qing",
        "H Wen",
        "H Chen",
        "R Jin",
        "Y Cheng",
        "Y Peng"
      ],
      "year": "2023",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "89",
      "title": "Isla: Temporal segmentation and labeling for audiovisual emotion recognition",
      "authors": [
        "Y Kim",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on affective computing"
    },
    {
      "citation_id": "90",
      "title": "Multimodal interaction enhanced representation learning for video emotion recognition",
      "authors": [
        "X Xia",
        "Y Zhao",
        "D Jiang"
      ],
      "year": "2022",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "91",
      "title": "A multi-modal emotion fusion classification method combined expression and speech based on attention mechanism",
      "authors": [
        "D Liu",
        "L Chen",
        "L Wang",
        "Z Wang"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "92",
      "title": "A joint crossattention model for audio-visual fusion in dimensional emotion recognition",
      "authors": [
        "R Praveen",
        "W Melo",
        "N Ullah",
        "H Aslam",
        "O Zeeshan",
        "T Denorme",
        "M Pedersoli",
        "A Koerich",
        "S Bacon",
        "P Cardinal"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "93",
      "title": "Multimodal fusion method based on self-attention mechanism",
      "authors": [
        "H Zhu",
        "Z Wang",
        "Y Shi",
        "Y Hua",
        "G Xu",
        "L Deng"
      ],
      "year": "2020",
      "venue": "Wireless Communications and Mobile Computing"
    },
    {
      "citation_id": "94",
      "title": "Global-local attention for emotion recognition",
      "authors": [
        "N Le",
        "K Nguyen",
        "A Nguyen",
        "B Le"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "95",
      "title": "Deep multimodal fusion: A hybrid approach",
      "authors": [
        "M Amer",
        "T Shields",
        "B Siddiquie",
        "A Tamrakar",
        "A Divakaran",
        "S Chai"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "96",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "97",
      "title": "Audio and video-based emotion recognition using multimodal transformers",
      "authors": [
        "V John",
        "Y Kawanishi"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "98",
      "title": "Self-attention fusion for audiovisual emotion recognition with incomplete data",
      "authors": [
        "K Chumachenko",
        "A Iosifidis",
        "M Gabbouj"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "99",
      "title": "An Approach to Environmental Psychology",
      "authors": [
        "A Mehrabian",
        "J Russell"
      ],
      "year": "1974",
      "venue": "An Approach to Environmental Psychology"
    },
    {
      "citation_id": "100",
      "title": "Supervised transformer network for efficient face detection",
      "authors": [
        "D Chen",
        "G Hua",
        "F Wen",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Computer Vision-ECCV 2016: 14th European Conference"
    },
    {
      "citation_id": "101",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "102",
      "title": "Arbee: Towards automated recognition of bodily expression of emotion in the wild",
      "authors": [
        "Y Luo",
        "J Ye",
        "R Adams",
        "J Li",
        "M Newman",
        "J Wang"
      ],
      "year": "2020",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "103",
      "title": "The perception of emotion in body expressions",
      "authors": [
        "B De Gelder",
        "A Borst",
        "R Watson"
      ],
      "year": "2015",
      "venue": "Wiley Interdisciplinary Reviews: Cognitive Science"
    },
    {
      "citation_id": "104",
      "title": "Multimodal affective state recognition in serious games applications",
      "authors": [
        "A Psaltis",
        "K Kaza",
        "K Stefanidis",
        "S Thermos",
        "K Apostolakis",
        "K Dimitropoulos",
        "P Daras"
      ],
      "year": "2016",
      "venue": "Ieee International Conference on Imaging Systems and Techniques (ist)"
    },
    {
      "citation_id": "105",
      "title": "Feature analysis for computational personality recognition using youtube personality data set",
      "authors": [
        "C Sarkar",
        "S Bhatia",
        "A Agarwal",
        "J Li"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 ACM Multi Media on Workshop on Computational Personality Recognition"
    },
    {
      "citation_id": "106",
      "title": "Toward detecting emotions in spoken dialogs",
      "authors": [
        "C Lee",
        "S Narayanan"
      ],
      "year": "2005",
      "venue": "IEEE transactions on speech and audio processing"
    },
    {
      "citation_id": "107",
      "title": "Rml: A generic language for integrated rdf mappings of heterogeneous data",
      "authors": [
        "A Dimou",
        "M Vander Sande",
        "P Colpaert",
        "R Verborgh",
        "E Mannens",
        "R Walle"
      ],
      "year": "2014",
      "venue": "Ldow"
    },
    {
      "citation_id": "108",
      "title": "Multimodal emotion recognition based on peak frame selection from video",
      "authors": [
        "S Zhalehpour",
        "Z Akhtar",
        "C Eroglu Erdem"
      ],
      "year": "2016",
      "venue": "Signal, Image and Video Processing"
    },
    {
      "citation_id": "109",
      "title": "Kernel cross-modal factor analysis for information fusion with application to bimodal emotion recognition",
      "authors": [
        "Y Wang",
        "L Guan",
        "A Venetsanopoulos"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "110",
      "title": "Survey on audiovisual emotion recognition: databases, features, and data fusion strategies",
      "authors": [
        "C.-H Wu",
        "J.-C Lin",
        "W.-L Wei"
      ],
      "year": "2014",
      "venue": "APSIPA transactions on signal and information processing"
    },
    {
      "citation_id": "111",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao",
        "Q Tian"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "112",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "113",
      "title": "Emonets: Multimodal deep learning approaches for emotion recognition in video",
      "authors": [
        "S Kahou",
        "X Bouthillier",
        "P Lamblin",
        "C Gulcehre",
        "V Michalski",
        "K Konda",
        "S Jean",
        "P Froumenty",
        "Y Dauphin",
        "N Boulanger-Lewandowski"
      ],
      "year": "2016",
      "venue": "Journal on Multimodal User Interfaces"
    },
    {
      "citation_id": "114",
      "title": "Long short term memory recurrent neural network based multimodal dimensional emotion recognition",
      "authors": [
        "L Chao",
        "J Tao",
        "M Yang",
        "Y Li",
        "Z Wen"
      ],
      "year": "2015",
      "venue": "Emotion Challenge"
    },
    {
      "citation_id": "115",
      "title": "Av+ ec 2015: The first affect recognition challenge bridging across audio, video, and physiological data",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "S Jaiswal",
        "E Marchi",
        "D Lalanne",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2015",
      "venue": "Proceedings of the 5th International Workshop on Audio/visual Emotion Challenge"
    },
    {
      "citation_id": "116",
      "title": "Multi-attention fusion network for videobased emotion recognition",
      "authors": [
        "Y Wang",
        "J Wu",
        "K Hoashi"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "117",
      "title": "Spatio-temporal encoder-decoder fully convolutional network for video-based dimensional emotion recognition",
      "authors": [
        "Z Du",
        "S Wu",
        "D Huang",
        "W Li",
        "Y Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "118",
      "title": "Introducing the recola multimodal corpus of remote collaborative and affective interactions",
      "authors": [
        "F Ringeval",
        "A Sonderegger",
        "J Sauer",
        "D Lalanne"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)"
    },
    {
      "citation_id": "119",
      "title": "Emotion recognition by body movement representation on the manifold of symmetric positive definite matrices",
      "authors": [
        "M Daoudi",
        "S Berretti",
        "P Pala",
        "Y Delevoye",
        "A Del Bimbo"
      ],
      "year": "2017",
      "venue": "Image Analysis and Processing-ICIAP 2017: 19th International Conference"
    },
    {
      "citation_id": "120",
      "title": "The combined role of motionrelated cues and upper body posture for the expression of emotions during human walking. Modeling, simulation and optimization of bipedal walking",
      "authors": [
        "H Hicheur",
        "H Kadone",
        "J Gr√®zes",
        "A Berthoz"
      ],
      "year": "2013",
      "venue": "The combined role of motionrelated cues and upper body posture for the expression of emotions during human walking. Modeling, simulation and optimization of bipedal walking"
    },
    {
      "citation_id": "121",
      "title": "Multimodal (audio, facial and gesture) based emotion recognition challenge",
      "authors": [
        "G Wei",
        "L Jian",
        "S Mo"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)"
    },
    {
      "citation_id": "122",
      "title": "Emotion and gesture guided action recognition in videos using supervised deep networks",
      "authors": [
        "N Nigam",
        "T Dutta"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "123",
      "title": "The kinetics human action video dataset",
      "authors": [
        "W Kay",
        "J Carreira",
        "K Simonyan",
        "B Zhang",
        "C Hillier",
        "S Vijayanarasimhan",
        "F Viola",
        "T Green",
        "T Back",
        "P Natsev"
      ],
      "year": "2017",
      "venue": "The kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "124",
      "title": "An end-to-end visual-audio attention network for emotion recognition in user-generated videos",
      "authors": [
        "S Zhao",
        "Y Ma",
        "Y Gu",
        "J Yang",
        "T Xing",
        "P Xu",
        "R Hu",
        "H Chai",
        "K Keutzer"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "125",
      "title": "Emotion recognition with multimodal features and temporal models",
      "authors": [
        "S Wang",
        "W Wang",
        "J Zhao",
        "S Chen",
        "Q Jin",
        "S Zhang",
        "Y Qin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "126",
      "title": "Deep learning based data fusion methods for multimodal emotion recognition",
      "authors": [
        "J Njoku",
        "A Caliwag",
        "W Lim",
        "S Kim",
        "H Hwang",
        "J Jung"
      ],
      "year": "2022",
      "venue": "The Journal of Korean Institute of Communications and Information Sciences"
    },
    {
      "citation_id": "127",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "128",
      "title": "Multimodal emotion recognition and sentiment analysis via attention enhanced recurrent model",
      "authors": [
        "L Sun",
        "M Xu",
        "Z Lian",
        "B Liu",
        "J Tao",
        "M Wang",
        "Y Cheng"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge"
    },
    {
      "citation_id": "129",
      "title": "The multimodal sentiment analysis in car reviews (muse-car) Collection, insights and improvements",
      "authors": [
        "L Stappen",
        "A Baird",
        "L Schumann",
        "S Bjorn"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "130",
      "title": "Multimodal emotion recognition with modality-pairwise unsupervised contrastive loss",
      "authors": [
        "R Franceschini",
        "E Fini",
        "C Beyan",
        "A Conti",
        "F Arrigoni",
        "E Ricci"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "131",
      "title": "Audio-video fusion with double attention for multimodal emotion recognition",
      "authors": [
        "B Mocanu",
        "R Tapu"
      ],
      "year": "2022",
      "venue": "2022 IEEE 14th Image, Video, and Multidimensional Signal Processing Workshop (IVMSP)"
    },
    {
      "citation_id": "132",
      "title": "Interactive multimodal attention network for emotion recognition in conversation",
      "authors": [
        "M Ren",
        "X Huang",
        "X Shi",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "133",
      "title": "Deep fusion: An attention guided factorized bilinear pooling for audio-video emotion recognition",
      "authors": [
        "Y Zhang",
        "Z.-R Wang",
        "J Du"
      ],
      "year": "2019",
      "venue": "2019 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "134",
      "title": "Feature extraction network with attention mechanism for data enhancement and recombination fusion for multimodal sentiment analysis",
      "authors": [
        "Q Qi",
        "L Lin",
        "R Zhang"
      ],
      "year": "2021",
      "venue": "Information"
    },
    {
      "citation_id": "135",
      "title": "Transformer-based interactive multi-modal attention network for video sentiment detection",
      "authors": [
        "X Zhuang",
        "F Liu",
        "J Hou",
        "J Hao",
        "X Cai"
      ],
      "year": "2022",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "136",
      "title": "Er-mrl: Emotion recognition based on multimodal representation learning",
      "authors": [
        "X Guo",
        "Y Wang",
        "Z Miao",
        "X Yang",
        "J Guo",
        "X Hou",
        "F Zao"
      ],
      "year": "2022",
      "venue": "2022 12th International Conference on Information Science and Technology (ICIST)"
    },
    {
      "citation_id": "137",
      "title": "Transformer-based multimodal information fusion for facial expression analysis",
      "authors": [
        "W Zhang",
        "F Qiu",
        "S Wang",
        "H Zeng",
        "Z Zhang",
        "R An",
        "B Ma",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "138",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "F Lv",
        "X Chen",
        "Y Huang",
        "L Duan",
        "G Lin"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "139",
      "title": "Mmtrans-mt: A framework for multimodal emotion recognition using multitask learning",
      "authors": [
        "J Shen",
        "J Zheng",
        "X Wang"
      ],
      "year": "2021",
      "venue": "2021 13th International Conference on Advanced Computational Intelligence (ICACI)"
    },
    {
      "citation_id": "140",
      "title": "Facial emotion recognition with inter-modality-attention-transformer-based self-supervised learning",
      "authors": [
        "A Chaudhari",
        "C Bhatt",
        "A Krishna",
        "C Travieso-Gonz√°lez"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "141",
      "title": "On calibration of modern neural networks",
      "authors": [
        "C Guo",
        "G Pleiss",
        "Y Sun",
        "K Weinberger"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "142",
      "title": "A multimodal emotion recognition system from video",
      "authors": [
        "S Thushara",
        "S Veni"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Circuit, Power and Computing Technologies (ICCPCT)"
    },
    {
      "citation_id": "143",
      "title": "Semantic video recommendation system based on video viewers impression from emotion detection",
      "authors": [
        "D Amali",
        "A Barakbah",
        "A Besari",
        "D Agata"
      ],
      "year": "2018",
      "venue": "2018 International Electronics Symposium on Knowledge Creation and Intelligent Computing (ies-kcic)"
    },
    {
      "citation_id": "144",
      "title": "Building a three-level multimodal emotion recognition framework",
      "authors": [
        "J Garcia-Garcia",
        "M Lozano",
        "V Penichet",
        "E Law"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    }
  ]
}