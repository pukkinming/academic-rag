{
  "paper_id": "2111.12082v2",
  "title": "Physformer: Facial Video-Based Physiological Measurement With Temporal Difference Transformer",
  "published": "2021-11-23T18:57:11Z",
  "authors": [
    "Zitong Yu",
    "Yuming Shen",
    "Jingang Shi",
    "Hengshuang Zhao",
    "Philip Torr",
    "Guoying Zhao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Remote photoplethysmography (rPPG), which aims at measuring heart activities and physiological signals from facial video without any contact, has great potential in many applications. Recent deep learning approaches focus on mining subtle rPPG clues using convolutional neural networks with limited spatio-temporal receptive fields, which neglect the long-range spatio-temporal perception and interaction for rPPG modeling. In this paper, we propose the PhysFormer, an end-to-end video transformer based architecture, to adaptively aggregate both local and global spatio-temporal features for rPPG representation enhancement. As key modules in PhysFormer, the temporal difference transformers first enhance the quasi-periodic rPPG features with temporal difference guided global attention, and then refine the local spatio-temporal representation against interference. Furthermore, we also propose the label distribution learning and a curriculum learning inspired dynamic constraint in frequency domain, which provide elaborate supervisions for PhysFormer and alleviate overfitting. Comprehensive experiments are performed on four benchmark datasets to show our superior performance on both intra-and cross-dataset testings. One highlight is that, unlike most transformer networks needed pretraining from large-scale datasets, the proposed PhysFormer can be easily trained from scratch on rPPG datasets, which makes it promising as a novel transformer baseline for the rPPG community. The codes are available at https://github.com/ZitongYu/PhysFormer.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Physiological signals such as heart rate (HR), respiration frequency (RF), and heart rate variability (HRV) are important vital signs to be measured in many circumstances, especially for healthcare or medical purposes. Traditionally, the Electrocardiography (ECG) and Photoplethysmograph (PPG) are the two most common ways for measuring heart * Corresponding author activities and corresponding physiological signals. However, both ECG and PPG sensors need to be attached to body parts, which may cause discomfort and are inconvenient for long-term monitoring. To counter for this issue, remote photoplethysmography (rPPG)  [12, 36, 66]  methods are developing fast in recent years, which aim to measure heart activity remotely without any contact.\n\nIn earlier studies of facial rPPG measurement, most methods analyze subtle color changes on facial regions of interest (ROI) with classical signal processing approaches  [30, 49, 50, 55, 57] . Besides, there are a few color subspace transformation methods  [13, 59]  which utilize all skin pixels for rPPG measurement. Based on the prior knowledge from traditional methods, a few learning based approaches  [25, 44, 45, 51]  are designed as non-endto-end fashions. ROI based preprocessed signal representations (e.g., time-frequency map  [25]  and spatio-temporal map  [44, 45] ) are generated first, and then learnable models could capture rPPG features from these maps. However, these methods need the strict preprocessing procedure and neglect the global contextual clues outside the pre-defined ROIs. Meanwhile, more and more end-to-end deep learning based rPPG methods  [11, 34, 53, 65, 67]  are developed, which treat facial video frames as input and predict rPPG and other physiological signals directly. However, pure endto-end methods are easily influenced by the complex scenarios (e.g., with head movement and various illumination conditions) and rPPG-unrelated features can not be ruled out in learning, resulting in large performance decrease  [63]  in realistic datasets (e.g., VIPL-HR  [45] ).\n\nRecently, due to its excellent long-range attentional modeling capacities in solving sequence-to-sequence issues, transformer  [22, 32]  has been successfully applied in many artificial intelligence tasks such as natural language processing (NLP)  [56] , image  [15]  and video  [3]  analysis. Similarly, rPPG measurement from facial videos can be treated as a video sequence to signal sequence problem, where the long-range contextual clues should be exploited for semantic modeling. As shown in Fig.  1 , rPPG clues from different skin regions and temporal locations (e.g., signal trajectories around t1, t2, and t3) share similar properties (e.g., trends with rising edge first then falling edge later and relative high magnitudes), which can be utilized for long-range feature modeling and enhancement. However, different from the most video tasks aiming at huge motion representation, facial rPPG measurement focuses on capturing subtle skin color changes, which makes it challenging for global spatiotemporal perception. Furthermore, video-based rPPG measurement is usually a long-time monitoring task, and it is challenging to design and train transformers with long video sequence inputs.\n\nMotivated by the discussions above, we propose an endto-end video transformer architecture, namely PhysFormer, for remote physiological measurement. On one hand, the cascaded temporal difference transfomer blocks in Phys-Former benefit the rPPG feature enhancement via global spatio-temporal attention based on the fine-grained temporal skin color differences. On the other hand, to alleviate the interference-induced overfitting issue and complement the weak temporal supervision signals, elaborate supervision in frequency domain is designed, which helps Phys-Former learn more intrinsic rPPG-aware features.\n\nThe contributions of this work are as follows:\n\n• We propose the PhysFormer, which mainly consists of a powerful video temporal difference transformer backbone. To our best knowledge, it is the first time to explore the long-range spatio-temporal relationship for reliable rPPG measurement.\n\n• We propose an elaborate recipe to supervise Phys-Former with label distribution learning and curriculum learning guided dynamic loss in frequency domain to learn efficiently and alleviate overfitting.\n\n• We conduct intra-and cross-dataset testings and show that the proposed PhysFormer achieves superior or on par state-of-the-art performance without pretraining on large-scale datasets like ImageNet-21K.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Remote physiological measurement. An early study of rPPG-based physiological measurement was reported in  [57] . Plenty of traditional hand-crafted approaches have been developed on this field since then. Selective merging information from different color channels  [30, 49, 50]  or different ROIs  [27, 30]  are proven to be efficient for subtle rPPG signal recovery. To improve the signal-to-noiseratio of the recovered rPPG signals, several signal decomposition methods such as independent component analysis (ICA)  [27, 49, 50]  and matrix completion  [55]  are also proposed. In recent years, deep learning based approaches dominate the field of rPPG measurement due to the strong spatio-temporal representation capabilities. On one hand, facial ROI based spatial-temporal signal maps  [40, 41, 44, 46, 47]  are developed, which alleviate the interference from non-skin regions. Based on these signal maps, 2D-CNNs are utilized for rPPG feature extraction. On the other hand, end-to-end spatial networks  [11, 53]  and spatio-temporal models  [20, 34, 35, 48, 63, 65, 67]  are developed, which could recover rPPG signals from the facial video directly. However, previous methods only consider the spatio-temporal rPPG features from adjacent frames and neglect the longrange relationship among quasi-periodic rPPG features.\n\nTransformer for vision tasks. Transformer  [32]  is proposed in  [56]  to model sequential data in the field of NLP. Then vision transformer (ViT)  [15]  is proposed recently by feeding transformer with sequences of image patches for image classification. Many other ViT variants  [8, 14, 22, 23, 26, 38, 54, 60, 70]  are proposed from then, which achieve promising performance compared with its counterpart CNNs for image analysis tasks  [6, 24, 74] . Recently, some works introduce vision transformer for video understanding tasks such as action recognition  [1, 3, 4, 16, 21, 39, 42] , action detection  [37, 58, 62, 73] , video superresolution  [5] , video inpainting  [33, 71] , and 3D animation  [9, 10] . Some works  [21, 42]  conduct temporal contextual modeling with transformer based on single-frame features from pretrained 2D networks, while other works  [1, 3, 4, 16, 39]  mine the spatio-temporal attentions via video transformer directly. Most of these works are incompatible for long-video-sequence (>150 frames) signal regression task. There are two related works  [35, 64]  using ViT for rPPG feature representation. TransRPPG  [64]  extracts rPPG features from the preprocessed signal maps via ViT for face 3D mask presentation attack detection  [68] . Based on the temporal shift networks  [31, 34] , EfficientPhys-T  [35]  adds several swin transformer  [38]  layers for global spatial attention. Different from these two works, the proposed PhysFormer is an end-to-end video transformer, which is able to capture long-range spatio-temporal attentional rPPG features from facial video directly.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "We will first introduce the architecture of PhysFormer in Sec. 3.1, then introduce label distribution learning for rPPG measurement in Sec. 3.2, and at last present the curriculum learning guided dynamic supervision in Sec. 3.3.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Physformer",
      "text": "As illustrated in Fig.  2 , PhysFormer consists of a shallow stem E stem , a tube tokenizer E tube , N temporal difference transformer blocks E i trans (i = 1, ..., N ) and a rPPG predictor head. Inspired by the study in  [61] , we adopt a shallow stem to extract coarse local spatio-temporal features, which benefits the fast convergence and clearer subsequent global self-attention. Specifically, the stem is formed by three convolutional blocks with kernel size (1x5x5), (3x3x3) and (3x3x3), respectively. Each convolution operator is cascaded with a batch normalization (BN), ReLU and MaxPool. The pooling layer only halves the spatial dimension. Therefore, given an RGB facial video input X ∈ R 3×T ×H×W , the stem output X stem = E stem (X), where X stem ∈ R D×T ×H/8×W/8 , and D, T , W , H indicate channel, sequence length, width, height, respectively. Then X stem would be partitioned into spatio-temporal tube tokens\n\nvia the tube tokenizer E tube . Subsequently, the tube tokens will be forwarded with N temporal difference transformer blocks and obtain the global-local refined rPPG features X trans , which has the same dimensions with X tube . Finally, the rPPG predictor head temporally upsamples, spatially averages, and projects the features X trans to 1D signal Y ∈ R T . Tube tokenization. Here the coarse feature X stem would be partitioned into non-overlapping tube tokens via E tube (X stem ), which aggregates the spatio-temporal neighbor semantics and reduces computational costs for the subsequent transformers. Specifically, with the targeted tube size T s × H s × W s (the same as the partition step size in non-overlapping setting), the tube token map X tube ∈ R D×T ′ ×H ′ ×W ′ has length, height and width\n\nPlease note that there are no position embeddings after the tube tokenization as the stem at early stage already captures relative spatio-temporal positions.\n\nTemporal difference multi-head self-attention. In selfattention mechanism  [15, 56] , the relationship between the tokens is modeled by the similarity between the projected query-key pairs, yielding the attention score. Instead of point-wise linear projection, we utilize temporal difference convolution (TDC)  [63, 69]  for query (Q) and key (K) projection, which could capture fine-grained local temporal difference features for subtle color change description. TDC with learnable w can be formulated as\n\nwhere p 0 , R and R ′ indicate the current spatio-temporal location, sampled local (3x3x3) neighborhood and sampled adjacent neighborhood, respectively. Then query and key are projected as\n\nFor the value (V ) projection, point-wise linear projection without BN is utilized. Then Q, K, V ∈ R D×T ′ ×H ′ ×W ′ are flattened into sequence, and separated into h heads (D h = D/h for each head). For the i-th head (i ≤ h), the self-attention (SA) can be formulated\n\nwhere τ controls the sparsity. We find that the default setting τ = √ D h in  [15, 56]  performs poorly for rPPG measurement. According to the periodicity of rPPG features, we use smaller τ value to obtain sparser attention activation. The corresponding study can be found in Table  6 . The output of TD-MHSA is the concatenation of SA from all heads and then with a linear projection U ∈ R D×D TD-MHSA = Concat(SA 1 ; SA 2 ; ...; SA h )U.\n\n(\n\nAs illustrated in Fig.  2 , residual connection and layer normalization (LN) would be conducted after TD-MHSA.\n\nSpatio-temporal feed-forward. The vanilla feed-forward network consists of two linear transformation layers, where the hidden dimension D ′ between two layers is expanded to learn a richer feature representation. In contrast, we introduce a depthwise 3D convolution (with BN and nonlinear activation) between these two layers with extra slight computational cost but remarkable performance improvement.\n\nThe benefits are two-fold: 1) as a complementation of TD-MHSA, ST-FF could refine the local inconsistency and parts of noisy features; 2) richer locality provides TD-MHSA sufficient relative position cues.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Label Distribution Learning",
      "text": "Similar to the facial age estimation task  [18, 19 ] that faces at close ages look quite similar, facial rPPG signals with close HR values usually have similar periodicity. Inspired by this observation, instead of considering each facial video as an instance with one label (HR), we regard each facial video as an instance associated with a label distribution. The label distribution covers a certain number of class labels, representing the degree that each label describes the instance. Through this way, one facial video can contribute to both targeted HR value and its adjacent HRs.\n\nTo consider the similarity information among HR classes during the training stage, we model the rPPG-based HR estimation problem as a specific L-class multi-label classification problem, where L=139 in our case (each integer HR value within  [42, 180]  bpm as a class). A label distribution p = {p 1 , p 2 , ..., p L } ∈ R L is assigned to each facial video X. It is assumed that each entry of p is a real value in the range [0,1] such that L k=1 p k = 1. We consider the Gaussian distribution function, centred at the ground truth HR label Y HR with the standard deviation σ, to construct the corresponding label distribution p.\n\nThe label distribution loss can be formulated as L LD = KL(p, Softmax(p)), where divergence measure KL(•) denotes the Kullback-Leibler (KL) divergence  [17] , and p is the power spectral density (PSD) of predicted rPPG signals.\n\nPlease note that the previous work  [43]  also considers the distribution learning for HR estimation. However, it is totally different with our work: 1) the motivation in  [43]  is to smooth the temporal HR outliers caused by facial movements across continuous video clips, while our work is more generic, aiming at efficient feature learning across adjacent labels under limited-scale training data; 2) the technique used in  [43]  is after a post-HR-estimation for the handcrafted rPPG signals, while our work is to design a reasonable supervision signal L LD for PhysFormer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Curriculum Learning Guided Dynamic Loss",
      "text": "Curriculum learning  [2] , as a major machine learning regime with philosophy of easy-to-hard curriculum, is utilized to train PhysFormer. In the rPPG measurement task, the supervision signals from temporal domain (e.g., mean square error loss  [11] , negative Pearson loss  [65, 67] ) and frequency domain (e.g., cross-entropy loss  [46, 63] , signalto-noise ratio loss  [53] ) provide different extents of constraints for model learning. The former one gives signaltrend-level constraints, which is straightforward and easy for model convergence but overfitting after that. In contrast, the latter one with strong constraints on frequency domain enforces the model learning periodic features within target frequency bands, which is hard to converged well due to the realistic rPPG-irrelevant noise. Inspired by the curriculum learning, we propose the dynamic supervision to gradually enlarge the frequency constraints, which alleviates the overfitting issue and benefits the intrinsic rPPGaware feature learning gradually. Specifically, exponential increment strategy is adopted, and comparison with other dynamic strategies (e.g., linear increment) will be shown in Table  7 . The dynamic loss L overall can be formulated as\n\nwhere hyperparameters α, β 0 and η equal to 0.1, 1.0 and 5.0, respectively. Negative Pearson loss  [65, 67]  and frequency cross-entropy loss  [46, 63]  are adopted as L time and L CE , respectively. With the dynamic supervision, Phys-Former could perceive better signal trend at the beginning while such perfect warming up facilitates the gradually stronger frequency knowledge learning later.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Evaluation",
      "text": "Experiments of rPPG-based physiological measurement for three types of physiological signals, i.e., heart rate (HR), heart rate variability (HRV), and respiration frequency (RF), are conducted on four benchmark datasets (VIPL-HR  [45] , MAHNOB-HCI  [52] , MMSE-HR  [55] , and OBF  [29] ).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets And Performance Metrics",
      "text": "VIPL-HR  [45]  is a large-scale dataset for remote physiological measurement under less-constrained scenarios. It contains 2,378 RGB videos of 107 subjects recorded with different head movements, lighting conditions and acquisition devices. MAHNOB-HCI  [52]  is one of the most widely used benchmark for remote HR measurement evaluations. It includes 527 facial videos of with 61 fps framerate and 780x580 resolution from 27 subjects. MMSE-HR  [55]  is a dataset including 102 RGB videos from 40 subjects, and the raw resolution of each video is at 1040x1392. OBF  [29]  is a high-quality dataset for remote physiological signal measurement. It contains 200 five-minute-long RGB videos with 60 fps framerate recorded from 100 healthy adults.\n\nAverage HR estimation task is evaluated on all four datasets while HRV and RF estimation tasks on high-quality OBF  [29]  dataset. Specifically, we follow existing methods  [41, 46, 67]  and report low frequency (LF), high frequency (HF), and LF/HF ratio for HRV and RF estimation. We report the most commonly used performance metrics for evaluation, including the standard deviation (SD), mean absolute error (MAE), root mean square error (RMSE), and Pearson's correlation coefficient (r).",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "Our proposed method is implemented with Pytorch. For each video clip, we use the MTCNN face detector  [72]  to crop the enlarged face area at the first frame and fix the region through the following frames. The videos in MAHNOB-HCI and OBF are downsampled to 30 fps for efficiency. The settings N =12, h=4, D=96, D ′ =144 are used for PhysFormer while θ=0.7 and τ =2.0 for TD-MHSA. The targeted tube size T s × H s × W s equals to 4×4×4. In the training stage, we randomly sample RGB face clips with size 160×128×128 (T ×H ×W ) as model inputs. Random horizontal flipping and temporally up/down-sampling  [63]  are used for data augmentation. The PhysFormer is trained with Adam optimizer and the initial learning rate and weight decay are 1e-4 and 5e-5, respectively. We cannot find obvious performance improvement using AdamW optimizer. We train models with 25 epochs with fixed setting α=0.1 for temporal loss while exponentially increased parameter β ∈  [1, 5]  for frequency losses. We set σ=1.0 for label distribution learning. The batch size is 4 on one 32G V100 GPU. In the testing stage, similar to  [45] , we uniformly separate 30-second videos into three short clips with 10 seconds, and then video-level HR is calculated via averaging HRs from three short clips.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Intra-Dataset Testing",
      "text": "HR estimation on VIPL-HR.\n\nIn these experiments, we follow  [45]  and use a subject-exclusive 5-fold crossvalidation protocol on VIPL-HR. As shown in Table  1 , all three traditional methods (Tulyakov2016  [55] , POS  [59]  and CHROM  [13] ) perform poorly due to the complex scenarios (e.g., large head movement and various illumination) in the VIPL-HR dataset. Similarly, the existing end-to-end learning based methods (e.g., PhysNet  [65] , DeepPhys  [11] , and AutoHR  [63] ) predict unreliable HR values with large RMSE compared with non-end-to-end learning approaches (e.g., RhythmNet  [45] , ST-Attention  [47] , NAS-HR  [40] , CVD  [46] , and Dual-GAN  [41] ). Such the large performance margin might be caused by the coarse and overfitted rPPG features extracted from the end-to-end models. In contrast, all five non-end-to-end methods first extract finegrained signal maps from multiple facial ROIs, and then more dedicated rPPG clues would be extracted via the cascaded models. Without strict and heavy preprocessing procedure in  [40, 41, [45] [46] [47] , our proposed PhysFormer can be trained from scratch on facial videos directly, and achieves comparable performance with state-of-the-art non-end-toend learning based method Dual-GAN  [41] . It indicates that PhysFormer is able to learn the intrinsic and periodic rPPG-aware features automatically. HR estimation on MAHNOB-HCI. For the HR estimation tasks on MAHNOB-HCI, similar to  [67] , subjectindependent 9-fold cross-validation protocol is adopted. In consideration of the convergence difficulty due to the low illumination and high compression videos in MAHNOB-HCI, we finetune the VIPL-HR pretrained model on MAHNOB-HCI for further 15 epochs. The HR estimation results are shown in Table  3 . The proposed PhysFormer achieves the lowest SD (3.87 bpm) and highest r (0.87) among the traditional, non-end-to-end learning, and end-toend learning methods, which indicates the reliability of the learned rPPG features from PhysFormer under sufficient supervision. Our performance is on par with the latest end-toend learning method Meta-rPPG  [28]  without transductive adaptation from target frames.\n\nHR, HRV and RF estimation on OBF. We also conduct experiments for three types of physiological signals, i.e., HR, RF, and HRV measurement on the OBF  [29]  dataset. Following  [46, 67] , we use a 10-fold subject-exclusive protocol for all experiments. All the results are shown in Table 2. From the results, we can see that the proposed approach outperforms the existing state-of-the-art traditional (ROI green  [29] , CHROM  [13] , POS  [59] ) and end-to-end learning (rPPGNet  [67] ) methods by a large margin on all evaluation metrics for HR, RF and all HRV features. The proposed PhysFormer also gives more accurate estimation in terms of HR, RF, and LF/HF compared with the preprocessed signal map based non-end-to-end learning method CVD  [46] . These results indicate that PhysFormer could not only handle the average HR estimation task but also give a promising prediction of the rPPG signal for RF measurement and HRV analysis, which shows its potential in many healthcare applications.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Cross-Dataset Testing",
      "text": "Besides of the intra-dataset testings on the VIPL-HR, MAHNOB-HCI, and OBF datasets, we also conduct crossdataset testing on MMSE-HR  [55]  following the protocol of  [45] . The models trained on VIPL-HR are directly tested on MMSE-HR. All the results of the proposed approach and the state-of-the-art methods are shown in Table  4 . It is clear that the proposed PhysFormer generalizes well in unseen domain. It is worth noting that PhysFormer achieves the lowest SD (5.22 bpm), MAE (2.84 bpm), RMSE (5.36 bpm) as well as the highest r (0.92) among the traditional, nonend-to-end learning and end-to-end learning based methods, indicating 1) the predicted HRs are highly correlated with the ground truth HRs, and 2) the model learns domaininvariant intrinsic rPPG-aware features. Compared with the spatio-temporal transformer based EfficientPhys-T1  [35] , our proposed PhysFormer is able to predict more accurate physiological signals, which indicates the effectiveness of the long-range spatio-temporal attention.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Study",
      "text": "We also provide the results of ablation studies for HR estimation on the Fold-1 of the VIPL-HR  [45]  dataset.\n\nImpact of tube tokenization. In the default setting of PhysFormer, a shallow stem cascaded with a tube tokenization is used. In this ablation, we consider other four tokenization configurations with or w/o stem. It can be seen from the first row in Table  5  that the stem helps the Phys-Former see better  [61] , and the RMSE increases dramatically (+3.06 bpm) when w/o the stem. Then we investigate the impacts of the spatial and temporal domains in tube tokenization. It is clear that the result in the fourth row with full spatial projection is quite poor (RMSE=10.61 bpm), indicating the necessity of the spatial attention. In contrast, tokenization with smaller tempos (e.g., [2x4x4]) or spatial inputs (e.g., 160x96x96) reduces performance slightly.\n\nImpact of TD-MHSA and ST-FF. As shown in Table  6 , both the TD-MHSA and ST-FF play vital roles in Phys-Former. The result in the first row shows that the performance degrades sharply without spatio-temporal attention. Moreover, it can be seen from the last two rows   7 . Although the L LD performs slightly worse (+0.12 bpm RMSE) than L CE , the best performance can be achieved using both losses, indicating the effectiveness of explicit distribution constraints for extreme-frequency interference alleviation and adjacent label knowledgement propagation. It is interesting to find from the last two rows that using real PSD distribution from groundtruth PPG signals as p, the performance is inferior due to the lack of an obvious peak and partial noise. We can also find from the Fig.  4 (a) that the σ ranged from 0.9 to 1.2 for L LD are suitable to achieve good performance.\n\nImpact of dynamic supervision. Fig.  3  illustrates the testing performance on Fold-1 VIPL-HR when training with fixed and dynamic supervision. It is clear that with exponential increased frequency loss, models in the blue curve converge faster and achieve smaller RMSE. We also compare several kinds of fixed and dynamic strategies in Table 7. The results in the first four rows indicate 1) using fixed higher β leads to poorer performance caused by the convergency difficulty; 2) models with the exponentially increased β perform better than using linear increment.\n\nImpact of θ and layer/head numbers. Hyperparameter θ tradeoffs the contribution of local temporal gradient information. As illustrated in Fig.  3 (b), PhysFormer could achieve smaller RMSE when θ=0.4 and 0.7, indicating the importance of the normalized local temporal difference features for global spatio-temporal attention. We also investigate how the layer and head numbers influence the performance. As shown in Fig.  5 (a), with deeper temporal trans-  former blocks, the RMSE are reduced progressively despite heavier computational cost. In terms of the impact of head numbers, it is clear to find from Fig.  5 (b) that PhysFormer with four heads perform the best while fewer heads lead to sharp performance drops.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Visualization And Discussion",
      "text": "We visualize the attention map from the last TD-MHSA module as well as one example about the query-key interaction in Fig.  6 . The x and y axes indicate the attention confidence from key and query tube tokens, respectively. From the attention map, we can easily find periodic or quasiperiodic responses along both axes, indicating the periodicity of the intrinsic rPPG features from PhysFormer. To be specific, given the 530th tube token (in blue) from the forehead (spatial face domain) and peak (temporal signal domain) locations as a query, the corresponding key responses are illustrated at the blue line in the attention map. On one hand, it can be seen from the key responses that dominant spatial attentions focus on the facial skin regions and discard unrelated background. On the other hand, the temporal localizations of the key responses are around peak positions in the predicted rPPG signals. All these patterns are reasonable: 1) the forehead and cheek regions  [57]  have richer blood volume for rPPG measurement and are also reliable since these regions are less affected by facial muscle movements due to e.g., facial expressions, talking; and 2) rPPG signals from healthy people are usually periodic.\n\nHowever, we also find two limitations of the spatiotemporal attention from Fig.  6 . First, there are still some unexpected responses (e.g., continuous query tokens with similar key responses) in the attention map, which might introduce task-irrelevant noise and damage to the perfor- mance. Second, the temporal attentions are not always accurate, and some are coarse with phase shifts (e.g., the first vertical dotted line of the rPPG signals in bottom Fig.  6 ).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusions And Future Work",
      "text": "In this paper, we propose an end-to-end video transformer architecture, namely PhysFormer, for remote physiological measurement. With temporal difference transformer and elaborate supervisions, PhysFormer is able to achieve superior performance on benchmark datasets. The study of video transformer based physiological measurement is still at an early stage. Future directions include: 1) Designing more efficient architectures. The proposed PhysFormer is with 7.03 M parameters and 47.01 GFLOPs, which is unfriendly for mobile deployment; 2) Exploring more accurate yet efficient spatio-temporal self-attention mechanism especially for long sequence rPPG monitoring.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The trajectories of rPPG signals around t1, t2, and t3",
      "page": 1
    },
    {
      "caption": "Figure 1: , rPPG clues from different",
      "page": 2
    },
    {
      "caption": "Figure 2: Framework of the PhysFormer. It consists of a shallow stem, a tube tokenizer, several temporal difference transformers, and a",
      "page": 3
    },
    {
      "caption": "Figure 2: , PhysFormer consists of a shal-",
      "page": 3
    },
    {
      "caption": "Figure 2: , residual connection and layer nor-",
      "page": 4
    },
    {
      "caption": "Figure 3: Testing results of fixed and dynamic frequency supervi-",
      "page": 7
    },
    {
      "caption": "Figure 4: (a) that the σ ranged from 0.9 to 1.2",
      "page": 7
    },
    {
      "caption": "Figure 3: illustrates the",
      "page": 7
    },
    {
      "caption": "Figure 3: (b), PhysFormer could",
      "page": 7
    },
    {
      "caption": "Figure 5: (a), with deeper temporal trans-",
      "page": 7
    },
    {
      "caption": "Figure 4: Impacts of the (a) σ in label distribution learning and",
      "page": 8
    },
    {
      "caption": "Figure 5: Ablation of the (a) layers and (b) heads in PhysFormer.",
      "page": 8
    },
    {
      "caption": "Figure 5: (b) that PhysFormer",
      "page": 8
    },
    {
      "caption": "Figure 6: The x and y axes indicate the attention con-",
      "page": 8
    },
    {
      "caption": "Figure 6: First, there are still some",
      "page": 8
    },
    {
      "caption": "Figure 6: Visualization of the attention map from the 1st head",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table 7: Although the L LD per-",
      "data": [
        {
          "Inputs": "160 × 128 × 128",
          "[Stem]\nFeature Size": "[\n]\n×\n160 × 128 × 128\n√",
          "[Tube Size]\nToken Numbers": "[4 × 32 × 32]\n40 × 4 × 4",
          "RMSE ↓\n(bpm)": "10.62"
        },
        {
          "Inputs": "160 × 128 × 128",
          "[Stem]\nFeature Size": "[\n]\n160 × 16 × 16\n√",
          "[Tube Size]\nToken Numbers": "[4 × 4 × 4]\n40 × 4 × 4",
          "RMSE ↓\n(bpm)": "7.56"
        },
        {
          "Inputs": "160 × 96 × 96",
          "[Stem]\nFeature Size": "[\n]\n160 × 12 × 12\n√",
          "[Tube Size]\nToken Numbers": "[4 × 4 × 4]\n40 × 3 × 3",
          "RMSE ↓\n(bpm)": "8.03"
        },
        {
          "Inputs": "160 × 128 × 128",
          "[Stem]\nFeature Size": "[\n]\n160 × 16 × 16\n√",
          "[Tube Size]\nToken Numbers": "[4 × 16 × 16]\n40 × 1 × 1",
          "RMSE ↓\n(bpm)": "10.61"
        },
        {
          "Inputs": "160 × 128 × 128",
          "[Stem]\nFeature Size": "[\n]\n160 × 16 × 16",
          "[Tube Size]\nToken Numbers": "[2 × 4 × 4]\n80 × 4 × 4",
          "RMSE ↓\n(bpm)": "7.81"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\n5\n10\n15\n20\n25\n30\n35\n40": "",
          "40": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Vivit: A video vision transformer",
      "authors": [
        "Anurag Arnab",
        "Mostafa Dehghani",
        "Georg Heigold",
        "Chen Sun",
        "Mario Lučić",
        "Cordelia Schmid"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "2",
      "title": "Ronan Collobert, and Jason Weston. Curriculum learning",
      "authors": [
        "Yoshua Bengio",
        "Jérôme Louradour"
      ],
      "year": "2009",
      "venue": "ICML"
    },
    {
      "citation_id": "3",
      "title": "Is space-time attention all you need for video understanding?",
      "authors": [
        "Gedas Bertasius",
        "Heng Wang",
        "Lorenzo Torresani"
      ],
      "year": "2021",
      "venue": "Is space-time attention all you need for video understanding?",
      "arxiv": "arXiv:2102.05095"
    },
    {
      "citation_id": "4",
      "title": "Space-time mixing attention for video transformer",
      "authors": [
        "Adrian Bulat",
        "Juan-Manuel Perez-Rua",
        "Swathikiran Sudhakaran",
        "Brais Martinez",
        "Georgios Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "5",
      "title": "Video super-resolution transformer",
      "authors": [
        "Jiezhang Cao",
        "Yawei Li",
        "Kai Zhang",
        "Luc Van Gool"
      ],
      "year": "2021",
      "venue": "Video super-resolution transformer",
      "arxiv": "arXiv:2106.06847"
    },
    {
      "citation_id": "6",
      "title": "End-toend object detection with transformers",
      "authors": [
        "Nicolas Carion",
        "Francisco Massa",
        "Gabriel Synnaeve",
        "Nicolas Usunier",
        "Alexander Kirillov",
        "Sergey Zagoruyko"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "7",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "Joao Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "8",
      "title": "Crossvit: Cross-attention multi-scale vision transformer for image classification",
      "authors": [
        "Chun-Fu Chen"
      ],
      "year": "2021",
      "venue": "Quanfu Fan, and Rameswar Panda"
    },
    {
      "citation_id": "9",
      "title": "Aniformer: Data-driven 3d animation with transformer",
      "authors": [
        "Haoyu Chen",
        "Hao Tang",
        "Nicu Sebe",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Aniformer: Data-driven 3d animation with transformer"
    },
    {
      "citation_id": "10",
      "title": "Geometry-contrastive transformer for generalized 3d pose transfer",
      "authors": [
        "Haoyu Chen",
        "Hao Tang",
        "Zitong Yu",
        "Nicu Sebe",
        "Guoying Zhao"
      ],
      "year": "2022",
      "venue": "AAAI"
    },
    {
      "citation_id": "11",
      "title": "Deepphys: Videobased physiological measurement using convolutional attention networks",
      "authors": [
        "Weixuan Chen",
        "Daniel Mcduff"
      ],
      "year": "2018",
      "venue": "ECCV"
    },
    {
      "citation_id": "12",
      "title": "Video-based heart rate measurement: Recent advances and future prospects",
      "authors": [
        "Xun Chen",
        "Juan Cheng",
        "Rencheng Song",
        "Yu Liu",
        "Rabab Ward",
        "Jane Wang"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "13",
      "title": "Robust pulse rate from chrominance-based rppg",
      "authors": [
        "Gerard De",
        "Vincent Jeanne"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "14",
      "title": "Hr-nas: Searching efficient high-resolution neural architectures with lightweight transformers",
      "authors": [
        "Mingyu Ding",
        "Xiaochen Lian",
        "Linjie Yang",
        "Peng Wang",
        "Xiaojie Jin",
        "Zhiwu Lu",
        "Ping Luo"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "15",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale"
    },
    {
      "citation_id": "16",
      "title": "Multiscale vision transformers. ICCV",
      "authors": [
        "Bo Haoqi Fan",
        "Karttikeya Xiong",
        "Yanghao Mangalam",
        "Zhicheng Li",
        "Jitendra Yan",
        "Christoph Malik",
        "Feichtenhofer"
      ],
      "year": "2021",
      "venue": "Multiscale vision transformers. ICCV"
    },
    {
      "citation_id": "17",
      "title": "Deep label distribution learning with label ambiguity",
      "authors": [
        "Bin-Bin Gao",
        "Chao Xing",
        "Chen-Wei Xie",
        "Jianxin Wu",
        "Xin Geng"
      ],
      "year": "2017",
      "venue": "Deep label distribution learning with label ambiguity"
    },
    {
      "citation_id": "18",
      "title": "Age estimation using expectation of label distribution learning",
      "authors": [
        "Bin-Bin Gao",
        "Hong-Yu Zhou",
        "Jianxin Wu",
        "Xin Geng"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "19",
      "title": "Facial age estimation by learning from label distributions",
      "authors": [
        "Xin Geng",
        "Zhi-Hua Chao Yin",
        "Zhou"
      ],
      "year": "2013",
      "venue": "IEEE TPAMI"
    },
    {
      "citation_id": "20",
      "title": "The way to my heart is through contrastive learning: Remote photoplethysmography from unlabelled video",
      "authors": [
        "John Gideon",
        "Simon Stent"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "21",
      "title": "Video action transformer network",
      "authors": [
        "Rohit Girdhar",
        "Joao Carreira",
        "Carl Doersch",
        "Andrew Zisserman"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "22",
      "title": "A survey on visual transformer",
      "authors": [
        "Kai Han",
        "Yunhe Wang",
        "Hanting Chen",
        "Xinghao Chen",
        "Jianyuan Guo",
        "Zhenhua Liu",
        "Yehui Tang",
        "An Xiao",
        "Chunjing Xu",
        "Yixing Xu"
      ],
      "year": "2020",
      "venue": "A survey on visual transformer",
      "arxiv": "arXiv:2012.12556"
    },
    {
      "citation_id": "23",
      "title": "",
      "authors": [
        "Kai Han",
        "An Xiao",
        "Enhua Wu",
        "Jianyuan Guo",
        "Chunjing Xu",
        "Yunhe Wang"
      ],
      "year": "2021",
      "venue": "",
      "arxiv": "arXiv:2103.00112"
    },
    {
      "citation_id": "24",
      "title": "Transreid: Transformer-based object reidentification",
      "authors": [
        "Shuting He",
        "Hao Luo",
        "Pichao Wang",
        "Fan Wang",
        "Hao Li",
        "Wei Jiang"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "25",
      "title": "Deep learning with time-frequency representation for pulse estimation from facial videos",
      "authors": [
        "Gee-Sern",
        "Arulmurugan Hsu",
        "Ming-Shiang Ambikapathi",
        "Chen"
      ],
      "year": "2017",
      "venue": "IJCB"
    },
    {
      "citation_id": "26",
      "title": "Transformers in vision: A survey",
      "authors": [
        "Salman Khan",
        "Muzammal Naseer",
        "Munawar Hayat",
        "Fahad Syed Waqas Zamir",
        "Mubarak Shahbaz Khan",
        "Shah"
      ],
      "year": "2021",
      "venue": "Transformers in vision: A survey",
      "arxiv": "arXiv:2101.01169"
    },
    {
      "citation_id": "27",
      "title": "Robust heart rate measurement from video using select random patches",
      "authors": [
        "Antony Lam",
        "Yoshinori Kuno"
      ],
      "year": "2015",
      "venue": "ICCV"
    },
    {
      "citation_id": "28",
      "title": "Meta-rppg: Remote heart rate estimation using a transductive meta-learner",
      "authors": [
        "Eugene Lee",
        "Evan Chen",
        "Chen-Yi Lee"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "29",
      "title": "The obf database: A large face video database for remote physiological signal measurement and atrial fibrillation detection",
      "authors": [
        "Xiaobai Li",
        "Iman Alikhani",
        "Jingang Shi",
        "Tapio Seppanen",
        "Juhani Junttila",
        "Kirsi Majamaa-Voltti",
        "Mikko Tulppo",
        "Guoying Zhao"
      ],
      "year": "2018",
      "venue": "FG"
    },
    {
      "citation_id": "30",
      "title": "Remote heart rate measurement from face videos under realistic situations",
      "authors": [
        "Xiaobai Li",
        "Jie Chen",
        "Guoying Zhao",
        "Matti Pietikainen"
      ],
      "year": "2014",
      "venue": "CVPR"
    },
    {
      "citation_id": "31",
      "title": "Tsm: Temporal shift module for efficient video understanding",
      "authors": [
        "Ji Lin",
        "Chuang Gan",
        "Song Han"
      ],
      "year": "2019",
      "venue": "CVPR"
    },
    {
      "citation_id": "32",
      "title": "A survey of transformers",
      "authors": [
        "Tianyang Lin",
        "Yuxin Wang",
        "Xiangyang Liu",
        "Xipeng Qiu"
      ],
      "year": "2021",
      "venue": "A survey of transformers",
      "arxiv": "arXiv:2106.04554"
    },
    {
      "citation_id": "33",
      "title": "Fuseformer: Fusing fine-grained information in transformers for video inpainting",
      "authors": [
        "Rui Liu",
        "Hanming Deng",
        "Yangyi Huang",
        "Xiaoyu Shi",
        "Lewei Lu",
        "Wenxiu Sun",
        "Xiaogang Wang",
        "Jifeng Dai",
        "Hongsheng Li"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "34",
      "title": "Multi-task temporal shift attention networks for on-device contactless vitals measurement",
      "authors": [
        "Xin Liu",
        "Josh Fromm",
        "Shwetak Patel",
        "Daniel Mcduff"
      ],
      "year": "2020",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "35",
      "title": "Efficientphys: Enabling simple, fast and accurate camera-based vitals measurement",
      "authors": [
        "Xin Liu",
        "Brian Hill",
        "Ziheng Jiang",
        "Shwetak Patel",
        "Daniel Mcduff"
      ],
      "year": "2021",
      "venue": "Efficientphys: Enabling simple, fast and accurate camera-based vitals measurement",
      "arxiv": "arXiv:2110.04447"
    },
    {
      "citation_id": "36",
      "title": "Camerabased physiological sensing: Challenges and future directions",
      "authors": [
        "Xin Liu",
        "Shwetak Patel",
        "Daniel Mcduff"
      ],
      "year": "2021",
      "venue": "Camerabased physiological sensing: Challenges and future directions",
      "arxiv": "arXiv:2110.13362"
    },
    {
      "citation_id": "37",
      "title": "End-to-end temporal action detection with transformer",
      "authors": [
        "Xiaolong Liu",
        "Qimeng Wang",
        "Yao Hu",
        "Xu Tang",
        "Song Bai",
        "Xiang Bai"
      ],
      "year": "2021",
      "venue": "End-to-end temporal action detection with transformer",
      "arxiv": "arXiv:2106.10271"
    },
    {
      "citation_id": "38",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Ze Liu",
        "Yutong Lin",
        "Yue Cao",
        "Han Hu",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Baining Guo"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "39",
      "title": "Video swin transformer",
      "authors": [
        "Ze Liu",
        "Jia Ning",
        "Yue Cao",
        "Yixuan Wei",
        "Zheng Zhang",
        "Stephen Lin",
        "Han Hu"
      ],
      "year": "2021",
      "venue": "Video swin transformer",
      "arxiv": "arXiv:2106.13230"
    },
    {
      "citation_id": "40",
      "title": "Nas-hr: Neural architecture search for heart rate estimation from face videos",
      "authors": [
        "Hao Lu",
        "Hu Han"
      ],
      "year": "2021",
      "venue": "Virtual Reality & Intelligent Hardware"
    },
    {
      "citation_id": "41",
      "title": "Dual-gan: Joint bvp and noise modeling for remote physiological measurement",
      "authors": [
        "Hao Lu",
        "Hu Han",
        "Kevin Zhou"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "42",
      "title": "Maya Zohar, and Dotan Asselmann. Video transformer network",
      "authors": [
        "Daniel Neimark",
        "Omri Bar"
      ],
      "year": "2021",
      "venue": "Maya Zohar, and Dotan Asselmann. Video transformer network",
      "arxiv": "arXiv:2102.00719"
    },
    {
      "citation_id": "43",
      "title": "Continuous heart rate measurement from face: A robust rppg approach with distribution learning",
      "authors": [
        "Xuesong Niu",
        "Hu Han",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2017",
      "venue": "Continuous heart rate measurement from face: A robust rppg approach with distribution learning"
    },
    {
      "citation_id": "44",
      "title": "Synrhythm: Learning a deep heart rate estimator from general to specific",
      "authors": [
        "Xuesong Niu",
        "Hu Han",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2018",
      "venue": "ICPR"
    },
    {
      "citation_id": "45",
      "title": "Rhythmnet: End-to-end heart rate estimation from face via spatial-temporal representation",
      "authors": [
        "Xuesong Niu",
        "Shiguang Shan",
        "Hu Han",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "46",
      "title": "Video-based remote physiological measurement via cross-verified feature disentangling",
      "authors": [
        "Xuesong Niu",
        "Zitong Yu",
        "Hu Han",
        "Xiaobai Li",
        "Shiguang Shan",
        "Guoying Zhao"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "47",
      "title": "Robust remote heart rate estimation from face utilizing spatial-temporal attention",
      "authors": [
        "Xuesong Niu",
        "Xingyuan Zhao",
        "Hu Han",
        "Abhijit Das",
        "Antitza Dantcheva",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "FG"
    },
    {
      "citation_id": "48",
      "title": "The benefit of distraction: Denoising camera-based physiological measurements using inverse attention",
      "authors": [
        "Ewa Nowara",
        "Daniel Mcduff",
        "Ashok Veeraraghavan"
      ],
      "year": "2021",
      "venue": "ICCV"
    },
    {
      "citation_id": "49",
      "title": "Advancements in noncontact, multiparameter physiological measurements using a webcam",
      "authors": [
        "Ming-Zher Poh",
        "Daniel Mcduff",
        "Rosalind Picard"
      ],
      "year": "2010",
      "venue": "IEEE transactions on biomedical engineering"
    },
    {
      "citation_id": "50",
      "title": "Non-contact, automated cardiac pulse measurements using video imaging and blind source separation",
      "authors": [
        "Ming-Zher Poh",
        "Daniel Mcduff",
        "Rosalind Picard"
      ],
      "year": "2010",
      "venue": "Optics express"
    },
    {
      "citation_id": "51",
      "title": "Evm-cnn: Real-time contactless heart rate estimation from facial video",
      "authors": [
        "Ying Qiu",
        "Yang Liu",
        "Juan Arteaga-Falconi",
        "Haiwei Dong",
        "Abdulmotaleb Saddik"
      ],
      "year": "2018",
      "venue": "IEEE TMM"
    },
    {
      "citation_id": "52",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "Mohammad Soleymani",
        "Jeroen Lichtenauer",
        "Maja Thierry Pun",
        "Pantic"
      ],
      "year": "2011",
      "venue": "A multimodal database for affect recognition and implicit tagging"
    },
    {
      "citation_id": "53",
      "title": "Visual heart rate estimation with convolutional neural network",
      "authors": [
        "Radim Špetlík",
        "Vojtech Franc",
        "Jirí Matas"
      ],
      "year": "2018",
      "venue": "BMVC"
    },
    {
      "citation_id": "54",
      "title": "Training data-efficient image transformers & distillation through attention",
      "authors": [
        "Hugo Touvron",
        "Matthieu Cord",
        "Matthijs Douze",
        "Francisco Massa",
        "Alexandre Sablayrolles",
        "Hervé Jégou"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "55",
      "title": "Self-adaptive matrix completion for heart rate estimation from face videos under realistic conditions",
      "authors": [
        "Sergey Tulyakov",
        "Xavier Alameda-Pineda",
        "Elisa Ricci",
        "Lijun Yin",
        "Jeffrey Cohn",
        "Nicu Sebe"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "56",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "NIPS"
    },
    {
      "citation_id": "57",
      "title": "Remote plethysmographic imaging using ambient light",
      "authors": [
        "Wim Verkruysse",
        "Lars Svaasand",
        "J Stuart"
      ],
      "year": "2008",
      "venue": "Optics express"
    },
    {
      "citation_id": "58",
      "title": "Temporal action proposal generation with transformers",
      "authors": [
        "Lining Wang",
        "Haosen Yang",
        "Wenhao Wu",
        "Hongxun Yao",
        "Hujie Huang"
      ],
      "year": "2021",
      "venue": "Temporal action proposal generation with transformers",
      "arxiv": "arXiv:2105.12043"
    },
    {
      "citation_id": "59",
      "title": "Algorithmic principles of remote ppg",
      "authors": [
        "Wenjin Wang",
        "Albertus C Den",
        "Sander Brinker",
        "Gerard Stuijk",
        "De Haan"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "60",
      "title": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions",
      "authors": [
        "Wenhai Wang",
        "Enze Xie",
        "Xiang Li",
        "Deng-Ping Fan",
        "Kaitao Song",
        "Ding Liang",
        "Tong Lu",
        "Ping Luo",
        "Ling Shao"
      ],
      "year": "2021",
      "venue": "Pyramid vision transformer: A versatile backbone for dense prediction without convolutions"
    },
    {
      "citation_id": "61",
      "title": "Early convolutions help transformers see better",
      "authors": [
        "Tete Xiao",
        "Mannat Singh",
        "Eric Mintun",
        "Trevor Darrell",
        "Piotr Dollár",
        "Ross Girshick"
      ],
      "year": "2021",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "62",
      "title": "Long short-term transformer for online action detection",
      "authors": [
        "Mingze Xu",
        "Yuanjun Xiong",
        "Hao Chen",
        "Xinyu Li",
        "Wei Xia",
        "Zhuowen Tu",
        "Stefano Soatto"
      ],
      "year": "2021",
      "venue": "Long short-term transformer for online action detection",
      "arxiv": "arXiv:2107.03377"
    },
    {
      "citation_id": "63",
      "title": "Autohr: A strong end-to-end baseline for remote heart rate measurement with neural searching",
      "authors": [
        "Zitong Yu",
        "Xiaobai Li",
        "Xuesong Niu",
        "Jingang Shi",
        "Guoying Zhao"
      ],
      "year": "2020",
      "venue": "IEEE SPL"
    },
    {
      "citation_id": "64",
      "title": "Transrppg: Remote photoplethysmography transformer for 3d mask face presentation attack detection",
      "authors": [
        "Zitong Yu",
        "Xiaobai Li",
        "Pichao Wang",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "IEEE SPL"
    },
    {
      "citation_id": "65",
      "title": "Remote photoplethysmograph signal measurement from facial videos using spatio-temporal networks",
      "authors": [
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2019",
      "venue": "BMVC"
    },
    {
      "citation_id": "66",
      "title": "Facial-videobased physiological signal measurement: Recent advances and affective applications",
      "authors": [
        "Zitong Yu",
        "Xiaobai Li",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "67",
      "title": "Remote heart rate measurement from highly compressed facial videos: an end-to-end deep learning solution with video enhancement",
      "authors": [
        "Zitong Yu",
        "Wei Peng",
        "Xiaobai Li",
        "Xiaopeng Hong",
        "Guoying Zhao"
      ],
      "year": "2019",
      "venue": "ICCV"
    },
    {
      "citation_id": "68",
      "title": "Deep learning for face antispoofing: a survey",
      "authors": [
        "Zitong Yu",
        "Yunxiao Qin",
        "Xiaobai Li",
        "Chenxu Zhao",
        "Zhen Lei",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "Deep learning for face antispoofing: a survey",
      "arxiv": "arXiv:2106.14948"
    },
    {
      "citation_id": "69",
      "title": "Searching multi-rate and multi-modal temporal enhanced networks for gesture recognition",
      "authors": [
        "Zitong Yu",
        "Benjia Zhou",
        "Jun Wan",
        "Pichao Wang",
        "Haoyu Chen",
        "Xin Liu",
        "Stan Li",
        "Guoying Zhao"
      ],
      "year": "2021",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "70",
      "title": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
      "authors": [
        "Li Yuan",
        "Yunpeng Chen",
        "Tao Wang",
        "Weihao Yu",
        "Yujun Shi",
        "Zihang Jiang",
        "Francis Tay",
        "Jiashi Feng",
        "Shuicheng Yan"
      ],
      "year": "2021",
      "venue": "Tokens-to-token vit: Training vision transformers from scratch on imagenet",
      "arxiv": "arXiv:2101.11986"
    },
    {
      "citation_id": "71",
      "title": "Learning joint spatial-temporal transformations for video inpainting",
      "authors": [
        "Yanhong Zeng",
        "Jianlong Fu",
        "Hongyang Chao"
      ],
      "year": "2020",
      "venue": "ECCV"
    },
    {
      "citation_id": "72",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "Kaipeng Zhang",
        "Zhanpeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "IEEE SPL"
    },
    {
      "citation_id": "73",
      "title": "Tubetransformer for action detection",
      "authors": [
        "Jiaojiao Zhao",
        "Xinyu Li",
        "Chunhui Liu",
        "Shuai Bing",
        "Hao Chen",
        "G Cees",
        "Joseph Snoek",
        "Tighe",
        "Tuber"
      ],
      "year": "2021",
      "venue": "Tubetransformer for action detection",
      "arxiv": "arXiv:2104.00969"
    },
    {
      "citation_id": "74",
      "title": "Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers",
      "authors": [
        "Sixiao Zheng",
        "Jiachen Lu",
        "Hengshuang Zhao",
        "Xiatian Zhu",
        "Zekun Luo",
        "Yabiao Wang",
        "Yanwei Fu",
        "Jianfeng Feng",
        "Tao Xiang",
        "Philip Hs Torr"
      ],
      "year": "2021",
      "venue": "CVPR"
    }
  ]
}