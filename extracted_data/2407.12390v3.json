{
  "paper_id": "2407.12390v3",
  "title": "Enhancing Facial Expression Recognition Through Dual-Direction Attention Mixed Feature Networks: Application To 7Th Abaw Challenge",
  "published": "2024-07-17T08:11:37Z",
  "authors": [
    "Josep Cabacas-Maso",
    "Elena Ortega-Beltrán",
    "Ismael Benito-Altamirano",
    "Carles Ventura"
  ],
  "keywords": [
    "ABAW Challenge",
    "Emotion recogntion",
    "DDAMFN"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "We present our contribution to the 7th ABAW challenge at ECCV 2024, by utilizing a Dual-Direction Attention Mixed Feature Network (DDAMFN) for multitask facial expression recognition, we achieve results far beyond the proposed baseline for the Multi-Task ABAW challenge. Our proposal uses the well-known DDAMFN architecture as base to effectively predict valence-arousal, emotion recognition, and facial action units. We demonstrate the architecture ability to handle these tasks simultaneously, providing insights into its architecture and the rationale behind its design. Additionally, we compare our results for a multitask solution with independent single-task performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial emotion recognition has emerged as a pivotal area of research within affective computing, driven by its potential applications in fields ranging from human-computer interaction to psychological research and clinical diagnostics. Since Ekman's classification of human expression faces into emotions  [5] , a lot of studies have emerged, in recent years. Calvo et al.  [2] , Baltrusaities et al.  [1] , or Kaya et al.  [7]  laid foundational frameworks for understanding facial expressions as a window into emotional states. In contemporary research, Liu et al.  [24]  and Kim et al.  [8]  continued to refine and expand these methodologies, by synthesizing insights from cognitive psychology, computer vision, and machine learning, researchers have made significant strides in enhancing the accuracy and applicability of facial emotion recognition systems. Moreover, the integration of valence and arousal dimensions  [31, 32]  added depth to the interpretation of emotional states, enabling more nuanced insights into human affective experiences.\n\nAction unit detection  [4, 25]  complemented these efforts by parsing facial expressions into discrete muscle movements, facilitating a finer-grained analysis of emotional expressions across cultures and contexts. Such advancements not only improved the reliability of automated emotion recognition systems but also opened the possibility to personalize affective computing applications in fields such as mental health monitoring  [26]  or user experience design  [33] .\n\nTo tackle all these challenges, researchers have explored innovative architectures such as the DDAMFN (Dual-Direction Attention Mixed Feature Network)  [34] . This novel approach integrates attention mechanisms  [6]  and mixed feature extraction  [3] , enhancing the network's ability to capture intricate details within facial expressions. This architecture shows promising results in multitask challenges, together with other pretrained networks  [28] .\n\nThere exists a modern day need to create machines capable to comprehend and appropriately respond to human feelings day-to-day on-the-wild applications. This challenge was presented series of comptetitions entitled \"Affective Behavior Analysis in-the-wild (ABAW)\" challenges  [9] [10] [11] [12] [13] [14] [15] [17] [18] [19] [20] . For the 7th ABAW challenge, at ECCV 2024, two competitions where presented: first, a competition focused on solving a multi-task classification, focused on valancearousal, emotion recognition, and action units, and second, a competition focused on compound expression recognition. In this work, we present our approach to the first competition, where we implemented our multi-task version of the DDAMFN architecture.\n\nThe competition presented a smaller dataset (s-AffWild2)  [21]  than in previous challenges (Aff-Wild2)  [16] . We proposed a fine-tuning training on the s-AffWild2 dataset to increase the performance of the model for a multitask challenge; plus, we evaluated its performance individually on the different task of the challenge: valence-arousal prediction, emotion recognition, and action unit detection.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Curation",
      "text": "The s-Aff-Wild2 was introduced by the organizers of 7th ABAW challenge  [21]  as a subset of Aff-Wild2, representing a subset of image frames of the original videos, without any audio data. Only the frames and the annotations for 221,928 images were presented. The dataset presented a preestablished train-validationtest partition: 142,382 instances were introduced for training; 26,876, for validation; and 52,670 where released later on in the challenge as test set. Aproximating, relative partions of: 65% for training, 12% for validation and 23% for test. The frames came with annotations in terms of valence-arousal, eight basic expressions (happiness, sadness, anger, fear, surprise, and disgust), the neutral state, and an 'other' category that included affective states not covered by the aforementioned categories. Additionally, the dataset contained annotations for 12 action units: AU1, AU2, AU4, AU6, AU7, AU10, AU12, AU15, AU23, AU24, AU25, and AU26.Així com la comparació dels histogrames de valance-arousal.\n\nFollowing the instructions provided by the competition organizers we filtered out image frames that contained invalid annotations. We decided to apply an strict criteria to filter out the frames, removing any frame that contained any annotation value outside the specified acceptable range. Specifically, annotation values of -5 for valence/arousal, -1 for expressions, and -1 for action units (AUs) were excluded from consideration in the analysis, this is summarized in Table  1 . Note also that we transformed the annotations of expressions and action units to binary values, where 1 indicates the presence of the expression or action unit, and 0 indicates the absence of the expression or action unit. We applied this rigorous filtering to the train-validation splits, frames containing any annotation values outside the specified acceptable range were excluded from use, even if all other values for that frame were within the normal range. This resulted in a refined dataset of 52,154 frames for training and 15,440 frames for validation, a summary of the dataset after filtering is shown in Table  2 .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Network Architecture",
      "text": "For the 7th ABAW challenge, we adapted the Dual-Direction Attention Mixed Feature Network (DDAMFN)  [34]  to the multitask problem with three different fully-connected layers at the end of the network. These layers consist of a valencearousal prediction layer with 2 output units, an emotion recognition layer with 8 output units, and an action unit layer with 12 output units.\n\nFigure  1  shows a diagram of the network, which features a base Mobile-FaceNet (MFN)  [3]  architecture for feature extraction, followed by a Dual-Direction Attention (DDA) module -with two attention heads-and a Global Depthwise Convolution (GDConv) layer. The output of the GDConv layer is reshaped and fed into the three fully-connected layers for the different tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training",
      "text": "Initially, the DDAMFN pretrained model was obtained from stock versions in the source code repository of the original work, which was trained on AffectNet-8  [34] . In both our expriments, we preserved the feature extraction, attention mechanisms and the GDConv layer pretained weights, but initialized the fullyconnected layers with random weights. For the first experiment, a multitask training was performed in two different stages. First, we focused on training our custom multitask classifier, so, for training, the entire network was frozen except for the custom classifier. The classifier was trained in isolation to learn the distinct characteristics of each task without interference from the feature extraction layers. Subsequently, the entire model was unfrozen, and fine-tuned on all layers. This comprehensive training stage ensured that the feature extraction, attention mechanisms, and classifier were all optimized to work cohesively for the multitask problem.\n\nLoss functions were calculated following these criteria:\n\n-For valence-arousal prediction, the loss function was calculated using the Concordance Correlation Coefficient (CCC). CCC is a measure that evaluates the agreement between two time series by assessing both their precision (how well the observations agree) and accuracy (how well the observations match the true values). -For emotion recognition, was cross-entropy, which is commonly employed in classification tasks to measure the difference between the predicted probability distribution and the true distribution. -For action unit (AU) detection, the binary cross-entropy loss was used, which is suitable for binary classification tasks, measuring the difference between the predicted probability and the actual binary outcome for each action unit.\n\nWhen the whole model was fine-tuned, an extra loss for the attention head was added. This loss works by calculating the mean squared error (MSE) between each pair of attention heads, and then normalizing the total loss by the number of pairs plus a small epsilon to avoid division by zero. This encourages the attention heads to produce consistent outputs  [34] .\n\nFurthermore, for the Action Unit task, a global threshold of 0.5 was initially tested across all AUs, followed by individual optimization for each AU  [27] .\n\nAs for the second experiment, we repeated a similar procedure. However, this time we trained each task individually. This approach allowed us to compare the results for each specific task, providing more detailed insights and enabling a clearer understanding of the performance variations across different tasks.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "The metrics evaluated include the Concordance Correlation Coefficient (CCC) for valence, arousal, and their combination (valence-arousal), F1 score for emotion classification, F1 score for action unit (AU) detection, and the combined performance score (P Score), as described in the challenge  [21] . And composed following the challenge P score formulation:\n\nThe performance metrics for the multitask challenge are summarized in Table 3. The fine-tuned DDAMFN model achieved a CCC of 0.549 for valence, 0.524 for arousal, and 0.536 for valence-arousal. The F1 scores for emotion classification and AU detection were 0.277 and 0.470, respectively, resulting in a P score of 1.287. Incorporating threshold adjustments slightly improved the F1 score for AU detection to 0.510 and the P score to 1.327.\n\nTraining a custom classifier showed a CCC of 0.548 for valence, 0.518 for arousal, and 0.533 for valence-arousal. The F1 scores for emotion classification and AU detection were 0.262 and 0.473, respectively, with a P score of 1.283. Threshold adjustments for the custom classifier improved the F1 score for AU detection to 0.500 and the P score to 1.313. The performance metrics for individual tasks are detailed in Table  4 . For individual tasks, fine-tuning the entire DDAMFN model demonstrated higher performance with a CCC of 0.604 for valence, 0.550 for arousal, and 0.577 for valence-arousal. The F1 scores for emotion classification and AU detection were 0.287 and 0.490, respectively, with a P Score of 1.354. Threshold adjustments further enhanced the F1 score for AU detection to 0.529 and the P Score to 1.393.\n\nTraining a custom classifier presented a CCC of 0.530 for valence, 0.537 for arousal, and 0.533 for valence-arousal. The F1 scores for emotion classification and AU detection were 0.243 and 0.487, respectively, resulting in a P Score of 1.263. Applying threshold adjustments increased the F1 score for AU detection to 0.524 and the P Score to 1.300. Overall, both approaches: fine-tuning the entire DDAMFN model and training a custom classifier, performed well across the evaluated metrics, with finetuning the DDAMFN model showing slightly better performance in individual task metrics.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Test Set Evaluation",
      "text": "We conducted an extensive evaluation by submitting three distinct sets of prediction models. The first set of predictions was generated using the model that had been fine-tuned across the entire architecture (DDAMFN+thresholds).\n\nThe second set of predictions was done leveraging DDAMFN as a feature extractor. In this approach, the sophisticated feature extraction capabilities of DDAMFN were utilized to capture complex patterns and representations within the data. A custom classifier was then trained on these extracted features, allowing for a focused and potentially more accurate classification performance(Custom classifier+thresholds).\n\nThe third set of predictions was derived from the ensemble approach, where the results from the models trained individually for each task were combined.\n\nResults are detailed in Table  5 . DDAMFN+thresholds: This model achieved the highest F1 score for expressions (F1Expr = 0.282), indicating its effectiveness in classifying expressions. However, it had the lowest combined CCC for valence and arousal (CCCV A = 0.262), suggesting it is less reliable in predicting emotional valence and arousal. The overall performance metric P was 1.022, positioning it as the least effective in terms of the combined performance metrics. Custom classifier+thresholds: This model excelled with the highest combined CCC for valence and arousal (CCCV A = 0.371), showing its strength in predicting emotional states accurately. It also had the highest overall performance metric P (1.114), reflecting its robust performance across the evaluated metrics. Despite this, its F1 scores for expressions (F1Expr = 0.277) and action units (F1 AU = 0.466) were slightly lower compared to the other models, indicating room for improvement in those specific areas.\n\nEnsembled individual model results: This model achieved the highest F1 score for action units (F1AU = 0.499), making it the best performer in classifying action units. Its combined CCC for valence and arousal was moderate (CCCV A = 0.292), and its F1 score for expressions was the lowest among the models (F1 Expr = 0.272). The performance metric P was 1.064, indicating a balanced performance but not leading in any single category except for action units.\n\nThe model that achieved the highest overall performance was ranked 5th in the 7th Affective Behavior Analysis in the Wild (ABAW) Challenge. This ranking reflects its competitive capability and robustness among numerous submissions from diverse research groups, highlighting its efficacy in real-world affective behavior analysis tasks. The key differences compared to other teams are multifaceted. Primarily, other teams have employed more sophisticated and heavier feature extractors, such as Masked Autoencoders (MAE)  [23] , or have integrated multiple feature extractors to enhance the overall quality of the extracted features  [30] . This approach has allowed them to achieve superior results.\n\nIn contrast, our methodology included a more aggressive data filtering process, which consequently led to the utilization of a smaller dataset for training purposes compared to our competitors. This reduction in data volume may have impacted our model's ability to generalize effectively.\n\nMoreover, a significant limitation of our training approach has been the treatment of each frame in isolation, without taking into account any temporal relationships or dependencies within the sequence of frames  [29] . This lack of temporal context has posed challenges for the training process, potentially hindering the model's performance in tasks where temporal dynamics are crucial.\n\nAnother notable distinction lies in our reliance on a single, streamlined model architecture. Unlike some other teams, which have explored the benefits of ensemble methods or hybrid models incorporating both classical and deep learning techniques, our approach has remained relatively straightforward  [22] .\n\nOverall, while our methods have certain strengths, these differences in feature extraction, data volume, and temporal context consideration have been critical factors influencing our comparative performance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "The results using DDAMFN as a feature extractor with the pretrained weigths were practically the same as those obtained by fine-tuning the entire model. This similarity in performance can be attributed to the architecture of DDAMFN, which has been trained to generalize exceptionally well for the multitask challenge problem. This inherent capability allows it to extract meaningful features that are sufficient for achieving high performance without the need for extensive fine-tuning.\n\nIn contrast, the single-task experiment revealed a different scenario. Finetuning the entire model for the single-task yielded better results than merely training the classifier, and even outperformed the multitask results. This finding underscored the importance of task-specific optimization. When the model is fine-tuned for a specific task, it can leverage the nuances and particularities of that task, leading to superior performance.\n\nThis suggests that with meticulous optimization of the loss functions and careful consideration of data imbalance, the DDAMFN architecture could potentially achieve even better results in the multitask challenge. Proper handling of these aspects could unlock the full potential of the model, leading to significant performance gains in multitask settings.\n\nMoreover, it is crucial to highlight the importance of effective threshold optimization in the Action Units task. By fine-tuning the thresholds, the results improved significantly, with an increase of 0.5 in performance metrics. This improvement demonstrates that beyond model architecture and training strategies, the post-processing steps such as threshold optimization play a vital role in achieving optimal results. Effective threshold optimization ensures that the model's predictions are more accurate and reliable, contributing to overall performance enhancement.\n\nInterestingly, although the least performant model on the evaluation set was the Custom classifier + thresholds, it achieved the best score on the test set. This suggests that the Custom classifier + thresholds might have better generalization capabilities, highlighting the importance of thorough evaluation on diverse test sets.\n\nIn summary, while DDAMFN as a feature extractor performs on par with full model fine-tuning in a multitask setting, the single-task fine-tuning shows that there is room for improvement with targeted optimization strategies in multitask scenarios. This includes better loss function optimization, addressing data imbalance, and refining threshold settings, all of which are crucial for maximizing the performance of the model in the multitask challenge.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows a diagram of the network, which features a base Mobile-",
      "page": 3
    },
    {
      "caption": "Figure 1: Our DDAMFN [34] architecture for the 7th ABAW challenge: MobileFaceNet",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Enhancing Facial Expression Recognition through": "Dual-Direction Attention Mixed Feature"
        },
        {
          "Enhancing Facial Expression Recognition through": "Networks: Application to 7th ABAW Challenge"
        },
        {
          "Enhancing Facial Expression Recognition through": "Josep Cabacas-Maso1, Elena Ortega-Beltrán1, Ismael Benito-Altamirano1,2,"
        },
        {
          "Enhancing Facial Expression Recognition through": "and Carles Ventura1"
        },
        {
          "Enhancing Facial Expression Recognition through": "1\neHealth Center, Faculty of Computer Science, Multimedia and"
        },
        {
          "Enhancing Facial Expression Recognition through": "Telecommunicactions, Universitat Oberta de Catalunya, 08016 Barcelona, Spain"
        },
        {
          "Enhancing Facial Expression Recognition through": "ibenitoal@uoc.edu, cventuraroy@uoc.edu"
        },
        {
          "Enhancing Facial Expression Recognition through": "2 MIND/IN2UB, Department of Electronic and Biomedical Engineeering, Universitat"
        },
        {
          "Enhancing Facial Expression Recognition through": "de Barcelona, 08028 Barcelona, Spain"
        },
        {
          "Enhancing Facial Expression Recognition through": "Abstract. We present our contribution to the 7th ABAW challenge at"
        },
        {
          "Enhancing Facial Expression Recognition through": "ECCV 2024, by utilizing a Dual-Direction Attention Mixed Feature Net-"
        },
        {
          "Enhancing Facial Expression Recognition through": "work (DDAMFN) for multitask facial expression recognition, we achieve"
        },
        {
          "Enhancing Facial Expression Recognition through": "results far beyond the proposed baseline for the Multi-Task ABAW chal-"
        },
        {
          "Enhancing Facial Expression Recognition through": "lenge. Our proposal uses the well-known DDAMFN architecture as base"
        },
        {
          "Enhancing Facial Expression Recognition through": "to effectively predict valence-arousal, emotion recognition, and facial ac-"
        },
        {
          "Enhancing Facial Expression Recognition through": "tion units. We demonstrate the architecture ability to handle these tasks"
        },
        {
          "Enhancing Facial Expression Recognition through": "simultaneously, providing insights into its architecture and the rationale"
        },
        {
          "Enhancing Facial Expression Recognition through": "behind its design. Additionally, we compare our results for a multitask"
        },
        {
          "Enhancing Facial Expression Recognition through": "solution with independent single-task performance."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nJosep Cabacas-Maso et al.": "of emotional expressions across cultures and contexts. Such advancements not"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "only improved the reliability of automated emotion recognition systems but also"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "opened the possibility to personalize affective computing applications\nin fields"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "such as mental health monitoring [26] or user experience design [33]."
        },
        {
          "2\nJosep Cabacas-Maso et al.": "To tackle all\nthese\nchallenges,\nresearchers have\nexplored innovative archi-"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "tectures such as the DDAMFN (Dual-Direction Attention Mixed Feature Net-"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "work) [34]. This novel approach integrates attention mechanisms [6] and mixed"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "feature extraction [3], enhancing the network’s ability to capture intricate details"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "within facial expressions. This architecture shows promising results in multitask"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "challenges, together with other pretrained networks [28]."
        },
        {
          "2\nJosep Cabacas-Maso et al.": "There exists a modern day need to create machines capable to comprehend"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "and appropriately respond to human feelings day-to-day on-the-wild applica-"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "tions. This\nchallenge was presented series of\ncomptetitions\nentitled “Affective"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "Behavior Analysis\nin-the-wild (ABAW)”\nchallenges\n[9–15, 17–20]. For\nthe 7th"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "ABAW challenge, at ECCV 2024,\ntwo competitions where presented: first, a"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "competition focused on solving a multi-task classification,\nfocused on valance-"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "arousal, emotion recognition, and action units, and second, a competition focused"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "on compound expression recognition.\nIn this work, we present our approach"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "to the first competition, where we implemented our multi-task version of\nthe"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "DDAMFN architecture."
        },
        {
          "2\nJosep Cabacas-Maso et al.": "The competition presented a smaller dataset (s-AffWild2) [21] than in pre-"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "vious\nchallenges\n(Aff-Wild2)\n[16]. We proposed a fine-tuning training on the"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "s-AffWild2 dataset\nto increase\nthe performance of\nthe model\nfor a multitask"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "challenge; plus, we evaluated its performance individually on the different task"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "of the challenge: valence-arousal prediction, emotion recognition, and action unit"
        },
        {
          "2\nJosep Cabacas-Maso et al.": "detection."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Note also that we transformed the annotations of expressions and action units",
      "data": [
        {
          "Enhancing Facial Expression Recognition through DDAMFN": "strict criteria to filter out\nthe frames,",
          "3": "removing any frame that contained any"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN": "annotation value outside the specified acceptable range. Specifically, annotation",
          "3": ""
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN": "values of -5 for valence/arousal, -1 for expressions, and -1 for action units (AUs)",
          "3": ""
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN": "were excluded from consideration in the analysis, this is summarized in Table 1.",
          "3": ""
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN": "Note also that we transformed the annotations of expressions and action units",
          "3": ""
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN": "to binary values, where 1 indicates the presence of the expression or action unit,",
          "3": ""
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN": "and 0 indicates the absence of the expression or action unit.",
          "3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "×"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "FCa.u.\nDDA"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "Fig. 1: Our DDAMFN [34] architecture for the 7th ABAW challenge: MobileFaceNet"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "(MFN) for feature extraction (grey), Dual-Direction Attention (DDA) module (green),"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "Global Depthwise Convolution (GDConv) layer (red), and three fully-connected layers"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "for valence-arousal prediction, emotion recognition, and action unit detection (yellow)."
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "2.3\nTraining"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "Initially,\nthe DDAMFN pretrained model was obtained from stock versions\nin"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "the source code repository of the original work, which was trained on AffectNet-"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "8 [34].\nIn both our expriments, we preserved the feature extraction, attention"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "mechanisms and the GDConv layer pretained weights, but initialized the fully-"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "connected layers with random weights."
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "For the first experiment, a multitask training was performed in two different"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "stages. First, we focused on training our custom multitask classifier, so, for train-"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "ing, the entire network was frozen except for the custom classifier. The classifier"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "was trained in isolation to learn the distinct characteristics of each task without"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "interference from the feature extraction layers. Subsequently,\nthe entire model"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "was unfrozen, and fine-tuned on all\nlayers. This comprehensive training stage"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "ensured that\nthe feature extraction, attention mechanisms, and classifier were"
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "all optimized to work cohesively for the multitask problem."
        },
        {
          "Reshape\nFCexp.\nFeatures\nDDA\nMAX\nGDConv\nMFN": "Loss functions were calculated following these criteria:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "all optimized to work cohesively for the multitask problem.": ""
        },
        {
          "all optimized to work cohesively for the multitask problem.": "– For valence-arousal prediction,"
        },
        {
          "all optimized to work cohesively for the multitask problem.": ""
        },
        {
          "all optimized to work cohesively for the multitask problem.": ""
        },
        {
          "all optimized to work cohesively for the multitask problem.": ""
        },
        {
          "all optimized to work cohesively for the multitask problem.": "match the true values)."
        },
        {
          "all optimized to work cohesively for the multitask problem.": ""
        },
        {
          "all optimized to work cohesively for the multitask problem.": ""
        },
        {
          "all optimized to work cohesively for the multitask problem.": ""
        },
        {
          "all optimized to work cohesively for the multitask problem.": ""
        },
        {
          "all optimized to work cohesively for the multitask problem.": ""
        },
        {
          "all optimized to work cohesively for the multitask problem.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Performance metrics for the multitask challenge. Table shows F1 and",
      "data": [
        {
          "for": ""
        },
        {
          "for": "F1AU already normalized by the number of classes for each classification task. CCCAV"
        },
        {
          "for": "is the combined CCC for valence and arousal."
        },
        {
          "for": ""
        },
        {
          "for": ""
        },
        {
          "for": ""
        },
        {
          "for": ""
        },
        {
          "for": "Custom classifier+thresholds"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: Performance metrics for individual tasks. Table shows F1 and F1",
      "data": [
        {
          "6\nJosep Cabacas-Maso et al.": "0.287 and 0.490,\nrespectively, with a P Score of 1.354. Threshold adjustments"
        },
        {
          "6\nJosep Cabacas-Maso et al.": "further enhanced the F1 score for AU detection to 0.529 and the P Score to"
        },
        {
          "6\nJosep Cabacas-Maso et al.": "1.393."
        },
        {
          "6\nJosep Cabacas-Maso et al.": "Training a custom classifier presented a CCC of 0.530 for valence, 0.537 for"
        },
        {
          "6\nJosep Cabacas-Maso et al.": "arousal, and 0.533 for valence-arousal. The F1 scores for emotion classification"
        },
        {
          "6\nJosep Cabacas-Maso et al.": "and AU detection were 0.243 and 0.487, respectively, resulting in a P Score of"
        },
        {
          "6\nJosep Cabacas-Maso et al.": "1.263. Applying threshold adjustments increased the F1 score for AU detection"
        },
        {
          "6\nJosep Cabacas-Maso et al.": "to 0.524 and the P Score to 1.300."
        },
        {
          "6\nJosep Cabacas-Maso et al.": "for\nindividual\ntasks. Table"
        },
        {
          "6\nJosep Cabacas-Maso et al.": "shows F1Expr\nand F1AU"
        },
        {
          "6\nJosep Cabacas-Maso et al.": "already normalized by the number of classes for each classification task. CCCAV"
        },
        {
          "6\nJosep Cabacas-Maso et al.": "combined CCC for valence and arousal."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 4: Performance metrics for individual tasks. Table shows F1 and F1",
      "data": [
        {
          "for": "",
          "individual\ntasks. Table": ""
        },
        {
          "for": "",
          "individual\ntasks. Table": ""
        },
        {
          "for": "combined CCC for valence and arousal.",
          "individual\ntasks. Table": ""
        },
        {
          "for": "",
          "individual\ntasks. Table": "CCCV CCCA CCCV A F1Expr F1AU"
        },
        {
          "for": "",
          "individual\ntasks. Table": "0.604 0.550\n0.577"
        },
        {
          "for": "",
          "individual\ntasks. Table": "0.604 0.550\n0.577"
        },
        {
          "for": "",
          "individual\ntasks. Table": "0.530\n0.537\n0.533"
        },
        {
          "for": "Custom classifier+thresholds",
          "individual\ntasks. Table": "0.530\n0.537\n0.533"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 6: Multi-Task Learning Challenge Results (7th ABAW)",
      "data": [
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": ""
        },
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": "Netease Fuxi AI Lab [23] 0.5420 0.4286 0.5580 1.5286"
        },
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": "0.4074"
        },
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": "0.3783"
        },
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": "0.3743"
        },
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": "0.3710"
        },
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": "0.292"
        },
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": "0.1193"
        },
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": "compared to other"
        },
        {
          "Table 6: Multi-Task Learning Challenge Results (7th ABAW)": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nJosep Cabacas-Maso et al.": "extractors\nto enhance\nthe overall quality of\nthe"
        },
        {
          "8\nJosep Cabacas-Maso et al.": "approach has allowed them to achieve superior results."
        },
        {
          "8\nJosep Cabacas-Maso et al.": ""
        },
        {
          "8\nJosep Cabacas-Maso et al.": "cess, which consequently led to the utilization of a smaller dataset for training"
        },
        {
          "8\nJosep Cabacas-Maso et al.": "purposes compared to our competitors. This reduction in data volume may have"
        },
        {
          "8\nJosep Cabacas-Maso et al.": "impacted our model’s ability to generalize effectively."
        },
        {
          "8\nJosep Cabacas-Maso et al.": ""
        },
        {
          "8\nJosep Cabacas-Maso et al.": "ment of each frame in isolation, without taking into account any temporal rela-"
        },
        {
          "8\nJosep Cabacas-Maso et al.": "tionships or dependencies within the sequence of"
        },
        {
          "8\nJosep Cabacas-Maso et al.": "poral context has posed challenges for the training process, potentially hindering"
        },
        {
          "8\nJosep Cabacas-Maso et al.": "the model’s performance in tasks where temporal dynamics are crucial."
        },
        {
          "8\nJosep Cabacas-Maso et al.": ""
        },
        {
          "8\nJosep Cabacas-Maso et al.": "architecture. Unlike some other teams, which have explored the benefits of en-"
        },
        {
          "8\nJosep Cabacas-Maso et al.": "semble methods or hybrid models incorporating both classical and deep learning"
        },
        {
          "8\nJosep Cabacas-Maso et al.": "techniques, our approach has remained relatively straightforward [22]."
        },
        {
          "8\nJosep Cabacas-Maso et al.": ""
        },
        {
          "8\nJosep Cabacas-Maso et al.": "extraction, data volume, and temporal context consideration have been critical"
        },
        {
          "8\nJosep Cabacas-Maso et al.": "factors influencing our comparative performance."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "model’s predictions are more accurate and reliable, contributing to overall per-"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "formance enhancement."
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "Interestingly, although the least performant model on the evaluation set was"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "the Custom classifier + thresholds, it achieved the best score on the test set. This"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "suggests that the Custom classifier + thresholds might have better generalization"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "capabilities, highlighting the importance of thorough evaluation on diverse test"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "sets."
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "In summary, while DDAMFN as a feature extractor performs on par with full"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "model fine-tuning in a multitask setting, the single-task fine-tuning shows that"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "there is room for improvement with targeted optimization strategies in multitask"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "scenarios. This\nincludes better\nloss\nfunction optimization, addressing data im-"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "balance, and refining threshold settings, all of which are crucial\nfor maximizing"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "the performance of the model\nin the multitask challenge."
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "Acknowledgments"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "This work is part of the project PLEC2021-007868, funded by MCIN and by the"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN."
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n9": "References"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "References"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "1. Baltrusaitis, T., Zadeh, A., Lim, Y., Morency, L.P.: Openface 2.0: Facial behavior"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "analysis toolkit.\nin IEEE Winter Conference on Applications of Computer Vision"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "(WACV) (2018)"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "2. Calvo, M.G., Nummenmaa, L.: Faces and feelings: The universal\nrecognition of"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "emotions. Annu. Rev. Psychol. 67, 451–471 (2016)"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "3. Chen, S., Liu, Y., Gao, X., Han, Z.: Mobilefacenets: Efficient cnns for accurate real-"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "time face verification on mobile devices.\nIn: Biometric Recognition: 13th Chinese"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "Conference, CCBR 2018, Urumqi, China, August 11-12, 2018, Proceedings 13. pp."
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "428–438. Springer (2018)"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "4. Ekman, P.: Darwin and facial expression: A century of research in review. General"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "Psychology 7(2), 121–125 (2003)"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "5. Ekman, P., Friesen, W.V.: Facial action coding system: A technique for the mea-"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "surement of\nfacial movement. Consulting Psychologists Press (1978)"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "6. Hou, Q., Zhou, D., Feng, J.: Coordinate attention for efficient mobile network de-"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "sign. In: Proceedings of the IEEE/CVF conference on computer vision and pattern"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "recognition. pp. 13713–13722 (2021)"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "7. Kaya, H., Gürpinar, E.: Exploring deep convolutional neural networks\nfor\nfacial"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "action unit recognition. Neurocomputing 387, 368–380 (2020)"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "8. Kim, J., Lee, S.C., Kim, J.S., Yoo, C.S.: A survey on facial emotion recognition:"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "Approaches, databases, and challenges. Pattern Recognition Letters 153, 84–92"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "(2023)"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "9. Kollias, D., Schulc, A., Hajiyev, E., Zafeiriou, S.: Analysing affective behavior in"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "the first abaw 2020 competition. In: 2020 15th IEEE International Conference on"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "Automatic Face and Gesture Recognition (FG 2020)(FG). pp. 794–800 (2020)"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "10. Kollias, D.: Abaw: Valence-arousal estimation, expression recognition, action unit"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "detection & multi-task learning challenges. In: Proceedings of the IEEE/CVF Con-"
        },
        {
          "EU, and the project PID2022-138721NB-I00,\nfunded by MCIN.": "ference on Computer Vision and Pattern Recognition. pp. 2328–2336 (2022)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10\nJosep Cabacas-Maso et al.": "11. Kollias, D.: Abaw:\nlearning from synthetic data & multi-task learning challenges."
        },
        {
          "10\nJosep Cabacas-Maso et al.": "In: European Conference on Computer Vision. pp. 157–172. Springer (2023)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "12. Kollias, D.: Multi-label compound expression recognition: C-expr database & net-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "work. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "tern Recognition. pp. 5589–5598 (2023)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "13. Kollias, D., Sharmanska, V., Zafeiriou, S.: Face behavior a la carte: Expressions,"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "affect and action units in a single network. arXiv preprint arXiv:1910.11111 (2019)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "14. Kollias, D., Sharmanska, V., Zafeiriou, S.: Distribution matching\nfor heteroge-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "neous multi-task learning: a large-scale face study. arXiv preprint arXiv:2105.03790"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "(2021)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "15. Kollias, D., Tzirakis, P., Baird, A., Cowen, A., Zafeiriou, S.: Abaw: Valence-arousal"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "estimation, expression recognition, action unit detection & emotional reaction in-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "tensity estimation challenges.\nIn: Proceedings of\nthe\nIEEE/CVF Conference on"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "Computer Vision and Pattern Recognition. pp. 5888–5897 (2023)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "16. Kollias, D., Tzirakis, P., Cowen, A., Zafeiriou, S., Shao, C., Hu, G.: The 6th affective"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "behavior analysis in-the-wild (abaw) competition. arXiv preprint arXiv:2402.19344"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "(2024)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "17. Kollias, D., Tzirakis, P., Nicolaou, M.A., Papaioannou, A., Zhao, G., Schuller,"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "B., Kotsia,\nI., Zafeiriou, S.: Deep affect prediction in-the-wild: Aff-wild database"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "and challenge, deep architectures, and beyond. International Journal of Computer"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "Vision pp. 1–23 (2019)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "18. Kollias, D., Zafeiriou, S.: Expression, affect, action unit\nrecognition: Aff-wild2,"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "multi-task learning and arcface. arXiv preprint arXiv:1910.04855 (2019)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "19. Kollias, D., Zafeiriou, S.: Affect analysis in-the-wild: Valence-arousal, expressions,"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "action units and a unified framework. arXiv preprint arXiv:2103.15792 (2021)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "20. Kollias, D., Zafeiriou, S.: Analysing affective behavior in the second abaw2 compe-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "tition.\nIn: Proceedings of the IEEE/CVF International Conference on Computer"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "Vision. pp. 3652–3660 (2021)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "21. Kollias, D., Zafeiriou, S., Kotsia,\nI., Dhall, A., Ghosh, S., Shao, C., Hu, G.:"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "7th abaw competition: Multi-task learning and compound expression recognition."
        },
        {
          "10\nJosep Cabacas-Maso et al.": "arXiv preprint arXiv:2407.03835 (2024)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "22. Li, X., Du, W., Yang, H.: Affective behavior analysis using task-adaptive and au-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "assisted graph network (2024), https://arxiv.org/abs/2407.11663"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "23. Liu, C., Zhang, W., Qiu, F., Li, L., Yu, X.: Affective behaviour analysis via pro-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "gressive learning (2024), https://arxiv.org/abs/2407.16945"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "24. Liu, X., Song, X., Tan, Z.H.: Deep learning for emotion recognition: A comprehen-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "sive review. Neurocomputing 440, 167–180 (2021)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "25. Lucey, P., Cohn, J.F., Kanade, T., Saragih, J., Ambadar, Z., Matthews, I.: The ex-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "tended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "specified expression.\nin IEEE Conference on Computer Vision and Pattern Recog-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "nition Workshops (CVPRW) (2010)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "26. Picard, R.W., Vyzas, E., Healey, J.: Toward machine emotional\nintelligence: Anal-"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "ysis of affective physiological states.\nIEEE Transactions on Pattern Analysis and"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "Machine Intelligence 23(10), 1175–1191 (2020)"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "27. Savchenko, A.V.: Frame-level prediction of\nfacial expressions, valence, arousal and"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "action units for mobile devices (2022), https://arxiv.org/abs/2203.13436"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "28. Savchenko, A.V.: Hsemotion team at the 6th abaw competition: Facial expressions,"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "valence-arousal and emotion intensity prediction (2024), https://arxiv.org/abs/"
        },
        {
          "10\nJosep Cabacas-Maso et al.": "2403.11590"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "29. Savchenko, A.V.: Hsemotion team at\nthe 7th abaw challenge: Multi-task learn-"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "ing and compound facial expression recognition (2024), https://arxiv.org/abs/"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "2407.13184"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "30. Shen, K., Liu, X., Wang, B., Yao, J., Liu, X., Guan, Y., Wang, Y., Li, G., Sun, X.:"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "Facial affect recognition based on multi architecture encoder and feature fusion for"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "the abaw7 challenge (2024), https://arxiv.org/abs/2407.12258"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "31. Soleymani, M., Garcia, D., Jou, B., Schuller, B.: Deep learning for affective com-"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "puting: Text-based emotion recognition in decision support. IEEE Transactions on"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "Affective Computing 8(1), 17–37 (2017)"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "32. Zafeiriou, S., Kollias, D., Nicolaou, M.A., Papaioannou, A., Zhao, G., Kotsia,\nI.:"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "Aff-wild: Valence and arousal\n‘in-the-wild’challenge. In: Computer Vision and Pat-"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "tern Recognition Workshops (CVPRW), 2017 IEEE Conference on. pp. 1980–1987."
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "IEEE (2017)"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "33. Zeng, Z., Pantic, M., Roisman, G.I., Huang, T.S.: A survey of affect\nrecognition"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "methods: Audio, visual, and spontaneous expressions. IEEE Transactions on Pat-"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "tern Analysis and Machine Intelligence 31(1), 39–58 (2019)"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "34. Zhang, S., Zhang, Y., Zhang, Y., Wang, Y., Song, Z.: A dual-direction attention"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "3595\nmixed feature network for facial expression recognition. Electronics 12(17),"
        },
        {
          "Enhancing Facial Expression Recognition through DDAMFN\n11": "(2023)"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L Morency"
      ],
      "year": "2018",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV"
    },
    {
      "citation_id": "2",
      "title": "Faces and feelings: The universal recognition of emotions",
      "authors": [
        "M Calvo",
        "L Nummenmaa"
      ],
      "year": "2016",
      "venue": "Annu. Rev. Psychol"
    },
    {
      "citation_id": "3",
      "title": "Mobilefacenets: Efficient cnns for accurate realtime face verification on mobile devices",
      "authors": [
        "S Chen",
        "Y Liu",
        "X Gao",
        "Z Han"
      ],
      "year": "2018",
      "venue": "Biometric Recognition: 13th Chinese Conference"
    },
    {
      "citation_id": "4",
      "title": "Darwin and facial expression: A century of research in review",
      "authors": [
        "P Ekman"
      ],
      "year": "2003",
      "venue": "General Psychology"
    },
    {
      "citation_id": "5",
      "title": "Facial action coding system: A technique for the measurement of facial movement",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Facial action coding system: A technique for the measurement of facial movement"
    },
    {
      "citation_id": "6",
      "title": "Coordinate attention for efficient mobile network design",
      "authors": [
        "Q Hou",
        "D Zhou",
        "J Feng"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "7",
      "title": "Exploring deep convolutional neural networks for facial action unit recognition",
      "authors": [
        "H Kaya",
        "E Gürpinar"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "8",
      "title": "A survey on facial emotion recognition: Approaches, databases, and challenges",
      "authors": [
        "J Kim",
        "S Lee",
        "J Kim",
        "C Yoo"
      ],
      "year": "2023",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "9",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "10",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "11",
      "title": "Abaw: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "12",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "D Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "14",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "15",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Cowen",
        "S Zafeiriou",
        "C Shao",
        "G Hu"
      ],
      "year": "2024",
      "venue": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "arxiv": "arXiv:2402.19344"
    },
    {
      "citation_id": "17",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "19",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "20",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "D Kollias",
        "S Zafeiriou",
        "I Kotsia",
        "A Dhall",
        "S Ghosh",
        "C Shao",
        "G Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "22",
      "title": "Affective behavior analysis using task-adaptive and auassisted graph network",
      "authors": [
        "X Li",
        "W Du",
        "H Yang"
      ],
      "year": "2024",
      "venue": "Affective behavior analysis using task-adaptive and auassisted graph network"
    },
    {
      "citation_id": "23",
      "title": "Affective behaviour analysis via progressive learning",
      "authors": [
        "C Liu",
        "W Zhang",
        "F Qiu",
        "L Li",
        "X Yu"
      ],
      "year": "2024",
      "venue": "Affective behaviour analysis via progressive learning"
    },
    {
      "citation_id": "24",
      "title": "Deep learning for emotion recognition: A comprehensive review",
      "authors": [
        "X Liu",
        "X Song",
        "Z Tan"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "25",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "26",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological states",
      "authors": [
        "R Picard",
        "E Vyzas",
        "J Healey"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "authors": [
        "A Savchenko"
      ],
      "year": "2022",
      "venue": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices"
    },
    {
      "citation_id": "28",
      "title": "Hsemotion team at the 6th abaw competition: Facial expressions, valence-arousal and emotion intensity prediction",
      "authors": [
        "A Savchenko"
      ],
      "year": "2024",
      "venue": "Hsemotion team at the 6th abaw competition: Facial expressions, valence-arousal and emotion intensity prediction"
    },
    {
      "citation_id": "29",
      "title": "Hsemotion team at the 7th abaw challenge: Multi-task learning and compound facial expression recognition",
      "authors": [
        "A Savchenko"
      ],
      "year": "2024",
      "venue": "Hsemotion team at the 7th abaw challenge: Multi-task learning and compound facial expression recognition"
    },
    {
      "citation_id": "30",
      "title": "Facial affect recognition based on multi architecture encoder and feature fusion for the abaw7 challenge",
      "authors": [
        "K Shen",
        "X Liu",
        "B Wang",
        "J Yao",
        "X Liu",
        "Y Guan",
        "Y Wang",
        "G Li",
        "X Sun"
      ],
      "year": "2024",
      "venue": "Facial affect recognition based on multi architecture encoder and feature fusion for the abaw7 challenge"
    },
    {
      "citation_id": "31",
      "title": "Deep learning for affective computing: Text-based emotion recognition in decision support",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "32",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "33",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "34",
      "title": "A dual-direction attention mixed feature network for facial expression recognition",
      "authors": [
        "S Zhang",
        "Y Zhang",
        "Y Zhang",
        "Y Wang",
        "Z Song"
      ],
      "year": "2023",
      "venue": "Electronics"
    }
  ]
}