{
  "paper_id": "2404.12260v1",
  "title": "Alleviating Catastrophic Forgetting In Facial Expression Recognition With Emotion-Centered Models",
  "published": "2024-04-18T15:28:34Z",
  "authors": [
    "Israel A. Laurensi",
    "Alceu de Souza Britto Jr.",
    "Jean Paul Barddal",
    "Alessandro Lameiras Koerich"
  ],
  "keywords": [
    "Facial expression recognition",
    "CNN",
    "Catastrophic forgetting",
    "Pseudo-rehearsal",
    "Regularization"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial expression recognition is a pivotal component in machine learning, facilitating various applications. However, convolutional neural networks (CNNs) are often plagued by catastrophic forgetting, impeding their adaptability. The proposed method, emotion-centered generative replay (ECgr), tackles this challenge by integrating synthetic images from generative adversarial networks. Moreover, ECgr incorporates a quality assurance algorithm to ensure the fidelity of generated images. This dual approach enables CNNs to retain past knowledge while learning new tasks, enhancing their performance in emotion recognition. The experimental results on four diverse facial expression datasets demonstrate that incorporating images generated by our pseudo-rehearsal method enhances training on the targeted dataset and the source dataset while making the CNN retain previously learned knowledge.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions are essential in human interaction and comprehension. In such a context, facial expressions play an important role  [14] . Thus, facial expression recognition (FER) is the functionality of numerous machine learning applications, including emotion-aware interfaces, personalized recommender systems, and human-robot interaction. One way to identify these emotions in complex systems is through the use of convolutional neural networks (CNNs). These networks have achieved remarkable success in computer vision tasks such as image classification, object detection, and facial expression recognition. However, a significant limitation of CNNs is their susceptibility to catastrophic forgetting. When sequentially trained on different tasks or datasets, CNNs often struggle to retain previously learned information, which leads to degraded performance on previously mastered tasks. This phenomenon impairs the practical application of CNNs in dynamic environments where models must continuously adapt to new data while retaining accuracy in the previous scenarios.\n\nEvaluating the catastrophic forgetting problem in FER -a complex learning scenario -allows us to observe the proposed method's ability to deal with datasets composed of diverse emotional expressions, unlike more straightforward tasks with more limited patterns. Moreover, such an evaluation sheds light on the model's adeptness at maintaining previously learned emotional recognition performance while assimilating the changes of a new domain, showing faces collected with other acquisition protocols, and representing people with different characteristics and cultures.\n\nCatastrophic forgetting arises due to CNN's optimization process, which tends to adjust the model's parameters to fit the current task, often overshadowing previously acquired representations. Numerous approaches have been proposed to mitigate catastrophic forgetting, including regularization techniques, dynamic neural network architecture, and rehearsal-based methods  [6, 7, 11, 12, 15] . Furthermore, several literature reviews have been published in this research field, as well as in continual learning, offering comprehensive insights into the state-ofthe-art methodologies, best practices, and emerging trends in mitigating catastrophic forgetting and advancing continual learning algorithms  [5, 10, 13] . While these state-of-the-art methodologies have demonstrated promising results in specific scenarios, they have limitations such as increased computational complexity or limited capacity to effectively retain information from past tasks, especially in facial expression recognition scenarios.\n\nIn this paper, we propose a novel approach to overcome the limitations of existing methods and effectively address catastrophic forgetting in CNNs when applied to facial expression recognition. Our approach capitalizes on generative adversarial networks (GANs) capabilities to generate synthetic samples that resemble the original training data. Incorporating these synthetic samples during training enables the CNN to re-learn and retain knowledge from previous tasks, thereby mitigating catastrophic forgetting. To achieve this, we generated synthetic images of each emotion (class) present in the datasets, aiming to better capture the intrinsic characteristics of each facial expression associated with human emotion. We refer to this method as emotion-centered generative replay (ECgr). Moreover, we introduce a quality assurance (QA) algorithm as a crucial component of our approach. The QA algorithm assesses the generated synthetic samples based on the CNN's original classification accuracy. Only the high-quality synthetic samples, which the original CNN can accurately classify, are retained for training. This filtering step ensures that only superior generated samples are utilized, thus augmenting the performance of the proposed method. In addition, we weigh the importance of the synthetic images, considering the CNN output score as an image quality assignment. Such a weight penalizes images that have been assigned a low confidence value by the CNN, which might positively influence the training convergence, as these images may be considered detrimental to the adaptation to the new dataset.\n\nOur hypothesis centers around the effectiveness of employing a pseudo-rehearsal method: H1) the utilization of a pseudo-rehearsal method, particularly our emotion-centered generative replay, offers a potential solution for memory decay in CNNs; H2) the fusion of our emotion-centered generative replay and the proposed QA algorithm offers a promising strategy to counteract memory decay within neural networks; and H3) the combination of emotion-centered generative replay, QA, and a weighted loss function is hypothesized to further strengthen memory retention and performance in neural networks, potentially surpassing the benefits of either ECgr or QA alone. To assess the proposed method's efficiency and validate our hypothesis, we undertook facial expression recognition experiments across various emotion datasets and employed diverse training methodologies.\n\nThe contribution of this work is three-fold: i) a new pseudo-rehearsal method focused on the emotions to mitigate catastrophic forgetting when learning facial emotion recognition; ii) a loss function considering a penalization schema for lowquality synthetic images generated in the pseudo-rehearsal strategy; iii) a robust experimental protocol considering well-known FER datasets and a pipeline of experiments to discuss the contributions of the proposed emotion-centered generative replay in mitigating catastrophic forgetting when compared to a regular fine-tuning process or the possibility of joining datasets.\n\nThe remainder of this paper is structured as follows: Section 2 reviews related work on catastrophic forgetting and existing methods for its mitigation. Section 3 presents the proposed emotion-centered generative replay approach and outlines the architecture of the QA algorithm. Section 4 describes the experimental setup and presents the results of our comprehensive evaluations. Section 5 discusses the implications of our findings, and Section 6 concludes the paper, outlining potential directions for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Catastrophic forgetting has spurred numerous research works to minimize its effects. In this section, we explore prominent algorithms and insights inspired by neuropsychology, all aimed at addressing forgetting and improving memory retention within neural networks.\n\nLearning without forgetting (LWF) stands out by employing knowledge distillation  [7] . This technique transfers distilled knowledge from a model trained on prior tasks to a new model, thereby allowing the assimilation of new information while safeguarding the retention of old information. This intelligent utilization of previous knowledge effectively counteracts the plague of forgetting and amplifies the network's overall performance. Another regularization method, elastic weight consolidation (EWC)  [6] , introduces a nuanced regularization term in the scenario. This term identifies and assigns significance to pivotal network parameters linked to previous tasks, penalizing alterations to these parameters during subsequent training phases. By preserving these key parameters diligently, EWC balances between accommodating novel tasks and upholding the wisdom derived from past experiences.\n\nSynaptic intelligence (SI)  [15]  offers an innovative perspective that stems from evaluating past task performance and assigns weight to synaptic connec-tions based on their influence. The more a synapse contributes, the higher its importance; in contrast, less influential synapses are assigned lower importance. By preserving these critical connections, SI bridges the gap between old and new information, thus mitigating forgetting while embracing novelty.\n\nDeep generative replay (DGR)  [11]  utilizes generative models to create simulated instances from prior tasks during training. This approach effectively enriches the current task's dataset. The augmented instances, fused with real-time data, offer the network a diverse and comprehensive pool of examples. With past knowledge seamlessly integrated, DGR effectively combats the erosion of previously gained insights, presenting itself as a powerful tool for memory retention.\n\nBeyond these algorithms, insights obtained from neuropsychological research paint a broader picture. Investigations into context-dependent learning have illuminated the crucial role of training and testing contexts in determining network performance. Thus, using contextual cues, algorithms can be designed to exploit the training and testing context better, thereby enhancing memory retention while countering the forgetting phenomenon  [10] .\n\nThe complementary learning system (CLS) hypothesis, which posits dual learning and memory systems within the human brain, further contributes to our understanding. The hippocampal-based system swiftly acquires new knowledge, while the neocortical system excels at long-term storage and retrieval. Incorporating this framework into neural network architectures yields models that deftly balance plasticity and stability  [10] . By synergizing the strengths of both systems, these models expertly deal with catastrophic forgetting while nurturing the growth of consolidated knowledge.\n\nIn light of these contributions, it is crucial to contextualize our work within the broader realm of the current state-of-the-art. The proposed methodology harmonizes the concepts of emotion-centered generative replay and QA. With CNNs as the focal point, our approach aims to prevent catastrophic forgetting in facial expression recognition, a domain where precise emotion identification relies heavily on image quality.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "In this section, we describe the methodology employed in our study to address the challenges of catastrophic forgetting in facial emotion recognition tasks. Our approach combines emotion-centered generative replay using a Wasserstein generative adversarial network with gradient penalty (WGAN-GP) and a QA algorithm. Fig.  1  presents a general overview of the proposed method.\n\nThe use of WGAN-GPs is attributed to the stable learning power of these networks, a factor crucial when dealing with catastrophic forgetting. After all, attempting to address this issue through training and employing a generative method may lead to catastrophic forgetting in the generative networks itself. WGAN-GPs  [4]  implement a penalty on the gradient norm during training and optimization of the WGAN  [2] , thereby ensuring more stable training and yielding higher-quality generated images. We have formalized our methodology using algorithmic representations to provide a more concrete understanding of the theoretical concepts presented. In Subsection 3.2, we provide detailed algorithms replicating our approach's offline preparation and training stages. These algorithms encapsulate the step-by-step processes of generating synthetic images and performing continual retraining.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Emotion-Centered Generative Replay With Wgan-Gp And Quality Assurance",
      "text": "We initiate by training a set of WGAN-GPs, one for each of the seven emotion classes present in the 'source' dataset -fear, anger, happiness, sadness, disgust, surprise, and neutral. Using these trained WGAN-GPs, we generate augmented datasets for each class. These generated images capture the intricate details of respective emotions, diversifying the training data for better generalization.\n\nThe WGAN-GP is built by two different networks: discriminator and generator. The discriminator network is crucial in distinguishing between real and synthetic images. It consists of several layers, including convolutional layers with leaky rectified linear united (ReLU) activation functions. These layers help the discriminator extract relevant features from input images. Additionally, dropout layers are applied to prevent overfitting. The discriminator network's output layer produces a single scalar value, representing the likelihood that the input image is a real sample. This network contains approximately 4.3 million trainable parameters, making it a powerful discriminator for the WGAN-GP.\n\nThe generator network is responsible for generating synthetic images. It starts with an input layer that takes random noise as input. This noise is passed through several layers, including dense, batch normalization, and convolutional layers, followed by leaky ReLU activations. These layers progressively upscale and refine the feature maps to generate higher-resolution images. The generator's output shape matches the desired image size. With around 1.6 million total parameters, the generator network creates convincing synthetic images to fool the discriminator.\n\nThe generator strives to produce indistinguishable images from real ones, while the discriminator aims to classify them correctly. This adversarial process leads to the training of both networks and ultimately results in the generation of high-quality synthetic data.\n\nWe employ our QA algorithm to ensure the quality of the generated images. The QA algorithm filters out low-quality or incorrect images generated by the WGAN-GP, retaining only high-quality images that the original classifier can correctly classify. The QA process is performed using the CNN trained on the source dataset. This network filters the good and bad images from the synthetic datasets. Given an empirically defined threshold, the images correctly classified by the network are used for future retraining, and the misclassified images are discarded. The QA process enhances the reliability of the emotion-centered generative replay, preventing the classifier from being influenced by poor-quality or misleading synthetic images. These images are then integrated into an improved dataset, which merges the synthetic images with the initial source data.\n\nDuring retraining, the new dataset and the target dataset are employed. This unified dataset facilitates CNN training, where knowledge from the original emotion classes combines with the new target emotions, minimizing forgetting.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "General Pipeline",
      "text": "To address the challenge of catastrophic forgetting, our proposed approach involves a two-stage process: offline preparation and a training phase.\n\nOffline preparation stage Initialization occurs as depicted in Algorithm 1, where a set of datasets represented by T is defined, encompassing datasets A, B, C, and D. Each dataset d t within T is traversed through an iterative process. For each specific dataset d t , a classifier, denoted as C dt , is trained using that particular dataset. The CNN used in the experiments comprises several layers, starting with 2D convolutional layers with 64 filters each. Batch normalization stabilizes and accelerates training, followed by additional convolutional layers with varying filter sizes. Max-pooling layers are introduced to downsample the feature maps, reducing the spatial dimensions and focusing on the most salient features. The network continues with more convolutional layers and batch normalization to extract higher-level representations from the input data. Eventually, the feature maps are flattened to form a one-dimensional vector, which is then passed through fully connected layers. Dropout layers are employed to prevent overfitting, enhancing the model's generalization. The final layer is dense with a softmax activation function, producing output probabilities for different classes. In total, this CNN architecture contains approximately 19.3 million parameters.",
      "page_start": 6,
      "page_end": 8
    },
    {
      "section_name": "Algorithm 1 Offline Stage",
      "text": "The training aims to optimize the Eq. (  1 ), where a weight w is applied to each prediction. This weight is determined by the CNN's confidence percentage when predicting for all y pred . L i (y\n\nIn summary, our general pipeline encompasses an offline preparation phase involving training WGAN-GPs and QA-based synthetic image generation. In the training stage, augmented and original datasets are combined, and the continual retraining approach adapts the classifier to multiple datasets while incorporating different strategies.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "To evaluate the performance of our methodology, we utilize several datasets that contain human facial images displaying various emotions. The datasets considered in our study include TFEID, MUG, CK+, and JAFFE. These datasets provide a diverse set of emotional contexts, allowing us to assess our approach's robustness and generalization capabilities. All datasets have the following classes: fear, anger, happiness, sadness, disgust, surprise, and neutral.\n\nThe multimodal understanding group (MUG)  [1]  dataset consists of approximately 1462 facial images, each annotated with the corresponding facial expression labels. The Japanese female facial expression (JAFFE)  [9]  dataset, despite its relatively small size, containing approximately 213 facial images, is valuable for evaluating and comparing facial expression recognition models. The Taiwanese facial expression image database (TFEID)  [3]  provides a suitable testbed for evaluating emotion recognition algorithms, with 1128 samples. Lastly, the extended Cohn-Kanade dataset (CK+)  [8]  dataset is commonly used for facial expression recognition research. It includes a substantial number of facial images, compiled into 123 videos of different subjects, totaling approximately 593 videos, with 327 labeled videos covering various emotional expressions.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "This section offers an in-depth analysis of the outcomes achieved by employing different retraining strategies, each suited to minimize memory degradation and maximize knowledge retention. Fig.  2 : Result samples on different classes from the MUG, JAFFE, and TFEID's synthetic dataset generated by the WGAN-GP. The first column (in green) displays the original samples from the MUG, TFEID, and JAFFE datasets from top to bottom, respectively. In contrast, the second-to-last column (in orange) features the corresponding synthetic images for each dataset.\n\nOn Quality of Synthetic Images In addition to quantitative assessments, we also conducted a qualitative analysis of the synthetic images generated through our approach. In this subsection, we present a comprehensive discussion of the results, including a thorough examination of the qualitative aspects of the synthetic data. This analysis provides valuable insights into the visual fidelity and relevance of the generated images. As shown in Fig.  2 , the left side features an image from the original dataset as a reference for the dataset's inherent visual characteristics. On the right side, seven columns display synthetic images generated for each class within the dataset. These columns showcase the diversity and fidelity of the synthetic samples produced by our ECgr approach.\n\nThe results obtained using QA filtering can also be evaluated qualitatively. In Fig.  3 , we can see some examples of images that were rejected during the QA process. These images are of low quality and do not demonstrate emotion; therefore, the CNN could not classify them correctly. Fig.  3 : Some rejected samples identified by the QA algorithm from the synthetic datasets of MUG, JAFFE, and TFEID.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "On Continual Learning",
      "text": "In this section, we will discuss the main results observed from the tests conducted with facial expression datasets, utilizing the combination of different methods outlined in this study.\n\nInitially, a CNN was trained on the MUG dataset, and subsequently, we employed this CNN to adapt it to other datasets continuously. All trainings were conducted with 20 replications. In methods involving image generation, each experiment replication utilizes images explicitly generated for that retraining; in other words, synthetic datasets are not identical across different replications of CNN adaptation.\n\nIn Tables  1, 2 , and 3, the columns baseline, joint, and fine-tune represent, respectively: testing datasets with the CNN trained on the source dataset; adapting the CNN trained on the source plus target dataset; adapting the CNN trained on the source dataset using only the target dataset. Additionally, the ECgr and QA methods were separately evaluated and combined to ascertain the impact of using synthetic image filtering in continuous training. Furthermore, this scenario assessed whether using weights on synthetic images has any effect compared to training without this technique.\n\nIn Table  1 , it is possible to observe the results when adapting the CNN trained on the MUG dataset (source) to the JAFFE dataset (target). Taking into account the baseline, joint, and fine-tune methods, we can assume that the upper limit is the joint method, which represents the ideal case where all datasets are available for training, and the lower limit is the fine-tune method, in which the source dataset is no longer available. It can be concluded that the combined method ECgr+wQA yielded the best results in this initial adaptation involving only one dataset, with a result very close to joint training. In Tables  2  Table  1 : Results on MUG's model fine-tuned to JAFFE dataset in terms of ECgr, QA, weighted QA, and the combination of ECgr with QA and wQA, alongside with fine-tune, joint, and current for a direct comparison. and 3, we will notice a change in this scenario, as more datasets are introduced in continuous training, the ECgr+QA method tends to outperform. Regarding the result obtained in the retraining for the JAFFE dataset, it is possible to justify this outcome, where the combined method came close to the joint method, as the adaptation can still be considered trivial since only one dataset is being adapted. Thus, the complexity for the CNN to assimilate synthetic images remains low. Table  2  shows that the best result lies between ECgr+QA and ECgr+wQA. Interestingly, in all results, the generative method -combined or not -performed equally or better on the target dataset when compared to joint training. This reveals that synthetic images not only aid the CNN in recalling something it has already seen but also assist in training for new data, reinforcing knowledge when adapting to the same context, in this case, facial expression recognition.\n\nIn Table  3 , we can observe a behavior similar to that observed when adapting to TFEID, where the best result lies between the ECgr+QA and ECgr+wQA methods. However, at this point, it becomes more apparent that using a weight for synthetic images brings an intrinsic problem to the training of the CNN being used for the filtering method. This CNN can carry certain behaviors into subsequent training steps, where errors from certain classes may compromise the entire training when using the confidence percentage.\n\nWe can better understand the results in the MUG dataset from the continuous training of all datasets with Fig.  4 . It is noticeable that the best method for the MUG dataset is ECgr+QA. We can also observe the poor performance of the fine-tuning method in the context of continuous training, where the knowl-   In this same scenario, we can observe the results for JAFFE and TFEID datasets as the original CNN -trained on the MUG dataset -is adapted to the new datasets. In these cases, the baseline is considered the test result obtained from fine-tuning the CNN retrained for the dataset for which the results are reported. Figs. 5a and 5b demonstrate these results. We do not report the results of the CK+ dataset in this scenario because it is the last dataset in the continuous training. Therefore, it does not have subsequent results after its retraining. Please refer to Table  3  for CK+ results.  Given that synthetic images for each class are generated independently in our method, it is essential to examine class-specific memory loss. Figure  6  compares the fine-tune and ECgr+QA methods, revealing subpar performance (F1 < 0.6) for the anger and disgust classes. The ECgr+QA method also experiences performance deterioration for the fear class during the final retraining step. This underscores the difficulty associated with training these classes, as even minor facial changes can be misinterpreted as another emotional state.",
      "page_start": 10,
      "page_end": 13
    },
    {
      "section_name": "Discussion",
      "text": "Firstly, the results support our hypothesis regarding using pseudo-rehearsal methods, specifically emotion-centered generative replay, to minimize memory decay. Our strategy demonstrated remarkable efficacy in alleviating catastrophic forgetting, consistently outperforming the fine-tuning methods across various tasks. The generation of synthetic data resembling past task patterns through WGAN-GPs proved positive in enabling the network to retain knowledge without using original data. This substantiates our anticipation that pseudo-rehearsal techniques, particularly our emotion-centered generative replay, play an important role in counteracting memory decay.\n\nFurthermore, synthesizing our WGAN-GP class-driven generative and QA methods substantiates our second hypothesis. Introducing a QA mechanism during replay significantly improved the quality of synthetic data, further augmenting the approach's effectiveness. The third hypothesis, in which applying a weight to synthetic images would benefit continuous training, can only be observed as positive in the retraining for the first dataset -from MUG to JAFFE. We observed that this technique was ineffective for more datasets subsequent to JAFFE. This may be directly related to the errors of the network that assigns these weights to the synthetic images -that is, the network may be making errors with high confidence, and this negatively affects the synthetic images, which in turn are not fully considered in the retraining, leading the CNN to not 'remember' these data.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Conclusion",
      "text": "In this study, we presented a comprehensive investigation into the challenge of catastrophic forgetting in CNNs within the context of facial expression recognition, proposing a novel approach to mitigate its effects. We employed a pseudorehearsal method, specifically our emotion-centered generative replay (ECgr) with WGAN-GPs, to generate synthetic images for each class of the datasets and combined this with a filtering method to exclude images that could hinder retraining. Across various tasks, ECgr consistently demonstrated superior performance compared to baseline and fine-tuned methods. Utilizing WGAN-GPs to synthesize task-specific data, along with our QA algorithm, resulted in substantial knowledge retention. This confirms the potential of pseudo-rehearsal methods to effectively retrain CNNs without revisiting original datasets, offering a promising strategy for addressing memory decay, particularly in challenging scenarios like facial expression recognition.\n\nDespite promising results with pseudo-rehearsal, its effectiveness may vary across network architectures, datasets, and tasks. Additionally, WGAN-GPbased data generation can be computationally expensive, limiting real-time use. These aspects highlight opportunities for future research, such as improved weight assignment algorithms and exploration of regularization techniques' synergy with pseudo-rehearsal approaches.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: presents a general overview of the proposed method.",
      "page": 4
    },
    {
      "caption": "Figure 1: An overview of the proposed method reveals two key components. At",
      "page": 5
    },
    {
      "caption": "Figure 2: Result samples on different classes from the MUG, JAFFE, and TFEID’s",
      "page": 9
    },
    {
      "caption": "Figure 2: , the left side features an",
      "page": 9
    },
    {
      "caption": "Figure 3: , we can see some examples of images that were rejected during the",
      "page": 9
    },
    {
      "caption": "Figure 3: Some rejected samples identified by the QA algorithm from the synthetic",
      "page": 10
    },
    {
      "caption": "Figure 4: It is noticeable that the best method for",
      "page": 11
    },
    {
      "caption": "Figure 4: Accuracy results on the MUG dataset, showcasing the continuous adap-",
      "page": 12
    },
    {
      "caption": "Figure 5: Accuracy comparison for the JAFFE and TFEID datasets.",
      "page": 13
    },
    {
      "caption": "Figure 6: Comparison of F1 scores by class on the MUG dataset between fine-tune",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "2 École de Technologie Supérieure (ÉTS), Montréal, Canada"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "Abstract. Facial expression recognition is a pivotal component in ma-"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "chine learning,\nfacilitating various applications. However, convolutional"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "neural networks (CNNs) are often plagued by catastrophic forgetting, im-"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "peding their adaptability. The proposed method, emotion-centered gen-"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "erative replay (ECgr), tackles this challenge by integrating synthetic im-"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "ages from generative adversarial networks. Moreover, ECgr incorporates"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "a quality assurance algorithm to ensure the fidelity of generated images."
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "This dual approach enables CNNs to retain past knowledge while learn-"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "ing new tasks, enhancing their performance in emotion recognition. The"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "experimental\nresults on four diverse facial expression datasets demon-"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "strate that incorporating images generated by our pseudo-rehearsal meth-"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "od enhances training on the targeted dataset and the source dataset while"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "making the CNN retain previously learned knowledge."
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "Keywords: Facial expression recognition · CNN · Catastrophic forget-"
        },
        {
          "Paraná (PUCPR), Curitiba, PR, Brazil": "ting · Pseudo-rehearsal\n· Regularization"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ting · Pseudo-rehearsal\n· Regularization": "1\nIntroduction"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "Emotions\nare\nessential"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "context,\nfacial expressions play an important"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "recognition (FER)\nis"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "tions,\nincluding emotion-aware"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "and human-robot"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "systems is through the use of convolutional neural networks (CNNs). These net-"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "works have achieved remarkable success in computer vision tasks such as image"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "classification, object detection, and facial"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "significant limitation of CNNs is their susceptibility to catastrophic forgetting."
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "When sequentially trained on different tasks or datasets, CNNs often struggle to"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "retain previously learned information, which leads to degraded performance on"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "previously mastered tasks. This phenomenon impairs the practical application"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "of CNNs"
        },
        {
          "ting · Pseudo-rehearsal\n· Regularization": "new data while retaining accuracy in the previous scenarios."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Evaluating the catastrophic forgetting problem in FER - a complex learn-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ing scenario - allows us\nto observe the proposed method’s ability to deal with"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "datasets composed of diverse emotional expressions, unlike more straightforward"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "tasks with more limited patterns. Moreover, such an evaluation sheds light on"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "the model’s adeptness at maintaining previously learned emotional recognition"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "performance while assimilating the changes of a new domain, showing faces col-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "lected with other acquisition protocols, and representing people with different"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "characteristics and cultures."
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Catastrophic\nforgetting\narises due\nto CNN’s\noptimization process, which"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "tends to adjust the model’s parameters to fit the current task, often overshad-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "owing previously acquired representations. Numerous approaches have been pro-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "posed to mitigate catastrophic forgetting,\nincluding regularization techniques,"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "dynamic neural network architecture, and rehearsal-based methods [6,7,11,12,15]."
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Furthermore, several literature reviews have been published in this research field,"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "as well as in continual learning, offering comprehensive insights into the state-of-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "the-art methodologies, best practices, and emerging trends in mitigating catas-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "trophic forgetting and advancing continual\nlearning algorithms [5,10,13]. While"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "these state-of-the-art methodologies have demonstrated promising results in spe-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "cific scenarios, they have limitations such as increased computational complexity"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "or limited capacity to effectively retain information from past tasks, especially"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "in facial expression recognition scenarios."
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "In this paper, we propose a novel approach to overcome the limitations of"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "existing methods and effectively address catastrophic forgetting in CNNs when"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "applied to facial expression recognition. Our approach capitalizes on generative"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "adversarial networks (GANs) capabilities to generate synthetic samples that re-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "semble the original training data. Incorporating these synthetic samples during"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "training enables the CNN to re-learn and retain knowledge from previous tasks,"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "thereby mitigating catastrophic forgetting. To achieve this, we generated syn-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "thetic images of each emotion (class) present in the datasets, aiming to better"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "capture\nthe\nintrinsic\ncharacteristics of\neach facial\nexpression associated with"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "human emotion. We\nrefer\nto this method as\nemotion-centered generative\nre-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "play (ECgr). Moreover, we introduce a quality assurance (QA) algorithm as a"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "crucial component of our approach. The QA algorithm assesses\nthe generated"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "synthetic samples based on the CNN’s original classification accuracy. Only the"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "high-quality synthetic samples, which the original CNN can accurately classify,"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "are retained for training. This filtering step ensures that only superior generated"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "samples are utilized, thus augmenting the performance of the proposed method."
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "In addition, we weigh the importance of\nthe synthetic images, considering the"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "CNN output score as an image quality assignment. Such a weight penalizes im-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ages that have been assigned a low confidence value by the CNN, which might"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "positively influence the training convergence, as these images may be considered"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "detrimental to the adaptation to the new dataset."
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Our hypothesis centers around the effectiveness of employing a pseudo-re-"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "hearsal method: H1) the utilization of a pseudo-rehearsal method, particularly"
        },
        {
          "2\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "our emotion-centered generative replay, offers a potential\nsolution for memory"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "decay in CNNs; H2)\nthe fusion of our emotion-centered generative replay and"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "the proposed QA algorithm offers a promising strategy to counteract memory"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "decay within neural networks; and H3)\nthe\ncombination of\nemotion-centered"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "generative replay, QA, and a weighted loss function is hypothesized to further"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "strengthen memory retention and performance in neural networks, potentially"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "surpassing the benefits of\neither ECgr or QA alone. To assess\nthe proposed"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "method’s efficiency and validate our hypothesis, we undertook facial expression"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "recognition experiments across various emotion datasets and employed diverse"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "training methodologies."
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "The contribution of this work is three-fold: i) a new pseudo-rehearsal method"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "focused on the emotions to mitigate catastrophic forgetting when learning facial"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "emotion recognition; ii) a loss function considering a penalization schema for low-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "quality synthetic images generated in the pseudo-rehearsal strategy;\niii) a robust"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "experimental protocol considering well-known FER datasets and a pipeline of"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "experiments to discuss the contributions of the proposed emotion-centered gen-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "erative replay in mitigating catastrophic forgetting when compared to a regular"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "fine-tuning process or the possibility of\njoining datasets."
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "The remainder of this paper is structured as follows: Section 2 reviews related"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "work on catastrophic forgetting and existing methods for its mitigation. Section 3"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "presents the proposed emotion-centered generative replay approach and outlines"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "the architecture of the QA algorithm. Section 4 describes the experimental setup"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "and presents\nthe results of our comprehensive evaluations. Section 5 discusses"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "the implications of our findings, and Section 6 concludes\nthe paper, outlining"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n3": "potential directions for future research."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "tions based on their\ninfluence. The more a synapse contributes,\nthe higher\nits"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "importance;\nin contrast,\nless influential synapses are assigned lower importance."
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "By preserving these critical connections, SI bridges the gap between old and new"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "information, thus mitigating forgetting while embracing novelty."
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Deep generative replay (DGR) [11] utilizes generative models to create sim-"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ulated instances from prior tasks during training. This approach effectively en-"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "riches the current task’s dataset. The augmented instances, fused with real-time"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "data, offer the network a diverse and comprehensive pool of examples. With past"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "knowledge seamlessly integrated, DGR effectively combats the erosion of previ-"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ously gained insights, presenting itself as a powerful tool\nfor memory retention."
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Beyond these algorithms,\ninsights obtained from neuropsychological research"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "paint a broader picture. Investigations into context-dependent learning have illu-"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "minated the crucial role of training and testing contexts in determining network"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "performance. Thus, using contextual cues, algorithms can be designed to exploit"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "the\ntraining and testing context better,\nthereby enhancing memory retention"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "while countering the forgetting phenomenon [10]."
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "The\ncomplementary learning system (CLS) hypothesis, which posits dual"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "learning and memory systems within the human brain,\nfurther contributes\nto"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "our understanding. The hippocampal-based system swiftly acquires new knowl-"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "edge, while\nthe neocortical\nsystem excels at\nlong-term storage and retrieval."
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Incorporating this\nframework into neural network architectures yields models"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "that deftly balance plasticity and stability [10]. By synergizing the\nstrengths"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "of both systems, these models expertly deal with catastrophic forgetting while"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "nurturing the growth of consolidated knowledge."
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "In light of these contributions,\nit is crucial to contextualize our work within"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "the broader\nrealm of\nthe\ncurrent\nstate-of-the-art. The proposed methodology"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "harmonizes\nthe concepts of emotion-centered generative replay and QA. With"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "CNNs as the focal point, our approach aims to prevent catastrophic forgetting"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "in facial expression recognition, a domain where precise emotion identification"
        },
        {
          "4\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "relies heavily on image quality."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "the top, the emotion-centered WGAN-GP with CNN QA is depicted. This com-"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "ponent\ninvolves\ntraining a WGAN-GP for each class\nin the source dataset\nto"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "generate synthetic data resembling that class. At\nthe bottom,\nthe fine-tuning"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "strategy is illustrated, where our synthetic dataset is replayed alongside the tar-"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "get dataset."
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "We have formalized our methodology using algorithmic representations\nto"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "provide a more concrete understanding of the theoretical concepts presented. In"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "Subsection 3.2, we provide detailed algorithms replicating our approach’s offline"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "preparation and training stages. These algorithms encapsulate the step-by-step"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "processes of generating synthetic images and performing continual retraining."
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "3.1\nEmotion-Centered Generative Replay with WGAN-GP and"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "Quality Assurance"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "We initiate by training a set of WGAN-GPs, one for each of the seven emotion"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "classes present in the ‘source’ dataset -\nfear, anger, happiness, sadness, disgust,"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "surprise, and neutral. Using these trained WGAN-GPs, we generate augmented"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "datasets for each class. These generated images capture the intricate details of"
        },
        {
          "Fig. 1: An overview of\nthe proposed method reveals\ntwo key components. At": "respective emotions, diversifying the training data for better generalization."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "The WGAN-GP is built by two different networks: discriminator and gen-"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "erator. The discriminator network is crucial\nin distinguishing between real and"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "synthetic images. It consists of several layers, including convolutional layers with"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "leaky rectified linear united (ReLU) activation functions. These layers help the"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "discriminator extract relevant features from input images. Additionally, dropout"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "layers are applied to prevent overfitting. The discriminator network’s output"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "layer produces a single scalar value, representing the likelihood that\nthe input"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "image is a real sample. This network contains approximately 4.3 million trainable"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "parameters, making it a powerful discriminator for the WGAN-GP."
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "The generator network is responsible for generating synthetic images. It starts"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "with an input layer that takes random noise as input. This noise is passed through"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "several layers, including dense, batch normalization, and convolutional layers, fol-"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "lowed by leaky ReLU activations. These layers progressively upscale and refine"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "the feature maps to generate higher-resolution images. The generator’s output"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "shape matches the desired image size. With around 1.6 million total parameters,"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "the generator network creates convincing synthetic images to fool the discrimi-"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "nator."
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "The generator\nstrives\nto produce\nindistinguishable\nimages\nfrom real ones,"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "while the discriminator aims to classify them correctly. This adversarial process"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "leads to the training of both networks and ultimately results in the generation"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "of high-quality synthetic data."
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "We employ our QA algorithm to ensure the quality of the generated images."
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "The QA algorithm filters out low-quality or incorrect images generated by the"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "WGAN-GP,\nretaining only high-quality images\nthat\nthe original classifier can"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "correctly classify. The QA process is performed using the CNN trained on the"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "source dataset. This network filters the good and bad images from the synthetic"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "datasets. Given an empirically defined threshold, the images correctly classified"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "by the network are used for future retraining, and the misclassified images are"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "discarded. The QA process enhances the reliability of the emotion-centered gen-"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "erative replay, preventing the classifier from being influenced by poor-quality or"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "misleading synthetic images. These images are then integrated into an improved"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "dataset, which merges the synthetic images with the initial source data."
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "During retraining,\nthe new dataset and the\ntarget dataset are\nemployed."
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "This unified dataset facilitates CNN training, where knowledge from the original"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "emotion classes combines with the new target emotions, minimizing forgetting."
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "3.2\nGeneral Pipeline"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "To address the challenge of catastrophic forgetting, our proposed approach in-"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "volves a two-stage process: offline preparation and a training phase."
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Offline preparation stage\nInitialization occurs as depicted in Algorithm 1,"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "where\na\nset\nof datasets\nrepresented by T\nis defined,\nencompassing datasets"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "is\ntraversed through an iterative\nA, B, C, and D. Each dataset dt within T"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "is trained using\nprocess. For each specific dataset dt, a classifier, denoted as Cdt,"
        },
        {
          "6\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "that particular dataset."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Algorithm 1 Offline stage": "1: T ← A, B, C, D"
        },
        {
          "Algorithm 1 Offline stage": "2: T ′ ← ∅"
        },
        {
          "Algorithm 1 Offline stage": "3:"
        },
        {
          "Algorithm 1 Offline stage": ""
        },
        {
          "Algorithm 1 Offline stage": "4:"
        },
        {
          "Algorithm 1 Offline stage": ""
        },
        {
          "Algorithm 1 Offline stage": ""
        },
        {
          "Algorithm 1 Offline stage": ""
        },
        {
          "Algorithm 1 Offline stage": ""
        },
        {
          "Algorithm 1 Offline stage": ""
        },
        {
          "Algorithm 1 Offline stage": ""
        },
        {
          "Algorithm 1 Offline stage": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14: end for": "15:\nreturn collection of synthetic datasets T ′"
        },
        {
          "14: end for": "In this context, we train a\nWe then iterate over each class c in dataset dt."
        },
        {
          "14: end for": "WGAN-GP for each class, denoted WGANGPc, and subsequently combine them to"
        },
        {
          "14: end for": "form an ensemble, denoted as Gdt. Through these WGANGPc, we generate synthetic"
        },
        {
          "14: end for": "images (SIc) mirroring the characteristics of each class. Continuing the process,"
        },
        {
          "14: end for": "we\nfeed these\nsynthetic\nimages\nthrough the\nresulting in a new\nclassifier Cdt,"
        },
        {
          "14: end for": "qa"
        },
        {
          "14: end for": "dataset, dt\n, consisting only of images correctly classified by Cdt. These refined\nc"
        },
        {
          "14: end for": "synthetic images are combined into a new dataset d′\nt. This procedure is executed"
        },
        {
          "14: end for": "for\nresulting datasets d′\neach dataset dt, and all\nt are unified into a collection"
        },
        {
          "14: end for": "labeled as T ′, encapsulating the sets of synthetic datasets corresponding to each"
        },
        {
          "14: end for": "in T .\noriginal dataset dt"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "As shown in Algorithm 2, we define a set of subsequent datasets,\nindicated"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "by T , which comprises datasets B, C, and D. Then, we iterate over each combi-"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "nation of the original dataset and subsequent dataset, referred to as dt and d′\nt"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "respectively,\nfrom set T and its counterpart T ′. For each dataset combination,"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "we create a unified dataset, du\nt , by merging dt and d′\nt. Subsequently, we train"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "on the unified dataset du\na classifier, Cdu\nt . Finally, we accumulate the trained"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "t"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "classifiers Cdu\nfor each combination, and this ensemble is returned as CT , repre-\nt"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "senting a comprehensive collection of classifiers suited for different datasets and"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "training strategies."
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "The CNN used in the\nexperiments\ncomprises\nseveral\nlayers,\nstarting with"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "2D convolutional\nlayers with 64 filters each. Batch normalization stabilizes and"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "accelerates training, followed by additional convolutional\nlayers with varying fil-"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ter\nsizes. Max-pooling layers are introduced to downsample the feature maps,"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "reducing the spatial dimensions and focusing on the most salient features. The"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "network continues with more convolutional\nlayers and batch normalization to"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "extract higher-level\nrepresentations\nfrom the\ninput data. Eventually,\nthe\nfea-"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ture maps are flattened to form a one-dimensional vector, which is then passed"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "through fully connected layers. Dropout\nlayers are employed to prevent over-"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "fitting,\nenhancing the model’s generalization. The final\nlayer\nis dense with a"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "softmax activation function, producing output probabilities for different classes."
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "In total, this CNN architecture contains approximately 19.3 million parameters."
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "The training aims\nto optimize the Eq.\n(1), where a weight w is applied to"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "each prediction. This weight is determined by the CNN’s confidence percentage"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "when predicting for all ypred."
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "wjy(i)\nLi(y(i)"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "C(cid:88) j\n(1)\ntrue, y(i)\ntrue j log(y(i)\npred j)\npred) = −"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "=1"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "In summary, our general pipeline encompasses an offline preparation phase"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "involving training WGAN-GPs and QA-based synthetic image generation. In the"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "training stage, augmented and original datasets are combined, and the continual"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "retraining approach adapts the classifier to multiple datasets while incorporating"
        },
        {
          "8\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "different strategies."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "its relatively small size, containing approximately 213 facial\nimages,",
          "9": "is valuable"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "for\nevaluating and comparing facial",
          "9": "expression recognition models. The Tai-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "wanese facial expression image database (TFEID) [3] provides a suitable testbed",
          "9": ""
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "for evaluating emotion recognition algorithms, with 1128 samples. Lastly,",
          "9": "the"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "extended Cohn-Kanade dataset (CK+) [8] dataset is commonly used for facial",
          "9": ""
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "expression recognition research.\nIt\nincludes a substantial number of\nfacial",
          "9": "im-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "ages, compiled into 123 videos of different subjects, totaling approximately 593",
          "9": ""
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "videos, with 327 labeled videos covering various emotional expressions.",
          "9": ""
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "4.1\nResults",
          "9": ""
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "This section offers an in-depth analysis of the outcomes achieved by employing",
          "9": ""
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "different retraining strategies, each suited to minimize memory degradation and",
          "9": ""
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models": "maximize knowledge retention.",
          "9": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 1: , it is possible to observe the results when adapting the CNN",
      "data": [
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "QA process. These images are of\nlow quality and do not demonstrate emotion;"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "therefore, the CNN could not classify them correctly."
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Fig. 3: Some rejected samples identified by the QA algorithm from the synthetic"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "datasets of MUG, JAFFE, and TFEID."
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "4.2\nOn Continual Learning"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "In this section, we will discuss the main results observed from the tests conducted"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "with facial expression datasets, utilizing the combination of different methods"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "outlined in this study."
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Initially, a CNN was\ntrained on the MUG dataset, and subsequently, we"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "employed this CNN to adapt it to other datasets continuously. All trainings were"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "conducted with 20 replications.\nIn methods\ninvolving image generation,\neach"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "experiment replication utilizes images explicitly generated for that retraining;\nin"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "other words, synthetic datasets are not identical across different replications of"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "CNN adaptation."
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "In Tables 1, 2, and 3, the columns baseline, joint, and fine-tune represent, re-"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "spectively: testing datasets with the CNN trained on the source dataset; adapting"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "the CNN trained on the source plus target dataset; adapting the CNN trained"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "on the source dataset using only the target dataset. Additionally, the ECgr and"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "QA methods were separately evaluated and combined to ascertain the impact of"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "using synthetic image filtering in continuous training. Furthermore, this scenario"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "assessed whether using weights on synthetic images has any effect compared to"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "training without this technique."
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "In Table 1,\nit\nis possible\nto observe\nthe\nresults when adapting the CNN"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "trained on the MUG dataset\n(source)\nto the JAFFE dataset\n(target). Taking"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "into account\nthe baseline,\njoint, and fine-tune methods, we\ncan assume\nthat"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "the upper\nlimit\nis\nthe joint method, which represents\nthe ideal case where all"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "datasets are available for training, and the lower limit is the fine-tune method,"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "in which the source dataset is no longer available. It can be concluded that the"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "combined method ECgr+wQA yielded the best results in this initial adaptation"
        },
        {
          "10\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "involving only one dataset, with a result very close to joint training. In Tables 2"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 2: shows that the best result lies between ECgr+QA and ECgr+wQA.",
      "data": [
        {
          "wQA, alongside with fine-tune,": "",
          "joint and current for a direct comparison.": "Proposed"
        },
        {
          "wQA, alongside with fine-tune,": "",
          "joint and current for a direct comparison.": "ECgr+QA ECgr+wQA"
        },
        {
          "wQA, alongside with fine-tune,": "Source datasets",
          "joint and current for a direct comparison.": ""
        },
        {
          "wQA, alongside with fine-tune,": "MUG",
          "joint and current for a direct comparison.": "0.87±0.04"
        },
        {
          "wQA, alongside with fine-tune,": "JAFFE",
          "joint and current for a direct comparison.": "0.62±0.07"
        },
        {
          "wQA, alongside with fine-tune,": "Mean",
          "joint and current for a direct comparison.": "0.74"
        },
        {
          "wQA, alongside with fine-tune,": "Target dataset",
          "joint and current for a direct comparison.": ""
        },
        {
          "wQA, alongside with fine-tune,": "TFEID",
          "joint and current for a direct comparison.": "0.84±0.04"
        },
        {
          "wQA, alongside with fine-tune,": "Updated mean",
          "joint and current for a direct comparison.": "0.78"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 3: Results on MUG plus JAFFE plus TFEID’s model fine-tuned to CK+",
      "data": [
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": ""
        },
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": ""
        },
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": "Source datasets"
        },
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": "MUG"
        },
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": "JAFFE"
        },
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": "TFEID"
        },
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": "Mean"
        },
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": "Target dataset"
        },
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": "CK+"
        },
        {
          "QA and wQA, alongside with fine-tune, joint and current for a direct comparison.": "Updated mean"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 3: for CK+ results.",
      "data": [
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": "refer to Table 3 for CK+ results."
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": ""
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": "1"
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": "0.9"
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": "0.8"
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": ""
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": ""
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": "0.7"
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": ""
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": "0.6"
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": "0.5"
        },
        {
          "training. Therefore, it does not have subsequent results after its retraining. Please": "Baseline"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "1"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "F1score\n0.8"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "0.6"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "0.4"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Baseline\nJAFFE\nTFEID\nCK+"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Fine-tuning (anger)\nFine-tuning (disgust)\nFine-tuning (fear)"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Fine-tuning (happiness)\nFine-tuning (sadness)\nFine-tuning (surprise)"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ECgr+QA (anger)\nECgr+QA (disgust)\nECgr+QA (fear)"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ECgr+QA (happiness)\nECgr+QA (sadness)\nECgr+QA (surprise)"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Fig. 6: Comparison of F1 scores by class on the MUG dataset between fine-tune"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "and ECgr+QA, showcasing the continuous adaptation across JAFFE, TFEID,"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "and CK+ datasets."
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "6\nConclusion"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "In this study, we presented a comprehensive investigation into the challenge of"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "catastrophic forgetting in CNNs within the context of\nfacial expression recogni-"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "tion, proposing a novel approach to mitigate its effects. We employed a pseudo-"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "rehearsal method,\nspecifically our\nemotion-centered generative\nreplay (ECgr)"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "with WGAN-GPs,\nto generate synthetic images\nfor each class of\nthe datasets"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "and combined this with a filtering method to exclude images that could hinder"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "retraining."
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Across various tasks, ECgr consistently demonstrated superior performance"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "compared to baseline and fine-tuned methods. Utilizing WGAN-GPs\nto syn-"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "thesize task-specific data, along with our QA algorithm, resulted in substantial"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "knowledge retention. This confirms the potential of pseudo-rehearsal methods to"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "effectively retrain CNNs without revisiting original datasets, offering a promis-"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ing strategy for addressing memory decay, particularly in challenging scenarios"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "like facial expression recognition."
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "Despite promising results with pseudo-rehearsal,\nits effectiveness may vary"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "across network\narchitectures, datasets,\nand tasks. Additionally, WGAN-GP-"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "based data generation can be computationally expensive,\nlimiting real-time use."
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "These\naspects highlight\nopportunities\nfor\nfuture\nresearch,\nsuch as\nimproved"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "weight assignment algorithms and exploration of regularization techniques’ syn-"
        },
        {
          "14\nI. A. Laurensi, A. S. Britto Jr., J. P. Barddal, and A. L. Koerich": "ergy with pseudo-rehearsal approaches."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "References"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "1. Aifanti, N., Papachristou, C., Delopoulos, A.: The mug facial expression database."
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "In: 11th International Workshop on Image Analysis\nfor Multimedia Interactive"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Services WIAMIS 10. pp. 1–4 (2010)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "2. Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial net-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "works. In: Precup, D., Teh, Y.W. (eds.) 34th International Conference on Machine"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Learning. Proc. of Machine Learning Research, vol. 70, pp. 214–223. PMLR (2017)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "3. Chen, C.C.,\nling Cho, S., Horszowska, K., Chen, M.Y., Wu, C.C., Chen, H.C.,"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Yeh, Y.Y., Cheng, C.M.: A facial expression image database and norm for Asian"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "population: a preliminary report.\nIn: Farnand, S.P., Gaykema, F.\n(eds.)\nImage"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Quality and System Performance VI. vol. 7242, p. 72421D. SPIE (2009)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "4. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., Courville, A.C.: Improved"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "training of wasserstein gans. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H.,"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Fergus, R., Vishwanathan, S., Garnett, R. (eds.) Advances in Neural Information"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Processing Systems. vol. 30. Curran Associates, Inc. (2017)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "5. Khetarpal, K., Riemer, M., Rish, I., Precup, D.: Towards continual reinforcement"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "learning: A review and perspectives. J Artif Intell Research 73, 295–333 (2022)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "6. Kirkpatrick, J., et al.: Overcoming catastrophic forgetting in neural networks. Proc."
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "of the National Academy of Sciences 114(13), 3521–3526 (2017)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "7. Li, Z., Hoiem, D.: Learning without forgetting.\nIEEE Trans on Pattern Analysis"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "and Machine Intelligence 40(12), 2935–2947 (2018)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "8. Lucey, P., Cohn, J.F., Kanade, T., Saragih, J., Ambadar, Z., Matthews, I.: The ex-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "tended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "specified expression.\nIn:\nIEEE CS onference\non Computer Vision and Pattern"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Recognition - Workshops. pp. 94–101 (2010)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "9. Lyons, M., Kamachi, M., Gyoba, J.: The japanese female facial expression (jaffe)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "dataset. Zenodo (Apr 1998). https://doi.org/10.5281/zenodo.3451524"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "10. Parisi, G.I., Kemker, R., Part, J.L., Kanan, C., Wermter, S.: Continual\nlifelong"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "learning with neural networks: A review. Neural Networks 113, 54–71 (2019)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "11. Shin, H., Lee, J.K., Kim, J., Kim, J.: Continual\nlearning with deep generative"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "replay.\nIn: Guyon,\nI., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vish-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "wanathan, S., Garnett, R. (eds.) Advances in Neural Information Processing Sys-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "tems. vol. 30. Curran Associates, Inc. (2017)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "12. Tannugi, D.C., Britto, A.S., Koerich, A.L.: Memory integrity of\ncnns\nfor\ncross-"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "dataset\nfacial\nexpression recognition.\nIn:\nIEEE Intl Conf on Systems, Man and"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Cybernetics. pp. 3826–3831 (2019)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "13.\nvan de Ven, G.M., Tuytelaars, T., Tolias, A.S.: Three types of incremental learning."
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Nature Machine Intelligence 4(12), 1185–1197 (2022)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "14. Zavaschi, T.H., Britto Jr, A.S., Oliveira, L.E., Koerich, A.L.: Fusion of\nfeature"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "sets and classifiers for facial expression recognition. Exp Syst App 40(2), 646–655"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "(2013)"
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "15. Zenke, F., Poole, B., Ganguli, S.: Continual\nlearning through synaptic intelligence."
        },
        {
          "Alleviating Catastrophic Forgetting in FER with Emotion-Centered Models\n15": "Proceedings of machine learning research 70, 3987–3995 (2017)"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The mug facial expression database",
      "authors": [
        "N Aifanti",
        "C Papachristou",
        "A Delopoulos"
      ],
      "year": "2010",
      "venue": "11th International Workshop on Image Analysis for Multimedia Interactive Services WIAMIS 10"
    },
    {
      "citation_id": "2",
      "title": "Wasserstein generative adversarial networks",
      "authors": [
        "M Arjovsky",
        "S Chintala",
        "L Bottou"
      ],
      "year": "2017",
      "venue": "34th International Conference on Machine Learning. Proc. of Machine Learning Research"
    },
    {
      "citation_id": "3",
      "title": "A facial expression image database and norm for Asian population: a preliminary report",
      "authors": [
        "C Chen",
        "S Ling Cho",
        "K Horszowska",
        "M Chen",
        "C Wu",
        "H Chen",
        "Y Yeh",
        "C Cheng"
      ],
      "year": "2009",
      "venue": "Image Quality and System Performance VI"
    },
    {
      "citation_id": "4",
      "title": "Improved training of wasserstein gans",
      "authors": [
        "I Gulrajani",
        "F Ahmed",
        "M Arjovsky",
        "V Dumoulin",
        "A Courville"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "5",
      "title": "Towards continual reinforcement learning: A review and perspectives",
      "authors": [
        "K Khetarpal",
        "M Riemer",
        "I Rish",
        "D Precup"
      ],
      "year": "2022",
      "venue": "J Artif Intell Research"
    },
    {
      "citation_id": "6",
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": [
        "J Kirkpatrick"
      ],
      "year": "2017",
      "venue": "Proc. of the National Academy of Sciences"
    },
    {
      "citation_id": "7",
      "title": "Learning without forgetting",
      "authors": [
        "Z Li",
        "D Hoiem"
      ],
      "year": "2018",
      "venue": "IEEE Trans on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "8",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotionspecified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "IEEE CS onference on Computer Vision and Pattern Recognition -Workshops"
    },
    {
      "citation_id": "9",
      "title": "The japanese female facial expression (jaffe) dataset",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "Zenodo",
      "doi": "10.5281/zenodo.3451524"
    },
    {
      "citation_id": "10",
      "title": "Continual lifelong learning with neural networks: A review",
      "authors": [
        "G Parisi",
        "R Kemker",
        "J Part",
        "C Kanan",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "11",
      "title": "Continual learning with deep generative replay",
      "authors": [
        "H Shin",
        "J Lee",
        "J Kim",
        "J Kim"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Memory integrity of cnns for crossdataset facial expression recognition",
      "authors": [
        "D Tannugi",
        "A Britto",
        "A Koerich"
      ],
      "year": "2019",
      "venue": "IEEE Intl Conf on Systems, Man and Cybernetics"
    },
    {
      "citation_id": "13",
      "title": "Three types of incremental learning",
      "authors": [
        "G Van De Ven",
        "T Tuytelaars",
        "A Tolias"
      ],
      "year": "2022",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "14",
      "title": "Fusion of feature sets and classifiers for facial expression recognition",
      "authors": [
        "T Zavaschi",
        "A Britto",
        "L Oliveira",
        "A Koerich"
      ],
      "year": "2013",
      "venue": "Exp Syst App"
    },
    {
      "citation_id": "15",
      "title": "Continual learning through synaptic intelligence",
      "authors": [
        "F Zenke",
        "B Poole",
        "S Ganguli"
      ],
      "year": "2017",
      "venue": "Proceedings of machine learning research"
    }
  ]
}