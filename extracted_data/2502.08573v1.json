{
  "paper_id": "2502.08573v1",
  "title": "A Novel Approach For Multimodal Emotion Recognition : Multimodal Semantic Information Fusion A Preprint",
  "published": "2025-02-12T17:07:43Z",
  "authors": [
    "Wei Dai",
    "Dequan Zheng",
    "Feng Yu",
    "Yanrong Zhang",
    "Yaohui Hou"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the advancement of artificial intelligence and computer vision technologies, multimodal emotion recognition has become a prominent research topic. However, existing methods face challenges such as heterogeneous data fusion and the effective utilization of modality correlations. This paper proposes a novel multimodal emotion recognition approach, DeepMSI-MER, based on the integration of contrastive learning and visual sequence compression. The proposed method enhances crossmodal feature fusion through contrastive learning and reduces redundancy in the visual modality by leveraging visual sequence compression. Experimental results on two public datasets, IEMOCAP and MELD, demonstrate that DeepMSI-MER significantly improves the accuracy and robustness of emotion recognition, validating the effectiveness of multimodal feature fusion and the proposed approach.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "With the rapid development of artificial intelligence and computer vision technologies, emotion recognition has become an important research direction in various fields such as human-computer interaction (HCI), intelligent customer service, and mental health monitoring  [Poria et al., 2017a] . The goal of emotion recognition is to analyze an individual's emotional state through multimodal information, such as speech, text, and visual data, to achieve emotional understanding in intelligent systems. However, traditional emotion recognition methods mainly focus on feature extraction and emotion classification from a single modality, which limits their effectiveness in complex real-world applications. In recent years, with the continuous advancement of multimodal learning and deep learning technologies, multimodal emotion recognition (MER) has gradually become a research hotspot. MER improves the accuracy and robustness of emotion classification by integrating multiple data sources.\n\nAlthough existing multimodal emotion recognition methods have achieved success in many scenarios, they still face several challenges. First, different modalities exhibit different representations in feature space, and effectively fusing these heterogeneous data to capture emotional features has become a key issue  [Hadsell et al., 2006 , Chen et al., 2020] . Second, temporal features and spatial information in the visual modality often contain a large amount of redundant data. Reducing this redundancy while retaining emotion-relevant information remains a valuable area for exploration  [Tran et al., 2018, Carreira and Zisserman, 2017] . Lastly, despite significant progress in feature extraction using deep learning models, fully leveraging the latent correlations between different modalities to enhance the emotional understanding capability of models in multimodal emotion recognition tasks remains a challenging problem  [Zadeh et al., 2017 , Liu et al., 2018a] .\n\nTo address these challenges, this paper proposes a novel multimodal emotion recognition method called DeepMSI-MER, based on contrastive learning and visual sequence compression integration. Through contrastive learning, the model better captures the similarities and differences between modalities during training, thereby enhancing cross-modal feature fusion. Visual sequence compression effectively reduces redundancy in the visual modality by compressing and extracting temporal information, thus enhancing the model's sensitivity to emotions. Our experimental results show that the proposed method performs excellently on two public datasets, IEMOCAP  [Busso et al., 2008]  and MELD  [Poria et al., 2019] , significantly improving the accuracy and robustness of multimodal emotion recognition and validating the effectiveness of multimodal feature fusion and the proposed approach.\n\n2 Related Work",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "Multimodal emotion recognition initially focused on emotion recognition from individual modalities. However, with advancements in technology, recent studies have increasingly integrated speech, text, and visual modalities to improve emotion recognition performance. Methods based on deep neural networks have been proposed to combine speech and text modalities, significantly enhancing emotion recognition accuracy  [Abdullah et al., 2021] . Additionally, frameworks that integrate speech, text, and visual features have further improved emotion prediction accuracy through joint training  [Gupta et al., 2024] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Application Of Contrastive Learning In Emotion Recognition",
      "text": "Contrastive learning, which maximizes the similarity between similar samples and minimizes the distance between dissimilar samples, has achieved success across various domains, including vision, speech, and text. In recent years, contrastive learning-based multimodal emotion recognition frameworks have been introduced to enhance the feature fusion of speech and text modalities, significantly improving model performance  [Mai et al., 2022] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Visual Sequence Compression",
      "text": "To address the redundant information in the visual modality, particularly in video data, methods for visual sequence compression have been proposed. These include CNN-based and LSTM-based compression methods, which effectively extract key frame information and improve emotion recognition accuracy by reducing the impact of redundant data  [Kugate et al., 2024 , Fan et al., 2016] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Modal Fusion Methods",
      "text": "One core challenge in multimodal emotion recognition is effectively fusing information from different modalities. Traditional fusion methods include early fusion, late fusion, and intermediate fusion. Recently, approaches based on self-attention mechanisms and graph convolution networks (GCNs) have emerged as new trends, allowing for effective interaction between modalities and consideration of their mutual influence during the fusion process  [Pang et al., 2023 , Hu et al., 2021] .\n\nFigure  1 : The overall architecture of DeepMSI-MER for multimodal emotion recognition. DeepMSI-MER consists of a high-level semantic feature module, an early feature fusion module, and a late feature fusion module. The high-level semantic feature module fuses the semantic features of text and audio to further extract contextual semantic features, which are ultimately used in VSC-Swin.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "The DeepMSI-MER framework proposed in this paper is shown in Figure  1 . The model consists of three stages: modality-specific feature extraction, early feature fusion, late feature fusion, and model prediction. In the following subsections, we will describe in detail the modality-specific feature extraction, early feature. The specific model code is available at https://github.com/ZMW-DW/DeepMSI-MER",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Modality-Specific Feature Extraction And Early Feature Fusion",
      "text": "Initially, we fine-tune the pre-trained models BERT  [Devlin et al., 2019]  and Wav2Vec  [Baevski et al., 2020]  on the audio and text data, respectively, to obtain their semantic features. Next, we fuse the audio semantic feature a c ls with the text semantic feature t cls to generate the high-level semantic feature G cls . This high-level semantic feature is then passed to the visual sequence compression module in the video feature extraction pipeline. During video feature extraction, 15 frames are sampled from each video, and the Swin Transformer is used to extract features from each frame. Finally, Temporal Convolution Networks (TCN)  [Bai et al., 2018]  are employed to capture temporal features from the frame-level features, producing the final video feature.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Visual Sequence Compression",
      "text": "To enhance the accuracy of visual information, we propose a visual sequence compression method based on multimodal semantic information, as shown in Figure  2 .\n\nBy applying average pooling to the visual sequence V N ×d in the Swin Transformer, we obtain the visual semantic feature v cls for this stage. The high-level semantic features are then fused with the visual semantic features, as following formula Equation 1:\n\nwhere, m cls represents the weighted sum of the high-level semantic feature G cls and the visual semantic feature v cls .\n\nThe fused semantic feature m cls is then broadcasted to the same dimension as the visual sequence, resulting in M N ×d . Subsequently, we compute the similarity between M N ×d and V N ×d , as following formula Equation 2:\n\nwhere, τ is the temperature coefficient, T represents the transpose of the matrix, and S N ×N is the similarity matrix. Since the values in each row of S N ×N are the same, the first row is selected as the similarity sequence. Finally, based on the similarity sequence and the similarity threshold γ, the visual sequence V N ×d is divided into relevant sequences Z r and irrelevant sequences Z lr .\n\nFinally, to prevent information loss during the visual sequence compression process, we compute the similarity between Z r and Z lr , and fuse the information of Z lr with the highest similarity corresponding to Z r , as following formula Equation 3:\n\nwhere, j represents the sequence position corresponding to the highest similarity between the non-relevant sequence Z lr and the relevant sequence Z r ; α is the fusion threshold; N -L is the length of the non-relevant sequence. The relevant sequence Z r to be fused is selected based on the sequence position j. Finally, the updated relevant sequence Z r ′ is output.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Image Feature Extraction",
      "text": "In sentiment analysis tasks, background noise in images can affect the accuracy of feature extraction when using Swin-Transformer. To address this, we fine-tuned the pre-trained Wav2Vec and BERT models for semantic feature extraction of audio and text, respectively, and fused these high-level semantic features as a reference to selectively filter video sequences within the Swin-Transformer. As shown in Figure  3 , the VSC-Swin framework compresses video sequences by adding a visual compression module to the original Swin-Transformer architecture, while layer normalization (LN) is applied to prevent gradient explosion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Video Feature Extraction",
      "text": "In this study, we use the Temporal Convolutional Network (TCN) for temporal feature extraction from videos. TCN is a deep neural network architecture designed for processing sequential data. As shown in Figure  5 , it captures long-range dependencies using dilated convolutions while maintaining low computational complexity. The core of TCN is the convolution operation, particularly the dilated convolution, as following formula Equation  4 :\n\nThe output at time step t is denoted as y(t); x t represents the input sequence at time step t; w k is the weight of the convolution kernel; d is the dilation factor, which controls the span of the convolution kernel application; and K is the size of the convolution kernel. The key feature of dilated convolution is that it expands the receptive field, allowing each convolution kernel to cover a longer input sequence without increasing computational complexity. By stacking multiple convolution layers, TCN can efficiently capture long-range temporal dependencies in the video. In video-based sentiment recognition, selecting 15 frames as the input frame count for each video is supported by both theoretical and experimental considerations. Based on observations of emotional change characteristics in videos, we selected 15 frames as the number of input frames for each video. This choice is supported by the following theoretical and experimental considerations:\n\n• Emotional Change Cycles: Emotional changes in videos typically exhibit periodicity, and 15 frames effectively cover these emotional transitions. Studies show that shorter time windows fail to capture emotional changes effectively, while longer time windows lead to information overload. Analysis of multiple video samples reveals that 15 frames strike a good balance.\n\n• Receptive Field of TCN: The dilated convolution design of TCN allows its receptive field to cover multiple time steps. Experiments show that 15 frames of input, processed through multiple TCN layers, capture key emotional transitions without losing important details.\n\nIn conclusion, selecting 15 frames as the input frame count is both reasonable and effective, fully leveraging temporal relationships for accurate sentiment recognition.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Late Feature Fusion",
      "text": "In order to better utilize information from different modalities, we propose an improved contrastive learning-based feature fusion approach in the model's late-stage feature fusion. As shown in Figure  6 , by performing low-dimensional mapping on the original text, audio, and video features, and using the current batch's labels to create positive and negative sample mask matrices, we calculate the loss value by comparing the mapped text and audio features with the video mapped features, which is then fed back into the mapping module. When calculating the loss value, we adopt an algorithm based on contrastive learning. The overall formula for the algorithm is as follows in formula Equation  5 : Therefore, during the training process of the model, the gradient calculation formula can be derived as follows in formula Equation  6 :\n\nThe gradient of the contrastive learning loss with respect to the positive and negative samples is calculated as follows:\n\nis the gradient for the positive sample, and\n\nis the gradient for the negative sample. The term B j exp (cos(x i , x j )/τ ) represents the accumulation of negative sample similarities for each row. From the gradient calculations for the positive and negative samples, two cases can be observed. When the accumulated value of the negative sample similarities for each row exceeds exp (cos(x i , x i )/τ ) by a certain degree, the gradient for the negative samples increases, while the gradient for the positive sample decreases. Conversely, when the accumulated value of the negative sample similarities is much smaller than exp (cos(x i , x i )/τ ), both the positive and negative sample gradients approach zero. Therefore, it can be concluded that by adjusting the model using the accumulated negative sample similarity B j exp (cos(x i , x j )/τ ) and the positive sample similarity exp (cos(x i , x i )/τ ), the model can effectively differentiate between positive and negative samples, and further refine the model's predictions in the subsequent steps.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Model Training",
      "text": "In model training, we will use cross-entropy loss and the aforementioned contrastive learning as the model's loss, as follows in formula Equation  7 :\n\nWhere, L ce is the cross-entropy loss, C is the total number of classes, y c is the one-hot encoding of the true label, p c is the predicted probability of the c-th class, α ce is the weight for the cross-entropy loss, and β cl is the weight for the contrastive learning loss.\n\nFinally, to evaluate the model's generalization ability and reduce biases caused by data splitting, we use 10-fold cross-validation. The dataset is randomly divided into 10 subsets, with one subset used as the validation set and the remaining subsets as the training set in each round. The model is trained and evaluated on these sets, and the final performance metric is the average of the results from all 10 rounds of validation. Specifically, we randomly divide the dataset into K equally sized subsets (with K = 10). In each round of cross-validation, one subset is selected as the validation set, and the remaining K -1 subsets are used as the training set. The model is trained on the training set and evaluated on the validation set. This process is repeated K times, with a different subset selected as the validation set each time. The final performance metric of the model is the average of the results from all K rounds of validation.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset",
      "text": "The DeepMSI-MER model proposed in this paper was evaluated on two benchmark datasets, IEMOCAP and MELD, which contain three modalities: text, video, and audio.\n\nIEMOCAP  [Busso et al., 2008]  is a widely used public dataset in emotion recognition research, created by the Sippy team at the University of Southern California. It provides detailed annotations of emotional interactions and speech/nonverbal behaviors, with six emotion categories: happiness, sadness, anger, excitement, frustration, and neutrality. The data were consistently annotated by multiple evaluators and involve 10 participants. The preprocessed data is available at https://pan.baidu.com/s/1OXYrDnNdxx72vIrSppdZ1w?pwd=4uaa MELD  [Poria et al., 2019]  is an open multimodal dataset created by researchers at the University of Toronto, containing text data from movie script dialogues. It includes annotations for six emotion categories: joy, sadness, anger, fear, surprise, and neutrality, with emotional annotations independently performed by multiple annotators.The preprocessed data is available at https://pan.baidu.com/s/1oJ19xlG7ad0DQjZ9eM4Ovg?pwd=i76d",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Baselines And Evaluation Metrics",
      "text": "CMN  [Hazarika et al., 2018a] : This method integrates speaker information and multimodal features by introducing an attention mechanism. bc-LSTM  [Poria et al., 2017b] : It performs final emotion recognition by extracting contextual information from discourse sequences, which is context-sensitive.\n\nLFM  [Liu et al., 2018b] : It efficiently addresses the dimensionality curse in multimodal feature fusion using low-rank decomposition.\n\nA-DMN  [Xing et al., 2020] : A-DMN considers both intra-and cross-speaker contextual information and employs GRU to perform cross-modal feature fusion.\n\nICON  [Hazarika et al., 2018b] : This approach utilizes GRU to extract contextual information from multimodal features and employs an attention layer for multimodal semantic information fusion.\n\nDialogueGCN  [Ghosal et al., 2020] : DialogueGCN constructs a speaker relationship graph using contextual semantic features and leverages both contextual semantic and speaker relationship information for emotion classification.\n\nDialogueRNN  [Majumder et al., 2019] : This method constructs three different gating units to extract and fuse speaker information, emotion information, and global information.\n\nRGAT  [Ishiwatari et al., 2020] : R GAT integrates positional encoding into graph attention networks to improve the model's ability to understand context. LR-GCN  [Ren et al., 2021] : LR-GCN constructs multiple graphs to capture latent dependencies between contexts and employs dense layers to extract speaker relationship and graph structural information. DER-GCN  [Ai et al., 2023] : DER-GCN enhances the model's emotion representation capabilities by constructing speaker relationship and event graphs. ELR-GCN  [Shou et al., 2024] : The model precomputes emotion propagation using an extended forward propagation algorithm and designs an emotion relation-aware operator to capture semantic connections between utterances.\n\nSDT  [Ma et al., 2023] : By leveraging intra-and cross-modal transformers, the model enhances the understanding of interactions between utterances, improving modality relationship comprehension.  [Meng et al., 2024] : From a graph spectral perspective, GS-MCC revisits multimodal emotion recognition, addressing the limitations in capturing long-term consistency and complementary information. To validate the effectiveness of the proposed DeepMSI-MER method, we compared its performance with baseline methods on the IEMOCAP and MELD datasets.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Gs-Mcc",
      "text": "IEMOCAP: As shown in Table  1 , the DeepMSI-MER model demonstrates excellent performance in sentiment classification tasks, particularly in the \"Sad\" and \"Angry\" categories, with accuracy rates of 87.5% and 89.4%, respectively. The model also achieves remarkable F1 scores across multiple categories, especially in \"Frustrated\" and \"Sad,\" with F1 scores of 92.4% and 93.2%, respectively. These results indicate that DeepMSI-MER not only effectively handles data imbalance but also achieves high precision and recall across various emotion categories. Compared to traditional models, DeepMSI-MER shows superior performance in distinguishing between highly similar emotions, particularly in the classification of \"Angry\" and \"Frustrated.\" Overall, DeepMSI-MER exhibits strong cross-category classification ability and high overall performance, making it a reliable solution for sentiment classification in practical applications.\n\nMELD: Table  2  illustrates that DeepMSI-MER also performed excellently on the MELD dataset. It achieved an accuracy of 86.2% and an F1 score of 92.6% for the neutral emotion category. In other categories, such as surprise, sadness, joy, and anger, DeepMSI-MER showed clear advantages in both accuracy and F1 score. While performance for fear and disgust emotions was lower, the overall results (average accuracy of 69.4% and F1 score of 67.9%) still outperformed other methods. This can be attributed to the effective fusion of visual and textual information using contrastive learning, improving fine-grained emotion differentiation.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Analysis Of Experimental Results",
      "text": "To intuitively assess the model's classification ability for each emotion category, we summarize the predicted sample counts for each category from the K-fold cross-validation (10-fold). We then analyze the model's performance on the IEMOCAP and MELD datasets. Figure  7  displays the confusion matrix for these datasets.\n\nIEMOCAP: Based on the confusion matrix, the DeepMSI-MER model demonstrates strong performance on the IEMOCAP dataset, particularly in the \"frustration\" and \"neutral\" emotion categories, where it exhibits high accuracy and classification capability. Most emotion categories show high correct classification counts, indicating the model's overall stability. However, the model experiences some misclassifications in the \"happiness\" and \"excited\" categories, primarily confusing them with \"neutral\" or \"sadness.\" This suggests that the model may encounter challenges in distinguishing between emotions with high similarity.\n\nMELD: From the confusion matrix, it is evident that the DeepMSI -MER model faces issues with class confusion and the difficulty of recognizing rare emotions. In terms of class confusion, the \"neutral\" category often has samples misclassified into other categories, such as 103 samples being misclassified as \"surprise\" and 87 samples misclassified as \"sadness,\" indicating that the model struggles to distinguish between neutral emotion and other emotions. This may be due to the blurred boundary between neutral and other emotions or the model's insufficient learning of the unique features of neutral emotion. At the same time, the \"surprise\" category has 100 samples misclassified as \"neutral,\" suggesting that the model fails to accurately capture surprise emotion features, leading to confusion with neutral emotion.Regarding rare emotion recognition, emotions such as \"fear\" and \"disgust\" have relatively few samples, and the model's insufficient learning of these rare emotion features results in a lower number of correctly classified samples and a higher frequency of misclassification, leading to lower recognition accuracy. Taken together, these issues affect the performance of the DeepMSI -MER model on certain datasets.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ablation Study",
      "text": "To assess the impact of text, video, and audio features in the DeepMSI-MER model, we conducted experiments on the IEMOCAP and MELD datasets, comparing the performance of different feature combinations. The results are presented in Table  3 . In the IEMOCAP dataset, visual features performed the best, with an accuracy of 78.46% and an F1 score of 78.46%, highlighting the critical role of facial expressions and body language in emotion recognition. Textual features also performed well (59.83%), while audio features were weaker (47.30%). Multimodal fusion (T+V, T+A+V) significantly improved performance, with the three-modal combination reaching 84.75%. In the MELD dataset, textual features showed good performance (65.25%), followed by visual features (68.22%), while audio features were the weakest (46.59%). The combination of text and visual features enhanced performance (accuracy of 68.22%), but the combination of audio and visual features performed poorly. The three-modal combination improved to 69.36%.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Conclusions",
      "text": "The DeepMSI-MER method demonstrates superior performance in sentiment classification tasks across the IEMOCAP and MELD datasets. On IEMOCAP, the model achieves high accuracy and F1 scores, particularly in the \"Sad\" and \"Angry\" categories, highlighting its ability to handle data imbalance and distinguish between highly similar emotions. On MELD, the model excels in the neutral, surprise, sadness, joy, and anger categories, with the effective fusion of visual and textual information via contrastive learning improving fine-grained emotion differentiation. However, challenges remain in recognizing certain emotions, particularly \"happiness\" and \"neutral\" in IEMOCAP, and \"fear\" and \"disgust\" in MELD, due to semantic similarity and class imbalance. Despite these challenges, DeepMSI-MER outperforms baseline methods and proves to be a reliable solution for practical sentiment classification applications.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The overall architecture of DeepMSI-MER for multimodal emotion recognition. DeepMSI-MER consists of a",
      "page": 3
    },
    {
      "caption": "Figure 1: The model consists of three stages:",
      "page": 3
    },
    {
      "caption": "Figure 2: By applying average pooling to the visual sequence V N×d in the Swin Transformer, we obtain the visual semantic",
      "page": 3
    },
    {
      "caption": "Figure 2: Visual Sequence Compression Process.",
      "page": 4
    },
    {
      "caption": "Figure 3: , the VSC-Swin framework compresses",
      "page": 5
    },
    {
      "caption": "Figure 3: VSC-Swin Model Improvement.",
      "page": 5
    },
    {
      "caption": "Figure 4: , by extracting visually relevant sequences from each frame, unnecessary background noise is",
      "page": 5
    },
    {
      "caption": "Figure 4: VSC-Swin Visual Sequence Compression Process.",
      "page": 5
    },
    {
      "caption": "Figure 5: , it captures long-range",
      "page": 5
    },
    {
      "caption": "Figure 5: TCN Model Architecture.",
      "page": 6
    },
    {
      "caption": "Figure 6: , by performing low-dimensional",
      "page": 6
    },
    {
      "caption": "Figure 6: Contrastive Learning Algorithm Process.",
      "page": 7
    },
    {
      "caption": "Figure 7: Confusion matrix of DeepMSI-MER classification on IEMOCAP and MELD datasets.",
      "page": 10
    },
    {
      "caption": "Figure 7: displays the confusion matrix for these datasets.",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Comparison with Other Baseline Models on the IEMOCAP Dataset",
      "page": 9
    },
    {
      "caption": "Table 1: , the DeepMSI-MER model demonstrates excellent performance in sentiment",
      "page": 9
    },
    {
      "caption": "Table 2: Comparison with Other Baseline Models on the MELD Dataset",
      "page": 10
    },
    {
      "caption": "Table 2: illustrates that DeepMSI-MER also performed excellently on the MELD dataset. It achieved an",
      "page": 10
    },
    {
      "caption": "Table 3: Table 3: The effect of DeepMSI-MER on the IEMOCAP and MELD datasets using unimodal features and multimodal",
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "2",
      "title": "Dimensionality reduction by learning an invariant mapping",
      "authors": [
        "R Hadsell",
        "S Chopra",
        "Y Lecun"
      ],
      "year": "2006",
      "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)"
    },
    {
      "citation_id": "3",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "T Chen",
        "S Kornblith",
        "M Norouzi",
        "G Hinton"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "4",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran",
        "H Wang",
        "L Torresani",
        "J Ray",
        "Y Lecun",
        "M Paluri"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "J Carreira",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "7",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Z Liu",
        "Y Shen",
        "V Lakshminarasimhan",
        "P Liang",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "9",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "10",
      "title": "Multimodal emotion recognition using deep learning",
      "authors": [
        "S Abdullah",
        "S Ameen",
        "M Sadeeq",
        "S Zeebaree"
      ],
      "year": "2021",
      "venue": "Journal of Applied Science and Technology Trends"
    },
    {
      "citation_id": "11",
      "title": "Visatronic: A multimodal decoder-only model for speech synthesis",
      "authors": [
        "A Gupta",
        "T Likhomanenko",
        "K Yang",
        "R Bai",
        "Z Aldeneh",
        "N Jaitly"
      ],
      "year": "2024",
      "venue": "Visatronic: A multimodal decoder-only model for speech synthesis",
      "arxiv": "arXiv:2411.17690"
    },
    {
      "citation_id": "12",
      "title": "Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis",
      "authors": [
        "S Mai",
        "Y Zeng",
        "S Zheng",
        "H Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "Efficient key frame extraction from videos using convolutional neural networks and clustering techniques",
      "authors": [
        "A Kugate",
        "B Balannanavar",
        "R Goudar",
        "V Rathod",
        "G Dhananjaya",
        "A Kulkarni"
      ],
      "year": "2024",
      "venue": "EAI Endorsed Transactions on Context-aware Systems and Applications"
    },
    {
      "citation_id": "14",
      "title": "Video-based emotion recognition using cnn-rnn and c3d hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "15",
      "title": "Caver: Cross-modal view-mixed transformer for bi-modal salient object detection",
      "authors": [
        "Y Pang",
        "X Zhao",
        "L Zhang",
        "H Lu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "16",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "arxiv": "arXiv:2107.06779"
    },
    {
      "citation_id": "17",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT 2019"
    },
    {
      "citation_id": "18",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proceedings of NeurIPS 2020"
    },
    {
      "citation_id": "19",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "X Zhan",
        "W Cheng"
      ],
      "year": "2018",
      "venue": "Proceedings of NeurIPS 2018"
    },
    {
      "citation_id": "20",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference on Association for Computational Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "22",
      "title": "Efficient multimodal fusion with factorized bilinear pooling",
      "authors": [
        "Z Liu",
        "Y Shen",
        "Y Li",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Adapted dynamic memory network for emotion recognition in conversation",
      "authors": [
        "S Xing",
        "S Mai",
        "H Hu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "Icon: Interactive conversational memory network for multimodal emotion detection",
      "authors": [
        "D Hazarika",
        "S Poria",
        "R Mihalcea",
        "E Cambria",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2020",
      "venue": "EMNLP-IJCNLP 2019: 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "28",
      "title": "Lr-gcn: Latent relation-aware graph convolutional network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "W Li",
        "D Song",
        "W Nie"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2023",
      "venue": "Der-gcn: Dialogue and event relation-aware graph convolutional neural network for multimodal dialogue emotion recognition",
      "arxiv": "arXiv:2312.10579"
    },
    {
      "citation_id": "30",
      "title": "Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "authors": [
        "Y Shou",
        "W Ai",
        "J Du",
        "T Meng",
        "H Liu",
        "N Yin"
      ],
      "year": "2024",
      "venue": "Efficient long-distance latent relation-aware graph neural network for multi-modal emotion recognition in conversations",
      "arxiv": "arXiv:2407.00119"
    },
    {
      "citation_id": "31",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "authors": [
        "T Meng",
        "F Zhang",
        "Y Shou",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2024",
      "venue": "Revisiting multimodal emotion recognition in conversation from the perspective of graph spectrum",
      "arxiv": "arXiv:2404.17862"
    }
  ]
}