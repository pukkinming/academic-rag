{
  "paper_id": "2409.09545v3",
  "title": "Multi-Microphone And Multi-Modal Emotion Recognition In Reverberant Environment",
  "published": "2024-09-14T21:58:39Z",
  "authors": [
    "Ohad Cohen",
    "Gershon Hazan",
    "Sharon Gannot"
  ],
  "keywords": [
    "Emotion recognition",
    "multi-modal",
    "reverberant conditions",
    "audio transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This paper presents a Multi-modal Emotion Recognition (MER) system designed to enhance emotion recognition accuracy in challenging acoustic conditions. Our approach combines a modified and extended Hierarchical Token-semantic Audio Transformer (HTS-AT) for multi-channel audio processing with an R(2 + 1)D Convolutional Neural Networks (CNN) model for video analysis. We trained and evaluated our proposed method on a reverberated version of the Ryerson audio-visual database of emotional speech and song (RAVDESS) dataset using synthetic and real-world Room Impulse Responses (RIRs). Our results demonstrate that integrating audio and video modalities yields superior performance compared to uni-modal approaches, especially in challenging acoustic conditions. Moreover, we show that the multimodal (audiovisual) approach that utilizes multiple microphones outperforms its single-microphone counterpart.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion Recognition (ER) is a crucial component in human-computer interaction, with applications ranging from healthcare to customer service. Humans naturally express emotions across multiple modalities, including facial expressions, language, speech, and gestures. Accurately modeling the interactions between these modalities, which contain complementary and potentially redundant information, is essential for effective emotion recognition. Most existing studies primarily focus on uni-modal emotion recognition, concentrating on either text, speech, or video  [1] ,  [2] ,  [3] . Although significant advancements in single-modal emotion recognition have been demonstrated, these models often fall short in complex scenarios since they do not utilize the inherently multi-modal nature of emotional expression. Moreover, research on jointly employing multi-modal and multi-microphones for ER is relatively scarce. Previous works have made significant strides in MER. Studies such as  [4] ,  [5] ,  [6] ,  [7]  have developed systems that simultaneously analyze visual and acoustic data. In  [8] , researchers presented an unsupervised MER feature learning approach incorporating audio-visual and textual information. These studies often overlooked the challenges posed by realworld acoustic conditions, particularly reverberation and noise,\n\nThe project has received funding from the European Union's Horizon 2020 Research and Innovation Programme, Grant Agreement No. 871245; and from the Audition Project, Data Science Program, Council of Higher Education, Israel.\n\nwhich can significantly impact the performance of audio-based emotion recognition. Feature selection is vital in designing effective MER systems. For acoustic features, log-mel filterbank energies and log-mel spectrograms have been widely adopted  [9] ,  [10] . In the video domain, various deep learning architectures such as VGG16  [11] , I3D  [12] , and FaceNet  [13]  have been employed, along with facial features like landmarks and action units extracted using tools like OpenFace  [14] .\n\nFor the text modality, Global Vectors for Word Representation (GloVe)  [15]  have been frequently used  [14] ,  [16] ,  [17] .\n\nDespite these advancements, a gap remains in addressing the challenges posed by reverberant and noisy environments. Real-world acoustic conditions can significantly alter speech signals, potentially degrading the performance of audio-based emotion recognition systems. Moreover, the integration of multi-channel audio processing techniques with video analysis for emotion recognition in such challenging conditions has not been thoroughly explored.\n\nThis work addresses these limitations by proposing a MER with multi-channel audio that outperforms solutions solely based on single-channel audio. We propose a novel approach that combines two state-of-the-art architectures for audiovisual emotion recognition. The multi-channel extension of the HTS-AT architecture for the audio modality  [18]  and the R(2 + 1)D CNN model  [19]  for the video modality. We use a reverberated version of the RAVDESS  [20]  dataset to analyze the proposed scheme's performance. Reverberation was added by convolving the speech utterances with real-life RIRs drawn from the Acoustic Characterisation of Environments (ACE) challenge dataset  [21] . The code of the proposed method is available.  1 II. PROBLEM FORMULATION Denote the two modalities as M = {video, audio} and the set of emotions as: E = {happy, calm, sad, angry, . . . neutral, fearful, disgust, surprised}.  (1)  Let v(t) be the video signal and s(t) the anechoic audio signal, with t the time index. An array of C microphones captures the audio signal after propagating in the acoustic environment. The signals, as captured by the microphones, are given by:\n\nwhere h i (t), i = 1, 2, . . . , C, are the RIRs from the source to the ith microphone. The feature embeddings for each modality are denoted f v and f s , respectively. This study aims to classify the utterance to one of the emotions using the available information and utilizing the relations between the feature embeddings of both modalities:\n\nwhere ⊕ stands for late fusion concatenation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. Proposed Model",
      "text": "Our proposed MER architecture combines two powerful models: the modified and extended HTS-AT  [18]  for multichannel audio processing and the R(2 + 1)D model  [19]  for video analysis. These uni-modal models are integrated to create a robust multi-modal system for emotion recognition in challenging acoustic conditions.\n\nThe input features of the models are the mel-spectrograms for the audio track and the raw RGB facial images for the visual track. For the audio modality, we followed the same preprocessing procedure as in  [18]  and used the SpecAugment Library  [22]  to augment the mel-spectrograms. For augmenting the video modality, we used the TorchVision Library  [23] . Audio: The extended multi-channel HTS-AT model addresses the integration of multi-microphone information, employing the Swin-Transformer architecture  [24] , 2  a variant of the Vision Transformer (ViT)  [25]  architecture. The architecture is also applicable to the single-microphone configurations, namely C = 1. The model's architecture consists of four groups, each comprising Swin-Transformer blocks with varying depths. In addition, the model uses a hierarchical structure and windowed attention mechanism to efficiently process melspectrograms, which serve as our audio feature extractor. We use the two multi-channel variants with the modified HTS-AT module, as proposed in  [18] : 1) Patch-Embed Summationthe mel-spectrogram of each channel is processed through a shared Patch-Embed layer, after which the outputs are summed across channels; and 2) Average mel-spectrogramsmel-spectrograms from multiple channels are averaged before being fed into the model. More details can be found in  [18] . Video: For video feature extraction, we employ the pretrained R(2 + 1)D model, an 18-layer ResNet-based architecture designed for action recognition. The R(2 + 1)D model decomposes the 3D convolutions into separate spatial (2D) and temporal (1D) convolutions, which allows it to effectively capture both spatial and temporal features in the video data. Fig.  1  presents the integration of the two modalities. Feature Concatenation: The feature embeddings are extracted from the extended multi-channel HTS-AT and the R(2 + 1)D models, followed by concatenation to create a unified multi-modal representation. This combined feature vector captures audio and visual cues relevant to emotion recognition. The concatenated features are then passed through two fully connected layers for final classification. These layers learn to interpret the combined audio-visual features and to map them to emotion categories. The output of the final layer corresponds to the predicted emotion class. This integrated scheme ensures that the multi-channel audio and visual data are effectively processed and leveraged. This allows the model to capture and utilize complementary information from both modalities, thus achieving improved ER accuracy.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Experimental Study",
      "text": "This section outlines the experimental setup and describes the comparative analysis between the proposed scheme and a baseline method.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Datasets",
      "text": "Our work utilized the RAVDESS dataset for emotion recognition. This dataset includes 24 actors, equally divided between male and female speakers, each delivering 60 English sentences. Hence, there are 1440 audio-video pairs representing eight different emotions ('sad,' 'happy,' 'angry,' 'calm,' 'fearful,' 'surprised,' 'neutral,' and 'disgust'). All utterances are pre-transcribed. Therefore, the emotions are expressed more artificially compared to spontaneous conversation. The RAVDESS dataset is balanced across most classes except for the neutral class, which has a relatively small number of utterances. We used an actor-split approach, dividing the data into 80% training, 10% validation, and 10% test sets, ensuring no actor appears in more than one split. As a result, model accuracy may be lower than reported in some prior works because the test set includes actors not seen during fine-tuning. As publicly available multi-microphone datasets for Speech Emotion Recognition (SER) do not exist, we generated our own dataset. We used synthesized RIRs to fine-tune the multichannel experiment model. We employed the 'gpuRIR' Python package  3  to simulate reverberant multi-channel microphone signals (setting the number of microphones to C = 3). Each clean audio sample from the RAVDESS dataset was convolved with distinct multi-channel RIRs, resulting in 1440 3-microphone audio samples. The associated video data is unaffected by reverberation. We simulated rooms with lengths and widths uniformly distributed between 3 m and 8 m, maintaining a constant height of 2.9 m and an aspect ratio between 1 and 1.6. We randomly positioned the sound source and microphones within these simulated environments under the following constraints. The sound source was placed at a fixed height of 1.75 m, with its x and y coordinates randomly determined within the room, ensuring a minimum distance of 0.5 m from the room walls. Similarly, the microphones were positioned at a fixed height of 1.6 m, with their x and y coordinates also randomly determined within the room dimensions. The reverberation time was set at the range T 60 = 500 -850 ms. The distance between the sound source and microphones was randomly selected in the range [0.2, d c ] m, where d c to the critical distance, determined by the room's volume and T 60 . This ensures the dominance of direct sound over reflections. Finally, we added spatiallywhite noise with signal-to-noise ratio (SNR) of 20 dB to each reverberant signal. This noise was synthesized by applying an auto-regressive filter of order 1 to a white Gaussian noise, emphasizing lower frequencies.\n\nThe proposed scheme was evaluated using real-world RIRs drawn from the ACE database  [21] . The ACE RIR database comprises recordings from seven different rooms with varying dimensions and reverberation levels (see Table  II ). We only used a subset of the database, recorded with a mobile phone equipped with three microphones in the near-field scenario, which is a practical choice for real-world SER applications. We convolved all audio utterances of the test set with the ACE RIRs to generate 3-microphone signals for each utterance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Algorithm Setup",
      "text": "As discussed earlier, the video modality leverages the R(2+1)D model pre-trained on the action recognition Kinetics dataset  [26] . To better suit our ER task, we modified the model's architecture by adjusting the final linear layer. Specifically, we reconfigured it to output 768-dimensional feature embeddings. This adjustment ensures that both modalities (video and audio) contribute equally-sized feature vectors to the multi-modal representation by fusion through concatenation.\n\nThe resolution of the RGB video frames was first reduced to 224 × 224 pixels. Then, eight frames from the video stream were randomly selected. To augment the dataset, these frames underwent refinement through random cropping using the TorchVision Library  [23] , yielding 180 × 180 images that enhance the model's robustness to spatial variations. In addition, we added random horizontal and vertical flips, each with a 30% probability of application, coupled with arbitrary rotations within the range of [-30\n\nThe audio modality applies an extended version of the HTS-AT model  [18] , suitable for both multi-channel and single-channel scenarios. As described in Sec. III, the network structure configuration is arranged into four groups, each containing several Swin-Transformer blocks: 2, 2, 6, and 2, respectively. The mel-spectrogram input is initially transformed into patches and linearly projected to a dimension of D = 96. This dimension expands exponentially through each transformer group, finally reaching a dimension of 768 (8D = 768), which matches the design principles of Audio Spectrogram Transformer (AST). Pre-processing was carried out as explained in  [18]  both for multi-channel and singlechannel experiments. We augmented the mel-spectrograms by using the SpecAugment Library  [22] , which consists of temporal masking, occluding four distinct \"strips\", each 64 time-frames long. Complementing this, we applied frequency domain masking, obscuring two strips, each 8 frequency bands wide.\n\nOur multi-modal approach combines the feature embeddings from both the video and audio modalities. The 768dimensional feature vectors extracted from the R(2 + 1)D model and the extended HTS-AT model are concatenated, resulting in a 1536-dimensional feature representation. This combined feature vector is then fed into a classification head for prediction. The right-hand side of Fig.  1  presents two fully connected layers (f c) with a Relu activation function between them, forming the sequence:\n\nThe fine-tuning processes are applied using the Adam optimizer with a learning rate of 1e -3 and a warm-up strategy. We used cross-entropy loss as the metric with a batch size of 32. The maximum number of epochs was set to 500 for all experiments, with an early stopping strategy with a patience of 12 to prevent overfitting. In practice, the maximum number of epochs was never reached, as the fine-tuning process was halted earlier due to the activation of the patience parameter. The overall number of parameters for the fine-tuned models are as follows: 32.3M for the uni-modal scheme based on video, 28.7M for the uni-modal scheme based on audio, and 62.7M for the multi-modal scheme.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Table I:",
      "text": "The Accuracy results of single-microphone MER method compared with SOTA MER methods tested on the original RAVDESS dataset. Results for the competing methods are taken from the corresponding articles.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methods Acc (%)",
      "text": "Human performance  [20]  80.00\n\nGaraiman et al.  [27]  65.76 Ghaleb et al.  [28]  76.30 Franceschini et al.  [8]  78.54 Radoi et al.  [29]  78.70 Luna-Jiménez et al.  [30]  80.08 Proposed MER (C = 1) 80.00",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Results",
      "text": "Table  I  compares the performance of the proposed MER approach (single-microphone variant, C = 1) with several stateof-the-art (SOTA) MER approaches evaluated on RAVDESS. The results indicate that our single-microphone MER achieves performance on par with  [8] ,  [29] ,  [30] . Moreover, to assess and visualize the separation capabilities of the proposed scheme across the clean RAVDESS dataset, we employed the t-distributed Stochastic Neighbor Embedding (t-SNE) visualization method. This nonlinear technique reduces highdimensional data into two-or three-dimensional representations suitable for graphical visualization. Importantly, it maps nearby points in the high-dimensional space to close points in the reduced space, while far-apart points remain distant in the visualization  [31] . In Fig.  2 , we compare the t-SNE mapping of the input features extracted from the RAVDESS dataset and the network output, depicting each emotion with a unique color and shape to visualize the clustering quality. The enhancement in classification performance following the network's application is immediately apparent. We now turn to the evaluation of the multi-channel schemes, using the reverberant RAVDESS dataset version, applying MER with a multi-microphone (C = 3). Table II details our Accuracy results of the various emotion recognition schemes for seven different rooms from the ACE database. The videoonly modality is compared with the audio-only modality (both single-and multi-channel models) and the combined multimodal approach. As the video modality is unaffected by the acoustic conditions, we only report the results once. We investigated two single-channel (C = 1) variants: one finetuned on clean speech and the other on reverberant speech. Both were evaluated using a single microphone from the ACE test set. To assess the reliability of our results, we report the mean results together with 75% confidence intervals.   Analyzing Table  II , it is observed that for the audioonly schemes, the multi-channel processing methods (Avg mel and Sum PE) consistently outperform the single-channel approaches. This is in line with the findings of  [18] . Notably, the multi-modal approaches significantly outperform their unimodal counterparts. These results are also visually demonstrated in Fig.  3 , demonstrating the advantages of multi-modal processing. In addition, Fig.  4  presents the confusion matrix for the multi-channel Sum PE MER model. The confusion matrix compares the actual target and predicted labels, showing the percentage of correct and incorrect predictions for each class. Beyond measuring accuracy, it also reveals the distribution of errors across different emotions, helping to identify specific misclassifications.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusions",
      "text": "In this paper, we presented a MER system designed to operate in reverberant and noisy acoustic environments. Our approach demonstrates robust performance across a range of TABLE II: Accuracy and the associated confidence intervals of the proposed method for the RAVDESS test set reverberated by RIRs drawn from the ACE database (using the 3-microphone of the near-filed cellular phone). The 'Single-Channel' columns use an arbitrarily chosen microphone, fine-tuned on either clean or reverberant data, respectively. The 'Avg mel' columns present results with mel-spectrograms averaged across three channels during fine-tuning and testing. The 'Sum PE' columns depict the Patch-Embed fusion approach fine-tuned and tested on the three channels. The asterisk in the video column describes the same result. The best results for each modality are underlined, while the overall best result is shown in boldface.",
      "page_start": 4,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: presents the integration of the two modalities.",
      "page": 2
    },
    {
      "caption": "Figure 1: The proposed Multi-modal Emotion Recognition (MER).",
      "page": 3
    },
    {
      "caption": "Figure 1: presents two fully",
      "page": 3
    },
    {
      "caption": "Figure 2: , we compare the t-SNE",
      "page": 4
    },
    {
      "caption": "Figure 2: t-SNE visualization.",
      "page": 4
    },
    {
      "caption": "Figure 3: Accuracy and confidence intervals assessed using the",
      "page": 4
    },
    {
      "caption": "Figure 3: , demonstrating the advantages of multi-modal",
      "page": 4
    },
    {
      "caption": "Figure 4: presents the confusion matrix",
      "page": 4
    },
    {
      "caption": "Figure 4: Confusion matrix of the results of multi-channel Sum",
      "page": 4
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Text-based emotion recognition using deep learning approach",
      "authors": [
        "S Bharti"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "2",
      "title": "Study of speech emotion recognition using BLSTM with attention",
      "authors": [
        "D Sherman",
        "G Hazan",
        "S Gannot"
      ],
      "year": "2023",
      "venue": "European Signal Processing Conf. (EUSIPCO)"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition from large-scale video clips with cross-attention and hybrid feature weighting neural networks",
      "authors": [
        "S Zhou"
      ],
      "year": "2023",
      "venue": "Int. Journal of Environmental Research and Public Health"
    },
    {
      "citation_id": "4",
      "title": "Audio and video-based emotion recognition using multimodal transformers",
      "authors": [
        "V John",
        "Y Kawanishi"
      ],
      "year": "2022",
      "venue": "Int. Conf. on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "5",
      "title": "Audio-visual emotion recognition in video clips",
      "authors": [
        "F Noroozi"
      ],
      "year": "2017",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Multimodal end-to-end sparse model for emotion recognition",
      "authors": [
        "W Dai"
      ],
      "year": "2021",
      "venue": "Multimodal end-to-end sparse model for emotion recognition",
      "arxiv": "arXiv:2103.09666"
    },
    {
      "citation_id": "7",
      "title": "An active learning paradigm for online audio-visual emotion recognition",
      "authors": [
        "I Kansizoglou",
        "L Bampis",
        "A Gasteratos"
      ],
      "year": "2019",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Multimodal emotion recognition with modalitypairwise unsupervised contrastive loss",
      "authors": [
        "R Franceschini"
      ],
      "year": "2022",
      "venue": "International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "9",
      "title": "Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "IEEE Int. Conf. on Multimedia and Expo (ICME"
    },
    {
      "citation_id": "10",
      "title": "Fusing visual attention cnn and bag of visual words for cross-corpus speech emotion recognition",
      "authors": [
        "M Seo",
        "M Kim"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "11",
      "title": "Video-audio emotion recognition based on feature fusion deep learning method",
      "authors": [
        "Y Song",
        "Y Cai",
        "L Tan"
      ],
      "year": "2021",
      "venue": "IEEE Int. Midwest Symposium on Circuits and Systems (MWSCAS)"
    },
    {
      "citation_id": "12",
      "title": "Multimodal and temporal perception of audio-visual cues for emotion recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "Int. Conf. on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "13",
      "title": "Facenet: A unified embedding for face recognition and clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conf. on computer vision and pattern recognition"
    },
    {
      "citation_id": "14",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh"
      ],
      "year": "2018",
      "venue": "Proc. of the Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "15",
      "title": "GloVe: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Conf. on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "16",
      "title": "M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues",
      "year": "2020",
      "venue": "AAAI Conf. on Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Modulated fusion using transformer for linguistic-acoustic emotion recognition",
      "authors": [
        "J.-B Delbrouck",
        "N Tits",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "Modulated fusion using transformer for linguistic-acoustic emotion recognition",
      "arxiv": "arXiv:2010.02057"
    },
    {
      "citation_id": "18",
      "title": "Multi-microphone speech emotion recognition using the hierarchical token-semantic audio transformer architecture",
      "authors": [
        "O Cohen",
        "G Hazan",
        "S Gannot"
      ],
      "venue": "2024, accepted to IEEE Int. Conf. on Acoustics, Speech and Signal Processing",
      "arxiv": "arXiv:2406.03272"
    },
    {
      "citation_id": "19",
      "title": "A closer look at spatiotemporal convolutions for action recognition",
      "authors": [
        "D Tran"
      ],
      "year": "2018",
      "venue": "IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "20",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "21",
      "title": "Estimation of room acoustic parameters: The ACE challenge",
      "authors": [
        "J Eaton"
      ],
      "year": "2016",
      "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "22",
      "title": "SpecAugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park"
      ],
      "year": "2019",
      "venue": "SpecAugment: A simple data augmentation method for automatic speech recognition"
    },
    {
      "citation_id": "23",
      "title": "TorchVision: PyTorch's Computer Vision library",
      "year": "2016",
      "venue": "TorchVision: PyTorch's Computer Vision library"
    },
    {
      "citation_id": "24",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu"
      ],
      "year": "2021",
      "venue": "IEEE/CVF Int. Conf. on computer vision"
    },
    {
      "citation_id": "25",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy"
      ],
      "venue": "Int. Conf. on Learning Representations (ICLR)"
    },
    {
      "citation_id": "26",
      "title": "The kinetics human action video dataset",
      "authors": [
        "W Kay"
      ],
      "year": "2017",
      "venue": "The kinetics human action video dataset",
      "arxiv": "arXiv:1705.06950"
    },
    {
      "citation_id": "27",
      "title": "Multimodal emotion recognition system based on x-vector embeddings and convolutional neural networks",
      "authors": [
        "F Garaiman",
        "A Radoi"
      ],
      "venue": "International Conference on Communications (COMM)"
    },
    {
      "citation_id": "28",
      "title": "Multimodal attentionmechanism for temporal emotion recognition",
      "authors": [
        "E Ghaleb",
        "J Niehues",
        "S Asteriadis"
      ],
      "year": "2020",
      "venue": "IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "29",
      "title": "An end-to-end emotion recognition framework based on temporal aggregation of multimodal information",
      "authors": [
        "A Radoi"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "30",
      "title": "Multimodal emotion recognition on RAVDESS dataset using transfer learning",
      "authors": [
        "C Luna-Jiménez"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "31",
      "title": "Visualizing data using t-SNE",
      "authors": [
        "L Van Der Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}