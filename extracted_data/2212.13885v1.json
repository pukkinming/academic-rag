{
  "paper_id": "2212.13885v1",
  "title": "Emotion Recognition With Pre-Trained Transformers Using Multimodal Signals Author Version",
  "published": "2022-12-22T14:32:52Z",
  "authors": [
    "Juan Vazquez-Rodriguez",
    "Grégoire Lefebvre",
    "Julien Cumin",
    "James L Crowley"
  ],
  "keywords": [
    "Affective Computing",
    "Multimodal Emotion Recognition",
    "Machine Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this paper, we address the problem of multimodal emotion recognition from multiple physiological signals. We demonstrate that a Transformer-based approach is suitable for this task. In addition, we present how such models may be pretrained in a multimodal scenario to improve emotion recognition performances. We evaluate the benefits of using multimodal inputs and pre-training with our approach on a state-ofthe-art dataset.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The increasing availability of mass-market wearable devices equipped with sensors for physiological signals provides new possibilities for monitoring the emotional health and wellbeing of users  [1] . Although less reliable than medical-grade sensors, signals from wearable sensors like electrocardiograms (ECG) and electroencephalograms (EEG) can be combined to provide estimates of the emotional state of users.\n\nIn this work, we report on experiments with a Transformerbased approach for interpreting emotional state from different physiological signals obtained from wearable devices. We explore the estimation of emotional state from individual sensor modalities, including EEG and ECG, and show that fusing the two modalities leads to better results indicating that these modalities convey complementary information. Furthermore, we demonstrate that a Transformer-based approach can be used to provide reliable estimates of emotional state from such signals.\n\nWe center our work on recognizing emotions from ECG and EEG signals. There are other works that address multimodal emotion recognition  [2] [3] [4] [5] , but the majority use signals such as images, sound and text, and not physiological signals. Although some authors have explored the use of physiological signals for emotion recognition  [6] [7] [8] , such signals have received less attention than other sensing modalities.\n\nA common problem when addressing the task of emotion recognition is the lack of labeled data to effectively train deeplearning models  [6] . A possible approach to address this problem is the use of unsupervised pre-training techniques  [9] . However, pre-training with multiple signal modalities raises additional challenges. In this work, we investigate the use of a late-fusion approach, where we pre-train and fine-tune different single-modality models, and then combine the outputs of the individual models to obtain a fused feature that can be used to perform emotion prediction.\n\nWe use a Transformer  [10]  to process physiological signals.\n\nThe Transformer was originally developed for Natural Language Processing (NLP) tasks, with the intent of processing sequences of words. Given that physiological signals are sequences of values, the Transformer can be adapted for physiological signal processing  [11] . Transformers employ a learned attention mechanism to dynamically score the relevance of different parts of an input according to context. Attentionbased processing is appropriate for processing physiological signals, as some parts of a signal may convey more information than other parts depending on the task and context. Another advantage of using a Transformer is that we can benefit from a very successful pre-training technique described in BERT  [12]  and developed for NLP tasks, which we can adapt to our needs. This pre-training strategy has been successfully adapted to other domains like Computer Vision  [13] , Speech Processing  [14]  and Affective Computing  [15] .\n\nThe main contributions of this paper are:\n\n1. We present a technique for recognizing emotions from multimodal physiological signals using a Transformer.\n\n2. We describe a method to pre-train the Transformer for recognizing emotions from multimodal physiological signals.\n\n3. We provide results from experiments that show that a multimodal pre-training strategy is effective for improving emotion recognition performances.\n\nContrary to traditional techniques like Gaussian naive Bayes  [16] , k-Nearest Neighbours  [17]  and Support Vector Machines  [6] , deep-learning may be used to recognize emotions directly from sensor signals without a need to design feature descriptors. This is particularly useful for the recognition of emotions from physiological signals where there are no well-established feature descriptors for signal encoding.\n\nAn example of a deep-learning approach is provided by the work of Santamaria et al.  [18] , where they employ models based on Convolutional Neural Networks (CNN) to perform emotion recognition. Another example is the work of Harper and Southern  [19] , who use a combination of Recurrent Neural Networks (RNN) and CNNs. The Transformer  [10] , which uses stacked layers of self-attention, has recently emerged as a powerful alternative to Convolutional and Recurrent Networks. In this work, we are interested in whether a Transformer architecture can be an effective tool to recognize emotions from multiple physiological signals.\n\nA variety of authors have explored deep-learning models for emotion recognition using multimodal signals. Most of these works use images, audio and/or text as inputs  [2-5, 20, 21] . In a few cases, physiological signals have been used to improve recognition from image, audio and text  [22] [23] [24] . A few authors have described the use of multiple physiological signal modalities  [6] [7] [8] . These works consistently demonstrate the benefits of exploiting multiple signal modalities to improve the performance of emotion recognition.\n\nSome multimodal approaches employ pre-training techniques to improve their results. The authors of  [2] [3] [4] [5]  develop models based on Transformers, using images, audio and text to recognize emotions. Rahman et al.  [2]  report on the use of BERT  [12] , a Transformer-based model pre-trained for NLP tasks, to process text, incorporating visual and audio modalities in a middle-fusion process. Siriwardhana et al.  [3, 4]  describes the use of pre-trained models to extract features from visual, audio and text modalities, and followed by a cross-modal Transformer  [25]  to combine these different features. Khare et al.\n\n[5] use a BERT-like approach, masking some words in the input text, along with the audio and visual parts that correspond to those words, and then pre-training the model by predicting the masked words. 3 Approach",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Multimodal Emotion Recognition",
      "text": "When performing a classification task using multiple modalities, an important problem is to determine the processing level at which different signal modalities should be combined or fused. One option is early-fusion, where the combination of input features is used as input for the model. A second option is to do late-fusion, where the outputs of single-modality models are combined and a second-level model is trained to perform the classification. A third option is a compromise between these two extremes: middle-fusion that combines features from intermediate layers of the models.\n\nEarly-fusion can be provided by simply concatenating the input signals in the temporal dimension, to form a single (longer) sequence. The problem with this approach is that the computational complexity of the Transformer is O(n 2 ), where n is the length of the input. Therefore, a late-fusion approach has the advantage of simplifying the training process: several singlemodality models can be trained one by one on less powerful hardware than the one required to train a single, more computationally expensive multimodal model. Then, if the singlemodality models are frozen, the second-level model of the latefusion approach can also be easily trained without using very powerful hardware resources. A similar reasoning can be applied to see the advantages of late-fusion over middle-fusion in this scenario.\n\nAnother major difficulty with early-fusion is that it hinders the benefits of pre-training techniques. With pre-training, we seek to use many different datasets, not necessarily related to the task of emotion recognition, to obtain a more general representation of the information from the different modalities. However, early-fusion limits pre-training to datasets that include all of the targeted modalities, thus severely limiting the availability of datasets that can be used. With late-fusion, one can pre-train each single-modality model independently from one another, with potentially different datasets.\n\nIn our case, we are interested in a multimodal approach that allows us to use pre-training techniques to improve the per-formance of the model. In particular, we are interested in attention-based models such as the Transformer  [10] , and in pre-training techniques similar to the ones used in BERT  [12] .\n\nIn this work, we explore the use of a late fusion approach. This allows self-supervised pre-training for individual sensor modalities by reconstructing masked values in the input signal, similar to what is done in BERT  [12] . We refer to this as Masked Value Prediction or simply MVP.\n\nWith this approach, recognition training is performed in two steps. In the first step, we train two single-modality models: one to recognize emotions from electrocardiogram (ECG) signals, and one to recognize emotions from electroencephalogram (EEG) signals. Both models are trained separately, using MVP pre-training. In the second step, we concatenate the outputs of the single-modality models and use this combined representation to train a Fully-Connected Network (FCN) to recognize emotions. While other fusion approaches may be possible like max or average pooling, or majority voting, concatenation has been found to provide a simple and effective technique for use with Transformers.\n\nIn the rest of this section, we provide details about the ECG and EEG single-modality models, and the fused model that combines both signals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ecg Single-Modality Emotion Recognition",
      "text": "For our ECG emotion recognizer, we employ the approach described in  [15]  and depicted in Figure  1 . In this approach, a pre-training step is first used prior to fine-tuning the model to improve its performance. We provide a brief description of this approach in the remainder of this subsection. We refer the reader to the original paper for further details.\n\nThe ECG single-modality emotion recognition model from  [15]  is based on the Transformer  [10] . The Transformer is an architecture capable of incorporating contextualized information thanks to its self-attention mechanisms. As shown in Figure  1 , the model first encodes ECG signals using a 1D Convolutional Neural Network (1D-CNN) to obtain a sequence of features that represent the input signal. Then, similar to BERT  [12] , a classification token named CLS is added to the beginning of the sequence of features. In the next step, the feature sequence appended with the CLS token is fed into a Transformer, which produces contextualized representations of the signal. Then, the process follows one of two paths, depending on if we are in the pre-training phase or the fine-tuning phase.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Training Phase",
      "text": "The pre-training task consists in using MVP, that is, masking some points from the original signal, and then predicting those masked points with the help of a Fully-Connected Network (FCN) placed on top of the Transformer, as can be seen in path A of Figure  1 . Since this task is self-supervised, we do not need labeled data for this phase.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fine-Tuning Phase",
      "text": "After the model has been pre-trained, it is fine-tuned to recognize emotions. As depicted in path B of Figure  1 , the vector CLS e , which is the representation of the CLS token, is used as input of an FCN that functions as a classifier to predict the emotion. Starting with the same pre-trained weights, the model is fine-tuned twice: one to predict arousal and another to predict valence. Thus, Predicted Emotion in Figure  1  refers to either arousal or valence. During this phase, all the parameters of the model, including the Transformer and the 1D-CNN parameters, are fine-tuned. This phase is supervised, therefore labeled data is employed.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Eeg Single-Modality Emotion Recognition",
      "text": "To build our EEG single-modality emotion recognition model, we adapt the ECG model from  [15]  and described in the previous subsection, to accept EEG signals.\n\nFirst, we need to take into account that the ECG signal has only one channel, while the EEG signal is typically multichannel. Therefore, the 1D-CNN that encodes the raw signal has to be changed from having one-channel input to having as many inputs as the number of channels present in the EEG signal, while the rest of the structure of the 1D-CNN remains the same. The shape of the output from the 1D-CNN encoder remains similar to the original ECG model, and thus we can keep the same Transformer architecture.\n\nSecondly, during the pre-training of the model, we mask the same temporal segments of the EEG signal across all the different channels. The size of the output layer of the FCN that works as masked-point predictor has to be the same as the number of input channels. This way, each predicted output value corresponds to the (masked) value of each channel.\n\nAside from these changes, no other adaptation is needed for fine-tuning the EEG model, as we employ the CLS e vector as input for the FCN used as a classifier to predict the emotion, as it is done in the ECG model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fused-Signals Emotion Recognition Model",
      "text": "We use late-fusion to fuse the ECG and EEG signals, using the outputs from each of the trained single-modality emotion recognizers. As depicted in Figure  2 , we take the output of the last hidden layer of the FCN (not from the output layer) of each single-modality model, and we concatenate those outputs to form our combined features. Although other methods might be considered to do this fusion, for example using average pooling of the outputs of the single-modality recognizers, we employ concatenation because it allows us to have singlemodality models with outputs of different sizes. This is convenient because this way we can choose without constraints the output sizes that make the fused model perform the best.\n\nTo perform the emotion prediction from the fused modalities, we use another FCN that we refer to as Multimodal Emotion Classifier in Figure  2 . For this step, we freeze the weights of the single-modality emotion recognizers, and only train the",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "We evaluate our model on the task of binary emotion prediction, that is to predict high and low levels of arousal and valence, from multimodal physiological signals.\n\nIn this section, we describe our experimental setup, giving details about the datasets and the hyper-parameters for both EEG and fused-based models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "To train and evaluate our models, we used the AMIGOS dataset  [6] . This dataset includes data from 40 subjects, where emotions were induced by making the subjects watch emotional videos. A total of 37 subjects watched 20 videos, while 3 subjects watched only 16. After watching each video, the subjects filled out a self-assessment form where they rated on a scale of 1 to 9 their levels of arousal and valence. We use the results of this self-assessment as labels in our experiments. Since we are interested in binary emotion classification, we use the average value as a threshold to obtain high and low classes of arousal and valence. In total, there are around 65 hours of data in the AMIGOS dataset.\n\nThe AMIGOS dataset includes both ECG and EEG signals.\n\nWe treat ECG as a single-channel signal, and we use the signal taken from the left arm. For EEG, we use 10 channels: F7, F3, T7, P7, O1, O2, P8, T8, F4, F8. We chose those 10 channels from the 14 available because these channels are also present in the datasets that we use for pre-training, which we will de-scribe shortly. We use the provided 128Hz down-sampled signals. ECG signals are filtered with a low-pass filter with a cut-off frequency of 60Hz, and EEG signals are filtered with a band-pass filter with frequencies between 4.0 and 45.0Hz.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Data For Pre-Training",
      "text": "As described in Section 3.2, the first step in training a singlemodality emotion recognizer is pre-training. To pre-train the EEG emotion recognizer, we gathered EEG data that does not necessarily include labels of emotion. We use the following datasets: WAY-EEG-GAL  [29] , BCI2000  [30, 31] , and Large-EEG-BCI  [32] . These datasets were gathered to develop Brain-Computer Interfaces, so they do not include any labels related to emotions. We also use parts of AMIGOS in the pre-training step, taking care of not using the same samples to pre-train and evaluate the model, for each fold of crossvalidation. The quantity of data that we gathered to pre-train the EEG model is comparable to the data used to pre-train the ECG model in  [15] : in total there are around 195 hours of EEG data available for pre-training, while the ECG model was pre-trained with around 230 hours of data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Signal Pre-Processing",
      "text": "Much like  [15]  for ECG data, we filter EEG signals using an 8 th order Butterworth band-pass filter, with cut-off frequencies of 0.8Hz and 50Hz. We also downsample signals to 128Hz.\n\nIn addition, we normalize signals with zero-mean and unitvariance for each subject. Finally, we split each signal into 10-second segments. Each segment is used as a sample in our experiments, as in other state-of-the-art works  [6, 8, 26] ,  [15] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ecg Emotion Recognizer",
      "text": "The ECG emotion recognition model follows the architecture presented by Vazquez-Rodriguez et al. in  [15] . This model was fine-tuned and evaluated on AMIGOS as presented in Section 4.1. This model was parameterized and pre-trained as described in  [15] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eeg Emotion Recognizer",
      "text": "The EEG emotion recognition model follows the architecture described in Section 3.3. The 1D-CNN is composed of three layers with kernel sizes (65,  33, 17) , with the number of channels equal to (64, 128, 256), with stride 1 in all the layers, and using the ReLU activation function. With this configuration, the size of the receptive field is 113 input points, which is equivalent to 0.88s. Based on preliminary studies, we believe that this receptive field size is suitable for EEG signals. For the Transformer, the number of layers is 2 and the number of heads is also 2, with a hidden size of 256.\n\nFor pre-training, the FCN used to predict masked points has one hidden layer with size 128 and ReLU activation, and an output layer that gives 10 outputs values, where each output value corresponds to the predicted value of each masked EEG channel. The mean squared error between the real and the predicted values is used as loss. We pre-train the model for Table  1 : Emotion recognition performances for the EEG model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Pretrain",
      "text": "Arousal Acc.\n\nArousal F1 Valence Acc.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Valence F1",
      "text": "No 0.76±7.3e -3 0.75±8.3e -3 0.7±6.4e -3 0.7±6.8e -3 Yes 0.81±10.7e -3 0.80±9.4e -3 0.77±9.3e -3 0.77±9.1e -3 500 epochs, warming up the learning rate during the first 30 epochs up to 0.0005, and then use linear decay. We use Adam optimization with β 1 = 0.9, β 2 = 0.999 and L 2 weight decay of 0.005. A dropout value of 0.1 is used in the Transformer.\n\nFor fine-tuning, the FCN that predicts binary emotions has one hidden layer with a size of 64 and ReLU activation functions.\n\nAn output layer is used to project the output to a single value, that corresponds to the prediction of the emotion. Two different networks are fine-tuned, one to predict arousal and another to predict valence. The models are fine-tuned using binary cross-entropy loss for 100 epochs, starting with a learning rate of 0.0001 and decreasing by a factor of 0.65 every 45 epochs. Adam optimization is used, with β 1 = 0.9, β 2 = 0.999 and L 2 weight decay of 0.00001. A dropout value of 0.6 is used in the FCN that predicts emotions.\n\nThese hyper-parameters were optimized using the Ray Tune toolkit  [33]  on validation data extracted from AMIGOS, for each fold of cross-validation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multimodal Emotion Recognizer",
      "text": "The multimodal emotion recognition model follows the architecture described in Section 3.4. The FCN has two hidden layers of size 64 and 32, and an output layer that projects the result to a single value used to predict the binary emotion. As we did for the single-modality models, we train one model to predict arousal and another to predict valence. The activation function used is ReLU. This network is trained for 52 epochs, starting with a learning rate of 0.00001 and decaying it every 20 epochs with a factor of 0.65. A dropout value of 0.1 is used during the training of this network. We employ Adam optimization with β 1 = 0.9, β 2 = 0.999 and L 2 weight decay of 0.00001. We also use the Ray Tune toolkit to optimize these hyper-parameters, as we did for the EEG emotion recognizer.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "In this section, we discuss the results of the different experiments we performed to evaluate our model for emotion recognition on the AMIGOS dataset. We use the mean accuracy and F1 score between the two predicted classes as metrics, averaged across 10 folds of cross-validation. We also report a twosided 95% confidence interval, calculated with a t-distribution with 9 degrees of freedom.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Eeg Single-Modality Emotion Recognition",
      "text": "We first report in Table  1  the performances of our singlemodality emotion recognizer on EEG signals, using the pre-",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Valence F1",
      "text": "ECG  [15] 0.88±5.4e -3 0.87±5.4e -3 0.83±7.8e -3 0.83±7.4e -3 EEG 0.81±10.7e -3 0.80±9.4e -3 0.77±9.3e -3 0.77±9.1e -3 Fused 0.89±5.0e -3 0.89±5.0e -3 0.85±3.8e -3 0.85±3.9e -3   training strategy described in section 3.2.1, and without using pre-training. We see that the pre-training strategy improves emotion recognition performances for all metrics, for both arousal and valence. For example, the F1 score for valence goes from 0.7±6.8e -3 when not using pre-training, to 0.77±9.1e -3 when pre-training is used. This confirms that the pre-training strategy we employ is useful when processing EEG signals, and helps the Transformer learn better representations that in turn produce better results when predicting emotion. The comparisons of all metrics shown in Table  1  have a two-tailed P value less than 1e -5 , thus the difference between them can be considered to be statistically significant.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Fused Model Results",
      "text": "In Table  2  we present the performances of our fused model, along with the performances of the single-modality models. Pre-training was used in the single-modality models, and the fused model employs those single-modality models as part of the fusion strategy, as described in Section 3.4. We see that our late-fusion approach improves performance over singlemodality models. For example, we obtain a valence accuracy in the fused model of 0.85±3.8e -3 compared to 0.83±7.8e -3 and 0.77±9.3e -3 when using only EEC and EEG signals respectively. Moreover, when comparing the results of the single-modality models with the fused strategy, the two-tailed P values are less than 1e -3 , thus the difference is statistically significant.\n\nFigures  3  and 4  show two different samples used in our experiments. Figure  3  For the sample of Figure  3 , the ECG model predicts the wrong class, while the EEG model predicts the correct class. We can see that the fused model is helpful when the ECG modality makes a wrong prediction, relying on the information from the EEG signal to do the classification correctly. In Figure  4 , we present the same channels as in the previous example, but for a sample that is classified correctly by the ECG model and misclassified by the EEG model. Now the fused model is capable of relying on the information from the ECG signal to classify correctly this sample. Therefore, our fusion model is capable of paying attention to the right modality when one is informative and the other leads to incorrect predictions. Looking at the signals for these 2 samples, it is not obvious why misclassifications occur for one modality or the other, compared to other signals. This showcases that our model is capable of extracting meaningful hidden features in both modalities.  Valence F1 No 0.86±4.9e -3 0.85±5.1e -3 0.82±6.5e -3 0.81±6.8e -3 Yes 0.89±5.0e -3 0.89±5.0e -3 0.85±3.8e -3 0.85±3.9e -3",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effectiveness Of Pre-Training In The Fused Model",
      "text": "In Table  3 , we compare the performances of our fused model, depending on whether it uses pre-trained single-modality models, or single-modality models with no pre-training. We see that the pre-trained fused model achieves superior performance compared to the fused model with no pre-training. For example, the arousal F1 score improves from 0.85±5.1e -3 to 0.89±5.0e -3 . The results shown in Table  2  have two-tailed P values less than 1e -4 , thus the difference between them is extremely statistically significant. These results indicate that the benefits obtained from pre-training single-modality models are carried over when combining them with our late-fusion strategy.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Comparison With Some Baselines On Amigos Dataset",
      "text": "We report in Table  4  the performance of our models and the results reported by the authors of the AMIGOS dataset  [6] , that we consider as a baseline. We should note that the experimental protocol used in the baseline is different than our protocol, for example the length of their segments is 20s, thus their and our results are not directly comparable. In any case, we present  those results to showcase results obtained by other works, and to see if our approach has acceptable performance. We see that our performances are much higher than the baseline, both using only EEG and also in the fused approach. The fused approach in the baseline uses Galvanic Skin Response (GSR) in addition to EEG and ECG. Our approach is performing at much higher F1 scores with one less modality, which further validates that our approach is promising.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion And Perspectives",
      "text": "In this work, we presented a new Transformer-based architecture with pre-training for emotion recognition on multimodal physiological signals. We experimentally showed, using the AMIGOS dataset, that our approach can predict valence and arousal with significant accuracy. In addition, we demonstrated that our late-fusion multimodal approach improves performances over single-modality. Finally, we compared the benefits of our pre-training strategy for multimodal situations. Overall, our architecture is capable of reaching state-of-the-art performance for emotion recognition.\n\nFuture works include investigating new ways to perform pretraining in multimodal situations: instead of pre-training individual modalities in independent Transformers, we can expect that using a single multimodal Transformer could lead to better performances. Indeed, this way, the model could start to incorporate information from different modalities as early as in the pre-training phase. In general, new ways of combining different modalities in a pre-trainable Transformer architecture, be it early-fusion, middle-fusion, or late-fusion, are valuable avenues of research to improve emotion recognition performances of such models. Moreover, the combination of physiological signals with more traditional modalities such as images and audio may help to better understand how pre-training and Transformer-based models behave for multimodal emotion recognition.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In this approach, a",
      "page": 3
    },
    {
      "caption": "Figure 1: , the model ﬁrst encodes ECG signals using a 1D Con-",
      "page": 3
    },
    {
      "caption": "Figure 1: Since this task is self-supervised, we do",
      "page": 3
    },
    {
      "caption": "Figure 1: , the vec-",
      "page": 3
    },
    {
      "caption": "Figure 2: , we take the output of",
      "page": 3
    },
    {
      "caption": "Figure 2: For this step, we freeze the weights",
      "page": 3
    },
    {
      "caption": "Figure 1: Single-Modality Emotion Recognizer: The raw signal is encoded by a 1D-CNN and processed with a Transformer.",
      "page": 4
    },
    {
      "caption": "Figure 2: Fused Model. Late-fusion is used to combine the",
      "page": 4
    },
    {
      "caption": "Figure 3: (a) and Figure 4(a) show the ECG signals,",
      "page": 6
    },
    {
      "caption": "Figure 3: (b) and 4(b) show channels F7 and F3 of the EEG",
      "page": 6
    },
    {
      "caption": "Figure 3: , the ECG model predicts",
      "page": 6
    },
    {
      "caption": "Figure 4: , we present the same channels as in the previous",
      "page": 6
    },
    {
      "caption": "Figure 3: Sample correctly classiﬁed by the Fused Model and",
      "page": 6
    },
    {
      "caption": "Figure 4: Sample correctly classiﬁed by the Fused Model and",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Emotion recognition performances for the EEG",
      "page": 5
    },
    {
      "caption": "Table 1: the performances of our single-",
      "page": 5
    },
    {
      "caption": "Table 2: Emotion recognition performances of single-modality",
      "page": 6
    },
    {
      "caption": "Table 2: we present the performances of our fused model,",
      "page": 6
    },
    {
      "caption": "Table 3: Fused Model: Pre-Training vs No Pre-Training",
      "page": 6
    },
    {
      "caption": "Table 3: , we compare the performances of our fused model,",
      "page": 6
    },
    {
      "caption": "Table 2: have two-tailed",
      "page": 6
    },
    {
      "caption": "Table 4: the performance of our models and the re-",
      "page": 6
    },
    {
      "caption": "Table 4: Comparison with the AMIGOS Dataset Baseline",
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Human-Computer Interaction Using Emotion Recognition from Facial Expression",
      "authors": [
        "F Abdat",
        "C Maaoui",
        "A Pruski"
      ],
      "year": "2011",
      "venue": "2011 UKSim 5th European Symposium on Computer Modeling and Simulation"
    },
    {
      "citation_id": "2",
      "title": "Integrating Multimodal Information in Large Pretrained Transformers",
      "authors": [
        "Wasifur Rahman",
        "Md Kamrul Hasan",
        "Sangwu Lee",
        "Ami-Rali Bagher Zadeh",
        "Chengfeng Mao",
        "Louis-Philippe Morency",
        "Ehsan Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "3",
      "title": "Multimodal Emotion Recognition With Transformer-Based Self Supervised Feature Fusion",
      "authors": [
        "S Siriwardhana",
        "T Kaluarachchi",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "Jointly Fine-Tuning \"BERT-like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "authors": [
        "Shamane Siriwardhana",
        "Andrew Reis"
      ],
      "year": "2020",
      "venue": "Jointly Fine-Tuning \"BERT-like\" Self Supervised Models to Improve Multimodal Speech Emotion Recognition",
      "arxiv": "arXiv:2008.06682"
    },
    {
      "citation_id": "5",
      "title": "Self-Supervised Learning with Cross-Modal Transformers for Emotion Recognition",
      "authors": [
        "Aparna Khare",
        "Srinivas Parthasarathy",
        "Shiva Sundaram"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "6",
      "title": "AMIGOS: A Dataset for Affect, Personality and Mood Research on Individuals and Groups",
      "authors": [
        "J Miranda Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Utilizing Deep Learning Towards Multi-modal Bio-sensing and Vision-based Affective Computing",
      "authors": [
        "S Siddharth",
        "T Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Unsupervised multi-modal representation learning for affective computing with multi-corpus wearable data",
      "authors": [
        "Kyle Ross",
        "Paul Hungler",
        "Ali Etemad"
      ],
      "year": "2021",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "9",
      "title": "Why Does Unsupervised Pre-training Help Deep Learning?",
      "authors": [
        "Dumitru Erhan",
        "Aaron Courville",
        "Yoshua Bengio",
        "Pascal Vincent"
      ],
      "year": "2010",
      "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics"
    },
    {
      "citation_id": "10",
      "title": "Attention is All you Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Fusing Transformer Model with Temporal Features for ECG Heartbeat Classification",
      "authors": [
        "Genshen Yan",
        "Shen Liang",
        "Yanchun Zhang",
        "Fan Liu"
      ],
      "year": "2019",
      "venue": "2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "12",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "13",
      "title": "VideoBERT: A Joint Model for Video and Language Representation Learning",
      "authors": [
        "Chen Sun",
        "Austin Myers",
        "Carl Vondrick",
        "Kevin Murphy",
        "Cordelia Schmid"
      ],
      "year": "2019",
      "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "14",
      "title": "Speech Recognition by Simply Fine-Tuning Bert",
      "authors": [
        "Wen-Chin Huang",
        "Chia-Hua Wu",
        "Shang-Bao Luo",
        "Kuan-Yu Chen",
        "Hsin-Min Wang",
        "Tomoki Toda"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "15",
      "title": "Transformer-Based Self-Supervised Learning for Emotion Recognition",
      "authors": [
        "Juan Vazquez-Rodriguez",
        "Grégoire Lefebvre",
        "Julien Cumin",
        "James Crowley"
      ],
      "year": "2022",
      "venue": "Transformer-Based Self-Supervised Learning for Emotion Recognition",
      "arxiv": "arXiv:2204.05103"
    },
    {
      "citation_id": "16",
      "title": "An Inter-domain Study for Arousal Recognition from Physiological Signals",
      "authors": [
        "Martin Gjoreski",
        "Blagoj Mitrevski",
        "Mitja Luštrek",
        "Matjaž Gams"
      ],
      "year": "2018",
      "venue": "Informatica"
    },
    {
      "citation_id": "17",
      "title": "Wearable Emotion Recognition Using Heart Rate Data from a Smart Bracelet",
      "authors": [
        "Lin Shu",
        "Yang Yu",
        "Wenzhuo Chen",
        "Haoqiang Hua",
        "Qin Li",
        "Jianxiu Jin",
        "Xiangmin Xu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "18",
      "title": "Using Deep Convolutional Neural Network for Emotion Detection on a Physiological Signals Dataset (AMIGOS)",
      "authors": [
        "L Santamaria-Granados",
        "M Munoz-Organero",
        "G Ramirez-González",
        "E Abdulhay",
        "N Arunkumar"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "19",
      "title": "A Bayesian Deep Learning Framework for End-To-End Prediction of Emotion from Heartbeat",
      "authors": [
        "R Harper",
        "J Southern"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Multi-modal Conditional Attention Fusion for Dimensional Emotion Prediction",
      "authors": [
        "Shizhe Chen",
        "Qin Jin"
      ],
      "year": "2016",
      "venue": "Proceedings of the 24th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Multimodal and Temporal Perception of Audio-visual Cues for Emotion Recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "22",
      "title": "Multimodal Attention Network for Continuous-Time Emotion Recognition Using Video and EEG Signals",
      "authors": [
        "D Choi",
        "D.-H Kim",
        "B Song"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "23",
      "title": "Exploiting EEG Signals and Audiovisual Feature Fusion for Video Emotion Recognition",
      "authors": [
        "B Xing",
        "H Zhang",
        "K Zhang",
        "L Zhang",
        "X Wu",
        "X Shi",
        "S Yu",
        "S Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "24",
      "title": "EmoTour: Multimodal Emotion Recognition using Physiological and Audio-Visual Features",
      "authors": [
        "Yuki Matsuda",
        "Dmitrii Fedotov",
        "Yuta Takahashi",
        "Yutaka Arakawa",
        "Keiichi Yasumoto",
        "Wolfgang Minker"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 ACM International Joint Conference and 2018 International Symposium on Pervasive and Ubiquitous Computing and Wearable Computers"
    },
    {
      "citation_id": "25",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Self-Supervised Learning for ECG-Based Emotion Recognition",
      "authors": [
        "P Sarkar",
        "A Etemad"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "27",
      "title": "Emotion Recognition Using Multimodal Deep Learning",
      "authors": [
        "Wei Liu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2016",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "28",
      "title": "An Attribute-invariant Variational Learning for Emotion Recognition Using Physiology",
      "authors": [
        "H Yang",
        "C Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "29",
      "title": "Multi-channel EEG recordings during 3,936 grasp and lift trials with varying weight and friction",
      "authors": [
        "Matthew Luciw",
        "Ewa Jarocka",
        "Benoni Edin"
      ],
      "year": "2014",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "30",
      "title": "BCI2000: A general-purpose brain-computer interface (BCI) system",
      "authors": [
        "Gerwin Schalk",
        "Dennis Mcfarland",
        "Thilo Hinterberger",
        "Niels Birbaumer",
        "Jonathan Wolpaw"
      ],
      "year": "2004",
      "venue": "IEEE transactions on bio-medical engineering"
    },
    {
      "citation_id": "31",
      "title": "PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals",
      "authors": [
        "A Goldberger",
        "L Amaral",
        "L Glass",
        "J Hausdorff",
        "P Ivanov",
        "R Mark",
        "J Mietus",
        "G Moody",
        "C Peng",
        "H Stanley"
      ],
      "year": "2000",
      "venue": "Circulation"
    },
    {
      "citation_id": "32",
      "title": "A large electroencephalographic motor imagery dataset for electroencephalographic brain computer interfaces",
      "authors": [
        "Murat Kaya",
        "Mustafa Binli",
        "Erkan Ozbay",
        "Hilmi Yanar",
        "Yuriy Mishchenko"
      ],
      "year": "2018",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "33",
      "title": "Tune: A Research Platform for Distributed Model Selection and Training",
      "authors": [
        "Richard Liaw",
        "Eric Liang",
        "Robert Nishihara",
        "Philipp Moritz",
        "Joseph Gonzalez",
        "Ion Stoica"
      ],
      "year": "2018",
      "venue": "Tune: A Research Platform for Distributed Model Selection and Training",
      "arxiv": "arXiv:1807.05118"
    }
  ]
}