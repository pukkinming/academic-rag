{
  "paper_id": "2308.04502v2",
  "title": "Revisiting Disentanglement And Fusion On Modality And Context In Conversational Multimodal Emotion Recognition",
  "published": "2023-08-08T18:11:27Z",
  "authors": [
    "Bobo Li",
    "Hao Fei",
    "Lizi Liao",
    "Yu Zhao",
    "Chong Teng",
    "Tat-Seng Chua",
    "Donghong Ji",
    "Fei Li"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "It has been a hot research topic to enable machines to understand human emotions in multimodal contexts under dialogue scenarios, which is tasked with multimodal emotion analysis in conversation (MM-ERC). MM-ERC has received consistent attention in recent years, where a diverse range of methods has been proposed for securing better task performance. Most existing works treat MM-ERC as a standard multimodal classification problem and perform multimodal feature disentanglement and fusion for maximizing feature utility. Yet after revisiting the characteristic of MM-ERC, we argue that both the feature multimodality and conversational contextualization should be properly modeled simultaneously during the feature disentanglement and fusion steps. In this work, we target further pushing the task performance by taking full consideration of the above insights. On the one hand, during feature disentanglement, based on the contrastive learning technique, we devise a Dual-level Disentanglement Mechanism (DDM) to decouple the features into both the modality space and utterance space. On the other hand, during the feature fusion stage, we propose a Contribution-aware Fusion Mechanism (CFM) and a Context Refusion Mechanism (CRM) for multimodal and context integration, * Fei Li is the corresponding author.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The analysis of conversational emotions  [47]  has received growing attention and has been applied in various downstream tasks, like empathetic response generation  [13, 24, 55]  and mental disease treatment  [48] . Recently, the research on conversational emotion analysis has extended the focus from text to multiple modalities such as video and audio  [16, 25, 47] . As illustrated in Figure  1 , There was hum... There was another reason that I thought it was time to end it with Joey. (Neutral) Figure  1 : An example of multimodal conversation from the MELD dataset  [47] . Each utterance comes with three modalities of content: video, audio, and text. The goal of MM-ERC is to recognize the emotion label of each utterance. multimodal emotion recognition in conversation (named MM-ERC) aims to detect the emotion label for each utterance in a given dialogue by jointly considering auditory, visual, and textual content. The introduction of audio and video compensates for the limitation of solely depending on text features and thus enriches the features used for emotion recognition.\n\nA good number of efforts have been devoted to building effective MM-ERC models and secured promising performance, where the core idea is to effectively disentangle different modalities and then properly fuse them so as to maximize the efficacy of multimodal features for the task  [9, 21, 23, 25, 60] . However, MM-ERC intrinsically involves two simultaneous key ingredients: multiple feature modality and conversational contextualization. While the majority of existing models treat MM-ERC as a typical multimodal classification problem, focusing predominantly on either multimodality or context modeling, the relationship between dialogue context and multimodal feature consistency is often neglected. By revisiting the task of MM-ERC, we note that a sound and effective MM-ERC system should place proper attention to simultaneously modeling the multimodality and contextualization during the feature disentanglement step and fusion step.\n\nFeature Disentanglement. The purpose of feature disentanglement is to extract the critical features from the original feature spaces and weaken the influence of irrelevant features, since multimodal inputs often contain features unrelated to emotion recognition (e.g., background video and noisy audio). While existing models, such as MISA  [20]  and FDMER  [57] , propose sophisticated disentanglement mechanisms for single pieces of utterance, disentangling on the conversational contexts has not been considered. On the one hand, different modality features within one utterance should exhibit similarities because, intuitively, multimodal signals under the same utterance can be semantically consistent in representing an identical emotion. On the other hand, features from different utterances with the same modality share similarities in modality-specific characteristics (e.g., timbre, facial expression, and strong wording), which may seem trivial for other modalities but are useful for recognizing emotions in the specific modality space. Feature disentanglement without effectively considering both the modality level and utterance level will inevitably limit further performance improvement of MM-ERC. Unfortunately, to the best of our knowledge, no existing research explores the disentanglement under these two aspects, indicating a potential research gap.\n\nFeature Fusion. The disentangled features from the above step further need to be properly fused, during which reasonable weights are assigned to maximize the utility of features for emotion prediction. Since different clues in varied modalities serve distinct contributions to the final prediction, fusing features across modalities has been extensively considered in existing MM-ERC studies  [9, 10, 21, 25] , with many sophisticated methods, such as tensor fusion  [60] , graph convolutional networks  [25] , gating mechanisms  [21] . However, no controllable weights were utilized in previous works, which may risk one modality dominating the multimodal fusion process  [45]  and potentially limiting the overall performance. Yet we note that the utterance-level fusion should also receive sufficient attention. Intuitively, it is less necessary to further introduce moderate history utterance contexts for prediction when the multimodal signals within the current utterance have indicated a clear emotion tendency in high consistency. Instead, aggressively feeding all the historical contexts would rather deteriorate the inference. For example, in Figure  1 , fully considering all previous dialogue contexts might lead to an incorrect emotion determination for the last utterance as \"Sadness\". This could happen due to the negative neighbor context (i.e., the emotion of the second-last utterance being \"Sadness\") and the negative atmosphere conveyed throughout the dialogue. Therefore, properly fusing the features from both multimodal ones and dialogue contexts is non-trivial.\n\nIn light of the above observations, in this work, we develop a niche targeting solution, i.e., DF-ERC (Disentanglement & Fusion for Emotion Recognition in Conversation), to fill the gaps and help achieve higher performance in MM-ERC. As shown in Figure 2, our system comprises four tiers. First, the raw multimodal inputs of dialogues are encoded into various feature extractors to obtain corresponding features. Then, the feature disentanglement layer performs feature disentanglement, where a Dual-level Disentanglement Mechanism (DDM) is proposed. DDM employs contrastive learning  [14]  to push the feature vectors of different modalities or different utterances away, thereby disentangling features at the modality level and utterance level, respectively. Next, the feature fusion layer performs modality-level and context-level integration, in which we propose a Contribution-aware Fusion Mechanism (CFM) and a Context Refusion Mechanism (CRM) for multimodal and context fusion, respectively. CFM fuses multimodal features based on the true classification probabilities  [11]  of each modality as their contributions, where such dynamic weighting advances in more controllable feature coordination. In contrast, CRM flexibly schedules the introduction of historical dialogue contexts into the current utterance via a novel emotion-prototype learning strategy. Specifically, CRM calculates the consistency degree of all modality features within an utterance, where a lower  consistency degree triggers the model to bring in more contexts for reassurance. Finally, the fused overall multimodal and contextual features are used for the emotion label prediction.\n\nTo evaluate the efficacy of our proposed approach, we performed extensive experiments on two widely-used benchmarks, namely MELD  [47]  and IEMOCAP  [6] . DF-ERC achieves state-of-the-art performance on overall results and most of the fine-grained emotion categories, demonstrating its effectiveness and stability. Furthermore, we find that DDM was able to effectively disentangle the features of different modalities or utterances (see Figure  10 ), and the disentangled features played a crucial role in the process of feature fusion (see Figure  6  and Figure  7 ). Additionally, both CFM and CRM play vital roles, as demonstrated by ablation studies (see Table  2 ) and in-depth analysis (see Section 4.4). Especially noteworthy is that CRM outperforms the models with no context or full context engagement, demonstrating its superiority (see Figure  8 ).\n\nOverall, our contributions are four-fold:\n\n‚Ä¢ We revisit the MM-ERC task and, for the first time, propose DF-ERC to enhance the task by performing disentanglement and fusion under both the modality and context perspectives.\n\n‚Ä¢ Technically, we propose three novel and effective mechanisms to disentangle and fuse both multimodal and contextual features. ‚Ä¢ Empirically, our system achieves state-of-the-art performance on two benchmarks. ‚Ä¢ Our proposed methods have great potential for facilitating a broader range of conversational multimodal applications.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal sentiment analysis  [12, 44, 56]  aims to extract sentiments or emotions using multiple modality resources, such as text (transcripts), acoustic (audio) and visual modalities. However, discrepancies across different modalities pose a challenge to the model. To address this issue, some studies have focused on modality alignment  [52]  and minimizing the discrepancies between modalities  [39, 59] . Moreover, the style of modality fusion can impact the model performance, leading to the exploration of effective fusion methods, such as hierarchical mutual information  [18, 41] , reconstruct loss  [20] , and graph neural network  [2, 27, 58] . Additionally, leveraging contextual information to predict dynamic emotions is also a popular approach  [1, 7, 8, 15] . However, the use of controllable weights to fuse multimodal features has not been considered in any of the existing approaches, which can limit their performance in practice and is one of the main focuses of our study. Emotion Recognition in Conversation (ERC)  [32, 40, 46]  is a subfield of affective computing that aims to recognize emotions for each utterance within a conversation. To develop the model, some studies focus on leveraging dialogue-related features, such as speaker-oriented dialogue modeling  [17, 22, 42] , context-aware modeling  [50, 63] , hierarchical feature modeling  [31, 34, 35] , and emotion transition  [4, 51] . With the development of multimodal technology  [3, 29, 62] , the research scope of ERC has been extended to multimodal scenarios. Many studies have explored multimodal fusion methods for the MM-ERC task, such as multimodal dynamic fusion  [21, 36] , hierarchical fusion  [10] , and adaptive modality drop  [9] . Although existing adaptive methods have been proposed, they neglect some crucial aspects, such as the contribution of each utterance and the relationship between modality consistency and the involvement of context, which are the main focuses of our paper.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Framework",
      "text": "Given a dialogue ùê∑ = {ùë¢ 0 , ùë¢ 1 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , ùë¢ ùëõ }, where ùë¢ ùëñ represents an utterance, the MM-ERC task aims to recognize the emotion type ùëí ùëñ corresponding to each utterance ùë¢ ùëñ . In each ùë¢ ùëñ , there are three kinds of data, namely text, audio, and video, which are used to predict ùëí ùëñ . ùëí ùëñ belongs to a pre-defined set of emotion labels, such as angry, sadness, joy, etc. To approach the task, we introduce a novel framework, termed DF-ERC, illustrated in Figure  2 , which performs four tiers of propagation for emotion prediction. Subsequently, we elaborate on the specific techniques employed at each step.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Feature Encoding",
      "text": "Given a dialogue ùê∑, we first perform feature extraction for each utterance ùë¢ ùëñ simultaneously. In this paper, we follow up-to-date previous works  [10, 51]  and employ RoBerta  [37]  to obtain contextualized text features. Specifically, all the utterances are concatenated and fed into a pre-trained language model (PLM) following the way in Span-BERT  [30] . The dialogue is represented as Utterance-level Modality-level where ùë§ ùëñ ùëó is the ùëó-th token in ùë¢ ùëñ and ùëô ùëñ is the length of ùë¢ ùëñ , ùë†ùë°ùëéùëüùë° ùëñ and ùëíùëõùëë ùëñ are the indices of the head and tail tokens of ùë¢ ùëñ in the sequence D, and ùíï ùë† ùëñ is the text feature for utterance ùë¢ ùëñ . For audio and visual content, following the approach described in previous work  [21, 25] , we adopt OpenSmile  [49]  and DenseNet  [26]  pre-trained on the Facial Expression Recognition Plus (FER+) corpus  [5]  as feature extractors. Finally, we obtain an audio feature ùíÇ ùë† ùëñ and a visual feature ùíó ùë† ùëñ for each utterance ùë¢ ùëñ .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dual-Level Disentanglement Mechanism (Ddm)",
      "text": "It should be noted that directly utilizing raw multimodal features for emotion analysis is problematic because they are entangled and noisy due to the unconstrained extraction process. Thus, it is necessary to disentangle multimodal features in order to refine them and boost the performance of downstream tasks. In this paper, we propose a dual-level disentanglement mechanism to disentangle raw features in both utterance and modality levels, as illustrated in Figure  3 . At the modality level, we apply an MLP layer to the features of different modalities to derive modality-level representations:\n\nNext, a list ùëπ ùëö , containing the items\n\nWe then apply contrastive learning to these features in order to draw features of the same modality closer to each other and push features of different modalities away, formalized as below:\n\nwhere ùíâ ùëñ is the ùëñ-th item in ùëπ ùëö . Note that ùíâ ùëò ‚àà ùëπ ùëö ùëñ+ = {ùíô ùëó | ùëó ‚â° ùëñ (mod 3), 1 < ùëó ‚â§ 3ùëõ} has the same modality as ùíâ ùëñ , which can be considered as positive instances. Here ùúè is a temperature parameter, and ùë†ùëñùëö(ùíâ ùëñ , ùíâ ùëò ) denotes the cosine similarity between two vectors.   At the utterance level, contrastive learning is exploited in a similar manner, clustering the multimodal features of the same utterance and disentangling the features of different utterances, formulated as below:\n\nwhere\n\nand ùíâ ùëò ‚àà ùëπ ùë¢ ùëñ+ = {ùíô ùëó |‚åäùëñ/3‚åã = ‚åä ùëó/3‚åã, 0 < ùëó ‚â§ 3ùëõ} denotes the feature in the same utterance with ùíâ ùëñ . ‚åäùë•‚åã is the round down symbol. Finally, we use residual connections to concatenate raw features with disentangle features, and two loss functions for contrastive learning are also combined:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Contribution-Aware Fusion Mechanism (Cfm)",
      "text": "As different modalities have different importance for the final emotion label prediction, they should be assigned different fusion weights in the modality fusion process. Here we adopt a contributeaware adaptive fusion module to assign the weight of each modality, which can give a dynamic weight according to their prediction performance, as illustrated in Figure  4 . Specifically, we apply a classifier on the representation of each modality and obtain the true classification probability (TCP)  [11, 19]  as their contribution in the fusion process, which can be obtained via:\n\nwhere ùíõ ùëö ùëñ is the prediction probability, and ùêº * ùëñ is the index of golden emotion label for ùë¢ ùëñ . Obviously, TCP m i ‚àà (0, 1) denotes how likely the prediction result is right. A larger TCP value indicates the feature representation ùíï ùëñ /ùíÇ ùëñ /ùíó ùëñ can yield a correct prediction result and verse visa. Therefore, we plan to adopt TCP to represent the fusion weight for each modality. However, a significant challenge arises during the evaluation phase as the true emotion label is unknown, making it impossible to directly utilize TCP as the weight.\n\nTo keep the consistency of training and test processes, we adopt a predicted value, which is trained to be close to TCP, as the weight of each modality:\n\nTo achieve the goal that we mentioned before, the following loss functions are used:\n\nwhere, L ùëö ùëù represents the prediction label loss and L ùëö ùëû is the prediction TCP loss. Finally, the fused multimodal features can be formulated as:",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Context Refusion Mechanism (Crm)",
      "text": "Except for fusing multimodal features, contextual feature fusion is also important, especially when multimodal features contradict each other regarding emotion prediction. Therefore, we compute the agreements among multimodal features as the weights to determine how many contextual features should be incorporated. However, since multimodal features are not aligned according to emotions, it may be inaccurate to directly compute the similarity based on multimodal features. To solve this problem, we propose a prototype-based alignment module (as shown in Figure  5 ) to learn the emotion-specific representations of multimodal features. Specifically, in each training epoch, we maintain a prototype vector for each kind of emotion:\n\nwhere ùëÖ ùëò ùë° represents the prototype of the ùëò-th emotion in the ùë°-th epoch,\n\nis the size of the ùëò-th emotion in the ùëü -th round. The prototype vector is updated in each iteration based on the previous values and the multimodal features in the current epoch. To ensure that each feature is close to its prototype vector, we use a margin-based loss function based on the mean squared error (MSE):\n\nwhere L ùë†ùëñùëö denotes the loss function and ùõΩ represents the margin.\n\nIf the MSE between a feature vector and its corresponding prototype vector is less than the margin, the model will not be updated.\n\nIn other words, the feature vector is expected to be close to the prototype vector but not necessarily identical to it, in order to avoid all feature vectors becoming indistinguishable. Once the multimodal feature vectors are aligned, the comprehensive similarity between different modalities in the utterance ùë¢ ùëñ can be computed as:\n\nIn the context fusion stage, we first utilize a bidirectional long short-term memory (Bi-LSTM) to generate contextual representations of the utterances as follows:\n\nWe then concatenate fused features with context-aware features as follows  1  :\n\nwhere ùíâ ùëí ùëñ represents the final representation of each utterance that is fused with multimodal and contextual information.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Prediction And Learning",
      "text": "Then, the fused representation ùíâ ùëí ùëñ is used for emotion recognition, which is performed as follows:\n\nwhere ùíö ùëñ represents the probability of predicted emotion. We use the cross-entropy loss function for training, which is defined as follows:\n\nwhere I * ùëñ denotes the golden label index of the utterance ùë¢ ùëñ . During the learning stage, our training loss functions consist of the following parts:\n\nwhere ùõº 1-3 ‚àà (0, 1] are hyper-parameters.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments75 4.1 Implementation Details",
      "text": "Datasets. We conducted experiments using two publicly available MM-ERC datasets, namely MELD  [47]  and IEMOCAP  [6] . Both of which include text, audio, and video modalities. The data split used in our experiments follows previous work  [21, 25] , and the detailed corpus statistics are presented in Section A.1 of the Appendix.\n\nTable  1 : Comparisons with the baselines. 'W-F1' refers to weighted F1 scores. The results with ::::::: waveline denote the best baseline results. The results with ‚òÖ denote significance at ùëù < 0.01 compared with the best baseline results. The '-' symbol denotes that the corresponding item is not reported in the original paper. Furthermore, the term 'KG' indicates the model is augmented with a knowledge graph.\n\nthe best-performing baseline, respectively. Similarly, for the IEMO-CAP dataset, DF-ERC improves Acc and W-F1 scores by 4.68 and 4.54 points, respectively. These findings underscore the efficacy of incorporating multimodal information and the efficient integration of multimodal features.\n\nInterestingly, without utilizing any external knowledge, DF-ERC still surpasses models that rely on external knowledge, such as TODKAT, CoMpM, COMSMIC, and SKSEC. The table demonstrates that DF-ERC improves the W-F1 scores by 0.51 and 2.29 points for the MELD and IEMOCAP datasets, respectively. These findings suggest that multimodal information effectively compensates for the absence of external knowledge, thus enhancing emotion recognition performance.\n\nAdditionally, DF-ERC achieves the best performance among all multimodal-based models. On the MELD dataset, DF-ERC improves Acc and W-F1 scores by 0.43 and 0.32 points, respectively. On the IEMOCAP dataset, the improvements are 1.28 and 1.09 points. These results indicate that DF-ERC is adept at discerning the differences and weighted contributions of each type of multimodal feature. Consequently, DF-ERC optimally utilizes multimodal features to bolster multimodal emotion recognition in conversation.\n\nLastly, we present the performance scores for each type of emotion, revealing that DF-ERC achieves the best performance for most emotions, thereby demonstrating the robustness of our model. It also contributes to the superior overall performance of our model.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies",
      "text": "Ablation studies for DDM, CFM, and CRM. As shown in Table 2, we observe that upon the removal of the DDM, utterance-level Table  2 : Ablation studies for DDM, CFM, and CRM, where '+Att' denotes the application of a self-attention mechanism for feature fusion, where 'full' and 'zero' means the weight of contextual features (Eq. (  22 )), i.e., using full contextual features or none of them. The notions 'Utterance' and 'Modality' correspond to the removal of utterance-level and modalitylevel contrastive learning, respectively.  We attribute this to the fact that different modalities make varying contributions, and the use of a contribution-aware approach allows for the adaptive learning of weights for different modality features, resulting in a better fusion of multimodal features. Last but not least, CRM also provides assistance in emotion recognition. After removing the CRM module, we directly set the context weight to 1 (full weight) or 0 (zero weight), signifying the introduction of all or no context information. We observe that these static weights impair the model's performance, resulting in a drop of more than 2 points on both the MELD and IEMOCAP datasets. This finding suggests that context representations are not inherently useful or useless. We ultimately achieve better performance using the CRM to determine the weights of the context.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Model",
      "text": "Ablation studies for modalities. As demonstrated in Table  3 , our initial findings reveal that the text-based model outperforms other modality-based models, providing evidence of the dominance of text as a modality, which is consistent with previous research findings  [20] . However, compared to state-of-the-art text-based models in Table  1 , our text-based model exhibits slightly lower performance. This is because DF-ERC primarily focuses on multimodal inputs and lacks complex structures specifically tailored for unimodal inputs, which slightly limits its performance. Nonetheless, incorporating audio and video features into the model with our efficient fusion techniques (i.e., CFM and CRM) leads to a significant improvement in performance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "In-Depth Analysis",
      "text": "To further investigate the effectiveness of DF-ERC, we conduct in-depth analyses to answer the following questions:\n\nQ1: How does utterance-level feature disentanglement influence the feature fusion process? We analyzed the influence of utterance-level disentanglement on context weight and final performance. As shown in Figure  6 , we divided all instances into five groups based on the similarities between modalities, sorted in ascending order. From the similarity curve, we found that the use of utterance-level disentanglement can significantly improve the similarities between modalities within an utterance, demonstrating that it effectively captures utterance-level information. Furthermore, we observed that utterance-level disentanglement is more effective in the case of utterances with a lower similarity between modalities (demonstrated by the larger F1 score gap). This is because low similarity often indicates that the features of the utterance are not fully exploited, and adding utterance-level disentanglement brings the utterance-level distance closer, thus improving similarity to a greater extent. Finally, considering the F1 metric, utterance-level disentanglement contributes more to the performance of utterances with low similarities, indicating that it can improve the final performance by leveraging utterance-level similarity.\n\nQ2: How does modality-level feature disentanglement influence the feature fusion process? We analyzed the effect of modality-level feature disentanglement on the weight of each modality and the final F1 score. From the curves depicted in Figure  7 , it is evident that integrating modality disentanglement resulted in increased weights for the video and audio modalities, especially in utterances with higher cross-entropy values. This can be attributed to the fact that modality disentanglement enables each modality's unique characteristics to be fully exploited, resulting in a subtle enhancement of weaker modalities, such as video and audio. Moreover, this increase of weight for weaker modalities is more evident in utterances with suboptimal prediction results, where the text modality does not perform well. We observed the most substantial improvement in the F1 score for utterances with poorer prediction results, approximately 5 points improvement for those with cross-entropy ranking percentile > 0.75. This suggests that the integration of modality-level feature disentanglement can lead to more accurate predictions in challenging situations.\n\nQ3: Do the CRM context weights decided by modality consistency really work? To verify the effectiveness of the modality similarity comparison module, we study the relationship between prediction performance and modality similarity. Additionally, we include the performance under full weight (incorporating all context representations) and zero weight (excluding all context representations). Figure  8  displays these results, with the x-axis representing the instance's similarity score ranking among all instances, while larger x values denote higher similarity. Firstly, we find that as modality similarity improves, the overall performance of the model also increases gradually. This is because the more similar the three modalities are, the more consistency they exhibit in emotion recognition, resulting in higher prediction scores. Secondly, when comparing full weight to zero weight, we observe that for utterances with relatively small similarities, the performance of full weight is superior to that of zero weight. This is because when the discrepancy between different modalities within an utterance is substantial, introducing context utterance representations can help to better recognize emotions. Conversely, as similarity increases and the discrepancy between different modalities diminishes, context representations interfere with emotional judgments. Thus, for the latter    8 : Correlation between performance and modality consistency, where a larger value of the x-axis represents a higher modality consistency. We use similarity to represent consistency, where a higher similarity of different modalities indicates a higher consistency. CRM: Our proposed Context Refusion Mechanism dynamically determines how much contextual features are used based on modality consistency; Zero: using no contextual features; Full: using 100% contextual features.\n\nhalf of the figures, the performance score of zero weight outperforms that of full weight. Finally, regardless of whether full weight or zero weight is used, both approaches have limitations in that they can not be flexibly adjusted as the consistency of modalities. Our CRM can adjust context weight according to the consistency between modalities, achieving the best performance in all instances and verifying that DF-ERC effectively captures the relationship between multimodal features and context features.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduce a novel MM-ERC system that emphasizes both feature disentanglement and fusion while taking into account both multimodalities and conversational contexts. Our proposed Dual-level Disentanglement Mechanism (DDM) successfully disentangles modality-and utterance-level features using contrastive learning, while the Contribution-aware Fusion Mechanism (CFM) and Context Refusion Mechanism (CRM) fuse multimodal and contextual features effectively. Extensive experiments on two public datasets demonstrate that DF-ERC achieves the best performance compared with 19 models. Ablation studies and in-depth analyses substantiate the rationality of our approaches for controllable fusing multimodal and context features. Intuitively, our proposed approaches are not limited to emotion recognition in dialogs, and we will evaluate them on other multimodal tasks in the future.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A Dataset & Experiment Details A.1 Dataset",
      "text": "We provide detailed descriptions of the two datasets used in this study: MELD and IEMOCAP.\n\nMELD. MELD is a multi-party dialogue dataset consisting of conversation snippets from the TV show Friends. The dataset includes 1,433 dialogues and 13,708 utterances, with an average of 9.6 turns per dialogue, and features 378 unique speakers. Each utterance in the dataset is labeled with one of seven emotions, namely joy, sadness, neutral, surprise, anger, fear, and disgust, based on the emotion conveyed by the speaker. The detailed statistics for the MELD dataset are provided in Table  4 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.2 Settings",
      "text": "We employ RoBERTa-Large to encode text content without any additional pre-processing operations. We utilize the AdamW  [38]  optimizer and LR scheduler with a warm-up mechanism for parameter optimization. The learning rates for the PLM layer and non-PLM layer are set to 1e-5 and 1e-3, respectively. Utterances exceeding 256 tokens are clipped to meet the model's input length requirement and reduce memory usage. Furthermore, we apply a dropout layer with a rate of 0.2 after the encoder to further enhance the performance of our model. We set the batch size to 8 and 4 for the MELD and IEMOCAP datasets, respectively. We set the temperature in DDM to 0.5 for Eq. (3) and 0.3 for Eq. (  5 ). All experiments are conducted on Ubuntu systems with two RTX A5000 GPUs. Additional parameters are shown in Table  6 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B Additional Experiment",
      "text": "In this section, we present more experiment results to investigate the performance of DF-ERC. Hyper-parameter Analysis In our study, we introduce certain hyperparameters and adopt a grid search strategy for their optimization. An example of these parameters is ùõº 3 , as referenced in Eq.  (25) . To assess the effect of the parameter and evaluate the robustness of DF-ERC, we document the variation in the F1 score as ùõº 3 is adjusted within a particular range. As depicted in Figure  9 , the F1 score fluctuates slightly with changes in ùõº 3 , peaking when ùõº 3 = 0.3. Performance experiences a minor decline when ùõº 3 deviates from this optimal value, illustrating the robustness of DF-ERC around ùõº 3 = 0.3.  Visualization for feature disentanglement. To visualize the effectiveness of DDM for feature disentanglement, we analyze the distribution of the three modalities after modality-level disentanglement (see Eq. (  2 )) and utterance-level disentanglement (see Eq. (  4 )) using t-SNE  [54] , as shown in Figure  10 . The result  indicates that modality-level contrastive learning effectively disentangles the three modalities from each other. Furthermore, we also observe that utterance-level disentanglement can align features within an utterance by entangling features from three distinct modality spaces. These findings highlight the effectiveness of our DDM in controlling the modality distribution in feature space based on the corresponding optimization objective and thus can further improve the emotion recognition performance.\n\nThe contribution of CFM for final performance. To verify the effect of CFM, we investigated the relationship between the prediction effect (evaluated using MSE) of TCP and the final emotion recognition performance. Figure  11  shows that for the majority of utterances, the MSE of the TCP prediction is less than 0.1, indicating satisfactory performance for TCP. However, it should be noted that a better TCP prediction for each modality does not necessarily result in a higher emotion prediction score, as shown by the wave pattern in the bottom half of the figure, which suggests that TCP has some limitations as a modality contribution evaluator. Nonetheless, after averaging the prediction MSE of the three modalities within an utterance, we observed a gradual increase in performance with a smoother tendency as the MSE decreased. This demonstrates that, overall, a predicted contribution weight can guide the model to assign the optimal weight for each modality and thus achieve better final performance.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of multimodal conversation from the",
      "page": 2
    },
    {
      "caption": "Figure 1: , fully considering all",
      "page": 2
    },
    {
      "caption": "Figure 2: Overall framework of the proposed DF-ERC model. DDM: Dual-level disentanglement Mechanism; CFM: Contribution-",
      "page": 3
    },
    {
      "caption": "Figure 10: ), and the",
      "page": 3
    },
    {
      "caption": "Figure 6: and Figure 7). Additionally, both CFM and CRM",
      "page": 3
    },
    {
      "caption": "Figure 2: , which performs",
      "page": 3
    },
    {
      "caption": "Figure 3: Dual-level disentanglement mechanism, where ùíï‚àó",
      "page": 4
    },
    {
      "caption": "Figure 4: Contribution-aware fusion mechanism. ùíïùëñ, ùíÇùëñand",
      "page": 4
    },
    {
      "caption": "Figure 4: Specifically, we apply a classifier",
      "page": 4
    },
    {
      "caption": "Figure 5: An illustration of the process of prototype-based",
      "page": 5
    },
    {
      "caption": "Figure 5: ) to learn",
      "page": 5
    },
    {
      "caption": "Figure 6: Influence of utterance-level disentanglement (u-",
      "page": 7
    },
    {
      "caption": "Figure 6: , we divided all instances into",
      "page": 7
    },
    {
      "caption": "Figure 7: The influence of modality-level disentanglement",
      "page": 8
    },
    {
      "caption": "Figure 8: displays these results, with the x-axis representing",
      "page": 8
    },
    {
      "caption": "Figure 8: Correlation between performance and modality",
      "page": 8
    },
    {
      "caption": "Figure 9: Trend in F1 score with respect to the changes in ùõº3",
      "page": 11
    },
    {
      "caption": "Figure 10: The result",
      "page": 11
    },
    {
      "caption": "Figure 10: T-SNE [54] visualization of multimodal features after applying DDM in modality and utterance levels.",
      "page": 12
    },
    {
      "caption": "Figure 11: Virtualization of the correlation between predic-",
      "page": 12
    },
    {
      "caption": "Figure 11: shows that for the majority of",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Neutral": "Angry",
          "Joy": "Sadness"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Neutral": "Angry",
          "Joy": "Sadness"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis",
      "authors": [
        "Shad Md",
        "Dushyant Akhtar",
        "Deepanway Chauhan",
        "Ghosal"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT",
      "doi": "10.18653/v1/N19-1034"
    },
    {
      "citation_id": "2",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "authors": [
        "Amirali Bagher Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of ACL",
      "doi": "10.18653/v1/P18-1208"
    },
    {
      "citation_id": "3",
      "title": "Multimodal Machine Learning: A Survey and Taxonomy",
      "authors": [
        "Tadas Baltrusaitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2019",
      "venue": "IEEE TPAMI",
      "doi": "10.1109/TPAMI.2018.2798607"
    },
    {
      "citation_id": "4",
      "title": "Shapes of Emotions: Multimodal Emotion Recognition in Conversations via Emotion Shifts",
      "authors": [
        "Keshav Bansal",
        "Harsh Agarwal",
        "Abhinav Joshi",
        "Ashutosh Modi"
      ],
      "year": "2022",
      "venue": "Workshop on COLING. International Conference on Computational Linguistics, Virtual"
    },
    {
      "citation_id": "5",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton-Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of ICMI",
      "doi": "10.1145/2993148.2993165"
    },
    {
      "citation_id": "6",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "7",
      "title": "Context-aware Interactive Attention for Multi-modal Sentiment and Emotion Analysis",
      "authors": [
        "Dushyant Singh"
      ],
      "year": "2019",
      "venue": "Proceedings of EMNLP",
      "doi": "10.18653/v1/D19-1566"
    },
    {
      "citation_id": "8",
      "title": "Complementary Fusion of Multi-Features and Multi-Modalities in Sentiment Analysis",
      "authors": [
        "Feiyang Chen",
        "Ziqian Luo",
        "Yanyan Xu",
        "Dengfeng Ke"
      ],
      "year": "2020",
      "venue": "Proceedings of Workshop Affective Content Analysis with AAAI (CEUR Workshop Proceedings"
    },
    {
      "citation_id": "9",
      "title": "Learning What and When to Drop: Adaptive Multimodal and Contextual Dynamics for Emotion Recognition in Conversation",
      "authors": [
        "Feiyu Chen",
        "Zhengxiao Sun",
        "Deqiang Ouyang",
        "Xueliang Liu",
        "Jie Shao"
      ],
      "year": "2021",
      "venue": "Proceedings of ACM MM. ACM, Virtual Event",
      "doi": "10.1145/3474085.3475661"
    },
    {
      "citation_id": "10",
      "title": "Pankaj Wasnik, and Naoyuki Onoe. 2022. M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation",
      "authors": [
        "Purbayan Vishal Chudasama",
        "Ashish Kar",
        "Nirmesh Gudmalwar",
        "Shah"
      ],
      "venue": "IEEE/CVF CVPR Workshops",
      "doi": "10.1109/CVPRW56347.2022.00511"
    },
    {
      "citation_id": "11",
      "title": "Addressing Failure Prediction by Learning Model Confidence",
      "authors": [
        "Charles Corbi√®re",
        "Nicolas Thome",
        "Avner Bar-Hen",
        "Matthieu Cord",
        "Patrick P√©rez"
      ],
      "year": "2019",
      "venue": "Proceedings of NeurIPS. IEEE"
    },
    {
      "citation_id": "12",
      "title": "Multimodal Sentiment Analysis: A Survey of Methods, Trends and Challenges",
      "authors": [
        "Ringki Das",
        "Thoudam Doren Singh"
      ],
      "year": "2023",
      "venue": "ACM Comput. Surv",
      "doi": "10.1145/3586075"
    },
    {
      "citation_id": "13",
      "title": "Improving Empathetic Response Generation by Recognizing Emotion Cause in Conversations",
      "authors": [
        "Jun Gao",
        "Yuhan Liu",
        "Haolin Deng",
        "Wei Wang",
        "Yu Cao",
        "Jiachen Du",
        "Ruifeng Xu"
      ],
      "year": "2021",
      "venue": "Findings of EMNLP",
      "doi": "10.18653/v1/2021.findings-emnlp.70"
    },
    {
      "citation_id": "14",
      "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
      "authors": [
        "Tianyu Gao",
        "Xingcheng Yao",
        "Danqi Chen"
      ],
      "year": "2021",
      "venue": "Proceedings of EMNLP",
      "doi": "10.18653/v1/2021.emnlp-main.552"
    },
    {
      "citation_id": "15",
      "title": "Contextual Inter-modal Attention for Multi-modal Sentiment Analysis",
      "authors": [
        "Deepanway Ghosal",
        "Shad Md",
        "Dushyant Akhtar",
        "Chauhan"
      ],
      "year": "2018",
      "venue": "Proceedings of EMNLP",
      "doi": "10.18653/v1/D18-1382"
    },
    {
      "citation_id": "16",
      "title": "COSMIC: COmmonSense knowledge for eMotion Identification in Conversations",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Alexander Gelbukh"
      ],
      "year": "2020",
      "venue": "Findings of EMNLP. Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.findings-emnlp.224"
    },
    {
      "citation_id": "17",
      "title": "DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation",
      "authors": [
        "Deepanway Ghosal",
        "Navonil Majumder",
        "Soujanya Poria",
        "Niyati Chhaya",
        "Alexander Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of EMNLP-IJCNLP",
      "doi": "10.18653/v1/D19-1015"
    },
    {
      "citation_id": "18",
      "title": "Improving Multimodal Fusion with Hierarchical Mutual Information Maximization for Multimodal Sentiment Analysis",
      "authors": [
        "Wei Han",
        "Hui Chen",
        "Soujanya Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of EMNLP",
      "doi": "10.18653/v1/2021.emnlp-main.723"
    },
    {
      "citation_id": "19",
      "title": "Multimodal Dynamics: Dynamical Fusion for Trustworthy Multimodal Classification",
      "authors": [
        "Zongbo Han",
        "Fan Yang",
        "Junzhou Huang",
        "Changqing Zhang",
        "Jianhua Yao"
      ],
      "year": "2022",
      "venue": "IEEE/CVF CVPR",
      "doi": "10.1109/CVPR52688.2022.02005"
    },
    {
      "citation_id": "20",
      "title": "MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of ACM MM",
      "doi": "10.1145/3394171.3413678"
    },
    {
      "citation_id": "21",
      "title": "MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Xiaolong Hou",
        "Lingwei Wei",
        "Lian-Xin Jiang",
        "Yang Mo"
      ],
      "year": "2022",
      "venue": "IEEE ICASSP. IEEE, Virtual and Singapore",
      "doi": "10.1109/ICASSP43922.2022.9747397"
    },
    {
      "citation_id": "22",
      "title": "DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations",
      "authors": [
        "Dou Hu",
        "Lingwei Wei",
        "Xiaoyong Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of ACL",
      "doi": "10.18653/v1/2021.acl-long.547"
    },
    {
      "citation_id": "23",
      "title": "UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition",
      "authors": [
        "Guimin Hu",
        "Ting-En Lin",
        "Yi Zhao",
        "Guangming Lu",
        "Yuchuan Wu",
        "Yongbin Li"
      ],
      "year": "2022",
      "venue": "Proceedings of EMNLP"
    },
    {
      "citation_id": "24",
      "title": "The Acoustically Emotion-Aware Conversational Agent With Speech Emotion Recognition and Empathetic Responses",
      "authors": [
        "Jiaxiong Hu",
        "Yun Huang",
        "Xiaozhu Hu",
        "Yingqing Xu"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/TAFFC.2022.3205919"
    },
    {
      "citation_id": "25",
      "title": "MMGCN: Multimodal Fusion via Deep Graph Convolution Network for Emotion Recognition in Conversation",
      "authors": [
        "Jingwen Hu",
        "Yuchen Liu",
        "Jinming Zhao",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of ACL",
      "doi": "10.18653/v1/2021.acl-long.440"
    },
    {
      "citation_id": "26",
      "title": "Densely Connected Convolutional Networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "IEEE Conference on CVPR",
      "doi": "10.1109/CVPR.2017.243"
    },
    {
      "citation_id": "27",
      "title": "Temporal Graph Convolutional Network for Multimodal Sentiment Analysis",
      "authors": [
        "Jian Huang",
        "Zehang Lin",
        "Zhenguo Yang",
        "Wenyin Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of ICMI. ACM",
      "doi": "10.1145/3462244.3479939"
    },
    {
      "citation_id": "28",
      "title": "Relationaware Graph Attention Networks with Relational Position Encodings for Emotion Recognition in Conversations",
      "authors": [
        "Taichi Ishiwatari",
        "Yuki Yasuda",
        "Taro Miyazaki",
        "Jun Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of EMNLP. Association for Computational Linguistics",
      "doi": "10.18653/v1/2020.emnlp-main.597"
    },
    {
      "citation_id": "29",
      "title": "A Review on Methods and Applications in Multimodal Deep Learning",
      "authors": [
        "Summaira Jabeen",
        "Xi Li",
        "Muhammad Shoib Amin",
        "Omar Bourahla",
        "Songyuan Li",
        "Abdul Jabbar"
      ],
      "year": "2023",
      "venue": "ACM Trans. Multimedia Comput. Commun. Appl",
      "doi": "10.1145/3545572"
    },
    {
      "citation_id": "30",
      "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans",
      "authors": [
        "Mandar Joshi",
        "Danqi Chen",
        "Yinhan Liu",
        "Daniel Weld",
        "Luke Zettlemoyer",
        "Omer Levy"
      ],
      "year": "2020",
      "venue": "Trans. Assoc. Comput. Linguistics",
      "doi": "10.1162/tacl_a_00300"
    },
    {
      "citation_id": "31",
      "title": "Graph Based Network with Contextualized Representations of Turns in Dialogue",
      "authors": [
        "Bongseok Lee",
        "Yong Choi"
      ],
      "year": "2021",
      "venue": "Proceedings of EMNLP. Association for Computational Linguistics",
      "doi": "10.18653/v1/2021.emnlp-main.36"
    },
    {
      "citation_id": "32",
      "title": "Modeling mutual influence of interlocutor emotion states in dyadic spoken interactions",
      "authors": [
        "Chi-Chun Lee",
        "Carlos Busso",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "1983",
      "venue": "INTERSPEECH. ISCA"
    },
    {
      "citation_id": "33",
      "title": "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation",
      "authors": [
        "Joosung Lee",
        "Wooin Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of NAACL-HLT",
      "doi": "10.18653/v1/2022.naacl-main.416"
    },
    {
      "citation_id": "34",
      "title": "Hi-Trans: A Transformer-Based Context-and Speaker-Sensitive Model for Emotion Detection in Conversations",
      "authors": [
        "Jingye Li",
        "Donghong Ji",
        "Fei Li",
        "Meishan Zhang",
        "Yijiang Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of COLING. International Committee on Computational Linguistics",
      "doi": "10.18653/v1/2020.coling-main.370"
    },
    {
      "citation_id": "35",
      "title": "A Hierarchical Transformer with Speaker Modeling for Emotion Recognition in Conversation",
      "authors": [
        "Jiangnan Li",
        "Zheng Lin",
        "Peng Fu",
        "Qingyi Si",
        "Weiping Wang"
      ],
      "year": "2020",
      "venue": "A Hierarchical Transformer with Speaker Modeling for Emotion Recognition in Conversation",
      "arxiv": "arXiv:2012.14781"
    },
    {
      "citation_id": "36",
      "title": "CTNet: Conversational Transformer Network for Emotion Recognition",
      "authors": [
        "Zheng Lian",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2021",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process",
      "doi": "10.1109/TASLP.2021.3049898"
    },
    {
      "citation_id": "37",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "38",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "Proceedings of ICLR. OpenReview.net"
    },
    {
      "citation_id": "39",
      "title": "Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion",
      "authors": [
        "Sijie Mai",
        "Haifeng Hu",
        "Songlong Xing"
      ],
      "year": "2020",
      "venue": "Proceedings of AAAI"
    },
    {
      "citation_id": "40",
      "title": "Using Linguistic Cues for the Automatic Recognition of Personality in Conversation and Text",
      "authors": [
        "Fran√ßois Mairesse",
        "Marilyn Walker",
        "Matthias Mehl",
        "Roger Moore"
      ],
      "year": "2007",
      "venue": "J. Artif. Intell. Res",
      "doi": "10.1613/jair.2349"
    },
    {
      "citation_id": "41",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "Navonil Majumder",
        "Devamanyu Hazarika",
        "Alexander Gelbukh"
      ],
      "year": "2018",
      "venue": "Knowl. Based Syst",
      "doi": "10.1016/j.knosys.2018.07.041"
    },
    {
      "citation_id": "42",
      "title": "DialogueRNN: An Attentive RNN for Emotion Detection in Conversations",
      "authors": [
        "Navonil Majumder",
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Rada Mihalcea",
        "Alexander Gelbukh",
        "Erik Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of AAAI",
      "doi": "10.1609/aaai.v33i01.33016818"
    },
    {
      "citation_id": "43",
      "title": "Dia-logueTRM: Exploring Multi-Modal Emotional Dynamics in a Conversation",
      "authors": [
        "Yuzhao Mao",
        "Guang Liu",
        "Xiaojie Wang",
        "Weiguo Gao",
        "Xuan Li"
      ],
      "year": "2021",
      "venue": "Findings of EMNLP",
      "doi": "10.18653/v1/2021.findings-emnlp.229"
    },
    {
      "citation_id": "44",
      "title": "Towards Multimodal Sentiment Analysis: Harvesting Opinions from the Web",
      "authors": [
        "Louis-Philippe Morency",
        "Rada Mihalcea",
        "Payal Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of ICMI",
      "doi": "10.1145/2070481.2070509"
    },
    {
      "citation_id": "45",
      "title": "Balanced Multimodal Learning via On-the-fly Gradient Modulation",
      "authors": [
        "Xiaokang Peng",
        "Yake Wei",
        "Andong Deng",
        "Dong Wang",
        "Di Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of CVPR. IEEE",
      "doi": "10.1109/CVPR52688.2022.00806"
    },
    {
      "citation_id": "46",
      "title": "Deep Emotion Recognition in Textual Conversations: A Survey",
      "authors": [
        "Patr√≠cia Pereira",
        "Helena Moniz",
        "Jo√£o Paulo Carvalho"
      ],
      "year": "2022",
      "venue": "Deep Emotion Recognition in Textual Conversations: A Survey",
      "doi": "10.48550/arXiv.2211.09172",
      "arxiv": "arXiv:2211.09172"
    },
    {
      "citation_id": "47",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of ACL",
      "doi": "10.18653/v1/p19-1050"
    },
    {
      "citation_id": "48",
      "title": "Towards Motivational and Empathetic Response Generation in Online Mental Health Support",
      "authors": [
        "Tulika Saha",
        "Vaibhav Gakhreja",
        "Anindya Sundar Das",
        "Souhitya Chakraborty",
        "Sriparna Saha"
      ],
      "year": "2022",
      "venue": "Proceedings of ACM SIGIR",
      "doi": "10.1145/3477495.3531912"
    },
    {
      "citation_id": "49",
      "title": "Recognising realistic emotions and affect in speech: State of the art and lessons learnt from the first challenge",
      "authors": [
        "W Bj√∂rn",
        "Anton Schuller",
        "Stefan Batliner",
        "Dino Steidl",
        "Seppi"
      ],
      "year": "2011",
      "venue": "Speech Commun",
      "doi": "10.1016/j.specom.2011.01.011"
    },
    {
      "citation_id": "50",
      "title": "Cross-Corpus Acoustic Emotion Recognition: Variances and Strategies",
      "authors": [
        "W Bj√∂rn",
        "Bogdan Schuller",
        "Florian Vlasenko",
        "Martin Eyben",
        "Andr√© W√∂llmer",
        "Andreas Stuhlsatz",
        "Gerhard Wendemuth",
        "Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Affect. Comput",
      "doi": "10.1109/T-AFFC.2010.8"
    },
    {
      "citation_id": "51",
      "title": "Emotionflow: Capture the Dialogue Level Emotion Transitions",
      "authors": [
        "Xiaohui Song",
        "Liangjun Zang",
        "Rong Zhang",
        "Songlin Hu",
        "Longtao Huang"
      ],
      "year": "2022",
      "venue": "IEEE, ICASSP. IEEE, Virtual and Singapore",
      "doi": "10.1109/ICASSP43922.2022.9746464"
    },
    {
      "citation_id": "52",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Yao-Hung Hubert Tsai",
        "Shaojie Bai",
        "Paul Pu Liang",
        "J Kolter",
        "Louis-Philippe Morency",
        "Ruslan Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of ACL. Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1656"
    },
    {
      "citation_id": "53",
      "title": "Sentiment-Emotionand Context-guided Knowledge Selection Framework for Emotion Recognition in Conversations",
      "authors": [
        "Geng Tu",
        "Bin Liang",
        "Dazhi Jiang",
        "Ruifeng Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2022.3223517"
    },
    {
      "citation_id": "54",
      "title": "Visualizing Data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "55",
      "title": "A Large-Scale Dataset for Empathetic Response Generation",
      "authors": [
        "Anuradha Welivita",
        "Yubo Xie",
        "Pearl Pu"
      ],
      "year": "2021",
      "venue": "Proceedings of EMNLP. Association for Computational Linguistics, Online and Punta Cana",
      "doi": "10.18653/v1/2021.emnlp-main.96"
    },
    {
      "citation_id": "56",
      "title": "YouTube Movie Reviews: Sentiment Analysis in an Audio-Visual Context",
      "authors": [
        "Martin W√∂llmer",
        "Felix Weninger",
        "Tobias Knaup",
        "Bj√∂rn Schuller",
        "Congkai Sun",
        "Kenji Sagae",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "57",
      "title": "Disentangled Representation Learning for Multimodal Emotion Recognition",
      "authors": [
        "Dingkang Yang",
        "Shuai Huang",
        "Haopeng Kuang",
        "Yangtao Du",
        "Lihua Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of ACM MM",
      "doi": "10.1145/3503161.3547754"
    },
    {
      "citation_id": "58",
      "title": "Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks",
      "authors": [
        "Xiaocui Yang",
        "Shi Feng",
        "Yifei Zhang",
        "Daling Wang"
      ],
      "year": "2021",
      "venue": "Proceedings of ACL",
      "doi": "10.18653/v1/2021.acl-long.28"
    },
    {
      "citation_id": "59",
      "title": "Learning Modality-Specific Representations with Self-Supervised Multi-Task Learning for Multimodal Sentiment Analysis",
      "authors": [
        "Wenmeng Yu",
        "Hua Xu",
        "Ziqi Yuan",
        "Jiele Wu"
      ],
      "year": "2021",
      "venue": "Proceedings of AAAI. AAAI Press, Virtual Event"
    },
    {
      "citation_id": "60",
      "title": "Tensor Fusion Network for Multimodal Sentiment Analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of EMNLP",
      "doi": "10.18653/v1/d17-1115"
    },
    {
      "citation_id": "61",
      "title": "Memory Fusion Network for Multi-view Sequential Learning",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Navonil Mazumder",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of AAAI"
    },
    {
      "citation_id": "62",
      "title": "DeepQoE: A Multimodal Learning Framework for Video Quality of Experience (QoE) Prediction",
      "authors": [
        "Huaizheng Zhang",
        "Linsen Dong",
        "Guanyu Gao",
        "Han Hu",
        "Yonggang Wen",
        "Kyle Guan"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Multim",
      "doi": "10.1109/TMM.2020.2973828"
    },
    {
      "citation_id": "63",
      "title": "MuCDN: Mutual Conversational Detachment Network for Emotion Recognition in Multi-Party Conversations",
      "authors": [
        "Weixiang Zhao",
        "Yanyan Zhao",
        "Bing Qin"
      ],
      "year": "2022",
      "venue": "Proceedings of COLING. International Committee on Computational Linguistics"
    },
    {
      "citation_id": "64",
      "title": "Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection",
      "authors": [
        "Lixing Zhu",
        "Gabriele Pergola",
        "Lin Gui",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2021",
      "venue": "Proceedings of ACL",
      "doi": "10.18653/v1/2021.acl-long.125"
    }
  ]
}