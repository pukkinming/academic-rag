{
  "paper_id": "2410.00010v1",
  "title": "Phemonet: A Multimodal Network For Physiological Signals",
  "published": "2024-09-13T21:14:27Z",
  "authors": [
    "Eleonora Lopez",
    "Aurelio Uncini",
    "Danilo Comminiello"
  ],
  "keywords": [
    "Emotion recognition",
    "EEG",
    "Physiological signals",
    "Hypercomplex Networks",
    "Hypercomplex algebra"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is essential across numerous fields, including medical applications and brain-computer interface (BCI). Emotional responses include behavioral reactions, such as tone of voice and body movement, and changes in physiological signals, such as the electroencephalogram (EEG). The latter are involuntary, thus they provide a reliable input for identifying emotions, in contrast to the former which individuals can consciously control. These signals reveal true emotional states without intentional alteration, thus increasing the accuracy of emotion recognition models. However, multimodal deep learning methods from physiological signals have not been significantly investigated. In this paper, we introduce PHemoNet, a fully hypercomplex network for multimodal emotion recognition from physiological signals. In detail, the architecture comprises modality-specific encoders and a fusion module. Both encoders and fusion modules are defined in the hypercomplex domain through parameterized hypercomplex multiplications (PHMs) that can capture latent relations between the different dimensions of each modality and between the modalities themselves. The proposed method outperforms current state-of-theart models on the MAHNOB-HCI dataset in classifying valence and arousal using electroencephalograms (EEGs) and peripheral physiological signals.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Hypercomplex neural networks represent a sophisticated class of neural models that extend traditional real-valued networks into higher-dimensional spaces using hypercomplex algebras, such as complex numbers, quaternions, and octonions. These networks exploit the inherent mathematical properties of hypercomplex numbers to model complex relationships within data, providing a more powerful mechanism for capturing both local and global patterns  [1] . Parameterized Hypercomplex Neural Networks (PHNNs) further extend this capability by a parameterization through a hyperparameter n which enables them to simulate different hypercomplex domains such as complex and quaternion spaces  [2] ,  [3] . This adaptability results in significant parameter efficiency and improved data representation, making PHNNs particularly suitable for tasks involving multidimensional inputs. Overall, hypercomplex layers offer a promising approach to advancing deep learning models by leveraging the unique advantages of hypercomplex algebra.\n\nFor these reasons, hypercomplex architectures have been developed for medical applications  [4] , as medical data often present multiple modalities or views that can be effectively exploited through an ad hoc hypercomplex architecture thanks to the properties of hypercomplex algebra  [5] . Specifically, a recent work has introduced a multimodal network for emotion recognition from physiological signals, featuring a hypercomplex fusion module  [6] . This module enhances fused representations by leveraging the relations among different modalities using hypercomplex algebra. It begins to address challenges in the literature, such as extensive preprocessing requiring domain-specific knowledge  [7] , which prevents neural networks from learning important features from raw data, and the use of single modalities when emotional responses are intrinsically multimodal  [8] . However, this approach still faces issues, mainly due to overfitting. Additionally, the current architecture employs hypercomplex layers only at the fusion step, while the encoders remain in the real domain. Incorporating hypercomplex layers in the encoders could further exploit the benefits of hypercomplex algebras, potentially improving the network performance and robustness.\n\nTherefore, in this paper, we propose a fully hypercomplex network (PHemoNet) comprising both encoders and fusion module in the hypercomplex domain. In detail, the encoders and the fusion module are composed of parameterized hypercomplex multiplications (PHMs). In this manner, at encoder level the model learns enhanced modality-specific embeddings thanks to hypercomplex algebra. In fact, each encoder specific to the different input physiological signal is defined in a different hypercomplex domain that is the natural domain of the signal, i.e., its original dimensionality. We employ as input signals the electroencephalogram (EEG), eye data, galvanic skin response (GSR), and electrocardiogram (ECG), as all of them reflect emotional changes  [9] . Then, a refined and more efficient hypercomplex fusion module learns the fused representation from the single modality embeddings. We evaluate our fully hypercomplex approach on the MAHNOB-HCI  [9]  benchmark for arousal and valence classification and compare it against previous state-of-the-art networks. We find that the proposed method surpasses the results in the literature on both tasks, advancing the state-of-the-art.\n\nThe rest of the paper is organized as follows. In section II we discuss the recent works on hypercomplex models and emotion recognition. In Section III we provide background on the theory behind hypercomplex networks, while we describe the details of the proposed architecture in Section IV. In Section V we present the experimental results from the conducted evaluation. Finally, we outline our conclusions in Section VI.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Hypercomplex Learning",
      "text": "Hypercomplex models are spreading across the research community unifying fields such as mathematics, signal processing, and deep learning. Starting from studying mathematical properties of different subdomains, including the widely studied quaternion domain  [10] -  [12] , along with the tessarine  [13]  and complex  [14]  domains, to exploiting their properties and capabilities tailoring models to specific applications. Indeed, QNNs excel in learning representations of multidimensional data such as audio  [15] ,  [16] , speech  [17] ,  [18] , and images  [19] ,  [20] . Even with medical images which are typically grayscale and are therefore 1-dimensional, quaternion algebra has been exploited by integrating the model with wavelet transforms  [21] ,  [22] . However, algebras follow the Cayley-Dickson system, i.e., there exists an algebra only for 2 m where m = 1, 2, • • • . Thus, PHNNs have been developed and have overcome the limitations imposed by specific hypercomplex domains by learning the algebra directly from the data  [2] ,  [3] ,  [23] . In this way, the capabilities of hypercomplex models can be brought to domains for which an algebra does not yet exist. For example, images can be processed in their natural 3D domain  [24] , and similarly, audio  [25]  and text  [26]  data can also be handled in their natural domains. Moreover, their flexibility allows to design architectures for multi-view  [5]  or multimodal  [27]  data that is typical in medical scenarios. This versatility also enables the development of lightweight models, a crucial property for medical applications like detecting atrial fibrillation from ECG signals collected by wearable devices  [4] . Finally, owing to their advantages, numerous studies are now delving into their theoretical aspects, ranging from initialization techniques  [28]  to studying their learning behavior by rendering them directly interpretable  [29] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Emotion Recognition",
      "text": "Given the reliability of physiological signals for the task of emotion recognition, several works developed machine learning-based methods, such as emotion recognition from photoplethysmography (PPG) and GSR  [30]  and valence recognition from EEG extracted features  [31] . Moreover, recent studies developed deep learning-based techniques, such as an attention-based hybrid model  [32]  and a heterogenous convolutional neural network (CNN) with multimodal factorized bilinear pooling  [33]  for EEG emotion recognition. The main issue with all aforementioned works lies in the extensive preprocessing and domain knowledge required. Indeed, they require hand-crafted features typical of pure machine learning methods  [30] ,  [31] , or power spectral density (PSD) and differential entropy (DE)  [32] , or a study of the impact of the frequency bands on the downstream task  [33] . In addition to preprocessing, a vast majority focuses on a single-modality approach, while humans express emotions in a multimodal manner. Recent multimodal approaches for emotion recognition include a temporal multimodal model from EEG and blood volume pulse (BVP)  [7]  and a cross-subject method based on completeness-induced adaptative broad learning from EEG and eye movement signals  [34] . Finally, in our previous work  [6]  we introduced HyperFuseNet, a multimodal architecture that learns directly from raw signals. In this paper, we extend this and develop a fully hypercomplex model to overcome its generalization issue.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Background",
      "text": "Hypercomplex models extend the conventional real-valued neural network paradigms by leveraging hypercomplex number systems, such as quaternions and octonions. These systems, which generalize complex numbers, enable more compact and efficient representations of data, especially in applications involving high-dimensional spaces and correlated features  [1] . Hypercomplex numbers extend complex numbers by incorporating additional dimensions. For instance, quaternions, a four-dimensional number system, consist of one real part and three imaginary parts. Mathematically, a quaternion q can be represented as q = q 0 + q 1 î + q 2 ij + q 3 κ, with q i ∈ R and î, ij and κ are imaginary units satisfying specific multiplication rules. Hypercomplex neural networks utilize these properties to construct layers that perform operations in the hypercomplex domain. A common approach is to replace real-valued weight matrices with hypercomplex-valued ones, resulting in models that can capture intricate correlations in the data with fewer parameters  [1] .\n\nParameterized hypercomplex networks (PHNNs)  [2] ,  [3]  introduce additional flexibility by allowing the network to learn optimal parameters that define the hypercomplex algebra used in the model. This approach generalizes fixed hypercomplex models like quaternion and octonion networks by incorporating trainable parameters that adjust the multiplication rules and interactions between different components of the hypercomplex numbers. In a parameterized hypercomplex neural network, the weight matrix W is not strictly defined by the fixed rules of quaternion or octonion algebra. Instead, it is parameterized by a set of learnable parameters leveraging Kronecker products:\n\nHerein, the matrices F i serve as the conventional learnable weights for a specific layer, while matrices A i encode the algebra rules. The hyperparameter n ∈ N defines the dimensionality of the number system used by the model, for instance, n = 4 corresponds to a Quaternion Neural Network (QNN). Following this definition of the weight matrix, a parameterized hypercomplex multiplication (PHM) layer is defined as its real-valued counterpart:\n\nwhere W is defines according to eq. 1. Thanks to this formulation, PHM layers and consequently PHNNs can operate also on domains for which an algebra does not yet exist, as it is encoded by A i during training. Moreover, they preserve the advantages of QNNs, i.e., efficiency and the ability to capture correlations.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iv. Proposed Method",
      "text": "We propose a fully hypercomplex network (PHemoNet) for multimodal emotion recognition, which we show in Fig.  1 . The architecture comprises both encoders and fusion module defined in different hypercomplex domains. The encoders learn an embedding specific to the modality, which will then be fused with the other modalities embeddings by the fusion module. In fact, the latter learns a fused representation which allows to leverage cross-modality information for a better classification output. Each input signal is processed by a specific encoder defined in the original domain of the physiological signal by setting the hyperparameter n of the PHM layers. In detail, for GSR we set n GSR = 1 since it is a 1-dimensional signal, for ECG we configure n ECG = 3, for the eye signals we set n eye = 4 since it comprises four dimensions including gaze coordinates, pupil dimensions, and eye distances, and finally, we set n EEG = 10 since we select ten electrodes. Each encoder is composed of one PHM layer with 128, 131, 1020, and 513 hidden units for eye signals, GSR, EEG, and ECG respectively, and interleaving batch normalization and ReLU activation functions. Then the refined fusion module is composed of three PHM layers with n f usion = 4 in accordance with the number of input modalities, and with an initial 1792 hidden units which are halved by each PHM layer. The same interleaving operations as in the encoders are applied with the addition of dropout layers with a rate of 0.5. In contrast with the fusion module in  [6] , which had only one dropout layer with a lower probability and had an additional PHM layer which contributed to the overfitting behavior of the model. A final classification layer yields the predicted class, i.e., calm, medium aroused, excited for arousal and unpleasant, neutral valence, and pleasant for valence. In addition to the refined fusion module, the main advantage of PHemoNet is the encoders operating in hypercomplex domains. In this way, the final architecture turns out to be more lightweight given the reduction of parameters brought by PHM layers  [2]  and the ability to capture relations among channels of each input signal thanks to hypercomplex algebra properties.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Experimental Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset And Preprocessing",
      "text": "The dataset we employ for our experiments is the MAHNOB-HCI database for affect recognition  [9] . It provides different physiological signals, video, and audio recordings of 27 subjects during an emotional experiment consisting of the view of video clips. Each recording is annotated with labels for arousal, i.e., calm, medium aroused, and excited, and valence, i.e., unpleasant, neutral, and pleasant. As already mentioned, for our experiments we utilize the eye data, which include eye Regarding preprocessing and data augmentation, we apply the same operations as in  [6] . For eye signals, we take the average of the left and right eye and do not remove -1 values as they are related to blinks which could be part of the emotional response. For EEG signals we select the ten electrodes most connected to emotions, i.e., F3, F4, F5, F6, F7, F8, T7, T8, P7, and P8  [36] ,  [37] , and we reference it with respect to the average. EEG, GSR, and ECG are resampled from 256Hz to 128Hz. We apply a band-pass filter of 0.5-45Hz for ECG signals, of 1-45Hz for EEG, and a low-pass filter at 50Hz for GSR. All of them are then filtered with a notch filter at 50Hz. Additionally, we correct the baseline of GSR to account for its initial offset as in  [6] . The signals are augmented with the addition of Gaussian noise and scaling operations as in  [6] . Each sample is a 10s segment of the original 30s recordings, which are split into training and testing in a stratified fashion, taking 80% and 20%, respectively.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Experimental Setup",
      "text": "We employ accuracy and F1-score as metrics. The F1score is defined as the harmonic mean of recall and precision metrics, thus it accounts for the imbalance present in the input data. The models are trained with a cross-entropy loss and Adam optimizer for 50 epochs with early stopping and the patience hyperparameter set to 10 in accordance with the F1-score. The loss is updated following a one-cycle policy configured with 7.96 × 10 -6 as maximum learning rate, 10 as dividing factors, 0.425% as the percentage of increasing steps, 0.7985 as maximum momentum, 0.7403 as minimum momentum and a linear annealing strategy.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Results And Discussion",
      "text": "We report the results of the experimental evaluation in Tab. I. We compare the proposed method against two stateof-art approaches  [6] ,  [35] . It is clear that the proposed fully hypercomplex architecture allows to generalize better and thus outperforms previous methods for both arousal and valence classification. Indeed, the generalization ability is due to several advantages of PHemoNet. Firstly, being a fully hypercomplex architecture, there is a greater reduction in parameters compared to HyperFuseNet, which only had the fusion module in the hypercomplex domain. This allows to build a model that is better tailored to the task instead of being overparameterized. Secondly, thanks to a refined fusion module with less layers and more dropout operations, it allows to not overfit during the fusion process. Finally, but most importantly, the encoders are able to process each input signal in its natural number domain by defining them in specific hypercomplex domains, and thus learn a better representation of the different modalities. These advantages yield an F1score of 0.401 for arousal classification and 0.505 for valence classification.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "We have proposed a fully hypercomplex network, PHe-moNet, for multimodal emotion recognition. Different physiological signals are processed by modality-specific encoders in their natural domain by setting the hyperparameter n of PHM layers as explained in Section IV. Moreover, we revise the fusion module previously proposed in  [6]  in order to not make the final architecture overfit. With the main advantage of a fully hypercomplex multimodal network, PHemoNet is able to generalize better with respect to state-of-the-art models and thus outperform them. In future works, we aim to investigate convolution operations at encoder level with parameterized hypercomplex convolutional (PHC) layers  [3] .",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: PHemoNet. Fully hypercomplex architecture with encoders operating in different hypercomplex domains according to n and a refined hypercomplex",
      "page": 3
    },
    {
      "caption": "Figure 1: The architecture comprises both encoders and fusion module",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "Email: eleonora.lopez@uniroma1.it."
        },
        {
          "Dept.": "Abstract—Emotion recognition is\nessential\nacross numerous",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "involving multidimensional inputs. Overall, hypercomplex lay-"
        },
        {
          "Dept.": "fields,\nincluding medical applications and brain-computer inter-",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "ers offer\na promising approach to advancing deep learning"
        },
        {
          "Dept.": "face\n(BCI). Emotional\nresponses\ninclude behavioral\nreactions,",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "models by leveraging the unique advantages of hypercomplex"
        },
        {
          "Dept.": "such\nas\ntone\nof\nvoice\nand\nbody movement,\nand\nchanges\nin",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "algebra."
        },
        {
          "Dept.": "physiological\nsignals,\nsuch as\nthe\nelectroencephalogram (EEG).",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "The latter are involuntary,\nthus they provide a reliable input\nfor",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "For\nthese\nreasons, hypercomplex architectures have been"
        },
        {
          "Dept.": "identifying emotions,\nin contrast to the former which individuals",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "developed for medical applications [4], as medical data often"
        },
        {
          "Dept.": "can\nconsciously\ncontrol. These\nsignals\nreveal\ntrue\nemotional",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "present multiple modalities or views\nthat\ncan be\neffectively"
        },
        {
          "Dept.": "states without\nintentional\nalteration,\nthus\nincreasing\nthe\nac-",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "exploited through an ad hoc hypercomplex architecture thanks"
        },
        {
          "Dept.": "curacy\nof\nemotion\nrecognition models. However, multimodal",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "to the properties of hypercomplex algebra\n[5]. Specifically,"
        },
        {
          "Dept.": "deep learning methods from physiological signals have not been",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "significantly investigated. In this paper, we introduce PHemoNet,",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "a\nrecent work\nhas\nintroduced\na multimodal\nnetwork\nfor"
        },
        {
          "Dept.": "a\nfully hypercomplex network for multimodal\nemotion recog-",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "emotion recognition from physiological\nsignals,\nfeaturing a"
        },
        {
          "Dept.": "nition\nfrom physiological\nsignals.\nIn\ndetail,\nthe\narchitecture",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "hypercomplex fusion module [6]. This module enhances fused"
        },
        {
          "Dept.": "comprises modality-specific encoders and a fusion module. Both",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "representations\nby\nleveraging\nthe\nrelations\namong\ndifferent"
        },
        {
          "Dept.": "encoders and fusion modules are defined in the hypercomplex",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "modalities using hypercomplex algebra.\nIt begins\nto address"
        },
        {
          "Dept.": "domain\nthrough\nparameterized\nhypercomplex multiplications",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "(PHMs)\nthat can capture latent relations between the different",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "challenges\nin the\nliterature,\nsuch as\nextensive preprocessing"
        },
        {
          "Dept.": "dimensions of each modality and between the modalities\nthem-",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "requiring domain-specific knowledge [7], which prevents neu-"
        },
        {
          "Dept.": "selves. The proposed method outperforms\ncurrent\nstate-of-the-",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "ral networks from learning important\nfeatures from raw data,"
        },
        {
          "Dept.": "art models on the MAHNOB-HCI dataset\nin classifying valence",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "and the use of single modalities when emotional responses are"
        },
        {
          "Dept.": "and arousal using electroencephalograms (EEGs) and peripheral",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "intrinsically multimodal [8]. However, this approach still faces"
        },
        {
          "Dept.": "physiological\nsignals. The\ncode\nfor\nthis work\nis\navailable\nat",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "https://github.com/ispamm/MHyEEG.",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "issues, mainly\ndue\nto\noverfitting. Additionally,\nthe\ncurrent"
        },
        {
          "Dept.": "Index Terms—Emotion recognition, EEG, Physiological signals,",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "architecture employs hypercomplex layers only at\nthe fusion"
        },
        {
          "Dept.": "Hypercomplex Networks, Hypercomplex algebra",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "step, while the encoders remain in the real domain. Incorporat-"
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "ing hypercomplex layers in the encoders could further exploit"
        },
        {
          "Dept.": "I.\nINTRODUCTION",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "the benefits of hypercomplex algebras, potentially improving"
        },
        {
          "Dept.": "Hypercomplex\nneural\nnetworks\nrepresent\na\nsophisticated",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "the network performance and robustness."
        },
        {
          "Dept.": "class of neural models that extend traditional\nreal-valued net-",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "Therefore,\nin this paper, we propose a fully hypercomplex"
        },
        {
          "Dept.": "works into higher-dimensional spaces using hypercomplex al-",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "network\n(PHemoNet)\ncomprising\nboth\nencoders\nand\nfusion"
        },
        {
          "Dept.": "gebras, such as complex numbers, quaternions, and octonions.",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "module in the hypercomplex domain.\nIn detail,\nthe encoders"
        },
        {
          "Dept.": "These networks exploit the inherent mathematical properties of",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "and the fusion module are composed of parameterized hyper-"
        },
        {
          "Dept.": "hypercomplex numbers to model complex relationships within",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "complex multiplications\n(PHMs).\nIn this manner, at encoder"
        },
        {
          "Dept.": "data, providing a more powerful mechanism for capturing both",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "level the model learns enhanced modality-specific embeddings"
        },
        {
          "Dept.": "local\nand\nglobal\npatterns\n[1]. Parameterized Hypercomplex",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "thanks to hypercomplex algebra. In fact, each encoder specific"
        },
        {
          "Dept.": "Neural Networks (PHNNs) further extend this capability by a",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "to\nthe\ndifferent\ninput\nphysiological\nsignal\nis\ndefined\nin\na"
        },
        {
          "Dept.": "parameterization through a hyperparameter n which enables",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "different\nhypercomplex\ndomain\nthat\nis\nthe\nnatural\ndomain"
        },
        {
          "Dept.": "them to\nsimulate\ndifferent\nhypercomplex\ndomains\nsuch\nas",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "of\nthe\nsignal,\ni.e.,\nits\noriginal\ndimensionality. We\nemploy"
        },
        {
          "Dept.": "complex\nand\nquaternion\nspaces\n[2],\n[3]. This\nadaptability",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "as\ninput\nsignals\nthe\nelectroencephalogram (EEG),\neye data,"
        },
        {
          "Dept.": "results\nin significant parameter efficiency and improved data",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "galvanic skin response (GSR), and electrocardiogram (ECG),"
        },
        {
          "Dept.": "representation, making PHNNs particularly suitable for\ntasks",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "as all of\nthem reflect emotional changes [9]. Then, a refined"
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "and more\nefficient\nhypercomplex\nfusion module\nlearns\nthe"
        },
        {
          "Dept.": "This work was partially supported by the Italian Ministry of University and",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "fused representation from the single modality embeddings. We"
        },
        {
          "Dept.": "Research (MUR) within the PRIN 2022 Program for the project “EXEGETE:",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "evaluate our\nfully hypercomplex approach on the MAHNOB-"
        },
        {
          "Dept.": "Explainable Generative Deep Learning Methods for Medical Signal and Image",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "Processing”,\nunder\ngrant\nnumber\n2022ENK9LS, CUP B53D23013030006,",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "HCI\n[9] benchmark for arousal and valence classification and"
        },
        {
          "Dept.": "and in part by the European Union under the National Plan for Complementary",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "compare it against previous state-of-the-art networks. We find"
        },
        {
          "Dept.": "Investments to the Italian National Recovery and Resilience Plan (NRRP) of",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "that\nthe proposed method surpasses the results in the literature"
        },
        {
          "Dept.": "NextGenerationEU, Project PNC 0000001 D3 4 Health - SPOKE 1 - CUP",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": ""
        },
        {
          "Dept.": "B53C22006120001.",
          "Information Engineering, Electronics and Telecommunications (DIET), Sapienza University of Rome,\nItaly": "on both tasks, advancing the state-of-the-art."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The rest of the paper is organized as follows. In section II we": "discuss the recent works on hypercomplex models and emotion",
          "differential entropy (DE) [32], or a study of the impact of the": "frequency bands on the downstream task [33].\nIn addition to"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "recognition.\nIn Section\nIII we\nprovide\nbackground\non\nthe",
          "differential entropy (DE) [32], or a study of the impact of the": "preprocessing, a vast majority focuses on a single-modality ap-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "theory behind hypercomplex networks, while we describe the",
          "differential entropy (DE) [32], or a study of the impact of the": "proach, while humans express emotions in a multimodal man-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "details of\nthe proposed architecture in Section IV.\nIn Section",
          "differential entropy (DE) [32], or a study of the impact of the": "ner. Recent multimodal\napproaches\nfor\nemotion recognition"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "V we\npresent\nthe\nexperimental\nresults\nfrom the\nconducted",
          "differential entropy (DE) [32], or a study of the impact of the": "include a temporal multimodal model\nfrom EEG and blood"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "evaluation. Finally, we outline our conclusions in Section VI.",
          "differential entropy (DE) [32], or a study of the impact of the": "volume pulse\n(BVP)\n[7]\nand a\ncross-subject method based"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "on completeness-induced adaptative broad learning from EEG"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "II. RELATED WORKS",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "and eye movement signals [34]. Finally,\nin our previous work"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "A. Hypercomplex learning",
          "differential entropy (DE) [32], or a study of the impact of the": "[6] we\nintroduced HyperFuseNet,\na multimodal\narchitecture"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "Hypercomplex models\nare\nspreading\nacross\nthe\nresearch",
          "differential entropy (DE) [32], or a study of the impact of the": "that\nlearns directly from raw signals. In this paper, we extend"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "community unifying fields\nsuch as mathematics,\nsignal pro-",
          "differential entropy (DE) [32], or a study of the impact of the": "this and develop a fully hypercomplex model\nto overcome its"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "cessing, and deep learning. Starting from studying mathemat-",
          "differential entropy (DE) [32], or a study of the impact of the": "generalization issue."
        },
        {
          "The rest of the paper is organized as follows. In section II we": "ical properties of different\nsubdomains,\nincluding the widely",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "III. BACKGROUND"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "studied quaternion domain [10]–[12], along with the tessarine",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "Hypercomplex models extend the conventional\nreal-valued"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "[13]\nand complex [14] domains,\nto exploiting their proper-",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "neural network paradigms by leveraging hypercomplex num-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "ties and capabilities tailoring models to specific applications.",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "ber\nsystems,\nsuch as quaternions\nand octonions. These\nsys-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "Indeed, QNNs\nexcel\nin\nlearning\nrepresentations\nof multi-",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "tems, which generalize complex numbers, enable more com-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "dimensional data such as audio [15],\n[16], speech [17],\n[18],",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "pact\nand efficient\nrepresentations of data,\nespecially in ap-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "and images\n[19],\n[20]. Even with medical\nimages which are",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "plications\ninvolving high-dimensional\nspaces\nand correlated"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "typically grayscale and are therefore 1-dimensional, quaternion",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "features [1]. Hypercomplex numbers extend complex numbers"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "algebra\nhas\nbeen\nexploited\nby\nintegrating\nthe model with",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "by incorporating additional dimensions. For\ninstance, quater-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "wavelet\ntransforms\n[21],\n[22]. However, algebras\nfollow the",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "nions, a four-dimensional number system, consist of one real"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "Cayley-Dickson system,\ni.e.,\nthere exists an algebra only for",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "part and three imaginary parts. Mathematically, a quaternion"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "2m where m = 1, 2, · · · . Thus, PHNNs have been developed",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "q\ncan be\nrepresented as\nq = q0 + q1ˆı + q2ij + q3ˆκ, with"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "and have overcome\nthe\nlimitations\nimposed by specific hy-",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "ij and ˆκ are imaginary units satisfying specific\nqi ∈ R and ˆı,"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "percomplex domains by learning the algebra directly from the",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "multiplication\nrules. Hypercomplex\nneural\nnetworks\nutilize"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "data [2], [3], [23]. In this way, the capabilities of hypercomplex",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "these properties to construct\nlayers that perform operations in"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "models can be brought\nto domains for which an algebra does",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "the hypercomplex domain. A common approach is to replace"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "not yet exist. For example,\nimages can be processed in their",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "real-valued weight matrices with hypercomplex-valued ones,"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "natural 3D domain [24], and similarly, audio [25] and text [26]",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "resulting in models\nthat can capture intricate correlations\nin"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "data can also be handled in their natural domains. Moreover,",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "the data with fewer parameters [1]."
        },
        {
          "The rest of the paper is organized as follows. In section II we": "their flexibility allows\nto design architectures\nfor multi-view",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "Parameterized\nhypercomplex\nnetworks\n(PHNNs)\n[2],\n[3]"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "[5] or multimodal [27] data that is typical in medical scenarios.",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "introduce\nadditional flexibility\nby\nallowing\nthe\nnetwork\nto"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "This versatility also enables\nthe development of\nlightweight",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "learn optimal parameters that define the hypercomplex algebra"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "models, a crucial property for medical applications like detect-",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "used\nin\nthe model. This\napproach\ngeneralizes fixed\nhyper-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "ing atrial fibrillation from ECG signals collected by wearable",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "complex models\nlike\nquaternion\nand\noctonion\nnetworks\nby"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "devices\n[4].\nFinally,\nowing\nto\ntheir\nadvantages,\nnumerous",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "incorporating trainable parameters\nthat adjust\nthe multiplica-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "studies are now delving into their\ntheoretical aspects,\nranging",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "tion rules\nand interactions between different\ncomponents of"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "from initialization techniques\n[28]\nto studying their\nlearning",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "the hypercomplex numbers. In a parameterized hypercomplex"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "behavior by rendering them directly interpretable [29].",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "neural network,\nthe weight matrix W is not\nstrictly defined"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "B. Emotion recognition",
          "differential entropy (DE) [32], or a study of the impact of the": "by the fixed rules of quaternion or octonion algebra.\nInstead,"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "it\nis parameterized by a set of learnable parameters leveraging"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "Given the\nreliability of physiological\nsignals\nfor\nthe\ntask",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "",
          "differential entropy (DE) [32], or a study of the impact of the": "Kronecker products:"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "of\nemotion\nrecognition,\nseveral works\ndeveloped machine",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "learning-based methods,\nsuch\nas\nemotion\nrecognition\nfrom",
          "differential entropy (DE) [32], or a study of the impact of the": ""
        },
        {
          "The rest of the paper is organized as follows. In section II we": "photoplethysmography\n(PPG)\nand GSR [30]\nand\nvalence",
          "differential entropy (DE) [32], or a study of the impact of the": "n(cid:88) i\nW =\n(1)\nAi ⊗ Fi."
        },
        {
          "The rest of the paper is organized as follows. In section II we": "recognition from EEG extracted features\n[31]. Moreover,\nre-",
          "differential entropy (DE) [32], or a study of the impact of the": "=1"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "cent\nstudies developed deep learning-based techniques,\nsuch",
          "differential entropy (DE) [32], or a study of the impact of the": "Herein,\nserve\nas\nthe\nconventional\nlearnable\nthe matrices Fi"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "as an attention-based hybrid model\n[32] and a heterogenous",
          "differential entropy (DE) [32], or a study of the impact of the": "weights\nfor\na\nspecific\nencode\nthe\nlayer, while matrices Ai"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "convolutional neural network (CNN) with multimodal\nfactor-",
          "differential entropy (DE) [32], or a study of the impact of the": "algebra rules. The hyperparameter n ∈ N defines the dimen-"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "ized bilinear pooling [33]\nfor EEG emotion recognition. The",
          "differential entropy (DE) [32], or a study of the impact of the": "sionality of the number system used by the model, for instance,"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "main issue with all aforementioned works lies in the extensive",
          "differential entropy (DE) [32], or a study of the impact of the": "n = 4 corresponds\nto a Quaternion Neural Network (QNN)."
        },
        {
          "The rest of the paper is organized as follows. In section II we": "preprocessing and domain knowledge required.\nIndeed,\nthey",
          "differential entropy (DE) [32], or a study of the impact of the": "Following this definition of the weight matrix, a parameterized"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "require hand-crafted features typical of pure machine learning",
          "differential entropy (DE) [32], or a study of the impact of the": "hypercomplex multiplication\n(PHM)\nlayer\nis\ndefined\nas\nits"
        },
        {
          "The rest of the paper is organized as follows. In section II we": "methods\n[30],\n[31],\nor\npower\nspectral\ndensity\n(PSD)\nand",
          "differential entropy (DE) [32], or a study of the impact of the": "real-valued counterpart:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PHM\nBatch": "Dense\nDropout\nModality-level"
        },
        {
          "PHM\nBatch": "Norm"
        },
        {
          "PHM\nBatch": "Fig. 1.\nPHemoNet. Fully hypercomplex architecture with encoders operating in different hypercomplex domains according to n and a refined hypercomplex"
        },
        {
          "PHM\nBatch": "for valence/arousal prefiction.\nfusion module with nfusion = 4. Finally, a classification layer yileds the final output"
        },
        {
          "PHM\nBatch": "ReLU activation functions. Then the\nrefined fusion module"
        },
        {
          "PHM\nBatch": "is\ncomposed\nof\nthree\nin\ny = PHM(x) = W · x + b,\n(2)\nPHM layers with nf usion = 4"
        },
        {
          "PHM\nBatch": "accordance with\nthe\nnumber\nof\ninput modalities,\nand with"
        },
        {
          "PHM\nBatch": "where W is defines according to eq. 1. Thanks to this formu-"
        },
        {
          "PHM\nBatch": "an initial 1792 hidden units which are halved by each PHM"
        },
        {
          "PHM\nBatch": "lation, PHM layers and consequently PHNNs can operate also"
        },
        {
          "PHM\nBatch": "layer. The same interleaving operations as in the encoders are"
        },
        {
          "PHM\nBatch": "on domains\nfor which an algebra does not yet exist, as\nit\nis"
        },
        {
          "PHM\nBatch": "applied with the addition of dropout\nlayers with a rate of 0.5."
        },
        {
          "PHM\nBatch": "they preserve the\nencoded by Ai during training. Moreover,"
        },
        {
          "PHM\nBatch": "In contrast with the fusion module in [6], which had only one"
        },
        {
          "PHM\nBatch": "advantages of QNNs,\ni.e., efficiency and the ability to capture"
        },
        {
          "PHM\nBatch": "dropout\nlayer with a lower probability and had an additional"
        },
        {
          "PHM\nBatch": "correlations."
        },
        {
          "PHM\nBatch": "PHM layer which contributed to the overfitting behavior of the"
        },
        {
          "PHM\nBatch": "IV. PROPOSED METHOD\nmodel. A final classification layer yields\nthe predicted class,"
        },
        {
          "PHM\nBatch": "i.e., calm, medium aroused, excited for arousal and unpleasant,"
        },
        {
          "PHM\nBatch": "We propose a fully hypercomplex network (PHemoNet) for"
        },
        {
          "PHM\nBatch": "neutral valence, and pleasant\nfor valence.\nIn addition to the"
        },
        {
          "PHM\nBatch": "multimodal\nemotion recognition, which we\nshow in Fig. 1."
        },
        {
          "PHM\nBatch": "refined fusion module,\nthe main advantage of PHemoNet\nis"
        },
        {
          "PHM\nBatch": "The architecture comprises both encoders and fusion module"
        },
        {
          "PHM\nBatch": "the encoders operating in hypercomplex domains. In this way,"
        },
        {
          "PHM\nBatch": "defined in different hypercomplex domains. The encoders learn"
        },
        {
          "PHM\nBatch": "the final architecture turns out\nto be more lightweight given"
        },
        {
          "PHM\nBatch": "an embedding specific\nto the modality, which will\nthen be"
        },
        {
          "PHM\nBatch": "the reduction of parameters brought by PHM layers\n[2] and"
        },
        {
          "PHM\nBatch": "fused with\nthe\nother modalities\nembeddings\nby\nthe\nfusion"
        },
        {
          "PHM\nBatch": "the ability to capture relations among channels of each input"
        },
        {
          "PHM\nBatch": "module. In fact,\nthe latter learns a fused representation which"
        },
        {
          "PHM\nBatch": "signal\nthanks to hypercomplex algebra properties."
        },
        {
          "PHM\nBatch": "allows to leverage cross-modality information for a better clas-"
        },
        {
          "PHM\nBatch": "sification output. Each input signal\nis processed by a specific"
        },
        {
          "PHM\nBatch": "V. EXPERIMENTAL RESULTS"
        },
        {
          "PHM\nBatch": "encoder defined in the original domain of\nthe physiological"
        },
        {
          "PHM\nBatch": "A. Dataset and preprocessing"
        },
        {
          "PHM\nBatch": "signal by setting the hyperparameter n of\nthe PHM layers.\nIn"
        },
        {
          "PHM\nBatch": "The\ndataset\nwe\nemploy\nfor\nour\nexperiments\nis\nthe\ndetail,\nis a 1-dimensional\nfor GSR we set nGSR = 1 since it"
        },
        {
          "PHM\nBatch": "MAHNOB-HCI database for affect recognition [9]. It provides\nsignal,\nfor\nthe eye signals\nfor ECG we configure nECG = 3,"
        },
        {
          "PHM\nBatch": "different physiological signals, video, and audio recordings of\nwe set neye = 4 since it comprises four dimensions including"
        },
        {
          "PHM\nBatch": "27 subjects during an emotional experiment consisting of\nthe\ngaze\ncoordinates, pupil dimensions,\nand eye distances,\nand"
        },
        {
          "PHM\nBatch": "view of video clips. Each recording is annotated with labels for\nfinally, we\nsince we\nselect\nten\nelectrodes.\nset nEEG = 10"
        },
        {
          "PHM\nBatch": "Each encoder\nis composed of one PHM layer with 128, 131,\narousal,\ni.e., calm, medium aroused, and excited, and valence,"
        },
        {
          "PHM\nBatch": "i.e., unpleasant, neutral, and pleasant. As already mentioned,\n1020, and 513 hidden units\nfor eye signals, GSR, EEG, and"
        },
        {
          "PHM\nBatch": "for our experiments we utilize the eye data, which include eye\nECG respectively,\nand interleaving batch normalization and"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "We\nemploy\naccuracy\nand\nF1-score\nas metrics. The\nF1-"
        },
        {
          "B. Experimental setup": "score is defined as the harmonic mean of recall and precision"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "metrics,\nthus\nit\naccounts\nfor\nthe\nimbalance\npresent\nin\nthe"
        },
        {
          "B. Experimental setup": "input data. The models are trained with a cross-entropy loss"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "and Adam optimizer\nfor 50 epochs with early stopping and"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "the patience hyperparameter set\nto 10 in accordance with the"
        },
        {
          "B. Experimental setup": "F1-score. The\nloss\nis updated following a one-cycle policy"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "configured with 7.96 × 10−6\nas maximum learning rate, 10"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "as dividing factors, 0.425% as\nthe percentage of\nincreasing"
        },
        {
          "B. Experimental setup": "steps, 0.7985 as maximum momentum, 0.7403 as minimum"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "momentum and a linear annealing strategy."
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "C. Results and discussion"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "We\nreport\nthe\nresults\nof\nthe\nexperimental\nevaluation\nin"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "Tab.\nI. We compare the proposed method against\ntwo state-"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "of-art\napproaches\n[6],\n[35].\nIt\nis\nclear\nthat\nthe\nproposed"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "fully\nhypercomplex\narchitecture\nallows\nto\ngeneralize\nbetter"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "and thus outperforms previous methods\nfor both arousal and"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "valence classification. Indeed,\nthe generalization ability is due"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "to\nseveral\nadvantages\nof PHemoNet. Firstly,\nbeing\na\nfully"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "hypercomplex\narchitecture,\nthere\nis\na\ngreater\nreduction\nin"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "parameters\ncompared to HyperFuseNet, which only had the"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "fusion module\nin the hypercomplex domain. This\nallows\nto"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "build a model\nthat\nis better\ntailored to the\ntask instead of"
        },
        {
          "B. Experimental setup": ""
        },
        {
          "B. Experimental setup": "being overparameterized. Secondly,\nthanks to a refined fusion"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "HyperFuseNet\n[6]\n0.397 ± 0.018\n41.56 ± 1.33",
          "0.383 ± 0.012\n40.24 ± 1.04": "0.436 ± 0.022\n44.30 ± 2.01"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "PHemoNet\n(ours)\n0.401 ± 0.022\n42.54 ± 1.98",
          "0.383 ± 0.012\n40.24 ± 1.04": "0.505 ± 0.005\n50.77 ± 0.50"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "distances, pupil dimensions and gaze coordinates, GSR, ECG,",
          "0.383 ± 0.012\n40.24 ± 1.04": "module with less layers and more dropout operations, it allows"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "and EEG.",
          "0.383 ± 0.012\n40.24 ± 1.04": "to\nnot\noverfit\nduring\nthe\nfusion\nprocess. Finally,\nbut most"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "Regarding preprocessing and data augmentation, we apply",
          "0.383 ± 0.012\n40.24 ± 1.04": "importantly,\nthe encoders are able to process each input signal"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "the\nsame operations\nas\nin [6]. For\neye\nsignals, we\ntake\nthe",
          "0.383 ± 0.012\n40.24 ± 1.04": "in\nits\nnatural\nnumber\ndomain\nby\ndefining\nthem in\nspecific"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "average of the left and right eye and do not remove -1 values as",
          "0.383 ± 0.012\n40.24 ± 1.04": "hypercomplex domains, and thus learn a better\nrepresentation"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "they are related to blinks which could be part of the emotional",
          "0.383 ± 0.012\n40.24 ± 1.04": "of\nthe\ndifferent modalities. These\nadvantages\nyield\nan F1-"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "response. For EEG signals we select\nthe ten electrodes most",
          "0.383 ± 0.012\n40.24 ± 1.04": "score of 0.401 for arousal classification and 0.505 for valence"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "connected to emotions,\ni.e., F3, F4, F5, F6, F7, F8, T7, T8,",
          "0.383 ± 0.012\n40.24 ± 1.04": "classification."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "P7,\nand P8 [36],\n[37],\nand we\nreference\nit with respect\nto",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "VI. CONCLUSION"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "the average. EEG, GSR, and ECG are resampled from 256Hz",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "to 128Hz. We apply a band-pass filter of 0.5-45Hz for ECG",
          "0.383 ± 0.012\n40.24 ± 1.04": "We\nhave\nproposed\na\nfully\nhypercomplex\nnetwork, PHe-"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "signals, of 1-45Hz for EEG, and a low-pass filter at 50Hz for",
          "0.383 ± 0.012\n40.24 ± 1.04": "moNet,\nfor multimodal emotion recognition. Different phys-"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "GSR. All of them are then filtered with a notch filter at 50Hz.",
          "0.383 ± 0.012\n40.24 ± 1.04": "iological signals are processed by modality-specific encoders"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "Additionally, we correct\nthe baseline of GSR to account\nfor",
          "0.383 ± 0.012\n40.24 ± 1.04": "in their natural domain by setting the hyperparameter n of"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "its initial offset as in [6]. The signals are augmented with the",
          "0.383 ± 0.012\n40.24 ± 1.04": "PHM layers as explained in Section IV. Moreover, we revise"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "addition of Gaussian noise and scaling operations as\nin [6].",
          "0.383 ± 0.012\n40.24 ± 1.04": "the fusion module previously proposed in [6]\nin order\nto not"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "Each sample is a 10s segment of\nthe original 30s recordings,",
          "0.383 ± 0.012\n40.24 ± 1.04": "make the final architecture overfit. With the main advantage of"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "which are split\ninto training and testing in a stratified fashion,",
          "0.383 ± 0.012\n40.24 ± 1.04": "a fully hypercomplex multimodal network, PHemoNet\nis able"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "taking 80% and 20%,\nrespectively.",
          "0.383 ± 0.012\n40.24 ± 1.04": "to generalize better with respect\nto state-of-the-art models and"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "thus outperform them.\nIn future works, we aim to investigate"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "B. Experimental setup",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "convolution\noperations\nat\nencoder\nlevel with\nparameterized"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "We\nemploy\naccuracy\nand\nF1-score\nas metrics. The\nF1-",
          "0.383 ± 0.012\n40.24 ± 1.04": "hypercomplex convolutional\n(PHC)\nlayers [3]."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "score is defined as the harmonic mean of recall and precision",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "REFERENCES"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "metrics,\nthus\nit\naccounts\nfor\nthe\nimbalance\npresent\nin\nthe",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "input data. The models are trained with a cross-entropy loss",
          "0.383 ± 0.012\n40.24 ± 1.04": "[1] D. Comminiello, E. Grassucci, D. P. Mandic,\nand A. Uncini,\n“De-"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "mystifying the hypercomplex:\nInductive biases\nin hypercomplex deep"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "and Adam optimizer\nfor 50 epochs with early stopping and",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "learning,” IEEE Signal Process. Mag., 2024."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "the patience hyperparameter set\nto 10 in accordance with the",
          "0.383 ± 0.012\n40.24 ± 1.04": "[2] A. Zhang, Y. Tay, S. Zhang, A. Chan, A. T. Luu, S. C. Hui, and J. Fu,"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "F1-score. The\nloss\nis updated following a one-cycle policy",
          "0.383 ± 0.012\n40.24 ± 1.04": "“Beyond fully-connected layers with quaternions: Parameterization of"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "hypercomplex multiplications with 1/n parameters,” Int. Conf. on Mach."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "configured with 7.96 × 10−6\nas maximum learning rate, 10",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "Learn.\n(ICML), 2021."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "as dividing factors, 0.425% as\nthe percentage of\nincreasing",
          "0.383 ± 0.012\n40.24 ± 1.04": "[3]\nE. Grassucci, A. Zhang,\nand D. Comminiello,\n“PHNNs: Lightweight"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "steps, 0.7985 as maximum momentum, 0.7403 as minimum",
          "0.383 ± 0.012\n40.24 ± 1.04": "neural networks via parameterized hypercomplex convolutions,” IEEE"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "Trans. on Neural Netw. and Learn. Syst., 2022."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "momentum and a linear annealing strategy.",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "[4]\nL. Basso, Z. Ren, and W. Nejdl, “Efficient ECG-based atrial fibrillation"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "detection via parameterised hypercomplex neural networks,”\nin Euro-"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "C. Results and discussion",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "pean Signal Process. Conf.\n(EUSIPCO).\nIEEE, 2023, pp. 1375–1379."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "[5]\nE. Lopez, E. Grassucci, M. Valleriani,\nand D. Comminiello,\n“Multi-"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "We\nreport\nthe\nresults\nof\nthe\nexperimental\nevaluation\nin",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "view hypercomplex learning for breast cancer screening,” ArXiv preprint"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "Tab.\nI. We compare the proposed method against\ntwo state-",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "arXiv:2204.05798, 2022."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "of-art\napproaches\n[6],\n[35].\nIt\nis\nclear\nthat\nthe\nproposed",
          "0.383 ± 0.012\n40.24 ± 1.04": "[6]\nE. Lopez, E. Chiarantano, E. Grassucci,\nand D. Comminiello,\n“Hy-"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "percomplex multimodal\nemotion recognition from eeg and peripheral"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "fully\nhypercomplex\narchitecture\nallows\nto\ngeneralize\nbetter",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "physiological signals,” in IEEE Int. Conf. on Acoust., Speech and Signal"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "and thus outperforms previous methods\nfor both arousal and",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "Process. Workshops (ICASSPW).\nIEEE, 2023, pp. 1–5."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "valence classification. Indeed,\nthe generalization ability is due",
          "0.383 ± 0.012\n40.24 ± 1.04": "[7] B. Nakisa, M. N. Rastgoo, A. Rakotonirainy, F. Maire, and V. Chan-"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "dran, “Automatic emotion recognition using temporal multimodal deep"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "to\nseveral\nadvantages\nof PHemoNet. Firstly,\nbeing\na\nfully",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "learning,” IEEE Access, vol. 8, pp. 225 463–225 474, 2020."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "hypercomplex\narchitecture,\nthere\nis\na\ngreater\nreduction\nin",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "[8] Y. Zhang, C. Cheng, and Y. Zhang, “Multimodal emotion recognition"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "parameters\ncompared to HyperFuseNet, which only had the",
          "0.383 ± 0.012\n40.24 ± 1.04": "based on manifold learning and convolution neural network,” Multimedia"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "Tools and Appl., vol. 81, no. 23, pp. 33 253–33 268, 2022."
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "fusion module\nin the hypercomplex domain. This\nallows\nto",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "[9] M. Soleymani,\nJ. Lichtenauer, T. Pun, and M. Pantic, “A multimodal"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "build a model\nthat\nis better\ntailored to the\ntask instead of",
          "0.383 ± 0.012\n40.24 ± 1.04": ""
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "",
          "0.383 ± 0.012\n40.24 ± 1.04": "database for affect\nrecognition and implicit\ntagging,” IEEE Trans. on"
        },
        {
          "Dolmans [35]\n0.389 ± 0.011\n40.90 ± 0.62": "being overparameterized. Secondly,\nthanks to a refined fusion",
          "0.383 ± 0.012\n40.24 ± 1.04": "Affect. Comput., vol. 3, no. 1, pp. 42–55, 2011."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "“Quaternion-valued correlation learning for\nfew-shot semantic segmen-",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "(IJCNN).\nIEEE, 2022, pp. 1–8."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "tation,” IEEE Trans. on Circuits and Syst.\nfor Video Technol., 2022.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "I.\nI. Panagos, G. Sfikas,\nand C. Nikou,\n“Compressing\naudio\nvisual"
        },
        {
          "[10]": "[11]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "S. Qin, X. Zhang, H. Xu, and Y. Xu, “Fast quaternion product units for",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "speech recognition models with parameterized hypercomplex layers,”"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "learning disentangled representations in SO(3),” IEEE Trans. on Pattern",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "Intell.\nin Hellenic Conf. on Artif.\n(SETN), 2022, pp. 1–7."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "Anal. and Mach.\nIntell., vol. 45, no. 4, pp. 4504–4520, 2022.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "[26] G. Sfikas, G. Retsinas, P. Dimitrakopoulos, B. Gatos,\nand C. Nikou,"
        },
        {
          "[10]": "[12] G. Vieira, E. Grassucci, M. E. Valle,",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "and D. Comminiello,\n“Dual",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "“Shared-operation hypercomplex networks for handwritten text recogni-"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "quaternion rotational and translational equivariance in 3D rigid motion",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "tion,” in Int. Conf. on Document Anal. and Recogni.\nSpringer, 2023,"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "for Signal Process.\nmodelling,” in IEEE Int. Workshop on Mach. Learn.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "pp. 200–216."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "(MLSP).\nIEEE, 2023, pp. 1–6.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "E. Lopez, F. Betello, F. Carmignani, E. Grassucci, and D. Comminiello,"
        },
        {
          "[10]": "[13]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "J. Navarro-Moreno, R. M. Fern´andez-Alcal´a, J. D. Jim´enez-L´opez, and",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "“Attention-map augmentation for hypercomplex breast cancer classifica-"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "J. C. Ruiz-Molina, “Tessarine signal processing under the T-properness",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "tion,” Pattern Recognit. Letters, vol. 182, pp. 140–146, 2024."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "condition,” Journal of the Franklin Institute, vol. 357, no. 14, pp. 10 100–",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "[28] M. Mancanelli, E. Grassucci, A. Uncini, and D. Comminiello, “PHYDI:"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "10 126, 2020.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "Initializing\nparameterized\nhypercomplex\nneural\nnetworks\nas\nidentity"
        },
        {
          "[10]": "[14]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "J. Bassey, L. Qian,\nand X. Li,\n“A survey\nof\ncomplex-valued\nneural",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "for Signal Process.\nfunctions,” in IEEE Int. Workshop on Mach. Learn."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "networks,” ArXiv preprint arXiv:2101.12249, 2021.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "(MLSP).\nIEEE, 2023, pp. 1–6."
        },
        {
          "[10]": "[15] C. Brignone, G. Mancini, E. Grassucci, A. Uncini, and D. Comminiello,",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "E. Lopez, E. Grassucci, D. Capriotti,\nand D. Comminiello,\n“Towards"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "“Efficient\nsound\nevent\nlocalization\nand\ndetection\nin\nthe\nquaternion",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "explaining hypercomplex neural networks,” Int. Joint Conf. on Neural"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "domain,” IEEE Trans. on Circuits and Syst.\nII: Express Briefs, vol. 69,",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "Netw.\n(IJCNN), pp. 1–8, 2024."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "no. 5, pp. 2453–2457, 2022.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "J. A. Dom´ınguez-Jim´enez, K. C. Campo-Landines,\nJ. C. Mart´ınez-"
        },
        {
          "[10]": "[16]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "E. Grassucci, G. Mancini, C. Brignone, A. Uncini, and D. Comminiello,",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "Santos, E. J. Delahoz, and S. H. Contreras-Ortiz, “A machine learning"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "“Dual quaternion ambisonics array for\nsix-degree-of-freedom acoustic",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "model\nfor\nemotion\nrecognition\nfrom physiological\nsignals,” Biomed."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "representation,” Pattern Recognit. Letters, vol. 166, pp. 24–30, 2023.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "Signal Process. and Control, vol. 55, p. 101646, 2020."
        },
        {
          "[10]": "[17] A. Muppidi and M. Radfar, “Speech emotion recognition using quater-",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "L. Abdel-Hamid,\n“An efficient machine\nlearning-based emotional va-"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "IEEE Int. Conf.\non Acoust.,\nnion\nconvolutional\nneural\nnetworks,”\nin",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "lence recognition approach towards wearable EEG,” Sensors, vol. 23,"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "Speech and Signal Process.\n(ICASSP).\nIEEE, 2021, pp. 6309–6313.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "no. 3, p. 1255, 2023."
        },
        {
          "[10]": "[18]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "E. Guizzo, T. Weyde, S. Scardapane, and D. Comminiello, “Learning",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "and S. Wang,\n“An attention-based hybrid deep"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "speech emotion representations\nin the quaternion domain,” IEEE/ACM",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "learning model for EEG emotion recognition,” Signal, Image and Video"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "Trans. on Audio, Speech, and Language Process., vol. 31, pp. 1200–",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "Process., vol. 17, no. 5, pp. 2305–2313, 2023."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "1212, 2023.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "[33] Y. Zhang, C. Cheng, S. Wang, and T. Xia, “Emotion recognition using"
        },
        {
          "[10]": "[19] V. Frants, S. Agaian, and K. Panetta, “QCNN-H: Single-image dehazing",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "heterogeneous convolutional neural networks combined with multimodal"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "using quaternion neural networks,” IEEE Trans. on Cybernetics, 2023.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "Biomed.\nSignal\nProcess.\nfactorized\nbilinear\npooling,”\nand Control,"
        },
        {
          "[10]": "[20]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "T. Parcollet, M. Morchid,\nand G. Linar`es,\n“Quaternion convolutional",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "vol. 77, p. 103877, 2022."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "neural networks for heterogeneous image processing,” in IEEE Int. Conf.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "[34] X. Gong, C. P. Chen, B. Hu, and T. Zhang, “CiABL: Completeness-"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "on Acoust., Speech and Signal Process.\n(ICASSP).\nIEEE, 2019, pp.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "induced adaptative broad learning for cross-subject emotion recognition"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "8514–8518.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "with EEG and eye movement signals,” IEEE Trans. on Affect. Comput.,"
        },
        {
          "[10]": "[21]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "L. Sigillo, E. Grassucci, A. Uncini, and D. Comminiello, “Generalizing",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "2024."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "medical\nimage representations via quaternion wavelet networks,” ArXiv",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "T. C. Dolmans, M. Poel,\nJ.-W.\nJ. van’t Klooster, and B. P. Veldkamp,"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "preprint arXiv:2310.10224, 2023.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "“Perceived mental workload\nclassification\nusing\nintermediate\nfusion"
        },
        {
          "[10]": "[22]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "E. Grassucci, L. Sigillo, A. Uncini, and D. Comminiello, “GROUSE:",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "multimodal deep learning,” Frontiers in Human Neuroscience, vol. 14,"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "A task\nand model\nagnostic wavelet-driven\nframework\nfor medical",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "p. 609096, 2021."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "imaging,” IEEE Signal Process. Letters, 2023.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "[36] A. Topic, M. Russo, M. Stella, and M. Saric, “Emotion recognition using"
        },
        {
          "[10]": "[23]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "T. Le, M. Bertolini, F. No´e, and D.-A. Clevert, “Parameterized hyper-",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "a\nreduced set of EEG channels based on holographic\nfeature maps,”"
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "complex graph neural networks\nfor graph classification,” in Int. Conf.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "Sensors, vol. 22, no. 9, p. 3248, 2022."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "on Artif. Neural Netw.\n(ICANN).\nSpringer, 2021, pp. 204–216.",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "J. R. Msonda, Z. He, and C. Lu, “Feature reconstruction based channel"
        },
        {
          "[10]": "[24]",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "E. Grassucci, L. Sigillo, A. Uncini, and D. Comminiello, “Hypercom-",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "selection for emotion recognition using EEG,” in IEEE Signal Process."
        },
        {
          "[10]": "",
          "Z. Zheng, G. Huang, X. Yuan, C.-M. Pun, H. Liu,\nand W.-K. Ling,": "",
          "Joint Conf. on Neural Netw.\nplex image-to-image\ntranslation,”\nin Int.": "in Med. and Biol. Symp.\n(SPMB), 2021, pp. 1–7."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Demystifying the hypercomplex: Inductive biases in hypercomplex deep learning",
      "authors": [
        "D Comminiello",
        "E Grassucci",
        "D Mandic",
        "A Uncini"
      ],
      "year": "2024",
      "venue": "IEEE Signal Process. Mag"
    },
    {
      "citation_id": "2",
      "title": "Beyond fully-connected layers with quaternions: Parameterization of hypercomplex multiplications with 1/n parameters",
      "authors": [
        "A Zhang",
        "Y Tay",
        "S Zhang",
        "A Chan",
        "A Luu",
        "S Hui",
        "J Fu"
      ],
      "venue": "Int. Conf. on Mach. Learn. (ICML)"
    },
    {
      "citation_id": "3",
      "title": "PHNNs: Lightweight neural networks via parameterized hypercomplex convolutions",
      "authors": [
        "E Grassucci",
        "A Zhang",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Neural Netw. and Learn. Syst"
    },
    {
      "citation_id": "4",
      "title": "Efficient ECG-based atrial fibrillation detection via parameterised hypercomplex neural networks",
      "authors": [
        "L Basso",
        "Z Ren",
        "W Nejdl"
      ],
      "year": "2023",
      "venue": "European Signal Process. Conf. (EUSIPCO)"
    },
    {
      "citation_id": "5",
      "title": "Multiview hypercomplex learning for breast cancer screening",
      "authors": [
        "E Lopez",
        "E Grassucci",
        "M Valleriani",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "Multiview hypercomplex learning for breast cancer screening",
      "arxiv": "arXiv:2204.05798"
    },
    {
      "citation_id": "6",
      "title": "Hypercomplex multimodal emotion recognition from eeg and peripheral physiological signals",
      "authors": [
        "E Lopez",
        "E Chiarantano",
        "E Grassucci",
        "D Comminiello"
      ],
      "year": "2023",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process. Workshops (ICASSPW)"
    },
    {
      "citation_id": "7",
      "title": "Automatic emotion recognition using temporal multimodal deep learning",
      "authors": [
        "B Nakisa",
        "M Rastgoo",
        "A Rakotonirainy",
        "F Maire",
        "V Chandran"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "8",
      "title": "Multimodal emotion recognition based on manifold learning and convolution neural network",
      "authors": [
        "Y Zhang",
        "C Cheng",
        "Y Zhang"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Appl"
    },
    {
      "citation_id": "9",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2011",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "10",
      "title": "Quaternion-valued correlation learning for few-shot semantic segmentation",
      "authors": [
        "Z Zheng",
        "G Huang",
        "X Yuan",
        "C.-M Pun",
        "H Liu",
        "W.-K Ling"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Circuits and Syst. for Video Technol"
    },
    {
      "citation_id": "11",
      "title": "Fast quaternion product units for learning disentangled representations in SO(3)",
      "authors": [
        "S Qin",
        "X Zhang",
        "H Xu",
        "Y Xu"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Pattern Anal. and Mach. Intell"
    },
    {
      "citation_id": "12",
      "title": "Dual quaternion rotational and translational equivariance in 3D rigid motion modelling",
      "authors": [
        "G Vieira",
        "E Grassucci",
        "M Valle",
        "D Comminiello"
      ],
      "year": "2023",
      "venue": "IEEE Int. Workshop on Mach. Learn. for Signal Process"
    },
    {
      "citation_id": "13",
      "title": "Tessarine signal processing under the T-properness condition",
      "authors": [
        "J Navarro-Moreno",
        "R Fernández-Alcalá",
        "J Jiménez-López",
        "J Ruiz-Molina"
      ],
      "year": "2020",
      "venue": "Journal of the Franklin Institute"
    },
    {
      "citation_id": "14",
      "title": "A survey of complex-valued neural networks",
      "authors": [
        "J Bassey",
        "L Qian",
        "X Li"
      ],
      "year": "2021",
      "venue": "A survey of complex-valued neural networks",
      "arxiv": "arXiv:2101.12249"
    },
    {
      "citation_id": "15",
      "title": "Efficient sound event localization and detection in the quaternion domain",
      "authors": [
        "C Brignone",
        "G Mancini",
        "E Grassucci",
        "A Uncini",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "IEEE Trans. on Circuits and Syst. II: Express Briefs"
    },
    {
      "citation_id": "16",
      "title": "Dual quaternion ambisonics array for six-degree-of-freedom acoustic representation",
      "authors": [
        "E Grassucci",
        "G Mancini",
        "C Brignone",
        "A Uncini",
        "D Comminiello"
      ],
      "year": "2023",
      "venue": "Pattern Recognit. Letters"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition using quaternion convolutional neural networks",
      "authors": [
        "A Muppidi",
        "M Radfar"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "18",
      "title": "Learning speech emotion representations in the quaternion domain",
      "authors": [
        "E Guizzo",
        "T Weyde",
        "S Scardapane",
        "D Comminiello"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Process"
    },
    {
      "citation_id": "19",
      "title": "QCNN-H: Single-image dehazing using quaternion neural networks",
      "authors": [
        "V Frants",
        "S Agaian",
        "K Panetta"
      ],
      "year": "2023",
      "venue": "IEEE Trans. on Cybernetics"
    },
    {
      "citation_id": "20",
      "title": "Quaternion convolutional neural networks for heterogeneous image processing",
      "authors": [
        "T Parcollet",
        "M Morchid",
        "G Linarès"
      ],
      "year": "2019",
      "venue": "IEEE Int. Conf. on Acoust., Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Generalizing medical image representations via quaternion wavelet networks",
      "authors": [
        "L Sigillo",
        "E Grassucci",
        "A Uncini",
        "D Comminiello"
      ],
      "year": "2023",
      "venue": "Generalizing medical image representations via quaternion wavelet networks",
      "arxiv": "arXiv:2310.10224"
    },
    {
      "citation_id": "22",
      "title": "GROUSE: A task and model agnostic wavelet-driven framework for medical imaging",
      "authors": [
        "E Grassucci",
        "L Sigillo",
        "A Uncini",
        "D Comminiello"
      ],
      "year": "2023",
      "venue": "IEEE Signal Process. Letters"
    },
    {
      "citation_id": "23",
      "title": "Parameterized hypercomplex graph neural networks for graph classification",
      "authors": [
        "T Le",
        "M Bertolini",
        "F Noé",
        "D.-A Clevert"
      ],
      "year": "2021",
      "venue": "Int. Conf. on Artif. Neural Netw. (ICANN)"
    },
    {
      "citation_id": "24",
      "title": "Hypercom-plex image-to-image translation",
      "authors": [
        "E Grassucci",
        "L Sigillo",
        "A Uncini",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "Int. Joint Conf. on Neural Netw. (IJCNN)"
    },
    {
      "citation_id": "25",
      "title": "Compressing audio visual speech recognition models with parameterized hypercomplex layers",
      "authors": [
        "I Panagos",
        "G Sfikas",
        "C Nikou"
      ],
      "year": "2022",
      "venue": "Hellenic Conf. on Artif. Intell. (SETN)"
    },
    {
      "citation_id": "26",
      "title": "Shared-operation hypercomplex networks for handwritten text recognition",
      "authors": [
        "G Sfikas",
        "G Retsinas",
        "P Dimitrakopoulos",
        "B Gatos",
        "C Nikou"
      ],
      "year": "2023",
      "venue": "Int. Conf. on Document Anal. and Recogni"
    },
    {
      "citation_id": "27",
      "title": "Attention-map augmentation for hypercomplex breast cancer classification",
      "authors": [
        "E Lopez",
        "F Betello",
        "F Carmignani",
        "E Grassucci",
        "D Comminiello"
      ],
      "year": "2024",
      "venue": "Pattern Recognit. Letters"
    },
    {
      "citation_id": "28",
      "title": "PHYDI: Initializing parameterized hypercomplex neural networks as identity functions",
      "authors": [
        "M Mancanelli",
        "E Grassucci",
        "A Uncini",
        "D Comminiello"
      ],
      "year": "2023",
      "venue": "IEEE Int. Workshop on Mach. Learn. for Signal Process"
    },
    {
      "citation_id": "29",
      "title": "Towards explaining hypercomplex neural networks",
      "authors": [
        "E Lopez",
        "E Grassucci",
        "D Capriotti",
        "D Comminiello"
      ],
      "year": "2024",
      "venue": "Int. Joint Conf. on Neural Netw. (IJCNN)"
    },
    {
      "citation_id": "30",
      "title": "A machine learning model for emotion recognition from physiological signals",
      "authors": [
        "J Domínguez-Jiménez",
        "K Campo-Landines",
        "J Martínez-Santos",
        "E Delahoz",
        "S Contreras-Ortiz"
      ],
      "year": "2020",
      "venue": "Biomed. Signal Process. and Control"
    },
    {
      "citation_id": "31",
      "title": "An efficient machine learning-based emotional valence recognition approach towards wearable EEG",
      "authors": [
        "L Abdel-Hamid"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "32",
      "title": "An attention-based hybrid deep learning model for EEG emotion recognition",
      "authors": [
        "Y Zhang",
        "Y Zhang",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Signal, Image and Video Process"
    },
    {
      "citation_id": "33",
      "title": "Emotion recognition using heterogeneous convolutional neural networks combined with multimodal factorized bilinear pooling",
      "authors": [
        "Y Zhang",
        "C Cheng",
        "S Wang",
        "T Xia"
      ],
      "year": "2022",
      "venue": "Biomed. Signal Process. and Control"
    },
    {
      "citation_id": "34",
      "title": "CiABL: Completenessinduced adaptative broad learning for cross-subject emotion recognition with EEG and eye movement signals",
      "authors": [
        "X Gong",
        "C Chen",
        "B Hu",
        "T Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. on Affect. Comput"
    },
    {
      "citation_id": "35",
      "title": "Perceived mental workload classification using intermediate fusion multimodal deep learning",
      "authors": [
        "T Dolmans",
        "M Poel",
        "J.-W Van't Klooster",
        "B Veldkamp"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "36",
      "title": "Emotion recognition using a reduced set of EEG channels based on holographic feature maps",
      "authors": [
        "A Topic",
        "M Russo",
        "M Stella",
        "M Saric"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "37",
      "title": "Feature reconstruction based channel selection for emotion recognition using EEG",
      "authors": [
        "J Msonda",
        "Z He",
        "C Lu"
      ],
      "year": "2021",
      "venue": "IEEE Signal Process. in Med. and Biol. Symp. (SPMB)"
    }
  ]
}