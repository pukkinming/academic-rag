{
  "paper_id": "2402.00250v1",
  "title": "Lrdif: Diffusion Models For Under-Display Camera Emotion Recognition",
  "published": "2024-02-01T00:19:57Z",
  "authors": [
    "Zhifeng Wang",
    "Kaihao Zhang",
    "Ramesh Sankaranarayana"
  ],
  "keywords": [
    "under-display camera (UDC)",
    "emotion recognition",
    "diffusion model"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This study introduces LRDif, a novel diffusion-based framework designed specifically for facial expression recognition (FER) within the context of under-display cameras (UDC). To address the inherent challenges posed by UDC's image degradation, such as reduced sharpness and increased noise, LRDif employs a two-stage training strategy that integrates a condensed preliminary extraction network (FPEN) and an agile transformer network (UDCformer) to effectively identify emotion labels from UDC images. By harnessing the robust distribution mapping capabilities of Diffusion Models (DMs) and the spatial dependency modeling strength of transformers, LRDif effectively overcomes the obstacles of noise and distortion inherent in UDC environments. Comprehensive experiments on standard FER datasets including RAF-DB, KDEF, and FERPlus, LRDif demonstrate state-of-the-art performance, underscoring its potential in advancing FER applications. This work not only addresses a significant gap in the literature by tackling the UDC challenge in FER but also sets a new benchmark for future research in the field.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expression recognition (FER) has undergone remarkable advancements in recent years. However, implementing emotion recognition under UDC environment poses unique challenges. The fundamental challenge lies in the image quality and clarity. UDC images often suffer from reduced sharpness, increased noise, and color fidelity issues compared to those captured with traditional external cameras in Fig .  1 . These quality constraints stem from the fact that the camera lens is positioned beneath the screen, which can obstruct light in unpredictable ways. For emotion recognition algorithms, which rely heavily on the nuances of facial expressions, this can lead to decreased accuracy.\n\nMoreover, UDC images may exhibit unique artifacts and lighting inconsistencies, further complicating the task. Machine learning models used for emotion recognition need to be adapted for UDC images, ensuring they can effectively recognise emotional states despite the additional noise and distortions. Previous studies on FER  [1] [2] [3] [4] [5] [6] [7]  have not suffi-Fig.  1 : Top: shows an image taken with an under-display camera (UDC), which looks less clear compared to a regular camera. Bottom: shows an image taken with a traditional external camera, which is much clearer. ciently focused on the impacts of additional noise and distortions brought by UDC images. However, tackling this issue is essential for improving the real-world application of fullscreen devices.\n\nCurrently, several approaches address the noise learning problem in the emotion recognition field. RUL  [8]  addresses uncertainties due to ambiguous expressions and inconsistent labels by weighting facial features based on their relative difficulty, improving performance in noisy data environments. EAC  [9]  addresses noisy labels by utilizing flipped semantic consistency and selective erasure of input images, which prevents the model from overly focusing on specific features associated with noisy labels. SCN  [10]  mitigates uncertainties in large-scale datasets by employing a self-attention mechanism to weight training samples and a relabeling mechanism to adjust the labels of low-ranked samples, reducing overfitting to uncertain facial images. However, these methods encounter limitations when applied to UDC images. Specifically, RUL  [8]  and EAC  [9]  rely on the assumption of small losses, which can lead to confusion between difficult and noisy samples since both tend to exhibit high loss values during training. Therefore, learning features from noisy labels and images remains a challenging task.\n\nTo address this issue, this paper adopts a fresh perspective on noisy UDC images. Rather than the traditional approach of identifying noisy samples based on their loss values, this paper introduces an innovative view that focuses on learn-  ing from noisy labels through feature extraction. Our goal is to create a diffusion-based FER system that utilises the distribution mapping capabilities of diffusion models (DMs) to effectively restore noise labels and their corresponding images. For this purpose, we introduce LRDif. Considering the transformer's proficiency in capturing long-distance pixel dependencies, we choose transformer blocks as the foundational components of our LRDif architecture. Dynamic transformer blocks are layered in a U-Net style to construct the Under-Display Camera Transformer (UDCformer), aimed at multilevel feature extraction. The UDCformer comprises two parallel networks: the DTnetwork, which extracts latent features from label and UDC images at multiple levels, and the DILnetwork, which learns the similarities between facial landmarks and UDC images. LRDif follows a two-stage training process: (1) Initially, as shown in Fig.  2  (a), we construct a condensed preliminary extraction network (FPEN) that extracts an Emotion Prior Representation (EPR) from latent label and UDC images. This EPR is then utilized to guide the UDCformer in restoring labels. (2) Subsequently, shown in Fig.  2 (b ), the diffusion model (DM) is trained to directly infer the precise EPR from UDC images. Owing to the lightweight nature of EPR Z, the DM can achieve highly accurate EPR predictions, leading to significant improvements in test accuracy after a few iterations.\n\nThe main contributions of this work are summarized as follows: 1) We introduce a novel approach to address the challenges posed by under-display cameras (UDC) in facial expression recognition, offering a diffusion-based solution aimed at mitigating the effects of extra noise and image distortions. 2) LRDif focuses on the powerful mapping capabilities of DMs to deduce a concise emotion prior representation (EPR), enhancing both the accuracy and consistency of FER predictions. This method distinguishes itself from previous approaches by not relying on the knowledge of the dataset's uncertainty distribution. 3) Extensive experiments demonstrate that LRDif achieves state-of-the-art performance in emotion recognition tasks on three synthesized UDC-FER datasets and several standard FER datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Facial Expression Recognition. A typical FER system comprises three main phases: face detection, feature extraction, and expression classification. In the face detection phase, tools such as MTCNN  [11]  and Dlib  [12]  are used to identify faces, which may then be aligned for further processing.\n\nDuring the feature extraction phase, various techniques  [13]  are employed to capture the distinct facial features associated with different expressions. Wang et al.  [14]  utilize a spatial attention mechanism to concentrate on emotion-relevant areas of the image, thus enhancing accuracy in real-world conditions with a lightweight network that can be integrated into standard convolutional neural networks. MRAN  [15]  improves performance in uncontrolled conditions by applying spatial attention to both global and local facial features and using transformers to understand the relationships within and between these features.\n\nDiffusion Models. Diffusion Models are applied in various fields such as image editing  [16] , image restoration  [17, 18] , and high-resolution image synthesis  [19] . Rombach et al.  [20]  enhance the efficiency and visual fidelity of diffusion models by training them in the latent space of pre-trained autoencoders. They integrate cross-attention layers for versatile conditioning inputs, enabling high-resolution image synthesis with reduced computational demands while preserving detail. DiffusionDet  [21]  introduces an innovative object detection framework that conceptualizes the detection process as a denoising diffusion from random to precise object boxes. It learns to reverse the diffusion process from the ground truth during training and iteratively refines randomly generated boxes during inference.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pretrained Dtnetwork",
      "text": "There are two basic networks in the stage 1: the condensed preliminary extraction network (FPEN) and the agile transformer network (DTnetwork). Illustrated in the yellow box of Fig.  2 , FPEN includes several linear layers to extract the EPR. After that, the DTnetwork utilizes this EPR to recover labels. The DTnetwork's architecture, illustrated in the red box of Fig.  2 , comprises the Dynamic Multi-Head Transposed Attention (DMNet) and the Gated Feed-Forward Network (DGNet). During the pretraining phase, illustrated in Fig.  2 (a) , both FPEN S1 and the DTnetwork are trained together. We employ CLIP  [22]  text and image encoders to obtain latent features from labels and UDC images, which are then fed into FPEN S1 . The output of FPEN S1 is the EPR Z ∈ R C . This process is depicted in Equation (1):\n\nThen, Z is input into the DGNet and DMNet within the DTnetwork, serving as learnable parameters to aid in label recovery, as described in (Eq. (  2 )).\n\nwhere, LN refers to layer normalization, W is the weight of fully connected layer, • stands for element-wise multiplication.\n\nNext, in the DMNet, we extract information from the entire image. We convert the processed features, F ′ , into three new vectors called query Q, key K and value V by using convolution layer. Then, we reshape\n\n, and V as R H \" W \" ×C \" into new formats suitable for further calculations. We multiply Q and K, which helps our model understand which parts of the image to focus on, creating attention map A ∈ R C \" ×C \"\n\n. This whole step in DM-Net can be summarized as follows (Eq. (  3 )):\n\nwhere α is an adjustable parameter during training. Next, the DGNet learns local and adjacent features through aggregation. We use a very small Conv (1 × 1) to merge details from different layers and a slightly larger Conv (3 × 3) that looks at each layer separately to gather details from nearby pixels.\n\nIn addition, we use a special gate to ensure the capture of the most useful information. This entire step in DGNet can be summarised as (Eq. (  4 )):",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Dynamic Image And Landmarks Network (Dilnetwork)",
      "text": "In the DILnetwork, we employ a window-based crossattention mechanism to extract features from facial landmarks and UDC images. For UDC image features, represented as X udc ∈ R N ×D , we initially segment them into several distinct, non-overlapping segments x udc ∈ R M ×D . As for the facial landmarks features, denoted by X f lm ∈ R C×H×W , we scale them down to match the size of these segments, resulting in x f lm ∈ R c×h×w , where the dimension c is equal to D and the product of h and w equals M . This allows us to perform cross-attention between the facial landmarks and UDC image features using N attention heads, as detailed in (Eq. (  7 )).\n\nwhere w Q w K , w V , w O are weight matrix of features and b is the related position bias. This cross-attention is applied to each window within the UDC image, which refers as window-based multi-head crossattention (MHCA). The LRDif's transformer encoder is described by the following equations (Eq. (  9 )):\n\nWe need to merge the output features F from DTnetwork with the output features O from DILnetwork to get the fused multi-scale features x 1 , x 2 and x 3 , where\n\nThen, these fused features X will input into vanilla transformer blocks for further processing.\n\nwhere M SA is multi-head self-attention blocks. LN is layer normalization function.The training loss is defined as follows (Eq. (  13 )):\n\nWe use cross-entropy loss to train our model, with N representing the total number of samples and M the total number of classes. In this context, y ic denotes the presence or absence of class c as the correct label for sample i (1 if correct, 0 if not), while p ic represents the model's predicted probability that sample i falls into class c.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Diffusion Model For Label Restoration",
      "text": "In stage 2 (Fig.  2 (b )), we utilise the powerful capabilities of DM to estimate the Emotion Prior Representation. Initially, we utilize the pre-trained FPEN S1 to obtain the EPR Z ∈ R C . Subsequently, we apply the diffusion process to Z to generate a sample Z T ∈ R C , as detailed in Equation (  14 ):\n\nIn this context, T denotes the total number of diffusion steps, with α t = 1 -β t and ᾱT being the cumulative product of α i from 0 to T . The term β t refers to a predefined hyperparameter, and N (.) represents the standard Gaussian distribution.\n\nIn the reverse process of DM, we initially utilize the CLIP image encoder E I to encode the input UDC image x (Eq. (  15 )). Subsequently, the encoded features are input into FPEN S2 to abtain a conditional vector x S2 ∈ R C from the UDC images.\n\nwhere FPEN S2 shares similar network structures as FPEN S1 .\n\nOnly the first layer's dimension is different. The denoising network, denoted by ϵ θ , estimates the noise at each time step t. It takes as input the current noisy data Z ′ t , the time step t, and a conditional vector x S2 , derived from the UDC image through the second-stage condensed preliminary extraction network FPEN S2 . The estimated noise, given by ϵ θ (Concat(Z ′ t , t, x S2 )), is then used in the following equation   16 )) :\n\nAfter a series of T iterations, the final estimated Emotion Prior Representation (EPR), denoted by Z ′ 0 , is obtained. The condensed preliminary extraction network for stage two (FPEN S2 ), the denoising network, and the Under-Display Camera Transformer (UDCformer) are jointly trained using the total loss function L total (Eq. (  18 )).\n\nwhere Z norm (i) and Znorm (i) are EPRs extracted by LRDif S1 and LRDif S2 , respectively, both of which are normalized using the softmax operation. L kl is a variant of the Kullback-Leibler divergences, calculated across C dimensions. We combine the Kullback-Leibler divergence loss L kl (Eq. 17) and the Cross-Entropy loss L ce (Eq. 13) to compute the total loss L total (Eq. 18 ARM  [1]  90.42 DACL  [23]  83.52 DACL  [23]  88.61 POSTER++  [13]  92.21 POSTER++  [13]  86.46 POSTER++  [13]  94.44 RUL  [8]  88.98 RUL  [8]  85.00 RUL  [8]  87.83 DAN  [24]  89.70 DAN  [24]  85.48 DAN  [24]  88.77 SCN  [10]  87.03 SCN  [10]  83.11 SCN  [10]  89.55 EAC  [9]  90.35 EAC  [9]  86.18 EAC  [9]  72.32 MANet  [3]  88  UDC-RAF-DB dataset. Both datasets feature a nearly identical distribution of expressions. UDC-FERPlus dataset expands the UDC domain, offering a comprehensive set of 28,709 UDC images for training and 7,178 for testing. FERPlus  [26]  builds upon the FER2013 dataset, augmented with a new set of labels provided by ten annotators. This enhanced dataset comprises 28,709 images for training and 7,178 for testing.\n\nUDC-KDEF dataset includes a total of 4,900 UDC images, offering varied perspectives with images captured from five different angles. The training set comprises 3,920 UDC images, while the testing set includes 980 images. KDEF  [27]  represents a comprehensive dataset encompassing a collection of 4,900 images with same settings.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details",
      "text": "We use the SOTA MPGNet  [28]  to synthesize three benchmark UDC-FER datasets. The image degradation process contains brightness attenuation, blurring, and noise corruption. The experiments were conducted using PyTorch, and the models were trained on a GTX-3090 GPU. We utilized the Adam optimizer for training over 100 epochs. The training protocol involved a batch size of 64, a learning rate of 3.5 × 10 -4 , and a weight decay of 1 × 10 -4 .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With Sota Fer Methods",
      "text": "Comparison with Typical FER-model. Table  1  presents a comprehensive comparison of accuracy between the proposed method and current state-of-the-art (SOTA) FER algo-  rithms  [1, 3, [8] [9] [10] 13, 23, 24]  across three benchmark datasets: RAF-DB, FERPlus, and KDEF. For the RAF-DB dataset, the proposed method ('Ours') achieves an accuracy of 92.24%, surpassing several established algorithms such as ARM  [1] , RUL  [8] , DAN  [24] , SCN  [10] , EAC  [9] , and MANet  [3] , and is competitive with POSTER++  [13] , which scores slightly lower at 92.21%. In the FERPlus dataset, the proposed method also demonstrates superior performance with an accuracy of 87.13%, outperforming other notable methods like DACL  [23] , RUL  [8] , DAN  [24] , SCN  [10] , and MANet  [3] . Remarkably, in the KDEF dataset comparison, the proposed method achieves the highest accuracy of 95.75%, indicating a significant advancement over other methodologies, including the second-highest performing POSTER++  [13]  at 94.44% and the lower-scoring MANet  [3]  at 91.75%. Overall, this table underscores the robustness and effectiveness of the proposed method in facial expression recognition tasks, as evidenced by its leading accuracy on diverse datasets.\n\nComparison with the UDC FER-model. We conducted a performance evaluation of our proposed model for UDC systems, with some samples illustrated in Fig.  3 . Table  2 , Table  3  and Table  4  present a comparative analysis of ac- curacy among various state-of-the-art FER models on the UDC-RAF-DB, UDC-FERPlus, and UDC-KDEF datasets, respectively. Models such as ARM  [1] , RUL  [8] , DAN  [24] , SCN  [10] , EAC  [9] , and MANet  [3]  primarily employ the ResNet-18 architecture. An exception is POSTER++  [13] , which utilizes the Vision Transformer architecture, and ARM  [1] , which uses a modified ResNet-18 architecture termed ResNet18-ARM. Our proposed model deviates from the conventional ResNet-18 framework by integrating a 'Diffusion' backbone. This novel approach achieves an accuracy of 88.55%, 84.89% and 94.07% on three UDC datasets, indicating a significant improvement in performance compared to other methods. The results highlight the effectiveness of the diffusion-based model in the realm of UDC-based FER systems.\n\nFeature Visualization. The t-SNE technique was employed to visualize the feature distribution patterns learned by different models. Unlike in Fig.  4  (a) and (b), where the SCN model struggles to distinguish between emotion categories, particularly in UDC images, our LRDif model demonstrates effective recognition of expressions within mixed and noisy images. This suggests that LRDif successfully identifies the most distinctive features essential for differentiating various emotional expressions.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Ablation Study",
      "text": "In this section, we evaluate the impact of key components of LRDif such as the Diffusion Model (DM), the loss functions, and the incorporation of noise during training, as presented in Table  5 .  (1)  The comparison between LRDif S2 -V3 and LRDif S2 -V1 highlights the DM's robust capability for precise EPR prediction. (2) For LRDif S2 -V4, removing noise in the DM process is shown to enhance the precision of EPR estimations. (3) Furthermore, we evaluate the effectiveness of loss functions. We find that employing LRDif S2 -V2 with L ce (as per Eq. (  13 )) and LRDif S2 -V4 with L total (as per Eq. (  18 )), indicates that L ce contributes to enhanced accuracy.\n\nImpact of iteration numbers. This section explores the effect of the number of iterations within the Diffusion Model (DM) on the performance of LRDif S2 . We tested various iteration counts in LRDif S2 , adjusting the β t parameter (with α t = 1 -β t , as defined in Eq. 14) to ensure that the variable Z converges to Gaussian noise, Z T ∼ N (0, 1). As illustrated in Figs.  6  and 5 , LRDif S2 shows a significant performance improvement when the iteration count reaches 4. Beyond this point, further increases in the number of iterations do not markedly affect performance, indicating an optimal threshold has been reached. Notably, LRDifS2 achieves faster convergence compared to traditional DM approaches, which often require more than 50 iterations. This increased efficiency is attributed to the application of the DM to the EPR, a concise, one-dimensional vector.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Conclusion",
      "text": "This paper introduces LRDif, a novel diffusion-based framework for facial expression recognition in UDC environments. LRDif overcomes UDC image degradation through a twostage training strategy, integrating a preliminary extraction network (FPEN) and a transformer network (UDCformer). These modules enable effective recovery of emotion labels from degraded UDC images. Experimental results indicate that the proposed DRDif model demonstrates superior performance, setting a new benchmark in state-of-the-art results across three UDC facial expression datasets.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Top: shows an image taken with an under-display",
      "page": 1
    },
    {
      "caption": "Figure 2: The proposed LRDif framework, comprising UDCformer, FPEN, and a denoising network. LRDif has two distinct",
      "page": 2
    },
    {
      "caption": "Figure 2: (a), we construct",
      "page": 2
    },
    {
      "caption": "Figure 2: (b), the diffusion model (DM) is trained to directly infer",
      "page": 2
    },
    {
      "caption": "Figure 2: , FPEN includes several linear layers to extract the",
      "page": 3
    },
    {
      "caption": "Figure 2: , comprises the Dynamic Multi-Head Trans-",
      "page": 3
    },
    {
      "caption": "Figure 2: (a), both FPENS1 and the DTnetwork are trained to-",
      "page": 3
    },
    {
      "caption": "Figure 2: (b)), we utilise the powerful capabilities of",
      "page": 4
    },
    {
      "caption": "Figure 3: Samples from RAF-DB and UDC-RAF-DB datasets",
      "page": 4
    },
    {
      "caption": "Figure 4: The learned feature distribution by SCN and LRDif",
      "page": 5
    },
    {
      "caption": "Figure 3: . Table 2,",
      "page": 5
    },
    {
      "caption": "Figure 4: (a) and (b), where the SCN",
      "page": 6
    },
    {
      "caption": "Figure 5: t-SNE feature visualization on for DM trained on",
      "page": 6
    },
    {
      "caption": "Figure 6: Investigation of number of iterations in DM.",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Surprise, Fear, Disgust, Happy\nSad, Anger, Neutral\nInput\nConv\nDynamic F1 O1 MHCA\nTransformer Flatten C Q Q\nC Block K V K V +\nLN\nDownScale\nMLP\nInput +\nLinear D D M GN N e e t t Flatten F2 C O2 Inp O u u t tput\nConv\nLRelu DownScale MHCA\nLinear +\nD yn a m ic F3 O3 L N\nLRelu Tr a B n l s o f c o k rm er Flatten C K Q V K Q V M L P\n+\nOutput Under-Display Camera Transformer (UDCformer) Output\n(a) Stage 1: Pretrain Diffusion LRDif (LRDifS1)": "Surprise, Fear,\nDisgust, Happy Train\nSad, Anger, Neutral\nC\nPDDM\nTrain/Test\nUDCformer\nPDDM\n(b) Stage 2: Train Diffusion LRDif (LRDifS2) & Inference Reverse Denoising Process",
          "UDC Image Grou L n a d b e T l ruth\nLRelu\nPr L e a d b ic e te l d Leaky ReLU\nCLIP Label CLIP Image\nEncoder Encoder\nDownScale\nSca Im le a D ge own Lan F d a m cia a l rks\nC\nConcatenation Par L a o m c e k ters\nF C ai r c o i N a ss l e l t a a w n tt o d e r m n k t a io r n ks F a a tt c e e n t I i m on a g N e e C tw r o o r s k s\nDGNet MHCA\nWindow-based Multi-\nDynamic Gated head Cross Attention\nForward Network\nDMNet\nDynamic Multi-head\nAttention Network VIT Decoder\nLN MLP\nLayer Multilayer Perceptron\nNormalization\nPDDM\nEx F tr u a s c i t o io n n I L N P e r tw io o r rk Diffusion Model": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MHCA\n+\nLN\nMLP\n+\nOutput": "Input\nConv\nMHCA\n+\nLN\nMLP\n+\nOutput"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Learning to amend facial expression representation via de-albino and affinity",
      "authors": [
        "Jiawei Shi",
        "Songhao Zhu",
        "Zhiwei Liang"
      ],
      "year": "2021",
      "venue": "Learning to amend facial expression representation via de-albino and affinity",
      "arxiv": "arXiv:2103.10189"
    },
    {
      "citation_id": "3",
      "title": "Dive into ambiguity: Latent distribution mining and pairwise uncertainty estimation for facial expression recognition",
      "authors": [
        "Jiahui She",
        "Yibo Hu",
        "Hailin Shi",
        "Jun Wang",
        "Qiu Shen",
        "Tao Mei"
      ],
      "year": "2021",
      "venue": "CVPR"
    },
    {
      "citation_id": "4",
      "title": "Learning deep global multi-scale and local attention features for facial expression recognition in the wild",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu",
        "Shanmin Wang"
      ],
      "year": "2021",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "5",
      "title": "Facial expression recognition based on deep evolutional spatial-temporal networks",
      "authors": [
        "Kaihao Zhang",
        "Yongzhen Huang",
        "Yong Du",
        "Liang Wang"
      ],
      "year": "2017",
      "venue": "IEEE TIP"
    },
    {
      "citation_id": "6",
      "title": "Four-player groupgan for weak expression recognition via latent expression magnification",
      "authors": [
        "Wenjia Niu",
        "Kaihao Zhang",
        "Dongxu Li",
        "Wenhan Luo"
      ],
      "year": "2022",
      "venue": "Four-player groupgan for weak expression recognition via latent expression magnification"
    },
    {
      "citation_id": "7",
      "title": "Htnet for micro-expression recognition",
      "authors": [
        "Zhifeng Wang",
        "Kaihao Zhang",
        "Wenhan Luo",
        "Ramesh Sankaranarayana"
      ],
      "year": "2023",
      "venue": "Htnet for micro-expression recognition",
      "arxiv": "arXiv:2307.14637"
    },
    {
      "citation_id": "8",
      "title": "Facial smile detection based on deep learning features",
      "authors": [
        "Kaihao Zhang",
        "Yongzhen Huang",
        "Hong Wu",
        "Liang Wang"
      ],
      "year": "2015",
      "venue": "ACPR"
    },
    {
      "citation_id": "9",
      "title": "Relative uncertainty learning for facial expression recognition",
      "authors": [
        "Yuhang Zhang",
        "Chengrui Wang",
        "Weihong Deng"
      ],
      "year": "2021",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "10",
      "title": "Learn from all: Erasing attention consistency for noisy label facial expression recognition",
      "authors": [
        "Yuhang Zhang",
        "Chengrui Wang",
        "Xu Ling",
        "Weihong Deng"
      ],
      "year": "2022",
      "venue": "ECCV"
    },
    {
      "citation_id": "11",
      "title": "Suppressing uncertainties for large-scale facial expression recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Shijian Lu",
        "Yu Qiao"
      ],
      "year": "2020",
      "venue": "CVPR"
    },
    {
      "citation_id": "12",
      "title": "Sd-mtcnn: Self-distilled multi-task cnn",
      "authors": [
        "Ankit Jha",
        "Awanish Kumar",
        "Biplab Banerjee",
        "Vinay Namboodiri"
      ],
      "year": "2020",
      "venue": "Sd-mtcnn: Self-distilled multi-task cnn"
    },
    {
      "citation_id": "13",
      "title": "Shallow optical flow three-stream cnn for macro-and microexpression spotting from long videos",
      "authors": [
        "Gen-Bing Liong",
        "John See",
        "Lai-Kuan Wong"
      ],
      "year": "2021",
      "venue": "ICIP"
    },
    {
      "citation_id": "14",
      "title": "Poster v2: A simpler and stronger facial expression recognition network",
      "authors": [
        "Jiawei Mao",
        "Rui Xu",
        "Xuesong Yin",
        "Yuanqi Chang",
        "Binling Nie",
        "Aibin Huang"
      ],
      "year": "2023",
      "venue": "Poster v2: A simpler and stronger facial expression recognition network",
      "arxiv": "arXiv:2301.12149"
    },
    {
      "citation_id": "15",
      "title": "Light attention embedding for facial expression recognition",
      "authors": [
        "Cong Wang",
        "Jian Xue",
        "Ke Lu",
        "Yanfu Yan"
      ],
      "year": "2021",
      "venue": "IEEE TCSVT"
    },
    {
      "citation_id": "16",
      "title": "Multi-relations aware network for inthe-wild facial expression recognition",
      "authors": [
        "Dongliang Chen",
        "Guihua Wen",
        "Huihui Li",
        "Rui Chen",
        "Cheng Li"
      ],
      "year": "2023",
      "venue": "IEEE TCSVT"
    },
    {
      "citation_id": "17",
      "title": "Paint by example: Exemplar-based image editing with diffusion models",
      "authors": [
        "Binxin Yang",
        "Shuyang Gu",
        "Bo Zhang",
        "Ting Zhang",
        "Xuejin Chen",
        "Xiaoyan Sun",
        "Dong Chen",
        "Fang Wen"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "18",
      "title": "Lldiffusion: Learning degradation representations in diffusion models for low-light image enhancement",
      "authors": [
        "Tao Wang",
        "Kaihao Zhang",
        "Ziqian Shao",
        "Wenhan Luo",
        "Bjorn Stenger",
        "Tae-Kyun Kim",
        "Wei Liu",
        "Hongdong Li"
      ],
      "year": "2023",
      "venue": "Lldiffusion: Learning degradation representations in diffusion models for low-light image enhancement",
      "arxiv": "arXiv:2307.14659"
    },
    {
      "citation_id": "19",
      "title": "Towards real-world blind face restoration with generative diffusion prior",
      "authors": [
        "Xiaoxu Chen",
        "Jingfan Tan",
        "Tao Wang",
        "Kaihao Zhang",
        "Wenhan Luo",
        "Xiaocun Cao"
      ],
      "year": "2023",
      "venue": "Towards real-world blind face restoration with generative diffusion prior",
      "arxiv": "arXiv:2312.15736"
    },
    {
      "citation_id": "20",
      "title": "Styleswin: Transformer-based gan for high-resolution image generation",
      "authors": [
        "Bowen Zhang",
        "Shuyang Gu",
        "Bo Zhang",
        "Jianmin Bao",
        "Dong Chen",
        "Fang Wen",
        "Yong Wang",
        "Baining Guo"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "21",
      "title": "High-resolution image synthesis with latent diffusion models",
      "authors": [
        "Robin Rombach",
        "Andreas Blattmann",
        "Dominik Lorenz",
        "Patrick Esser",
        "Björn Ommer"
      ],
      "year": "2022",
      "venue": "CVPR"
    },
    {
      "citation_id": "22",
      "title": "Diffusiondet: Diffusion model for object detection",
      "authors": [
        "Shoufa Chen",
        "Peize Sun",
        "Yibing Song",
        "Ping Luo"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "23",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Chris Hallacy",
        "Aditya Ramesh",
        "Gabriel Goh",
        "Sandhini Agarwal",
        "Girish Sastry",
        "Amanda Askell",
        "Pamela Mishkin",
        "Jack Clark"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "24",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "Amir Hossein",
        "Xiaojun Qi"
      ],
      "year": "2021",
      "venue": "Facial expression recognition in the wild via deep attentive center loss"
    },
    {
      "citation_id": "25",
      "title": "Distract your attention: Multi-head cross attention network for facial expression recognition",
      "authors": [
        "Zhengyao Wen",
        "Wenzhong Lin",
        "Tao Wang",
        "Ge Xu"
      ],
      "year": "2023",
      "venue": "Biomimetics"
    },
    {
      "citation_id": "26",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "CVPR"
    },
    {
      "citation_id": "27",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "Training deep networks for facial expression recognition with crowd-sourced label distribution"
    },
    {
      "citation_id": "28",
      "title": "Latentofer: Detect, mask, and reconstruct with latent vectors for occluded facial expression recognition",
      "authors": [
        "Isack Lee",
        "Eungi Lee",
        "Bong Seok",
        "Yoo"
      ],
      "year": "2023",
      "venue": "CVPR"
    },
    {
      "citation_id": "29",
      "title": "Modular degradation simulation and restoration for under-display camera",
      "authors": [
        "Yang Zhou",
        "Yuda Song",
        "Xin Du"
      ],
      "year": "2022",
      "venue": "ACCV"
    }
  ]
}