{
  "paper_id": "2409.07388v2",
  "title": "Recent Trends Of Multimodal Affective Computing: A Survey From An Nlp Perspective",
  "published": "2024-09-11T16:24:06Z",
  "authors": [
    "Guimin Hu",
    "Yi Xin",
    "Weimin Lyu",
    "Haojian Huang",
    "Chang Sun",
    "Zhihong Zhu",
    "Lin Gui",
    "Ruichu Cai",
    "Erik Cambria",
    "Hasti Seifi"
  ],
  "keywords": [
    "Multimodal Affective Computing Multitask Learning ( §VI-A) MSA ( §VI-A1) Self-MM [136]",
    "ARGF [137]",
    "MultiSE [138]",
    "VCAN [139]",
    "DTN [140]",
    "MMMIE [90]",
    "MMIM [88]",
    "MISA [91]",
    "MERC ( §VI-A2) FacialMMT [25]",
    "MMMIE [90]",
    "AuxEmo [141]",
    "TDFNet [142]",
    "MALN [143]",
    "LGCCT [144]",
    "MultiEMO [84]",
    "RLEMO [145]",
    "MABSA ( §VI-A3) CMMT [146]",
    "AbCoRD [147]",
    "JML [148]",
    "MPT [37]",
    "MMRBN [85]",
    "MMER ( §VI-A4) AMP [95]",
    "MEGLN-LDA [149]",
    "MultiSE [138]",
    "AMP [95]. Pre-trained Model ( §VI-B) MSA ( §VI-B1) MAG-XLNet [22]",
    "UniMSE [38]",
    "AOBERT [150]",
    "SKESL [151]",
    "TEASAL [152]",
    "TO-BERT [153]",
    "SPT [154]",
    "ALMT [155] MERC ( §VI-B2) FacialMMT [25]",
    "QAP [20]",
    "UniMSE [38]",
    "GraphSmile [156]",
    "MABSA ( §VI-B3) MIMN [25]",
    "GMP [18]",
    "ERUP [157]",
    "VLP-MABSA [158]",
    "DR-BERT [159]",
    "DTCA [160]",
    "MSRA [161]",
    "AOF-ABSA [162]",
    "AD-GCFN [163]",
    "MOCOLNet [164]",
    "MMER ( §VI-B4) TAILOR [94]",
    "Enhanced Knowledge ( §VI-C) MSA ( §VI-C1) TETFN [165]",
    "ITP [19]",
    "SKEAFN [166]",
    "SAWFN [167]",
    "MTAG [107]",
    "MERC ( §VI-C2) ConSK-GCN [168]",
    "DMD [169]",
    "MRST [170]",
    "SF [171]",
    "TGMFN [172]",
    "RLEMO [145]",
    "DEAN [173]",
    "MABSA ( §VI-C3) KNIT [174]",
    "FITE [175]",
    "CoolNet [176]",
    "HIMT [177]",
    "MMER ( §VI-C4) UniVA-RoBERTa [178]",
    "CARAT [179]",
    "M3TR [180]",
    "MAGDRA [86]",
    "HHMPN [181]",
    "Contextual Information ( §VI-D) MSA ( §VI-D1) MuLT [77]",
    "CIA [182]",
    "CAT-LSTM [183]",
    "CAMFNet [184]",
    "MTAG [107]",
    "CTNet [185]",
    "ScaleVLAD [104]",
    "MMML [186]",
    "GFML [186]",
    "CHFusion [108]",
    "MERC ( §VI-D2) CMCF-SRNet [83]",
    "MMGCN [187]",
    "MM-DFN [188]",
    "SAMGN [189]",
    "M3Net [190]",
    "M3GAT [187]",
    "RL-EMO [145]",
    "SCMFN [191]",
    "EmoCaps [192]",
    "GA2MIF [193]",
    "MALN [143]",
    "COGMEN [49]",
    "MABSA ( §VI-D3) DTCA [160]",
    "MCPR [194]",
    "Elbphilharmonie [195]",
    "M2DF [196]",
    "AoM [197]",
    "FGSN [198]",
    "MIMN [16]",
    "MMER ( §VI-D4) MMS2S [199]",
    "MESGN [200]",
    "MDI [201]"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal affective computing has garnered increasing attention due to its broad applications in analyzing human behaviors and intentions, especially in text-dominated multimodal affective computing field. This survey presents the recent trends of multimodal affective computing from an NLP perspective through four hot tasks: multimodal sentiment analysis, multimodal emotion recognition in conversation, multimodal aspect-based sentiment analysis and multimodal multilabel emotion recognition. The goal of this survey is to explore the current landscape of multimodal affective research, identify development trends, and highlight the similarities and differences across various tasks, offering a comprehensive report on the recent progress in multimodal affective computing from an NLP perspective. This survey covers the formalization of tasks, provides an overview of relevant works, describes benchmark datasets, and details the evaluation metrics for each task. Additionally, it briefly discusses research in multimodal affective computing involving facial expressions, acoustic signals, physiological signals, and emotion causes. Additionally, we discuss the technical approaches, challenges, and future directions in multimodal affective computing. Finally, we release a repository that compiles related works in multimodal affective computing, providing detailed resources and references for the community 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Affective computing combines expertise in computer science, psychology, and cognitive science and its goal is to equip machines with the ability to recognize, interpret, and emulate human emotions  [1] -  [7] . Today, the world around us comprises various modalities-we perceive objects visually, hear sounds audibly, feel textures tangibly, smell odors olfactorily, and so forth. A modality refers to the way an experience is perceived or occurs, and is often associated with sensory modalities such as vision or touch, which are essential for communication and sensation. The significant advancements in multimodal learning across various fields  [8] ,  [9]  garnered increasing attention and accelerated the progress of multimodal affective computing. Multimodal affective computing ♠ University of Copenhagen, Denmark. email: rice.hu.x@gmail.com.\n\n† Nanjing University, China. In its early stages, researchers of affective computing predominantly focused on unimodal tasks, examining text-based, audio-based, and vision-based affective computing separately. For instance, D-MILN  [10]  is a textual sentiment classification model, while work  [11]  utilizes BiLSTM models trained on raw audio to predict the average sentiment of crowd responses. Today, sentiment analysis is widely employed across various modalities for applications such as market research, brand monitoring, customer service analysis, and social media monitoring. Recent advancements in multimedia technology  [12] -  [15]  have diversified the channels for information dissemination, with an influx of news, social media platforms like Weibo, and video content. These developments have integrated textual (spoken features), acoustic (rhythm, pitch), and visual (facial attributes) information to comprehensively analyze human emotions. For example, Xu et al.  [16]  introduced image modality data into traditional text-based aspect-level sentiment analysis, creating the new task of multimodal aspect-based sentiment analysis. Similarly, Wang et al.  [17]  extended textual emotion-cause pair extraction to a multimodal conversation setting, utilizing multimodal signals to enhance the model's ability to understand the emotions and their causes.\n\nMultimodal affective computing tasks closely related to several learning paradigms in machine learning, including transfer learning  [18] -  [20] , multimodal learning  [21] ,  [22] , muti-task learning  [23] -  [25]  and semantic understanding  [26] ,  [27] . Regarding transfer learning, it allows affective analysis models trained in one domain to be adapted for effective performance in different domains. By fine-tuning pre-trained models on limited data from the target domain, these models can be transferred to new domain, thereby enhancing their performance in multimodal affective computing tasks. In multimodal learning, cross-modal attention dynamically aligns and focuses on relevant information from different modalities, enhancing the model's ability to capture sentiment by highlighting key features and their interactions. In multitask learning, shared representations across affective computing tasks and modalities improve performance by capturing common sentiment-related features from text, audio, and video. More recently, the studies of multimodal learning have advanced the field by pre-training multimodal models on extensive multimodal datasets over years, further improving the performance on downstream tasks such as multimodal sentiment analysis  [28] -  [31] . With the scaling of the pretrained model, parameter-efficient transfer learning emerges such as adapter  [32] , prompt  [33] , instruction-tuning  [34]  and in-context learning  [35] ,  [36] .\n\nMore and more works of multimodal affective computing leverage these parameter-efficient transfer learning methods to transfer knowledge from pre-trained models (e.g., unimodal pre-trained model or multimodal pre-trained model) to downstream affective tasks to improve model performance by further fine-tuning the pre-trained model. For instance, Zou et al.  [37]  design a multimodal prompt Transformer (MPT) to perform cross-modal information fusion. UniMSE  [38]  proposes an adapter-based modal fusion method, which injects acoustic and visual signals into the T5 model to fuse them with multi-level textual information.\n\nMultimodal affective computing encompasses tasks like sentiment analysis, opinion mining, and emotion recognition using modalities such as text, audio, images, video, physiological signals, and haptic feedback. This survey focuses mainly on three key modalities: natural language, visual signals, and vocal signals. We highlight four main tasks in this survey: Multimodal Sentiment Analysis (MSA), Multimodal Emotion Recognition in Conversation (MERC), Multimodal Aspect-Based Sentiment Analysis (MABSA), and Multimodal Multilabel Emotion Recognition (MMER). A considerable volume of studies exists in the field of multimodal affective computing, and several reviews have been published  [15] ,  [39] -  [43] . However, these reviews primarily focus on specific affective computing tasks or specific single modality and overlook an overview of multimodal affective computing across multiple tasks, and the consistencies and differences among these tasks. The goal of this survey is twofold. First, this survey aims to provide a comprehensive overview of multimodal affective computing for beginners exploring deep learning in emotion analysis, detailing tasks, inputs, outputs, and relevant datasets. Second, it also offers insights for researchers to reflect on past developments, explore future trends, and examine technical approaches, challenges, and research directions in areas such as multimodal sentiment analysis and emotion recognition.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Organization Of This Survey",
      "text": "Section III outlines task formalization and application scenarios for multimodal affective tasks. Section IV introduces feature extraction methods and recent multimodal pre-trained models like (e.g., CLIP, BLIP, BLIP2). Section V analyzes multimodal affective works from the two perspectives: multimodal fusion and multimodal alignment, and shortly summarizes the parameter-efficient transfer methods used for further tuning the pre-trained model. Section VI reviews literature on MSA, MERC, MABSA, and MMER, focusing on multitask learning, pre-trained models, enhanced knowledge, and contextual information. Furthermore, Section VII summarizes multimodal datasets, and Section VIII covers evaluation metrics for each multimodal affective computing task. After the reviews of multimodal affective computing works. Section IX briefly reviews multimodal affective computing works based on facial expressions, acoustic signals, physiological signals, and emotion causes. It also highlights the consistency, differences, and recent trends of multimodal affective computing tasks from an NLP perspective. Section X looks ahead to future work from three aspects of the unification of multimodal affective computing tasks, the incorporation of external knowledge, and affective computing with less-studied modalities. Lastly, Section XI concludes this survey and its contribution to multimodal affective computing community.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Multimodal Affective Computing Tasks",
      "text": "In this section, we show the definition of each task and discuss their application scenarios. Table  I  presents basic information, including task input, output, type, and parent task, for each of the four tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Multimodal Sentiment Analysis",
      "text": "Multimodal sentiment analysis (MSA)  [44]  origins from sentiment analysis (SA) task  [45]  and it extends SA with the multimodal input. As a key research topics for computers to understand human behaviors, the goal of multimodal sentiment analysis (MSA) is to predict sentiment polarity and sentiment intensity based on multimodal signals  [46] . This task belongs to binary classification and regression task.\n\n1) Task Formalization: Given a multimodal signal I i = {I t i , I a i , I v i }, we use I m i , m ∈ {t, a, v} to represent unimodal raw sequence drawn from the video fragment i, where {t, a, v} denote the three types of modalities-text, acoustic and visual. Multimodal sentiment analysis aims to predict the real number y r i ∈ R, where y r i ∈ [-3, 3] reflects the sentiment strength. We feed I i as the model input and train a model to predict y r i . 2) Application Scenarios: We categorize multimodal sentiment analysis applications into key areas: social media monitoring, customer feedback, market research, content creation, healthcare, and product reviews. For example, analyzing sentiment in text, images, and videos on social media helps gauge public opinion and monitor brand perception, while analyzing multimedia product reviews can improve personalized recommendations and user satisfaction.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Multimodal Emotion Recognition In Conversation",
      "text": "Initially, MERC  [40] ,  [41] ,  [47]  extends from emotion recognition in a conversation (ERC) task  [48] ,  [49]  and it takes multimodal signals as the model inputs instead of signal modality. The goal of MERC is to automatically detect and monitor the emotional states of speakers in a dialogue using multimodal signals like text, audio and vision. In the community of multimodal emotion recognition in a conversation, MERC is a multi-class classification task and then categorizes the given utterance into one basic emotion from a pre-define emotion set.\n\n1) Task Formalization: Given a dialog for k number of utterances, it can be formulated as\n\ndenotes ith utterance of a conversation containing text (text transcript), audio (speech segment) and visual (video clip) modalities, denoted by {I i t , I i a , I i v }. We use Y as the label set of U and each utterance can be formalized as follows:   [54] ,  [55]  Here, y i indicates ith utterance's emotion category that is predefined before.\n\n2) Application Scenarios: Multimodal Emotion Recognition in Conversation (MERC) has broad applications across key areas: human-computer interaction, virtual assistants, healthcare, and customer service. (i) In Human-Computer Interaction, MERC enhances user experience by enabling systems to recognize and respond to emotional states, leading to more personalized interactions. (ii) Virtual Assistants and Chatbots benefit from improved emotional understanding, making conversations more natural and engaging. (iii) In Customer Service, MERC helps agents better respond to customer emotions, enhancing satisfaction. Additionally, biosensing systems measuring physiological signals like ECG, PPG, EEG, and GSR expand MERC applications in robotics, healthcare, and virtual reality.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Multimodal Aspect-Based Sentiment Analysis",
      "text": "Xu et al.  [50]  are among the first to put forward the new task, aspect based multimodal sentiment analysis. Multimodal aspect-based sentiment analysis (MABSA) is contructed based on aspect-based sentiment analysis in texts  [51] ,  [52] . In contrast with MSA and ERC, multimodal aspect-based sentiment analysis performs on fined granularity multimodal signals. MABSA receives texts and vision (image) modalitie as the inputs and outputs the tuple including aspect and its sentiment polarity. This task can be viewed as the classification, tuple extraction and triple extraction tasks. Recently, MABSA has attracted increasing attention. Given an image and corresponding text, MABSA is defined as jointly extracting all aspect terms from image-text pairs and predicting their sentiment polarities, i.e., positive, negative and neutral.\n\n1) Task Formalization: Suppose the multimodal inputs include a textual content T = {w 1 , w 2 , ..., w L } and an image set I = {I 1 , I 2 , • • • , I K }, the goal of MABSA is to predict the sentiment polarities with a given aspect phrase A = {a 1 , a 2 , • • • , a N }, where a i denotes the ith aspect (e.g., food), L is the length of textual context, K is the number of images, and N is the length of aspect phrase.\n\n2) Application Scenarios: Multimodal aspect-based sentiment analysis (MABSA) focuses on improving products and services by analyzing reviews across text, images, and videos to identify customer opinions on specific aspects. For example, MABSA can assess dining experiences, like food quality or service, to enhance restaurant operations. It also applies to social media, where analyzing mixed content provides deeper insights into public opinion, aiding better decision-making and marketing strategies.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Multimodal Multi-Label Emotion Recognition",
      "text": "Multimodal signals may show more than one emotion label, which boosts the rise of a new task: multimodal multi-label emotion recognition (MMER). MMER inherits the characteristic of multimodal emotion recognition and multi-label classification  [54] ,  [55] . MMER is developed from multi-label emotion recognition, which predicts two or more basic emotion categories to analysis the given multimodal information and it is a multi-label multi-class classification.\n\n1) Task Formalization: Given a multimodal signal I i = {I t i , I a i , I v i }, I i contains three types of modalities-text, audio and visual. Formally, we use I m i ∈ R dm×lm , m ∈ {t, a, v} to represent the raw sequence of text, audio, and visual modalities from the sample i. d m and l m denote the feature dimension and sequence length of modality m. The goal of MMER is to recognize at least one emotion categories from |L| predefined label space Y = {y 1 , y 2 , • • • , y |L| } according to the multimodal signal I i .\n\n2) Application Scenarios: Multimodal multi-label emotion recognition seeks to create AI systems that can understand and categorize emotions expressed through various modalities simultaneously. This task is challenging due to the complexity and variability of human emotions, differences in emotional expression across individuals and cultures, and the need for effective integration of diverse modalities.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. Modal Feature Extractor",
      "text": "For multimodal affective computing tasks, the model input typically includes at least two modalities. In this section, we introduce the common feature extractors that transform raw sequences into a feature vectors.\n\na) Text Feature Extractor: For text modality, researchers adopt static word embedding methods like Word2Vec  [56]  and GloVec  [57]  to initialize word representation. Also, text modality can be encoded into feature vector through pretrained language models like BERT  [58] , BART  [59] , and T5  [60]  to extract the text representation. More recently, a collection of foundation language models like LLaMA  [61] ,  [62] , Mamba  [63]  emerge and are used for encoding text modality.\n\nb) Audio Feature Extractor: For audio modality, raw acoustic input needs to be processed into numerical sequential vectors. The common way is to use librosa 2 to extract Melspectrogram as audio features. It is the short-term power spectrum of sound and is widely used in modern audio processing. Transformer structure has achieved tremendous success of in the field of NLP and computer vision. Gong et al.  [64]  propose audio spectrogram Transformer (AST), which converted waveform into a sequence of 128-dimensional log Mel filterbank (fbank) features to encode audio modality. c) Vision Feature Extractor: For image modality, researchers can extract fixed T frames from each segment and use effecientNet  [65]  pre-trained (supervised) on VG-Gface 3 and AFEW dataset as vision initial representation. Furthermore, Dosovitskiy et al.  [66]  propose to use standard Transformer directly to images, which split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. CLIP  [67]  jointly trained image and its caption with the contrastive learning, thereby extraction vision features that correspond to texts.\n\nd) Multimodal Feature Extractor: The emergence of multimodal pre-trained model (MPM) marks a significant advancement in integrating multimodal signals, as demonstrated by groundbreaking developments like GPT-4  [68]  and Gemini  [69] . Among the open-source innovations, Flamingo  [70]  represents an early effort to integrate visual features with LLMs using cross-attention layers. BLIP-2  [71]  introduces a trainable adaptor module (Q-Former) that efficiently connects a pre-trained image encoder with a pre-trained LLM, ensuring precise alignment of visual and textual information. Similarly, MiniGPT-4  [72]  achieves visual and textual alignment through a linear projection layer. InstructBLIP  [73]  advances the field by focusing on vision-language instruction tuning, building upon BLIP-2, and requiring a deeper understanding and larger datasets for effective training. LLaVA  [74]  integrates CLIP's image encoder with LLaMA's language decoder to enhance instruction tuning capabilities. Akbari et al.  [31]  train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Based on multimodal pre-trained model, raw modal signals can be used to extract modal features.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Multimodal Learning On Multimodal Affective Computing",
      "text": "Multimodal learning involves learning representations from different modalities. Generally, the multimodal model should first align the modalities based on their semantics before fusing multimodal signals. After alignment, the model combines multiple modalities into one representation vector.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Preliminary",
      "text": "With the scaling of the pre-trained model, parameterefficient transfer learning emerges such as adapter  [32] , prompt  [33] , instruction-tuning  [34]  and in-context learning  [35] ,  [36] . In this paradigm, instead of adapting pretrained LMs to downstream tasks via objective engineering, downstream tasks are reformulated to look more like those solved during the original LM training with the help of prompt, instruction-tuning and in-context learning. The use of prompts in Vision Language Models (VLMs) like GPT-4V [68] and 3 https://www.robots.ox.ac.uk/ vgg/software/vgg face/.\n\nFlamingo  [70]  allows the models to interpret and generate outputs based on combined visual and textual inputs. In contrast with prompt, instruction-tuning belongs to the learning paradigm of prompt. Also, models like InstructBLIP  [73]  and FLAN  [75]  have demonstrated that instruction-tuning not only improves the model's adherence to instructions but also enhances its ability to generalize across tasks. In the community of multimodal affective computing, researchers can leverage these parameter-efficient transfer learning methods (e.g., adapter, prompt and instruction tuning) to transfer knowledge from pre-trained models (e.g., unimodal pre-trained model or multimodal pre-trained model) to downstream affective tasks, further tune the pre-trained model with the affective dataset.\n\nConsidering that multimodal affective computing involves multimodal learning, therefore, we analyze multimodal affective computing works from multimodal fusion and multimodal alignment, as shown in Fig.  1 .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "B. Multimodal Fusion",
      "text": "Multimodal signals are heterogeneous and derived from various information sources, making integrating multimodal signals into one representation essential. Tasi et al.  [77]  summarize multimodal fusion into early, late or intermediate fusion based on the fusion stage. Early fusion combines features from different modalities at the input level before the model processes them. Late fusion processes features from different modalities separately through individual sub-networks, and the outputs of these sub-networks are combined at a later stage, typically just before making the final decision. Late fusion uses unimodal decision values and combines them using mechanisms such as averaging  [124] , voting schemes  [125] , weighting based on channel noise  [126]  and signal variance  [127] , or a learned model  [6] ,  [128] . The two fusion strategies face some problems. For example, early fusion at the feature level can underrate intra-modal dynamics after the fusion operation, while late fusion at the decision level may struggle to capture inter-modal dynamics before the fusion operation. Different from the previous two methods by combining features from different modalities at intermediate layers of the model learner, Intermediate fusion allows for more interaction between the modalities at different processing stages, potentially leading to richer representations  [38] ,  [129] ,  [130] . Based on these fusion strategies, we review multimodal fusion from three aspects: cross-modality learning, modal consistency and difference, and multi-stage modal fusion. Fig.  2  illustrates the three aspects of modal fusion.\n\n1) Cross-modality Learning: Cross-modality learning focuses on the incorporation of inter-modality dependencies and interactions for better modal fusion in representation learning. Early works of multimodal fusion  [76]  mainly operate geometric manipulation in the feature spaces to fuse multiple modalities. The recent common way of cross-modality learning is to introduce attention-based learning method to model inter-modality and intra-modality interactions. For example, MuLT  [77]  proposes multimodal Transformer to learn intermodal interaction. Chen et al.  [78]  augment the inter-intra modal features with trimodal collaborative interaction and Multimodal Learning on Affective Computing",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Multimodal Fusion ( §V-B)",
      "text": "Cross-modal Learning TFN  [76] , MuLT  [77] , TCDN  [78] , CM-BERT  [79] , HGraph-CL  [80] , BAFN  [81] , TeFNA  [82] , CMCF-SRNet  [83] , MultiEMO  [84] , MM-RBN  [85] , MAGDRA  [86] , AMuSE  [87] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Modal Consistency And Difference",
      "text": "MMIM  [88] , MPT  [89] , MMMIE  [90] , MISA  [91] , CoolNet  [92] , ModalNet  [93] , MAN  [87] , TAILOR  [94] , AMP  [95] , STCN  [96] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Multi-Stage Modal Fusion",
      "text": "TSCL-FHFN  [97] , HFFN  [98] , CLMLF  [99] , RMFN  [100] , CTFN  [101] , MCM  [102] , FmlMSN  [103] , ScaleVLAD  [104] , MUG  [105] , HFCE  [106] , MTAG  [107] , CHFusion  [108] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Multimodal Alignment ( §V-C)",
      "text": "Miss Modality MMIN  [109] , CMAL  [110] , M2R2  [111] , EMMR  [112] , TFR-Net  [113] , MRAN  [114] , VIGAN  [115] , TATE  [116] , IF-MMIN  [117] , CTFN  [101] , MTMSA  [118] , FGR  [119] , MMTE+AMMTD  [120] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Semantic Alignment",
      "text": "MuLT  [77] , ScaleVLAD  [104] , Robust-MSA  [121] , HGraph-CL  [80] , SPIM  [122] , MA-CMU-SGRNet  [123] . unifies the characteristics of the three modals (inter-modal).\n\nYang et al.  [79]  propose the cross-modal BERT (CM-BERT), aiming to model the interaction of text and audio modality based on pre-trained BERT model. Lin et al.  [80]  explore the intricate relations of intra-and inter-modal representations for sentiment extraction. More recently, Tang et al.  [81]  propose the multimodal dynamic enhanced block to capture the intramodality sentiment context, which decrease the intra-modality redundancy of auxiliary modalities. Huang et al.  [82]  propose a Text-centered fusion network with cross-modal attention (TeFNA), a multimodal fusion network that uses crossmodal attention to model unaligned multimodal timing information.\n\nIn the community of emotion recognition, CMCF-SRNet  [83]  is a cross-modality context fusion and semantic refinement network, which contains a cross-modal locality-constrained transformer and a graph-based semantic refinement transformer, aiming to explore the multimodal interaction and dependencies among utterances. Shi et al.  [84]  propose an attention-based correlation-aware multimodal fusion framework MultiEMO, which captures cross-modal mapping relationships across textual, audio and visual modalities based on bidirectional multi-head cross attention layers. In summary, cross-modality learning mainly focuses on modeling the relation between modalities.\n\n2) Modal Consistency and Difference: Modal consistency refers to the shared feature space across different modalities for the same sample, while modal difference highlights the unique information each modality provides. Most multimodal fusion approaches separate representations into modalinvariant (consistency) and modal-specific (difference) components. Modal consistency helps handle missing modalities, while modal difference leverages complementary information from each modality to improve overall data understanding. For example, several works  [89] ,  [90]  have explored learning modal consistency and difference using contrastive learning. Han et al.  [88]  maximized the mutual information between modalities and between each modality to explore the modal consistency. Another study  [89]  proposes a hybrid contrastive learning framework that performs intra-/inter-modal contrastive learning and semi-contrastive learning simultaneously, models cross-modal interactions, preserves inter-class relationships, and reduces the modality gap. Additionally, Zheng et al.  [90]  combined mutual information maximization between modal pairs with mutual information minimization between input data and corresponding features. This method aims to extract modal-invariant and task-related information. Modal consistency can also be viewed as the process of projecting multiple modalities into a common latent space (modality-invariant representation), while modal difference refers to projecting modalities into modality-specific representation spaces. For example, Hazarika et al.  [91]  propose a method that projects each modality into both a modalityinvariant and a modality-specific space. They implemented a decoder to reconstruct the original modal representation using both modality-invariant and modality-specific features. AMuSE  [87]  proposes a multimodal attention network to capture cross-modal interactions at various levels of spatial abstraction by jointly learning its interactive bunch of modespecific peripheral and central networks. For the fine-grain sentiment analysis, Xiao et al.  [92]  present CoolNet to boost the performance of visual-language models in seamlessly integrating vision and language information. Zhang et al.  [93]   propose an aspect-level sentiment classification model by exploring modal consistency with fusion discriminant attention network.\n\n3) Multi-stage Modal Fusion: Multi-stage multimodal fusion  [131] ,  [132]  refers to combine modal information extracted from multiple stages or multiple scales to fuse modal representation. Li et al.  [97]  design a two-stage contrastive learning task, which learns similar features for data with the same emotion category and learns distinguishable features for data with different emotion categories. HFFN  [98]  divides the procession of multimodal fusion into divide, conquer and combine, which learns local interactions at each local chunk and explores global interactions by conveying information across local interactions. Different from the work of HFFN, Li et al.  [99]  align and fused the token-level features of text and image and designed label based contrastive learning and data based contrastive learning to capture common features related to sentiment in multimodal data. There are some work  [100]  decomposed the fusion procession into multiple stages, each of them focused on a subset of multimodal signals for specialized, effective fusion. Also, CTFN  [108]  presents a novel feature fusion strategy that proceeds in a hierarchical fashion, first fusing the modalities two in two and only then fusing all three modalities. Moreover, the modal fusion at multiple levels has made progress, such as Li et al.  [102]  propose a multimodal sentiment analysis method based on multi-level correlation mining and self-supervised multi-task learning, Peng et al.  [103]  propose a fine-grained modal label-based multi-stage network (FmlMSN), which utilize seven sentiment labels in unimodal, bimodal and trimodal information at different granularities from text, audio, image and the combinations of them. Researchers generally focus on the scale-level modal alignment and modal fusion before model' decision. Sharafi et al.  [96]  design a new fusion method was proposed for multimodal emotion recognition utilizing different scales.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Multimodal Alignment",
      "text": "Multimodal alignment involves synchronizing modal semantics before fusing multimodal data. A key challenge is handling missing modalities, which can occur due to issues like a camera being turned off, a user being silent, or device errors affecting both voice and text. Since the assumption of always having all modalities is often unrealistic, multimodal alignment must address these gaps. Additionally, it involves aligning objects across images, text, and audio through semantic alignment. Thus, we discuss multimodal alignment in terms of managing missing modalities and achieving semantic alignment. Fig.  3  illustrates the multimodal alignment.\n\n1) Alignment for Missing Modality: In real-world scenarios, data collection can sometimes result in the simultaneous loss of certain modalities due to unforeseen events. While multimodal affective computing typically assumes the availability of all modalities, this assumption often fails in practice, which can cause issues in modal fusion and alignment models when some modalities are missing. We classify existing methods for handling missing modalities into four groups.\n\nThe first group features the data augmentation approach, which randomly ablates the inputs to mimic missing modality cases. Parthasarathy et al.  [110]  propose a strategy to randomly ablate visual inputs during training at the clip or frame level to mimic real world scenarios. Wang et al.  [111]  deal with the utterance-level modalities missing problem by training emotion recognition model with iterative data augmentation by learned common representation. The second group is based on generative methods to directly predict the missing modalities given the available modalities  [133] . For example, Zhao et al.  [109]  propose a missing modality imagination network (MMIN), which can predict the representation of any missing modality given available modalities under different missing modality conditions, so as to to deal with the uncertain missing modality problem. Zeng et al.  [112]  propose an ensemblebased missing modality reconstruction (EMMR) network to detect and recover semantic features of the key missing modality. Yuan et al.  [113]  propose a transformer-based feature reconstruction network (TFR-Net), which improves the robustness of models for the random missing in non-aligned modality sequences. Luo et al.  [114]  propose the multimodal reconstruction and align net (MRAN) to tackle the missing modality problem, especially to relieve the decline caused by the text modality's absence.\n\nThe third group aims to learn the joint multimodal representations that can contain related information from these modalities  [134] . For example, Ma et al.  [135]  propose a unified deep learning framework to efficiently handle missing labels and missing modalities for audio-visual emotion recognition through correlation analysis. Zeng et al.  [116]  propose a tag-assisted Transformer encoder (TATE) network to handle the problem of missing uncertain modalities, which designs a tag encoding module to cover both the single modality and multiple modalities missing cases, so as to guide the network's attention to those missing modalities. Zuo et al.  [117]  propose to use invariant features for a missing modality imagination network (IF-MMIN), which includes an invariant feature learning strategy and an invariant feature based imagination module (IF-IM). Through the two strategies, IF-MMIN can alleviate the modality gap during the missing modalities prediction, thus improving the robustness of multimodal joint representation. Zhou et al.  [119]  propose a novel brain tumor segmentation network in the case of missing one or more modalities. The proposed network consists of three sub-networks: a feature-enhanced generator, a correlation constraint block and a segmentation network. The last group is translation-base methods. Tang et al.  [101]  propose the coupled-translation fusion network (CTFN) to model bi-direction interplay via couple learning, ensuring the robustness in respect to missing modalities. Liu et al.  [118]  propose a modality translationbased MSA model (MTMSA), which is robust to uncertain missing modalities. In summary, the works about alignment for miss modality focus on miss modality reconstruction and learning based on the available modal information.\n\n2) Alignment for Cross-modal Semantics: Semantic alignment aims to find the connection between multiple modalities in one sample, which refers to searching one modal information through another modal information and vice versa. In the filed of MSA, Tsai et al.  [77]  leverage cross-modality and multi-scale modal alignment to implement the modal consistency in the semantic aspects, respectively. ScaleVLAD  [202]  proposes a fusion model to gather multi-Scale representation from text, video, and audio with shared vectors of locally aggregated descriptors to improve unaligned multimodal sentiment analysis. Yang et al.  [107]  convert unaligned multimodal sequence data into a graph with heterogeneous nodes and edges that captures the rich interactions across modalities and through time. Lee et al.  [203]  segment the audio and the underlying text signals into equal number of steps in an aligned way so that the same time steps of the sequential signals cover the same time span in the signals. Zong et al.  [204]  exploit multiple bi-direction translations, leading to double multimodal fusing embeddings compared with traditional translation methods. Wang et al.  [205]  propose a multimodal encoding-decoding translation network with a transformer and adopted a joint encoding-decoding method with text as the primary information and sound and image as the secondary information. Zhang et al.  [123]  propose a novel multi-level alignment to bridge the gap between acoustic and lexical modalities, which can effectively contrast both the instance-level and prototype-level relationships, separating the multimodal features in the latent space. Yu et al.  [206]  propose an unsupervised approach which minimizes the Wasserstein distance between both modalities, forcing both encoders to produce more appropriate representations for the final extraction to align text and image.\n\nLai et al.  [122]  propose a deep modal shared information learning module based on the covariance matrix to capture the shared information between modalities. Additionally, we use a label generation module based on a self-supervised learning strategy to capture the private information of the modalities. Our module is plug-and-play in multimodal tasks, and by changing the parameterization, it can adjust the information exchange relationship between the modes and learn the private or shared information between the specified modes. We also employ a multi-task learning strategy to help the model focus its attention on the modal differentiation training data. For model robustness, Robust-MSA  [121]  present an interactive platform that visualizes the impact of modality noise to help researchers improve model capacity.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Models Across Multimodal Affective Computing",
      "text": "In the community of multimodal affective computing, the works appear to significant consistency in term of development technical route. For clarity, we group the these works based on multitask learning, pre-trained model, enhanced knowledge, contextual information. Meanwhile, we briefly summarized the advancements of MSA, MERC, MABSA and MMER tasks through the above four aspects. Fig.  4  summarizes the typical works of multimodal affective computing from these aspects and Table  II  shows the taxonomy of multimodal affective computing.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Multitask Learning",
      "text": "Multitask learning trains a model on multiple related tasks simultaneously, using shared information to enhance performance. The loss function combines losses from all tasks, with model parameters updated via gradient descent. In multimodal affective computing, multitask learning helps distinguish between modal-invariant and modal-specific features and integrates emotion-related sub-tasks into a unified framework. Fig.  5  shows the learning paradigm of multitask learning in multimodal affective learning task.\n\n1) Multimodal Sentiment Analysis: In filed of multimodal sentiment analysis, Self-MM  [136]  generates a pseudolabel  [207] -  [209]  for single modality and then jointly train unimodal and multimodal representations based on the generated and original labels. Furthermore, a translation framework ARGF between modalities, i.e., translating from one modality to another is used as an auxiliary task to regualize the multimodal representation learning  [137] . Akhtar et al.  [138]  leverage the interdependence of the tasks sentiment and emotion to improve the model performance on two tasks. Chen et al.  [139]  propose a video-based cross-modal auxiliary network (VCAN), which is comprised of an audio features map module and a cross-modal selection module to make use of auxiliary information. Zheng et al.  [140]  propose a disentanglement translation network (DTN) with slack reconstruction to capture desirable information properties, obtain a unified feature distribution and reduce redundancy. Zheng et al.  [90]  combine mutual information maximization (MMMIE) between modal pairs with mutual information minimization between input data and corresponding features, jointly extract modalinvariant and task-related information in a single architecture.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "2) Multimodal Emotion Recognition In Conversation:",
      "text": "In community of multimodal emotion recognition, Zheng et al.  [25]  propose a two-stage framework named Facial expression-aware multimodal multi-task learning (Fa-cialMMT), which jointly trains multimodal face recognition, unsupervised face clustering, and face matching in a unified architecture, so as to leverages the frame-level facial emotion distributions to help improve utterance-level emotion recognition based on multi-task learning. Zhang et al.  [210]  design two kinds of multitask learning (MTL) decoders, i.e., singlelevel and multi-level decoders, to explore their potential. More specifically, the core of a single-level decoder is a masked outer-modal self-attention mechanism. Sun et al.  [141]  design two auxiliary tasks to alleviate the insufficient fusion between modalities and guide the network to capture and align emotionrelated features. Zhao et al.  [142]  propose a transformerbased deep-scale fusion network (TDFNet) for multimodal emotion recognition, solving the aforementioned problems. The multimodal embedding (ME) module in TDFNet uses pre-trained models to alleviate the data scarcity problem by providing a prior knowledge of multimodal information to the model with the help of a large amount of unlabeled data. Ren et al.  [143]  propose a novel multimodal adversarial learning network (MALN), which first mines the speaker's characteristics from context sequences and then incorporate them with the unimodal features. Liu et al.  [144]  propose LGCCT, a light gated and crossed complementation transformer for multimodal speech emotion recognition.\n\n3) Multimodal Aspect-based Sentiment Analysis: Yang et al.  [146]  propose a multi-task learning framework named cross-modal multitask Transformer (CMMT), which incor-porates two auxiliary tasks to learn the aspect/sentimentaware intra-modal representations and introduces a text-guided cross-modal interaction module to dynamically control the contributions of the visual information to the representation of each word in the inter-modal interaction. Jain et al.  [147]  propose a hierarchical multimodal generative approach (Ab-CoRD) for aspect-based complaint and rationale detection that reframes the multitasking problem as a multimodal text-to-text generation task. Ju et al.  [148]  is the first to jointly perform multimodal ATE (MATE) and multimodal ASC (MASC), and propose a joint framework JML with auxiliary crossmodal relation detection for multimodal aspect-level sentiment analysis (MALSA) to control the proper exploitation of visual information. Zou et al.  [37]  design a multimodal prompt Transformer (MPT) to perform cross-modal information fusion. Meanwhile, this work used the hybrid contrastive learning (HCL) strategy to optimize the model's ability to handle labels with few samples. Chen et al.  [85]  design that audio module should be more expressive than the text module, and the single-modality emotional representation should be dynamically fused into the multimodal emotion representation, and proposes corresponding rule-based multimodal multi-task network (MMRBN) to restrict the representation learning.\n\n4) Multimodal Multi-label Emotion Recognition: For multimodal multi-label emotion recognition, Ge et al.  [95]  design adversarial temporal masking strategy and adversarial parameter perturbation strategy to jointly enhance the encoding of other modalities and generalization of the model respectively. MER-MULTI  [149]  is label distribution adaptation which adapts the label distribution between the training set and testing set to remove training samples that do not match the features of the testing set. Akhtar et al.  [211]  present a deep multi-task learning framework that jointly performs sentiment and emotion analysis both, which leverage the interdependence of two related tasks (i.e. sentiment and emotion) in improving each others performance using an effective multimodal framework.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "B. Pre-Trained Model",
      "text": "In recent years, large language model (LLM)  [59] ,  [212]  and multimodal pre-trained model  [22] ,  [27] ,  [213] ,  [214]  has achieved significant progress  [26] ,  [212] ,  [215] . Compared with non-pretrained model, pre-trained model contains massive transferred knowledge  [28] ,  [32] , which can be introduced into multimodal representation learning to probe the richer information. Fig.  6  shows the use of pre-trained model in multimodal affective learning task.\n\n1) Multimodal Sentiment Analysis: In filed of multimodal sentiment analysis, Rahman et al.  [22]  propose an attachment to pre-trained model BERT and XLNet called multimodal adaptation gate (MAG), which allows BERT and XL-Net to accept multimodal nonverbal data by generating a shift that is conditioned on the visual and acoustic modalities to internal representation of BERT and XLNet. UniMSE  [38]  is a unified sentiment-sharing framework based on T5 model  [60] , which injects the non-verbal signals into pre-trained Transformerbased model for probing the knowledge stored in LLM. AOBERT  [150]  introduces a single-stream transformer structure, which integrates all modalities into one BERT model. Qian et al.  [151]  embed sentiment information at the word level into pre-trained multimodal representation to facilitate further learning on limited labeled data. TEASAL  [152]  is a Transformer-Based speech-prefixed language model, which exploits a conventional pre-trained language model as a crossmodal Transformer encoder. Yu et al.  [153]  study targetoriented multimodal sentiment classification (TMSC) and propose a multimodal BERT architecture for multimodal sentiment analysis task. Cheng et al.  [154]  set layer-wise parameter sharing and factorized co-attention that share parameters between cross attention blocks, so as to allow multimodal signal to interact within every layer. ALMT  [155]  incorporates an adaptive hyper-modality learning (AHL) module to learn an irrelevance/conflict-suppressing representation from visual and audio features under the guidance of language features at different scales.\n\n2) Multimodal Emotion Recognition in Conversation: In the domain of multimodal emotion recognition in conversation, FacialMMT  [25]  is a two-stage framework, which takes RoBERTa  [216]  and Swin Transformer as the backbone for representation learning. Qiu et al.  [217]  adopt VATT  [31]  to encode vision, text and audio respectively, and makes an alignment between the learned modal representation. QAP  [20]  is a quantum-inspired adaptive-priority-learning model, which employs ALBERT as the text encoder and introduces quantum theory (QT) to learn modal priority adaptively. UniMSE  [38]  proposes a multimodal fusion method based on pre-trained model T5, aiming to fuse the modal information with pretrained knowledge. GraphSmile  [156]  adopts Roberta  [216]  to track intricate emotional cues in multimodal dialogues, alternately assimilating inter-modal and intra-modal emotional dependencies layer by layer, adequately capturing cross-modal cues while effectively circumventing fusion conflicts.\n\n3) Multimodal Aspect-based Sentiment Analysis: In the study of multimodal aspect-based sentiment analysis, Xu et al.  [50]  are the first to put forward the task, multimodal aspectbased sentiment analysis, and propose a novel multi-interactive memory network (MIMN), which includes two interactive   [77]  cross-attention MOSI,MOSEI MISA  [91]  Transformer MOSI,MOSEI,UR FUNNY Self-MM  [136]  BiLSTM MOSI,MOSEI,SIMS MTAG  [107]  Graph-based neural network MOSI, IEMOCAP ScaleVLAD  [104]  VLAD,CNN MOSI,MOSEI,IEMOCAP MMIM  [88]  BERT, CPC MOSI,MOSEI UniMSE  [38]  Adapter, T5, contrastive learning MOSI,MOSEI,IEMOCAP,MELD CHFusion  [108]  CNN MOSI,IEMOCAP MMHA  [218]  Multi-head Attention MOUD,MOSI QMR  [219]  Quantum Language Mode Getty Images,Flickr RMFN  [100]  LSTHM MOSI HMM-BLSTM  [220]  HMM,BiLSTM IEMOCAP HALCB  [221]  Cognitive brain limbic system MOSI,YouTube,MOSEI CSFC  [222]  CNN,fuzzy logic Alh,Mos,Sag,MOUD SFNN  [223]  CNN,attention vista-net GFML  [186]  Multi-head attention MOSEI,MOSI MMML  [186]  cross-attention MOSEI,MOSI,CH-SIMS TATE  [116]  Transformer IEMOCAP,MOSI MERC FacialMMT-RoBERTa  [224]  RoBERTa MELD, Aff-Wild2 MultiEMO  [84]  Multi-head attention IEMOCAP, MELD MMGCN  [187]  Graph convolutional network IEMOCAP, MELD MM-DFN  [188]  GRU,Graph conventional network IEMOCAP,MELD EmoCaps  [192]  Multi-head attention IEMOCAP,MELD GA2MIF  [193]  Graph attention networks,Multi-head attention IEMOCAP,MELD MALN  [143]  LSTM, cross-attention, Transformer-based model IEMOCAP, MELD M2R2  [111]  GRU IEMOCAP, MELD TDF-Net  [142]  CNN,GRU,Transformer IEMOCAP SDT  [225]  CNN,Transformer IEMOCAP,MELD HCT-DMG  [226]  Cross-modal Transformer CMU-MOSI, MOSEI,IEMOCAP COGMEN  [49]  Graph neural network, Transformer CMU-MOSI, IEMOCAP Qiu et al.  [217]  VATT(Video-Audio-Text Transformer) CMU-MOSEI QAP  [20]  VGG,AlBERT IEMOCAP,CMU-MOSEI MPT-HCL  [37]  Transformer IEMOCAP,MELD CMCF-SRNet  [83]  Transformer,RGCN IEMOCAP,MELD SAMGN  [189]  Graph neural network IEMOCAP,MELD M3Net  [190]  Graph neural network IEMOCAP,MELD M3GAT  [227]  Graph attention network MELD,MEISD,MESD IF-MMIN  [117]  Imagination Module IEMOCAP AMuSE  [87]  Attention-based model IEMOCAP,MELD DEAN  [173]  Transformer MOSI,MOSEI,IEMOCAP MMIN  [109]  Transformer IEMOCAP,MSP-IMPROV RLEMO  [145]  Graph Convolutional Network,GRU IEMOCAP,MELD Yao et al.  [191]  Graph convolutional network IEMOCAP,MELD BCFN  [228]  CNN CHERMA,CH-SIMS MM-RBN  [85]  Transformer IEMOCAP MABSA AoM  [197]  BART,CNN Twitter2015, Twitter2017 JML  [148]  BERT, ResNet TRC, Twitter2015, Twitter2017 VLP-MABSA  [158]  Vision-Language pre-trained model Twitter2015, Twitter2017 CMMT  [146]  cross-attention,self-attention Twitter-2015,Twitter-2017,Political-Twitter DTCA  [160]  RoBERTa, ViT Twitter2015, Twitter2017 M2DF  [229]  pre-trained model CLIP Twitter2015, Twitter2017 MIMN  [16]  Memory Network ZOL KNIT  [174]  Transformer, Twitter2015, Twitter2017 MMER MMS2S  [199]  cross-attention, multi-head attention MOSEI MESGN  [200]  cross-modal transformer MOSEI TAILOR  [230]  Transformer, cross-attention MOSEI HHMPN  [231]  MPNN, multi-head attention MOSEI AMP  [95]  Transformer-based encoder-decoder, mask learning CMU-MOSEI,NEMu M3TR  [232]  CNN, Transformer, cross-attention MS-COCO,VOC 2007 UniVA  [178]  VAD model, contrastive learning MOSEI, M 3 ED memory networks to supervise the textual and visual information with the given aspect, and learns not only the interactive influences between cross-modality data but also the self influences in single-modality data. Yang et al.  [18]  propose a novel generative multimodal prompt (GMP) model for MABSA, which includes the multimodal encoder module and the N-Stream decoders module and perform three MABSA-related tasks with quite a small number of labeled multimodal samples. Liu et al.  [157]  propose an entity-related unsupervised pre-training with visual prompts for MABSA. Instead of using sentiment-related supervised pre-training, two entity-related unsupervised pre-training tasks are applied and compared, which are targeted at locating the entities in text with the support of visual prompts. Ling et al.  [158]  propose a task-specific Vision-Language Pre-training framework for MABSA (VLP-MABSA), which is a unified multimodal encoder-decoder architecture for all the pre-training and downstream tasks. Zhang et al.  [159]  construct a dynamic re-weighting BERT (DR-BERT) based BERT and designed to learn dynamic aspectoriented semantics for ABSA. Jin et al.  [161]  propose a multiaspect semantic relevance model that takes into account the match between search queries and the title, attribute and image information of items simultaneously. Wang et al.  [162]  propose an end-to-end MABSA framework with image conversion and noise filtration, which bridges the representation gap in different modalities by translating images into the input space of a pre-trained language model (PLM). Wang et al.  [163]  propose an adaptive dual graph convolution fusion network (AD-GCFN) for aspect-based sentiment analysis. This model uses two graph convolution networks: one for the semantic layer to learn semantic correlations by an attention mechanism, and the other for the syntactic layer to learn syntactic structure by dependency parsing. Mu et al.  [164]  propose a novel momentum contrastive learning network (MOCOLNet) which is an unified multimodal encoder-decoder framework for multimodal aspect-level sentiment analysis task. An endto-end training manner is proposed to optimize MOCOLNet parameters, which alleviates the problem of scarcity of labelled pre-training data. To fully explore multimodal contents especially the location information, Zhang et al.  [233]  design a multimodal interactive network to model the textual and visual information of each aspect. Two memory network-based modules are used to capture intra-modality features, including text to aspect alignment and image to aspect alignment. Yang et al.  [234]  propose multi-grained fusion network with self-distillation (MGFN-SD) to analyze aspect-based sentiment polarity, which can effectively integrate multi-grained representation learning with self-distillation to obtain more representative multimodal features.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "4) Multimodal Multi-Label Emotion Recognition:",
      "text": "A few works on multimodal multi-label emotion recognition leverage pre-trained model to improve model performance. To our best known, TAILOR  [94]  is a novel framework of versatile multimodal learning for multi-labeL emotion recognition, which adversarially depicts commonality and diversity among multiple modalities. TAILOR adversarially extracts private and common modality representations. Then a BERTlike transformer encoder is devised to gradually fuse these representations in a granularity descent way.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Enhanced Knowledge",
      "text": "External knowledge in machine learning and AI refers to information from outside the training dataset, including knowledge bases, text corpora, knowledge graphs, pre-trained models, and expert insights. Integrating this knowledge can improve performance, generalization, interpretability, and robustness to noisy or limited data  [235] ,  [236] . Fig.  7  shows the common way of incorporating external knowledge into multimodal affective learning task.\n\n1) Multimodal Sentiment Analysis: In the area of study focused on multimodal sentiment analysis, Rahmani et al.  [19]  construct an adaptive tree by hierarchically dividing users and utilizes an attention-based fusion to transfer cognitiveoriented knowledge within the tree. TETFN  [165]  is a novel method named text enhanced Transformer fusion network, which learns text-oriented pairwise cross-modal mappings for obtaining effective unified multimodal representations. Zhu et al.  [166]  propose the sentiment knowledge enhanced attention fusion network (SKEAFN), a novel end-to-end fusion network that enhances multimodal fusion by incorporating additional sentiment knowledge representations from an external knowledge base. Chen et al.  [167]  try to incorporate sentimental words knowledge into the fusion network to guide the learning of joint representation of multimodal features.\n\n2) Multimodal Emotion Recognition in Conversation: In the discipline of research related to multimodal emotion recognition in conversation, Fu et al.  [168]  integrate context modeling, knowledge enrichment, and multimodal (text and audio) learning into a GCN-based architecture. Li et al.  [169]  propose a decoupled multimodal distillation (DMD) approach that facilitates flexible and adaptive crossmodal knowledge distillation, aiming to enhance the discriminative features of each modality. Sun et al.  [170]  investigate a multimodal fusion transformer network based on rough set theory, which facilitates the interaction of multimodal information and feature guidance through rough set cross-attention. Wang et al.  [171]  devise a novel label-guided attentive fusion module to fuse the label-aware text and speech representations, which learns the label-enhanced text/speech representations for each utterance via label-token and label-frame interactions.\n\n3) Multimodal Aspect-based Sentiment Analysis: In the field of research on multimodal aspect-based sentiment analysis, Xu et al.  [174]  introduce external knowledge, including textual syntax and cross-modal relevancy knowledge to Transformer layer, which cut off the irrelevant connections among textual or cross-modal modalities by using a knowledgeinduced matrix. Yang et al.  [175]  distill visual emotional cues and align them with the textual content to selectively match and fuse with the target aspect in textual modality. CoolNet  [176]  is a cross-modal fine-grained alignment and fusion network, aims to boost the performance of visuallanguage models in seamlessly integrating vision and language information, which transforms an image into a textual caption and a graph structure, then dynamically aligns the semantic and syntactic information from both the input sentence and the generated caption, as well as models the object-level visual features. To strengthen the semantic meanings of image representations, Yu et al.  [177]  propose to detect a set of salient objects in each image based on a pre-trained Faster R-CNN model, and represent each object by concatenating its hidden representation and associated semantic concepts, followed by an Aspect-Guided Attention layer to learn the relevance of each semantic concept with the guidance of given aspects.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "4) Multimodal Multi-Label Emotion Recognition:",
      "text": "In the area of study focused on multimodal multi-label emotion recognition, Zheng et al.  [178]  propose to represent each emotion category with the valence-arousal (VA) space to capture the correlation between emotion categories and design a unimodal VA-driven contrastive learning algorithm. CARAT  [179]  presents contrastive feature reconstruction and aggregation for the MMER task. Specifically, CARAT devises a reconstruction-based fusion mechanism to better model fine-grained modality-to-label dependencies by contrastively learning modal-separated and label-specific features. Zhao et al.  [180]  propose a novel multimodal multi-label TRansformer (M3TR) learning framework, which embeds the high-level semantics, visual structures and label-wise co-occurrences of multiple modalities into one unified encoding. Li et al.  [86]  propose a novel multimodal attention graph network with dynamic routing-by-agreement (MAGDRA) for MMER. MAG-DRA is able to efficiently fuse graph data with various node and edge types as well as properly learn the crossmodal and temporal interactions between multimodal data without pre-aligning. Zhang et al.  [181]  propose heterogeneous hierarchical message passing network (HHMPN), which can simultaneously model the feature-to-label, label-to-label and modality-to-label dependencies via graph message passing.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "D. Contextual Information",
      "text": "Context refers to the surrounding words, sentences, or paragraphs that give meaning to a particular word or phrase. Understanding context is crucial for tasks like dialogue systems or sentiment analysis. In a conversation, context includes the history of previous utterances, while for news, it refers to the overall description provided by the entire document. Overall, contextual information helps machines make more accurate predictions. Fig.  8  shows the significance of context information to multimodal affective learning task. 1) Multimodal Sentiment Analysis: In the community of multimodal sentiment analysis, Chauhan et al.  [182]  employ a context-aware attention module to learn intra-modality interaction among particating modalities through encoder-decoder structure. Multimodal context integrate the unimodal context, Poria et al.  [183]  propose a recurrent model with multi-level multiple attentions to capture contextual information among utterances, and design a recurrent model to capture contextual information among utterances and introduced attention-based networks for improving both context learning and dynamic feature fusion. Huang et al.  [184]  propose a novel contextbased adaptive multimodal fusion network (CAMFNet) for consecutive frame-level sentiment prediction. Li et al.  [185]  propose a spatial context extraction block to explore the spatial context by calculating the relationships between feature maps and the higher-level semantic representation in images.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "2) Multimodal Emotion Recognition In Conversation:",
      "text": "In the realm of research concerning multimodal emotion recognition in conversation, Hu et al.  [187]  make use of multimodal dependencies effectively, and leverages speaker information to model inter-speaker and intra-speaker dependency. Zhang et al.  [83]  propose a cross-modality context fusion and semantic refinement network (CMCF-SRNet) to solve the limitation of insufficient semantic relationship information between utterances. Zhang et al.  [189]  construct multiple modality-specific graphs to model the heterogeneity of the multimodal context. Chen et al.  [190]  propose a GNN-based model that explores multivariate relationships and captures the varying importance of emotion discrepancy and commonality by valuing multifrequency signals. Zhang et al.  [227]  propose a multimodal, multi-task interactive graph attention network, termed M3GAT, to simultaneously solve conversational context dependency, multimodal interaction, and multi-task correlation in a unified framework. RL-EMO  [145]  is a novel reinforcement Learning framework for the multimodal emotion recognition task, which combines reinforcement learning (RL) module to model context at both the semantic and emotional levels respectively. Yao et al.  [191]  propose a speaker-centric multimodal fusion network for emotion recognition in a conversation, to model intra-modal feature fusion and speaker-centric cross-modal feature fusion.\n\n3) Multimodal Aspect-based Sentiment Analysis: In the stduy of multimodal aspect-based sentiment analysis, Yu et al.  [160]  propose an unsupervised approach which minimizes the Wasserstein distance between both modalities, forcing both encoders to produce more appropriate representations for the final extraction. Xu et al.  [194]  design and construct a multimodal Chinese product review dataset (MCPR) to support the research of MABSA. Anschutz et al.  [195]  report the results of an empirical study on how semantic computing can provide insights into user-generated content for domain experts. In addition, this work discussed different image-based aspect retrieval and aspect-based sentiment analysis approaches to handle and structure large datasets. Zhao et al.  [196]  borrow the idea of Curriculum Learning and propose a multi-grained multicurriculum denoising Framework (M2DF) to adjust the order of training data, so as to obtain more contextual information. Zhou et al.  [197]  propose an aspect-oriented method (AoM) to detect aspect-relevant semantic and sentiment information. Specifically, an aspect-aware attention module is designed to simultaneously select textual tokens and image blocks that are semantically related to the aspects. Zhao et al.  [198]  propose a fusion with GCN and SE ResNeXt Network (FGSN), which constructs a graph convolution network on the dependency tree of sentences to obtain the context representation and aspects words representation by using syntactic information and word dependency.\n\n4) Multimodal Multi-label Emotion Recognition: MMS2S  [199]  is a multimodal sequence-to-set approach to effectively model label dependence and modality dependence. MESGN  [200]  firstly proposes this task, which simultaneously models the modality-to-label and label-to-label dependencies. Many works consider the dependencies of multi-label based on the characteristics of co-occurrence labels. Zhao et al.  [201]  propose a general multimodal dialogue-aware interaction framework, named by MDI, to model the impacts of dialogue context on emotion recognition.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Vii. Datasets Of Multimodal Affective Computing",
      "text": "In this section, we introduce the benchmark datasets of MSA, MERC, MABSA, and MMER tasks. To facilitate easy navigation and reference, the details of datasets are shown in Table  III  with a comprehensive overview of the studies that we cover.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "A. Multimodal Sentiment Analysis",
      "text": "• MOSI  [237]  contains 2,199 utterance video segments, and each segment is manually annotated with a sentiment score ranging from -3 to +3 to indicate the sentiment polarity and relative sentiment strength of the segment.\n\n• MOSEI  [238]  is an upgraded version of MOSI, annotated with both sentiment and emotion. MOSEI contains 22,856 movie review clips from YouTube. Each sample in MOSEI includes sentiment annotations ranging from -3 to +3 and multi-label emotion annotations. • CH-SIMS  [239]  is a Chinese single-and multimodal sentiment analysis dataset, which contains 2,281 refined video segments in the wild with both multimodal and independent unimodal annotations. • CH-SIMS v2.0  [242]  is an extended version of CH-SIMS that includes more data instances, spanning text, audio and visual modalities. Each modality of sample is annotated with sentiment polarity, and then sample is annotated with a concluded sentiment. • CMU-MOSEAS  [240]  is the first large-scale multimodal language dataset for Spanish, Portuguese, German and French, and it is collect from YouTube and its samples are 4,000 in total. • ICT-MMMO  [241]  is collected from online social review videos that encompass a strong diversity in how people express opinions about movies and include a real-world variability in video recording quality  4  .\n\n• YouTube  [46]  collects 47 videos from the social media web site YouTube. Each video contains 3-11 utterances with most videos having 5-6 utterances in the extracted 30 seconds.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "B. Multimodal Emotion Recognition In Conversation",
      "text": "• MELD  [243]  contains 13,707 video clips of multi-party conversations, with labels following Ekman's six universal emotions, including joy, sadness, fear, anger, surprise and disgust.\n\n• IEMOCAP  [244]  has 7,532 video clips of multi-party conversations, with labels following Ekman's six universal emotions, including joy, sadness, fear, anger, surprise and disgust.\n\n• HED  [245]  contains happy, sad, disgust, angry and scared emotion-aligned face, body and text samples, which are much larger than existing datasets. Moreover, the emotion labels were correspondingly attached to those samples by strictly following a standard psychological paradigm.    [238]  T,A,V YouTube English 22,856 CH-SIMS  [239]  T,A,V Movies,TVs Chinese 2,281 CMU-MOSEAS  [240]  T,A,V YouTube Spanish,Portuguese,German,French 4,000 ICT-MMMO  [241]  T,A,V reviews ----YouTube  [46]  T,A,V YouTube --English 300 CH-SIMS v2.0  [242]  T,A,V TV series,Shows,Movies Chinese 14,402 MERC MELD  [243]  T,A,V Friends TV English 13,707 IEMOCAP  [244]  T,A,V Act English 7,532 HED  [245]  T,V Movies, TVs English 17,441 RML  [246]  A,V Video English, Mandarin, Urdu, Punjabi, Persian, and Italian -BAUM-1  [247]  A,V Data collection Turkish 1,184 MAHNOB-HCI  [248]  V, EEG Data collection --Deap  [249]  EEG Act, Data collection physiological signal -MuSe-CaR  [250]  T,A,V Car reviews,YouTube English CHEAVD  [251]  A,V Movies,TVs --Mandarin 7,030 MSP-IMPROV  [252]  T,A,V Act English 8,438 MEISD  [253]  T,A,V TVs English -MESD  [254]  T,A,V TVs English 9,190 Ulm-TSST  [255]  A,V,EEG job interviews English -CHERMA  [256]  T,A,V TV series,Shows,Movies Chinese 28,717 AMIGOS  [257]  EEG, ECG, GSR Data collection --",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Mabsa",
      "text": "Twitter2015  [258]  T,V Twitter English -Twitter2017  [258]  T,V Twitter English -MCPR  [259]  T,V Product reviews Chinese 15,000 Multi-ZOL  [50]  T,V Product reviews Chinese 5,288 MACSA  [260]  T,V Hotel service reviews Chinese 21,108 MASAD  [261]  T,V Visual sentiment ontology datasets English 38,532 PanoSent  [262]  T,A,V Social media English,Chinese, Spanish 10,000 MMER CMU-MOSEI  [238]  T,A,V YouTube English 22,856 M 3 ED  [201]  T,A,V 56 TVs Mandarin 24,449 from 1 to 9 for arousal, valence, dominance, like/dislike, and familiarity. • MuSe-CaR  [250]  focuses on the tasks of emotion, emotion-target engagement, and trustworthiness recognition by means of comprehensively integrating the audiovisual and language modalities. • CHEAVD 2.0  [251]  is selected from Chinese movies, soap operas and TV shows, which contains noise in the background to mimic real-world conditions. • MSP-IMPROV  [252]  is a multimodal emotional database comprised of spontaneous dyadic interactions, designed to study audiovisual perception of expressive behaviors.\n\n• MEISD  [253]  is a large-scale balanced multimodal multilabel emotion, intensity, and sentiment dialogue dataset (MEISD) collected from different TV series that has textual, audio, and visual features.\n\n• MESD  [254]  is the first multimodal and multi-task sentiment, emotion, and desire dataset, which contains 9,190 text-image pairs, with English text. • Ulm-TSST  [255]  is a multimodal dataset, where participants were recorded in a stressful situation emulating a job interview, following the TSST protocol.\n\n• CHERMA  [256]  provides uni-modal labels for each individual modality, and multi-modal labels for all modalities jointly observed. It is collected from various source, including 148 TV series, 7 variety shows, and 2 movies.\n\n• AMIGOS  [257]  is collected in two experimental settings. In the first setting, 40 participants viewed 16 short emotional videos. In the second setting, participants watched 4 longer videos, some individually and others in groups. During these sessions, participants' physiological signals-Electroencephalogram (EEG), Electrocardiogram (ECG), and Galvanic Skin Response (GSR)-were recorded using wearable sensors.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "C. Multimodal Aspect-Based Sentiment Analysis",
      "text": "• Twitter2015 and Twitter2017 are originally provided by the work  [258]  for multimodal named entity recognition and annotated with the sentiment polarity for each aspect by the work  [14] .",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Viii. Evaluation Metrics",
      "text": "In this section, we report the mainstream evaluation metrics for each multimodal affective computing task.\n\na) Multimodal Sentiment Analysis: Previous works adopt mean absolute error (MAE), Pearson correlation (Corr), sevenclass classification accuracy (ACC-7), binary classification accuracy (ACC-2) and F1 score computed for positive/negative and non-negative/negative classification as evaluation metrics.\n\nb) Multimodal Emotion Recognition in a conversation: Accuracy (ACC) and weighted F1 (WF1) are used for evaluation. Additionally, the imbalance label distribution results in a phenomenon that the trained model performs better on some categories and perform poorly on others. In order to verify the impacts of data distribution on model performance, researchers also provide ACC and F1 on each emotion category to measure the model performance.\n\nc) Multimodal Aspect-based Sentiment Analysis: With the previous methods, for multimodal aspect term extraction (MATE) and joint multimodal aspect sentiment analysis (JMASA) tasks, researchers use precision (P), recall (R) and micro-F1 (F1) as the evaluation metrics. For the multimodal aspect sentiment classification(MASC) task, accuracy (ACC) and macro-F1 are as evaluation metrics.\n\nd) Multimodal Multi-label Emotion Recognition: According to the prior work, multi-label classification works mostly adopt accuracy (ACC), micro-F1, precision (P) and recall (R) as evaluation metrics.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Ix. Discuss",
      "text": "In this section, we briefly discuss the works of multimodal affective computing based on facial expression, acoustic signal, physiological signals, and emotion cause. Furthermore, we discuss the technical routes across multiple multimodal affective computing tasks to track their consistency and difference.\n\nA. Other Multimodal Affective Computing a) Multimodal Affective Computing Based on Facial Expression Recognition: Facial expression recognition has significantly evolved over the years, progressing from static to dynamic methods. Initially, static facial expression recognition (SFER) relied on single-frame images, utilizing traditional image processing techniques such as Local Binary Patterns (LBP) and Gabor filters to extract features for classification. The advent of deep learning brought Convolutional Neural Networks (CNNs), which markedly improved the accuracy of SFER  [264] -  [267] . However, static methods were limited in capturing the temporal dynamics of facial expressions  [268] . Some methods attempt to approach the problem from a local-global feature perspective, extracting more finegrained visual representations and identifying key informative segments  [269] -  [274] . These approaches enhance robustness against noisy frames, enabling uncertainty-aware inference. To further enhance accuracy, recent advancements in DFER focus on integrating multimodal data and employing parameterefficient fine-tuning (PEFT) to adapt large pre-trained models for enhanced performance  [275] -  [277] , while Liu et al.  [278]  introduces the concept of expression reenactment (i.e. normalization), harnessing generative AI to mitigate noise in inthe-wild datasets. Moreover, the burgeoning evidential deep learning (EDL) has shown considerable promise by enabling explicit uncertainty quantification through the distributional measurement in latent spaces for improved interpretability, with demonstrated efficacy in zero-shot learning  [279] , multiview classification  [280] -  [282] , video understanding  [283] -  [285]  and multi-modal named entity recognition.\n\nb) Multimodal Affective Computing Based on Acoustic Signal: The model based on single-sentence single-task is the most common model in speech emotion recognition. For example, Aldeneh et al.  [286]  use CNN to perform convolutions in the time direction of handcrafted temporal features (40-dimensional MFSC) to identify emotion-salient regions and used global max pooling to capture important temporal areas. Li et al.  [287]  apply two different convolution kernels on spectrograms to extract temporal and frequency domain features, concatenated them, and input them into a CNN for learning, followed by attention mechanism pooling for classification. Trigeorgis et al.  [288]  use CNN for end-to-end learning directly on speech signals, avoiding the problem of feature extraction not being robust for all speakers. Mirsamadi et al.  [289]  combine Bidirectional LSTM (Bi-LSTM) with a novel pooling strategy, utilizing attention mechanisms to enable the network to focus on emotionally prominent parts of sentences. Zhao et al.  [290]  consider the temporal and spatial characteristics of the spectrum in the attention mechanism to learn time-related features in spectrograms, and using CNN to learn frequency-related features in spectrograms. Luo et al.  [291]  propose a dual-channel speech emotion recognition model that uses CNN and RNN to learn from spectrograms on one hand, and separately learns HSFs features on the other, finally concatenating the obtained features for classification. c) Multimodal Affective Computing Based on Physiological Signals: In medical measurements and health monitoring, EEG-based emotion recognition (EER) is one of the most promising directions within emotion recognition and has attracted substantial research attention  [292] -  [294] . Notably, the field of affective computing has seen nearly 1,000 publications related to EER since 2010  [295] . Numerous EEG-based multimodal emotion recognition (EMER) methods have been proposed  [296] -  [300] , leveraging the complementarity and redundancy between EEG and other physiological signals in expressing emotions. For example, Vazquez et al.  [301]  address the problem of multimodal emotion recognition from multiple physiological signal, which demonstrates Transformer-based approach is suitable for emotion recognition based on physiological signal.\n\nd) Multimodal Affective Computing Based on Emotion Cause: Apart from focusing on the emotions themselves, the capacity of machine for understanding the cause that triggers an emotion is essential for comprehending human behaviors, which makes emotion-cause pair extraction (ECPE) crucial. Over the years, text-based ECPE has made significant progress  [302] ,  [303] . Based on ECPE, Li et al.  [304]  propose multimodal emotion-cause pair extraction (MECPE), which aims to extract emotion-cause pairs with multimodal information. Initially, Li et al.  [304]  construct a joint training architecture, which contains the main task, i.e., multimodal emotion-cause pair extraction and two subtasks, i.e., multimodal emotion detection and cause detection. To solve MECPE, researchers borrowed the multitask learning framework to train the model using multiple training objectives of sub-tasks, aiming to enhance the knowledge sharing among them. For example, Li et al.  [305]  propose a novel model that captures holistic interaction and label constraint (HiLo) features for the MECPE task. HiLo enables cross-modality and cross-utterance feature interactions through various attention mechanisms, providing a strong foundation for accurate cause extraction.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "B. Consistency Among Multimodal Affective Computing",
      "text": "We categorize the multimodal affective computing tasks into several key areas: multimodal alignment and fusion, multi-task learning, pre-trained models, enhanced knowledge, and contextual information. To ensure clarity, we discuss the consistencies across these aspects.\n\na) Multimodal alignment and fusion: Among MSA, MERC, MABSA and MMER tasks, each is fundamentally a multimodal task that involves considering and combining at least two modalities to make decisions. This process includes extracting features from each modality and integrating them into a unified representation vector. In multimodal representation learning, modal alignment and fusion are two critical issues that must be addressed to advance the field of multimodal affective computing. For vision-dominated multimodal tasks such as image captioning  [8] ,  [306] , the impact of vision is more significant than language. In contrast, multimodal affective computing tasks place a greater emphasis on language  [38] ,  [307] .\n\nb) Pre-trained model: Generally, pre-trained models are used to encode raw modal information into vectors. From this perspective, multimodal affective computing tasks adopt pretrained models as the backbone and then fine-tune them for downstream tasks. For example, UniMSE  [38]  uses T5 as the backbone, while GMP  [18]  utilizes BART. These approaches aim to transfer the general knowledge embedded in pre-trained language models to the field of affective computing. c) Enhanced knowledge: Commonsense knowledge encompasses facts and judgments about our natural world. In the field of affective computing, this knowledge is crucial for enabling machines to understand human emotions and their underlying causes. Researchers enhance affective computing by integrating external knowledge sources such as sentiment lexicons  [308] , English knowledge bases  [309] -  [313] , and Chinese knowledge bases  [314]  as the external knowledge to enhance affective computing.\n\nd) Contextual information: Affective computing tasks require an understanding of contextual information. In MERC, contextual information encompasses the entire conversation, including both previous and subsequent utterances relative to the current utterance. For MABSA, contextual information refers to the full sentence containing customer opinions. Researchers integrate contextual information using hierarchical approaches  [315] ,  [316] , self-attention mechanisms  [58] , and graph-based dependency modeling  [317] ,  [318] . Additionally, affective computing tasks can enhance understanding by incorporating non-verbal cues such as facial expressions and vocal tone, alongside textual information.",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "C. Difference Among Multimodal Affective Computing",
      "text": "We examine the differences among multimodal affective computing tasks by considering the type of downstream tasks, sentiment granularity, and application contexts to identify the unique characteristics of each task.\n\nFor downstream tasks, MSA predicts sentiment strength as a regression task. MERC is a multi-class classification task for identifying emotion categories. MMER performs multilabel emotion recognition, detecting multiple emotions simultaneously. MABSA involves extracting aspects and opinions to determine sentiment polarity, categorizing it as information extraction. In terms of analysis granularity, MERC and MECPE focus on utterances and speakers within a conversation, while MSA and MMER concentrate on sentence-level information within a document. MABSA, on the other hand, focuses on aspects within comments. Some studies infer fine-grained sentiment from coarse-grained sentiment  [209] ,  [319]  or integrate tasks of different granularities into a unified training framework  [307] . Due to these differences in granularity, the contextual information varies as well. For instance, in MABSA, the context includes the comment along with any associated images and short descriptions of aspects, whereas in MERC, the context encompasses the entire conversation and speaker information. In terms of application scenarios, MSA, MMER, and MABSA are used for public opinion analysis and mining user experiences related to products or services. MERC and MECPE help machines understand and mimic human behaviors, generating empathetic responses in dialogue agents. While many tasks are context-specific, there is a growing trend toward unified frameworks for analyzing human emotions across diverse settings (e.g., task type and emotion granularity)  [38] ,  [138] .",
      "page_start": 16,
      "page_end": 17
    },
    {
      "section_name": "X. Future Work",
      "text": "We outline directions for future work in multimodal affective computing from Transfer Learning with Multimodal Pre-trained Model, Unification of Multimodal Affective Computing tasks, Model with External Knowledge Distill, and Affective Computing with Less-studied Modalities.\n\na) Unification of Multimodal Affective Computing tasks: Recent advances have made significant strides by unifying related yet distinct tasks into a single framework  [26] ,  [212] ,  [215] . For example, T5  [60]  integrates various NLP tasks by representing all text-based problems in a text-to-text format, achieving state-of-the-art results across numerous benchmarks. These studies highlight the effectiveness of such unified frameworks in enhancing model performance and generalization  [214] ,  [214] ,  [320] . Meanwhile, the progress also suggests the potential for unifying multimodal affective computing tasks across diverse application scenarios. First, unification across different granularity-from fine to coarse-has been employed to train models effectively  [209] . Second, an increasing number of pre-trained models now handle language, vision, and audio simultaneously, enabling end-to-end processing across single, dual, and multiple modalities  [321] . Third, integrating emotion-cause analysis with emotion recognition in a multimodal setting within a single architecture can enhance their mutual indications and improve overall performance.\n\nb) Transfer Learning with External Knowledge Distill: In the field of affective computing, incorporating external knowledge such as sentiment lexicons and commonsense knowledge is crucial for a deeper understanding of emotional expressions within the context of social norms and cultural backgrounds  [322] . For example, The expression and perception of emotion also varies across cultures, both in text and in face-to-face communication  [323] . These differences are critical for cross-cultural sentiment analysis. c) Affective Computing with Less-studied Modalities: Natural language (spoken and written), visual data (images and videos), and auditory signals (speech, sound, and music) have long been central to multimodal affective computing. Recently, new sensing data types, like haptic and ECG signals, are gaining attention  [324] . Haptic signals, which involve touch and convey sensory and emotional attributes, enhance user experiences in areas such as gaming, virtual reality, and mobile apps  [325] . These signals offer immediate feedback and can improve user engagement. As research progresses, less-studied modalities like haptic will likely become crucial, complementing established methods and advancing the field of affective computing.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Xi. Conclusion",
      "text": "Multimodal Affective Computing has emerged as a crucial research direction in artificial intelligence with significant progress in understanding and interpreting emotions. This survey provides a comprehensive overview of the diverse tasks associated with multimodal affective computing, covering its research background, definitions, related work, technical approaches, benchmark dataset, and evaluation metrics. We group multimodal affective computing across MSA, MERC, MABSA and MMER tasks into four categories: multi-task learning, pre-trained modal, enhanced knowledge and context information. Additionally, we summarize the consistency and differences among various affective computing tasks. Also, we report the inherent challenges in multimodal sentiment analysis and explore potential directions for future research and development.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: B. Multimodal Fusion",
      "page": 4
    },
    {
      "caption": "Figure 2: illustrates the three aspects",
      "page": 4
    },
    {
      "caption": "Figure 1: Taxonomy of multimodal affective computing from multimodal fusion and multimodal alignment.",
      "page": 5
    },
    {
      "caption": "Figure 2: Illustration of multimodal fusion from following aspects: 1) cross-modality modal fusion, 2) modal fusion based on modal consistency and difference",
      "page": 5
    },
    {
      "caption": "Figure 3: Illustration multimodal alignment:(a) semantic alignment and (b)",
      "page": 6
    },
    {
      "caption": "Figure 3: illustrates the multimodal alignment.",
      "page": 6
    },
    {
      "caption": "Figure 4: summarizes the typical",
      "page": 7
    },
    {
      "caption": "Figure 5: shows the learning paradigm of multitask learning in",
      "page": 7
    },
    {
      "caption": "Figure 4: Taxonomy of multimodal affective computing works from aspects multitask learning, pre-trained model, enhanced knowledge and contextual information.",
      "page": 8
    },
    {
      "caption": "Figure 5: Illustration of multitask learning in multimodal affective computing",
      "page": 8
    },
    {
      "caption": "Figure 6: shows the use of pre-trained model in",
      "page": 9
    },
    {
      "caption": "Figure 6: An illustration of pre-trained model in multimodal affective computing",
      "page": 9
    },
    {
      "caption": "Figure 7: Illustration of enhanced knowledge in multimodal affective computing",
      "page": 11
    },
    {
      "caption": "Figure 8: shows the significance of context",
      "page": 12
    },
    {
      "caption": "Figure 8: Illustration of context information in multimodal affective computing",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task": "MSA",
          "Model\nModel architecture\nMiss Modality\nDatasets": "(cid:37)\nTFN [76]\nLSTM\nMOSI,MOSEI\n(cid:37)\nMuLT [77]\ncross-attention\nMOSI,MOSEI\n(cid:37)\nMISA [91]\nTransformer\nMOSI,MOSEI,UR FUNNY\n(cid:37)\nSelf-MM [136]\nBiLSTM\nMOSI,MOSEI,SIMS\n(cid:37)\nMTAG [107]\nGraph-based neural network\nMOSI,\nIEMOCAP\n(cid:37)\nScaleVLAD [104]\nVLAD,CNN\nMOSI,MOSEI,IEMOCAP\n(cid:37)\nMMIM [88]\nBERT, CPC\nMOSI,MOSEI\n(cid:37)\nUniMSE [38]\nAdapter, T5, contrastive learning\nMOSI,MOSEI,IEMOCAP,MELD\n(cid:37)\nCHFusion [108]\nCNN\nMOSI,IEMOCAP\n(cid:37)\nMMHA [218]\nMulti-head Attention\nMOUD,MOSI\n(cid:37)\nQMR [219]\nQuantum Language Mode\nGetty Images,Flickr\n(cid:37)\nRMFN [100]\nLSTHM\nMOSI\n(cid:37)\nHMM-BLSTM [220]\nHMM,BiLSTM\nIEMOCAP\n(cid:37)\nHALCB [221]\nCognitive brain limbic system\nMOSI,YouTube,MOSEI\n(cid:37)\nCSFC [222]\nCNN,fuzzy logic\nAlh,Mos,Sag,MOUD\n(cid:37)\nSFNN [223]\nCNN,attention\nvista-net\n(cid:37)\nGFML [186]\nMulti-head attention\nMOSEI,MOSI\n(cid:37)\nMMML [186]\ncross-attention\nMOSEI,MOSI,CH-SIMS\n(cid:33)\nTATE [116]\nTransformer\nIEMOCAP,MOSI"
        },
        {
          "Task": "MERC",
          "Model\nModel architecture\nMiss Modality\nDatasets": "(cid:37)\nFacialMMT-RoBERTa [224]\nRoBERTa\nMELD, Aff-Wild2\n(cid:37)\nMultiEMO [84]\nMulti-head attention\nIEMOCAP, MELD\n(cid:37)\nMMGCN [187]\nGraph convolutional network\nIEMOCAP, MELD\n(cid:37)\nMM-DFN [188]\nGRU,Graph conventional network\nIEMOCAP,MELD\n(cid:37)\nEmoCaps [192]\nMulti-head attention\nIEMOCAP,MELD\n(cid:37)\nGA2MIF [193]\nGraph attention networks,Multi-head attention\nIEMOCAP,MELD\n(cid:37)\nMALN [143]\nLSTM, cross-attention, Transformer-based model\nIEMOCAP, MELD\n(cid:33)\nM2R2 [111]\nGRU\nIEMOCAP, MELD\n(cid:33)\nTDF-Net\n[142]\nCNN,GRU,Transformer\nIEMOCAP\n(cid:37)\nSDT [225]\nCNN,Transformer\nIEMOCAP,MELD\n(cid:37)\nHCT-DMG [226]\nCross-modal Transformer\nCMU-MOSI, MOSEI,IEMOCAP\n(cid:37)\nCOGMEN [49]\nGraph neural network, Transformer\nCMU-MOSI,\nIEMOCAP\n(cid:37)\nQiu et al.\n[217]\nVATT(Video-Audio-Text Transformer)\nCMU-MOSEI\n(cid:37)\nQAP [20]\nVGG,AlBERT\nIEMOCAP,CMU-MOSEI\n(cid:37)\nMPT-HCL [37]\nTransformer\nIEMOCAP,MELD\n(cid:37)\nCMCF-SRNet\n[83]\nTransformer,RGCN\nIEMOCAP,MELD\n(cid:37)\nSAMGN [189]\nGraph neural network\nIEMOCAP,MELD\n(cid:37)\nM3Net\n[190]\nGraph neural network\nIEMOCAP,MELD\n(cid:37)\nM3GAT [227]\nGraph attention network\nMELD,MEISD,MESD\n(cid:33)\nIF-MMIN [117]\nImagination Module\nIEMOCAP\n(cid:33)\nAMuSE [87]\nAttention-based model\nIEMOCAP,MELD\n(cid:37)\nDEAN [173]\nTransformer\nMOSI,MOSEI,IEMOCAP\n(cid:33)\nMMIN [109]\nTransformer\nIEMOCAP,MSP-IMPROV\n(cid:37)\nRLEMO [145]\nGraph Convolutional Network,GRU\nIEMOCAP,MELD\n(cid:37)\nYao et al.\n[191]\nGraph convolutional network\nIEMOCAP,MELD\n(cid:37)\nBCFN [228]\nCNN\nCHERMA,CH-SIMS\n(cid:37)\nMM-RBN [85]\nTransformer\nIEMOCAP"
        },
        {
          "Task": "MABSA",
          "Model\nModel architecture\nMiss Modality\nDatasets": "(cid:37)\nAoM [197]\nBART,CNN\nTwitter2015, Twitter2017\n(cid:37)\nJML [148]\nBERT, ResNet\nTRC, Twitter2015, Twitter2017\n(cid:37)\nVLP-MABSA [158]\nVision-Language pre-trained model\nTwitter2015, Twitter2017\n(cid:37)\nCMMT [146]\ncross-attention,self-attention\nTwitter-2015,Twitter-2017,Political-Twitter\n(cid:37)\nDTCA [160]\nRoBERTa, ViT\nTwitter2015, Twitter2017\n(cid:37)\nM2DF [229]\npre-trained model CLIP\nTwitter2015, Twitter2017\n(cid:37)\nMIMN [16]\nMemory Network\nZOL\n(cid:37)\nKNIT [174]\nTransformer,\nTwitter2015, Twitter2017"
        },
        {
          "Task": "MMER",
          "Model\nModel architecture\nMiss Modality\nDatasets": "(cid:37)\nMMS2S [199]\ncross-attention, multi-head attention\nMOSEI\n(cid:37)\nMESGN [200]\ncross-modal\ntransformer\nMOSEI\n(cid:37)\nTAILOR [230]\nTransformer, cross-attention\nMOSEI\n(cid:37)\nHHMPN [231]\nMPNN, multi-head attention\nMOSEI\n(cid:37)\nAMP [95]\nTransformer-based encoder-decoder, mask learning\nCMU-MOSEI,NEMu\n(cid:37)\nM3TR [232]\nCNN, Transformer, cross-attention\nMS-COCO,VOC 2007\n(cid:37)\nUniVA [178]\nVAD model, contrastive learning\nMOSEI, M 3ED"
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Tfcd: Towards multi-modal sarcasm detection via training-free counterfactual debiasing",
      "authors": [
        "Z Zhu",
        "X Zhuang",
        "Y Zhang",
        "D Xu",
        "G Hu",
        "X Wu",
        "Y Zheng"
      ],
      "year": "2024",
      "venue": "Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24"
    },
    {
      "citation_id": "2",
      "title": "Towards multimodal sarcasm detection via disentangled multi-grained multi-modal distilling",
      "authors": [
        "Z Zhu",
        "X Cheng",
        "G Hu",
        "Y Li",
        "Z Huang",
        "Y Zou"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation, LREC/COLING 2024"
    },
    {
      "citation_id": "3",
      "title": "The subtlety of emotions",
      "authors": [
        "A Ben-Ze'ev"
      ],
      "year": "2001",
      "venue": "The subtlety of emotions"
    },
    {
      "citation_id": "4",
      "title": "Emotions, sentiments, and performance expectations",
      "authors": [
        "R Shelly"
      ],
      "year": "2004",
      "venue": "Theory and research on human emotions"
    },
    {
      "citation_id": "5",
      "title": "Handbook of affective sciences",
      "authors": [
        "R Davidson",
        "K Sherer",
        "H Goldsmith"
      ],
      "year": "2009",
      "venue": "Handbook of affective sciences"
    },
    {
      "citation_id": "6",
      "title": "Modeling latent discriminative dynamic of multi-dimensional affective signals",
      "authors": [
        "G Ramírez",
        "T Baltrusaitis",
        "L Morency"
      ],
      "year": "2011",
      "venue": "Affective Computing and Intelligent Interaction -Fourth International Conference, ACII 2011"
    },
    {
      "citation_id": "7",
      "title": "A multitask learning framework for multimodal sentiment analysis",
      "authors": [
        "D Jiang",
        "R Wei",
        "H Liu",
        "J Wen",
        "G Tu",
        "L Zheng",
        "E Cambria"
      ],
      "year": "2021",
      "venue": "2021 International conference on data mining workshops (ICDMW)"
    },
    {
      "citation_id": "8",
      "title": "Align before fuse: Vision and language representation learning with momentum distillation",
      "authors": [
        "J Li",
        "R Selvaraju",
        "A Gotmare",
        "S Joty",
        "C Xiong",
        "S Hoi"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021"
    },
    {
      "citation_id": "9",
      "title": "Modality distillation with multiple stream networks for action recognition",
      "authors": [
        "N Garcia",
        "P Morerio",
        "V Murino"
      ],
      "year": "2018",
      "venue": "Computer Vision -ECCV 2018 -15th European Conference"
    },
    {
      "citation_id": "10",
      "title": "Diversified multiple instance learning for document-level multi-aspect sentiment classification",
      "authors": [
        "Y Ji",
        "H Liu",
        "B He",
        "X Xiao",
        "H Wu",
        "Y Yu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing"
    },
    {
      "citation_id": "11",
      "title": "Identifying sentiment from crowd audio",
      "authors": [
        "P Donnelly",
        "A Prestwich"
      ],
      "year": "2022",
      "venue": "7th International Conference on Frontiers of Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Learning relationships between text, audio, and video via deep canonical correlation for multimodal language analysis",
      "authors": [
        "Z Sun",
        "P Sarma",
        "W Sethares",
        "Y Liang"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Factorized multimodal transformer for multimodal sequential learning",
      "authors": [
        "A Zadeh",
        "C Mao",
        "K Shi",
        "Y Zhang",
        "P Liang",
        "S Poria",
        "L Morency"
      ],
      "year": "2019",
      "venue": "CoRR"
    },
    {
      "citation_id": "14",
      "title": "Visual attention model for name tagging in multimodal social media",
      "authors": [
        "D Lu",
        "L Neves",
        "V Carvalho",
        "N Zhang",
        "H Ji"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018"
    },
    {
      "citation_id": "15",
      "title": "Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions",
      "authors": [
        "A Gandhi",
        "K Adhvaryu",
        "S Poria",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2023",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "16",
      "title": "Multi-interactive memory network for aspect based multimodal sentiment analysis",
      "authors": [
        "N Xu",
        "W Mao",
        "G Chen"
      ],
      "year": "2019",
      "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "17",
      "title": "Multimodal emotion-cause pair extraction in conversations",
      "authors": [
        "F Wang",
        "Z Ding",
        "R Xia",
        "Z Li",
        "J Yu"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "18",
      "title": "Few-shot joint multimodal aspect-sentiment analysis based on generative multimodal prompt",
      "authors": [
        "X Yang",
        "S Feng",
        "D Wang",
        "Q Sun",
        "W Wu",
        "Y Zhang",
        "P Hong",
        "S Poria"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "19",
      "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects",
      "authors": [
        "S Rahmani",
        "S Hosseini",
        "R Zall",
        "M Kangavari",
        "S Kamran",
        "W Hua"
      ],
      "year": "2023",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "20",
      "title": "QAP: A quantum-inspired adaptive-priority-learning model for multimodal emotion recognition",
      "authors": [
        "Z Li",
        "Y Zhou",
        "Y Liu",
        "F Zhu",
        "C Yang",
        "S Hu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "21",
      "title": "Multimodal deep learning",
      "authors": [
        "J Ngiam",
        "A Khosla",
        "M Kim",
        "J Nam",
        "H Lee",
        "A Ng"
      ],
      "year": "2011",
      "venue": "Proceedings of the 28th International Conference on Machine Learning, ICML 2011"
    },
    {
      "citation_id": "22",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "W Rahman",
        "M Hasan",
        "S Lee",
        "A Zadeh",
        "C Mao",
        "L Morency",
        "M Hoque"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020"
    },
    {
      "citation_id": "23",
      "title": "A survey on multi-task learning",
      "authors": [
        "Y Zhang",
        "Q Yang"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Knowl. Data Eng"
    },
    {
      "citation_id": "24",
      "title": "Knowledge-interactive network with sentiment polarity intensity-aware multi-task learning for emotion recognition in conversations",
      "authors": [
        "Y Xie",
        "K Yang",
        "C Sun",
        "B Liu",
        "Z Ji"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana"
    },
    {
      "citation_id": "25",
      "title": "A facial expression-aware multimodal multi-task learning framework for emotion recognition in multi-party conversations",
      "authors": [
        "W Zheng",
        "J Yu",
        "R Xia",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "26",
      "title": "Unidu: Towards A unified generative dialogue understanding framework",
      "authors": [
        "Z Chen",
        "L Chen",
        "B Chen",
        "L Qin",
        "Y Liu",
        "S Zhu",
        "J Lou",
        "K Yu"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "27",
      "title": "Univilm: A unified video and language pre-training model for multimodal understanding and generation",
      "authors": [
        "H Luo",
        "L Ji",
        "B Shi",
        "H Huang",
        "N Duan",
        "T Li",
        "X Chen",
        "M Zhou"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "28",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning, ICML 2021"
    },
    {
      "citation_id": "29",
      "title": "Vlmo: Unified vision-language pretraining with mixture-of-modality-experts",
      "authors": [
        "H Bao",
        "W Wang",
        "L Dong",
        "Q Liu",
        "O Mohammed",
        "K Aggarwal",
        "S Som",
        "S Piao",
        "F Wei"
      ],
      "year": "2022",
      "venue": "Vlmo: Unified vision-language pretraining with mixture-of-modality-experts"
    },
    {
      "citation_id": "30",
      "title": "Coca: Contrastive captioners are image-text foundation models",
      "authors": [
        "J Yu",
        "Z Wang",
        "V Vasudevan",
        "L Yeung",
        "M Seyedhosseini",
        "Y Wu"
      ],
      "year": "2022",
      "venue": "Trans. Mach. Learn. Res"
    },
    {
      "citation_id": "31",
      "title": "VATT: transformers for multimodal self-supervised learning from raw video, audio and text",
      "authors": [
        "H Akbari",
        "L Yuan",
        "R Qian",
        "W Chuang",
        "S Chang",
        "Y Cui",
        "B Gong"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021"
    },
    {
      "citation_id": "32",
      "title": "Parameter-efficient transfer learning for NLP",
      "authors": [
        "N Houlsby",
        "A Giurgiu",
        "S Jastrzebski",
        "B Morrone",
        "Q De Laroussilhe",
        "A Gesmundo",
        "M Attariyan",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "33",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "X Li",
        "P Liang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "34",
      "title": "Finetuned language models are zero-shot learners",
      "authors": [
        "J Wei",
        "M Bosma",
        "V Zhao",
        "K Guu",
        "A Yu",
        "B Lester",
        "N Du",
        "A Dai",
        "Q Le"
      ],
      "year": "2021",
      "venue": "Finetuned language models are zero-shot learners",
      "arxiv": "arXiv:2109.01652"
    },
    {
      "citation_id": "35",
      "title": "Language models are fewshot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "36",
      "title": "A survey on in-context learning",
      "authors": [
        "Q Dong",
        "L Li",
        "D Dai",
        "C Zheng",
        "Z Wu",
        "B Chang",
        "X Sun",
        "J Xu",
        "Z Sui"
      ],
      "year": "2022",
      "venue": "A survey on in-context learning",
      "arxiv": "arXiv:2301.00234"
    },
    {
      "citation_id": "37",
      "title": "Multimodal prompt transformer with hybrid contrastive learning for emotion recognition in conversation",
      "authors": [
        "S Zou",
        "X Huang",
        "X Shen"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "38",
      "title": "Unimse: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "G Hu",
        "T Lin",
        "Y Zhao",
        "G Lu",
        "Y Wu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Affective computing in the era of large language models: A survey from the nlp perspective",
      "authors": [
        "Y Zhang",
        "X Yang",
        "X Xu",
        "Z Gao",
        "Y Huang",
        "S Mu",
        "S Feng",
        "D Wang",
        "Y Zhang",
        "K Song"
      ],
      "year": "2024",
      "venue": "Affective computing in the era of large language models: A survey from the nlp perspective",
      "arxiv": "arXiv:2408.04638"
    },
    {
      "citation_id": "40",
      "title": "A review of multimodal emotion recognition from datasets, preprocessing, features, and fusion methods",
      "authors": [
        "B Pan",
        "K Hirota",
        "Z Jia",
        "Y Dai"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "41",
      "title": "Emotion recognition from unimodal to multimodal analysis: A review",
      "authors": [
        "K Ezzameli",
        "H Mahersia"
      ],
      "year": "2023",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "42",
      "title": "A review of chinese sentiment analysis: Subjects, methods, and trends",
      "authors": [
        "Z Wang",
        "X Zhang",
        "J Cui",
        "S.-B Ho",
        "E Cambria"
      ],
      "venue": "A review of chinese sentiment analysis: Subjects, methods, and trends"
    },
    {
      "citation_id": "43",
      "title": "Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions",
      "authors": [
        "A Gandhi",
        "K Adhvaryu",
        "S Poria",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "44",
      "title": "Multimodal sentiment analysis based on fusion methods: A survey",
      "authors": [
        "L Zhu",
        "Z Zhu",
        "C Zhang",
        "Y Xu",
        "X Kong"
      ],
      "year": "2023",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "45",
      "title": "Sentiment classification using document embeddings trained with cosine similarity",
      "authors": [
        "T Thongtan",
        "T Phienthrakul"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics"
    },
    {
      "citation_id": "46",
      "title": "Towards multimodal sentiment analysis: harvesting opinions from the web",
      "authors": [
        "L Morency",
        "R Mihalcea",
        "P Doshi"
      ],
      "year": "2011",
      "venue": "Proceedings of the 13th International Conference on Multimodal Interfaces, ICMI 2011"
    },
    {
      "citation_id": "47",
      "title": "Survey on multimodal approaches to emotion recognition",
      "authors": [
        "V Vetriselvi"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "48",
      "title": "A discourse-aware graph neural network for emotion recognition in multi-party conversation",
      "authors": [
        "Y Sun",
        "N Yu",
        "G Fu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana"
    },
    {
      "citation_id": "49",
      "title": "COGMEN: contextualized GNN based multimodal emotion recognition",
      "authors": [
        "A Joshi",
        "A Bhat",
        "A Jain",
        "A Singh",
        "A Modi"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "50",
      "title": "Multi-interactive memory network for aspect based multimodal sentiment analysis",
      "authors": [
        "N Xu",
        "W Mao",
        "G Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "Transfer capsule network for aspect level sentiment classification",
      "authors": [
        "Z Chen",
        "T Qian"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics"
    },
    {
      "citation_id": "52",
      "title": "A unified generative framework for aspect-based sentiment analysis",
      "authors": [
        "H Yan",
        "J Dai",
        "T Ji",
        "X Qiu",
        "Z Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "53",
      "title": "Sentiprompt: Sentiment knowledge enhanced prompt-tuning for aspect-based sentiment analysis",
      "authors": [
        "C Li",
        "F Gao",
        "J Bu",
        "L Xu",
        "X Chen",
        "Y Gu",
        "Z Shao",
        "Q Zheng",
        "N Zhang",
        "Y Wang",
        "Z Yu"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "54",
      "title": "SGM: sequence generation model for multi-label classification",
      "authors": [
        "P Yang",
        "X Sun",
        "W Li",
        "S Ma",
        "W Wu",
        "H Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics, COLING 2018"
    },
    {
      "citation_id": "55",
      "title": "Label-specific dual graph neural network for multi-label text classification",
      "authors": [
        "Q Ma",
        "C Yuan",
        "W Zhou",
        "S Hu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "56",
      "title": "Distributed representations of words and phrases and their compositionality",
      "authors": [
        "T Mikolov",
        "I Sutskever",
        "K Chen",
        "G Corrado",
        "J Dean"
      ],
      "year": "2013",
      "venue": "Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of a meeting"
    },
    {
      "citation_id": "57",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "J Pennington",
        "R Socher",
        "C Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "58",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "59",
      "title": "BART: denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020"
    },
    {
      "citation_id": "60",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "J. Mach. Learn. Res"
    },
    {
      "citation_id": "61",
      "title": "Llama: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar",
        "A Rodriguez",
        "A Joulin",
        "E Grave",
        "G Lample"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "62",
      "title": "",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale",
        "D Bikel",
        "L Blecher",
        "C Canton-Ferrer",
        "M Chen",
        "G Cucurull",
        "D Esiobu",
        "J Fernandes",
        "J Fu",
        "W Fu",
        "B Fuller",
        "C Gao",
        "V Goswami",
        "N Goyal",
        "A Hartshorn",
        "S Hosseini",
        "R Hou",
        "H Inan",
        "M Kardas",
        "V Kerkez",
        "M Khabsa",
        "I Kloumann",
        "A Korenev",
        "P Koura",
        "M Lachaux",
        "T Lavril",
        "J Lee",
        "D Liskovich",
        "Y Lu",
        "Y Mao",
        "X Martinet",
        "T Mihaylov",
        "P Mishra",
        "I Molybog",
        "Y Nie",
        "A Poulton",
        "J Reizenstein",
        "R Rungta",
        "K Saladi",
        "A Schelten",
        "R Silva",
        "E Smith",
        "R Subramanian",
        "X Tan",
        "B Tang",
        "R Taylor",
        "A Williams",
        "J Kuan",
        "P Xu",
        "Z Yan",
        "I Zarov",
        "Y Zhang",
        "A Fan",
        "M Kambadur",
        "S Narang",
        "A Rodriguez",
        "R Stojnic",
        "S Edunov",
        "T Scialom"
      ],
      "year": "2023",
      "venue": ""
    },
    {
      "citation_id": "63",
      "title": "Mamba: Linear-time sequence modeling with selective state spaces",
      "authors": [
        "A Gu",
        "T Dao"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "64",
      "title": "AST: audio spectrogram transformer",
      "authors": [
        "Y Gong",
        "Y Chung",
        "J Glass"
      ],
      "year": "2021",
      "venue": "22nd Annual Conference of the International Speech Communication Association, Interspeech 2021"
    },
    {
      "citation_id": "65",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proceedings of the 36th International Conference on Machine Learning, ICML"
    },
    {
      "citation_id": "66",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale,\" in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale,\" in 9th International Conference on Learning Representations, ICLR 2021, Virtual Event"
    },
    {
      "citation_id": "67",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "68",
      "title": "Gpt-4v(ision) system card",
      "year": "2023",
      "venue": "Gpt-4v(ision) system card"
    },
    {
      "citation_id": "69",
      "title": "Gemini: a family of highly capable multimodal models",
      "authors": [
        "G Team",
        "R Anil",
        "S Borgeaud",
        "Y Wu",
        "J.-B Alayrac",
        "J Yu",
        "R Soricut",
        "J Schalkwyk",
        "A Dai",
        "A Hauth"
      ],
      "year": "2023",
      "venue": "Gemini: a family of highly capable multimodal models",
      "arxiv": "arXiv:2312.11805"
    },
    {
      "citation_id": "70",
      "title": "Flamingo: a visual language model for few-shot learning",
      "authors": [
        "J.-B Alayrac",
        "J Donahue",
        "P Luc",
        "A Miech",
        "I Barr",
        "Y Hasson",
        "K Lenc",
        "A Mensch",
        "K Millican",
        "M Reynolds"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "71",
      "title": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Blip-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "arxiv": "arXiv:2301.12597"
    },
    {
      "citation_id": "72",
      "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "D Zhu",
        "J Chen",
        "X Shen",
        "X Li",
        "M Elhoseiny"
      ],
      "year": "2023",
      "venue": "Minigpt-4: Enhancing vision-language understanding with advanced large language models",
      "arxiv": "arXiv:2304.10592"
    },
    {
      "citation_id": "73",
      "title": "Instructblip: Towards general-purpose visionlanguage models with instruction tuning",
      "authors": [
        "W Dai",
        "J Li",
        "D Li",
        "A Tiong",
        "J Zhao",
        "W Wang",
        "B Li",
        "P Fung",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Instructblip: Towards general-purpose visionlanguage models with instruction tuning"
    },
    {
      "citation_id": "74",
      "title": "Visual instruction tuning",
      "authors": [
        "H Liu",
        "C Li",
        "Q Wu",
        "Y Lee"
      ],
      "year": "2024",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "75",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "H Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "Y Li",
        "X Wang",
        "M Dehghani",
        "S Brahma"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "76",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "77",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019"
    },
    {
      "citation_id": "78",
      "title": "Inter-intra modal representation augmentation with trimodal collaborative disentanglement network for multimodal sentiment analysis",
      "authors": [
        "C Chen",
        "H Hong",
        "J Guo",
        "B Song"
      ],
      "year": "2023",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "79",
      "title": "CM-BERT: cross-modal BERT for textaudio sentiment analysis",
      "authors": [
        "K Yang",
        "H Xu",
        "K Gao"
      ],
      "year": "2020",
      "venue": "MM '20: The 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "80",
      "title": "Modeling intra-and inter-modal relations: Hierarchical graph contrastive learning for multimodal sentiment analysis",
      "authors": [
        "Z Lin",
        "B Liang",
        "Y Long",
        "Y Dang",
        "M Yang",
        "M Zhang",
        "R Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics, COLING 2022"
    },
    {
      "citation_id": "81",
      "title": "BAFN: bi-direction attention based fusion network for multimodal sentiment analysis",
      "authors": [
        "J Tang",
        "D Liu",
        "X Jin",
        "Y Peng",
        "Q Zhao",
        "Y Ding",
        "W Kong"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Circuits Syst. Video Technol"
    },
    {
      "citation_id": "82",
      "title": "Tefna: Text-centered fusion network with crossmodal attention for multimodal sentiment analysis",
      "authors": [
        "C Huang",
        "J Zhang",
        "X Wu",
        "Y Wang",
        "M Li",
        "X Huang"
      ],
      "year": "2023",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "83",
      "title": "A cross-modality context fusion and semantic refinement network for emotion recognition in conversation",
      "authors": [
        "X Zhang",
        "Y Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "84",
      "title": "Multiemo: An attention-based correlation-aware multimodal fusion framework for emotion recognition in conversations",
      "authors": [
        "T Shi",
        "S Huang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "85",
      "title": "Mmrbn: Rule-based network for multimodal emotion recognition",
      "authors": [
        "X Chen"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "86",
      "title": "Magdra: a multimodal attention graph network with dynamic routing-by-agreement for multi-label emotion recognition",
      "authors": [
        "X Li",
        "J Liu",
        "Y Xie",
        "P Gong",
        "X Zhang",
        "H He"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "87",
      "title": "Amuse: Adaptive multimodal analysis for speaker emotion recognition in group conversations",
      "authors": [
        "N Devulapally",
        "S Anand",
        "S Bhattacharjee",
        "J Yuan",
        "Y Chang"
      ],
      "year": "2024",
      "venue": "CoRR"
    },
    {
      "citation_id": "88",
      "title": "Improving multimodal fusion with hierarchical mutual information maximization for multimodal sentiment analysis",
      "authors": [
        "W Han",
        "H Chen",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana"
    },
    {
      "citation_id": "89",
      "title": "Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis",
      "authors": [
        "S Mai",
        "Y Zeng",
        "S Zheng",
        "H Hu"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "90",
      "title": "Multimodal representations learning based on mutual information maximization and minimization and identity embedding for multimodal sentiment analysis",
      "authors": [
        "J Zheng",
        "S Zhang",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2022",
      "venue": "Multimodal representations learning based on mutual information maximization and minimization and identity embedding for multimodal sentiment analysis",
      "arxiv": "arXiv:2201.03969"
    },
    {
      "citation_id": "91",
      "title": "MISA: modality-invariant and -specific representations for multimodal sentiment analysis",
      "authors": [
        "D Hazarika",
        "R Zimmermann",
        "S Poria"
      ],
      "year": "2020",
      "venue": "MM '20: The 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "92",
      "title": "Cross-modal fine-grained alignment and fusion network for multimodal aspect-based sentiment analysis",
      "authors": [
        "L Xiao",
        "X Wu",
        "S Yang",
        "J Xu",
        "J Zhou",
        "L He"
      ],
      "year": "2023",
      "venue": "Inf. Process. Manag"
    },
    {
      "citation_id": "93",
      "title": "Modalnet: an aspect-level sentiment classification model by exploring multimodal data with fusion discriminant attentional network",
      "authors": [
        "Z Zhang",
        "Z Wang",
        "X Li",
        "N Liu",
        "B Guo",
        "Z Yu"
      ],
      "year": "2021",
      "venue": "World Wide Web"
    },
    {
      "citation_id": "94",
      "title": "Tailor versatile multimodal learning for multi-label emotion recognition",
      "authors": [
        "Y Zhang",
        "M Chen",
        "J Shen",
        "C Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "95",
      "title": "Learning robust multi-modal representation for multi-label emotion recognition via adversarial masking and perturbation",
      "authors": [
        "S Ge",
        "Z Jiang",
        "Z Cheng",
        "C Wang",
        "Y Yin",
        "Q Gu"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM Web Conference 2023"
    },
    {
      "citation_id": "96",
      "title": "A novel spatiotemporal convolutional neural framework for multimodal emotion recognition",
      "authors": [
        "M Sharafi",
        "M Yazdchi",
        "R Rasti",
        "F Nasimi"
      ],
      "year": "2022",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "97",
      "title": "Tscl-fhfn: two-stage contrastive learning and feature hierarchical fusion network for multimodal sentiment analysis",
      "authors": [
        "Y Li",
        "W Weng",
        "C Liu"
      ],
      "year": "2024",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "98",
      "title": "Divide, conquer and combine: Hierarchical feature fusion network with local and global perspectives for multimodal affective computing",
      "authors": [
        "S Mai",
        "H Hu",
        "S Xing"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019"
    },
    {
      "citation_id": "99",
      "title": "CLMLF: A contrastive learning and multi-layer fusion method for multimodal sentiment detection",
      "authors": [
        "Z Li",
        "B Xu",
        "C Zhu",
        "T Zhao"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2022"
    },
    {
      "citation_id": "100",
      "title": "Multimodal language analysis with recurrent multistage fusion",
      "authors": [
        "P Liang",
        "Z Liu",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "101",
      "title": "CTFN: hierarchical learning for multimodal sentiment analysis using coupledtranslation fusion network",
      "authors": [
        "J Tang",
        "K Li",
        "X Jin",
        "A Cichocki",
        "Q Zhao",
        "W Kong"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "102",
      "title": "Multi-level correlation mining framework with self-supervised label generation for multimodal sentiment analysis",
      "authors": [
        "Z Li",
        "Q Guo",
        "Y Pan",
        "W Ding",
        "J Yu",
        "Y Zhang",
        "W Liu",
        "H Chen",
        "H Wang",
        "Y Xie"
      ],
      "year": "2023",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "103",
      "title": "A fine-grained modal label-based multi-stage network for multimodal sentiment analysis",
      "authors": [
        "J Peng",
        "T Wu",
        "W Zhang",
        "F Cheng",
        "S Tan",
        "F Yi",
        "Y Huang"
      ],
      "year": "2023",
      "venue": "Expert Syst. Appl"
    },
    {
      "citation_id": "104",
      "title": "Scalevlad: Improving multimodal sentiment analysis via multi-scale fusion of locally descriptors",
      "authors": [
        "H Luo",
        "L Ji",
        "Y Huang",
        "B Wang",
        "S Ji",
        "T Li"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "105",
      "title": "Meta-learn unimodal signals with weak supervision for multimodal sentiment analysis",
      "authors": [
        "S Mai",
        "Y Zhao",
        "Y Zeng",
        "J Yao",
        "H Hu"
      ],
      "year": "2024",
      "venue": "Meta-learn unimodal signals with weak supervision for multimodal sentiment analysis",
      "arxiv": "arXiv:2408.16029"
    },
    {
      "citation_id": "106",
      "title": "Multimodal emotion recognition based on hierarchical fusion strategy and contextual information embedding",
      "authors": [
        "S Minglong",
        "O Chunping",
        "L Yongbin",
        "R Lin"
      ],
      "year": "2024",
      "venue": "Beijing Da Xue Xue Bao"
    },
    {
      "citation_id": "107",
      "title": "MTAG: modal-temporal attention graph for unaligned human multimodal language sequences",
      "authors": [
        "J Yang",
        "Y Wang",
        "R Yi",
        "Y Zhu",
        "A Rehman",
        "A Zadeh",
        "S Poria",
        "L Morency"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021"
    },
    {
      "citation_id": "108",
      "title": "Multimodal sentiment analysis using hierarchical fusion with context modeling",
      "authors": [
        "N Majumder",
        "D Hazarika",
        "A Gelbukh",
        "E Cambria",
        "S Poria"
      ],
      "year": "2018",
      "venue": "Knowledge-based systems"
    },
    {
      "citation_id": "109",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "J Zhao",
        "R Li",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "110",
      "title": "Training strategies to handle missing modalities for audio-visual expression recognition",
      "authors": [
        "S Parthasarathy",
        "S Sundaram"
      ],
      "year": "2020",
      "venue": "Companion Publication of the 2020 International Conference on Multimodal Interaction, ICMI Companion 2020, Virtual Event"
    },
    {
      "citation_id": "111",
      "title": "M2R2: missing-modality robust emotion recognition framework with iterative data augmentation",
      "authors": [
        "N Wang",
        "H Cao",
        "J Zhao",
        "R Chen",
        "D Yan",
        "J Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Artif. Intell"
    },
    {
      "citation_id": "112",
      "title": "Mitigating inconsistencies in multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "J Zeng",
        "J Zhou",
        "T Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "113",
      "title": "Transformer-based feature reconstruction network for robust multimodal sentiment analysis",
      "authors": [
        "Z Yuan",
        "W Li",
        "H Xu",
        "W Yu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "114",
      "title": "Multimodal reconstruct and align net for missing modality problem in sentiment analysis",
      "authors": [
        "W Luo",
        "M Xu",
        "H Lai"
      ],
      "year": "2023",
      "venue": "MultiMedia Modeling -29th International Conference, MMM 2023"
    },
    {
      "citation_id": "115",
      "title": "VIGAN: missing view imputation with generative adversarial networks",
      "authors": [
        "C Shang",
        "A Palmer",
        "J Sun",
        "K Chen",
        "J Lu",
        "J Bi"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Big Data"
    },
    {
      "citation_id": "116",
      "title": "Tag-assisted multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "J Zeng",
        "T Liu",
        "J Zhou"
      ],
      "year": "2022",
      "venue": "SIGIR '22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    {
      "citation_id": "117",
      "title": "Exploiting modalityinvariant feature for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "H Zuo",
        "R Liu",
        "J Zhao",
        "G Gao",
        "H Li"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023"
    },
    {
      "citation_id": "118",
      "title": "Modality translationbased multimodal sentiment analysis under uncertain missing modalities",
      "authors": [
        "Z Liu",
        "B Zhou",
        "D Chu",
        "Y Sun",
        "L Meng"
      ],
      "year": "2024",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "119",
      "title": "Feature-enhanced generation and multi-modality fusion based deep neural network for brain tumor segmentation with missing MR modalities",
      "authors": [
        "T Zhou",
        "S Canu",
        "P Vera",
        "S Ruan"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "120",
      "title": "Accommodating missing modalities in time-continuous multimodal emotion recognition",
      "authors": [
        "J Vazquez-Rodriguez",
        "G Lefebvre",
        "J Cumin",
        "J Crowley"
      ],
      "year": "2023",
      "venue": "11th International Conference on Affective Computing and Intelligent Interaction, ACII 2023"
    },
    {
      "citation_id": "121",
      "title": "Robust-msa: Understanding the impact of modality noise on multimodal sentiment analysis",
      "authors": [
        "H Mao",
        "B Zhang",
        "H Xu",
        "Z Yuan",
        "Y Liu"
      ],
      "year": "2023",
      "venue": "Thirty-Seventh AAAI Conference on Artificial Intelligence, AAAI 2023, Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence, IAAI 2023, Thirteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2023"
    },
    {
      "citation_id": "122",
      "title": "Shared and private information learning in multimodal sentiment analysis with deep modal alignment and self-supervised multi-task learning",
      "authors": [
        "S Lai",
        "X Hu",
        "Y Li",
        "Z Ren",
        "Z Liu",
        "D Miao"
      ],
      "year": "2023",
      "venue": "Shared and private information learning in multimodal sentiment analysis with deep modal alignment and self-supervised multi-task learning",
      "arxiv": "arXiv:2305.08473"
    },
    {
      "citation_id": "123",
      "title": "A multi-level alignment and crossmodal unified semantic graph refinement network for conversational emotion recognition",
      "authors": [
        "X Zhang",
        "W Cui",
        "B Hu",
        "Y Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "124",
      "title": "Black holes and white rabbits: Metaphor identification with visual features",
      "authors": [
        "E Shutova",
        "D Kiela",
        "J Maillard"
      ],
      "year": "2016",
      "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "125",
      "title": "Multimodal transfer deep learning for audio visual recognition",
      "authors": [
        "S Moon",
        "S Kim",
        "H Wang"
      ],
      "year": "2014",
      "venue": "CoRR"
    },
    {
      "citation_id": "126",
      "title": "Recent advances in the automatic recognition of audiovisual speech",
      "authors": [
        "G Potamianos",
        "C Neti",
        "G Gravier",
        "A Garg",
        "A Senior"
      ],
      "year": "2003",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "127",
      "title": "Multimodal saliency and fusion for movie summarization based on aural, visual, and textual attention",
      "authors": [
        "G Evangelopoulos",
        "A Zlatintsi",
        "A Potamianos",
        "P Maragos",
        "K Rapantzikos",
        "G Skoumas",
        "Y Avrithis"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Multim"
    },
    {
      "citation_id": "128",
      "title": "Multiple classifier systems for the classification of audio-visual emotional states",
      "authors": [
        "M Glodek",
        "S Tschechne",
        "G Layher",
        "M Schels",
        "T Brosch",
        "S Scherer",
        "M Kächele",
        "M Schmidt",
        "H Neumann",
        "G Palm",
        "F Schwenker"
      ],
      "year": "2011",
      "venue": "Affective Computing and Intelligent Interaction -Fourth International Conference, ACII 2011"
    },
    {
      "citation_id": "129",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "130",
      "title": "Deep learning based multimodal emotion recognition using model-level fusion of audio-visual modalities",
      "authors": [
        "A Middya",
        "B Nag",
        "S Roy"
      ],
      "year": "2022",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "131",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltrušaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "132",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2009",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "133",
      "title": "Semi-supervised deep generative modelling of incomplete multimodality emotional data",
      "authors": [
        "C Du",
        "C Du",
        "H Wang",
        "J Li",
        "W.-L Zheng",
        "B.-L Lu",
        "H He"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "134",
      "title": "Transmodality: An end2end fusion method with transformer for multimodal sentiment analysis",
      "authors": [
        "Z Wang",
        "Z Wan",
        "X Wan"
      ],
      "year": "2020",
      "venue": "Transmodality: An end2end fusion method with transformer for multimodal sentiment analysis"
    },
    {
      "citation_id": "135",
      "title": "An efficient approach for audio-visual emotion recognition with missing labels and missing modalities",
      "authors": [
        "F Ma",
        "S Huang",
        "L Zhang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Multimedia and Expo, ICME 2021"
    },
    {
      "citation_id": "136",
      "title": "Learning modality-specific representations with self-supervised multi-task learning for multimodal sentiment analysis",
      "authors": [
        "W Yu",
        "H Xu",
        "Z Yuan",
        "J Wu"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event"
    },
    {
      "citation_id": "137",
      "title": "Modality to modality translation: An adversarial representation learning and graph fusion network for multimodal fusion",
      "authors": [
        "S Mai",
        "H Hu",
        "S Xing"
      ],
      "year": "2020",
      "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "138",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "authors": [
        "M Akhtar",
        "D Chauhan",
        "D Ghosal",
        "S Poria",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019"
    },
    {
      "citation_id": "139",
      "title": "Video-based cross-modal auxiliary network for multimodal sentiment analysis",
      "authors": [
        "R Chen",
        "W Zhou",
        "Y Li",
        "H Zhou"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Circuits Syst. Video Technol"
    },
    {
      "citation_id": "140",
      "title": "Disentanglement translation network for multimodal sentiment analysis",
      "authors": [
        "Y Zeng",
        "W Yan",
        "S Mai",
        "H Hu"
      ],
      "year": "2024",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "141",
      "title": "Using auxiliary tasks in multimodal fusion of wav2vec 2.0 and BERT for multimodal emotion recognition",
      "authors": [
        "D Sun",
        "Y He",
        "J Han"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "142",
      "title": "Tdfnet: Transformerbased deep-scale fusion network for multimodal emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Wang",
        "G Shen",
        "Y Xu",
        "J Zhang"
      ],
      "year": "2023",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "143",
      "title": "MALN: multimodal adversarial learning network for conversational emotion recognition",
      "authors": [
        "M Ren",
        "X Huang",
        "J Liu",
        "M Liu",
        "X Li",
        "A Liu"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Circuits Syst. Video Technol"
    },
    {
      "citation_id": "144",
      "title": "LGCCT: A light gated and crossed complementation transformer for multimodal speech emotion recognition",
      "authors": [
        "F Liu",
        "S Shen",
        "Z Fu",
        "H Wang",
        "A Zhou",
        "J Qi"
      ],
      "year": "2022",
      "venue": "Entropy"
    },
    {
      "citation_id": "145",
      "title": "Rl-emo: A reinforcement learning framework for multimodal emotion recognition",
      "authors": [
        "C Zhang",
        "Y Zhang",
        "B Cheng"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "146",
      "title": "Cross-modal multitask transformer for end-to-end multimodal aspect-based sentiment analysis",
      "authors": [
        "L Yang",
        "J Na",
        "J Yu"
      ],
      "year": "2022",
      "venue": "Inf. Process. Manag"
    },
    {
      "citation_id": "147",
      "title": "Abcord: Exploiting multimodal generative approach for aspect-based complaint and rationale detection",
      "authors": [
        "R Jain",
        "A Singh",
        "V Gangwar",
        "S Saha"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia, MM 2023"
    },
    {
      "citation_id": "148",
      "title": "Joint multi-modal aspect-sentiment analysis with auxiliary crossmodal relation detection",
      "authors": [
        "X Ju",
        "D Zhang",
        "R Xiao",
        "J Li",
        "S Li",
        "M Zhang",
        "G Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana"
    },
    {
      "citation_id": "149",
      "title": "Label distribution adaptation for multimodal emotion recognition with multi-label learning",
      "authors": [
        "H Lian",
        "C Lu",
        "S Li",
        "Y Zhao",
        "C Tang",
        "Y Zong",
        "W Zheng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 1st International Workshop on Multimodal and Responsible Affective Computing, MRAC 2023"
    },
    {
      "citation_id": "150",
      "title": "Aobert: All-modalities-in-one bert for multimodal sentiment analysis",
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "151",
      "title": "Sentiment knowledge enhanced self-supervised learning for multimodal sentiment analysis",
      "authors": [
        "F Qian",
        "J Han",
        "Y He",
        "T Zheng",
        "G Zheng"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "152",
      "title": "TEASEL: A transformerbased speech-prefixed language model",
      "authors": [
        "M Arjmand",
        "M Dousti",
        "H Moradi"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "153",
      "title": "Adapting BERT for target-oriented multimodal sentiment classification",
      "authors": [
        "J Yu",
        "J Jiang"
      ],
      "year": "2019",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "154",
      "title": "Multimodal phased transformer for sentiment analysis",
      "authors": [
        "J Cheng",
        "I Fostiropoulos",
        "B Boehm",
        "M Soleymani"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana"
    },
    {
      "citation_id": "155",
      "title": "Learning language-guided adaptive hyper-modality representation for multimodal sentiment analysis",
      "authors": [
        "H Zhang",
        "Y Wang",
        "G Yin",
        "K Liu",
        "Y Liu",
        "T Yu"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "156",
      "title": "Tracing intricate cues in dialogue: Joint graph structure and sentiment dynamics for multimodal emotion recognition",
      "authors": [
        "J Li",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "Tracing intricate cues in dialogue: Joint graph structure and sentiment dynamics for multimodal emotion recognition",
      "arxiv": "arXiv:2407.21536"
    },
    {
      "citation_id": "157",
      "title": "Entity-related unsupervised pretraining with visual prompts for multimodal aspect-based sentiment analysis",
      "authors": [
        "K Liu",
        "J Wang",
        "X Zhang"
      ],
      "year": "2023",
      "venue": "Natural Language Processing and Chinese Computing -12th National CCF Conference, NLPCC 2023"
    },
    {
      "citation_id": "158",
      "title": "Vision-language pre-training for multimodal aspect-based sentiment analysis",
      "authors": [
        "Y Ling",
        "J Yu",
        "R Xia"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "159",
      "title": "Incorporating dynamic semantics into pre-trained language model for aspect-based sentiment analysis",
      "authors": [
        "K Zhang",
        "K Zhang",
        "M Zhang",
        "H Zhao",
        "Q Liu",
        "W Wu",
        "E Chen"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "160",
      "title": "Dual-encoder transformers with cross-modal alignment for multimodal aspect-based sentiment analysis",
      "authors": [
        "Z Yu",
        "J Wang",
        "L Yu",
        "X Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing, AACL/IJCNLP 2022"
    },
    {
      "citation_id": "161",
      "title": "MSRA: A multi-aspect semantic relevance approach for e-commerce via multimodal pre-training",
      "authors": [
        "H Jin",
        "J Tan",
        "L Liu",
        "L Qiu",
        "S Yao",
        "X Chen",
        "X Zeng"
      ],
      "year": "2023",
      "venue": "Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, CIKM 2023"
    },
    {
      "citation_id": "162",
      "title": "Image-to-text conversion and aspect-oriented filtration for multimodal aspect-based sentiment analysis",
      "authors": [
        "Q Wang",
        "H Xu",
        "Z Wen",
        "B Liang",
        "M Yang",
        "B Qin",
        "R Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "163",
      "title": "An adaptive dual graph convolution fusion network for aspect-based sentiment analysis",
      "authors": [
        "C Wang",
        "Y Luo",
        "C Meng",
        "F Yuan"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "164",
      "title": "Mocolnet: A momentum contrastive learning network for multimodal aspectlevel sentiment analysis",
      "authors": [
        "J Mu",
        "F Nie",
        "W Wang",
        "J Xu",
        "J Zhang",
        "H Liu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "165",
      "title": "TETFN: A text enhanced transformer fusion network for multimodal sentiment analysis",
      "authors": [
        "D Wang",
        "X Guo",
        "Y Tian",
        "J Liu",
        "L He",
        "X Luo"
      ],
      "year": "2023",
      "venue": "Pattern Recognit"
    },
    {
      "citation_id": "166",
      "title": "SKEAFN: sentiment knowledge enhanced attention fusion network for multimodal sentiment analysis",
      "authors": [
        "C Zhu",
        "M Chen",
        "S Zhang",
        "C Sun",
        "H Liang",
        "Y Liu",
        "J Chen"
      ],
      "year": "2023",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "167",
      "title": "Swafn: Sentimental words aware fusion network for multimodal sentiment analysis",
      "authors": [
        "M Chen",
        "X Li"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th international conference on computational linguistics"
    },
    {
      "citation_id": "168",
      "title": "Context-and knowledge-aware graph convolutional network for multimodal emotion recognition",
      "authors": [
        "Y Fu",
        "S Okada",
        "L Wang",
        "L Guo",
        "Y Song",
        "J Liu",
        "J Dang"
      ],
      "year": "2022",
      "venue": "IEEE Multim"
    },
    {
      "citation_id": "169",
      "title": "Decoupled multimodal distilling for emotion recognition",
      "authors": [
        "Y Li",
        "Y Wang",
        "Z Cui"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023"
    },
    {
      "citation_id": "170",
      "title": "Multimodal rough set transformer for sentiment analysis and emotion recognition",
      "authors": [
        "X Sun",
        "H He",
        "H Tang",
        "K Zeng",
        "T Shen"
      ],
      "year": "2023",
      "venue": "th IEEE International Conference on Cloud Computing and Intelligent Systems, CCIS 2023"
    },
    {
      "citation_id": "171",
      "title": "Leveraging label information for multimodal emotion recognition",
      "authors": [
        "P Wang",
        "S Zeng",
        "J Chen",
        "L Fan",
        "M Chen",
        "Y Wu",
        "X He"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "172",
      "title": "Topics guided multimodal fusion network for conversational emotion recognition",
      "authors": [
        "P Yuan",
        "G Cai",
        "M Chen",
        "X Tang"
      ],
      "year": "2024",
      "venue": "International Conference on Intelligent Computing"
    },
    {
      "citation_id": "173",
      "title": "Deep emotional arousal network for multimodal sentiment analysis and emotion recognition",
      "authors": [
        "F Zhang",
        "X Li",
        "C Lim",
        "Q Hua",
        "C Dong",
        "J Zhai"
      ],
      "year": "2022",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "174",
      "title": "Multimodal aspect-based sentiment classification with knowledge-injected transformer",
      "authors": [
        "Z Xu",
        "Q Su",
        "J Xiao"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Multimedia and Expo, ICME 2023"
    },
    {
      "citation_id": "175",
      "title": "Face-sensitive image-to-emotionaltext cross-modal translation for multimodal aspect-based sentiment analysis",
      "authors": [
        "H Yang",
        "Y Zhao",
        "B Qin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "176",
      "title": "Cross-modal fine-grained alignment and fusion network for multimodal aspect-based sentiment analysis",
      "authors": [
        "L Xiao",
        "X Wu",
        "S Yang",
        "J Xu",
        "J Zhou",
        "L He"
      ],
      "year": "2023",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "177",
      "title": "Hierarchical interactive multimodal transformer for aspect-based multimodal sentiment analysis",
      "authors": [
        "J Yu",
        "K Chen",
        "R Xia"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "178",
      "title": "A unimodal valence-arousal driven contrastive learning framework for multimodal multi-label emotion recognition",
      "authors": [
        "W Zheng",
        "J Yu",
        "R Xia"
      ],
      "year": "2024",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "179",
      "title": "Carat: Contrastive feature reconstruction and aggregation for multi-modal multi-label emotion recognition",
      "authors": [
        "C Peng",
        "K Chen",
        "L Shou",
        "G Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "180",
      "title": "M3tr: Multi-modal multi-label recognition with transformer",
      "authors": [
        "J Zhao",
        "Y Zhao",
        "J Li"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM international conference on multimedia"
    },
    {
      "citation_id": "181",
      "title": "Multimodal multi-label emotion recognition with heterogeneous hierarchical message passing",
      "authors": [
        "D Zhang",
        "X Ju",
        "W Zhang",
        "J Li",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "182",
      "title": "Contextaware interactive attention for multi-modal sentiment and emotion analysis",
      "authors": [
        "D Chauhan",
        "M Akhtar",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019"
    },
    {
      "citation_id": "183",
      "title": "Multi-level multiple attentions for contextual multimodal sentiment analysis",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Data Mining, ICDM 2017"
    },
    {
      "citation_id": "184",
      "title": "Context-based adaptive multimodal fusion network for continuous frame-level sentiment prediction",
      "authors": [
        "M Huang",
        "C Qing",
        "J Tan",
        "X Xu"
      ],
      "year": "2023",
      "venue": "IEEE ACM Trans. Audio Speech Lang. Process"
    },
    {
      "citation_id": "185",
      "title": "Ctnet: Context-based tandem network for semantic segmentation",
      "authors": [
        "Z Li",
        "Y Sun",
        "L Zhang",
        "J Tang"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell"
    },
    {
      "citation_id": "186",
      "title": "A novel multimodal sentiment analysis model based on gated fusion and multi-task learning",
      "authors": [
        "X Sun",
        "X Ren",
        "X Xie"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "187",
      "title": "MMGCN: multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "188",
      "title": "MM-DFN: multimodal dynamic fusion network for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "X Hou",
        "L Wei",
        "L Jiang",
        "Y Mo"
      ],
      "year": "2022",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2022, Virtual and Singapore"
    },
    {
      "citation_id": "189",
      "title": "Structure aware multi-graph network for multi-modal emotion recognition in conversations",
      "authors": [
        "D Zhang",
        "F Chen",
        "J Chang",
        "X Chen",
        "Q Tian"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multim"
    },
    {
      "citation_id": "190",
      "title": "Multivariate, multi-frequency and multimodal: Rethinking graph neural networks for emotion recognition in conversation",
      "authors": [
        "F Chen",
        "J Shao",
        "S Zhu",
        "H Shen"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023"
    },
    {
      "citation_id": "191",
      "title": "Speaker-centric multimodal fusion networks for emotion recognition in conversations",
      "authors": [
        "B Yao",
        "W Shi"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "192",
      "title": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Z Li",
        "F Tang",
        "M Zhao",
        "Y Zhu"
      ],
      "year": "2022",
      "venue": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "arxiv": "arXiv:2203.13504"
    },
    {
      "citation_id": "193",
      "title": "GA2MIF: graph and attention based two-stage multi-source information fusion for conversational emotion detection",
      "authors": [
        "J Li",
        "X Wang",
        "G Lv",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "194",
      "title": "MCPR: A chinese product review dataset for multimodal aspect-based sentiment analysis",
      "authors": [
        "C Xu",
        "X Luo",
        "D Wang"
      ],
      "year": "2022",
      "venue": "Cognitive Computing -ICCC 2022 -6th International Conference, Held as Part of the Services Conference Federation"
    },
    {
      "citation_id": "195",
      "title": "Retrieving users' opinions on social media with multimodal aspect-based sentiment analysis",
      "authors": [
        "M Anschütz",
        "T Eder",
        "G Groh"
      ],
      "year": "2023",
      "venue": "17th IEEE International Conference on Semantic Computing, ICSC 2023"
    },
    {
      "citation_id": "196",
      "title": "M2DF: multi-grained multi-curriculum denoising framework for multimodal aspect-based sentiment analysis",
      "authors": [
        "F Zhao",
        "C Li",
        "Z Wu",
        "Y Ouyang",
        "J Zhang",
        "X Dai"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "197",
      "title": "Aom: Detecting aspect-oriented information for multimodal aspect-based sentiment analysis",
      "authors": [
        "R Zhou",
        "W Guo",
        "X Liu",
        "S Yu",
        "Y Zhang",
        "X Yuan"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "198",
      "title": "Fusion with gcn and se-resnext network for aspect based multimodal sentiment analysis",
      "authors": [
        "J Zhao",
        "F Yang"
      ],
      "year": "2023",
      "venue": "2023 IEEE 6th Information Technology, Networking, Electronic and Automation Control Conference (ITNEC)"
    },
    {
      "citation_id": "199",
      "title": "Multi-modal multi-label emotion detection with modality and label dependence",
      "authors": [
        "D Zhang",
        "X Ju",
        "J Li",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "200",
      "title": "Transformer-based label set generation for multi-modal multi-label emotion detection",
      "authors": [
        "X Ju",
        "D Zhang",
        "J Li",
        "G Zhou"
      ],
      "year": "2020",
      "venue": "MM '20: The 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "201",
      "title": "M3ED: multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "J Zhao",
        "T Zhang",
        "J Hu",
        "Y Liu",
        "Q Jin",
        "X Wang",
        "H Li"
      ],
      "year": "2022",
      "venue": "ACL 2022"
    },
    {
      "citation_id": "202",
      "title": "Scalevlad: Improving multimodal sentiment analysis via multi-scale fusion of locally descriptors",
      "authors": [
        "H Luo",
        "L Ji",
        "Y Huang",
        "B Wang",
        "S Ji",
        "T Li"
      ],
      "year": "2021",
      "venue": "Scalevlad: Improving multimodal sentiment analysis via multi-scale fusion of locally descriptors",
      "arxiv": "arXiv:2112.01368"
    },
    {
      "citation_id": "203",
      "title": "Multimodal speech emotion recognition using cross attention with aligned audio and text",
      "authors": [
        "Y Lee",
        "S Yoon",
        "K Jung"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "204",
      "title": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021",
      "authors": [
        "C Zong",
        "F Xia",
        "W Li",
        "R Navigli"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021"
    },
    {
      "citation_id": "205",
      "title": "TEDT: transformer-based encoding-decoding translation network for multimodal sentiment analysis",
      "authors": [
        "F Wang",
        "S Tian",
        "L Yu",
        "J Liu",
        "J Wang",
        "K Li",
        "Y Wang"
      ],
      "year": "2023",
      "venue": "Cogn. Comput"
    },
    {
      "citation_id": "206",
      "title": "Dual-encoder transformers with cross-modal alignment for multimodal aspect-based sentiment analysis",
      "authors": [
        "Z Yu",
        "J Wang",
        "L.-C Yu",
        "X Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "207",
      "title": "Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification",
      "authors": [
        "Y Ge",
        "D Chen",
        "H Li"
      ],
      "year": "2020",
      "venue": "Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification",
      "arxiv": "arXiv:2001.01526"
    },
    {
      "citation_id": "208",
      "title": "Meta pseudo labels",
      "authors": [
        "H Pham",
        "Z Dai",
        "Q Xie",
        "Q Le"
      ],
      "year": "2021",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual"
    },
    {
      "citation_id": "209",
      "title": "Towards unifying the label space for aspect-and sentence-based sentiment analysis",
      "authors": [
        "Y Zhang",
        "M Zhang",
        "S Wu",
        "J Zhao"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "210",
      "title": "A multitask learning model for multimodal sarcasm, sentiment and emotion recognition in conversations",
      "authors": [
        "Y Zhang",
        "J Wang",
        "Y Liu",
        "L Rong",
        "Q Zheng",
        "D Song",
        "P Tiwari",
        "J Qin"
      ],
      "year": "2023",
      "venue": "Inf. Fusion"
    },
    {
      "citation_id": "211",
      "title": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "authors": [
        "M Akhtar",
        "D Chauhan",
        "D Ghosal",
        "S Poria",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2019",
      "venue": "Multi-task learning for multi-modal emotion recognition and sentiment analysis",
      "arxiv": "arXiv:1905.05812"
    },
    {
      "citation_id": "212",
      "title": "Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models",
      "authors": [
        "T Xie",
        "C Wu",
        "P Shi",
        "R Zhong",
        "T Scholak",
        "M Yasunaga",
        "C Wu",
        "M Zhong",
        "P Yin",
        "S Wang",
        "V Zhong",
        "B Wang",
        "C Li",
        "C Boyle",
        "A Ni",
        "Z Yao",
        "D Radev",
        "C Xiong",
        "L Kong",
        "R Zhang",
        "N Smith",
        "L Zettlemoyer",
        "T Yu"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "213",
      "title": "Vilbert: Pretraining taskagnostic visiolinguistic representations for vision-and-language tasks",
      "authors": [
        "J Lu",
        "D Batra",
        "D Parikh",
        "S Lee"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "214",
      "title": "Vlmo: Unified visionlanguage pre-training with mixture-of-modality-experts",
      "authors": [
        "W Wang",
        "H Bao",
        "L Dong",
        "F Wei"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "215",
      "title": "Unims: A unified framework for multimodal summarization with knowledge distillation",
      "authors": [
        "Z Zhang",
        "X Meng",
        "Y Wang",
        "X Jiang",
        "Q Liu",
        "Z Yang"
      ],
      "year": "2022",
      "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event"
    },
    {
      "citation_id": "216",
      "title": "Roberta: A robustly optimized BERT pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "1907",
      "venue": "CoRR"
    },
    {
      "citation_id": "217",
      "title": "Topic and style-aware transformer for multimodal emotion recognition",
      "authors": [
        "S Qiu",
        "N Sekhar",
        "P Singhal"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2023"
    },
    {
      "citation_id": "218",
      "title": "Multimodal sentiment analysis based on multi-head attention mechanism",
      "authors": [
        "C Xi",
        "G Lu",
        "J Yan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 4th international conference on machine learning and soft computing"
    },
    {
      "citation_id": "219",
      "title": "A quantum-inspired multimodal sentiment analysis framework",
      "authors": [
        "Y Zhang",
        "D Song",
        "P Zhang",
        "P Wang",
        "J Li",
        "X Li",
        "B Wang"
      ],
      "year": "2018",
      "venue": "Theoretical Computer Science"
    },
    {
      "citation_id": "220",
      "title": "Context-sensitive learning for enhanced audiovisual emotion classification",
      "authors": [
        "A Metallinou",
        "M Wollmer",
        "A Katsamanis",
        "F Eyben",
        "B Schuller",
        "S Narayanan"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "221",
      "title": "A cognitive brain model for multimodal sentiment analysis based on attention neural networks",
      "authors": [
        "Y Li",
        "K Zhang",
        "J Wang",
        "X Gao"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "222",
      "title": "Fuzzy commonsense reasoning for multimodal sentiment analysis",
      "authors": [
        "I Chaturvedi",
        "R Satapathy",
        "S Cavallari",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Pattern Recognition Letters"
    },
    {
      "citation_id": "223",
      "title": "Sfnn: semantic features fusion neural network for multimodal sentiment analysis",
      "authors": [
        "W Wu",
        "Y Wang",
        "S Xu",
        "K Yan"
      ],
      "year": "2020",
      "venue": "2020 5th International Conference on Automation, Control and Robotics Engineering (CACRE)"
    },
    {
      "citation_id": "224",
      "title": "A facial expression-aware multimodal multi-task learning framework for emotion recognition in multi-party conversations",
      "authors": [
        "W Zheng",
        "J Yu",
        "R Xia",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "225",
      "title": "A transformer-based model with self-distillation for multimodal emotion recognition in conversations",
      "authors": [
        "H Ma",
        "J Wang",
        "H Lin",
        "B Zhang",
        "Y Zhang",
        "B Xu"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "226",
      "title": "Cross-attention is not enough: Incongruity-aware multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Y Wang",
        "Y Li",
        "P Bell",
        "C Lai"
      ],
      "year": "2023",
      "venue": "CoRR"
    },
    {
      "citation_id": "227",
      "title": "M3GAT: A multi-modal, multi-task interactive graph attention network for conversational sentiment analysis and emotion recognition",
      "authors": [
        "Y Zhang",
        "A Jia",
        "B Wang",
        "P Zhang",
        "D Zhao",
        "P Li",
        "Y Hou",
        "X Jin",
        "D Song",
        "J Qin"
      ],
      "year": "2024",
      "venue": "ACM Trans. Inf. Syst"
    },
    {
      "citation_id": "228",
      "title": "Fusing modality-specific representations and decisions for multimodal emotion recognition",
      "authors": [
        "Y.-P Ruan",
        "S Han",
        "T Li",
        "Y Wu"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "229",
      "title": "M2DF: multigrained multi-curriculum denoising framework for multimodal aspectbased sentiment analysis",
      "authors": [
        "F Zhao",
        "C Li",
        "Z Wu",
        "Y Ouyang",
        "J Zhang",
        "X Dai"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023"
    },
    {
      "citation_id": "230",
      "title": "Tailor versatile multimodal learning for multi-label emotion recognition",
      "authors": [
        "Y Zhang",
        "M Chen",
        "J Shen",
        "C Wang"
      ],
      "year": "2022",
      "venue": "Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event"
    },
    {
      "citation_id": "231",
      "title": "Multimodal multi-label emotion recognition with heterogeneous hierarchical message passing",
      "authors": [
        "D Zhang",
        "X Ju",
        "W Zhang",
        "J Li",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2021",
      "venue": "Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event"
    },
    {
      "citation_id": "232",
      "title": "M3TR: multi-modal multi-label recognition with transformer",
      "authors": [
        "J Zhao",
        "Y Zhao",
        "J Li"
      ],
      "year": "2021",
      "venue": "MM '21: ACM Multimedia Conference, Virtual Event"
    },
    {
      "citation_id": "233",
      "title": "Modalnet: an aspect-level sentiment classification model by exploring multimodal data with fusion discriminant attentional network",
      "authors": [
        "Z Zhang",
        "Z Wang",
        "X Li",
        "N Liu",
        "B Guo",
        "Z Yu"
      ],
      "year": "2021",
      "venue": "World Wide Web"
    },
    {
      "citation_id": "234",
      "title": "Multi-grained fusion network with self-distillation for aspect-based multimodal sentiment analysis",
      "authors": [
        "J Yang",
        "Y Xiao",
        "X Du"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "235",
      "title": "Skier: A symbolic knowledge integrated model for conversational emotion recognition",
      "authors": [
        "W Li",
        "L Zhu",
        "R Mao",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "236",
      "title": "Dynamic interactive multiview memory network for emotion recognition in conversation",
      "authors": [
        "J Wen",
        "D Jiang",
        "G Tu",
        "C Liu",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "237",
      "title": "Multimodal sentiment intensity analysis in videos: Facial gestures and verbal messages",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L Morency"
      ],
      "year": "2016",
      "venue": "IEEE Intell. Syst"
    },
    {
      "citation_id": "238",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018"
    },
    {
      "citation_id": "239",
      "title": "Ch-sims: A chinese multimodal sentiment analysis dataset with finegrained annotation of modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "240",
      "title": "Cmu-moseas: A multimodal language dataset for spanish, portuguese, german and french",
      "authors": [
        "A Zadeh",
        "Y Cao",
        "S Hessner",
        "P Liang",
        "S Poria",
        "L.-P Morency"
      ],
      "year": "2020",
      "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing. Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "241",
      "title": "Youtube movie reviews: Sentiment analysis in an audio-visual context",
      "authors": [
        "M Wöllmer",
        "F Weninger",
        "T Knaup",
        "B Schuller",
        "C Sun",
        "K Sagae",
        "L Morency"
      ],
      "year": "2013",
      "venue": "IEEE Intell. Syst"
    },
    {
      "citation_id": "242",
      "title": "Make acoustic and visual cues matter: Ch-sims v2. 0 dataset and av-mixup consistent module",
      "authors": [
        "Y Liu",
        "Z Yuan",
        "H Mao",
        "Z Liang",
        "W Yang",
        "Y Qiu",
        "T Cheng",
        "X Li",
        "H Xu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "243",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019"
    },
    {
      "citation_id": "244",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Lang. Resour. Evaluation"
    },
    {
      "citation_id": "245",
      "title": "FAF: A novel multimodal emotion recognition approach integrating face, body and text",
      "authors": [
        "Z Fang",
        "A He",
        "Q Yu",
        "B Gao",
        "W Ding",
        "T Zhang",
        "L Ma"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "246",
      "title": "Recognizing human emotional state from audiovisual signals",
      "authors": [
        "Y Wang",
        "L Guan"
      ],
      "year": "2008",
      "venue": "IEEE Trans. Multim"
    },
    {
      "citation_id": "247",
      "title": "BAUM-1: A spontaneous audio-visual face database of affective and mental states",
      "authors": [
        "S Zhalehpour",
        "O Onder",
        "Z Akhtar",
        "C Erdem"
      ],
      "year": "2017",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "248",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "249",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "250",
      "title": "The multimodal sentiment analysis in car reviews (muse-car) dataset: Collection, insights and improvements",
      "authors": [
        "L Stappen",
        "A Baird",
        "L Schumann",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "251",
      "title": "Mec 2017: Multimodal emotion recognition challenge",
      "authors": [
        "Y Li",
        "J Tao",
        "B Schuller",
        "S Shan",
        "D Jiang",
        "J Jia"
      ],
      "year": "2018",
      "venue": "2018 First Asian Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "252",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "253",
      "title": "Meisd: A multimodal multi-label emotion, intensity and sentiment dialogue dataset for emotion recognition and sentiment analysis in conversations",
      "authors": [
        "M Firdaus",
        "H Chauhan",
        "A Ekbal",
        "P Bhattacharyya"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th international conference on computational linguistics"
    },
    {
      "citation_id": "254",
      "title": "Beyond emotion: A multi-modal dataset for human desire understanding",
      "authors": [
        "A Jia",
        "Y He",
        "Y Zhang",
        "S Uprety",
        "D Song",
        "C Lioma"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022"
    },
    {
      "citation_id": "255",
      "title": "The muse 2021 multimodal sentiment analysis challenge: Sentiment, emotion, physiologicalemotion, and stress",
      "authors": [
        "L Stappen",
        "A Baird",
        "L Christ",
        "L Schumann",
        "B Sertolli",
        "E Meßner",
        "E Cambria",
        "G Zhao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "The muse 2021 multimodal sentiment analysis challenge: Sentiment, emotion, physiologicalemotion, and stress"
    },
    {
      "citation_id": "256",
      "title": "Layer-wise fusion with modality independence modeling for multi-modal emotion recognition",
      "authors": [
        "J Sun",
        "S Han",
        "Y.-P Ruan",
        "X Zhang",
        "S.-K Zheng",
        "Y Liu",
        "Y Huang",
        "T Li"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "257",
      "title": "AMIGOS: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "J Correa",
        "M Abadi",
        "N Sebe",
        "I Patras"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "258",
      "title": "Adaptive co-attention network for named entity recognition in tweets",
      "authors": [
        "Q Zhang",
        "J Fu",
        "X Liu",
        "X Huang"
      ],
      "year": "2018",
      "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)"
    },
    {
      "citation_id": "259",
      "title": "Mcpr: A chinese product review dataset for multimodal aspect-based sentiment analysis",
      "authors": [
        "C Xu",
        "X Luo",
        "D Wang"
      ],
      "year": "2022",
      "venue": "International Conference on Cognitive Computing"
    },
    {
      "citation_id": "260",
      "title": "MACSA: A multimodal aspect-category sentiment analysis dataset with multimodal finegrained aligned annotations",
      "authors": [
        "H Yang",
        "Y Zhao",
        "J Liu",
        "Y Wu",
        "B Qin"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "261",
      "title": "Masad: A large-scale dataset for multimodal aspect-based sentiment analysis",
      "authors": [
        "J Zhou",
        "J Zhao",
        "J Huang",
        "Q Hu",
        "L He"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "262",
      "title": "Panosent: A panoptic sextuple extraction benchmark for multimodal conversational aspect-based sentiment analysis",
      "authors": [
        "M Luo",
        "H Fei",
        "B Li",
        "S Wu",
        "Q Liu",
        "S Poria",
        "E Cambria",
        "M.-L Lee",
        "W Hsu"
      ],
      "year": "2024",
      "venue": "Panosent: A panoptic sextuple extraction benchmark for multimodal conversational aspect-based sentiment analysis",
      "arxiv": "arXiv:2408.09481"
    },
    {
      "citation_id": "263",
      "title": "Large-scale visual sentiment ontology and detectors using adjective noun pairs",
      "authors": [
        "D Borth",
        "R Ji",
        "T Chen",
        "T Breuel",
        "S.-F Chang"
      ],
      "year": "2013",
      "venue": "Proceedings of the 21st ACM international conference on Multimedia"
    },
    {
      "citation_id": "264",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "265",
      "title": "Relative uncertainty learning for facial expression recognition",
      "authors": [
        "Y Zhang",
        "C Wang",
        "W Deng"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "266",
      "title": "Attention mechanismbased cnn for facial expression recognition",
      "authors": [
        "J Li",
        "K Jin",
        "D Zhou",
        "N Kubota",
        "Z Ju"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "267",
      "title": "Facial expression recognition in the wild via deep attentive center loss",
      "authors": [
        "A Farzaneh",
        "X Qi"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "268",
      "title": "Rethinking the learning paradigm for dynamic facial expression recognition",
      "authors": [
        "H Wang",
        "B Li",
        "S Wu",
        "S Shen",
        "F Liu",
        "S Ding",
        "A Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "269",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Z Zhao",
        "Q Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "270",
      "title": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "authors": [
        "H Li",
        "M Sui",
        "Z Zhu"
      ],
      "year": "2022",
      "venue": "Nr-dfernet: Noise-robust network for dynamic facial expression recognition",
      "arxiv": "arXiv:2206.04975"
    },
    {
      "citation_id": "271",
      "title": "Dpcnet: Dual path multi-excitation collaborative network for facial expression representation learning in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "W Song",
        "S Gao",
        "Y Huang",
        "Z Chen",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "272",
      "title": "Expression snippet transformer for robust video-based facial expression recognition",
      "authors": [
        "Y Liu",
        "W Wang",
        "C Feng",
        "H Zhang",
        "Z Chen",
        "Y Zhan"
      ],
      "year": "2021",
      "venue": "Expression snippet transformer for robust video-based facial expression recognition"
    },
    {
      "citation_id": "273",
      "title": "Logo-former: Local-global spatio-temporal transformer for dynamic facial expression recognition",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "274",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "H Li",
        "H Niu",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2022",
      "venue": "Intensity-aware loss for dynamic facial expression recognition in the wild"
    },
    {
      "citation_id": "275",
      "title": "Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters",
      "authors": [
        "H Chen",
        "H Huang",
        "J Dong",
        "M Zheng",
        "D Shao"
      ],
      "year": "2024",
      "venue": "Finecliper: Multi-modal fine-grained clip for dynamic facial expression recognition with adapters",
      "arxiv": "arXiv:2407.02157"
    },
    {
      "citation_id": "276",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Z Zhao",
        "I Patras"
      ],
      "year": "2023",
      "venue": "British Machine Vision Conference (BMVC)"
    },
    {
      "citation_id": "277",
      "title": "A 3 lign-dfer: Pioneering comprehensive dynamic affective alignment for dynamic facial expression recognition with clip",
      "authors": [
        "Z Tao",
        "Y Wang",
        "J Lin",
        "H Wang",
        "X Mai",
        "J Yu",
        "X Tong",
        "Z Zhou",
        "S Yan",
        "Q Zhao",
        "L Han",
        "W Zhang"
      ],
      "year": "2024",
      "venue": "A 3 lign-dfer: Pioneering comprehensive dynamic affective alignment for dynamic facial expression recognition with clip"
    },
    {
      "citation_id": "278",
      "title": "Norface: Improving facial expression analysis by identity normalization",
      "authors": [
        "H Liu",
        "R An",
        "Z Zhang",
        "B Ma",
        "W Zhang",
        "Y Song",
        "Y Hu",
        "W Chen",
        "Y Ding"
      ],
      "year": "2024",
      "venue": "Norface: Improving facial expression analysis by identity normalization",
      "arxiv": "arXiv:2407.15617"
    },
    {
      "citation_id": "279",
      "title": "Crest: Cross-modal resonance through evidential deep learning for enhanced zero-shot learning",
      "authors": [
        "H Huang",
        "X Qiao",
        "Z Chen",
        "H Chen",
        "B Li",
        "Z Sun",
        "M Chen",
        "X Li"
      ],
      "year": "2024",
      "venue": "Crest: Cross-modal resonance through evidential deep learning for enhanced zero-shot learning",
      "arxiv": "arXiv:2404.09640"
    },
    {
      "citation_id": "280",
      "title": "Trusted multi-view classification with dynamic evidential fusion",
      "authors": [
        "Z Han",
        "C Zhang",
        "H Fu",
        "J Zhou"
      ],
      "year": "2022",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "281",
      "title": "Evidential deep partial multi-view classification with discount fusion",
      "authors": [
        "H Huang",
        "Z Liu",
        "S Letchmunan",
        "M Lin",
        "M Deveci",
        "W Pedrycz",
        "P Siarry"
      ],
      "year": "2024",
      "venue": "Evidential deep partial multi-view classification with discount fusion",
      "arxiv": "arXiv:2408.13123"
    },
    {
      "citation_id": "282",
      "title": "Trusted unified feature-neighborhood dynamics for multi-view classification",
      "authors": [
        "H Huang",
        "C Qin",
        "Z Liu",
        "K Ma",
        "J Chen",
        "H Fang",
        "C Ban",
        "H Sun",
        "Z He"
      ],
      "year": "2024",
      "venue": "Trusted unified feature-neighborhood dynamics for multi-view classification",
      "arxiv": "arXiv:2409.00755"
    },
    {
      "citation_id": "283",
      "title": "Vectorized evidential learning for weakly-supervised temporal action localization",
      "authors": [
        "J Gao",
        "M Chen",
        "C Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "284",
      "title": "Beyond uncertainty: Evidential deep learning for robust video temporal grounding",
      "authors": [
        "K Ma",
        "H Huang",
        "J Chen",
        "H Chen",
        "P Ji",
        "X Zang",
        "H Fang",
        "C Ban",
        "H Sun",
        "M Chen"
      ],
      "year": "2024",
      "venue": "Beyond uncertainty: Evidential deep learning for robust video temporal grounding",
      "arxiv": "arXiv:2408.16272"
    },
    {
      "citation_id": "285",
      "title": "Vectorized evidential learning for weakly-supervised temporal action localization",
      "authors": [
        "J Gao",
        "M Chen",
        "C Xu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "286",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "287",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L Dai"
      ],
      "year": "2018",
      "venue": "19th Annual Conference of the International Speech Communication Association, Interspeech"
    },
    {
      "citation_id": "288",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "289",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "290",
      "title": "Exploring spatio-temporal representations by integrating attention-based bidirectional-lstm-rnns and fcns for speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Y Zheng",
        "Z Zhang",
        "H Wang",
        "Y Zhao",
        "C Li"
      ],
      "year": "2018",
      "venue": "19th Annual Conference of the International Speech Communication Association, Interspeech"
    },
    {
      "citation_id": "291",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "D Luo",
        "Y Zou",
        "D Huang"
      ],
      "year": "2018",
      "venue": "19th Annual Conference of the International Speech Communication Association, Interspeech"
    },
    {
      "citation_id": "292",
      "title": "Cross-subject eeg-based emotion recognition via semisupervised multisource joint distribution adaptation",
      "authors": [
        "M Jiménez-Guarneros",
        "G Pineda"
      ],
      "year": "2023",
      "venue": "IEEE Trans. Instrum. Meas"
    },
    {
      "citation_id": "293",
      "title": "S 3 lrr: A unified model for joint discriminative subspace identification and semisupervised EEG emotion recognition",
      "authors": [
        "Y Peng",
        "Y Zhang",
        "W Kong",
        "F Nie",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Instrum. Meas"
    },
    {
      "citation_id": "294",
      "title": "Self-weighted semi-supervised classification for joint eeg-based emotion recognition and affective activation patterns mining",
      "authors": [
        "Y Peng",
        "W Kong",
        "F Qin",
        "F Nie",
        "J Fang",
        "B Lu",
        "A Cichocki"
      ],
      "year": "2021",
      "venue": "IEEE Trans. Instrum. Meas"
    },
    {
      "citation_id": "295",
      "title": "Physiological signals based affective computing: A systematic review",
      "authors": [
        "X Quan",
        "Z Zeng",
        "J Jiang",
        "Y Zhang",
        "B Lu",
        "D Wu"
      ],
      "year": "2021",
      "venue": "Acta Automatica Sinica"
    },
    {
      "citation_id": "296",
      "title": "Emotion recognition from EEG and facial expressions: a multimodal approach",
      "authors": [
        "V Chaparro",
        "A Gomez",
        "A Salgado",
        "O Quintero",
        "N López",
        "L Villa"
      ],
      "year": "2018",
      "venue": "40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "297",
      "title": "Fusion of facial expressions and EEG for multimodal emotion recognition",
      "authors": [
        "Y Huang",
        "J Yang",
        "P Liao",
        "J Pan"
      ],
      "year": "2017",
      "venue": "Comput. Intell. Neurosci"
    },
    {
      "citation_id": "298",
      "title": "Valence-arousal model based emotion recognition using eeg, peripheral physiological signals and facial expression",
      "authors": [
        "Q Zhu",
        "G Lu",
        "J Yan"
      ],
      "year": "2020",
      "venue": "ICMLSC 2020: The 4th International Conference on Machine Learning and Soft Computing"
    },
    {
      "citation_id": "299",
      "title": "Multimodal emotion recognition using deep neural networks",
      "authors": [
        "H Tang",
        "W Liu",
        "W Zheng",
        "B Lu"
      ],
      "year": "2017",
      "venue": "Neural Information Processing -24th International Conference"
    },
    {
      "citation_id": "300",
      "title": "A multimodal emotion recognition method based on facial expressions and electroencephalography",
      "authors": [
        "Y Tan",
        "Z Sun",
        "F Duan",
        "J Solé-Casals",
        "C Caiafa"
      ],
      "year": "2021",
      "venue": "Biomed. Signal Process. Control"
    },
    {
      "citation_id": "301",
      "title": "Emotion recognition with pre-trained transformers using multimodal signals",
      "authors": [
        "J Vazquez-Rodriguez",
        "G Lefebvre",
        "J Cumin",
        "J Crowley"
      ],
      "year": "2022",
      "venue": "10th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "302",
      "title": "Bidirectional hierarchical attention networks based on document-level context for emotion cause extraction",
      "authors": [
        "G Hu",
        "G Lu",
        "Y Zhao"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana"
    },
    {
      "citation_id": "303",
      "title": "Experiencer-driven and knowledge-aware graph model for emotion-cause pair extraction",
      "authors": [
        "M Li",
        "H Zhao",
        "T Gu",
        "D Ying"
      ],
      "year": "2023",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "304",
      "title": "ECPEC: Emotion-cause pair extraction in conversations",
      "authors": [
        "W Li",
        "Y Li",
        "V Pandelea",
        "M Ge",
        "L Zhu",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "305",
      "title": "Multimodal emotion-cause pair extraction with holistic interaction and label constraint",
      "authors": [
        "B Li",
        "H Fei",
        "F Li",
        "T -S. Chua",
        "D Ji"
      ],
      "year": "2024",
      "venue": "ACM Trans. Multimedia Comput. Commun. Appl",
      "doi": "10.1145/3689646"
    },
    {
      "citation_id": "306",
      "title": "Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation",
      "authors": [
        "J Li",
        "D Li",
        "C Xiong",
        "S Hoi"
      ],
      "year": "2022",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "307",
      "title": "Which is making the contribution: Modulating unimodal and cross-modal dynamics for multimodal sentiment analysis",
      "authors": [
        "Y Zeng",
        "S Mai",
        "H Hu"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021, Virtual Event / Punta Cana"
    },
    {
      "citation_id": "308",
      "title": "A knowledge regularized hierarchical approach for emotion cause analysis",
      "authors": [
        "C Fan",
        "H Yan",
        "J Du",
        "L Gui",
        "L Bing",
        "M Yang",
        "R Xu",
        "R Mao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "309",
      "title": "Conceptnet 5.5: An open multilingual graph of general knowledge",
      "authors": [
        "R Speer",
        "J Chin",
        "C Havasi"
      ],
      "year": "2017",
      "venue": "Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "310",
      "title": "SenticNet 8: Fusing emotion AI and commonsense AI for interpretable, trustworthy, and explainable affective computing",
      "authors": [
        "E Cambria",
        "X Zhang",
        "R Mao",
        "M Chen",
        "K Kwok"
      ],
      "venue": "International Conference on Human-Computer Interaction (HCII)"
    },
    {
      "citation_id": "311",
      "title": "Transomcs: From linguistic graphs to commonsense knowledge",
      "authors": [
        "H Zhang",
        "D Khashabi",
        "Y Song",
        "D Roth"
      ],
      "year": "2020",
      "venue": "Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence"
    },
    {
      "citation_id": "312",
      "title": "COMET: commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "A Bosselut",
        "H Rashkin",
        "M Sap",
        "C Malaviya",
        "A Celikyilmaz",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019"
    },
    {
      "citation_id": "313",
      "title": "ATOMIC: an atlas of machine commonsense for if-then reasoning",
      "authors": [
        "M Sap",
        "R Bras",
        "E Allaway",
        "C Bhagavatula",
        "N Lourie",
        "H Rashkin",
        "B Roof",
        "N Smith",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence"
    },
    {
      "citation_id": "314",
      "title": "C3KG: A chinese commonsense conversation knowledge graph",
      "authors": [
        "D Li",
        "Y Li",
        "J Zhang",
        "K Li",
        "C Wei",
        "J Cui",
        "B Wang"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "315",
      "title": "Hierarchical attention networks for document classification",
      "authors": [
        "Z Yang",
        "D Yang",
        "C Dyer",
        "X He",
        "A Smola",
        "E Hovy"
      ],
      "year": "2016",
      "venue": "NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "316",
      "title": "Hierarchy-aware global model for hierarchical text classification",
      "authors": [
        "J Zhou",
        "C Ma",
        "D Long",
        "G Xu",
        "N Ding",
        "H Zhang",
        "P Xie",
        "G Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020"
    },
    {
      "citation_id": "317",
      "title": "FSS-GCN: A graph convolutional networks with fusion of semantic and structure for emotion cause analysis",
      "authors": [
        "G Hu",
        "G Lu",
        "Y Zhao"
      ],
      "year": "2021",
      "venue": "Knowl. Based Syst"
    },
    {
      "citation_id": "318",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019"
    },
    {
      "citation_id": "319",
      "title": "Are they different? affect, feeling, emotion, sentiment, and opinion detection in text",
      "authors": [
        "M Munezero",
        "C Montero",
        "E Sutinen",
        "J Pajunen"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Affect. Comput"
    },
    {
      "citation_id": "320",
      "title": "Uniker: A unified framework for combining embedding and definite horn rule reasoning for knowledge graph inference",
      "authors": [
        "K Cheng",
        "Z Yang",
        "M Zhang",
        "Y Sun"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana"
    },
    {
      "citation_id": "321",
      "title": "Image as a foreign language: Beit pretraining for all vision and vision-language tasks",
      "authors": [
        "W Wang",
        "H Bao",
        "L Dong",
        "J Bjorck",
        "Z Peng",
        "Q Liu",
        "K Aggarwal",
        "O Mohammed",
        "S Singhal",
        "S Som",
        "F Wei"
      ],
      "year": "2022",
      "venue": "CoRR"
    },
    {
      "citation_id": "322",
      "title": "Challenges and strategies in cross-cultural NLP",
      "authors": [
        "D Hershcovich",
        "S Frank",
        "H Lent",
        "M De Lhoneux",
        "M Abdou",
        "S Brandl",
        "E Bugliarello",
        "L Piqueras",
        "I Chalkidis",
        "R Cui",
        "C Fierro",
        "K Margatina",
        "P Rust",
        "A Søgaard"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "323",
      "title": "A cross-cultural study on emotion expression and the learning of social norms",
      "authors": [
        "S Hareli",
        "K Kafetsios",
        "U Hess"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "324",
      "title": "Neurosymbolic ai for personalized sentiment analysis",
      "authors": [
        "L Zhu",
        "R Mao",
        "E Cambria",
        "B Jansen"
      ],
      "year": "2024",
      "venue": "Proceedings of HCII"
    },
    {
      "citation_id": "325",
      "title": "Talking about tactile experiences",
      "authors": [
        "M Obrist",
        "S Seah",
        "S Subramanian"
      ],
      "year": "2013",
      "venue": "Proceedings of Human Factors in Computing Systems"
    }
  ]
}