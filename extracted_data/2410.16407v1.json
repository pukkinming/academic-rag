{
  "paper_id": "2410.16407v1",
  "title": "Enhancing Multimodal Affective Analysis With Learned Live Comment Features",
  "published": "2024-10-21T18:19:09Z",
  "authors": [
    "Zhaoyuan Deng",
    "Amith Ananthram",
    "Kathleen McKeown"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Live comments, also known as Danmaku, are user-generated messages that are synchronized with video content. These comments overlay directly onto streaming videos, capturing viewer emotions and reactions in real-time. While prior work has leveraged live comments in affective analysis, its use has been limited due to the relative rarity of live comments across different video platforms. To address this, we first construct the Live Comment for Affective Analysis (LCAffect) dataset which contains live comments for English and Chinese videos spanning diverse genres that elicit a wide spectrum of emotions. Then, using this dataset, we use contrastive learning to train a video encoder to produce synthetic live comment features for enhanced multimodal affective content analysis. Through comprehensive experimentation on a wide range of affective analysis tasks (sentiment, emotion recognition, and sarcasm detection) in both English and Chinese, we demonstrate that these synthetic live comment features significantly improve performance over state-of-the-art methods. We will release our dataset, code, and models upon publication.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Live comments, or Danmaku, are a subtitle system originating from Japan and popularized in China. This system overlays real-time user-generated messages directly onto online video streams. These comments, synchronized with a video's timeline, offer a rich layer of viewer interaction, providing insights into user emotions, user reactions, and the video's broader context (see Figure  1 ). The richness of these comments makes them an ideal resource for enhancing multimodal affective analysis, which seeks to interpret and categorize the emotional content and dynamics of videos. Although prior research, such as the study by  Niu et al. (2018) , has demonstrated how live comments can improve performance in affective analysis tasks like emotion prediction, their broader application remains constrained. Many video platforms do not support live comments, rendering these insights impossible for videos on those platforms. Consequently, the rarity of live comments poses significant challenges in fully exploiting their analytical potential.\n\nIn this work, we bridge this gap by proposing a novel method for producing synthetic live comment features to improve the performance of multimodal affective analysis. We demonstrate the effectiveness of these features in English  and Chinese across several video understanding tasks: sentiment analysis, emotion recognition, and sarcasm detection.\n\nOne major challenge in affective learning from live comments stems from the nature of existing large-scale live comment datasets which are primarily tailored to the live comment generation task. These datasets typically feature a large percentage of popular content genres such as video games, sports, and music videos. However, these categories typically lack the rich emotional context required for detailed affective analysis. This mismatch complicates the training of models capable of accurately understanding and interpreting nuanced emotions. Additionally, the majority of these datasets are collected from Bilibili, which features amateurcreated videos of everyday life. These videos often provoke a more limited and homogeneous range of emotional responses  (Rosenbusch, Evans, and Zeelenberg 2019) , espe-cially when compared to the diverse and intense emotions elicited by content like dramas or documentaries. Moreover, most datasets comprise exclusively Chinese content, which further restricts their multilingual applicability.\n\nA second significant challenge is the dependency of existing methods on the availability of live comments. In the realworld, the presence of live comments can be limited and not uniformly distributed across all video genres. This reliance restricts the practical application of these methods, as they cannot work in scenarios where comments are non-existent.\n\nTo address these challenges, we first construct a largescale, diverse dataset comprising everyday life videos in Chinese as well as bilingual TV shows, movies, and documentaries, enabling our system to learn a broad spectrum of emotional experiences across various contexts. Then, utilizing our dataset, we develop a multimodal encoder that effectively learns from our collected videos and comments by contrasting different contexts. This encoder is capable of producing synthetic live comment features from videos, thereby enabling the inference of emotional context across modalities, even in the absence of live comments.\n\nOur experimental analysis demonstrates that adding our live comment features improves over state-of-the-art (SOTA) approaches that leverage text, acoustic, and visual modalities directly. Our system achieves new SOTA performance on multiple multimodal datasets, particularly enhancing accuracy in detecting sentiment, emotions, and sarcasm. This not only underscores the utility of live comments in affective video analysis but also opens new avenues for multimodal learning applications.\n\nOur main contributions are: • 1) Live Comment for Affective Analysis dataset (LCAffect), an expansive corpus of over 11 million live comments. It includes bilingual video content (Chinese and English) spanning a diverse range of topics and genres, curated to capture a wide spectrum of emotions. • 2) A contrastive encoder that learns to project videos into live comment representation space, allowing the inference of synthetic live comment features for any video. • 3) A downstream multimodal fusion model for affective analysis tasks which utilizes not only three modalities but also our synthetic live comment feature. • 4) New SOTA across three affective analysis tasks, including a 3.18-point increase in accuracy on CH-SIMS v2 (sentiment analysis), a 2.89-point increase in F1 on MELD (emotion recognition), and a 3.0-point increase in F1 on MuSTARD (sarcasm detection).",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Learning from Comments Previous research has highlighted the value of user comments in improving performance across various multimodal tasks.  Fu et al. (2017)  show that live comments on Twitch help predict highlights in e-sport games. Similarly,  Yang, Ai, and Hirschberg (2019)  show that live comments help identify humor in online videos.  Additionally, Niu et al. (2018)  show that features derived from live comments improve affective content analysis. Moreover,  Hanu et al. (2022)  show that user comments can help learn more contextualized representations for image, video, and audio, thus improving video-text retrieval.\n\nThese studies depend on the availability of user comments which are not always present in the real-world. In contrast, our work seeks to develop a system that is pre-trained to align video segments with corresponding live comments. This approach aims to learn a multimodal representation space that effectively augments various downstream understanding tasks. Consequently, our system can be directly applied to videos lacking associated live comments, producing synthetic live comment features for such videos. This capability directly addresses the limitations of prior work, enhancing the analysis and interpretation of videos that would be excluded due to the absence of live comments.\n\nLive Comment Datasets Several datasets featuring live comments have been established in the literature. LiveBot  (Ma et al. 2018)  and VideoIC  (Wang, Chen, and Jin 2020)  are large-scale datasets drawn from Bilibili, a site which primarily features short, user-generated videos focused on everyday life. In contrast,  Lalanne, Bournet, and Yu (2023)  collected live-streamed video and comments from Twitch, a platform centered around video games. Finally, MovieLC  (Chen et al. 2023 ) is a compilation of famous movies with accompanying comments from Tencent Video.\n\nOur work builds on these efforts by creating a new dataset that includes videos from a broader range of platforms. By integrating multiple video hosting platforms, our dataset includes both a variety of short, user-generated content and longer formats like TV shows, movies, and documentaries in Chinese and English. This expansive collection allows the study of many genres and enables the training of a multimodal encoder that can learn live comment features effectively for affective analysis from diverse videos.\n\nMultimodal Affective Analysis Previously, multimodal affective computing often relied on hand-crafted algorithms to perform initial feature extraction. Recently, however, there has been a shift towards using pre-trained modality encoders with end-to-end tuning to improve performance. Specifically,  Yi et al. (2024)  use CLIP  (Radford et al. 2021 )'s spatial encoder and TimeSformer  (Bertasius, Wang, and Torresani 2021)  to encode visual features, while  Wu et al. (2024)  employ pre-trained audio encoders such as Data2Vec  (Baevski et al. 2022)  and Hubert  (Hsu et al. 2021)  to process acoustic features. Although these studies demonstrate the efficacy of an end-to-end framework, they are limited to two modalities. Our work extends this approach by integrating all three modalities (text, acoustic, visual), achieving more robust video analysis. Another promising research direction in affective analysis involves the integration of external knowledge.  Ghosal et al. (2020)  employ features produced by a common sense knowledge model to enhance emotion recognition, while  Hu et al. (2022)  exploit the complementary knowledge underlying sentiment analysis and emotion recognition to build a knowledge-sharing framework. Our work parallels these efforts by leveraging external knowledge derived from live comments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Category",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "The Live Comment Dataset Collection Of Videos",
      "text": "We collect videos and their accompanying live comments from several popular Chinese video streaming websites: Bilibili, Tencent Video, iQIYI, and Youku. Our dataset includes both Chinese and English content and is partitioned into three distinct subsets based on content type: User-Generated Content This subset includes 2, 464 videos from Bilibili featuring naturally occurring conversations from everyday life. The initial set of 682 videos is available from the Linguistic Data Consortium (UPenn) and was manually inspected to ensure content quality and relevance. Each video includes interactions involving at least two people with observable emotions. The remaining 1, 782 videos are sourced from Bilibili's recommendation API, which selects content similar to the manually inspected set. TV shows and documentaries We collect 443 episodes from 8 TV shows and documentaries. This subset includes content from multiple platforms, including dramas, sitcoms, and documentaries focused on social life. Their inclusion aims to provide the model with insights into scripted content that elicits a wide range of emotions in different contexts. Movies Sourced from Tencent Video, this subset features 59 popular comedy, drama, and action films. They feature the longest videos, enabling the analysis of viewer interactions over extended contexts through live comments.\n\nIn addition, we collect text transcripts for the videos. For user-generated content, transcripts are obtained using Bilibili's API, which uses automatic speech recognition. For movies and TV shows, we extract transcripts from hardcoded subtitles via optical character recognition (OCR). We extract both English and Chinese subtitles when available.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Statistics",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Contrastive Pre-Training",
      "text": "Although multiple studies have demonstrated the effectiveness of live comments in enhancing video understanding, the availability of live comments is largely limited outside of China and Japan. This restriction significantly limits the practical application of these studies. To overcome this challenge, we propose a video encoder pre-trained on our diverse live comment dataset. This model leverages a standard contrastive learning framework akin to CLIP  (Radford et al. 2021) , where positive examples consist of matching live comments and videos, and negative examples involve non-matching pairs. The aim is to produce a synthetic live comment feature from a video span that can be used to enhance performance in various affective analysis tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Training Model Architecture",
      "text": "We adopt a CLIP-style contrastive pre-training approach.\n\nDuring the training phase, we partition each video into segments s of σ seconds. For each segment s i , we gather the associated text transcript t i , video frames f i , and live comments c i , and treat the segment as an individual training sample. For this stage, we use only the text and visual modalities. We create our Video-to-Live Comment (V2LC) encoder by employing a pre-trained text encoder to encode the video text transcript into embedding S T and a Video Vision Transformer  (Arnab et al. 2021)  to encode the video frames into embedding S F . These encoded outputs are then integrated using a cross-modality encoder to produce the final output video segment embedding S. The cross-modality encoder employs the Cross-Attention to Concatenation strategy, where we first perform two streams of cross-attention, and then the output is concatenated and processed by another Transformer to model the global context. Let S T and S F denote the transcript text and video frame embeddings respectively. Let S denote the V2LC output segment embedding produced by the multimodal interactions:\n\nThen, we create the live comment feature embedding C by encoding a segment s i 's corresponding live comments c i with a second text encoder. We perform contrastive learning by maximizing the cosine similarity of the video segment embedding S and live comment embedding C.\n\nThe goal of the V2LC encoder is to project each input video segment s i into the live comment representation\n\nwhere N is the number of video segment-live comment pairs in each batch, (S i • C i ) is the similarity (dot product) between video segment embedding S i and live comment embedding C i , and τ is a learned temperature parameter.\n\nWe use the V2LC encoder to produce synthetic live comment features to augment models for downstream tasks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Multi-Label Objective",
      "text": "The original CLIP architecture was designed primarily for single-label classification tasks. In our setting, there exists a many-to-many relationship, where a single video segment usually contains multiple live comments, and identical comments may appear in multiple video segments. To reconcile this discrepancy, we have modified the CLIP training framework to accommodate multiple labels: Given a batch of N videos and K live comments, the model is trained to predict which of the N × K possible (video, live comment) pairings across a batch actually co-occur.\n\nAn additional challenge arises when multiple similar comments from different video segments appear within the same batch. In our dataset, similar comments naturally occur more frequently than the more distinct content pairs seen in the image-text datasets used in CLIP. During pre-training, if similar comments from different video segments are sampled within the same batch, it might lead to confusion in the contrastive learning setup. To mitigate this issue, we refine the training targets such that for a given video segment s, any live comment c in the batch with a vector similarity exceeding a predefined threshold θ to any actual live comment of s is also included in the target label set. Such comments are deemed correct, thereby allowing the model to learn a good live comment representation more efficiently.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Downstream Fine-Tuning Multimodal Fusion Encoder",
      "text": "Currently, in multimodal affective analysis, the text modality is commonly encoded with pre-trained language models like BERT  (Devlin et al. 2019) . In contrast, the acoustic and visual modalities have traditionally relied on hand-crafted feature extractors such as OpenSmile  (Eyben, Wöllmer, and Schuller 2010)  for audio and OpenFace  (Baltrušaitis, Robinson, and Morency 2016)  for facial expressions  (Liu et al. 2022) . These tools manually define the features to be ex-tracted, which might not capture the full complexity of the data. However, recent work has introduced large-scale pretrained encoders for these modalities which have yielded significant improvements in multimodal affective analysis. Therefore, we propose a downstream Multimodal Fusion Encoder that utilizes large-scale pre-trained encoders across all three modalities-text, acoustic, and visual-to allow more comprehensive modeling of affective tasks.\n\nWe first extract features f m from each modality with pretrained encoders, then we use a cross-modality encoder to fuse them. To accommodate three modalities, we construct the following cross-modality encoder:\n\n(3) where\n\nFinally, we pass the concatenated features Z through feedforward layers to produce our final prediction.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Augmenting With Live Comment Features",
      "text": "To enhance performance on downstream tasks, first, we apply self-attention to the synthetic live comment features produced by the V2LC encoder before mean-pooling. Then, we take the mean of these attentionally-pooled synthetic live comment features and concatenate it with Z, our multimodal representation from our fusion encoder. These combined features are then processed through feed-forward layers. We hypothesize that this joint training enables rich integration of the emotional context captured by the synthetic live comment features from our V2LC encoder, improving their efficacy in downstream affective analysis.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments Evaluation Datasets",
      "text": "We evaluated our work on six widely used affective analysis datasets: Chinese Multimodal Sentiment Analysis Dataset (CH-SIMS)  (Yu et al. 2020) , Chinese Multimodal Sentiment Analysis Dataset v2.0 (CH-SIMS v2)  (Liu et al. 2022) , Multimodal Opinion Sentiment and Emotion Intensity (MOSI)  (Zadeh et al. 2016) , Multimodal Sentiment Analysis (MO-SEI)  (Bagher Zadeh et al. 2018)  for sentiment analysis, Multimodal EmotionLines Dataset (MELD)  (Poria et al. 2019)  for emotion recognition, and Multimodal Sarcasm Detection Dataset (MUStARD)  (Castro et al. 2019)  for sarcasm detection. Summaries of the datasets can be found in Table  3 .\n\nCH-SIMS: This dataset contains short video segments from Chinese TV shows and movies, annotated with modality-specific and multimodal sentiment. We predict the multimodal sentiment labels. CH-SIMS v2: An extension of the original CH-SIMS dataset, CH-SIMS v2 adds an additional 2, 121 video segments and improves the balance and diversity of the dataset.\n\nMOSI: This dataset contains English movie review monologues from YouTube labeled with multimodal sentiment.\n\nMOSEI: An extension of MOSI, MOSEI expands its coverage to include more videos and topics.\n\nMELD: Drawn from the TV show Friends, this dataset contains utterance-level labels chosen from 7 emotions: anger, sadness, joy, neutral, fear, surprise, or disgust.\n\nMUStARD: This dataset contains video clips from Friends, The Golden Girls, Sarcasmaholics Anonymous, and The Big Bang Theory. These audiovisual utterances, with their surrounding context, are annotated with sarcasm labels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation",
      "text": "Data Processing For pre-training, we use a segment length σ of 8 seconds, balancing between required context and the length of downstream datasets. We sample 8 frames uniformly from each segment. We filter out comments that contain no meaningful information such as comments that are too short (less than 2 characters). For user generated videos, we trim the first and last 15 seconds as they tend to include repetitive comments such as greetings and farewells. For movies, we trim the first and last 5 minutes, and for TV shows, we trim the start and end of each show.\n\nSegments containing fewer than 5 live comments are excluded from pre-training to allow efficient GPU batching. For each epoch, we randomly select 5 live comments per segment so that a batch with N samples has K = 5N comments. We sample 10% of our data for validation.\n\nImplementation Details We pre-train two variants of our V2LC encoder: one that uses only Chinese text transcripts and another that uses English text transcripts when available. For the Chinese-only variant, we use Chinese-RoBERTa-wwm-ext  (Cui et al. 2020 ) as our transcript encoder; for the bilingual variant, we use XLM-RoBERTa-base  (Conneau et al. 2019) . For encoding live comments, We use Chinese-RoBERTa-wwm-ext.\n\nFor downstream fine-tuning, we use pre-trained encoders tailored to each modality and language. For the text modality, we use Chinese-RoBERTa-wwm-ext for Chinese datasets and RoBERTa-base for English datasets  (Liu et al. 2019) . For the acoustic modality, we adopt Chinese-HuBERT-base to encode Chinese data and Data2Vec-audio-base for English data. For visual features, we process all data using TimeSformer-base.\n\nOther details are specified in the Appendix.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline Models",
      "text": "We compare our model against multiple competitive baselines: Multimodal Transformer (MulT)  (Tsai et al. 2019 ): Chosen for its pioneering approach of applying attention across modalities, MulT enables dynamic adaptation between modalities at different time steps, addressing the challenges posed by unaligned multimodal data.\n\nSupport Vector Machine (SVM): Known for its robust performance on small-sized datasets, SVM can, at times, surpass neural models. Following  Castro et al. (2019) , features are concatenated via early fusion and fed into an SVM classifier.\n\nAcoustic Visual Mixup Consistent framework (AV-MC)  (Liu et al. 2022 ): The current SOTA for CH-SIMS v2, AV-MC introduces a modality mixup module as data augmentation, which mixes the acoustic and visual modalities from different videos to enhance performance.\n\nMultimodal Sentiment Knowledge-sharing Framework (UniMSE)  (Hu et al. 2022 ): Chosen for its strong performance on MOSEI and MELD, this model proposes a knowledge-sharing framework that unifies two affective analysis tasks: multimodal sentiment analysis and emotion recognition in conversation, demonstrating the effectiveness of utilizing complementary knowledge in affective tasks.\n\nVision-Language Pre-Training To Multimodal Sentiment Analysis (VLP2MSA)  (Yi et al. 2024 ): Chosen for its use of pre-trained vision models, VLP2MSA extracts spatiotemporal features from sparsely sampled video frames, offering advantages over traditional visual feature extraction. It captures not only facial expressions but also body movements, providing a more comprehensive analysis of visual information.\n\nMultimodal Multi-loss Fusion Network (MMML)  (Wu et al. 2024 ): The current SOTA for CH-SIMS and MOSI, MMML utilizes pre-trained acoustic models as feature extractors for the acoustic modality.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Metrics",
      "text": "We evaluate model performance using standard metrics for each task as established in the literature. Further details can be found in the Appendix.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "We present our experimental results on sentiment analysis in Tables  4  and 5  and emotion recognition and sarcasm detection in Table  6 . First, we note that our vanilla Multimodal Fusion Encoder before live comment feature augmentation (as indicated by \"Ours\") outperforms all of our baselines in nearly every metric across all tasks. This demonstrates the strength of utilizing pre-trained encoders for all modalities in affective analysis and establishes a new SOTA on its own. When we treat this vanilla Multimodal Fusion Encoder as a strong baseline, we see that the addition of synthetic live comment features (as indicated by \"+ LC\") yields additional gains, outperforming the vanilla Multimodal Fusion Encoder (and our other baselines) by significant margins.\n\nFor Chinese sentiment analysis, we observe a 3.18-point increase in accuracy and a 3.82-point improvement in Pearson Correlation (Corr) on CH-SIMS v2, and a 0.9-point decrease in Mean Absolute Error (MAE) on CH-SIMS.\n\nFor English, in emotion recognition, we see a 2.89-point increase in weighted F1-score on MELD. In sarcasm detection, we see a 3.0-point increase in weighted F1-score on MuSTARD. And in sentiment analysis, we also see improvements, though more modest. This is likely due to the genre shift from our pre-training dataset to the English sentiment analysis benchmarks. As noted in the dataset section, MOSI and MOSEI primarily contain YouTube monologues featuring one speaker which differs significantly from the conversational content in our collected pre-training corpus.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Affective Features Comparison Study",
      "text": "Affective Feature Baseline Models We compare our synthetic live comment features with features from models trained for related affective analysis tasks:\n\nText-based Chinese sentiment features from Erlangshen-Roberta-sentiment (Erlangshen)  (Zhang et al. 2022 ): representations from a text sentiment analysis model (Chinese-RoBERTa-wwm-ext fine-tuned on 227, 347 sentiment-labeled texts from 8 datasets).\n\nText-based Chinese offensive language features from COLDETECTOR  (Deng et al. 2022 ): representations from an offensive language detection model (Chinese-RoBERTa-wwm-ext fine-tuned on 37, 480 sentences annotated with binary offense labels from the COLDataset).\n\nImage-based facial emotion recognition features from Vit-face-expression (VIT-face) (Todor Pakov 2024): representations from a facial emotion recognition model  (Vision Transformer (Dosovitskiy et al. 2020)  fine-tuned on 35, 887 faces from the FER2013 dataset, annotated with one of seven emotions: Anger, Disgust, Fear, Happiness, Sadness, Surprise, and Neutral).\n\nAffective Feature Experiments We evaluate the effectiveness of our synthetic live comment features through controlled experimentation in two settings. In the first, we train a logistic regression classifier on individual features to measure how well each feature discriminates our evaluation data. In the second, we augment our vanilla Multimodal Fusion Encoder with individual features to evaluate how well each feature can be leveraged by more expressive multimodal models. For these experiments, we focus on Chinese sentiment analysis with CH-SIMS v2 due to the diversity of its content and the amount of data available for fine-tuning. Additional experimental details can be found in the Appendix. We present the results of these experiments in Table  7 .\n\nAffective Feature Comparison Results When using a simple logistic regression classifier, our synthetic live comment features outperform text-based Chinese sentiment from the Erlangshen even though that model was trained on a large quantity of task-specific sentiment data. Additionally, though less performant, our features are competitive with our other baseline features from models that benefit from supervised training on related affective tasks.\n\nThe strength of our synthetic live comment features becomes apparent when we move to our Multimodal Fusion Encoder. In this setting, they outperform the baselines by wide margins across all metrics considered. We hypothesize that though our synthetic live comment features are less discriminative in a linear model for a specific affective task (like sentiment analysis), they encode rich information that can be adapted to any affective task by the more expressive multimodal models that have become the standard in AI today. Our experimental results speak to this effectiveness.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we demonstrate the efficacy of synthetic live comment features in multimodal affective analysis. By constructing the LCAffect dataset and training a multimodal encoder on it, we enable the inference of these synthetic features for any video content. Our enhanced multimodal model, augmented with synthetic live comment features, achieves new SOTA results in sentiment analysis, emotion recognition, and sarcasm detection across both English and Chinese content. This advance confirms that synthetic live comment features can effectively capture a broad spectrum of affective states, opening new avenues across diverse dimensions of affective analysis.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). The richness of these",
      "page": 1
    },
    {
      "caption": "Figure 1: An example video frame and transcript from the",
      "page": 1
    },
    {
      "caption": "Figure 2: Our contrastive pre-training approach. We train our V2LC encoder to predict the correct pairings of a batch of",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1\nS1·C1": "1\nS2·C1",
          "2\nS1·C1": "1\nS2·C1",
          "3\nS1·C1": "3\nS2·C1",
          "1\nS1·C2": "1\nS2·C2",
          "2\nS1·C2": "2\nS2·C2",
          "3\nS1·C2": "3\nS2·C2",
          "1\nS1·C3": "1\nS2·C3",
          "2\nS1·C3": "2\nS2·C3",
          "3\nS1·C3": "3\nS2·C3",
          "...": "...",
          "S1·CN": "S2·CN"
        },
        {
          "1\nS1·C1": "1\nS3·C1",
          "2\nS1·C1": "1\nS3·C1",
          "3\nS1·C1": "3\nS3·C1",
          "1\nS1·C2": "1\nS3·C2",
          "2\nS1·C2": "2\nS3·C2",
          "3\nS1·C2": "3\nS3·C2",
          "1\nS1·C3": "1\nS3·C3",
          "2\nS1·C3": "2\nS3·C3",
          "3\nS1·C3": "3\nS3·C3",
          "...": "...",
          "S1·CN": "S3·CN"
        },
        {
          "1\nS1·C1": "...",
          "2\nS1·C1": "...",
          "3\nS1·C1": "...",
          "1\nS1·C2": "...",
          "2\nS1·C2": "...",
          "3\nS1·C2": "...",
          "1\nS1·C3": "...",
          "2\nS1·C3": "...",
          "3\nS1·C3": "...",
          "...": ".\n.\n.",
          "S1·CN": "..."
        },
        {
          "1\nS1·C1": "1\nSN·C1",
          "2\nS1·C1": "2\nSN·C1",
          "3\nS1·C1": "3\nSN·C1",
          "1\nS1·C2": "1\nSN·C2",
          "2\nS1·C2": "2\nSN·C2",
          "3\nS1·C2": "3\nSN·C2",
          "1\nS1·C3": "1\nSN·C3",
          "2\nS1·C3": "2\nSN·C3",
          "3\nS1·C3": "3\nSN·C3",
          "...": "...",
          "S1·CN": "SN·CN"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "ViViT: A Video Vision Transformer",
      "authors": [
        "A Arnab",
        "M Dehghani",
        "G Heigold",
        "C Sun",
        "M Lučić",
        "C Schmid"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "2",
      "title": "General Framework for Self-supervised Learning in Speech, Vision and Language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "General Framework for Self-supervised Learning in Speech, Vision and Language",
      "arxiv": "arXiv:2202.03555"
    },
    {
      "citation_id": "3",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "authors": [
        "A Bagher Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Australia Melbourne"
      ],
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "OpenFace: An open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "6",
      "title": "Is Space-Time Attention All You Need for Video Understanding",
      "authors": [
        "G Bertasius",
        "H Wang",
        "L Torresani"
      ],
      "year": "2021",
      "venue": "Proceedings of the 38th International Conference on Machine Learning"
    },
    {
      "citation_id": "7",
      "title": "Towards Multimodal Sarcasm Detection (An Obviously Perfect Paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Knowledge Enhanced Model for Live Video Comment Generation",
      "authors": [
        "J Chen",
        "J Ding",
        "W Chen"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "9",
      "title": "Unsupervised crosslingual representation learning at scale",
      "authors": [
        "A Conneau",
        "K Khandelwal",
        "N Goyal",
        "V Chaudhary",
        "G Wenzek",
        "F Guzmán",
        "E Grave",
        "M Ott",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Unsupervised crosslingual representation learning at scale",
      "arxiv": "arXiv:1911.02116"
    },
    {
      "citation_id": "10",
      "title": "Revisiting Pre-Trained Models for Chinese Natural Language Processing",
      "authors": [
        "Y Cui",
        "W Che",
        "T Liu",
        "B Qin",
        "S Wang",
        "G Hu"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "11",
      "title": "COLD: A Benchmark for Chinese Offensive Language Detection",
      "authors": [
        "J Deng",
        "J Zhou",
        "H Sun",
        "C Zheng",
        "F Mi",
        "H Meng",
        "M Huang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "12",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova",
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "13",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, MM '10"
    },
    {
      "citation_id": "14",
      "title": "COSMIC: COmmonSense knowledge for eMotion Identification in Conversations",
      "authors": [
        "C.-Y Fu",
        "J Lee",
        "M Bansal",
        "A Berg",
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "15",
      "title": "VTC: Improving Video-Text Retrieval with User Comments",
      "authors": [
        "L Hanu",
        "J Thewlis",
        "Y Asano",
        "C Rupprecht"
      ],
      "year": "2022",
      "venue": "ECCV"
    },
    {
      "citation_id": "16",
      "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc"
    },
    {
      "citation_id": "17",
      "title": "UniMSE: Towards Unified Multimodal Sentiment Analysis and Emotion Recognition",
      "authors": [
        "G Hu",
        "T.-E Lin",
        "Y Zhao",
        "G Lu",
        "Y Wu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "LiveChat: Video Comment Generation from Audio-Visual Multimodal Contexts",
      "authors": [
        "J Lalanne",
        "R Bournet",
        "Y Yu"
      ],
      "year": "2023",
      "venue": "LiveChat: Video Comment Generation from Audio-Visual Multimodal Contexts"
    },
    {
      "citation_id": "19",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "20",
      "title": "Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup Consistent Module",
      "authors": [
        "Y Liu",
        "Z Yuan",
        "H Mao",
        "Z Liang",
        "W Yang",
        "Y Qiu",
        "T Cheng",
        "X Li",
        "H Xu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 International Conference on Multimodal Interaction, ICMI '22"
    },
    {
      "citation_id": "21",
      "title": "Affective Content Analysis of Online Video Clips with Live Comments in Chinese",
      "authors": [
        "S Ma",
        "L Cui",
        "D Dai",
        "F Wei",
        "X Sun",
        "J Niu",
        "S Li",
        "S Mo",
        "S Yang",
        "B Fan"
      ],
      "year": "2018",
      "venue": "2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing"
    },
    {
      "citation_id": "22",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Learning Transferable Visual Models From Natural Language Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark",
        "G Krueger",
        "I Sutskever"
      ],
      "year": "2021",
      "venue": "Learning Transferable Visual Models From Natural Language Supervision",
      "arxiv": "arXiv:2103.00020"
    },
    {
      "citation_id": "24",
      "title": "Multilevel Emotion Transfer on YouTube: Disentangling the Effects of Emotional Contagion and Homophily on Video Audiences",
      "authors": [
        "H Rosenbusch",
        "A Evans",
        "M Zeelenberg"
      ],
      "year": "2019",
      "venue": "Social Psychological and Personality Science"
    },
    {
      "citation_id": "25",
      "title": "2024. vit-face-expression",
      "authors": [
        "Todor Pakov"
      ],
      "venue": "2024. vit-face-expression"
    },
    {
      "citation_id": "26",
      "title": "Multimodal Transformer for Unaligned Multimodal Language Sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "VideoIC: A Video Interactive Comments Dataset and Multimodal Multitask Learning for Comments Generation",
      "authors": [
        "W Wang",
        "J Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, MM '20"
    },
    {
      "citation_id": "28",
      "title": "Multimodal Multi-loss Fusion Network for Sentiment Analysis",
      "authors": [
        "Z Wu",
        "Z Gong",
        "J Koo",
        "J Hirschberg"
      ],
      "year": "2024",
      "venue": "Multimodal Multi-loss Fusion Network for Sentiment Analysis",
      "arxiv": "arXiv:2308.00264"
    },
    {
      "citation_id": "29",
      "title": "Multimodal Indicators of Humor in Videos",
      "authors": [
        "Z Yang",
        "L Ai",
        "J Hirschberg"
      ],
      "year": "2019",
      "venue": "2019 IEEE Conference on Multimedia Information Processing and Retrieval (MIPR)"
    },
    {
      "citation_id": "30",
      "title": "VLP2MSA: Expanding vision-language pre-training to multimodal sentiment analysis",
      "authors": [
        "G Yi",
        "C Fan",
        "K Zhu",
        "Z Lv",
        "S Liang",
        "Z Wen",
        "G Pei",
        "T Li",
        "J Tao"
      ],
      "year": "2024",
      "venue": "VLP2MSA: Expanding vision-language pre-training to multimodal sentiment analysis"
    },
    {
      "citation_id": "31",
      "title": "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang",
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "32",
      "title": "",
      "authors": [
        "J Zhang",
        "R Gan",
        "J Wang",
        "Y Zhang",
        "L Zhang",
        "P Yang",
        "X Gao",
        "Z Wu",
        "X Dong",
        "J He",
        "J Zhuo",
        "Q Yang",
        "Y Huang",
        "X Li",
        "Y Wu",
        "J Lu",
        "X Zhu",
        "W Chen",
        "T Han",
        "K Pan",
        "R Wang",
        "H Wang",
        "X Wu",
        "Z Zeng",
        "C Chen"
      ],
      "year": "2022",
      "venue": ""
    }
  ]
}