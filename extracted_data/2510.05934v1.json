{
  "paper_id": "2510.05934v1",
  "title": "Revisiting Modeling And Evaluation Approaches In Speech Emotion Recognition: Considering Subjectivity Of Annotators And Ambiguity Of Emotions",
  "published": "2025-10-07T13:45:09Z",
  "authors": [
    "Huang-Cheng Chou",
    "Chi-Chun Lee"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "I am also thankful for the opportunities to present my work at international conferences, such as the ACII 2017 and International Speech Communication Association VIII Grants (INTERSPEECH 2022), which provided valuable feedback and new perspectives. Publishing my work in esteemed journals, including APSIPA Transactions on Signal and Information Processing and IEEE Transactions on Affective Computing, has also been an enriching experience. I want to express my gratitude to my family, my lab-mates (BIICers), and friends",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Abstract",
      "text": "Over the past twenty years, there has been a growing focus on speech emotion recognition (SER). To develop SER systems capable of identifying emotions in speech, researchers need to gather emotional databases for training purposes. This process involves training crowdsourced raters or in-house annotators to express their emotional responses after experiencing emotional recordings by selecting from a set list of emotions. Nevertheless, it is common for raters to disagree on emotion selection from these predefined categories. To address this issue, many researchers consider such disagreements noise and apply label aggregation techniques to produce a unified consensus label, which serves as the target for training SER systems. While the common practice simplifies the task as a single-label recognition task, it ignores the natural behaviors of human emotion perception. In this dissertation, we contend that we should revisit the modeling and evaluation approaches in SER. The driving research questions are  (1)  Should we remove the minority of emotional ratings? (2) Should we only let the SER systems learn the emotional perceptions of a few people? (3) Should SER systems only predict one emotion per speech?\n\nBased on the findings of psychological studies, emotion perception is subjective.\n\nEach individual could have varying responses to the same emotional stimulus. Additionally, boundaries of emotions in human perception are overlapped, blended, and ambiguous. Those ambiguities of emotions and subjectivity of emotion perceptions inspire us to revisit modeling and evaluation approaches in SER. This dissertation explores novel perspectives on three main views of building SER systems. First, we embrace the subjectivity of emotional perception and consider every emotional rating from annotators. Also, the conventional approach only allows each rater to provide one vote for each sample. Still, we re-calculate label representation in the distributional format with the existing soft-label method by considering all ratings from all raters. Moreover, we V directly utilize ratings of individual annotators to train SER systems and jointly train the individual SER systems and the standard SER systems. The modeling of individual annotators improves the performances of SER systems on the test sets with the consensus labels obtained by the majority vote.\n\nSecondly, we rethink the determination of methods to evaluate SER systems and the formulation and definition of the SER task. We argue that we should not remove any data and emotional ratings when assessing the performances of SER systems. Also, we think the definition of SER task can have a co-occurrence of emotions (e.g., sad and angry). Therefore, the ground truth of samples should not be the one-hot single label, and it can be distributional to include more diversity of emotion perception. We propose a novel label aggregation rule, named the \"all-inclusive rule,\" to use all data and include the maximum emotional rating for the test set. The results across 4 public English emotion databases show that the SER systems trained by the train set decided by the proposed method outperformed the ones trained by the conventional techniques, including majority rule and plurality rule on the various testing conditions. Finally, we draw inspiration from psychological research on the co-occurrence of emotions. We assess the frequency with which different categorical emotions occur together, using emotional ratings from the training data of emotion databases. This matrix is then normalized, considering the frequency of each emotion class. We derive a penalization matrix by subtracting the normalized matrix from an identity matrix. We aim to apply penalties to SER systems during training when they predict rarely occurring combinations of emotions. This penalization matrix is integrated into objective functions like cross-entropy loss. The findings from the largest English emotion database indicate that using the penalization matrix enhances the performance of SER systems, even under single-label testing conditions.\n\nWith the extensive results, we conclude that  (1)  we should involve the minority of emotional ratings instead of removing them to build better-performance SER systems, VI  (2)  we should consider emotional ratings from more people instead of fewer people during training SER systems to get better-performance SER systems; (3) we should allow SER systems to predict multiple emotions to handle the possibility of co-occurring emotions in the real-life scenarios. In future work, we plan to investigate training emotion recognition systems with multi-modalities (e.g., video, text, and audio) to process signals to improve the performance of SER systems. Also, we are interested in the relationship between the number of training human-labeled data and the performances of SER systems. Furthermore, we aim to understand the performance bias in the demographic groups, such as gender, race, and age. Last but not least, we plan to build a multi-lingual emotion recognition system. with different databases and label-learning strategies on the varied evaluation sets generated by the PR-MR and AR-PR rules. The notations * , †, and ⋆ indicate significantly better performance compared to models trained using the M R T rain , P R T rain , and AR T rain sets, respectively. . . tion  [5] . Moreover, SER is crucial for voice assistants as it enables them to generate speech with appropriate emotions by predicting the emotional tone of users' voices. I also observe some startups contribute to building SER solutions to improve users' experiences, such as HUME, BEHAVIORAL SIGNALS, UNIPHORE, and COGNOVI LABS. Those companies provide emotion-aware solutions for their clients to understand users' emotions and improve user experiences.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vii",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "List Of Figures",
      "text": "Additionally, SER needs interdisciplinary collaboration, such as studies of psychology, spoken language, and natural language understanding. Most researchers follow psychological studies to design and collect emotional corpus. In common practice, researchers provide pre-defined categorical emotions for raters to choose from after listening to or watching emotional stimuli. In most public emotion databases, each sample was annotated by at least 3 annotators. However, it is expected to observe disagreement among raters on the same sample. The prior SER studies utilize label aggregation methods, such as majority vote or plurality vote, to obtain a single consensus label as the ground truth to train SER systems. This common practice removes a minority of emotional perceptions and the data without a consensus label.\n\nThe recent findings of psychological studies  [6, 7]  reveal that the emotion perception of humans is high-dimensional, and boundaries of emotions among emotion perception of humans are blended and overlap. The critical findings inspire and motivate me to revisit standard SER systems' whole modeling and evaluation methods. We have come up with the following research questions to answer: (1) Should we remove those minority emotional ratings? (2) Should we only let SER systems learn the emotional perception of a few annotators? (3) Should SER systems only predict one single emotion for each sample? Those questions could be split into two main factors, the subjectivity of emotion perception and ambiguity of emotions, contributing to the disagreement of emotion perception among raters because of human bias, including gender  [8] , culture  [9] , and age  [10] . This dissertation dives into the whole process of modeling and evaluating SER systems, and the entire process is threefold, as shown in Figure  1",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Background, Related Works, And Challenges",
      "text": "This section introduces an overview of the background and challenges of the prior SER studies.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Emotion Representations",
      "text": "There are two main ways to represent emotion perception. One is a dimensional attribute that assumes every attribute is independent of each other, like arousal  [11]  or valence  [12] . The other one is categorical emotions  [6, 7] , such as anger or sadness. This dissertation only focuses on categorical emotions since the perception of categorical emotions is better perceived across cultures than dimensional attributions  [13] .",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Emotion Recognition Systems Using Multi-/Uni-Modality",
      "text": "Emotion can be expressed through various modalities, including facial expressions, body movements, vocal speech, and text. According to Cowen and Keltner  [7] , humans can recognize at least 27 distinct emotions from video, 24 from vocal expression, and 28 from facial and bodily cues. Furthermore, humans can discern approximately 13 emotions from music. Consequently, different modalities lead to varying emotional perceptions in humans.\n\nPrevious research has employed various modalities to develop emotion recognition systems. For example, Goncalves et al.  [14]  utilized audio-visual data to build such systems. Almedia et al.  [15]  focused on training systems to identify emotions from facial expressions. Additionally, studies  [16, 17]  have successfully detected emotions through music. This dissertation, however, is concerned solely with speech-based emotion recognition systems  [18, 19] .",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Evaluation Of Ser Systems",
      "text": "Many research studies on SER primarily focus on predicting a single emotion, using methods like majority voting  [1]  or plurality rules  [20]  to derive a consensus label for evaluating SER systems. This approach often discards emotional ratings with minority opinions and data lacking a consensus label, resulting in a simpler test set. Consequently, SER system performance evaluations may only partially represent their true capabilities due to excluding some data and emotional ratings. Figure  1 .3 showcases the trends in previous SER research, highlighting that many studies approach SER as a single-label task with hard-label (one-hot encoding) targets. For instance, in Figure  1 .3, a sample from IEMOCAP was rated as frustration, frustration, anger, anger, and sadness. Since frustration and anger received equal votes, no consensus label was achieved, leading to the sample being removed from the training set (development set), resulting in data and emotional rating loss. Moreover, defining SER as a single-label task neglects the frequent co-occurrence of emotions in real-world situations  [21] . Although some research considers the subjective nature of emotion perception by using all emotional ratings as soft labels for training  [22] , the \"tie\" samples are still excluded from the test set, so the evaluation of SER systems does not fully reflect their actual performance.\n\nFew studies treat Speech Emotion Recognition (SER) tasks as multi-label tasks, contrary to other emotion recognition fields, which do. For example, in text emotion recognition  [23, 24] , image emotion classification  [25] , and audio-visual emotion recognition  [26, 27] , emotions are considered valid if they receive any vote. They use multi-hot labels, as illustrated in Figure  1 .3. A close study by  [28]  on facial expression recognition calculated soft labels based on vote frequency for each emotion, converting these into binary vectors (either multi-hot or single-hot) based on the threshold (1/(C -1), where C is the number of emotion classes). However, this misses key information about primary and secondary emotions. In label distribution learning  [29] ,  [30]  used facial expression databases to collect emotion intensity scores from multiple annotators, av- eraging these to normalize the distributional labels for system training and evaluation.\n\nSimilarly,  [31]  did this for text emotion recognition. However, these methods are challenging to apply in SER due to the lack of emotional intensity scores, as SER databases typically ask raters to select pre-defined emotions. This is possible because querying raters on intensity could be overwhelming and bias-inducing  [32, 33] . Our objective is to incorporate all data and emotional ratings in test sets to assess SER systems' performance accurately. We employ a soft-label format as ground truth, convertible to a binary vector to highlight traditional accuracy. Thus, we propose metrics to evaluate the distribution similarity between predictions and the actual data. This dissertation will report results using two such metrics.",
      "page_start": 25,
      "page_end": 26
    },
    {
      "section_name": "Label Prepossessing For Training Ser Systems",
      "text": "The most common approaches to obtain consensus labels are majority rule and plurality rule, whose definitions are introduced below. Table  1 .1 summarizes the loss of data and emotional ratings using two conventional label aggregation methods, majority rule and plurality rule. We aim to try our best to retain all data and emotional ratings to train the SER systems.\n\n• Majority Rule (MR): it selects one of the pre-defined emotion classes only if more than half of the votes select that class.\n\n• Plurality Rule (PR): it selects a class if one emotional class obtains more votes than others.\n\nWe argue that the two above conventional aggregation methods lose variations of emotion perception and the number of data during SER systems, leading to the poor ability to predict the samples that have co-occurrence of emotions. Therefore, we propose two ways to model the variations of emotion perception.  (1)  The first is to model individual raters' SER systems since each person has a different sensitivity to various emotions  [34] . For instance, some raters are good at perceiving sad emotions; some can easily sense happy emotions.  (2)  The second approach proposes a novel label aggregation rule, named the \"all-inclusive\" rule, incorporating all the labeled data with the emotional ratings in the emotion databases.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Co-Occurrence Of Emotions",
      "text": "In everyday situations, emotions often occur simultaneously  [21] . However, most previous research in SER does not account for the possibility of concurrent emotions like \"contempt and anger,\" treating the task as one requiring a single label. Although soft-label or multi-label approaches can accommodate emotion co-occurrence, they do not effectively depict the relationships between different emotions. For example, a person is more likely to feel simultaneously sad and neutral than both sad and happy. In text emotion recognition, a study by  [35]  adapted a loss function initially proposed by  [36]  to account for label correlation among raters, thereby quantifying dependency between emotions. Additionally, research by  [37]  introduced a multi-label focal objective function designed to enhance the differentiation between positive and negative emotions to improve emotion recognition perceptivity in text systems. This approach, however, overlooks the possibility of both negative and positive emotions occurring together. Unlike the studies mentioned earlier, we directly examine the relationships between co-existing categories of emotions based on their co-occurrence frequencies, allowing for mixed emotional states such as happiness and anger. For instance, consider a scenario where a girl was upset at her boyfriend for being over an hour late, but upon his arrival, she saw he was holding her a present, her favorite dress. By conceptualizing the co-occurrence frequency of emotions in a matrix and normalizing it to create a penalty matrix, we can integrate this into existing objective functions like cross-entropy to penalize models during training. The penalty matrix assigns greater loss values when SER systems predict rare emotional co-occurrences, as indicated by the training set annotations. Importantly, this approach allows for the recognition of both positive and negative emotions happening concurrently.",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "Disagreement Between Raters On The Emotion Datasets",
      "text": "Unlike common classification tasks (e.g., speaker identification or laughter/cry detection) with well-established \"gold standard\" labels, subjective tasks like emotion recognition lack clear labels and often rely on perceptual evaluations. Researchers frequently use crowd-sourcing platforms such as Amazon Mechanical Turk to collect labels rapidly and extensively  [38] . While cost-effective, this method often compromises label quality.\n\nThis trade-off is particularly significant in subjective tasks, where the inherent ambiguity amplifies annotation variability. Subjective tasks, such as emotion perception  [39]  or hate speech tagging  [40, 41] , pose unique challenges due to their fundamentally subjective nature. Obtaining labels for these tasks is complex because they heavily depend on individual interpreters. Annotator disagreements can result from various factors  [42] [43] [44] , such as diverse backgrounds leading to different interpretations, lack of interest in providing accurate labels, emotional biases, and contextual differences  [45] .\n\nThese variances introduce substantial noise into the labeling process, particularly problematic in crowd-sourced evaluations  [46] .\n\nAnnotation noise poses a major challenge, and various strategies have been developed to lessen its effects. For speech emotion classification tasks, it's important to understand that while noise can lead to discrepancies in labels, it's not the only cause of disagreement. In line with the methodologies applied in the MSP-PODCAST corpus  [39] , efficient noise reduction approaches include excluding evaluators with persistently low agreement rates, pausing crowd-sourcing efforts when agreement falls below a certain threshold, and employing in-house staff who can undergo specialized training to enhance label consistency. Yet, it's crucial to note that perceptual variations are not merely noise; they can offer valuable insights that a speech emotion recognition system should utilize.\n\nThis dissertation aims to demonstrate that traditional methods of label aggregation, such as majority or plurality voting, often overlook the nuanced nature of subjective perception and may not be suitable for speech emotion classification. We propose an alternative aggregation method for the speech emotion recognition task, aiming for a more comprehensive and inclusive approach to label aggregation.",
      "page_start": 28,
      "page_end": 29
    },
    {
      "section_name": "Contributions",
      "text": "The research goals of this dissertation aims to answer three questions:\n\n(1) Should we remove those minority emotional ratings?\n\n(2) Should we only let SER systems learn the emotional perception of a few annotators?\n\n(3) Should SER systems only predict one emotion for each sample?\n\nThis dissertation proposes three methods to improve the process for label preprocessing, evaluation of SER systems, and training of SER systems, respectively.",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Every Rating Matters Considering The Subjectivity Of Annotators",
      "text": "We first explore the inherent subjectivity of how each annotator perceives emotions to improve the performances of SER systems. We aim to maximize the emotional ratings using the existing soft-label method introduced by  [22]  for training SER systems.\n\nAdditionally, we develop an individual SER model for each annotator based on their respective ratings. To integrate all possible emotional data, we merged the embeddings obtained from pre-trained SER models using the traditional hard-label and the existing soft-label methods across five individual annotator SER systems for late-fusion. The results indicate that the proposed framework improves performance when assessed on a test set determined by a majority vote.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Novel Evaluation Method By An All-Inclusive Aggregation Rule",
      "text": "In addition, we implemented an innovative label aggregation approach that incorporates all annotated data from the test phase, ensuring no data is overlooked. We propose maintaining all emotional ratings within test data samples to precisely evaluate the performance of SER systems, since determining consensus labels isn't practical in realworld situations. Our ground truth is structured as a distribution reflecting the frequency ratio of emotional votes for each emotion class. Like the study conducted by  [22] , we believe that using a distributional similarity metric offers a more accurate assessment of SER systems' performance compared to accuracy-based metrics like the macro-F1 score, due to the subjective nature of SER. Distributional metrics better match how humans perceive emotions  [6, 7] . Nevertheless, since the SER field is more accustomed to accuracy-based metrics, we also provide results using those. However, we've noticed that transforming distributional labels into binary vectors for accuracy evaluation might lead to losing some emotional ratings, which is different from our goal. Still, accuracy-based metrics give the community and reviewers a more precise understanding.",
      "page_start": 30,
      "page_end": 31
    },
    {
      "section_name": "Training Loss Using Co-Occurrence Frequency Of Emotions",
      "text": "To model the connection between co-occurrence of emotions, we counted the counts of co-occurrence of emotions based on labeled data in the train set of the emotion database as a matrix and normalized the frequency matrix by the number of individual emotion classes. Then, we use the unit matrix to subtract the normalized frequency matrix as a penalization matrix. To penalize the SER systems during training when the models predict rare co-occurrence of emotions, we integrate the designed penalization matrix into the current common objective functions, e.g., cross-entropy. Considering the link to the co-occurrence of emotions, this method improves the SER performances on the test conditions, including single-label and multi-label tasks.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Outline Of The Dissertation",
      "text": "The rest of the dissertation is structured as follows. It primarily focuses on three key aspects of modeling and evaluating SER systems. We begin by presenting the public emotion databases (Chapter 2), and then introduce the three proposed methods: modeling the subjectivity of annotators (Chapter 3), a novel evaluation method (Chapter 4), and a training loss that accounts for the co-occurrence of emotion classes (Chapter 5).",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Chapter 2 Emotion Databases",
      "text": "The chapter discusses four public emotional databases leveraged in this dissertation.\n\nWe might use one or multiple databases in different chapters, but we introduce them here.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Iemocap",
      "text": "The IEMOCAP dataset  [47]  was carefully constructed from motion capture, audio, and visual data. Ten professional actors speak English. This unique dataset includes scripted and spontaneous dialogues, primarily focusing on romantic relationship scenarios to evoke diverse emotions. Each session involved one male and one female actor. To guarantee an expressive variation, performers utilized scripts to elicit distinct feelings. The final recordings were segmented into 10,039 utterances. All utterances have human-typed transcripts. Raters watched segmented clips and selected emotions from a predefined list of ten categories: neutral, happy, sad, angry, surprised, fear, disgusted, frustrated, excited, and \"other.\" Addressing issues related to results reproducibility, mentioned in prior research  [48] , we have provided meticulous details on dataset splits in Section 2.5.2. Given the original dataset's deficiency of standardized split sets, our documentation aims to bridge that gap, ensuring greater consistency and reproducibility. The IEMOCAP is used in Chapter 3 and Chapter 4.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Improv",
      "text": "The MSP-IMPROV dataset, also known as IMPROV  [49] , collects audio-visual",
      "page_start": 34,
      "page_end": 34
    },
    {
      "section_name": "Crema-D",
      "text": "The CREMA-D dataset  [51]  collects high-fidelity audio-visual recordings from 91 professional actors, comprising forty-three women and forty-eight men. They were directed to deliver one of six unique emotions with the given scripts: angry, disgusted, fearful, happy, sad, or neutral. A key highlight is the comprehensive labeling procedure; spanning over 7,442 segments, each segment was evaluated by more than two thousand distinct crowdsourcing raters. All data were subjected to assessments from no fewer than six annotators, who identified one of the six defined emotions for each performance.\n\nThe perceptual annotation process operates within three contexts: voice-and faceonly and audio-visual. Raters focus exclusively on listening to the segments' audio in the voice-only context. They watch the actors' faces without audio input in the faceonly context. Lastly, in the audio-visual context, annotators evaluate both the facial expressions and the audio concurrently.\n\nFor this dissertation on SER, we specifically concentrate on the emotional labels gathered from the voice-only scenario. Unlike numerous previous SER studies that leveraged labels from the audio-visual context or omitted annotation specifics entirely, we decided to depend exclusively on voice-only labels for the evaluation. Furthermore, our paper includes detailed specifications regarding the dataset splits we utilized, which can be found in Section 2.5.3. The CREMA-D is only used in Chapter 4.",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Msp-Podcast",
      "text": "The MSP-PODCAST dataset  [39]",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Standard Partition",
      "text": "The preceding study  [48]  indicates that 80.77% of SER research papers produce irreproducible results with the widely recognized IEMOCAP dataset. The primary obstacle to reproducibility is the absence of standardized data splits (e.g., training, development, and test sets) within the database. Previous studies each defined their partitions; however, they often withheld detailed partitioning methodologies or source code, complicating repeatability. Consequently, this dissertation aims to make SER more transparent and reproducible for everyone. We establish and define standard partitions for four prominent and publicly available emotion databases, facilitating future SER advancements.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Standard Partition Of The Iemocap",
      "text": "In a speaker-independent scenario, models are trained using data from specific individuals and evaluated using data from entirely different individuals who were not part of the training set. This approach ensures an unbiased and robust assessment of the model's performance. For instance, in the IEMOCAP study, we summarize the data partitioning approach in Table  2 .1. Each session involves two speakers in interactive dialogues and allows us to define five speaker-independent splits, referred to as Ses. 1 through Ses.\n\n5. We perform a five-fold cross-validation, as detailed in Table  2 .1, whereby every fold",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Standard Partition Of The Msp-Imrpov",
      "text": "The IMPROV dataset is segmented into 6 distinct folds for cross-validation purposes. All folds feature a specific blend of development, training, and test data as detailed in Table  2 .2. The splitting method provides the SER system with information on interactions featuring various pairs of speakers and testing on completely new speaker pairings. Consequently, this strategy systematically evaluates the model's capacity to generalize to various dyadic exchanges within the IMPROV dataset.",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Standard Partition Of The Crema-D",
      "text": "The CREMA-D database is split into 5 subsets according to speaker IDs for the speaker-independent context. Each subset comprises a unique blend of males and females and specific speaker IDs, elaborated in Table  2 .3. The partitioning strategy aligns with the methodology employed for the IEMOCAP dataset discussed in section 2.5.1.",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Chapter 3 Every Rating Matters Considering",
      "text": "",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Subjectivity Of Annotators",
      "text": "We present a methodology in which joint learning addresses emotional rating uncertainty and annotator idiosyncrasies by leveraging both hard and soft emotion label annotations and individual and crowd annotator modeling. Further analyses reveal that emotion perception heavily depends on raters. Combining hard labels with soft emotion distributions provides a well-rounded approach to affect modeling. Additionally, the integrated learning of both general emotional insights and specific rater profiles yields the highest accuracy in emotion recognition.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Motivation",
      "text": "Traditional SER systems  [1, 2]  typically use the plurality rule or majority rule from a group of raters as the learning targets, termed the hard label, to train emotion recognizers. However, factors like gender  [8] , culture  [9] , and age  [10]  significantly influence emotion perception, leading to natural disagreement and ambiguity in annotations  [52, 53] . Consequently, the hard label approach overlooks emotion perception's diverse annotations and subjective nuances. To address this limitation, researchers  [2, 22]  have proposed using soft labels-a distributional representation instead of a single definitive label-to capture blended emotion perceptions better. While the soft labeling method enhances flexibility in representing the variability of emotion perception, it still disregards the unique input of individual annotators because it creates the label distribution by aggregating inputs from all annotators. Therefore, we first build individual-based SER systems to model diverse and accurate subjectivity of emotion perception to improve aggregated emotion performance.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Background And Related Works",
      "text": "",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Subjectivity Of Emotion Perception",
      "text": "Variability in how raters perceive emotions is not a new observation. Annotator modeling, which addresses this issue, has gained attention for years. For instance, the work  [54]  employed agreement between raters to assign weights to training instances, which facilitated the development of a speaker-dependent audio-visual emotion recognition system. Similarly, Han et al.  [55]  introduced a model that leverages inter-rater disagreement to estimate perception uncertainty, thereby enhancing performance in continuous dimensional emotion tracking from audio and video sources. However, they still consider the ratings of all raters at that same time, but our method directly models individual raters' emotion perception, which can preserve more subjectivity of emotion perception.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Mixture Of Annotators",
      "text": "Disagreement is present not just in emotion perception but also in various other areas like medical image tagging. Yan et al.  [56]  suggested that discrepancies arise because each annotator possesses unique medical domain knowledge. To address this, they proposed a method involving multiple annotators, which takes into account all available information by repeatedly using training data points until the models fully grasp each annotator's input. Their approach was found to be more effective than the traditional method that uses majority voting to determine ground truth.",
      "page_start": 38,
      "page_end": 39
    },
    {
      "section_name": "Soft-Label Training Method For Ser Systems",
      "text": "Steidl et al.  [22]  initially argued that using only a single emotion label as ground truth in emotion recognition tasks might not be suitable due to the subjective nature of emotion perception. They proposed using soft labels as ground truth based on count data and utilized entropy loss as an evaluation criterion. However, they re-assigned categorical emotions to enhance inter-rater agreement. In contrast, we retained the original labels to reflect the original emotion perception. Furthermore, Fayek et al.  [57]  showed that training SER systems with soft labels can yield better performance than those trained with hard labels on test sets where the majority rule determines the consensus label. Additionally, Ando et al.  [2]  adjusted the soft labels with a small α value, creating an effect similar to label smoothing for training their SER systems. Their findings indicated that SER systems trained with these modified soft labels outperformed those trained with common soft labels and hard labels. Finally, Zhang  [58]  was the first to demonstrate the advantages of using soft labels in cross-corpus SER, through their proposed objective function.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Resource And Task Formulation",
      "text": "We employ the IEMOCAP database as referenced in section 2.1, encompassing emotional ratings by 12 distinct raters across 10 categorical emotion classes. For consistency with previous research, we utilize the identical evaluation data wherein the entry is tagged with a singular emotion state, determined by a majority vote from more than three annotators. This study focuses on recognizing four primary emotion classes: sad, neutral, happy, and angry. Following the practices in  [1, 2] , we consolidate the happiness and excitement categories into one: happiness. This aggregation includes 5,531 data samples used in the emotion recognition evaluation; this approach aligns with the traditional use of the IEMOCAP dataset as a benchmarking standard. The test set excludes the data without consensus labels. The emotion class distributions of data samples are sad: 19.60%, happy: 29.58%, neutral: 30.88%, and angry: 19.94%. Half of the raters are also actors, while the other half consists of in-house raters. We selected only 5 out of the 6 in-house raters (E1, E2, E4, E5, and E6) since these five provided annotations for samples across all 5 sessions. Therefore, the proposed method only builds those 5 individual annotators.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Speech Emotion Classifier",
      "text": "",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Input Features",
      "text": "We use the openSMILE toolkit  [59]  to derive frame-level acoustic features from utterances. Specifically, the \"Emobase.config\" configuration helps us obtain a 45-dimensional feature set. This set includes 12-dimensional Mel-Frequency Cepstral Coefficients (MFCCs), voice probability measures, zero-crossing rates, fundamental frequency (F0) values, loudness metrics, and their respective first-order derivatives. Additionally, the secondorder derivatives of both loudness and MFCCs are featured. The feature extraction process is carried out with a frame length of 60ms and a step size of 10ms. These features are normalized for each speaker with z-score normalization and then downsampled by averaging over sets of five consecutive frames.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Model Structure",
      "text": "Utilizing the framework  [1] , all models in this study are designed. This framework comprises an input layer, a bidirectional long short-term memory (LSTM) layer, a fully connected layer, and an output layer. Mirsamadi et al.  [1]  investigated the various attention mechanisms, and their proposed weighted pooling layer considering the attention weights applied over the frame-level input features achieved the best performance when the input features extracted by the \"Emobase.config\" file in the OpenS-MILE toolkit. Therefore, all models used the same structure, and we denoted the model as the BLSTM-FC.",
      "page_start": 41,
      "page_end": 41
    },
    {
      "section_name": "Training Labels",
      "text": "All models utilize the BLSTM-FC architecture, consistent with the design proposed in prior research  [1] . Our objective in this study is to address the subjectivity of emotional perception. To tackle the variability in emotion perception, we train BLSTM-FC models using two distinct label learning: hard-label (denoted as CROWD H ) and softlabel (denoted as CROWD S ). The hard label is the conventional way to obtain a singlelabel emotion as ground truth. To consider that emotion perception could be overlapped and blended, Steidl et al.  [22]  first propose the soft labels to calculate the distributional label based on the votes for each emotion class. Fayek et al.  [57]  integrated the interrater variability with soft-label to build SER systems. Also, Ando et al.  [2]  modified the formulation of the calculation of soft labels by introducing α to slightly change the distribution of conventional soft labels as below.\n\nwhere c i means the i-th emotion class, n represents the n-rater, v n i is the binary value to check whether n-rater chooses c i emotions, C is how many categorical emo-tions and the R represents the number of annotators for t samples. In this dissertation, we follow the study  [2]  to assign the α value as 0.75, and C is 4 since the number of emotion classes is 4.\n\n3.5 Proposed Method",
      "page_start": 41,
      "page_end": 41
    },
    {
      "section_name": "Rater-Modeling",
      "text": "Due to the inherent subjectivity and unique individual differences, different people may interpret the exact spoken phrase in varied ways. To enhance the SER systems, we incorporate rater modeling. We categorize annotators into two groups: Crowd and E.\n\nIn this context, Crowd refers to their collective annotations in the used dataset, while E pertains to the annotations by each specific individual rater. For the Crowd category, we develop two distinct models based on whether the targets are hard or soft labels (as detailed in Section 3.4.3). Conversely, for the E category, models are trained using soft labels available to each annotator, implying that each annotator's quantity of annotated data varies.",
      "page_start": 42,
      "page_end": 42
    },
    {
      "section_name": "Final Concatenation Layer",
      "text": "After successfully computing all model components, including the two Crowd models and 5 E N models, we froze their respective model weights. Then, we concatenate the representation from the final layer before the softmax activation in each BLSTM-FC model (depicted within the square box in Fig.  3 .1). This concatenated layer is followed by an additional softmax layer to output the last prediction. The entire architecture is presented in Fig.  3 .1.  tified Linear Unit (ReLU) activation functions, a BLSTM layer enhanced using attention weights, and concludes with a fully-connected layer employing a softmax function.",
      "page_start": 43,
      "page_end": 44
    },
    {
      "section_name": "Experimental Setup",
      "text": "Specifically, the model comprises 256 hidden units in the initial dense layer, 128 hidden units in the BLSTM layer with attention, 256 hidden units in the second dense layer, and the final softmax-enabled layer differentiates into four units. Each layer also integrates a dropout mechanism with a 50% • Baseline CROWD H : This model closely parallels the previous proposed, but it utilizes a BLSTM-FC framework trained on hard labels in the study  [1] .",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "All Model Comparison",
      "text": "• Baseline CROWD S : This model employs soft label training, a method proposed by the work  [2] , designed to utilize all labeled samples.\n\n• Baseline CROWD HS : The model represents a fusion of Crowd H and Crowd S .\n\nIt combines all Crowd-relevant information by concatenating the representations from Crowd H and Crowd S before feeding them into the final softmax layer.\n\n• Proposed Rater Model, E N : Each of the E N models is trained using soft label learning based on the annotations made by individual raters.\n\n• Proposed Fusion of E N : The model integrates all individual E N components (five separate annotators). It consolidates all rater-specific information by concatenating the representations from each E N before passing them through the final softmax layer.\n\n• Proposed Model: As depicted in Figure  3 .",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Other Training Details",
      "text": "We run the experiments on the cross-validation in the leave-one-session-out condition (as shown in Table  3 .3), evaluating performance in an unweighted average recall (UAR). The batch size is 32; the learning rate is 0.0001; the number of epochs is 200.\n\nEarly stopping criteria to minimize the loss value on the validation set are applied during training across all configurations to prevent overfitting and ensure model effectiveness.\n\nThe optimizer utilized in this study is ADAMMAX  [60] .    Last but not least, by employing individual rater models, our proposed method can expand the dataset used for building SER systems, compared to the traditional hardlabel approach that limits the number of data in the train set to instances where consensus among raters is achieved. This allows for a more robust and comprehensive understanding of emotional nuances.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Summary",
      "text": "Human perception of emotions is relatively subjective and differs greatly from per- We emphasize the need to account for all annotations and samples in the dataset, as focusing only on performance metrics derived from a test set filtered by majority or plurality rules may skew the model's performance evaluations. We specifically investigate SER tasks and note that traditional aggregation rules result in data loss ratios between 4.63% and 92.01%, as shown in Table  4 .1. Based on this insight, we introduce a versatile, all-inclusive rule, a label aggregation approach to appraise SER systems using comprehensive test data. We differentiate the conventional single-label approach with a multi-label methodology catering to the coexistence of various emotions. Training an SER model with data chosen by the all-inclusive rule consistently achieves better macro-F1 scores when evaluated on the whole test set, including ambiguous, non-consensus samples.",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Motivation And Background",
      "text": "Given the inherently subjective nature of these tasks, models are often evaluated using labels derived from human perceptual assessments, where multiple raters annotate each data point. The common practice for processing these annotations and creating training and testing sets relies on majority or plurality aggregation methods. These methods ignore annotations that do not correspond with the consensus label. However, co-existing emotions are frequently observed in everyday interactions  [21] , making it We propose a practical methodology that amalgamates all annotations gathered from subjective evaluations for training and test sets, thereby enhancing the practical applicability of these systems. Although this approach is compatible with any domain requiring labels derived from perceptual assessments, our focus is on the SER task, where co-occurring emotions are common in everyday interactions. Unlike traditional methods that neglect non-consensus labels, we retain all data points in the training and test sets. This strategy enables SER models to utilize comprehensive data during training  • How is the performance of SER systems influenced by using different aggregation methods for the training set annotations?\n\n• Does utilizing data from the all-inclusive rule in training an SER system enhance performance on ambiguous emotions compared to data processed with majority or plurality rules?\n\n• Which label learning strategy should be employed for training SER systems to achieve optimal performance when tested on the entire data set?\n\n4.2 Previous Literature",
      "page_start": 49,
      "page_end": 50
    },
    {
      "section_name": "Evaluation Of Ser Systems",
      "text": "Evaluating SER systems on a complete test set is crucial. However, the common approach involves discarding samples that lack consensus emotion labels. When gathering emotional annotations from multiple workers, significant disagreement often exists among annotators  [66] [67] [68] , leading many studies to eliminate numerous data points from the test set. For instance, the IEMOCAP and CREMA-D corpora use the majority rule (MR) to construct ground-truth labels, discarding approximately 31.37% and 35.8% of the data, respectively, as shown in Table  4 .1  [47, 51, 69] . Researchers using these corpora often adhere to this rule for testing their models  [70] [71] [72] [73] . Similarly, the IMPROV and PODCAST corpora use the plurality rule (PR) to annotate primary and secondary emotional labels for each speaking turn  [39, 49] , and this default aggregation method has been widely adopted in subsequent studies  [74] [75] [76] .\n\nThe aforementioned studies assume that each speaking turn has only one emotional category, ignoring secondary emotions in the recordings. In reality, emotional states often co-exist (e.g., a person can be sad and angry)  [21] . Therefore, consolidating multiple annotations into a single class and discarding non-consensus data points does not accurately capture whether SER system predictions reflect the complex emotional behaviors observed in daily interactions. Although some studies have explored using a \"multiple-hot\" vector to frame SER as a multi-label problem  [23, 27, 77] , this approach does not discern dominant emotions. It treats all annotations equally valid, even if a single annotator selected a class.\n\nTo our knowledge, Riera et al.  [78]  is the only study advocating for including all test samples in evaluating SER systems rather than discarding non-consensus data. How-",
      "page_start": 51,
      "page_end": 51
    },
    {
      "section_name": "Curriculum Learning For Emotion Recognition",
      "text": "",
      "page_start": 51,
      "page_end": 51
    },
    {
      "section_name": "Methodology",
      "text": "We present a new aggregation methodology called the all-inclusive rule (AR), designed to facilitate the training and evaluation of SER systems using an exhaustive test set. This includes data points lacking Majority Rule (MR) or Plurality Rule (PR) consensus. The definition, significance, and application of this rule are thoroughly explained.",
      "page_start": 53,
      "page_end": 53
    },
    {
      "section_name": "Proposed All-Inclusive Rule",
      "text": "The All-Inclusive Rule (AR) is an aggregation methodology that retains all annotated samples within a dataset, regardless of vote frequencies. This method ensures data points are never disregarded. Initially, AR collects all classifications assigned to each data point, creating the ground truth. When forming the training set, the ground truth is represented either by a one-hot encoding or the vote distribution based on the selected label learning strategy. For the hard-label approach, AR identifies the emotional class with the most votes as the ground truth-akin to the plurality rule. In cases where a clear majority is absent, one of the top-voted classes is randomly selected as the ground truth.\n\nAn example is shown in Table  4 .2 Case (C1), where the hard label could be (1,0,0,0) or (0,0,1,0). AR produces the ground truth for the soft-label or distribution-label approach by reflecting the vote distribution among emotional classes.\n\nAR consistently utilizes the distributional ground truth when producing the test set, irrespective of the chosen label-learning strategy, which is highlighted in the rightmost column of Table  4 .2. This comprehensive approach ensures every annotated data point and all its annotations are integral to the test set. The all-inclusive rule enhances the label descriptor, more accurately capturing the emotional nuances of data points by integrating sentences that reflect ambiguous emotions into the test set.",
      "page_start": 53,
      "page_end": 53
    },
    {
      "section_name": "Employing The All-Inclusive Rule For Test Set Construction",
      "text": "We include all data samples and prioritize every available emotion as a learning target to capture the full range of opinions gathered during perceptual evaluation. For instance, previous research using the IEMOCAP corpus has aggregated annotated emotions into a 4-class emotion classification task (e.g., combining excitement and happiness while disregarding less frequent classes such as fear, surprise, and disgust). Unlike this methodology, we do not overlook any emotional states nor confine the SER models to be trained or tested solely on a few selected emotions.\n\nIn addition, our all-inclusive rule allows SER models to be tested on the entire test set, including secondary emotions. Previous studies have often disregarded secondary emotional annotations due to considerable data loss from standard aggregation methods (up to 92.01% as noted in Table  4 .1). As the AR method utilizes the fully annotated test set, we can assess the SER model with secondary emotions, which have not been examined before.",
      "page_start": 54,
      "page_end": 54
    },
    {
      "section_name": "Speech Emotion Classifier",
      "text": "To assess the performance of various aggregation methods, we utilize the Wav2vec2.0 architecture  [82] , which has demonstrated strong performance in SER tasks in multiple studies  [83, 84] . Specifically, we implement the \"wav2vec2-large-robust\" variant, as proposed in Hsu et al.  [85] , which has proven to be the best-performing model in the Our implementation is based on the HuggingFace library  [86]  and uses a pre-trained \"wav2vec2-large-robust\" model. During fine-tuning, convolutional and transformer layers of the Wav2vec2.0 model are frozen-a strategy that has shown better performance than fully fine-tuning all parameters  [83] . We employ the Adam optimizer  [87]  and set",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "Train/Test Set Defined By Aggregation Rules",
      "text": "In this evaluation, the models are trained and tested using both matching and mismatching aggregation rules. For hard-label learning, the ground truth is constructed with a one-hot encoding, where the class receiving the highest number of votes from annotators is represented as \"1.0\". When utilizing the training set aggregated with AR, if there is no clear consensus, one of the top-voted emotions is randomly chosen as the ground-truth emotion. We smooth the one-hot encoding ground truth vector using the smoothing strategy proposed by Szegedy et al.  [89]  with a parameter set of 0.05. This method slightly adjusts probabilities for classes assigned initially a zero value. The SER systems are then trained using the cross-entropy (CE) objective function.\n\nFor both soft-label learning and distribution-label learning, the ground-truth vector reflects the distribution of annotator votes. This is achieved by dividing the vote count for each class by the total number of votes for each data point. We also apply the labelsmoothing strategy used in hard-label learning. Soft-label learning targets are optimized using the CE loss function, while distribution-label learning uses the Kullback-Leibler divergence (KLD) as the cost function.",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Evaluation Metrics And Statistical Significance Hard-Decision-Based Assessment",
      "text": "This study employs macro-F1 scores to evaluate SER performance, which involves calculating precision and recall rates. The MR, PR, and PR-MR test sets are structured by selecting a single class, making them compatible with macro-F1 scoring. The class receiving the most votes is chosen as the target for these test sets. An output is valid if the emotion category with the highest predicted probability matches the target class.\n\nFor test sets gathered under AR and AR-PR conditions, which contain non-consensus labels, we permit the coexistence of multiple emotions to compute the macro-F1 score.\n\nTargets are selected based on using the threshold to the binarized vectors. A prediction is valid if the proportion for a specific category exceeds 1/C, with C representing how many emotion classes. This method is in line with those employed in previous studies  [78, 90] .\n\nConsider an emotion recognition task that distinguishes between four emotions: neutral, anger, sadness, and happiness. In one instance, five different reviewers each gave their rating based on the sample, resulting in the following annotations: angry (A), sad (S), sad (S), neutral (N), and angry (A). To determine the label distributions, we categorize the data into (N, A, S, H) and get the proportions (0.2, 0.4, 0.4, 0.0).\n\nThe threshold is set to (1/4 = 0.25), so we convert the ground truth to (0,1,1,0). During the inference, suppose we have predictions from three different models: (0.1, 0.45, 0.45, 0.0), (0.2, 0.35, 0.35, 0.1), and (0.45, 0.1, 0, 0.45). Applying the (0.25) threshold, these outputs are converted into (0,1,1,0), (0,1,1,0), and (1,0,0,1), respectively. In this example, the (0,1,1,0) and (0,1,1,0) fully match the ground truth.",
      "page_start": 58,
      "page_end": 59
    },
    {
      "section_name": "Distribution-Based Assessment",
      "text": "Following the approach proposed by Steidl et al.  [22] , where results are assessed using an entropy-based metric, we employ the Kullback-Leibler divergence (KLD) to determine the similarity between the model's predicted distribution and the subjective annotations. This method checks whether an SER model aligns with human emotional perception. Unlike the macro-F1 evaluations, which binarize the model's output for single-label or multi-label tasks, we utilize the model's probability distributions across all test sets and measure them using KLD. As illustrated in Table  4 .6, lower KLD values indicate better performance in this context.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Distribution-Based Assessment Versus Hard-Decision-Based Assessment",
      "text": "Distribution-based assessments, such as KLD, are suitable metrics for evaluating distributions. In contrast, hard-decision-based assessments like the F1-score necessitate categorical decisions, making them less appropriate for comparing distribution similarities. Distribution-based assessments keep all data and maximum usage of emotional ratings while evaluating the SER performances. Every assessment has different advantages and disadvantages, so both are presented in the paper. Table  4 .4 outlines the benefits and limitations of using KLD versus traditional accuracy metrics (e.g., macro-F1, micro-F1, and weighted-F1). Reporting both metrics offers complementary and more detailed insights.",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "Statistical Significance",
      "text": "We assess the statistical significance of the results per each aggregation method utilized. For cross-validation experiments (IEMOCAP, CREMA-D, and IMPROV), the",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "Metric Distribution-Based Assessment Hard-Decision-Based Assessment",
      "text": "",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "Advantages",
      "text": "• We can directly compare the models' predictions to human perception without needing extra thresholding methods.\n\n• It is tuned to the overall shape and structure of the distributions, reflecting both the accuracy and confidence of the predictions.\n\n• The macro-F1 score has a defined range between 0 and 1, facilitating straightforward interpretation and comparison across various models and datasets.\n\n• By applying thresholds to the labels, we can mitigate the influence of noisy annotations from raters who were not attentive.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Limitations",
      "text": "• The performance differences between the baseline and proposed models are difficult to interpret because the scale differences are minimal.\n\n• It lacks a fixed range, which makes it challenging to interpret and compare absolute values across different datasets or models.\n\n• It is susceptible to slight distribution variations, especially when dealing with sparse or high-dimensional data.\n\n• All predictions and ground truth need to be binary (0 or 1)-this requirement can lead to information loss and reduced granularity, as it oversimplifies the distributional nature of the ground truth.\n\n• It is challenging to assess model performance on less common emotions, such as fear, due to limited available data.\n\npredictions for each condition across all folds are concatenated, ensuring that every piece of data is considered (i.e., each sample appears in one fold of the test set). Predictions from all pre-defined test sets using a single model are employed directly for the PODCAST experiments. After collecting these predictions, they are segmented into 40 folds to average the macro-F1 score. A two-tailed t-test is then conducted to determine statistical significance and to confirm whether the p-value is below 0.05. The notations * , †, and ⋆ are used to indicate when a model's performance is significantly superior to models trained using M R T rain , P R T rain , and AR T rain sets, respectively.",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "Experimental Results And Analyses",
      "text": "The experimental analysis begins by benchmarking our presented method against state-of-the-art (SOTA) baselines, showcasing the advantages of the SER strategy utilized in this research (Section 4.5.1). Following that, we address the three research questions outlined in Section 4.1 (Sections 4.5.2, 4.5.3, and 4.5.4).",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Comparison Of Results With Prior Sota Methods",
      "text": "We assess the SER model's performance against three existing SOTA benchmarks using the IMPROV(P), CREMA-D, PODCAST(P), and IEMOCAP corpora. As discussed by Li et al.  [91] , the first reference model establishes an end-to-end system that converts speech into spectrograms and utilizes a self-attention mechanism to highlight emotional elements within sentences. At its time of publication, this model set the SOTA performance for identifying four primary emotions from the IEMOCAP database. The second baseline is presented in the work by Pepino et al.  [92] , which leverages wav2vec All these baseline models were reconstructed as described in their respective research studies and evaluated under the same experimental conditions as the model.\n\nFollowing previous research, the investigations treat SER as a single-label problem, providing performance results under MR or PR test paradigms while training and evaluating models across all primary emotion categories. Table  4 .5 details the comparative   databases. These findings substantiate that the SER approach is robust against the SOTA references.\n\nSpecifically, on the IMPROV(P) corpus, our method achieves a macro-F1 score of 0.562 with ART rain, outstripping the result from Goncalves and Busso  [75]  which had a score of 0.539. Nevertheless, training with PRT rain resulted in a lower macro-F1 score of 0.512 compared to 0.539. For the CREMA-D set, we report a result of 0.591 using the MR T rain set and 0.585 with the AR T rain scenario in a macro-F1 score, both excelling beyond the SOTA score of 0.574. Moreover, our approach similarly outperforms other SOTA methodologies  [75, 91, 92]  in both the IEMOCAP and PODCAST(P) datasets.    and incomplete test data in a cross-corpus setting.",
      "page_start": 61,
      "page_end": 64
    },
    {
      "section_name": "Assessment With Full And Partial Test Data",
      "text": "",
      "page_start": 64,
      "page_end": 64
    },
    {
      "section_name": "Assessment On The Complete Test Set (Ar)",
      "text": "When evaluating using the AR approach with all annotated data in the test set, Fig-",
      "page_start": 64,
      "page_end": 64
    },
    {
      "section_name": "Assessment Of The Incomplete Test Sets (Mr & Pr)",
      "text": "The single-label SER performance was assessed under the MR and PR test conditions. As detailed in Table  4 .7, testing with the PR set consistently yielded lower performance than testing with the MR set, as the MR set omits more ambiguous samples. Similarly, performance in the PR-MR condition was generally worse than in the PR conditions. These results highlight that including more ambiguous samples (lacking majority consensus) in the test set-common in practical scenarios-can degrade SER model performance. Therefore, using only PR or MR to define the test set might not accurately reflect the outcomes likely to be encountered in real-world deployments where  recognition of every sentence is necessary.\n\nOut of the 18 conditions analyzed, Table  4 .7 indicates that when tested with the PR set, training with the AR T rain configuration resulted in the best performance in 11 out of 18 cases (approximately 61%), and 14 out of 18 cases (approximately 78%) when tested with the AR set. Figure  4 .2b presents statistically significant evidence that the average macro-F1 scores for models trained with AR T rain were superior to those trained with the P R T rain set. These findings suggest that using the AR approach for training aggregation may enhance SER performance on samples with lower annotation agreement.\n\nFor tests evaluated on the MR condition, models trained with the AR T rain set outperformed others in only 7 out of 18 experiments (approximately 39%). Figure  4 .2a shows a decline in performance for the AR T rain trained model compared to those trained with either the M R T rain or P R T rain sets.\n\nIncluding more complex samples in the training set (AR T rain ) appears to reduce accuracy for the most straightforward samples; however, this trade-off bolsters the model's resilience in real-world contexts where both ambiguous and unambiguous samples are commonly encountered. This suggests that training SER systems with the AR T rain set could be more efficient for real-life applications, reflecting the genuine mix of sample types in practical environments. Additionally, increased training samples with the P R T rain set displayed better performance than those trained with M R T rain , aligning with findings reported by Chou et al.  [18] .",
      "page_start": 64,
      "page_end": 65
    },
    {
      "section_name": "Assessment In Cross-Corpus Scenarios",
      "text": "The past experimental outcomes were obtained using within-corpus settings. Our interest is to investigate the impact of training models with various aggregation strategies in cross-corpus settings. We've opted to train the model with the PODCAST (P) corpus and test its performance using the IMPROV (P) corpus to showcase the advantages of the AR method in cross-corpus experiments. The IMPROV (P) corpus comprises anger, sadness, happiness, and neutral emotions. On the other hand, the PODCAST (P) corpus includes the additional emotions of surprise, fear, disgust, and contempt, along with the emotions present in IMPROV (P). Given the emotional overlap, we can perform this cross-corpus evaluation where a SER model trained with the PODCAST (P) corpus aims to predict emotions within the IMPROV (P) corpus. Our approach involves directly utilizing the models trained on the PODCAST (P) corpus and assessing their efficacy on the IMPROV (P) set. We specifically focus on predictions for anger, sadness, happiness, and the neutral state. We transform the distribution predictions into binary labels by applying a threshold and then compute the results using the macro F1 score.\n\nFor instance, let's consider a sample prediction for the IMPROV (P) dataset as: (angry, sad, happy, surprised, fear, disgusted, contempt, neutral) = (0.2, 0.2, 0.1, 0.1, 0.2, Table  4 .9: The table presents the cross-corpus macro-F1 scores for models trained using the 8-class MSP-PODCAST (P) dataset, applied to predict emotions in the 4-class IMPROV (P) dataset.   Table  4 .9 presents the macro-F1 scores for cross-corpus testing, generated using the MR, PR, AR, PR-MR, and AR-PR labels. The results indicate that using the AR T rain set for training yields superior performance on the MR, PR, AR, and AR-PR test sets.\n\nThis assessment also highlights the presented approach's efficacy for cross-corpus evaluations.\n\ndataset, concentrating on anger, sadness, happiness, and neutral emotions for clearer visual representation. Our focus is on test set segments with either high or low levels of agreement-Cohen's Kappa statistic  [94]  is used to define high and low agreement groups. From the test samples, the top 2% showing high agreement is chosen: 21 samples for \"sadness,\" 33 for \"anger,\" 97 for \"happiness,\" and 139 for \"neutral,\" as these are assumed to represent speech with emotional solid consensus.\n\nAdditionally, we explore ambiguous cases by selecting the top 2% of test samples with low agreement, meaning these samples largely lack clear consensus. We analyze sentences containing a mix of two emotions along with their respective quantities in brackets: anger-neutral (30), sadness-happiness (  18 ), neutral-happiness (  67 ), and angersadness  (30) . These cases are indicative of low agreement among annotators.\n\nWe utilized T-SNE (T-distributed stochastic neighbor embedding) to project the 1,024-dimensional feature vectors onto two-dimensional plots, aiming to visualize the data distribution. The visualizations include two primary emotions along with one composite emotion; for instance, the plot may feature \"anger,\" \"sadness,\" and the composite \"anger-sadness\" emotions. Each plot centers the emotional label around the mean values of the sentences from each class.  samples often appearing between pure emotions with high agreement.\n\nComparing embeddings from models trained on the AR T rain versus M R T rain sets, it is apparent that the AR T rain set offers superior separation between classes, as evidenced by the centralized labels of emotions in the plots. Further validation comes from the silhouette score  [95] , a metric assessing how healthy clusters are formed within the embedding space (ranging from -1 for poor clustering to +1 for ideal clustering). We derived 1,024-dimensional feature representations using models trained under differing consensus-agreement conditions. Table  4 .10 provides the silhouette scores across three clusters: the first emotion, the second emotion, and their respective composite cases.\n\nThis analysis incorporates embeddings generated from models trained on P R T rain , AR -P R T rain , and P R -M R T rain sets.\n\nAccording to the table, the model trained with the AR T rain set ranks highest in silhouette score for \"anger-neutral,\" \"sadness-happiness,\" and \"neutral-happiness\" clusters. Intriguingly, the model trained with P R -M R T rain had the top silhouette score for the \"anger-sadness\" cluster. Generally, models trained on datasets containing more ambiguous samples are better able to cluster responses to complex emotion scenarios than models trained solely on the M R T rain engagement.",
      "page_start": 66,
      "page_end": 66
    },
    {
      "section_name": "Impact Of Extra Data Incorporated By The Ar Approach",
      "text": "One advantage of using the AR T rain set is the larger amount of data incorporated in training, as it utilizes all available samples, unlike the M R T rain or P R T rain sets.\n\nHowever, this extra data isn't the only contributor to the effectiveness of this strategy.\n\nWe performed experiments comparing models trained on datasets of similar size using both oversampling and undersampling strategies.\n\nFor the oversampling approach, we generated synthetic data following the method proposed by Pappagari et al.  [76] . The data generation continued until it matched the quantity used in the AR T rain set. Table  4 .    4 .12 lists the macro-F1 scores for both conditions, revealing that adding more samples is consistently beneficial. Notably, the AR T rain models frequently achieved the highest performance in both conditions (20,000 and 32,831 samples). These findings suggest that the AR method boosts the performance of SER models by including ambiguous data, with its benefits extending beyond simply enlarging the training set.    4 .7 shows that SER systems using soft-label learning surpassed those using hard-label learning in 17 out of 18 cases (around 94%). This finding is consistent with prior studies, which have demonstrated    ognize mixed emotions from the ambiguous samples within the AR -P R set.",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Summary",
      "text": "This paper examined speaker-independent categorical SER systems' performance using an all-inclusive test set, where no data was excluded, and our aggregation rule was applied. Compared to traditional label aggregation methods like the majority rule and plurality rule, the all-inclusive rule allows for the retention of all annotated data and emotional ratings, thereby making it possible to train and evaluate SER systems that are into S2ST systems is believed to significantly enhance their realism in speech conversion. The dissertation posits that categorical emotions are interrelated and suggests that considering the commonness of emotion co-occurrence in emotional ratings can create better SER systems.\n\nExamining simultaneous emotions that appear in everyday life  [21] , Xu et al.  [105]  capitalized on the prevalence of concurrent emotions in perceptual assessments to forge initial linkages within their presented graph-based DNN. Meanwhile, the work  [22]  leveraged frequent emotional misclassifications by evaluators to evaluate an SER system's performance. Furthermore, some research works have utilized the soft-label method to capture secondary emotions present in voice  [57, 61, 64] . This body of work highlights the importance of accounting for feedback from all evaluators in perceptual evaluations, even when there is variation from agreed-upon labels.\n\nThis research explores how incorporating emotional co-occurrence data can improve the training process of a SER system. It involves creating a co-occurrence frequency matrix that captures the relationships between different emotions as determined by perceptual evaluations. This matrix is subsequently converted into a penalization matrix, which adjusts the loss functions by assigning greater penalties for predicting combinations of emotions that seldom occur together. Our approach folds the penalization matrix into existing cost functions as a \"penalty loss,\" thereby increasing the loss value if the model forecasts emotions with low co-occurrence frequencies. For instance, since anger and contempt are seldom co-selected in perceptual evaluations, predicting these two emotions jointly will incur a higher penalty than predicting commonly co-occurring emotions like anger and sadness. This strategy acknowledges the interdependence of emotions, leveraging their relationships to enhance model performance.",
      "page_start": 75,
      "page_end": 80
    },
    {
      "section_name": "Background And Related Works",
      "text": "",
      "page_start": 80,
      "page_end": 80
    },
    {
      "section_name": "Contrastive Learning In Emotion Recognition",
      "text": "To enhance SER systems, Wang et al.",
      "page_start": 80,
      "page_end": 80
    },
    {
      "section_name": "Label Learning In Emotion Recognition",
      "text": "This work explores the application of co-occurrence frequencies of emotional classes derived from perceptual evaluations to enhance the training process of an SER system. It examines related methods such as distribution-, hard-, multi-label learning, and emotion classification. To illustrate, we use a 4-class emotion classification task that includes angry (A), neutral (N), happy (H), and sad (S) emotions. In this scenario, five annotators independently assess a sentence, each assigning one label. For a single sample, an example set of labels might look like this: \"S, N, S, N, S.\"",
      "page_start": 81,
      "page_end": 81
    },
    {
      "section_name": "Emotion Recognition Using Hard-Label Learning",
      "text": "In SER studies, emotion databases annotated via perceptual evaluations often employ consensus labels derived from collating individual annotations through techniques like majority voting or the plurality rule. Such methods produce a definitive hardlabel vector indicating a single emotional class, thereby excluding secondary emotions noted by raters who did not align with the majority group. For instance, the constructed one-hot vector could be (0, 0, 1, 0). However, few works have introduced soft labels  [57, 61, 64] , enabling each sample to be linked with multiple emotions. The work  [18]  built each annotator individually to capture the subjective nature of perceptual evaluations. These approaches permit the inclusion of sentences in the training dataset even when annotators have differing emotional perceptions. Contrary to systems trained with hard-label learning-which predict a single emotion per utterance and assume that emotions are independent-these advanced techniques consider emotional co-occurrence. My findings reveal that adopting the \"penalty loss\" method improves performance. The entry (\"S\", \"S\") has the value 18,546, indicating that angry was chosen across 18,546 utterances. Among these occurrences, neutral was chosen 14,337 times. Consequently, the entries at positions (\"S\", \"N\") and (\"N\", \"S\") are populated with 14,337, respectively.\n\nIn phase 2, we create a matrix indicating the probability of co-existing emotions by dividing each element by the total count of instances marked with the corresponding emotion in each column. Take the second column labeled \"S\" from Fig.  5 .1 (b) as an example-the co-occurrence frequencies of anger with other emotions are 6,331, 18,456, 12,767, 3,613, 6,807, 9,260, 3,835, and 5,796. By dividing each of these frequencies by the total occurrences of sad (6,331), we obtain co-occurrence probabilities: 0.34, 1.00, 0.37, 0.25, 0.52, 0.77, 0.27, and 0.41. As an example, the co-existing possibility of disgust and sad (0.37) is greater than that of sad and contempt (0.25). This normalized matrix is termed the co-existing weight matrix (Figure  5 .1 (c)), which is asymmetric owing to the normalization performed column-wise.\n\nIn phase 3, the co-occurrence weight matrix converts into a \"penalization matrix.\"\n\nThis conversion is essential as it penalizes the SER models for forecasting rare emotion combinations. The conversion process is simple: Each element in the co-existing emotion weight matrix is subtracted from one, resulting in the penalization matrix (Fig- Furthermore, we apply the label smoothing strategy suggested by  [89]  to both hardlabel and distribution-label learning, setting the parameter at 0.05. A slight value (10 -6 ) is also added to entries initially quantified as zero for the multi-label vector.",
      "page_start": 81,
      "page_end": 84
    },
    {
      "section_name": "Loss Functions Integrated By The Proposed Penalization Matrix",
      "text": "The technique of integrating a penalization matrix into loss functions is proposed.\n\nThis method requires the definition of N × K matrices for both the model's predictions (Y P ) and the actual labels (Y T ). Here, C signifies the number of emotional categories, while N denotes the number of samples under consideration. The variation in each row of Y T depends on the label learning approach in use, whether it is distribution-, multi-, or hard-label learning. Afterwards, the loss value matrix (L ∈ R N ×C ) is determined using the following approach:\n\nThe loss function, represented by (f loss ), could be the cross-entropy (CE). The elements of ( L ) are calculated as ( f loss (Y T ij, Y P ij) ), where ( j ∈ 1, . . . , K ) and ( i ∈ 1, . . . , N ), to estimate the categorical emotion loss for each utterance. Subsequently, the proposed matrix has been incorporated into Equation  5 .1. The matrix introduced, denoted as ( P ), belongs to ( R K×K ) (refer to Fig.  5 .1). The integration of the loss function is achieved through the penalization matrix, represented by ( L P +loss\n\n), as follows:\n\n(5.2)\n\nWhen we replace f loss with the objective functions, the equations will be:\n\nIn training, the initial loss is modified by the proposed loss, resulting in the total loss as described by Equation  5 .6. In this equation, α ∈ R and β are assigned a value of into a specified number of uniformly sized chunks through overlapping adjustments.\n\nBased on the paper's suggestions, we utilized LSTM as the feature encoder at the chunk level, along with the RNN-AttenVec model for chunk-level attention  [112] . This combination allows us to capture emotion-related information at different levels. For further details on the network architecture, see Lin and Busso  [112] . Model parameters adhered to those in Chou et al.  [19] . Based on previous studies, for the output layer, we used softmax activation for CE  [99, 101, 103]  and KLD  [19, 29] , with sigmoid activation for BCE  [104, 113] . We used Adam optimizer and set the learning rate to 0.0001; we set the batch size to 128. Then, we train all models with the epochs of 25. Finally, we saved the best-performing model according to their minimum loss on the development set. To analyze the influence of the suggested objective function on the accuracy of SER systems, the parameter α in Equ. 5.6 was set to either 0.5 or 1.0. We performed experiments using only L P +loss and using only L loss .",
      "page_start": 84,
      "page_end": 85
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "To gauge the accuracy of prediction labels against ground truth, we resort to a suite of tailored metrics based on the type of label learning method in use. For hard-label learning, I used macro-, micro-, and weighted-F1 scores, unweighted average recall, and unweighted average precision. In the case of multi-label and distribution-label learning, I employ measures similar to those referenced in the study by  [113] : ranking and hamming loss, coverage error, alongside macro F1. Moreover, micro-and weighted-F1 scores are integrated to estimate multi-label tasks specifically. For a performance assessment of multi-label and distribution-label methods, binarization of predictions is necessary. For multi-label learning, predictions' probabilities are changed into \"multihot\" binary vectors using a threshold of 0.5, following  [104, 113] 's methodology. For distribution-label learning, a threshold is fixed at 1/convertingion of probability predictions into binary vectors, in line with Chou et al.  [19] .",
      "page_start": 87,
      "page_end": 87
    },
    {
      "section_name": "Statistical Significance",
      "text": "The methodology from Lin and Busso  [112]  is employed, where the original test set is divided at random into 30 smaller subsets of roughly equal size. The average results for all metrics are then reported. A two-tailed t-test is utilized to conduct the statistical significance test, evaluating the difference between the proposed approach and the baseline models. A result is considered statistically significant if the p-value is less than 0.05. 5.5.1 Does incorporating the penalty loss (L P +loss ) benefit SER Systems?",
      "page_start": 88,
      "page_end": 88
    },
    {
      "section_name": "Experimental Results And Analyses",
      "text": "Table  5 .1 reveals that models utilizing the proposed penalty loss (L P +loss ) typically achieve the best outcomes across different label learning strategies. For example, when the results are evaluated on the single-emotion utterances, the model with L P +loss where Table  5 .1: The table overviews the results on distributional-label, multi-hard-label, and single-label tasks for the primary emotion classification task. The mark * denotes that the outcomes for SER systems utilizing the presented matrix have statistical significance compared to the baseline (α = 0; β = 1).  documented by the work  [19]  (31.6% maF1). This suggests that the presented penalty loss (L P +loss ) significantly improves model accuracy in primary emotion classification tasks. Furthermore, the results exhibit that aggregating the primary loss L loss with the presented loss L P +loss tends to boost overall effectiveness.",
      "page_start": 89,
      "page_end": 89
    },
    {
      "section_name": "Effect Of Co-Occurrence Matrix",
      "text": "The aim is to determine if the proposed methodology effectively allows systems to capture the intended co-existing emotions matrix accurately. This is evaluated by measuring the distance between the co-existing emotions matrices derived from the learn-ing targets in the training set and those obtained from model predictions, utilizing the Frobenius norm as the distance metric. The focus is on models that use either L P +loss (α = 1; β = 0) or L loss (α = 0; β = 1). When L P +loss is implemented, a decrease in the distance was observed from 4.27 to 4.00 using the multi-label approach and from 4.13 to 3.39 using the distribution-label approach. This reduction signifies that the coexisting emotion matrix, as the proposed penalization method predicted, is more closely aligned with the target co-occurrence matrix.",
      "page_start": 89,
      "page_end": 89
    },
    {
      "section_name": "Summary",
      "text": "",
      "page_start": 89,
      "page_end": 89
    },
    {
      "section_name": "Chapter 6 Conclusion",
      "text": "This dissertation introduces three proposed methods to optimize the pipeline for constructing speech emotion recognition (SER) systems. These methods have been documented in our studies, including rater-modeling  [18] , all-inclusive  [114] , and the penalization objective function that accounts for co-currencies of emotions  [90] . The findings from this dissertation and the respective studies  [18, 90, 114]  have facilitated answering the primary research questions.\n\n(1) Should we remove the minority of emotional ratings? The minority of emotional ratings should not be discarded, as retaining them can enhance the recognition of ambiguous utterances with complex emotional perceptions. Adding these minority ratings also boosts the performance of SER systems on comprehensive test sets that mirror real-world conditions. Further analyses reveal that standard approaches, which neglect these minority ratings, show reduced capabilities in clustering samples with dual emotions. Therefore, including the minority of emotional ratings is essential to account for the subjectivity inherent in emotion perception.\n\n(2) Should we only let the SER systems learn the emotional perceptions of a few people? The findings of this dissertation indicate that allowing SER systems to learn emotion perception from a larger group can enhance recognition capabilities through the proposed all-inclusive rule or individual-rater modeling method. This not only im-proves the performance of SER systems but also offers a unique opportunity for personal growth. By incorporating emotional ratings from a broader range of people, each person's unique sensitivity and emotional background can contribute valuable insights.\n\nThis should inspire and motivate researchers, developers, and practitioners in the field of SER to continue their work and strive for excellence.\n\n(3) Should SER systems only predict one emotion per speech? To address realworld conditions that involve the simultaneous occurrence of various emotion classes, SER systems need training to predict multiple emotions simultaneously. This dissertation convincingly illustrates that multi-label SER systems can achieve superior performance on test sets featuring a single consensus label determined by the plurality rule and the all-inclusive rule. This underlines the effectiveness of multi-label emotion prediction, which should be incorporated into SER systems rather than focusing on a single emotion.",
      "page_start": 91,
      "page_end": 92
    },
    {
      "section_name": "Discussion And Limitation",
      "text": "While the dissertation proposes three methods to address challenges in SER, several other factors need consideration. For instance, the effect of various inputs, such as handcrafted features or raw audio signals, on prediction outcomes should have been analyzed. Additionally, the relationships between layer embeddings and label spaces should be explored. Visualizing these relationships could provide fascinating insights. Furthermore, all the suggested methods are designed for traditional SER emotion databases, while the latest dataset  [115]  includes intensity scores for each categorical emotion.\n\nUsing this type of emotion database might mean our methods must be optimized for training and evaluating SER systems. It would be intriguing to see if our methods could be adapted for samples with intensity scores.\n\nFurthermore, the proposed comprehensive rule recommends incorporating all data into the emotion dataset. However, an examination of how the dataset size impacts SER system performance was not conducted. This omission stems from the fact that the largest existing emotion dataset comprises merely around 200,000 utterances, significantly less than databases used in other speech tasks such as Automatic Speech Recognition (ASR). Additionally, extensive experiments under cross-domain conditions were not undertaken, and only a single experiment was performed. It is essential to note that cross-domain experiments are crucial for real-world application of SER systems.\n\nIn conclusion, this dissertation recommends utilizing the original emotion classifications within the database. A pertinent question that arises is how many emotion classes are sufficient to accurately capture emotional perception in real-world scenarios.\n\nAccording to  [7] , humans are capable of detecting 24 distinct emotions through vocal expressions. Therefore, it would be intriguing to combine multiple emotion databases to encompass these 24 emotions.",
      "page_start": 92,
      "page_end": 93
    },
    {
      "section_name": "Future Works",
      "text": "All data and emotional ratings in the emotion databases can be utilized with the proposed methods during inference. In future studies, the first benchmark for SER will be proposed based on the partitions defined in this dissertation, allowing for a standardized evaluation of models and easy comparison of performances across different SER models. The intention is to examine performance disparities caused by natural biases in emotional perceptions, including gender, race, and ethnicity. Additionally, the impact of missing modality on SER system performance will be explored, considering that real-world conditions might lead to signal loss. Noisy label learning, such as facial expression recognition  [116] , is worth investigating to check whether it can be useful in improving SER systems. Multilingual SER systems will be developed in at least ten languages due to slight variations in emotion perception across languages, which might offer complementary information to enhance overall system performance. Finally, personalized SER systems will be constructed based on speaker or user profiles to improve user experience in real-world applications, recognizing that each individual could have unique emotional responses.\n\nInternational Conference on Acoustics, Speech, and Signal Processing, 2005.,",
      "page_start": 93,
      "page_end": 99
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: 1 shows increasing research studies on speech emo-",
      "page": 21
    },
    {
      "caption": "Figure 1: 1: The figure illustrates the trends of papers whose title includes speech emo-",
      "page": 21
    },
    {
      "caption": "Figure 1: 2: The figure illustrates three main contributions to modeling and evaluation",
      "page": 22
    },
    {
      "caption": "Figure 1: 2: (1) label prepro-",
      "page": 23
    },
    {
      "caption": "Figure 1: 3: The figure illustrates the current trends of prior works on SER.",
      "page": 23
    },
    {
      "caption": "Figure 1: 3 showcases",
      "page": 25
    },
    {
      "caption": "Figure 1: 3. A close study by [28] on facial expression recogni-",
      "page": 25
    },
    {
      "caption": "Figure 3: 1). This concatenated layer is followed",
      "page": 43
    },
    {
      "caption": "Figure 3: 1: The figure illustrates the overall proposed model.",
      "page": 43
    },
    {
      "caption": "Figure 3: 2: The figure illustrates the baselines and models for the ablation study.",
      "page": 44
    },
    {
      "caption": "Figure 3: 2 illustrates the various baselines and models considered in the ablation",
      "page": 44
    },
    {
      "caption": "Figure 3: 1, this model represents our ultimate",
      "page": 45
    },
    {
      "caption": "Figure 4: 1: A diagram illustrating how much data and ratings are used in the final test",
      "page": 50
    },
    {
      "caption": "Figure 4: 1: PR-MR includes test samples accepted by PR but not by MR, whereas",
      "page": 57
    },
    {
      "caption": "Figure 4: 2: Averaged macro-F1 scores across 18 experiments (as shown in Table 4.7)",
      "page": 65
    },
    {
      "caption": "Figure 4: 2b presents statistically significant evidence that the average",
      "page": 65
    },
    {
      "caption": "Figure 4: 3: Averaged macro-F1 scores across 18 experiments (detailed in Table 4.7)",
      "page": 67
    },
    {
      "caption": "Figure 4: 3b indicates that the",
      "page": 68
    },
    {
      "caption": "Figure 4: 3 reveals that models trained with the ARTrain set achieve",
      "page": 68
    },
    {
      "caption": "Figure 4: 4. The investigation utilizes the PODCAST (P)",
      "page": 68
    },
    {
      "caption": "Figure 4: 5 illustrates the embeddings produced",
      "page": 69
    },
    {
      "caption": "Figure 4: 4: The figure depicts the procedure of visualizing feature embeddings.",
      "page": 69
    },
    {
      "caption": "Figure 4: 5: T-SNE visualizations using embeddings generated from models trained",
      "page": 70
    },
    {
      "caption": "Figure 4: 6 illustrates the performance of SER systems trained with various aggre-",
      "page": 73
    },
    {
      "caption": "Figure 4: 6: The bar plots depict the macro-F1 scores achieved with distributional-label",
      "page": 74
    },
    {
      "caption": "Figure 4: 7 provides an overview of the macro-F1 scores for each",
      "page": 74
    },
    {
      "caption": "Figure 4: 7: The macro-F1 scores for each database are provided for distributional-label",
      "page": 75
    },
    {
      "caption": "Figure 5: 1: Illustration of a process for generating the presented penalization matrix.",
      "page": 83
    },
    {
      "caption": "Figure 5: 1 details the three-phase process of forming the penalization matrix. In",
      "page": 83
    },
    {
      "caption": "Figure 5: 1 (b) as an example. The entry (”S”, ”S”) has the",
      "page": 83
    },
    {
      "caption": "Figure 5: 1 (b) as an ex-",
      "page": 83
    },
    {
      "caption": "Figure 5: 1 (c)), which is asymmetric",
      "page": 84
    },
    {
      "caption": "Figure 5: 1). The integration of",
      "page": 85
    }
  ],
  "tables": [
    {
      "caption": "Table 4: 4: Comparison of distribution-based and hard-decision-based assessment met-",
      "data": [
        {
          "Metric": "Advantages",
          "Distribution-based assessment": "• We can directly compare the models’ predictions to hu-\nman perception without needing extra thresholding meth-\nods.\n•\nIt\nis tuned to the overall shape and structure of\nthe dis-\ntributions, reflecting both the accuracy and confidence of\nthe predictions.",
          "Hard-decision-based assessment": "•\nThe macro-F1 score has a defined range between 0 and 1,\nfacilitating straightforward interpretation and comparison\nacross various models and datasets.\n•\nBy applying thresholds to the labels, we can mitigate the\ninfluence of noisy annotations from raters who were not\nattentive."
        },
        {
          "Metric": "Limitations",
          "Distribution-based assessment": "•\nThe performance differences between the baseline\nand\nproposed models\nare\ndifficult\nto interpret because\nthe\nscale differences are minimal.\n•\nIt lacks a fixed range, which makes it challenging to inter-\npret and compare absolute values across different datasets\nor models.\n•\nIt is susceptible to slight distribution variations, especially\nwhen dealing with sparse or high-dimensional data.",
          "Hard-decision-based assessment": "•\nAll predictions and ground truth need to be binary (0 or\n1)—this requirement can lead to information loss and re-\nduced granularity, as\nit oversimplifies\nthe distributional\nnature of the ground truth.\n•\nIt\nis\nchallenging to assess model performance on less\ncommon emotions, such as fear, due to limited available\ndata."
        }
      ],
      "page": 60
    },
    {
      "caption": "Table 4: 6: Table illustrates the Kullback-Leibler divergence (KLD) when training",
      "data": [
        {
          "M RT rain\nIMPROV(P)\nP RT rain\nART rain": "M RT rain\nCREMA-D\nP RT rain\nART rain",
          "0.235†\n0.231†\n0.229†\n0.143\n0.190\n0.138\n0.256\n0.252\n0.249\n0.193\n0.180\n0.211∗† 0.208∗† 0.206∗† 0.139": "0.078\n0.091\n0.094\n0.122\n0.129\n0.064∗\n0.073∗\n0.075∗\n0.094∗\n0.099∗\n0.092∗\n0.095∗\n0.068∗\n0.075∗\n0.076∗",
          "0.108\n0.150\n0.175\n0.172\n0.171\n0.172\n0.169\n0.169\n0.117\n0.162\n0.182\n0.180\n0.178\n0.115\n0.157": "0.055\n0.058\n0.059\n0.066\n0.066\n0.058\n0.057\n0.057\n0.056∗\n0.059∗\n0.056\n0.056∗ 0.053∗\n0.055∗\n0.058",
          "0.232\n0.233\n0.236\n0.271\n0.285\n0.240\n0.242\n0.244\n0.289\n0.286\n0.283\n0.237\n0.238\n0.241\n0.277": "0.112\n0.136\n0.142\n0.192\n0.203\n0.109\n0.131\n0.136\n0.185\n0.188∗\n0.103∗ 0.122∗† 0.126∗† 0.166∗† 0.178∗"
        },
        {
          "M RT rain\nIMPROV(P)\nP RT rain\nART rain": "M RT rain\nPODCAST (P)\nP RT rain\nART rain",
          "0.235†\n0.231†\n0.229†\n0.143\n0.190\n0.138\n0.256\n0.252\n0.249\n0.193\n0.180\n0.211∗† 0.208∗† 0.206∗† 0.139": "0.150\n0.142\n0.141\n0.128\n0.136\n0.137\n0.135\n0.112∗⋆ 0.125∗⋆\n0.150\n0.170\n0.153\n0.150\n0.123\n0.134",
          "0.108\n0.150\n0.175\n0.172\n0.171\n0.172\n0.169\n0.169\n0.117\n0.162\n0.182\n0.180\n0.178\n0.115\n0.157": "0.157\n0.142\n0.139\n0.114\n0.125\n0.110\n0.122\n0.164\n0.144\n0.140\n0.172\n0.151\n0.146\n0.113\n0.125",
          "0.232\n0.233\n0.236\n0.271\n0.285\n0.240\n0.242\n0.244\n0.289\n0.286\n0.283\n0.237\n0.238\n0.241\n0.277": "0.242\n0.203\n0.219\n0.223\n0.248\n0.203\n0.218\n0.222\n0.244\n0.243\n0.200\n0.214\n0.220\n0.240\n0.245"
        },
        {
          "M RT rain\nIMPROV(P)\nP RT rain\nART rain": "M RT rain\nIEMOCAP\nP RT rain\nART rain",
          "0.235†\n0.231†\n0.229†\n0.143\n0.190\n0.138\n0.256\n0.252\n0.249\n0.193\n0.180\n0.211∗† 0.208∗† 0.206∗† 0.139": "0.203\n0.201\n0.202\n0.180\n0.205\n0.202\n0.201\n0.201\n0.186\n0.201\n0.185\n0.183∗† 0.182∗† 0.159∗† 0.178∗†",
          "0.108\n0.150\n0.175\n0.172\n0.171\n0.172\n0.169\n0.169\n0.117\n0.162\n0.182\n0.180\n0.178\n0.115\n0.157": "0.162\n0.160\n0.156\n0.128\n0.147\n0.150∗\n0.148∗ 0.145∗ 0.125\n0.135∗\n0.143∗\n0.141∗ 0.138∗ 0.120\n0.130∗",
          "0.232\n0.233\n0.236\n0.271\n0.285\n0.240\n0.242\n0.244\n0.289\n0.286\n0.283\n0.237\n0.238\n0.241\n0.277": "0.211\n0.212\n0.221\n0.223\n0.245\n0.204\n0.206\n0.214\n0.225\n0.237\n0.214\n0.217\n0.227∗\n0.208\n0.209"
        },
        {
          "M RT rain\nIMPROV(P)\nP RT rain\nART rain": "M RT rain\nIMPROV (S)\nP RT rain\nART rain",
          "0.235†\n0.231†\n0.229†\n0.143\n0.190\n0.138\n0.256\n0.252\n0.249\n0.193\n0.180\n0.211∗† 0.208∗† 0.206∗† 0.139": "0.118\n0.128\n0.130\n0.139\n0.147\n0.103∗⋆ 0.103∗\n0.104∗\n0.109∗\n0.102∗\n0.099∗\n0.116\n0.108∗\n0.108∗\n0.110∗",
          "0.108\n0.150\n0.175\n0.172\n0.171\n0.172\n0.169\n0.169\n0.117\n0.162\n0.182\n0.180\n0.178\n0.115\n0.157": "0.090†⋆ 0.088\n0.089\n0.087\n0.094\n0.103\n0.089\n0.088\n0.074∗\n0.082∗\n0.087\n0.086\n0.072∗\n0.080∗\n0.100",
          "0.232\n0.233\n0.236\n0.271\n0.285\n0.240\n0.242\n0.244\n0.289\n0.286\n0.283\n0.237\n0.238\n0.241\n0.277": "0.116\n0.158\n0.163\n0.205\n0.196\n0.119\n0.150\n0.155\n0.184∗\n0.190\n0.146∗\n0.150∗\n0.174∗\n0.180∗\n0.120"
        },
        {
          "M RT rain\nIMPROV(P)\nP RT rain\nART rain": "M RT rain\nPODCAST (S)\nP RT rain\nART rain",
          "0.235†\n0.231†\n0.229†\n0.143\n0.190\n0.138\n0.256\n0.252\n0.249\n0.193\n0.180\n0.211∗† 0.208∗† 0.206∗† 0.139": "0.085\n0.098\n0.102\n0.100\n0.113\n0.075\n0.062∗\n0.063∗\n0.060∗\n0.067∗\n0.065∗\n0.084\n0.064∗\n0.064∗\n0.061∗",
          "0.108\n0.150\n0.175\n0.172\n0.171\n0.172\n0.169\n0.169\n0.117\n0.162\n0.182\n0.180\n0.178\n0.115\n0.157": "0.074†⋆ 0.071\n0.073\n0.071\n0.078\n0.060∗ 0.060∗ 0.056∗\n0.058∗\n0.091\n0.097\n0.062∗ 0.061∗ 0.057∗\n0.058∗",
          "0.232\n0.233\n0.236\n0.271\n0.285\n0.240\n0.242\n0.244\n0.289\n0.286\n0.283\n0.237\n0.238\n0.241\n0.277": "0.079\n0.144\n0.151\n0.153\n0.171\n0.067\n0.123∗\n0.132∗\n0.131∗\n0.156∗\n0.117∗\n0.126∗\n0.124∗\n0.149∗\n0.073"
        }
      ],
      "page": 62
    },
    {
      "caption": "Table 4: 7 and Table 4.6 present the macro-F1 scores and KLD values for the dif-",
      "data": [
        {
          "MRT rain\nIMPROV(P)\nPRT rain\nART rain": "MRT rain\nCREMA-D\nPRT rain\nART rain",
          "0.516†\n0.512†\n0.507†\n0.555†\n0.300\n0.450\n0.448\n0.513\n0.305\n0.465\n0.562∗† 0.555∗† 0.593∗† 0.335\n0.498": "0.591\n0.532\n0.551\n0.381\n0.500\n0.600\n0.545\n0.390\n0.595∗\n0.572∗\n0.607∗\n0.593∗\n0.585\n0.528\n0.386",
          "0.346\n0.595\n0.587\n0.613\n0.530\n0.600\n0.593\n0.623\n0.531\n0.341\n0.576\n0.569\n0.602\n0.339\n0.518": "0.640\n0.575\n0.671\n0.409\n0.651\n0.667\n0.594\n0.699∗\n0.416\n0.688\n0.673\n0.615∗\n0.710∗\n0.444\n0.706∗",
          "0.612\n0.604\n0.401\n0.599\n0.440\n0.601\n0.596\n0.590\n0.359\n0.436\n0.600\n0.441\n0.602\n0.594\n0.340": "0.357\n0.518⋆ 0.474⋆\n0.411\n0.368\n0.419\n0.374\n0.518⋆ 0.473⋆\n0.357\n0.486\n0.442\n0.414\n0.340\n0.370"
        },
        {
          "MRT rain\nIMPROV(P)\nPRT rain\nART rain": "MRT rain\nPODCAST (P)\nPRT rain\nART rain",
          "0.516†\n0.512†\n0.507†\n0.555†\n0.300\n0.450\n0.448\n0.513\n0.305\n0.465\n0.562∗† 0.555∗† 0.593∗† 0.335\n0.498": "0.214⋆\n0.184⋆\n0.303\n0.143\n0.300\n0.259∗⋆ 0.232∗⋆ 0.403∗⋆ 0.187∗⋆ 0.420∗⋆\n0.192\n0.166\n0.330∗\n0.129\n0.351∗",
          "0.346\n0.595\n0.587\n0.613\n0.530\n0.600\n0.593\n0.623\n0.531\n0.341\n0.576\n0.569\n0.602\n0.339\n0.518": "0.215\n0.185\n0.326\n0.145\n0.328\n0.241⋆ 0.207∗⋆ 0.397∗⋆ 0.160⋆\n0.408∗⋆\n0.199\n0.174\n0.355∗\n0.138\n0.367∗",
          "0.612\n0.604\n0.401\n0.599\n0.440\n0.601\n0.596\n0.590\n0.359\n0.436\n0.600\n0.441\n0.602\n0.594\n0.340": "0.161\n0.137\n0.162\n0.102\n0.159\n0.195∗ 0.166∗\n0.192∗\n0.126∗\n0.184∗\n0.204∗ 0.175∗\n0.200∗\n0.139∗\n0.192∗"
        },
        {
          "MRT rain\nIMPROV(P)\nPRT rain\nART rain": "MRT rain\nIEMOCAP\nPRT rain\nART rain",
          "0.516†\n0.512†\n0.507†\n0.555†\n0.300\n0.450\n0.448\n0.513\n0.305\n0.465\n0.562∗† 0.555∗† 0.593∗† 0.335\n0.498": "0.269\n0.260\n0.339\n0.203\n0.351\n0.259\n0.254\n0.345\n0.186\n0.355\n0.279\n0.268\n0.365\n0.238†\n0.378",
          "0.346\n0.595\n0.587\n0.613\n0.530\n0.600\n0.593\n0.623\n0.531\n0.341\n0.576\n0.569\n0.602\n0.339\n0.518": "0.346\n0.343\n0.412\n0.257\n0.426\n0.279\n0.369\n0.359\n0.433\n0.453\n0.390∗ 0.383∗\n0.479∗\n0.464∗† 0.266",
          "0.612\n0.604\n0.401\n0.599\n0.440\n0.601\n0.596\n0.590\n0.359\n0.436\n0.600\n0.441\n0.602\n0.594\n0.340": "0.354\n0.341\n0.299\n0.253\n0.287\n0.377\n0.361\n0.320\n0.253\n0.306\n0.361\n0.325∗\n0.265\n0.317\n0.369"
        },
        {
          "MRT rain\nIMPROV(P)\nPRT rain\nART rain": "MRT rain\nIMPROV (S)\nPRT rain\nART rain",
          "0.516†\n0.512†\n0.507†\n0.555†\n0.300\n0.450\n0.448\n0.513\n0.305\n0.465\n0.562∗† 0.555∗† 0.593∗† 0.335\n0.498": "0.424\n0.254\n0.229\n0.234\n0.245\n0.455⋆\n0.340∗\n0.318∗\n0.328∗\n0.360∗\n0.337∗\n0.365∗\n0.391\n0.315∗\n0.311∗",
          "0.346\n0.595\n0.587\n0.613\n0.530\n0.600\n0.593\n0.623\n0.531\n0.341\n0.576\n0.569\n0.602\n0.339\n0.518": "0.451\n0.299\n0.379\n0.278\n0.386\n0.433\n0.353∗\n0.483∗\n0.342∗\n0.505∗\n0.360∗\n0.491∗\n0.343∗\n0.522∗\n0.410",
          "0.612\n0.604\n0.401\n0.599\n0.440\n0.601\n0.596\n0.590\n0.359\n0.436\n0.600\n0.441\n0.602\n0.594\n0.340": "0.361\n0.185\n0.137\n0.149\n0.150\n0.397\n0.248∗\n0.181∗\n0.219∗\n0.189∗\n0.431∗ 0.306∗† 0.216∗† 0.282∗† 0.227∗†"
        },
        {
          "MRT rain\nIMPROV(P)\nPRT rain\nART rain": "MRT rain\nPODCAST (S)\nPRT rain\nART rain",
          "0.516†\n0.512†\n0.507†\n0.555†\n0.300\n0.450\n0.448\n0.513\n0.305\n0.465\n0.562∗† 0.555∗† 0.593∗† 0.335\n0.498": "0.344\n0.078\n0.138\n0.076\n0.141\n0.392⋆\n0.113∗\n0.327∗\n0.111∗\n0.328∗\n0.125∗\n0.352∗† 0.124∗† 0.357∗†\n0.283",
          "0.346\n0.595\n0.587\n0.613\n0.530\n0.600\n0.593\n0.623\n0.531\n0.341\n0.576\n0.569\n0.602\n0.339\n0.518": "0.389⋆ 0.080\n0.199\n0.076\n0.198\n0.321\n0.122∗\n0.450∗\n0.122∗\n0.457∗\n0.139∗\n0.457∗\n0.142∗\n0.466∗\n0.237",
          "0.612\n0.604\n0.401\n0.599\n0.440\n0.601\n0.596\n0.590\n0.359\n0.436\n0.600\n0.441\n0.602\n0.594\n0.340": "0.352\n0.051\n0.060\n0.047\n0.059\n0.412\n0.076∗\n0.078∗\n0.072∗\n0.074∗\n0.425\n0.078∗\n0.091∗† 0.075∗\n0.088∗†"
        }
      ],
      "page": 63
    }
  ],
  "citations": [
    {
      "citation_id": "8",
      "title": "Novel Evaluation Method by an All-Inclusive Aggregation Rule 4.1",
      "venue": "Novel Evaluation Method by an All-Inclusive Aggregation Rule 4.1"
    },
    {
      "citation_id": "13",
      "title": "Proposed All-inclusive Rule",
      "venue": "Proposed All-inclusive Rule"
    },
    {
      "citation_id": "14",
      "title": "Employing the All-Inclusive Rule for Test Set Construction",
      "venue": "Employing the All-Inclusive Rule for Test Set Construction"
    },
    {
      "citation_id": "18",
      "title": "Train/Test Set Defined by",
      "venue": "Train/Test Set Defined by"
    },
    {
      "citation_id": "20",
      "title": "Evaluation Metrics and",
      "venue": "Evaluation Metrics and"
    },
    {
      "citation_id": "22",
      "title": "Comparison of Results with Prior SOTA Methods",
      "venue": "Comparison of Results with Prior SOTA Methods"
    },
    {
      "citation_id": "25",
      "title": "What is the most effective label learning method for SER?",
      "venue": "What is the most effective label learning method for SER?"
    },
    {
      "citation_id": "33",
      "title": "Loss Functions Integrated by the Proposed Penalization Matrix",
      "venue": "Loss Functions Integrated by the Proposed Penalization Matrix"
    },
    {
      "citation_id": "37",
      "title": "SER Models and",
      "venue": "SER Models and"
    },
    {
      "citation_id": "41",
      "title": "Does incorporating the penalty loss (L P +loss ) benefit SER Systems?",
      "venue": "Does incorporating the penalty loss (L P +loss ) benefit SER Systems?"
    },
    {
      "citation_id": "46",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "47",
      "title": "Soft-Target Training with Ambiguous Emotional Utterances for DNN-Based Speech Emotion Classification",
      "authors": [
        "A Ando",
        "S Kobashikawa",
        "H Kamiyama",
        "R Masumura",
        "Y Ijima",
        "Y Aono"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Robust Speech Recognition via Large-Scale Weak Supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Proceedings of the 40th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "49",
      "title": "Seamless: Multilingual Expressive and Streaming Speech Translation",
      "authors": [
        "S Communication",
        "L Barrault",
        "Y.-A Chung",
        "M Meglioli",
        "D Dale",
        "N Dong",
        "M Duppenthaler",
        "P.-A Duquenne",
        "B Ellis",
        "H Elsahar",
        "J Haaheim",
        "J Hoffman",
        "M.-J Hwang",
        "H Inaguma",
        "C Klaiber",
        "I Kulikov",
        "P Li",
        "D Licht",
        "J Maillard",
        "R Mavlyutov",
        "A Rakotoarison",
        "K Sadagopan",
        "A Ramakrishnan",
        "T Tran",
        "G Wenzek",
        "Y Yang",
        "E Ye",
        "I Evtimov",
        "P Fernandez",
        "C Gao",
        "P Hansanti",
        "E Kalbassi",
        "A Kallet",
        "A Kozhevnikov",
        "G Gonzalez",
        "R Roman",
        "C Touret",
        "C Wong",
        "C Wood",
        "B Yu",
        "P Andrews",
        "C Balioglu",
        "P.-J Chen",
        "M Costa-Jussà",
        "M Elbayad",
        "H Gong",
        "F Guzmán",
        "K Heffernan",
        "S Jain",
        "J Kao",
        "A Lee",
        "X Ma",
        "A Mourachko",
        "B Peloquin",
        "J Pino",
        "S Popuri",
        "C Ropers",
        "S Saleem",
        "H Schwenk",
        "A Sun",
        "P Tomasello",
        "C Wang",
        "J Wang",
        "S Wang",
        "M Williamson"
      ],
      "year": "2023",
      "venue": "Seamless: Multilingual Expressive and Streaming Speech Translation"
    },
    {
      "citation_id": "50",
      "title": "Classification of emotions and evaluation of customer satisfaction from speech in real world acoustic environments",
      "authors": [
        "L Parra-Gallego",
        "J Orozco-Arroyave"
      ],
      "year": "2022",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "51",
      "title": "Self-report captures 27 distinct categories of emotion bridged by continuous gradients",
      "authors": [
        "A Cowen",
        "D Keltner"
      ],
      "year": "2017",
      "venue": "Proceedings of the National Academy of Sciences",
      "doi": "10.1073/pnas.1702247114"
    },
    {
      "citation_id": "52",
      "title": "Semantic Space Theory: A Computational Approach to Emotion",
      "year": "2021",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "53",
      "title": "Gender differences in judgments of multiple emotions from facial expressions",
      "authors": [
        "J Hall",
        "D Matsumoto"
      ],
      "year": "2004",
      "venue": "Emotion",
      "doi": "10.1037/1528-3542.4.2.201"
    },
    {
      "citation_id": "54",
      "title": "American-Japanese Cultural Differences in the Recognition of Universal Facial Expressions",
      "authors": [
        "D Matsumoto"
      ],
      "year": "1992",
      "venue": "Journal of Cross-Cultural Psychology",
      "doi": "10.1177/0022022192231005"
    },
    {
      "citation_id": "55",
      "title": "Decline or improvement?: Age-related differences in facial expression recognition",
      "authors": [
        "A Suzuki",
        "T Hoshino",
        "K Shigemasu",
        "M Kawamura"
      ],
      "year": "2007",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "56",
      "title": "Core affect and the psychological construction of emotion",
      "authors": [
        "J Russell"
      ],
      "year": "2003",
      "venue": "Psychological review",
      "doi": "10.1037/0033-295x.110.1.145"
    },
    {
      "citation_id": "57",
      "title": "Valence is a basic building block of emotional life",
      "authors": [
        "L Barrett"
      ],
      "year": "2006",
      "venue": "proceedings of the 2005 Meeting of the Association of Research in Personality"
    },
    {
      "citation_id": "58",
      "title": "The primacy of categories in the recognition of 12 emotions in speech prosody across two cultures",
      "authors": [
        "A Cowen",
        "P Laukka",
        "H Elfenbein",
        "R Liu",
        "D Keltner"
      ],
      "year": "2019",
      "venue": "Nature human behaviour"
    },
    {
      "citation_id": "59",
      "title": "AuxFormer: Robust Approach to Audiovisual Emotion Recognition",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "60",
      "title": "Emotion Identification in Movies through Facial Expression Recognition",
      "authors": [
        "J Almeida",
        "L Vilac ¸a",
        "I Teixeira",
        "P Viana"
      ],
      "year": "2021",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "61",
      "title": "Let's agree to disagree: Consensus Entropy Active Learning for Personalized Music Emotion Recognition",
      "authors": [
        "J Gómez-Cañón",
        "E Cano",
        "Y.-H Yang",
        "P Herrera",
        "E Gomez"
      ],
      "year": "2021",
      "venue": "Proceedings of the 22nd International Society for Music Information Retrieval Conference",
      "doi": "10.5281/zenodo.5624399"
    },
    {
      "citation_id": "62",
      "title": "Music Emotion Recognition: Toward new, robust standards in personalized and context-sensitive applications",
      "authors": [
        "J Gómez-Cañón",
        "E Cano",
        "T Eerola",
        "P Herrera",
        "X Hu",
        "Y.-H Yang",
        "E Gómez"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "63",
      "title": "Every Rating Matters: Joint Learning of Subjective Labels and Individual Annotators for Speech Emotion Classification",
      "authors": [
        "H.-C Chou",
        "C.-C Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "64",
      "title": "Exploiting annotators' typed description of emotion perception to maximize utilization of ratings for speech emotion recognition",
      "authors": [
        "H.-C Chou",
        "W.-C Lin",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "65",
      "title": "Semi-Supervised Speech Emotion Recognition With Ladder Networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "66",
      "title": "The co-occurrence of emotions in daily life: A multilevel approach",
      "authors": [
        "K Vansteelandt",
        "I Van Mechelen",
        "J Nezlek"
      ],
      "year": "2005",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "67",
      "title": "Of all things the measure is man\" automatic classification of emotions and inter-labeler consistency [speech-based emotion recognition]",
      "authors": [
        "S Steidl",
        "M Levit",
        "A Batliner",
        "E Noth",
        "H Niemann"
      ],
      "year": "2005",
      "venue": "Proceedings. (ICASSP '05"
    },
    {
      "citation_id": "68",
      "title": "Multi-modal Multi-label Emotion Detection with Modality and Label Dependence",
      "authors": [
        "D Zhang",
        "X Ju",
        "J Li",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "69",
      "title": "Active Learning With Complementary Sampling for Instructing Class-Biased Multi-Label Text Emotion Classification",
      "authors": [
        "X Kang",
        "X Shi",
        "Y Wu",
        "F Ren"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "70",
      "title": "Image emotion multi-label classification based on multi-graph learning",
      "authors": [
        "M Wang",
        "Y Zhao",
        "Y Wang",
        "T Xu",
        "Y Sun"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "71",
      "title": "Transformer-based Label Set Generation for Multi-modal Multi-label Emotion Detection",
      "authors": [
        "X Ju",
        "D Zhang",
        "J Li",
        "G Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, ser. MM '20",
      "doi": "10.1145/3394171.3413577"
    },
    {
      "citation_id": "72",
      "title": "Multi-modal Multi-label Emotion Recognition with Heterogeneous Hierarchical Message Passing",
      "authors": [
        "D Zhang",
        "X Ju",
        "W Zhang",
        "J Li",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "73",
      "title": "Blended Emotion in-the-Wild: Multi-label Facial Expression Recognition Using Crowdsourced Annotations and Deep Locality Feature Learning",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2019",
      "venue": "Int. J. Comput. Vision",
      "doi": "10.1007/s11263-018-1131-1"
    },
    {
      "citation_id": "74",
      "title": "Label Distribution Learning",
      "authors": [
        "X Geng"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Knowledge and Data Engineering"
    },
    {
      "citation_id": "75",
      "title": "Emotion Distribution Recognition from Facial Expressions",
      "authors": [
        "Y Zhou",
        "H Xue",
        "X Geng"
      ],
      "year": "2015",
      "venue": "Proceedings of the 23rd ACM International Conference on Multimedia, ser. MM '15",
      "doi": "10.1145/2733373.2806328"
    },
    {
      "citation_id": "76",
      "title": "Emotion Distribution Learning from Texts",
      "authors": [
        "D Zhou",
        "X Zhang",
        "Y Zhou",
        "Q Zhao",
        "X Geng"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "77",
      "title": "The ordinal nature of emotions",
      "authors": [
        "G Yannakakis",
        "R Cowie",
        "C Busso"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "78",
      "title": "The Ordinal Nature of Emotions: An Emerging Approach",
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "79",
      "title": "Emotion Perception Threshold: Individual Differences in Emotional Sensitivity",
      "authors": [
        "R Martin",
        "G Berry",
        "T Dobranski",
        "M Horne",
        "P Dodgson"
      ],
      "year": "1996",
      "venue": "Journal of Research in Personality"
    },
    {
      "citation_id": "80",
      "title": "SpanEmo: Casting Multi-label Emotion Classification as Span-prediction",
      "authors": [
        "H Alhuzali",
        "S Ananiadou"
      ],
      "year": "2021",
      "venue": "Proceedings of the 16th Conference of the European Chapter"
    },
    {
      "citation_id": "81",
      "title": "Learning Deep Latent Space for Multi-Label Classification",
      "authors": [
        "C.-K Yeh",
        "W.-C Wu",
        "W.-J Ko",
        "Y.-C Wang"
      ],
      "year": "2017",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "82",
      "title": "Multi-Label Emotion Detection via Emotion-Specified Feature Extraction and Emotion Correlation Learning",
      "authors": [
        "J Deng",
        "F Ren"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "83",
      "title": "Corpus Annotation through Crowdsourcing: Towards Best Practice Guidelines",
      "authors": [
        "M Sabou",
        "K Bontcheva",
        "L Derczynski",
        "A Scharl"
      ],
      "year": "2014",
      "venue": "Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14"
    },
    {
      "citation_id": "84",
      "title": "Building Naturalistic Emotionally Balanced Speech Corpus by Retrieving Emotional Speech from Existing Podcast Recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "85",
      "title": "Are You a Racist or Am I Seeing Things? Annotator Influence on Hate Speech Detection on Twitter",
      "authors": [
        "Z Waseem",
        "D Bamman",
        "A Dogruöz",
        "J Eisenstein",
        "D Hovy",
        "D Jurgens",
        "B O'connor"
      ],
      "year": "2016",
      "venue": "Proceedings of the First Workshop on NLP and Computational Social Science"
    },
    {
      "citation_id": "86",
      "title": "Dealing with Disagreements: Looking Beyond the Majority Vote in Subjective Annotations",
      "authors": [
        "A Davani",
        "M Díaz",
        "V Prabhakaran"
      ],
      "year": "2022",
      "venue": "Transactions of the Association for Computational Linguistics",
      "doi": "10.1162/tacl_a_00449"
    },
    {
      "citation_id": "87",
      "title": "On Releasing Annotator-Level Labels and Information in Datasets",
      "authors": [
        "V Prabhakaran",
        "A Davani",
        "M Diaz"
      ],
      "year": "2021",
      "venue": "Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop"
    },
    {
      "citation_id": "88",
      "title": "Why Don't You Do It Right? Analysing Annotators' Disagreement in Subjective Tasks",
      "authors": [
        "M Sandri",
        "E Leonardelli",
        "S Tonelli",
        "E Jezek"
      ],
      "year": "2023",
      "venue": "Proceedings of the 17th Conference of the European Chapter"
    },
    {
      "citation_id": "89",
      "title": "Corpus Considerations for Annotator Modeling and Scaling",
      "authors": [
        "S Oluyemi",
        "B Neuendorf",
        "J Plepi",
        "L Flek",
        "J Schlötterer",
        "C Welch"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "90",
      "title": "Analyzing the Effect of Affective Priming on Emotional Annotations",
      "authors": [
        "L Martinez-Lucas",
        "A Salman",
        "S.-G Leem",
        "S Upadhyay",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2023",
      "venue": "2023 11th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "91",
      "title": "Understanding and Mitigating Worker Biases in the Crowdsourced Collection of Subjective Judgments",
      "authors": [
        "C Hube",
        "B Fetahu",
        "U Gadiraju"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, ser. CHI '19",
      "doi": "10.1145/3290605.3300637"
    },
    {
      "citation_id": "92",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "93",
      "title": "Designing and Evaluating Speech Emotion Recognition Systems: A Reality Check Case Study with IEMOCAP",
      "authors": [
        "N Antoniou",
        "A Katsamanis",
        "T Giannakopoulos",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "94",
      "title": "MSP-IMPROV: An Acted Corpus of Dyadic Interactions to Study Emotion Perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "95",
      "title": "Increasing the Reliability of Crowdsourcing Evaluations Using Online Quality Assessment",
      "authors": [
        "A Burmania",
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "96",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "97",
      "title": "Interpreting ambiguous emotional expressions",
      "authors": [
        "E Mower",
        "A Metallinou",
        "C.-C Lee",
        "A Kazemzadeh",
        "C Busso",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "98",
      "title": "The Ambiguous World of Emotion Representation",
      "authors": [
        "V Sethu",
        "E Provost",
        "J Epps",
        "C Busso",
        "N Cummins",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "The Ambiguous World of Emotion Representation"
    },
    {
      "citation_id": "99",
      "title": "Leveraging inter-rater agreement for audio-visual emotion recognition",
      "authors": [
        "Y Kim",
        "E Provost"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "100",
      "title": "From Hard to Soft: Towards more Human-like Emotion Recognition by Modelling the Perception Uncertainty",
      "authors": [
        "J Han",
        "Z Zhang",
        "M Schmitt",
        "M Pantic",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "Proceedings of the 25th ACM International Conference on Multimedia, ser. MM '17",
      "doi": "10.1145/3123266.3123383"
    },
    {
      "citation_id": "101",
      "title": "Modeling annotator expertise: Learning when everybody knows a bit of something",
      "authors": [
        "Y Yan",
        "R Rosales",
        "G Fung",
        "M Schmidt",
        "G Hermosillo",
        "L Bogoni",
        "L Moy",
        "J Dy"
      ],
      "year": "2010",
      "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, ser. Proceedings of Machine Learning Research"
    },
    {
      "citation_id": "102",
      "title": "Modeling subjectiveness in emotion recognition with deep neural networks: Ensembles vs soft labels",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2016",
      "venue": "2016 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "103",
      "title": "f-similarity preservation loss for soft labels: A demonstration on cross-corpus speech emotion recognition",
      "authors": [
        "B Zhang",
        "Y Kong",
        "G Essl",
        "E Provost"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "104",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "F Eyben",
        "M Wöllmer",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM International Conference on Multimedia, ser. MM '10",
      "doi": "10.1145/1873951.1874246"
    },
    {
      "citation_id": "105",
      "title": "Adam: A Method for Stochastic Optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "106",
      "title": "Formulating emotion perception as a probabilistic model with application to categorical emotion classification",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2017",
      "venue": "enth International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "107",
      "title": "Human-Like Emotion Recognition: Multi-Label Learning from Noisy Labeled Audio-Visual Expressive Speech",
      "authors": [
        "Y Kim",
        "J Kim"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "108",
      "title": "Speech Emotion Recognition Based on Multi-Label Emotion Existence Model",
      "authors": [
        "A Ando",
        "R Masumura",
        "H Kamiyama",
        "S Kobashikawa",
        "Y Aono"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "109",
      "title": "Generative Approach Using Soft-Labels to Learn Uncertainty in Predicting Emotional Attributes",
      "authors": [
        "K Sridhar",
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2021",
      "venue": "2021 9th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "110",
      "title": "Multimodal Feature Extraction and Fusion for Emotional Reaction Intensity Estimation and Expression Classification in Videos With Transformers",
      "authors": [
        "J Li",
        "Y Chen",
        "X Zhang",
        "J Nie",
        "Z Li",
        "Y Yu",
        "Y Zhang",
        "R Hong",
        "M Wang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops"
    },
    {
      "citation_id": "111",
      "title": "Challenges in real-life emotion annotation and machine learning based detection",
      "authors": [
        "L Devillers",
        "L Vidrascu",
        "L Lamel"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "112",
      "title": "Human Perception of Audio-Visual Synthetic Character Emotion Expression in the Presence of Ambiguous and Conflicting Information",
      "authors": [
        "E Mower",
        "M Mataric",
        "S Narayanan"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "113",
      "title": "The ambiguous world of emotion representation",
      "authors": [
        "V Sethu",
        "E Provost",
        "J Epps",
        "C Busso",
        "N Cummins",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "The ambiguous world of emotion representation",
      "arxiv": "arXiv:1909.00360"
    },
    {
      "citation_id": "114",
      "title": "Scripted dialogs versus improvisation: lessons learned about emotional elicitation techniques from the IEMOCAP database",
      "authors": [
        "C Busso",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "115",
      "title": "Evaluating deep learning architectures for Speech Emotion Recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Cognitive Engineering Using Neural Networks"
    },
    {
      "citation_id": "116",
      "title": "Combining a parallel 2D CNN with a self-attention Dilated Residual Network for CTC-based discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Q Li",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "J Tao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "117",
      "title": "Negative Emotion Recognition using Deep Learning for Thai Language",
      "authors": [
        "S Mekruksavanich",
        "A Jitpattanakul",
        "N Hnoohom"
      ],
      "year": "2020",
      "venue": "2020 Joint International Conference on Digital Arts, Media and Technology with ECTI Northern Section Conference on Electrical, Electronics, Computer and Telecommunications Engineering"
    },
    {
      "citation_id": "118",
      "title": "Utterance Level Feature Aggregation with Deep Metric Learning for Speech Emotion Recognition",
      "authors": [
        "B Mocanu",
        "R Tapu",
        "T Zaharia"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "119",
      "title": "Improving Speech Emotion Recognition with Unsupervised Representation Learning on Unlabeled Speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "120",
      "title": "Improving Speech Emotion Recognition Using Self-Supervised Learning with Domain-Specific Audiovisual Tasks",
      "authors": [
        "L Goncalves",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "121",
      "title": "Copypaste: An augmentation method for speech emotion recognition",
      "authors": [
        "R Pappagari",
        "J Villalba",
        "P Żelasko",
        "L Moro-Velazquez",
        "N Dehak"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "122",
      "title": "Transformer-based Label Set Generation for Multi-modal Multi-label Emotion Detection",
      "authors": [
        "X Ju",
        "D Zhang",
        "J Li",
        "G Zhou"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia, ser. MM '20",
      "doi": "10.1145/3394171.3413577"
    },
    {
      "citation_id": "123",
      "title": "No Sample Left Behind: Towards a Comprehensive Evaluation of Speech Emotion Recognition Systems",
      "authors": [
        "P Riera",
        "L Ferrer",
        "A Gravano",
        "L Gauder"
      ],
      "year": "2019",
      "venue": "Workshop on Speech, Music and Mind (SMM 2019)"
    },
    {
      "citation_id": "124",
      "title": "Curriculum Learning for Speech Emotion Recognition From Crowdsourced Labels",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "125",
      "title": "Hybrid Curriculum Learning for Emotion Recognition in Conversation",
      "authors": [
        "L Yang",
        "Y Shen",
        "Y Mao",
        "L Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "126",
      "title": "ERNetCL: A novel emotion recognition network in textual conversation based on curriculum learning strategy",
      "authors": [
        "J Li",
        "X Wang",
        "Y Liu",
        "Z Zeng"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "127",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "128",
      "title": "Be Confident! Towards Trustworthy Graph Neural Networks via Confidence Calibration",
      "authors": [
        "X Wang",
        "H Liu",
        "C Shi",
        "C Yang"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "129",
      "title": "Dawn of the Transformer Era in Speech Emotion Recognition: Closing the Valence Gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "130",
      "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "131",
      "title": "HuggingFace's transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz",
        "J Davison",
        "S Shleifer",
        "P Von Platen",
        "C Ma",
        "Y Jernite",
        "J Plu",
        "C Xu",
        "T Scao",
        "S Gugger",
        "M Drame",
        "A Rush"
      ],
      "year": "2019",
      "venue": "HuggingFace's transformers: State-of-the-art natural language processing",
      "arxiv": "arXiv:1910.03771v5"
    },
    {
      "citation_id": "132",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "133",
      "title": "PyTorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga",
        "A Desmaison",
        "A Kopf",
        "E Yang",
        "Z Devito",
        "M Raison",
        "A Tejani",
        "S Chilamkurthy",
        "B Steiner",
        "L Fang",
        "J Bai",
        "S Chintala"
      ],
      "year": "2019",
      "venue": "Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "134",
      "title": "Rethinking the Inception Architecture for Computer Vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "135",
      "title": "Exploiting Co-occurrence Frequency of Emotions in Perceptual Evaluations To Train A Speech Emotion Classifier",
      "authors": [
        "H.-C Chou",
        "C.-C Lee",
        "C Busso"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "136",
      "title": "Improved End-to-End Speech Emotion Recognition Using Self Attention Mechanism and Multitask Learning",
      "authors": [
        "Y Li",
        "T Zhao",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "137",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "138",
      "title": "The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and Affective Computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "139",
      "title": "A coefficient of agreement for nominal scales",
      "authors": [
        "J Cohen"
      ],
      "year": "1960",
      "venue": "A coefficient of agreement for nominal scales"
    },
    {
      "citation_id": "140",
      "title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
      "authors": [
        "P Rousseeuw"
      ],
      "year": "1987",
      "venue": "Journal of Computational and Applied Mathematics"
    },
    {
      "citation_id": "141",
      "title": "Speech Synthesis With Mixed Emotions",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Rana",
        "B Schuller",
        "H Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "142",
      "title": "Textless Speech-to-Speech Translation on Real Data",
      "authors": [
        "A Lee",
        "H Gong",
        "P.-A Duquenne",
        "H Schwenk",
        "P.-J Chen",
        "C Wang",
        "S Popuri",
        "Y Adi",
        "J Pino",
        "J Gu",
        "W.-N Hsu"
      ],
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "143",
      "title": "Textless Direct Speech-to-Speech Translation with Discrete Speech Representation",
      "authors": [
        "X Li",
        "Y Jia",
        "C.-C Chiu"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "144",
      "title": "Speech emotion recognition with acoustic and lexical features",
      "authors": [
        "Q Jin",
        "C Li",
        "S Chen",
        "H Wu"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "145",
      "title": "Ladder Networks for Emotion Recognition: Using Unsupervised Auxiliary Tasks to Improve Predictions of Emotional Attributes",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Ladder Networks for Emotion Recognition: Using Unsupervised Auxiliary Tasks to Improve Predictions of Emotional Attributes"
    },
    {
      "citation_id": "146",
      "title": "Using regional saliency for speech emotion recognition",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "147",
      "title": "Study Of Dense Network Approaches For Speech Emotion Recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "148",
      "title": "Speech Emotion Recognition Using Multi-hop Attention Mechanism",
      "authors": [
        "S Yoon",
        "S Byun",
        "S Dey",
        "K Jung"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "149",
      "title": "Active Learning With Complementary Sampling for Instructing Class-Biased Multi-Label Text Emotion Classification",
      "authors": [
        "X Kang",
        "X Shi",
        "Y Wu",
        "F Ren"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "150",
      "title": "EmoGraph: Capturing Emotion Correlations using Graph Networks",
      "authors": [
        "P Xu",
        "Z Liu",
        "G Winata",
        "Z Lin",
        "P Fung"
      ],
      "year": "2020",
      "venue": "CoRR"
    },
    {
      "citation_id": "151",
      "title": "Supervised Contrastive Learning with Nearest Neighbor Search for Speech Emotion Recognition",
      "authors": [
        "X Wang",
        "S Zhao",
        "Y Qin"
      ],
      "year": "2023",
      "venue": "Proc. INTERSPEECH 2023"
    },
    {
      "citation_id": "152",
      "title": "Emotion-Aware Contrastive Adaptation Network for Source-Free Cross-Corpus Speech Emotion Recognition",
      "authors": [
        "Y Zhao",
        "J Wang",
        "C Lu",
        "S Li",
        "B Schuller",
        "Y Zong",
        "W Zheng"
      ],
      "year": "2024",
      "venue": "ICASSP 2024 -2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "153",
      "title": "Multi-modal multi-label emotion detection with modality and label dependence",
      "authors": [
        "D Zhang",
        "X Ju",
        "J Li",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2020",
      "venue": "Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "154",
      "title": "Estimating the Uncertainty in Emotion Class Labels With Utterance-Specific Dirichlet Priors",
      "authors": [
        "W Wu",
        "C Zhang",
        "X Wu",
        "P Woodland"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "155",
      "title": "Acoustic Features and Neural Representations for Categorical Emotion Recognition from Speech",
      "authors": [
        "A Keesing",
        "Y Koh",
        "M Witbrock"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "156",
      "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "157",
      "title": "Chunk-Level Speech Emotion Recognition: A General Framework of Sequence-to-One Dynamic Temporal Modeling",
      "authors": [
        "W.-C Lin",
        "C Busso"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "158",
      "title": "Latent Emotion Memory for Multi-Label Emotion Classification",
      "authors": [
        "H Fei",
        "Y Zhang",
        "Y Ren",
        "D Ji"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "159",
      "title": "Minority views matter: Evaluating speech emotion classifiers with human subjective annotations by an all-inclusive aggregation rule",
      "authors": [
        "H Chou",
        "L Goncalves",
        "S Leem",
        "A Salman",
        "C Lee",
        "C Busso"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "160",
      "title": "The MERSA Dataset and a Transformer-Based Approach for Speech Emotion Recognition",
      "authors": [
        "E Zhang",
        "R Trujillo",
        "C Poellabauer"
      ],
      "venue": "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "161",
      "title": "",
      "authors": [
        "Thailand Bangkok"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "162",
      "title": "Facial emotion recognition with noisy multi-task annotations",
      "authors": [
        "S Zhang",
        "Z Huang",
        "D Paudel",
        "L Van Gool"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)"
    }
  ]
}