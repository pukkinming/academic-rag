{
  "paper_id": "2401.15743v1",
  "title": "Real-Time Eeg-Based Emotion Recognition Model Using Principal Component Analysis And Tree-Based Models For Neurohumanities",
  "published": "2024-01-28T20:02:13Z",
  "authors": [
    "Miguel A. Blanco-Rios",
    "Milton O. Candela-Leal",
    "Cecilia Orozco-Romo",
    "Paulina Remis-Serna",
    "Carol S. Velez-Saboya",
    "Jorge De-J. Lozoya-Santos",
    "Manuel Cebral-Loureda",
    "Mauricio A. Ramirez-Moreno"
  ],
  "keywords": [
    "EEG",
    "emotion recognition",
    "real-time",
    "PCA",
    "random forest",
    "humanities",
    "neurohumanities",
    "Descartes"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Within the field of Humanities, there is a recognized need for educational innovation, as there are currently no reported tools available that enable individuals to interact with their environment to create an enhanced learning experience in the humanities (e.g., immersive spaces). This project proposes a solution to address this gap by integrating technology and promoting the development of teaching methodologies in the humanities, specifically by incorporating emotional monitoring during the learning process of humanistic context inside an immersive space. In order to achieve this goal, a real-time emotion detection EEG-based system was developed to interpret and classify specific emotions. These emotions aligned with the early proposal by Descartes (Passions), including admiration, love, hate, desire, joy, and sadness. This system aims to integrate emotional data into the Neurohumanities Lab interactive platform, creating a comprehensive and immersive learning environment. This work developed a ML, real-time emotion detection model that provided Valence, Arousal, and Dominance (VAD) estimations every 5 seconds. Using PCA, PSD, RF, and Extra-Trees, the best 8 channels and their respective best band powers were extracted; furthermore, multiple models were evaluated using shift-based data division and cross-validations. After assessing their performance, Extra-Trees achieved a general accuracy of 96%, higher than the reported in the literature (88% accuracy). The proposed model provided real-time predictions of VAD variables and was adapted to classify Descartes' six main passions. However, with the VAD values obtained, more than 15 emotions can be classified (reported in the VAD emotion mapping) and extend the range of this application.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "An emotion is a psycho-physiological process triggered by the conscious or unconscious perception of an object or situation. This process is associated with a broad range of feelings, thoughts, and behaviors  (Jarymowicz and Bar-Tal, 2006) . The study of emotion generation is pivotal, as it underpins the human experience, influencing cognition, perception, and daily activities, including learning, communication, and rational decision-making  (Koelstra et al., 2012; Mina Mikhail and Allen, 2013) .\n\nAmong the early descriptions of emotion, one is provided by Descartes, which he described as passions in his work 'The Six Passions of Descartes.' For Descartes, the passions/emotions were experiences of the body on the soul, who, applying his famous method to moral philosophy, represented the problem of the passions of the soul in terms of its simplest integral components, distinguishing six different fundamental passions: admiration, love, hate, desire, joy, and sadness  (Descartes, 1649) .\n\nAccording to Descartes, admiration is understood as a sudden surprise of the soul that makes it consider (carefully) objects perceived as rare and extraordinary. This passion is directly related to the search for knowledge and philosophical reflection; love can be interpreted as the origin of the desire for union with someone or something that seems to be convenient; hate leads or drives to the rejection of something or someone; desire leads to an urge of possessing something that is out of reach; joy manifests when someone obtains that which they desire, while under a pleasant situation; lastly, sadness is experienced when losing something desired or during the experience of a painful situation  (Descartes, 1649) .\n\nThe study of emotion has evolved over the years. While early definitions of emotions characterized them as bodily phenomena, modern approaches also acknowledge a cognitive component  (Dixon, 2012) . The concept of emotion has been delved into both in the Humanistic and Scientific domains.\n\nThomas J.  Carew (2020)  define the humanities as encompassing all facets of human society and culture, including language, literature, philosophy, law, politics, religion, art, history, and social psychology. They underscore the importance of establishing closer collaborations between specific scientific domains, such as Neuroscience and the humanities. They posit that these collaborations will be mutually enriching and usher in a new era of profound and influential academic endeavors (Thomas J.  Carew, 2020) . Hartley and Poeppel contend that advancements in theoretical, computational, neuroimaging, and experimental psychology have enabled linguistics, music, and emotion to emerge as central pillars of contemporary cognitive neuroscience (Catherine A.  Hartley, 2020) .\n\nIn education, the advancement of Humanities teaching methodologies has been notably slower compared to other fields, such as Science and Engineering  (Manuel Cebral-Loureda, 2022) . This lag underscores the necessity of integrating technology into the Humanities, given its potential to foster human flourishing and enhance emotional education. Recent studies suggest that immersive and interactive teaching environments can significantly improve learning experiences and outcomes  (Chih-Ming Chen, 2014) . Moreover, the infusion of neuroscience principles into pedagogical strategies is a burgeoning area of interest, with preliminary results indicating promising potential for improved educational experiences  (Wilcox et al., 2021) .\n\nDeveloping research-based teaching approaches that combine Neuroscience and Education can help implement immersive and interactive systems for the teaching of Humanities. With this in mind, the Neurohumanities Lab project emerges, which intends to implement an immersive and interactive platform for the education of humanities that allows users to interact with the environment (the classroom) and fosters impactful, logical and intuitive learning  (Cebral-Loureda and Torres-Huitzil, 2021) . The idea behind the Neurohumanities Lab is to integrate a system that can detect movements, actions, emotions, and physiological and mental states through cameras and wearable biometric devices to modify the classroom environment (e.g., through changes in images, lighting, and sound). The proposal for this project is to implement a real-time emotion detection system using portable Electroencephalography (EEG) that can be integrated into the interactive platform of Neurohumanities Lab.\n\nOur solution is both timely and fitting to address the outlined challenge. It revolves around developing a real-time prediction model for the previously mentioned emotions (admiration, love, hate, desire, joy, and sadness) using brain signals as input. This model is seamlessly integrated into the Neurohumanities Lab's interactive platform. This integration makes the platform an ideal tool for educational innovation, offering students an immersive experience. As they interact within this enriched environment, they can explore and deepen their understanding of Humanities in a classroom setting, which traditionally might not have had such technological engagement. At the same time, students and teachers can understand emotion generation during such experiences and analyze them in context.\n\nCentral to our solution is the use of EEG signals. When properly processed, these signals reveal features pivotal for classifying the target emotions. The use of EEG in emotion recognition is not novel; its efficacy has been demonstrated in previous studies  (Islam et al., 2021; Stefano Valenzi, 2014) . Many studies suggest that EEG signals provide enough information for detecting human emotions with feature-based classification methods  (Stefano Valenzi, 2014) . Others have shown that emotional processing in the brain can be seen from the asymmetry in the brain activity recorded by EEG  (Brown et al., 2011) .\n\nVarious models have been proposed in the intriguing journey to understand and classify human emotions. One of the notable ones is the Circumplex 2D model put forward by James Russel  (Rusell, 1980) . This innovative model utilizes a two-dimensional approach, mapping emotions based on Valence and Arousal. Valence measures the emotion's intrinsic appeal, determining whether it is perceived as positive or negative. On the other hand, Arousal gauges the level of excitement or intensity associated with the emotion. However, the quest for deeper understanding did not stop there. A subsequent, more intricate, 3D model known as Pleasure, Arousal, and Dominance (PAD), or Valence, Arousal Dominance (VAD), came to the fore  (Islam et al., 2021) . This model broadened the horizon by incorporating these three elements. While Pleasure and Arousal are reminiscent of Russel's 2D model, adding Dominance provides additional insight. Dominance delves into the realm of control, assessing the extent to which an individual feels in command of, or subdued by, a particular emotion  (Islam et al., 2021) . This addition elevates the complexity of the model, shedding light on the dynamic interplay between emotions and the sense of power or submission they instill.\n\nThe EEG data acquisition process is characterized by several factors: the number of electrodes/channels, electrode placement system on the scalp (measurement of different brain regions), types of stimuli, sampling frequency, and the device used for signal acquisition. The International 10-20 electrode placement system is commonly used for emotion recognition using EEG  (Islam et al., 2021) .\n\nThe most fundamental and challenging task of recognizing human emotion is to find the most relevant features that vary with emotional state changes. The extracted EEG features for shallow and deep learning-based emotion recognition methods are the following: Time-domain features, which include statistical features such as mean, median, standard deviation, mode, variance, minimum, and maximum. The EEG frequency domain features usually contain more relevant information. The main methods are Power Spectral Density (PSD), Fast Fourier Transform (FFT), and the Short Time Fourier Transform (STFT) (Archana  Chaudhary, 2016; Giyoung Lee, 2014) . The Wavelet transform method of analysis presents a good performance both in the time and frequency domain  (Zeynab Mohammadi, 2017)  and can be classified into two types: Continuous Wavelet Transform  (Vladimir Bostanov, 2004)  and Discrete Wavelet Transform  (Islam et al., 2021) . The frequency domain approach was used for this work, focusing on PSD analysis. PSD analysis is a widely used technique for examining the power distribution of various frequency components in a signal and allows us to gain insight into the underlying frequency characteristics of the data. This approach enables the identification of prominent frequency bands or patterns that may indicate specific phenomena or attributes of the EEG signal. In addition, PSD analysis allows for quantifying the relative power contributions of different frequency components, providing valuable information for further analysis and interpretation of the mental states of the participants. Deep Auto Encoder (BDAE), Voting Ensembles (VEn), as classifiers. On the other hand, the second one includes Support Vector Machine (SVM), k Nearest Neighbor (kNN), Random Forest (RF), Decision Tree (DT), Multi-Layer Perceptron (MLP)  (Islam et al., 2021) . Deep learning techniques are more effective than shallow learning-based algorithms among a wide range of algorithms. However, it may be noted that the SVM performs well in EEG-based emotion. Whenever portability and simplicity are not required, the multimodal data incorporating the other physiological signals (e.g., heart rate, skin conductance, among others) can significantly improve the performance of the emotion recognition system.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Eeg",
      "text": "In this work, an evaluation of different classification algorithms was implemented to obtain the most accurate model that classifies the desired emotions through VAD estimations in real time. In order to achieve this, a feature extraction, feature selection, and model evaluation process was followed. The length of time windows for the real-time estimation was also included as a parameter when evaluating the models.\n\nGiven the aforementioned background, this work describes a real-time emotion detection algorithm based on VAD estimations for the prediction of the six Descartes' passions. Section 2 presents the details about the dataset used during this work; Section 3 shows a detailed description of the data analysis, feature extraction and selection, and model performance evaluation; Section 4 presents the results of the implementation and a detailed analysis of the variables and models, as well as the specifics of the best predictive model. Finally, section 5 presents a discussion of the obtained results.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Materials 2.1 Datasets",
      "text": "A review on EEG-based emotion recognition algorithms using deep and shallow learning techniques is presented in  (Islam et al., 2021) , analyzing 41 papers on this topic. Within those articles, 85% use publicly open datasets; in the rest 15%, a self-generated dataset was preferred. Among the 85% articles using publicly available datasets, 61%, 7%, 2%, and 15% of the articles used: A Database for Emotion Analysis using Physiological Signals (DAEP)  (Koelstra et al., 2012) , The Shanghai Jiao Tong University (SJTU) Emotion EEG Dataset (SEED)  (Zheng and Lu, 2015) , MAHNOB  (Soleymani et al., 2012)  and other datasets, respectively. About 26% used the images as stimuli, 23.8% used video, 17.5% used audio, 22.2% used the existing dataset comprising a combination of physiological and emotional data  (Alarcão and Fonseca, 2019) . The rest of the 10.5% works exploited the emotional data related to games, live performances, or life events. Among these works, different researchers used a diverse range of frequency band-pass filters, and among them, the 4-45 Hz is predominantly used (M. Rajya Lakshmi, 2014).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Deap Dataset",
      "text": "One of the main areas where Human-Computer Interfaces (HCI) are deficient is in the field of emotional intelligence. Most HCI systems are unable to interpret information derived from human emotional states and use it to prompt appropriate actions. With this in mind, the article by  (Koelstra et al., 2012)  describes a multimodal dataset that aids in analyzing human affective states. With this objective, the experiment was divided into two parts.\n\nThe chosen dataset used to train the model for this project was the DEAP dataset  (Koelstra et al., 2012) . The first part consists of a self-assessment where 16 subjects observed 120 one-minute videos and rated the three variables, Valence, Arousal, and Dominance, on a discrete (1-9) scale. The participants self-rated this scale using the Self Assessment Mannequins (SAM) (Margaret M.  Bradley, 1994) . These three planes can be used to quantitatively describe emotion with (1) Arousal, ranging from inactive (uninterested) to active (excited); (2) Dominance, either feeling weak (without control) or empowered (in control); and (3) Valence, ranging from unpleasant to pleasant, where sadness or stress are considered unpleasant and happiness or excitement are considered as pleasant  (Koelstra et al., 2012) .\n\nFrom 120 videos used, 60 were chosen semi-automatic using the \"Last.fm\" music enthusiast website, which allows users to assign tags to songs and retrieve songs assigned to those tags. In order to select these songs, a list of emotional keywords, as well as inflections and keywords, was used to generate a list of 304 keywords. For each of these keywords, a corresponding tag was searched in the \"Last.fm\" database, and the ten songs that were most often labeled with this particular tag were selected. This criterion yielded a total of 1084 songs (from which 60 of them were selected for the experiment). In order to choose these 60 songs, the valence-arousal space was divided into four quadrants, and 15 songs were manually selected for each quadrant. The quadrants are Low Arousal/Low Valence (LALV), Low Arousal/High Valence (LAHV), High Arousal/Low Valence (HALV) and High Arousal/High Valence (HAHV)  (Koelstra et al., 2012) . In addition to those 60 videos, another 60 were manually selected (15 videos for each quadrant).\n\nFor each of the selected (120) videos, a 1-minute segment was extracted for the experiment. Subjects were asked to watch each video and provide a VAD rating. Based on the subjective rating obtained from the volunteers, 40 videos were selected out of the original 120 videos, where videos with the strongest ratings and smallest variance were selected for the second part of the experiment  (Koelstra et al., 2012) .\n\nThe second part of the experiment consists of 32 subjects who watched 40 videos in a laboratory environment with controlled illumination and rated through a self-assessment the familiarity of the video on a discrete scale of 1-5 and the liking, arousal, valence, and dominance on a continuous scale of 1-9. While the volunteers were watching the videos, EEG and peripheral physiological signals were recorded, and face video was recorded for 22 participants. Peripheral physiological signals included in this experiment are Galvanic Skin Response (GSR) and Photoplethysmography (PPG).\n\nThe database shows an in-depth analysis of the correlates between the EEG signals from the subjects and the subjective ratings given to each video in order to be able to propose a new method for stimuli selection for emotion characterization, providing a statistical analysis of the data obtained  (Koelstra et al., 2012) .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Nhlab Functionality",
      "text": "The NeuroHumanities (NH) Lab's immersive platform integrates three primary functionalities: Real-time emotion detection, movement detection, and brain synchronization, which form the foundation of the proposed interactive platform.\n\nThe real-time emotion detection functionality aims to monitor and identify the emotions of individuals within the platform's space. This capability allows for real-time environmental adjustments (colored lights, sounds, music) based on detected emotions and physiological signals, ensuring a tailored experience that aligns with the user's current emotional state. The movement detection functionality provides an interactive dimension to the platform. It enables individuals to interface with the environment using bodily movements and carry out interactive tasks such as painting over a projected screen, selecting words from a projection, and generating changes on images using facial expression recognition. This interaction facilitates active participation and increases user engagement, fostering a more effective learning process. The brain synchronization function seeks to analyze the synchrony between the brain activity of two users. By monitoring the synchronization between EEG channels of both and comparing it with their concurrent behaviors, insights into neural activity related to different aspects of social interactions can be obtained.\n\nBy merging these three functionalities, the NH Lab's immersive platform offers a comprehensive educational experience that incorporates real-time emotion monitoring, active user movement interaction, and insights into brain synchronization.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Emotion Classification Model",
      "text": "The classification model used in this study is based on a 3D model of emotion, which utilizes the VAD framework, shown at Figure  1 . This 3D model represents a three-coordinate system, with each coordinate corresponding to one of the VAD labels.   (Islam et al., 2021) . Each VAD value had their unique direction and color: Arousal is green and at the y-axis (upwards and downwards), Valence is blue and at the x-axis (leftwards and rightwards) and dominance is red and at the z-axis (in and out). Additionally, Descartes' emotions are colored in purple (Hate, Love, Admiration, Joy, Desire, Sadness), while others are are colored in yellow.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Arousal",
      "text": "In order to obtain emotions based on the VAD values, a new classification framework was created. Values between 1 and 3.6 were considered low and represented by -1 in the 3D model. Values between 3.7 and 6.3 were considered as medium and represented by 0. Values between 6.4 and 9 were considered high and represented by 1 in the 3D model.\n\nCoordinates were then obtained based on the relationship between the three classes, where each class is associated with a specific emotion. The 3D emotion model used in this study is based on the six different passions proposed by Descartes, including admiration, love, hate, desire, joy, and sadness. The 3D model of emotion used is based on  (Islam et al., 2021) , with added passions of Descartes. The emotions highlighted in Table  1  are selected for classification in this work. This approach allows for the classification and representation of emotions within the 3D model, providing a framework for understanding and analyzing the participants' emotional responses.\n\nTable  1 : Relation between the emotion and its coordinate system based on the 3D model at Figure  1  Highlighted emotions are Descartes' passions.\n\nArousal Valence Dominance Emotion 0\n\n3 Methods",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Preparation",
      "text": "A pre-processed subset of data was used from the aforementioned DEAP Dataset to obtain the desired emotion detection model. The pre-processing consists of three main steps: (1) downsampling the data from 512 Hz to 128 Hz, (2) applying a band-pass filter between 0.4 Hz and 45 Hz, and (3) averaging the data to the common reference. These files were then combined, and the VAD values were extracted after applying the continuous to discrete transformation explained in Section 2.4.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Preliminary Model Testing",
      "text": "In the initial stages of the project, a preliminary analysis was performed to obtain a suitable machine-learning model for extracting valuable insights from the EEG data. A comparative study involved three well-established algorithms: the RF Classifier, XGBoost, and the SVM. These models were selected based on their known efficacy in handling high-dimensional data. For the preliminary analysis, data with all the available channels was used to train these models. Evaluation metrics, including accuracy, sensitivity, and specificity, were then used to measure the performance of each model. This foundational step was deemed critical for guiding the subsequent feature and channel selection procedures and validating them.\n\n3.3 Feature Engineering",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Psd Estimation",
      "text": "The pre-processed EEG data was segmented into 5-second time windows to capture transient and evolving neural dynamics over time. FFT was then applied to each segment, transforming the raw time-domain EEG data into the frequency domain to obtain PSD using the FFTProcessing function created by  Xu (2018) . Selecting an optimal time window length for practical real-time EEG analysis remains critical. A balance between precision and computational speed is essential: while longer windows often provide increased resolution and potential model accuracy, they may compromise processing speed and real-time responsiveness. On the other hand, shorter windows can facilitate rapid processing but might reduce precision. In order to address this balance, the accuracy of VAD prediction was evaluated across different time window lengths and identified an optimal window length. This strategic decision underpins the reliability and efficiency of the real-time implementation.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Frequency Band Ratios",
      "text": "To aid with the models' performance, different frequency band ratios were implemented for each channel, Relaxation Index ( θ δ ): This ratio accentuates the interplay between the theta waves, linked with drowsiness or light meditation states, and delta waves, associated with deep sleep and unconscious processes  (Borges Machado et al., 2010 ). An elevated Theta/Delta ratio often suggests a state of relaxation without lapsing into deep unconsciousness, making it a valuable marker for assessing relaxation in awake individuals.\n\nExcitement Index ( β α ): This ratio is served as an indicator of attention and engagement. It is suggested by a higher ratio that individuals are more alert and attentive, which can be interpreted as excitement or heightened interest. In the context of the research paper, the efficiency of advertisements at a population level was predicted using this ratio, indicating that ads evoking higher engagement, as measured by the beta/alpha ratio, are deemed more effective  (Kislov et al., 2023) .\n\nMental Fatigue Index ( α θ ): By examining the Alpha/Theta ratio, signs of mental weariness can be observed. A predominant alpha wave activity in relation to theta can signify a relaxed or idling state of the brain, which, especially during tasks requiring sustained attention, can indicate cognitive fatigue  (Ramírez-Moreno et al., 2021) .\n\nEngagement Index ( β θ+α ): In order to represent the balance of active cognitive processing versus a more passive state, this ratio is particularly crucial in contexts where the depth of cognitive immersion or focus is under scrutiny. A high value suggests robust cognitive engagement or alertness, paramount in activities demanding continuous mental effort  (Ismail and Karwowski, 2020) .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Feature Selection",
      "text": "",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Channel Selection Using Pca And Rf",
      "text": "Among the crucial features of the real-time model, it is noteworthy that the evaluated dataset includes data from 32 EEG channels; however, the proposed real-time algorithm is intended to be integrated into an 8-channel, dry-electrode OpenBCI system (OpenBCI, New York, NY, USA) for a highly-portable, practical implementation  (Lakhan et al., 2019; Yun Zhou, 2019) . Many studies have used only 8 channels to obtain EEG signals  (Brown et al., 2011; Mina Mikhail and Allen, 2013; Stefano Valenzi, 2014; Yun Zhou, 2019) . This 8-channel system allows the re-configuring of different positions of electrodes around the scalp.\n\nThe channel selection was conducted to assess which are the best 8 channels to use by the OpenBCI for the proposed emotion detection algorithm. Following this idea, channels were grouped into their respective lobes and joined their data via dimensionality reduction techniques; then, three RF models were fitted (one for each emotion component) in order to assess lobe importance with respect to a series of frequency bands.\n\nFeature importance becomes quite challenging given the high dimensionality of source features (32 channels • 5 frequency bands = 160), and the vast amount of total samples (593, 920; 58 seconds of a video clip through a 0.125 second moving window, 58/0.125 = 464 samples for each 40 videos and 32 subjects, 464 • 40 • 32 = 593, 920). Regression models such as RF would be not only slow due to high dimensionality, but slight differentiation between features' importance from one to another would be absent due to using normalized relative importance, which gives each feature an importance such that the sum of importance is equal to 1.\n\nPCA was used to reduce the dimensionality of the data; it was applied to source features regarding their channel anatomical location, separated within the Frontal, Temporal, Parietal, Occipital, Central, and Central-Parietal lobes, for each frequency band, and subjects' data. Table  2  shows which EEG channels were assigned to which lobe. The objective of applying PCA to the dataset is to gather insights about the brain region whose features are most linearly correlated to the target features, which would suggest a strong relation. The calculation of the PCA first consists of the Singular Value Decomposition (SVD) technique, which decomposes matrix X into the matrix multiplication of three matrices U Σ V T , as in Equation  1 .\n\nV has the unit vectors of the k components, represented in Equation  2 .\n\nV has n × k dimensionality, although we only require the first component in order to compress all the channels' vectors on each lobe to a single vector. So for each V matrix calculated, when considering the first component W , which is the eigenvector corresponding to the largest eigenvalue of the covariance matrix, for each lobe l and frequency band b with respect to a subject s, the whole X data (consisting of n samples by m channels), was projected into a Z vector (consisting of n samples by 1 dimension), thus leaving a unique vector of values for each combination of l and b on a subject s. The computation can be seen in Equation  3 .\n\nPearson correlation coefficient is calculated for each subject s as in Eq. 4; afterwards, an average across S subjects was calculated; variance between each subject's data was thus reduced as if PCA would be calculated on all complete data, features' domain between subjects would make PCA unstable, as the technique assumes that the dataset is centered around the origin; where z i represents each sample for each PCA l lobe and b frequency band, while y i represents each sample for each e emotion (VAD) component. Thus, a single mean absolute correlation coefficient is calculated for each source feature, averaged among each subject's data. This coefficient was further used in order to determine which channels are the most linearly correlated to the target emotion VAD, hence reducing the dimensionality of the whole dataset when sub-setting the best lobes on a particular frequency band for each emotion component. Once the most relevant lobes were found, an RF regressor was used to determine feature importance, thus obtaining the best 8 channels using feature selection. The RF model from Sklearn, an ML package from Python, contains the Gini importance (GI) metric, which is the normalized total reduction of a given criterion brought by each feature. On classification, it represents the number of splits in a decision tree that used that feature within the RF ensemble (Morales-Menendez et al., 2021); while on regression, it uses the Mean Squared Error (MSE) criterion, which follows the Euclidean norm, thus reducing the Euclidean distance between two points in a given vector space  (Candela-Leal et al., 2022)  and giving highest importance to the feature that reduced most variance.\n\nThe MSE criterion is shown in Equation  5 , which is essential when calculating GI in RF regression; the reduction of variance is calculated by minimizing the squared difference between target feature y and predicted target feature ŷ, based on RF model. The reduction of this criterion, at most, by including a specific channel c as a feature would then have the highest GI normalized across all C channels.\n\nGI was obtained for each subject's data s, then averaged across the total number of subjects S. Although RF is randomly initialized, to generalize better, I iterations were carried out in order to ensure that a random initialization would not benefit a specific feature (in which I = 10). Therefore, for each subject s, I RF models with different random initializations were run, and GI was averaged to obtain GI for a specific subject. Furthermore, each subject's GIs were averaged to obtain a global importance based on all subjects. This aforementioned calculation is shown in Equation  6 .\n\nConsidering only the best band and lobe combination, the criterion is computed for 32 channels c (C = 32); hence, the normalized feature selection criterion would be capable of detecting slight importance changes between features. Three RF models, one for each emotion e component (VAD), would be fitted; thus, each emotion component would have their best set of features, in which Equation 7 must be satisfied.\n\nGiven that the three GI would have the same domain and the same number of features, an Emotion Importance Index (EII) was calculated in order to evaluate the overall importance of each feature to the process of predicting E emotions, as in Equation  8 , hence proposing a more holistic approach on the feature selection process of selecting the best 8 EEG channels.\n\n3.5 Model evaluation",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Model Selection",
      "text": "A range of metrics was adopted to identify the optimal classifier model. Recognizing that a single metric might not fully capture a model's effectiveness, especially with varied data distributions, three metrics were employed: accuracy, balanced accuracy, and F1-score. These metrics were selected to offer a comprehensive understanding of model performance and to ensure a reliable choice was made. Metrics were calculated as in  (Aguilar-Herrera et al., 2021) , where True Positives (TP), True Negatives (TN), False Positives (FP) or type-I errors, and False Negatives (FN) or type-II errors are used to build up these metrics.\n\nAccuracy: In the model selection process, accuracy was utilized as a primary metric. Defined by the equation:\n\nAccuracy (Equation  9 ) represents the ratio of correctly predicted observations to the total observations. This measure assessed the overall correctness of the model's predictions.\n\nBalanced Accuracy (BA): Given the potential pitfalls of using accuracy alone, especially in imbalanced datasets, balanced accuracy was also employed. It is computed as the average of the true positive rate, or sensitivity (Equation  11 ) and the true negative rate, or specificity (Equation  12 ), given by: Balanced Accuracy = Sensitivity + Specificity 2 (10)\n\nWhere\n\nAnd\n\nThis metric gained a more nuanced understanding of the model's performance on both the positive and negative classes.\n\nF1-Score: For further depth in evaluation, the F1-score was incorporated. Represented by the equation:\n\nBy employing these metrics, a comprehensive understanding of the performance of different classifier models was ensured, allowing for a more informed model selection to be made. For each of the VAD classes (Low, Medium, and High), a total of 5 different classifier models were trained. Including three tree-based classification models: Extra-Trees (ET), Random Forest (RF), XGBoost (XGB); as well as other models such as k-Nearest Neighbor (kNN), Support Vector Classifier (SVC).",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Feature Importance Selection",
      "text": "Once the model was selected, its performance was evaluated over a range of feature subsets, specifically between 20 and 40 features. The optimal subset was identified based on the best performance metrics. The feature importance function from scikit-learn, applied using the ExtraTrees model, was utilized for this assessment.\n\nIn tree-based models such as Extra-Trees, a feature's importance is determined by the frequency and depth of its use for data splits across all trees  (Olivas et al., 2021) . A feature frequently used and closer to the tree roots is considered more crucial. The importance of a feature is typically calculated by averaging the decrease in impurity, often quantified using the Gini criterion, across all nodes where the feature facilitates a split  (Martínez et al., 2021) . By aggregating over the ensemble of trees, more robust and less biased feature importances are typically achieved.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Real-Time Implementation",
      "text": "In this work's final phase, the real-time application was developed. Emotion detection was achieved by integrating insights derived from three distinct Machine Learning (ML) models. This methodology was further enriched by including the three-dimensional emotion model proposed by  (Islam et al., 2021) . Figure  3  shows the real-time pipeline implemented for the EEG-based VAD estimation and thus detecting of Descartes' passions.\n\nThe pipeline consists of retrieving EEG signals from an 8-channel OpenBCI Ultracortex IV EEG helmet using a Cyton board. Further, using 5-second data and pre-processing it according to Section 3.1, features are created according to Section 3.3, further subsetting the best features according to each VAD model, as well as their respective prediction. Finally, the user received feedback based on the VAD predictions and the 3D emotion model mapping described in Figure  1 . The OpenBCI Ultracortex IV EEG helmet has a total of 35 possible node locations, with the default version being FP1, FP2, C3, C4, P7, P8, O1, and O2. However, these channels were used in the initial iteration of the project; these would be further replaced with the optimal emotion recognition channels regarding the channel selection results in Section 4.2.\n\nOn the other hand, the real-time use of the NH Lab platform can be seen in Figure  4 . In this platform, the user wears the OpenBCI helmet, and the emotion detection model identifies in real-time one of the six Descartes' passions. Depending on the detected emotion, the lighting of the environment changes. A camera, and a motion tracking algorithm are used to detect the users' movements that allow the generation of a painting on the projected screen. The color palette of the visualization, as well as sound effects and music related to the movement, are different depending on the detected emotion.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Time Window Selection",
      "text": "Using the standard OpenBCI channel configuration, PSD was obtained over various time window lengths, ranging from 1 to 10 seconds. In order to determine the optimal window length, the performance-to-time ratio was considered, especially since the intended application was in real-time scenarios  (Lozoya-Santos et al., 2022) . The results for time windows of 2, 4, 5, 8, and 10 seconds are presented in Table  3    3 : Accuracy comparison of different time windows on the three VAD components prior to the feature selection process using a standard RF model.\n\nFrom the results, the 5-second time window was selected for the final model since it provided an average accuracy of 88%, which was also observed for 8-second windows. Moreover, no significant improvements were observed for the 10-second window. Therefore, given the positive accuracy and the fast implementation it entails, the 5-second window was selected.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Channel Selection",
      "text": "According to the lobe regions defined in Table  2 , Sklearn library from Python was used to obtain the first component for each lobe and spectral band combination (6 lobes • 5 spectral bands = 30). This process significantly reduces the number of features analyzed, in contrast to the initial (32 channels • 5 spectral bands = 160). After the PCA was calculated for each subject, the mean absolute Pearson correlation coefficient was obtained, as stated in Equation  4 , which provided a unique importance value for each combination tested.\n\nResults of the PCA-found features' correlation are shown in Table  4 . A unique VAD value was calculated for each frequency b and and lobe combination; the summation of the three coefficients was also calculated to evaluate overall feature importance. The highest linearly correlated features correspond to the γ frequency band, as it has the best overall correlation among VAD for each lobe: Frontal with 0.4272, Temporal with 0.4433, Parietal with 0.4585, Occipital with 0.4563, Central with 0.4443, Central-Parietal with 0.4251; along with the best correlation coefficients for each emotion component: γ O for Arousal with 0.2011, γ P for Valence with 0.1316, γ T for Dominance with 0.1327.\n\nThere is a linear correlation between frequency bands and the sum of Pearson correlation coefficients; when frequency increases, this coefficient also increases. Considering the overall sum coefficients for each lobe, on each frequency band: δ has coefficients between 0.12 -0.14, θ has coefficients between 0.24 -0.26, α has coefficients between 0.25 -0.29, β has coefficients between 0.36 -0.41, γ has coefficients between 0.42 -0.45. Hence, based on those overall coefficients, there seem to be three clusters: δ, θ and α, β and γ, with the lowest frequency bands being the least linearly correlated and the highest frequency bands being the most linearly correlated. These results are similar to the reported by other authors  (Li and Lu, 2009; Martini et al., 2012; Yang et al., 2020) , who also determined that γ bandpower in EEG is the most suitable for emotion classification.\n\nIt is important to note that linear correlation does not directly mean more feature importance, as some features might be non-linearly correlated and still be critical features for the target feature prediction. However, for the first assessment and feature discrimination, the assumption of higher lineal correlation means higher feature importance is made. Furthermore, an RF regression model would be fitted on the best three lobes' channels in order to gather true feature importance with respect to the MDI criterion, which does not necessarily give importance to linearly correlated features with the target emotion.\n\nRegarding statistically significant linearly correlated features for more than 95% of the subjects on at least one of the emotion components (VAD), the higher frequency bands trend is displayed, as 4 features correspond to the γ band, 2 to the β band, and 3 to the α band. Arousal and Valence have several statistically significant linearly correlated features (α A , α C , β C , β P C , γ F , γ T , γ P C ) and (α O , α C , γ T , γ P ) respectively, although Dominance only has γ F .\n\nIn order to assess which are the best 8 channels to use in the OpenBCI helmet (available for real-time implementation), the best features 6 lobes with the highest sum of VAD coefficients were considered (γ F , γ T , γ P , γ O , γ C , γ P C ). Hence, all the lobes on the γ frequency band would be a good choice, so each channel c would have an assigned importance value (higher is better), calculated based on Equation 6, described in Section 3.4.1. In order to visually understand the importance of each channel, a topoplot was created, illustrating a spatial map of the obtained GI values. The topoplot is shown in Figure  5 .\n\nOverall, Arousal, Valence, and Dominance topoplots are similar, as the channels at the center of the brain are not as important as the ones at the exterior part of the brain; this pattern is also shared on EII, which displays the average patterns found on each of other three emotion component's plot. Focusing on EII's topoplot better generalizes overall",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Arousal",
      "text": "Valence Dominance EII 0.02 0.03 0.04 0.05 0.06 Gini importance feature importance on determining which channel is more related to VAD values, and thus it is more useful when identifying emotions.\n\nThe coefficients are also presented in Table  5 . Both T7 and T8 have the highest GI importance according to EII (0.0586, 0.0557), which are the only channels from the temporal lobe, following Fp1 (0.0519), and F7 (0.0442) from the frontal lobe. These results are similar to the reported by  (Zhang et al., 2016; Wang et al., 2019) , where temporal and frontal regions had the highest importance for emotion recognition. Next important channels are FC5 (0.0434), O2 (0.0432), P7 (0.0388) and FC6 (0.0387). Which are from emotion component regions such as the middle left and right hemispheres, as well as frontal and parietal lobes  (Wang et al., 2019) . Other authors have also used these channels, such as F7, F8, and T7-FC2  (Javidan et al., 2021) ; FP1, T7 and T8 on γ, FC6 on β  (Guo et al., 2022) ; O2, T8, FC5, and P7  (Dura and Wosiak, 2021) ; FP1-F7  (Taran and Bajaj, 2019) ; F7, FC5, FC6, O2, and P7  (Wang et al., 2019) .\n\nIn this sense, the proposed set of 8 channels consists of 4 frontal channels (Fp1, F7, FC5, FC6), 2 temporal channels (T7, T8), 1 parietal channel (P7) and 1 occipital channel (O2); which correspond to 5 channels from the left hemisphere and 3 channels from the right hemisphere, and not including any channels from the previously established central and centro-parietal lobes. These results suggest that this set of 8 EEG channels would allow us to obtain an optimized version of the model in future iterations. Since the OpenBCI system allows channel reconfiguration, it could be easily implemented to measure EEG signals from such electrodes. So instead of the default OpenBCI channel configuration shown in Figure  6  A), the proposed set of channels would be used as in Figure  6 B ).",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Model Selection",
      "text": "After the channel selection analysis described in Section 4.2 was performed, 42 different classifier models were trained using these 8 channels and their respective PSDs, although results from the best 15 models are shown in Table  6 . The Extra-Trees model achieves the best accuracy, BA, and F1-score, as it performed significantly better than the rest of the models (95% accuracy) on all VAD components, with only the Random Forest following behind. In order to avoid doing an expensive computation, these models were trained using an extract of the training data, taking into account every 16 steps; this allowed for model selection in a short period of time. In constructing the Extra-Trees model, decision trees are generated from the entire dataset. Unlike traditional tree algorithms where the best split among all possible splits is chosen, in the Extra Trees methodology, splits are randomly selected for each candidate feature, and the best of these randomly generated splits is used. When combined with ensemble techniques where multiple trees are built and averaged, this randomness often produces a model that is less prone to overfitting. Additionally, this random selection eliminates the need for bootstrapping in sampling, meaning the whole sample is used in constructing each tree  (Géron, 2019) .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Feature Selection",
      "text": "In order to determine the optimal number of features, the Extra-Trees' GI was applied on all the 8-channel's PSD and index features. Furthermore, a series of Extra-Trees models were fitted using the best 25 to 35 features. Results of the accuracy of scores on Valence, Arousal, and Dominance prediction with different numbers of features with these models are shown in Table  7 . Highlighted results show the best number of features for each label and the best overall average. The average accuracy of the best number of features for each channel was calculated; there can be a decrease in average accuracy using the best 34 features than the best 35 features, so 34 features were selected as the optimal number of features to be used in the final model. In this sense, a final Extra-Trees model was fitted using the best 34 features for each VAD component. Table  8  shows the final chosen features for each of the VAD-trained models. There are a total of 34 features for each VAD classification model. Powerbands are ordered according to their most prevalent powerband in descending order. It can be seen that in the three models (VAD), the most predominant features were from θ, γ, β, and α, as delta only had 1 feature on Valence and 1 feature on Arousal. Furthermore, θ, β, and γ had the most number of channels, with 23, 24, and 24 channels, respectively, following α with 17 channels. It is quite interesting that the γ bandpower is still prevalent, as shown in the channel selection analysis on Section 4.2, and that the higher frequency features are the most related to emotions, as also suggested in the same section at Table  4 . The hybrid feature selection method implemented, which used Pearson's linear correlation coefficient and Extra-Trees' non-linear GI, led to better generalization across the dataset, thus gathering essential insights that lead to optimized performance on 8-channel emotion recognition while including additional index features.\n\nBest model's performance (Extra-Trees with 34 features) achieved an accuracy of 0.96 for Valence, 0.959 for Arousal and 0.967 for Dominance, with an average accuracy of 0.963. The respective confusion matrices are shown in Figure  7 .\n\nIt can be seen that, overall, the accuracy for each of the quadrants is > 0.96, and there is not a clear sign of missclassification of each of the true and predicted labels, thus showing that residuals are randomly distributed and that high accuracy is balanced among classes of each of the models.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusions",
      "text": "After all previous analyses were performed, a final model was trained. The chosen model was an Extremely Randomized Trees Classifier (Extra-Trees), as it showed significantly higher precision than the rest of the chosen models. The final features are shown in Table  8 , calculated with 5-second time windows, using the proposed channels for the optimal OpenBCI Ultracortex IV configuration at Figure  6 B ).\n\nThe current model is based on PSD estimations only. However, different types of features could add higher complexity, such as neural connectivity metrics. Functional connectivity metrics could be added to the model, for instance, between each channel opposite pair FC5-FC6, T7-T8, as well as other types of bipolar connections such as FP1-F7  (Taran and Bajaj, 2019) , in addition to T7-P7 and T7-FP1  (Meyers et al., 2021) , that have been reported previously as efficient neural markers.\n\nIt is also important to note that the model can be expanded to include variables other than EEG. Integrating other physiological measures, such as Electro-oculography (EOG), Electromyography (EMG), and Electrocardiography (ECG), can bolster the robustness and accuracy of our emotion prediction. These complementary measures provide a multifaceted view of the human emotional response, ensuring a more comprehensive analysis  (Koelstra et al., 2012) .\n\nA future iteration of this algorithm will include such variables, into account, aiming to increase the complexity and accuracy of the model.\n\nThis project presented several limitations. Firstly, the OpenBCI Ultracortex Mark IV that was employed for the real-time application only has 8 available channels for data collection. This can potentially limit the granularity and spatial resolution of the EEG data. In a future scenario, we could add a daisy extension to have more available channels.\n\nSecondly, the utilized dataset is derived from a relatively small sample size of just 32 subjects, which can introduce biases and might not be able to correctly represent the broader population, affecting the model's overall generalizability.\n\nFuture steps of this work will also include the implementation of the real-time algorithm into experimental tests of the users while interacting with the immersive platform, and the generated emotions will be compared to the ones experienced by users in a (non-immersive) control group. Moreover, the inclusion of more biometric signals will offer more insights into the interactions experienced by test subjects when presented with these different scenarios.",
      "page_start": 17,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: This 3D model represents a three-coordinate system, with each coordinate corresponding to one of",
      "page": 5
    },
    {
      "caption": "Figure 1: Three-dimensional VAD emotion model based on the work by (Islam et al., 2021). Each VAD value had their",
      "page": 5
    },
    {
      "caption": "Figure 1: Highlighted",
      "page": 6
    },
    {
      "caption": "Figure 2: Flowchart of the Model Training Process, including feature extraction, selection, model performance",
      "page": 6
    },
    {
      "caption": "Figure 3: shows the real-time pipeline",
      "page": 10
    },
    {
      "caption": "Figure 3: Flowchart of the real-time emotion detection implementation.",
      "page": 11
    },
    {
      "caption": "Figure 1: The OpenBCI Ultracortex IV EEG helmet has a total of 35 possible node locations, with the default version",
      "page": 11
    },
    {
      "caption": "Figure 4: In this platform, the user wears the",
      "page": 11
    },
    {
      "caption": "Figure 4: Testing the real-time application of the algorithm at the NeuroHumanities Lab at Tecnologico de Monterrey,",
      "page": 11
    },
    {
      "caption": "Figure 5: Overall, Arousal, Valence, and Dominance topoplots are similar, as the channels at the center of the brain are not as",
      "page": 13
    },
    {
      "caption": "Figure 5: Average GI topoplot for each channel, considering only γ frequency band (30-45 Hz) and fitting a separate",
      "page": 14
    },
    {
      "caption": "Figure 6: A), the proposed set of channels would be used as in Figure 6 B).",
      "page": 15
    },
    {
      "caption": "Figure 6: OpenBCI channel configurations, A) Default configuration (FP1, FP2, C3, C4, P7, P8, O1, O2), B) Optimal",
      "page": 15
    },
    {
      "caption": "Figure 7: It can be seen that, overall, the accuracy for each of the quadrants is > 0.96, and there is not a clear sign of miss-",
      "page": 16
    },
    {
      "caption": "Figure 7: In confusion matrices for each VAD component model, the highest accuracy is expected to be in the diagonal",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Lobe\nfband": "F\nT\nP\nδ\nO\nC\nCP",
          "Σ\nArousal\nValence\nDominance": "0.0472\n0.0447\n0.0422\n0.1341"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.0511\n0.0455\n0.0436\n0.1402"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.05\n0.0435\n0.0445\n0.138"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.0438\n0.0423\n0.0426\n0.1287"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.0511\n0.043\n0.0447\n0.1388"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.0456\n0.0446\n0.0406\n0.1308"
        },
        {
          "Lobe\nfband": "F\nT\nP\nθ\nO\nC\nCP",
          "Σ\nArousal\nValence\nDominance": "0.0877\n0.0795\n0.0778\n0.245"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.0954\n0.0804\n0.0785\n0.2543"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.0971\n0.083\n0.0833\n0.2634"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.0946\n0.0762\n0.0794\n0.2502"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.0945\n0.0783\n0.0812\n0.254"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.0839\n0.083\n0.0773\n0.2442"
        },
        {
          "Lobe\nfband": "F\nT\nP\nα\nO\nC\nCP",
          "Σ\nArousal\nValence\nDominance": "0.1146∗\n0.0686\n0.0763\n0.2595"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1123\n0.073\n0.078\n0.2633"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1138\n0.087\n0.0845\n0.2853"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1233\n0.0883∗\n0.0844\n0.296"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1182∗\n0.0828∗\n0.0836\n0.2846"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1108\n0.0815\n0.08\n0.2723"
        },
        {
          "Lobe\nfband": "F\nT\nP\nβ\nO\nC\nCP",
          "Σ\nArousal\nValence\nDominance": "0.1642\n0.0989\n0.1065\n0.3696"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1684\n0.0988\n0.1183\n0.3855"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1894\n0.1113\n0.1184\n0.4191"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1797\n0.1072\n0.1094\n0.3963"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1788∗\n0.0985\n0.1207\n0.398"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1696∗\n0.1083\n0.1084\n0.3863"
        },
        {
          "Lobe\nfband": "F\nT\nP\nγ\nO\nC\nCP",
          "Σ\nArousal\nValence\nDominance": "0.1868∗\n0.1228\n0.1176∗\n0.4272"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.188∗\n0.1226∗\n0.1327\n0.4433"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.2005\n0.1264\n0.1316∗\n0.4585"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1295\n0.1257\n0.2011\n0.4563"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1958\n0.1178\n0.1307\n0.4443"
        },
        {
          "Lobe\nfband": "",
          "Σ\nArousal\nValence\nDominance": "0.1783∗\n0.1281\n0.1187\n0.4251"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Valence": "93.71\n4.17\n2.13\n1.05\n97.72\n1.24\n1.14\n3.7\n95.17",
          "Arousal": "92.62\n4.79\n2.58\n0.63\n97.37\n2\n0.79\n3.87\n95.34",
          "Dominance": "92.71\n5.54\n1.75\n0.48\n98.33\n1.19\n0.59\n2.24\n97.17"
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Advanced learner assistance system's (ALAS) recent results",
      "authors": [
        "A Aguilar-Herrera",
        "E Delgado-Jimenez",
        "M Candela-Leal",
        "G Olivas-Martinez",
        "G Alvarez-Espinosa",
        "M Ramirez-Moreno",
        "Jesus De",
        "J Lozoya-Santos",
        "R Ramirez-Mendoza"
      ],
      "year": "2021",
      "venue": "2021 Machine Learning-Driven Digital Technologies for Educational Innovation Workshop"
    },
    {
      "citation_id": "2",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "S Alarcão",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "An improved random forest classifier for multi-class classification",
      "authors": [
        "Archana Chaudhary",
        "R Savita Kolhe"
      ],
      "year": "2016",
      "venue": "An improved random forest classifier for multi-class classification"
    },
    {
      "citation_id": "4",
      "title": "Modulation of sleep homeostasis by corticotropin releasing hormone in rem sleep-deprived rats",
      "authors": [
        "R Borges Machado",
        "S Tufik",
        "D Suchecki"
      ],
      "year": "2010",
      "venue": "International journal of endocrinology"
    },
    {
      "citation_id": "5",
      "title": "Towards wireless emotional valence detection from eeg",
      "authors": [
        "L Brown",
        "B Grundlehner",
        "J Penders"
      ],
      "year": "2011",
      "venue": "2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "6",
      "title": "Multi-output sequential deep learning model for athlete force prediction on a treadmill using 3d markers",
      "authors": [
        "M Candela-Leal",
        "E Gutiérrez-Flores",
        "G Presbítero-Espinosa",
        "A Sujatha-Ravindran",
        "R Ramírez-Mendoza",
        "Jesús De",
        "J Lozoya-Santos",
        "M Ramírez-Moreno"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "7",
      "title": "Beyond the stimulus: A neurohumanities approach to language, music, and emotion",
      "authors": [
        "Catherine Hartley"
      ],
      "year": "2020",
      "venue": "Neuron"
    },
    {
      "citation_id": "8",
      "title": "Neural deep learning models for learning analytics in a digital humanities laboratory",
      "authors": [
        "M Cebral-Loureda",
        "C Torres-Huitzil"
      ],
      "year": "2021",
      "venue": "Neural deep learning models for learning analytics in a digital humanities laboratory"
    },
    {
      "citation_id": "9",
      "title": "Effects of different text display types on reading comprehension, sustained attention and cognitive load in mobile reading contexts",
      "authors": [
        "Chih-Ming Chen"
      ],
      "year": "2014",
      "venue": "Interactive Learning Environments"
    },
    {
      "citation_id": "10",
      "title": "Les passions de l'áme",
      "authors": [
        "R Descartes"
      ],
      "year": "1649",
      "venue": "Les passions de l'áme"
    },
    {
      "citation_id": "11",
      "title": "The history of a keyword in crisis",
      "authors": [
        "T Dixon"
      ],
      "year": "2012",
      "venue": "The history of a keyword in crisis"
    },
    {
      "citation_id": "12",
      "title": "EEG channel selection strategy for deep learning in emotion recognition",
      "authors": [
        "A Dura",
        "A Wosiak"
      ],
      "year": "2021",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "13",
      "title": "Emotion recognition based on 3d fuzzy visual and eeg features in movie clips",
      "authors": [
        "A ; Géron",
        "O Tensorflow",
        "Reilly"
      ],
      "year": "2014",
      "venue": "On Machine Learning with Scikit-Learn &"
    },
    {
      "citation_id": "14",
      "title": "A transformer based neural network for emotion recognition and visualizations of crucial EEG channels",
      "authors": [
        "J.-Y Guo",
        "Q Cai",
        "J.-P An",
        "P.-Y Chen",
        "C Ma",
        "J.-H Wan",
        "Z.-K Gao"
      ],
      "year": "2022",
      "venue": "Physica A: Statistical Mechanics and its Applications"
    },
    {
      "citation_id": "15",
      "title": "Emotion recognition from eeg signal focusing on deep learning and shallow learning techniques",
      "authors": [
        "M Islam",
        "M Moni",
        "M Islam",
        "M Rashed-Al-Mahfuz",
        "M Islam",
        "M Hasan",
        "M Hossain",
        "M Ahmad",
        "S Uddin",
        "A Azad",
        "S Alyami",
        "M Ahad",
        "P Lió"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Applications of EEG indices for the quantification of human cognitive performance: A systematic review and bibliometric analysis",
      "authors": [
        "L Ismail",
        "W Karwowski"
      ],
      "year": "2020",
      "venue": "PloS one"
    },
    {
      "citation_id": "17",
      "title": "The dominance of fear over hope in the life of individuals and collectives",
      "authors": [
        "M Jarymowicz",
        "D Bar-Tal"
      ],
      "year": "2006",
      "venue": "European Journal of Social Psychology"
    },
    {
      "citation_id": "18",
      "title": "Feature and channel selection for designing a regression-based continuous-variable emotion recognition system with two EEG channels",
      "authors": [
        "M Javidan",
        "M Yazdchi",
        "Z Baharlouei",
        "A Mahnam"
      ],
      "year": "2021",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "19",
      "title": "Central eeg beta/alpha ratio predicts the population-wide efficiency of advertisements",
      "authors": [
        "A Kislov",
        "A Gorin",
        "N Konstantinovsky",
        "V Klyuchnikov",
        "B Bazanov",
        "V Klucharev"
      ],
      "year": "2023",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "20",
      "title": "Deap: A database for emotion analysis ;using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Consumer grade brain sensing for emotion recognition",
      "authors": [
        "P Lakhan",
        "N Banluesombatkul",
        "V Changniam",
        "R Dhithijaiyratn",
        "P Leelaarporn",
        "E Boonchieng",
        "S Hompoonsup",
        "T Wilaiprasitporn"
      ],
      "year": "2019",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "22",
      "title": "Emotion classification based on gamma-band EEG",
      "authors": [
        "M Li",
        "B.-L Lu"
      ],
      "year": "2009",
      "venue": "2009 Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "23",
      "title": "Current and Future Biometrics: Technology and Applications",
      "authors": [
        "J Lozoya-Santos",
        "M Ramírez-Moreno",
        "G Diaz-Armas",
        "L Acosta-Soto",
        "M Leal",
        "R Abrego-Ramos",
        "R Ramirez-Mendoza"
      ],
      "year": "2022",
      "venue": "Current and Future Biometrics: Technology and Applications"
    },
    {
      "citation_id": "24",
      "title": "Survey on eeg signal processing methods",
      "authors": [
        "M Lakshmi",
        "T Prasad"
      ],
      "year": "2014",
      "venue": "Journal of Advanced Research in Computer Science and Software Engineering"
    },
    {
      "citation_id": "25",
      "title": "The fertility of a concept: A bibliometric review of human flourishing",
      "authors": [
        "Manuel Cebral-Loureda",
        "Enrique Tamés-Muñoz"
      ],
      "year": "2022",
      "venue": "Int J Environ Res Public Health"
    },
    {
      "citation_id": "26",
      "title": "Journal of behavior therapy and experimental psychiatry. Measuring emotion: The self-assessment manikin and the semantic differential",
      "authors": [
        "Margaret Bradley"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry. Measuring emotion: The self-assessment manikin and the semantic differential"
    },
    {
      "citation_id": "27",
      "title": "The dynamics of EEG gamma responses to unpleasant visual stimuli: From local activity to functional connectivity",
      "authors": [
        "N Martini",
        "D Menicucci",
        "L Sebastiani",
        "R Bedini",
        "A Pingitore",
        "N Vanello",
        "M Milanesi",
        "L Landini",
        "A Gemignani"
      ],
      "year": "2012",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "28",
      "title": "Detection of engineering interest in children through an intelligent system using biometric signals",
      "authors": [
        "G Martínez",
        "M Leal",
        "J Alvarado",
        "L Soto",
        "Diego Mauricio Botín",
        "G Sanabria",
        "Judith Aimé",
        "E Herrera",
        "M Espinosa",
        "M Moreno",
        "R Menéndez",
        "R Ramírez",
        "Jesús Lozoya Santos De",
        "J Mendoza"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Industrial Engineering and Operations Management (IEOM)"
    },
    {
      "citation_id": "29",
      "title": "The association of polygenic risk for schizophrenia, bipolar disorder, and depression with neural connectivity in adolescents and young adults: examining developmental and sex differences",
      "authors": [
        "J Meyers",
        "D Chorlian",
        "T Bigdeli",
        "E Johnson",
        "F Aliev",
        "A Agrawal",
        "L Almasy",
        "A Anokhin",
        "H Edenberg",
        "T Foroud",
        "A Goate",
        "C Kamarajan",
        "S Kinreich",
        "J Nurnberger",
        "A Pandey",
        "G Pandey",
        "M Plawecki",
        "J Salvatore",
        "J Zhang",
        "A Fanous",
        "B Porjesz"
      ],
      "year": "2021",
      "venue": "Translational Psychiatry"
    },
    {
      "citation_id": "30",
      "title": "Using minimal number of electrodes for emotion detection using brain signals produced from a new elicitation technique",
      "authors": [
        "Mina Mikhail",
        "Khaled El-Ayat",
        "J Allen"
      ],
      "year": "2013",
      "venue": "International Journal of Autonomous and Adaptive Communications Systems"
    },
    {
      "citation_id": "31",
      "title": "Real-time biofeedback system for interactive learning using wearables and iot",
      "authors": [
        "R Morales-Menendez",
        "M Ramírez-Moreno",
        "J Lozoya-Santos"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Industrial Engineering and Operations Management (IEOM)"
    },
    {
      "citation_id": "32",
      "title": "Detecting change in engineering interest in children through machine learning using biometric signals",
      "authors": [
        "G Olivas",
        "M Candela",
        "J Ocampo",
        "L Acosta",
        "A Aguilar",
        "E Delgado",
        "M Alanis",
        "M Ramirez",
        "J Lozoya",
        "J Murrieta",
        "R Ramírez"
      ],
      "year": "2021",
      "venue": "2021 Machine Learning-Driven Digital Technologies for Educational Innovation Workshop"
    },
    {
      "citation_id": "33",
      "title": "Evaluation of a fast test based on biometric signals to assess mental fatigue at the workplace-a pilot study",
      "authors": [
        "M Ramírez-Moreno",
        "P Carrillo-Tijerina",
        "M Candela-Leal",
        "M Alanis-Espinosa",
        "J Tudón-Martínez",
        "A Roman-Flores",
        "R Ramírez-Mendoza",
        "J Lozoya-Santos"
      ],
      "year": "2021",
      "venue": "International Journal of Environmental Research and Public Health"
    },
    {
      "citation_id": "34",
      "title": "A circumplex model of affect",
      "authors": [
        "J Rusell"
      ],
      "year": "1980",
      "venue": "J. Personality Social Psychol"
    },
    {
      "citation_id": "35",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Individual classification of emotions using eeg",
      "authors": [
        "Stefano Valenzi",
        "Tanvir Islam"
      ],
      "year": "2014",
      "venue": "Journal of Biomedical Science and Engineering"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition from single-channel EEG signals using a two-stage correlation and instantaneous frequency-based filtering method",
      "authors": [
        "S Taran",
        "V Bajaj"
      ],
      "year": "2019",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "38",
      "title": "The neurohumanities: An emerging partnership for exploring the human experience",
      "authors": [
        "Thomas Carew"
      ],
      "year": "2020",
      "venue": "Neuron"
    },
    {
      "citation_id": "39",
      "title": "Recognition of affective prosody: Continuous wavelet measures of event-related brain potentials to emotional exclamations",
      "authors": [
        "B Vladimir Bostanov"
      ],
      "year": "2004",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "40",
      "title": "Channel selection method for EEG emotion recognition using normalized mutual information",
      "authors": [
        "Z.-M Wang",
        "S.-Y Hu",
        "H Song"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "41",
      "title": "Why educational neuroscience needs educational and school psychology to effectively translate neuroscience to educational practice",
      "authors": [
        "G Wilcox",
        "L Morett",
        "Z Hawes",
        "E Dommett"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "42",
      "title": "Eeg classifier based on deap database",
      "authors": [
        "T Xu"
      ],
      "year": "2018",
      "venue": "Eeg classifier based on deap database"
    },
    {
      "citation_id": "43",
      "title": "Brain processes while struggling with evidence accumulation during facial emotion recognition: An ERP study",
      "authors": [
        "Y.-F Yang",
        "E Brunet-Gouet",
        "M Burca",
        "E Kalunga",
        "M.-A Amorim"
      ],
      "year": "2020",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "44",
      "title": "Beyond engagement: an eeg-based methodology for assessing user's confusion in an educational game",
      "authors": [
        "Yun Zhou",
        "Tao Xu"
      ],
      "year": "2019",
      "venue": "Universal Access in the Information Society"
    },
    {
      "citation_id": "45",
      "title": "Wavelet-based emotion recognition system using eeg signal",
      "authors": [
        "J Zeynab Mohammadi"
      ],
      "year": "2017",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "46",
      "title": "ReliefF-based EEG sensor selection methods for emotion recognition",
      "authors": [
        "J Zhang",
        "M Chen",
        "S Zhao",
        "S Hu",
        "Z Shi",
        "Y Cao"
      ],
      "year": "2016",
      "venue": "Sensors"
    },
    {
      "citation_id": "47",
      "title": "Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    }
  ]
}