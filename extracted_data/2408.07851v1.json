{
  "paper_id": "2408.07851v1",
  "title": "Ser Evals: In-Domain And Out-Of-Domain Benchmarking For Speech Emotion Recognition",
  "published": "2024-08-14T23:33:10Z",
  "authors": [
    "Mohamed Osman",
    "Daniel Z. Kaplan",
    "Tamer Nadeem"
  ],
  "keywords": [
    "speech recognition",
    "human-computer interaction",
    "computational paralinguistics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) has made significant strides with the advent of powerful self-supervised learning (SSL) models. However, the generalization of these models to diverse languages and emotional expressions remains a challenge. We propose a large-scale benchmark to evaluate the robustness and adaptability of state-of-the-art SER models in both in-domain and out-of-domain settings. Our benchmark includes a diverse set of multilingual datasets, focusing on less commonly used corpora to assess generalization to new data. We employ logit adjustment to account for varying class distributions and establish a single dataset cluster for systematic evaluation. Surprisingly, we find that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. Our results highlight the need for more robust and generalizable SER models, and our benchmark serves as a valuable resource to drive future research in this direction.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition has garnered significant attention due to its potential to enable more natural and empathetic human-computer interaction. Recent advancements in selfsupervised learning have led to powerful speech representation models like wav2vec2  [1] , HuBERT  [2] , and WavLM  [3] , which have shown impressive performance on various speech processing tasks. However, the generalization of these models to diverse languages and emotional expressions remains a critical challenge  [4] .\n\nExisting SER benchmarks often focus on a limited set of well-studied datasets, which may not accurately reflect realworld scenarios  [5] . Moreover, the emphasis on in-domain evaluation fails to capture the crucial aspect of out-of-domain generalization, which is essential for deploying SER systems in practical applications. For our paper's purpose, we define in-domain as evaluating on the same data distribution seen in training, and out-of-domain as evaluating on a different data distribution. This can manifest as different speakers, tones, decision boundaries, etc. To address these limitations, we propose a large-scale benchmark that evaluates SER models on a diverse collection of multilingual datasets, emphasizing zero-shot performance.\n\nOur benchmark focuses on less commonly used datasets to mitigate overfitting and encourage the development of more robust and adaptable models. We employ state-of-the-art speech representation models, including Whisper  [6] , an automatic speech recognition model, and CLAP  [7, 8] , a contrastive learn-",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Self-supervised learning has revolutionized speech representation learning, enabling models to capture rich acoustic features without relying on labeled data. Models like wav2vec 2.0  [1] , HuBERT  [2] , and WavLM  [3]  have achieved state-of-theart performance on various speech processing tasks, including speech recognition, speaker identification, and emotion recognition  [29] . Cross-lingual SER has gained attention as a means to develop models that can generalize across languages. Several studies have explored the use of SSL models for cross-lingual SER  [30, 4] . However, these works often focus on a limited set of languages and datasets, making it difficult to assess the true  generalization capabilities of the models.\n\nExisting well-known SER benchmarks, such as IEMOCAP  [31]  and MSP-Podcast  [32] , have played a crucial role in advancing the field. However, these benchmarks often emphasize in-domain evaluation and may not adequately capture the challenges of real-world deployment  [5] . Our work aims to address these limitations by introducing a large-scale benchmark that focuses on out-of-domain generalization and includes a diverse set of multilingual datasets.\n\nRecent works such as EMO-SUPERB  [33]  and SERAB  [34]  have made notable contributions to the field of Speech Emotion Recognition (SER). However, these works have limitations in terms of the diversity of languages, datasets, and the emphasis on out-of-domain generalization.\n\nOur benchmark significantly advances the state-of-the-art in SER evaluation by addressing these limitations. We curate an extensive collection of multilingual datasets, carefully selected to cover diverse linguistic and cultural contexts, ensuring a thorough evaluation of SER models in real-world scenarios. Moreover, our benchmark places a strong emphasis on out-ofdomain generalization, a crucial aspect that has been largely overlooked in previous works. We evaluate SER models in both in-domain and out-of-domain settings, providing valuable insights into their ability to adapt to unseen data distributions. This focus on generalizability is essential for developing SER models that can be effectively deployed in real-world applications, where the variability in speech patterns, emotions, and recording conditions is vast.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "The primary objectives of this section are to detail the dataset selection and preprocessing steps, introduce the backbone models employed, describe the model architecture and training process, explain the logit adjustment technique, and outline the evaluation protocol. The methodology is designed to ensure a comprehensive and fair evaluation of state-of-the-art SER models across diverse languages and emotional expressions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Checkpoint Name",
      "text": "Training Dataset Hours # Params facebook/w2v-bert-2.0  [35, 36]  4500k 580M facebook/hubert-large-ll60k  [2]  60k 315M microsoft/wavlm-large  [3]  94k 315M laion/larger clap music and speech  [7, 8]  >10k 193M m-a-p/MERT-v1-330M  [7]  160k 315M openai/whisper-medium  [6]  680k 307M openai/whisper-large-v2  [6]  680k 636M openai/whisper-large-v3  [6]  5000k 636M openai/whisper-large  [6]  680k 636M",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Selection And Preprocessing",
      "text": "We curate a diverse collection of multilingual datasets for our benchmark, covering various languages and emotional expressions. Table  1  provides an overview of the datasets used in our evaluation. We focus on less commonly used datasets to mitigate overfitting and encourage the development of more robust models.\n\nThe datasets are preprocessed to ensure consistency and compatibility with our evaluation protocol. We set the maximum audio length to 30 seconds, and process the audios with appropriately for each backbone model we test (detailed in the next section). We rely on the Huggingface library for model preprocessing and inference implementations. Additionally, we remap the label space by mapping the original emotion labels to a unified eight-class space, facilitating cross-dataset comparisons. Due to complexity, detailing the exact remapping for each dataset is relegated to the open-source code.\n\nThe datasets used for out-of-domain evaluations are matched by having the same classes (excluding 'other') and their eligibility is indicated in the 'OOD Eligible' column of Table 1. These datasets were found to have the same exact classes after the class mapping, making them eligible for out-of-domain testing. When calculating out-of-domain metrics, samples with the 'other' label were discarded, and models were banned from predicting the 'other' class.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Backbone Models",
      "text": "We employ state-of-the-art speech representation models as backbones for our benchmark, as listed in Table  2 . These models are selected based on their strong performance on various speech processing tasks and their ability to capture rich acoustic features.\n\nIn addition to the SSL models, we also evaluate MERT  [37] , a music recognition model, and CLAP  [38, 8] , a contrastive learning model. Including these models allows us to assess the effectiveness of different learning paradigms for crosslingual SER. Lastly, we evaluate the Whisper  [6]  encoder which is trained under an encoder-decoder setup for ASR.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture And Training",
      "text": "We employ a simple multilayer perceptron (MLP) architecture with approximately 500K parameters for emotion classification. The MLP consists of two hidden layers and is trained for 100 epochs. Due to the small parameter size and shallow depth, we do not expect substantial overfitting. We apply label smoothing with a factor of 0.1 to improve generalization.\n\nInstead of the typical approach of averaging the features before classification, we execute the MLP on every feature frame and then take the mean of the predictions. We find that this approach preserves more information and leads to stronger and more consistent results.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Logit Adjustment",
      "text": "To account for the varying class distributions across datasets, we employ logit adjustment during evaluation. This technique adjusts the model's output logits based on the difference between the training and testing dataset distributions, mitigating the impact of class imbalance and enabling fair comparisons.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Protocol",
      "text": "Figure  1  provides an overview of our benchmark's methodology. As we described in Subsection 3.1, we establish a subset of our datasets as OOD eligible, which have the same exact classes after the class mapping. Effectively, all datasets are accounted in in-domain tests. Only OOD-eligible datasets are accounted for our out of domain metrics.\n\nFor each model, we construct a performance matrix where the rows represent the training datasets and the columns represent the evaluation datasets. When the training and evaluation datasets are the same (diagonal elements), it indicates indomain performance. Off-diagonal elements correspond to outof-domain zero-shot performance.\n\nWe assess the quality of the backbone models based on three key metrics: 1. In-domain separability: We compute the mean of the diagonal elements to measure how well the features learned by a model can separate emotions within a dataset. 2. Out-of-domain performance given training dataset: We calculate the mean of each row, excluding the diagonal element, to evaluate the model's ability to generalize to unseen datasets when trained on a specific dataset. 3. Average performance on unseen datasets: We compute the mean of each column, excluding the diagonal element, to assess the average performance on a dataset when the model is not trained on it. All metrics are reported in terms of macro-averaged F1 score to account for class imbalance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "The results of our benchmark provide valuable insights into the performance and generalization capabilities of state-of-the-art SER models across diverse languages and emotional expressions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "In-Domain Separability",
      "text": "The second and third column in Table  3  present the in-domain separability performance of various models, focusing on their ability to distinguish between different emotional states in speech. Performance is quantified by two metrics: the mean average performance across datasets (Mean) and the variability of performance across these datasets (Standard Deviation). From the table, Whisper-Large-v2 leads the evaluated models in in-domain SER performance, with the highest mean accuracy and low variability across datasets, closely followed by the original Whisper-Large. Other models like Whisper-Large-v3, Whisper-medium, WavLM-Large, and CLAP Music & Speech show competent but slightly more variable performances. Conversely, Hubert Large, MERT v1 330M, and w2v-bert-2.0 exhibit the lowest accuracies with higher fluctuations in their effectiveness across different datasets, indicating potential limitations in generalization capabilities for speech emotion contexts.\n\nThe outcome of this evaluation highlights a clear hierarchy among the models in terms of both accuracy and consistency in emotion recognition within the same domain. Whisper-Large variants stand out as the most effective, with their newer versions, particularly Whisper-Large-v2, slightly improving upon the original's already high benchmark. Lower-ranked models, though less consistent and accurate overall, may still offer valuable insights or perform well in specific niches or datasets. This analysis underscores the importance of choosing the right model for specific SER applications, balancing between performance and consistency across diverse emotional speech datasets.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Out-Of-Domain Performance Given Training Dataset",
      "text": "Figure  2  shows the average out-of-domain performance for each model, obtained by row-wise reduction of the performance matrix. The Whisper models demonstrate the highest out-of-domain performance, indicating their superior generalization capabilities compared to the SSL models. However, there is high variability in OOD performance across training sets. Training on some datasets like BAUM leads to much better OOD generalization than others like MELD. This warrants further investigation into what properties of datasets lead to more generalizable models. The strong performance of Whisper challenges the common belief that ASR models are suboptimal for SER and highlights the potential of leveraging ASR models for emotion recognition tasks.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Average Performance On Unseen Datasets",
      "text": "Figure  3  presents the average performance of the evaluated models on each dataset when the models are not trained on that dataset. The results highlight the varying levels of difficulty across datasets, with some datasets posing greater challenges for out-of-domain generalization. Notably, EMOVO, MELD, and MEAD are the most challenging for models not trained on them, suggesting they have unique characteristics that are harder to learn indirectly. On the other hand, models generalize best to URDU and AESDD, indicating these datasets share more common features with others. Interestingly, the Whisper model consistently achieves strong performance across most  datasets, surpassing the SSL models in many cases.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "General Outcomes",
      "text": "Table  3  provides a summary of the key performance metrics for the evaluated models. The second and third columns show the average and standard deviations of the in-domain results, while the next two columns show the out-of-domain performance. The weighted average column is calculated as follows:\n\nwhere λ f actor is the discounting factor, which we set to 1.0. The Whisper models consistently achieve the highest scores across all metrics, further confirming their effectiveness in cross-lingual SER. However, the high standard deviations indicate that performance is quite variable depending on the specific train/test combination. This suggests that model robustness is still a challenge and there is room for improvement in developing models that perform consistently well across diverse datasets.\n\nOur benchmark also demonstrates the effectiveness of logit adjustment in addressing the challenges posed by varying class distributions across datasets. By incorporating this technique, we ensure fair comparisons and mitigate the impact of class imbalance on model performance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced a comprehensive benchmark for evaluating the robustness and generalization of speech emotion recognition models across diverse languages and emotional expressions. Our benchmark focuses on less commonly used datasets to mitigate overfitting and encourage the development of more robust models. Through extensive experiments with state-of-the-art speech representation models, we found that the Whisper model, primarily designed for automatic speech recognition, outperforms dedicated SSL models in cross-lingual SER. This finding challenges the common belief that ASR models are suboptimal for SER and highlights the potential of leveraging ASR models for emotion recognition tasks.\n\nOur benchmark, along with the released code and evaluation protocol, serves as a valuable resource for the research community to assess and advance the state of cross-lingual SER. The insights gained from our work can guide future research efforts in developing more robust and generalizable SER models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Future Works",
      "text": "Future directions include exploring advanced techniques for domain adaptation, few-shot learning, and meta-learning to further improve the generalization capabilities of SER models. Additionally, investigating the specific characteristics of datasets that contribute to better generalization can provide valuable insights for dataset design and selection.\n\nWe hope that our benchmark and findings will inspire researchers to push the boundaries of cross-lingual SER and develop models that can effectively handle the diversity of languages and emotional expressions encountered in real-world applications.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our benchmark’s methodology.",
      "page": 2
    },
    {
      "caption": "Figure 1: provides an overview of our benchmark’s methodol-",
      "page": 3
    },
    {
      "caption": "Figure 2: Average out-of-domain performance given the train-",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the average out-of-domain performance for",
      "page": 3
    },
    {
      "caption": "Figure 3: presents the average performance of the evaluated",
      "page": 3
    },
    {
      "caption": "Figure 3: Average performance on individual datasets when not",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": "In-domain\n(20% split)"
        },
        {
          "EMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": "Not Eligible!"
        },
        {
          "EMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": "Out-of-domain\n(Eligible)"
        },
        {
          "EMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": "..."
        },
        {
          "EMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": "Out-of-domain\n(Eligible)"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "EmoDB ✗\nCaFE ✓\nSUBESCO ✓\nEMOVO ✓\n...\nSER Evals Methodology Overview\nTested on\nEMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": ""
        },
        {
          "EmoDB ✗\nCaFE ✓\nSUBESCO ✓\nEMOVO ✓\n...\nSER Evals Methodology Overview\nTested on\nEMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": ""
        },
        {
          "EmoDB ✗\nCaFE ✓\nSUBESCO ✓\nEMOVO ✓\n...\nSER Evals Methodology Overview\nTested on\nEMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": ""
        },
        {
          "EmoDB ✗\nCaFE ✓\nSUBESCO ✓\nEMOVO ✓\n...\nSER Evals Methodology Overview\nTested on\nEMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": ""
        },
        {
          "EmoDB ✗\nCaFE ✓\nSUBESCO ✓\nEMOVO ✓\n...\nSER Evals Methodology Overview\nTested on\nEMOVO ✓\nEmoDB ✗\nCaFE ✓                                                     \nSUBESCO ✓\n...": ""
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Wavlm: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition from speech using wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "6",
      "title": "Towards generalizable ser: Soft labeling and data augmentation for modeling temporal emotion shifts in large-scale multilingual speech",
      "authors": [
        "M Osman",
        "T Nadeem",
        "G Khoriba"
      ],
      "year": "2023",
      "venue": "Towards generalizable ser: Soft labeling and data augmentation for modeling temporal emotion shifts in large-scale multilingual speech",
      "arxiv": "arXiv:2311.08607"
    },
    {
      "citation_id": "7",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "8",
      "title": "Efficient selfsupervised learning with contextualized target representations for vision, speech and language",
      "authors": [
        "A Baevski",
        "A Babu",
        "W.-N Hsu",
        "M Auli"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "9",
      "title": "Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation",
      "authors": [
        "Y Wu",
        "K Chen",
        "T Zhang",
        "Y Hui",
        "T Berg-Kirkpatrick",
        "S Dubnov"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Long-tail learning via logit adjustment",
      "authors": [
        "A Menon",
        "S Jayasumana",
        "A Rawat",
        "H Jain",
        "A Veit",
        "S Kumar"
      ],
      "year": "2020",
      "venue": "Long-tail learning via logit adjustment",
      "arxiv": "arXiv:2007.07314"
    },
    {
      "citation_id": "11",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International conference on frontiers of information technology (FIT)"
    },
    {
      "citation_id": "12",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "13",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "14",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "22nd international conference on data engineering workshops (ICDEW'06)"
    },
    {
      "citation_id": "15",
      "title": "The mexican emotional speech database (mesd): elaboration and assessment based on machine learning",
      "authors": [
        "M Duville",
        "L Alonso-Valerdi",
        "D Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "16",
      "title": "Masc: A speech corpus in mandarin for emotion analysis and affective speaker recognition,\" in 2006 IEEE Odyssey-the speaker and language recognition workshop",
      "authors": [
        "T Wu",
        "Y Yang",
        "Z Wu",
        "D Li"
      ],
      "year": "2006",
      "venue": "Masc: A speech corpus in mandarin for emotion analysis and affective speaker recognition,\" in 2006 IEEE Odyssey-the speaker and language recognition workshop"
    },
    {
      "citation_id": "17",
      "title": "Demos: An italian emotional speech corpus: Elicitation methods, machine learning, and perception",
      "authors": [
        "E Parada-Cabaleiro",
        "G Costantini",
        "A Batliner",
        "M Schmitt",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "18",
      "title": "Design of speech corpus for Mandarin text to speech",
      "authors": [
        "J Tao",
        "F Liu",
        "M Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge Workshop"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "20",
      "title": "Baum-1: A spontaneous audio-visual face database of affective and mental states",
      "authors": [
        "S Zhalehpour",
        "O Onder",
        "Z Akhtar",
        "C Erdem"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "21",
      "title": "Estonian emotional speech corpus: culture and age in selecting corpus testers",
      "authors": [
        "R Altrov",
        "H Pajupuu"
      ],
      "year": "2010",
      "venue": "Human Language Technologies-The Baltic Perspective"
    },
    {
      "citation_id": "22",
      "title": "Please use it to make the world a better place for whole humankind",
      "authors": [
        "T Müller",
        "D Kreutz"
      ],
      "year": "2021",
      "venue": "Please use it to make the world a better place for whole humankind",
      "doi": "10.5281/zenodo.5525342"
    },
    {
      "citation_id": "23",
      "title": "Aniemore",
      "authors": [
        "I Lubenets",
        "N Davidchuk",
        "A Amentes"
      ],
      "venue": "Aniemore"
    },
    {
      "citation_id": "24",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "25",
      "title": "MEAD: A large-scale audio-visual dataset for emotional talking-face generation",
      "authors": [
        "K Wang",
        "Q Wu",
        "L Song",
        "Z Yang",
        "W Wu",
        "C Qian",
        "R He",
        "Y Qiao",
        "C Loy"
      ],
      "year": "2020",
      "venue": "Proc. ECCV"
    },
    {
      "citation_id": "26",
      "title": "A Canadian French emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "27",
      "title": "Expresso: A benchmark and analysis of discrete expressive speech resynthesis",
      "authors": [
        "T Nguyen",
        "W.-N Hsu",
        "A Avirro",
        "B Shi",
        "I Gat",
        "M Fazel-Zarani",
        "T Remez",
        "J Copet",
        "G Synnaeve",
        "M Hassid"
      ],
      "year": "2023",
      "venue": "Expresso: A benchmark and analysis of discrete expressive speech resynthesis",
      "arxiv": "arXiv:2308.05725"
    },
    {
      "citation_id": "28",
      "title": "ShEMO: a large-scale validated database for Persian speech emotion detection",
      "authors": [
        "O Nezami",
        "P Lou",
        "M Karami"
      ],
      "year": "2019",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "29",
      "title": "SUST Bangla emotional speech corpus (SUBESCO): An audio-only emotional speech corpus for Bangla",
      "authors": [
        "S Sultana",
        "M Rahman",
        "M Selim"
      ],
      "year": "2021",
      "venue": "Proc. PloS One"
    },
    {
      "citation_id": "30",
      "title": "Superb: Speech processing universal performance benchmark",
      "authors": [
        "S.-W Yang",
        "P.-H Chi",
        "Y.-S Chuang",
        "C.-I Lai",
        "K Lakhotia",
        "Y Lin",
        "A Liu",
        "J Shi",
        "X Chang",
        "G.-T Lin"
      ],
      "year": "2021",
      "venue": "Superb: Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "31",
      "title": "Semi-supervised crosslingual speech emotion recognition",
      "authors": [
        "M Agarla",
        "S Bianco",
        "L Celona",
        "P Napoletano",
        "A Petrovsky",
        "F Piccoli",
        "R Schettini",
        "I Shanin"
      ],
      "year": "2024",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "32",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "33",
      "title": "Building naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Emo-superb: An in-depth look at speech emotion recognition",
      "authors": [
        "H Wu",
        "H.-C Chou",
        "K.-W Chang",
        "L Goncalves",
        "J Du",
        "J.-S Jang",
        "C.-C Lee",
        "H.-Y Lee"
      ],
      "year": "2024",
      "venue": "Emo-superb: An in-depth look at speech emotion recognition",
      "arxiv": "arXiv:2402.13018"
    },
    {
      "citation_id": "35",
      "title": "Serab: A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "N Scheidwasser-Clow",
        "M Kegler",
        "P Beckmann",
        "M Cernak"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training",
      "authors": [
        "Y.-A Chung",
        "Y Zhang",
        "W Han",
        "C.-C Chiu",
        "J Qin",
        "R Pang",
        "Y Wu"
      ],
      "year": "2021",
      "venue": "2021 IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "37",
      "title": "Seamless: Multilingual expressive and streaming speech translation",
      "authors": [
        "L Barrault",
        "Y.-A Chung",
        "M Meglioli",
        "D Dale",
        "N Dong",
        "M Duppenthaler",
        "P.-A Duquenne",
        "B Ellis",
        "H Elsahar",
        "J Haaheim"
      ],
      "year": "2023",
      "venue": "Seamless: Multilingual expressive and streaming speech translation",
      "arxiv": "arXiv:2312.05187"
    },
    {
      "citation_id": "38",
      "title": "Mert: Acoustic music understanding model with large-scale self-supervised training",
      "authors": [
        "Y Li",
        "R Yuan",
        "G Zhang",
        "Y Ma",
        "X Chen",
        "H Yin",
        "C Lin",
        "A Ragni",
        "E Benetos",
        "N Gyenge"
      ],
      "year": "2023",
      "venue": "Mert: Acoustic music understanding model with large-scale self-supervised training",
      "arxiv": "arXiv:2306.00107"
    },
    {
      "citation_id": "39",
      "title": "Clap learning audio concepts from natural language supervision",
      "authors": [
        "B Elizalde",
        "S Deshmukh",
        "M Ismail",
        "H Wang"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}