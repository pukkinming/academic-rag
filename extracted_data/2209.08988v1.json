{
  "paper_id": "2209.08988v1",
  "title": "Msa-Gcn:Multiscale Adaptive Graph Convolution Network For Gait Emotion Recognition",
  "published": "2022-09-19T13:07:16Z",
  "authors": [
    "Yunfei Yin",
    "Li Jing",
    "Faliang Huang",
    "Guangchao Yang",
    "Zhuowei Wang"
  ],
  "keywords": [
    "Emotion recognition",
    "Gait emotion recognition",
    "Graph convolutional network",
    "Multiscale mapping"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Gait emotion recognition plays a crucial role in the intelligent system. Most of the existing methods recognize emotions by focusing on local actions over time. However, they ignore that the effective distances of different emotions in the time domain are different, and the local actions during walking are quite similar. Thus, emotions should be represented by global states instead of indirect local actions. To address these issues, a novel MultiScale Adaptive Graph Convolution Network (MSA-GCN) is presented in this work through constructing dynamic temporal receptive fields and designing multiscale information aggregation to recognize emotions. In our model, a adaptive selective spatial-temporal graph convolution is designed to select the convolution kernel dynamically to obtain the soft spatiotemporal features of different emotions. Moreover, a Cross-Scale mapping Fusion Mechanism (CSFM) is designed to construct an adaptive adjacency matrix to enhance information interaction and reduce redundancy. Compared with previous state-of-the-art methods, the proposed method achieves the best performance on two public datasets, improving the mAP by 2%. We also conduct extensive ablations studies to show the effectiveness of different components in our methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "E MOTION recognition plays an important role in the field of human-computer interaction  [1]  with applications in various fields, including behavior prediction  [2] , video surveillance  [3] , character generation  [4] , etc. There are many cues of emotion recognition, such as speeches  [5] ,  [6] , texts  [7] , facial expressions  [8] ,  [9] ,  [10] , gestures  [11] , and gaits  [12] . In the above mentioned cues, gait is defined as an ordered time sequence of joint transformations during a single walking cycle. Compared with other verbal and non-verbal cues, it can be collected from a long distance without human cooperration. And gait is also not easily imitated or deliberately forged  [13] . Some psychological works show that individuals in different emotional states may have differences in some gait characteristics, such as arm swing, speed, angle, stride length, etc.  [14] ,  [15] ,  [16] . Therefore, gait emotion recognition is a promising research field.\n\nAccording to the feature extraction schemes, current mainstream gait emotion recognition methods can be roughly divided into two categories: hand-crafted methods and deep learning methods. The hand-crafted method perceives emotions by calculating walking speed, joint amplitude increase, chest flexion, etc.  [17] . However, the features obtained by hand-crafted methods usually need to be designed according to the characteristics of the data, and thus, only work well for a certain data set. When the data source changes, the features need to be redesigned, which are very inflexible and inefficient. Deep learning based methods extract emotional features through CNN  [1] ,  [18] , LSTM  [12] ,  [19]  or GCN  [20] ,  [21] . Because the human skeleton is naturally a graph in non-Euclidean space and 3D joint points are directly modeled by GCN, using GCN for gait emotion recognition is the most commonly used method. However, some experts reveal that GCN based deep learning methods can not model comprehensively enough emotional information  [21] , which in turn leads to an overall limitation of their performance in gait emotion recognition tasks. Consequently, how to solve the shortcomings of existing feature representation models is still an active area of research.\n\nTo provide a visual illustration, a skeleton diagram of angry and sad people with frames 1, 5, 10, 15, 20, 25 is shown in Figure  1 . There is a phenomenon that the two people express different emotions while walking, but the local movements of them are quite similar. For example, from the lower limb movements of the angry person in frame 1 and the sad person in frame 5 marked by the red box, the two people express different emotions, while the movements in the local joints are similar. This means that emotions can not be identified by relying only on local features. However, the existing GCN methods extract spatial information only by modeling local relationships, ignoring the overall state of the gait is the key to identifying emotions, which will increase unnecessary and redundant information.\n\nIn addition, a more prominent and discriminative emotional representation for the anger is the swing amplitude of the shoulders and arms in the last three frames from the yellow box. A more pronounced and discriminative emotional expression for the sad is the forward tilt of the back and the collapse of the spine in the last four frames. It proves that different emotions have different effective ranges in the time domain. But, the existing GCN method will weaken the effectively important information in the time domain by using the same receptive field caused by the same time convolution kernel, which further leads to an overall performance degradation.\n\nTo address the above two challenges of GCN, we propose a MultiScale Adaptive Graph Convolution Network (MSA-GCN) to obtain more discriminative and robust emotional features to recognize emotion. First, we present an adaptively selected spatio-temporal graph convolution to solve the problem of rigidity in extracting temporal features. Second, we design a new global-local mapping fusion mechanism to obtain discriminative expression of emotion, reduce redundant information and enhance global representation. Specifically, coarse-grained graphs are used to extract overall information. Fine-grained graphs are used to extract local information. Then, the scales features at different scales are fused in a mutually instructive way. The main contributions of this paper are as follows:\n\n1) We propose a novel multiscale adaptive graph convolution network to extract multi-scale emotional features and implement efficient perception of discrete emotions such as happy, angry, sad, and neutral. 2) We design two key components, adaptive selective spatial-temporal graph convolution and cross-scale mapping fusion mechanism, to obtain the soft spatiotemporal features, enhance information interaction, and reduce redundancy. 3) We conduct extensive experiments to show that the proposed MSK-GCN outperforms most state-of-the-art methods for gait emotion perception on two public datasets. The rest of this paper is organized as follows. We review previous works related to our proposed model in Section II. Section III describes the proposed model, MSA-GCN, in detail. We present experiment results on two public datasets and conduct detailed discussion in Section IV. Finally, we conclude this paper in Section V.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this section, we briefly review prior work in classifying perceived emotions from gaits, as well as the related task of skeleton-based action recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Skeleton-Based Action Recognition",
      "text": "Skeleton-based action recognition  [22] ,  [23]  is to identify human movements through 3D key points, such as waving, touching the head, etc. It usually obtains effective spatiotemporal features by modeling the spatial and temporal domains of skeleton to identify different actions. In recent years, action recognition has become a hot research direction in the field of computer vision, which is of great significance to applications such as human-computer interaction and intelligent monitoring.\n\nDeep learning-based action recognition methods initially use RNN, LSTM, and other networks to extract human action features. For example, Wang et al.  [24]  used a Fourier temporal pyramid to model the relative position of the skeleton. Liu et al.  [25]  learned the temporal and spatial relationships of skeleton sequences through LSTM and used a gate to remove noise. Although these methods can handle the temporal and spatial information of joint points, they did not fully exploit the natural graph structure of joint points. The graph convolutional network (GCN) that appeared in recent years can make full use of the connection relationship between nodes to model data, which was very suitable for action recognition based on skeleton. Therefore, Yan et al.  [22]  took the lead in introducing graph convolutional network into skeletal action recognition, and proposed a spatio-temporal graph convolutional network (ST-GCN). ST-GCN represented the skeleton sequence as a spatio-temporal graph and used a graph convolutional layer based on the distance of nodes for feature extraction. On the basis of ST-GCN, Zhang et al.  [26]  suggested that the bone could also provide information for the network to learn. They proposed an end-to-end learning network that optimizes the topology of the graph and the network, and introduced bone and joint information as a two-stream model to improve accuracy. Considering both spatial and temporal modeling, Liu et al.  [27]  proposed a unified approach to directly capture complex joint correlations across time and space domains. Zheng et al.  [28]  considered the self-attention mechanism, which can capture the intrinsic correlation of long-term input. They proposed a convolution-free spatio-temporal Transformer network to extract features. Lei et al.  [29]  represented a posebased graph convolutional networks, which use both video data and skeleton data to model.\n\nIn the above mentioned literatures, the method of Liu et al.  [27]  is similar to our method, which also adopts the multiscale method to extract features. The difference is that our method can refine the fine-grained feature learning by coarse grain instead of simple scale fusion. Zheng et al.  [28]  used Transformer to model features, which can capture long-term input. However, compared with our method of using GCN for modeling, the input 3D point data needs to be processed in Transformer first, leading to partial information loss.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Emotion Recognition",
      "text": "Emotions are a series of complex physiological and psychological reactions produced by people's cognition and perception of the outside world  [23] . At present, many researchers have been working on how to make computers capable of observing, understanding and even expressing various emotions like humans  [30] .\n\nFor facial expressions, Zhang et al.  [8]  propoesd a regionbased multiscale network that learns features for the local affective region as well as the broad context for emotion image recognition. Mollahosseini et al.  [31]  combined AlexNet  [32]  and GoogleNet  [33]  to recognize facial expressions.For language and tone, Lee et al.  [34]  used bi-directional long short-term memory (Bi-LSTM) to extract high-level temporal dynamic emotion features. Nie et al.  [5]  considered the correlation of the intra-class and inter-class videos, and proposed a correlation based graph convolutional network for audiovideo emotion recognition. Some scholars also consider using multiple modalities to improve the performance of emotion recognition. For example, Zheng et al.  [35]  proposed a DNNsbased multi-channel weight-sharing autoencoder with Cascade multi-head attention to address the emotional heterogeneity gap in multimodality.\n\nAlthough the accuracy of these cues has reached a high degree, they are not effective in some situations, such as occlusion and scene noise. Gait can be used as an auxiliary emotional cue to achieve better emotional recognition in a wider range of scenes.\n\nIn the early stage of gait emotion recognition, hand-crafted features were mainly used to identify emotion. For example, Li et al.  [36] ,  [37]  used Fourier transform and principal component analysis (PCA) to extract gait features from the 3D coordinates of 14 human body joint points for emotion recognition. Crenn et al.  [38]  used hand-designed gait features, employed support vector machines, and introduced a cost function to generate neutral actions for emotion recognition.\n\nRecently, deep learning has achieved great success in the field of computer vision, and a large number of methods use deep learning for gait emotion perception. Randhavane et al.  [12]  adopted a time sequence-based approach, using an LSTM to extract temporal features, and then combined it with hand-extracted emotional features for classification using a random forest classifier. Bhattacharya et al.  [19]  utilized gated recursive units (GRUs) to extract features from joint coordinates at a single time step and perform temporal analysis on them to identify emotions. Narayanan et al.  [1]  adopted an image-based method to encode skeleton sequences, convert 3D joint point data into images, and then used convolutional neural networks (CNN) to extract features related to emotions in images to identify emotions. Bhattacharya et al.  [39]  adopted a graph-based method to exploit the characteristic that skeletons are naturally graphs in non-Euclidean space. They expressed the relationship of joints through a spatio-temporal graph convolutional network (ST-GCN)  [22] , and extracted spatiotemporal features to classify. Sheng et al.  [20]  adopted an attention module and proposed an attention-enhanced temporal convolutional network (AT-GCN) to capture discriminative features in spatial dependencies and temporal dynamics for sentiment classification. Zhuang et al.  [21]  also adopted a graph-based approach to extract global features by constructing global connections and proposed shrinking blocks to reduce noise to improve classification performance. Similarity, Sheng et al.  [40]  also considered the noise of data, and proposed a multi-task Learning with denoising transformer network to identify emotions. Bhatia et al.  [41]  presented a light architecture based on LSTM to reduce inference time for gait samples. Different from the above-mentioned methods which only model emotion features from a single-scale, the method proposed in this paper is based on multi-scale to obtain richer spatial and semantic information. It is worth noting that our model MSA-GCN is similar to G-GCSN  [21]  proposed by Zhuang et al., which tries to learn better emotion representation by finding global information. However, there is a distinct difference between the two methods. G-GCSN searches for global information by artificially adding edges from the center node to other nodes when constructing the skeleton graph, limiting the expression ability of the network. Our method is more flexible and adaptable because MSA-GCN obtains global information from the constructed coarse-grained maps in a multi-scale manner.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. The Proposed Msa-Gcn",
      "text": "In this section, MSA-GCN is introduced in detail. In Section A, a brief overview of our framework is given. In Section B, the input initialization operation is given. In Sections C and D, the two proposed modules, adaptively selected spatiotemporal graph convolution and cross-scale mapping fusion mechanism, are discussed in detail. Finally, the data flow and the loss function of the model are introduced in Section E.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Framework",
      "text": "The framework of our approach is illustrated in Figure  2 . On a high level, it contains a stack of l adaptively selected spatiotemporal graph convolution (ASST-GCN) blocks and crossscale mapping fusion module (CSFM) bolcks to extract features from skeleton sequences after multi-scale initialization, followed by a fully connected layer and a softmax classifier. ASST-GCN and CSFM are cross-deployed to simultaneously capture complex regional spatio-temporal correlations as well as changing spatial and temporal dependencies. First, skeleton maps of different scales are constructed by initializing 3D skeleton sequences. Second, the ASST-GCN1 module (inside the stacked block) is used to extract spatial and temporal features at different scales. Then, the features of different scales are interacted, and fused through the CSFM module, so that the information of one scale can guide the information of another scale. Next, the feature maps of different scales are fused into one scale through the attention mechanism, and then the ASST-GCN2 module (outside the stacked block) is used for overall spatiotemporal feature extraction. Finally, through the fully connected layer and Softmax to obtain classification results: Happy, sad, angry, or neutral.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Multiscale Initialization",
      "text": "The multi-scale initialization changes the single scale of the input data to produce coarse-grained scale maps. As shown in Figure  3 , the close joints in the fine-grained graph are merged as a joint of the coarse-grained graph, so that the coarsegrained graph can represent semantic information.\n\nThe multiple nodes in the fine-grained graph are averaged to represent a node in the coarse-grained scale graph by using  4 . Adaptive selection of spatial temporal graph convolution modules. ASST-GCN adopts a series structure to extract spatio-temporal features. First, the spatial information is extracted through GCN, and then the temporal information is extracted through the AS-TCN structure in the gray box. TCN1 and TCN2 with different convolution kernels are weighted by global average pooling and softmax to obtain adaptive receptive fields.\n\n2D average pooling, and then concatenate operation is used to obtain the whole graph representation of the coarse-grained graph. The formula of multi-scale initialization is expressed as follows:\n\nwhere V ck represents the joint information of k nodes in the coarse-grained graph, V f h represents the joint information of h nodes in the fine-grained graph, Graph c represents the physical skeleton of the coarse-grained graph.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Adaptively Selected Spatio-Temporal Graph Convolution",
      "text": "ASST-GCN performs spatio-temporal feature extraction from different scales. Considering the extremely complex dynamic characteristics of video data in the temporal dimension, gaits with different emotions present different motion patterns in the temporal dimension. This means the emotion-related features may lack sufficient expressive ability if a convolution kernel is shared for all video data.\n\nThe method of multiple convolution kernels and adaptively select for each person is proposed to aggregate the temporal information. Since each sample has a different receptive field, ASST-GCN can more effectively extract temporal information of different emotions.\n\nASST-GCN consists of two components, graph convolutional neural network (GCN) and Aptively Selected Temporal Convolutional Neural Network (AS-TCN), as shown in Fig-  ure 4 . GCN is a traditional graph convolution operation, which extracts information in the spatial domain. AS-TCN constructs different receptive fields and extracts features at different levels using multiple convolution kernels. The features are extracted from different convolution kernels through global maximum pooling, fully connected layers, and weighted summation to achieve adaptive selection and fusion of features in the time domain. Finally, the residual connection is added to accelerate convergence and alleviate over-smoothing of graph convolution.\n\nIn Figure  4 , the light green square represents the graph convolutional neural network, orange and earth-colored squares represent temporal convolution of two different convolution kernels, GAP represents global average pooling, and FC represents a fully connected layer. The outputs of ASST-GCN are given by Eq. 3 -8.\n\nIn equations 3 and 4, x G represents the features after GCN. U 1 and U 2 represent the output after TCN of different convolution kernels. In equations 5 and 6, V 1 and V 2 represent the results of time convolution after information fusion. In equation 7, V represents the result of AS-TCN. In equation 8, X out represents the output of ASST-GCN.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Cross-Scale Mapping Fusion Mechanism",
      "text": "Information at one scale can guide information at another scale  [42] . On the human gait represented by the skeleton, the coarse-grained arm information guides the fine-grained hand and elbow information. A cross-scale mapping fusion mechanism is proposed to make the cross-scale information diffusion on different scale graphs, so that information of different scales can interact in the process of feature extraction. CSFM can transform features from one scale to another by building an adjacency matrix between the two scales, i.e. the correspondence between different joints in different graphs. For example, 2 node in second graph can be mapped to 4 and 5 node in first graph in Figure  3 .\n\nBy constructing an adjacency matrix between two scales, cross-scale fusion can achieve information interaction at different scales and reduce unnecessary redundancy caused by information fusion. The description of CSFM (Figure  ?? ) is as follows.\n\nFor each scale, after get the output of ASST-GCN, locally spatially enhanced features are obtained through the attention mechanism to more effectively express the features of different emotions. Then, inspired by the FM model in the recommender system  [43] , the inner product of the enhanced features in different scale maps is performed and softmax is used to obtain the correspondence between nodes of different scales, which is the adjacency matrix. Finally, the graph convolution operation is performed to obtain features that fuse other scale information by the adjacency matrix.\n\nOrange and yellow cubes represent features at two scales, green blocks represent spatial attention, orange blocks represent embedding operations, gray blocks represent multilayer perceptron, light green blocks represent ordinary graph convolution operations, and blue cubes represent fusion. Later scale features.\n\nTaking the scale of 16 joints and the scale of 10 joints as an example, we first define the feature maps of the scale of 16 joints and the scale of 10 joints as X A ,X B . Then the correspondence between nodes of scales is the adjacency matrix A XA,XB and the 16-node feature X out after scale fusion is:\n\nf 1 ,f 2 represents spatial attention, g 1 ,g 2 represents multilayer perceptrons (MLPs), ( X B T\n\n• X B ) represents the dot product of X B T and X B , GCN (A XA ,XB , X A )indicates that the fused features are obtained by performing graph convolution operations on the scale of 16 nodes through the adjacency matrix A XA,XB .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Data Flow And Loss Function",
      "text": "The input of the model consists of X : B × C (0) × T × V 16 . B:Batch size, C (0) :number of channels in layer 0, T :time, V 16 :number of joints is 16 (depended by the dataset). After initialization, we get three branches corresponding to three scales, which only the number of joints V is different. and through the l + 1-th layer of ASST-GCN and CSFM, the data is X : B×C (l+1) ×T (l+1) ×V 16 . Finally, the data is X : B×4 through the FC layer and Softmax.\n\nThrough the whole process, the output of MSA-GCN can be given by Eq. 13:\n\nwhere H(x) represents the output of MSA-GCN,f (x) represents the ASST-GCN, g(x) represents the CSFM, att X=n X=1 indicates that adaptive attention fusion for feature map of the 1st to the n-th scale.\n\nWith the output of MSA-GCN H(x), defined in Eq. 13, FC layer and softmax classifer is used to estimate sentiment distribution of the input x, as shown in the following:\n\nwhere p i,k denotes the probability that the i-th sample is predicted to be the k-th label, and i = 0, 1, ..., N , k = 0, 1, ..., K.\n\nFor model training, the cross-entropy between the predicted class probabilities and the ground-truth labels is utilized as the loss function of our model.\n\nwhere q and p denotes ground-truth and predicted sentiment distribution, K the sentiment label set, N the number of samples.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Iv. Experiments And Analysis",
      "text": "In Section A, the datasets and evaluation criteria used in the experiments are given. In Section B, the implementation details and training details of our model are listed. In Section C, the comparison and analysis results with state-of-theart methods for gait emotion recognition and skeleton-based action recognition are presented. In Section D, the results of ablation experiments on each part are listed. In Section E, the visual analysis is given.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets And Evaluation Criterion",
      "text": "We adopt two public datasets to validate the effectiveness of the method. The first dataset is the Emotion-Gait-16 dataset  [44] . It has 16 joints and 240 frame, which consists of 2177 ground-truth gait sequences annotated as happy, sad, angry, and neutral. The second dataset is the Emotion-Gait-21 dataset  [19] . The gait is defined as a skeleton with 21 joints, and the step of the gait sequence is 48. It consists of 1835 groundtruth gait sequences with emotional labels provided by 10 annotators, labeled happy, sad, angry, and neutral.\n\nFor the evaluation criterion, we quantitatively evaluate our experimental results by the classification accuracy, precision, recall and F1-measure given by formulas 16 -19:\n\nP recision = T P T P + F P ,\n\nIn formulas 16 -19, T P ,F P ,T N ,F N represent the number of true positive, false positive, true negative, and false negative of the four emotions, and TD represents the total data number.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Experimental Setup",
      "text": "All experiments are implemented on an NVIDIA Tesla P40 with the Pytorch framework. 8:1:1 split is used for the training set, validation set, and test set, and the loss function adopts the cross-entropy loss function. The main hyperparameters include the network and training phase. For network, when l=1, 2, m, n=1. When l=3, 4, m=1, n=0, 5 and 9 are selected for the temporal convolution kernels in ASST-GCN, the corresponding dimensions are 32, 64, 128, 256. For training, we used a batch size of 16 and the Adam optimizer for 400 epochs with an initial learning rate of 0.001. After 200, 300, and 350 epochs, the learning rate decays to one-tenth of its current value. We also used a momentum of 0.9 and a weight decay of 5 × 10 -4 .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Analysis Of Results",
      "text": "MSA-GCN is compared with several state-of-the-art methods of gait emotion recognition. The same dataset partitioning and evaluation method is used for training.\n\nEmotion-Gait-16. The results of comparison with stateof-the-art gait emotion recognition are shown in Table  I . The methods used for comparison include sequence-based methods  [12] ,  [19] , image-based methods  [1] , and graph-based methods  [39] ,  [21] . It can be observed that our method has the best results on Happy, Angry, Neutral, and average accuracy, improving the accuracy by around 0.02-0.05, and the accuracy gap with the best effect in Sad is only about 0.01, which proves that our method is effective. In precision, recall, and F1, our method has a large improvement in precision, recall, and F1, and the improvement is all about 0.1, which means that the classification ability of our method on positive samples is excellent, further proving that the higher accuracy is not due to the imbalanced dataset.\n\nWe also compare our method with action recognition to verify that we focus more on the mapping of gait to a set of emotional labels rather than the mapping of actions as shown in Table  II . Our method achieves the best results on both Happy and Angry, which is about higher than the other best results. Although it does not achieve the best results on Sad and Neutral, the accuracy is also high. Neutral is also only about 0.005 lower than the best method. Our average accuracy is also the highest among all methods, which proves that our method finds more emotion-focused mapping than modeling with action recognition. In precision, recall, and F1, our method has good results and all reaching above 0.8. Especially compared with 2AS-GCN  [26] , although this method has the highest accuracy in the Sad class in Table  II , and mAP is not much lower than our method. So, the classification ability of our method is better, and it is less affected by imbalanced data than 2AS-GCN. It fully demonstrates the superiority of our method.\n\nEmotion-Gait-21. The results of comparison with state-ofthe-art gait emotion recognition are shown in Table  III . Our proposed method has the best results on sad, Angry, Neutral, and average accuracy, improving the accuracy by around 0.02-0.03. Our method has a large improvement in precision, recall and, F1, and the improvement is about 0.1, which means that the classification ability of our method on positive samples is excellent. From the Table  III , the accuracy of LSTM (VAnilla)  [12]  is higher than our method in happy, but F1 is about 0.17 less than our method. The high accuracy of happy maybe since this method prefers to classify samples as happy, rather than classifying samples labeled as happy.\n\nTable  IV  presents the results of comparison of our method with the methods for action recognition. Our method achieves the best results on both Sad and Angry. While on Happy and Neutral, it does not get the best results, but the accuracy is only about 0.002 lower than the best method on Neutral. Our average accuracy is also the highest among all methods, which proves that our method finds more emotion-focused mapping than modeling with action recognition. Our method has the best results in accuracy, and F1, all reaching above 0.78. Although the recall rate does not reach the highest, it is also 0.006 worse than the highest. The 2AS-GCN  [26]  method\n\nThe label Predicted label and the PoseFormer  [28]  method are higher than our method in single-class accuracy but far lower than our values in precision, recall, and F1, This shows that these two methods only have a stronger preference for the sentiment of that category, and tend to classify the samples that are not of this category into this category, resulting in high accuracy of this category, rather than learning the classification ability. Therefore, our method has better classification ability in positive samples, which fully demonstrates the superiority of our method.\n\nFrom Table  I  -IV, mAP is not high on both datasets, which may be because the actions of different emotions in local joints are extremely similar. So, the improved accuracy of our method confirms that it is necessary to consider the global information. The confusion matrix is given to show the discriminatory ability of MSK-GCN in Figure  5 . It can be seen that the accuracy of our classifier in each class is greater than 80%, which means that the classifier can identify each class equally well.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Ablation Study",
      "text": "We perform ablation experiments on our method to highlight the benefit of some crucial elements.\n\nEffects of multiple scales. To verify the proposed multiscale representation, we employ various scales in MSA-GCN. Besides the three scales in our model, we introduce additional one scale, which represents a body as 3 parts: upper limbs, lower limbs and torso. Table  V  presents the accuracy of various scales. As you can see, two or more scales are most of the time preferable to only using a single scale (1). More scale does not mean better performance. You can see that when combining the four scales (1, 2, 3, 4), the accuracy is not higher than that of a single scale  (1) . Too high scale will not only bring redundant information and affect the accuracy, but also increase the training time. In addition, the model accuracy reaches the best when three scales (1, 2, 3) are used. without CSFM module and adaptive attention fusion module (ATT). Compared with the backbone network without the CSFM, the model recognition performance with CSFM is significantly improved. And ATT (i.e., spatial attention and channel attention) adopted in the fusion process proves to be useful for emotion recognition.\n\nIt can be seen that the CSFM works best with two layers, which proves that the cross-scale interactive fusion proposed by us is effective. When the level is greater than 2, the accuracy begins to decrease, which may be due to the number of fusion layers being too high, the features tend to be homogenized, and the discriminative emotional features are lost. Effects of ASST-GCN. To verify the effectiveness of ASST-GCN framework, ASST-GCN was compared with traditional ST-GCN  [22]  (with a single temporal convolution TCN) and ASST-GCN without TCN, as shown in Table  VII . It can be seen from the results without and with TCN that the temporal convolution has a great impact on the accuracy. Compared with the traditional ST-GCN with only a single TCN, our method provides a huge improvement in accuracy. This proves the effectiveness of our proposed framework. Effects of SA-TCN. The ablation results of AS-TCN are shown in Table  VIII . The baseline is the ST-GCN network with a single convolution kernel 5 and a multi-scale graph (joints 16, 10, and 5). Compared with the baseline without Adaptively selected temporal graph convolution (AS-TCN), the recognition performance of models with adaptive selection is improved, which means that our proposed AS-TCN is effective. The convolution kernel has the highest performance when selecting 5 and 9. For some convolution kernels with large differences, such as 5 and 75, the effect is not significant. This may be due to the receptive field of the convolution kernel being too large, and invalid information is extracted instead.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "E. Visual Analysis",
      "text": "To demonstrate the robustness and limitation of the proposed method, a visualization of the correct classification of MSA-GCN and the wrong classification of G-GCSN (the best gait emotion recognition method other than ours in the experiment) under the same sample (top) and a visualization of the wrong of MSA-GCN (bottom) are shown in Figure  6 .\n\nComparing between MSK-GCN and G-GCSN (top), there are fewer samples classified correctly by G-GCSN and wrong by MSK-GCN, so only the samples classified correctly by MSA-GCN and wrongly classified by G-GCSN are listed. The samples given in the upper part have an obvious feature. The collected samples are not facing the camera directly, but have an oblique angle (except for the first sample). Such samples are much more difficult to classify than normal front-facing samples. Because the joint points are more likely to overlap and obscure the important emotional representation of the shoulder. Comparing the sad and angry samples with the same left-facing side, the lower limb movements of the two samples are basically the same, and the upper limbs tend to be the same due to the overlapping joint points of walking, which may be the reason why G-GCSN classifies the angry samples as the label sad. This also proves that we mentioned in the introduction that when the actions of local joints are similar, and if only focusing on the relationship of local joints may confuse the conjecture of the classification results.\n\nFor the analysis of MSA-GCN misclassification (bottom), the misclassified samples is selected from the class that is more prone to misclassification, which is more conducive to the analysis of why the classification error occurs. It can be seen that when misclassified, the samples tend to be classified as happy, sad, and neutral. Angry was less likely to be classified, it should be that the limbs swayed more and faster when angry. For the four samples that were misclassified, the reason for the misclassification may be that the gait of these samples expressing emotions is relatively restrained, and the reaction is that the degree of posture collapse and the amplitude of hand swing are small on the skeleton, so few discriminant features are extracted by the network, leading to classification is incorrect.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this work, we proposed the multiscale adaptive graph convolution network for gait-based emotion recognition. Considering that the emotion of gait is presented by the overall state and the emotion information of every sample in time domain is different, we obtain emotional mapping through a multiscale adaptive graoh convolution network we proposed. We conducted extensive experiments on two public datasets. As a result, we showed the effectiveness of our module and showed that the model in which our module is implemented achieves state-ofthe-art performance compared with previous state-of-the-art methods on both of them. In the future, we will extend our method to noisy label learning  [45]  and federated learning  [46]  with gait-based emotion recognition.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Gait of people with different emotions at different frames. Red box:",
      "page": 1
    },
    {
      "caption": "Figure 1: There is a phenomenon that the two people express",
      "page": 1
    },
    {
      "caption": "Figure 2: The architecture of MSK-GCN, which mainly uses adaptive selection ST-GCN and cross-scale mapping fusion module to classify emotion.(To better",
      "page": 3
    },
    {
      "caption": "Figure 3: Multi-scale initialization mapping relationship. The three scales consist",
      "page": 4
    },
    {
      "caption": "Figure 3: , the close joints in the ﬁne-grained graph are merged",
      "page": 4
    },
    {
      "caption": "Figure 4: Adaptive selection of spatial temporal graph convolution modules.",
      "page": 4
    },
    {
      "caption": "Figure 4: , the light green square represents the graph con-",
      "page": 4
    },
    {
      "caption": "Figure 3: By constructing an adjacency matrix between two scales,",
      "page": 5
    },
    {
      "caption": "Figure 5: Confusion matrix for MSA-GCN. MSA-GCN has > 80% accuracy",
      "page": 7
    },
    {
      "caption": "Figure 5: It can be seen",
      "page": 7
    },
    {
      "caption": "Figure 6: Comparing between MSK-GCN and G-GCSN (top), there",
      "page": 8
    },
    {
      "caption": "Figure 6: Visual result analysis. The top row shows 4 gaits from the Emotion-Gait dataset where the predicted labels of our network exactly matched the",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "touching the head, etc.\nIt usually obtains effective spatiotem-"
        },
        {
          "2": "poral\nfeatures by modeling the spatial and temporal domains"
        },
        {
          "2": "of skeleton to identify different actions. In recent years, action"
        },
        {
          "2": "recognition has become a hot research direction in the ﬁeld of"
        },
        {
          "2": "computer vision, which is of great signiﬁcance to applications"
        },
        {
          "2": "such as human-computer\ninteraction and intelligent monitor-"
        },
        {
          "2": "ing."
        },
        {
          "2": "Deep\nlearning-based\naction\nrecognition methods\ninitially"
        },
        {
          "2": "use RNN, LSTM, and other networks to extract human action"
        },
        {
          "2": "features. For example, Wang et al. [24] used a Fourier temporal"
        },
        {
          "2": "pyramid to model\nthe\nrelative position of\nthe\nskeleton. Liu"
        },
        {
          "2": "et\nal.\n[25]\nlearned the\ntemporal and spatial\nrelationships of"
        },
        {
          "2": "skeleton sequences through LSTM and used a gate to remove"
        },
        {
          "2": "noise. Although these methods can handle the temporal and"
        },
        {
          "2": "spatial information of joint points, they did not fully exploit the"
        },
        {
          "2": "natural graph structure of joint points. The graph convolutional"
        },
        {
          "2": "network (GCN)\nthat appeared in recent years can make full"
        },
        {
          "2": "use of\nthe\nconnection relationship between nodes\nto model"
        },
        {
          "2": "data, which was very suitable for action recognition based on"
        },
        {
          "2": "skeleton. Therefore, Yan et al. [22] took the lead in introducing"
        },
        {
          "2": "graph convolutional network into skeletal action recognition,"
        },
        {
          "2": "and proposed a spatio-temporal graph convolutional network"
        },
        {
          "2": "(ST-GCN). ST-GCN represented the\nskeleton sequence as a"
        },
        {
          "2": "spatio-temporal graph and used a graph convolutional\nlayer"
        },
        {
          "2": "based\non\nthe\ndistance\nof\nnodes\nfor\nfeature\nextraction. On"
        },
        {
          "2": "the basis of ST-GCN, Zhang et\nal.\n[26]\nsuggested that\nthe"
        },
        {
          "2": "bone could also provide information for the network to learn."
        },
        {
          "2": "They proposed an end-to-end learning network that optimizes"
        },
        {
          "2": "the\ntopology of\nthe graph and the network, and introduced"
        },
        {
          "2": ""
        },
        {
          "2": "bone and joint\ninformation as a two-stream model\nto improve"
        },
        {
          "2": ""
        },
        {
          "2": "accuracy. Considering both spatial and temporal modeling, Liu"
        },
        {
          "2": ""
        },
        {
          "2": "et\nal.\n[27]\nproposed a uniﬁed approach to directly capture"
        },
        {
          "2": ""
        },
        {
          "2": "complex joint\ncorrelations\nacross\ntime\nand\nspace\ndomains."
        },
        {
          "2": ""
        },
        {
          "2": "Zheng et\nal.\n[28]\nconsidered the\nself-attention mechanism,"
        },
        {
          "2": ""
        },
        {
          "2": "which can capture the intrinsic correlation of long-term input."
        },
        {
          "2": ""
        },
        {
          "2": "They proposed a convolution-free spatio-temporal Transformer"
        },
        {
          "2": ""
        },
        {
          "2": "network to extract features. Lei et al. [29] represented a pose-"
        },
        {
          "2": ""
        },
        {
          "2": "based graph convolutional networks, which use both video data"
        },
        {
          "2": ""
        },
        {
          "2": "and skeleton data to model."
        },
        {
          "2": ""
        },
        {
          "2": "In the above mentioned literatures,\nthe method of Liu et al."
        },
        {
          "2": ""
        },
        {
          "2": "[27]\nis\nsimilar\nto our method, which also adopts\nthe multi-"
        },
        {
          "2": ""
        },
        {
          "2": "scale method to extract\nfeatures. The difference\nis\nthat our"
        },
        {
          "2": ""
        },
        {
          "2": "method can reﬁne the ﬁne-grained feature learning by coarse"
        },
        {
          "2": ""
        },
        {
          "2": "grain instead of\nsimple scale fusion. Zheng et al.\n[28] used"
        },
        {
          "2": ""
        },
        {
          "2": "Transformer\nto model\nfeatures, which can capture long-term"
        },
        {
          "2": ""
        },
        {
          "2": "input. However, compared with our method of using GCN for"
        },
        {
          "2": ""
        },
        {
          "2": "modeling,\nthe input 3D point data needs\nto be processed in"
        },
        {
          "2": ""
        },
        {
          "2": "Transformer ﬁrst,\nleading to partial\ninformation loss."
        },
        {
          "2": "B. Emotion recognition"
        },
        {
          "2": "Emotions are a series of complex physiological and psycho-"
        },
        {
          "2": "logical\nreactions produced by people’s cognition and percep-"
        },
        {
          "2": "tion of\nthe outside world [23]. At present, many researchers"
        },
        {
          "2": "have been working on how to make computers capable of ob-"
        },
        {
          "2": "serving, understanding and even expressing various emotions"
        },
        {
          "2": ""
        },
        {
          "2": "like humans [30]."
        },
        {
          "2": "For\nfacial expressions, Zhang et al.\n[8] propoesd a region-"
        },
        {
          "2": "based multiscale\nnetwork\nthat\nlearns\nfeatures\nfor\nthe\nlocal"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "×l": "×m\n×n"
        },
        {
          "×l": "CSFM\nASST-GCN1"
        },
        {
          "×l": ""
        },
        {
          "×l": "5\nV\n(cid:117)"
        },
        {
          "×l": ""
        },
        {
          "×l": "ASST-GCN1"
        },
        {
          "×l": ""
        },
        {
          "×l": ""
        },
        {
          "×l": "10"
        },
        {
          "×l": "V\n(cid:117)"
        },
        {
          "×l": ""
        },
        {
          "×l": ""
        },
        {
          "×l": "ASST-GCN1"
        },
        {
          "×l": ""
        },
        {
          "×l": ""
        },
        {
          "×l": "16\nV"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "Adaptive selection ST-GCN 1\nAttention weighted sum\nInitialization Multiscale initialization"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "Fig. 2.\nThe architecture of MSK-GCN, which mainly uses adaptive selection ST-GCN and cross-scale mapping fusion module to classify emotion.(To better"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "describe the architecture, ASST-GCN in the stacked block was named ASST-GCN1 and ASST-GCN outside the stacked block was named ASST-GCN2. The"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "two modules are essentially the same.)"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "affective region as well as the broad context\nfor emotion im-\ncoordinates at a single time step and perform temporal analysis"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "age recognition. Mollahosseini et al.\n[31] combined AlexNet\non them to identify emotions. Narayanan et al. [1] adopted an"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "[32] and GoogleNet\n[33]\nto recognize facial expressions.For\nimage-based method to encode\nskeleton sequences,\nconvert"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "language\nand tone, Lee\net\nal.\n[34] used bi-directional\nlong\n3D joint point data into images, and then used convolutional"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "short-term memory (Bi-LSTM) to extract high-level\ntemporal\nneural networks (CNN) to extract features related to emotions"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "dynamic emotion features. Nie et al.\n[5] considered the cor-\nin\nimages\nto\nidentify\nemotions. Bhattacharya\net\nal.\n[39]"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "relation of the intra-class and inter-class videos, and proposed\nadopted a graph-based method to exploit the characteristic that"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "a\ncorrelation based graph convolutional network for\naudio-\nskeletons are naturally graphs\nin non-Euclidean space. They"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "video emotion recognition. Some scholars also consider using\nexpressed the relationship of\njoints through a spatio-temporal"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "multiple modalities\nto improve the performance of\nemotion\ngraph convolutional network (ST-GCN)\n[22],\nand extracted"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "recognition. For example, Zheng et al. [35] proposed a DNNs-\nspatiotemporal features to classify. Sheng et al. [20] adopted an"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "based multi-channel weight-sharing autoencoder with Cascade\nattention module and proposed an attention-enhanced temporal"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "multi-head attention to address\nthe\nemotional heterogeneity\nconvolutional\nnetwork\n(AT-GCN)\nto\ncapture\ndiscriminative"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "gap in multimodality.\nfeatures\nin spatial dependencies and temporal dynamics\nfor"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "sentiment\nclassiﬁcation. Zhuang et\nal.\n[21]\nalso\nadopted a"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "Although the\naccuracy of\nthese\ncues has\nreached a high"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "graph-based approach to extract global\nfeatures by construct-"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "degree,\nthey\nare\nnot\neffective\nin\nsome\nsituations,\nsuch\nas"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "ing\nglobal\nconnections\nand\nproposed\nshrinking\nblocks\nto"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "occlusion and scene noise. Gait can be used as an auxiliary"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "reduce noise to improve classiﬁcation performance. Similarity,"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "emotional\ncue\nto achieve better\nemotional\nrecognition in a"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "Sheng\net\nal.\n[40]\nalso\nconsidered\nthe\nnoise\nof\ndata,\nand"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "wider\nrange of scenes."
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "proposed a multi-task Learning with\ndenoising transformer"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "In the early stage of gait emotion recognition, hand-crafted"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "network to identify emotions. Bhatia et al.\n[41] presented a"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "features were mainly used to identify emotion. For example,"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "light\narchitecture based on LSTM to reduce\ninference\ntime"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "Li\net\nal.\n[36],\n[37]\nused\nFourier\ntransform and\nprincipal"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "for gait samples."
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "component analysis\n(PCA)\nto extract gait\nfeatures\nfrom the"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "3D coordinates of 14 human body joint points\nfor emotion"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "Different\nfrom the\nabove-mentioned methods which only"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "recognition. Crenn et al. [38] used hand-designed gait features,"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "model emotion features from a single-scale,\nthe method pro-"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "employed\nsupport\nvector machines,\nand\nintroduced\na\ncost"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "posed in this paper\nis based on multi-scale to obtain richer"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "function to generate neutral actions for emotion recognition."
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "spatial and semantic information.\nIt\nis worth noting that our"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "Recently, deep learning has achieved great\nsuccess\nin the\nmodel MSA-GCN is\nsimilar\nto G-GCSN [21] proposed by"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "ﬁeld\nof\ncomputer\nvision,\nand\na\nlarge\nnumber\nof methods\nZhuang et al., which tries to learn better emotion representa-"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "use deep learning for gait\nemotion perception. Randhavane\ntion by ﬁnding global information. However, there is a distinct"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "et\nal.\n[12]\nadopted a\ntime\nsequence-based approach, using\ndifference between the\ntwo methods. G-GCSN searches\nfor"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "an LSTM to extract\ntemporal\nfeatures, and then combined it\nglobal information by artiﬁcially adding edges from the center"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "with hand-extracted emotional features for classiﬁcation using\nnode\nto other nodes when constructing the\nskeleton graph,"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "a\nrandom forest\nclassiﬁer. Bhattacharya\net\nal.\n[19] utilized\nlimiting the expression ability of\nthe network. Our method is"
        },
        {
          "Cross scale fusion module\nAdaptive selection ST-GCN 2": "gated recursive units\n(GRUs)\nto extract\nfeatures\nfrom joint\nmore ﬂexible and adaptable because MSA-GCN obtains global"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "12"
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": "Fig. 3. Multi-scale initialization mapping relationship. The three scales consist"
        },
        {
          "9": "of 16, 10,"
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        },
        {
          "9": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "matrix AXA,XB and the 16-node feature Xoutafter scale fusion"
        },
        {
          "5": "is:"
        },
        {
          "5": "(9)\nA = g1[embedding(f1(XA))]"
        },
        {
          "5": "gX"
        },
        {
          "5": ""
        },
        {
          "5": "gX\n(10)\nB = g2[embedding(f2(XB))]"
        },
        {
          "5": ""
        },
        {
          "5": "T"
        },
        {
          "5": "·\n(11)\nB\nB) ∈ [0, 1]\nAXA,XB = sof tmax("
        },
        {
          "5": "gX\ngX"
        },
        {
          "5": "]"
        },
        {
          "5": "(12)\nXout = GCN (AXA ,XB , XA)"
        },
        {
          "5": ""
        },
        {
          "5": "represents multilayer\nf1,f2\nrepresents spatial attention, g1,g2"
        },
        {
          "5": "T"
        },
        {
          "5": "·\nrepresents the dot product\nperceptrons (MLPs), ("
        },
        {
          "5": "gX\ngX\nB)\nB"
        },
        {
          "5": "T"
        },
        {
          "5": "of\nand\nB\nB, GCN (AXA ,XB , XA)indicates that the fused"
        },
        {
          "5": "gX\ngX"
        },
        {
          "5": "features are obtained by performing graph convolution opera-"
        },
        {
          "5": ""
        },
        {
          "5": "tions on the scale of 16 nodes through the adjacency matrix"
        },
        {
          "5": ""
        },
        {
          "5": "AXA,XB ."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Neutral"
        },
        {
          "TABLE I": "0.9135"
        },
        {
          "TABLE I": "0.9402"
        },
        {
          "TABLE I": "0.8990"
        },
        {
          "TABLE I": "0.9183"
        },
        {
          "TABLE I": "0.9087"
        },
        {
          "TABLE I": "0.9567"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": ""
        },
        {
          "TABLE II": "Neutral"
        },
        {
          "TABLE II": "0.9375"
        },
        {
          "TABLE II": "0.9231"
        },
        {
          "TABLE II": "0.9615"
        },
        {
          "TABLE II": "0.9087"
        },
        {
          "TABLE II": "0.9567"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Neutral"
        },
        {
          "TABLE III": "0.9318"
        },
        {
          "TABLE III": "0.9505"
        },
        {
          "TABLE III": "0.9205"
        },
        {
          "TABLE III": "0.9348"
        },
        {
          "TABLE III": "0.9022"
        },
        {
          "TABLE III": "0.9511"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE IV": ""
        },
        {
          "TABLE IV": "Neutral"
        },
        {
          "TABLE IV": "0.9348"
        },
        {
          "TABLE IV": "0.9531"
        },
        {
          "TABLE IV": "0.9457"
        },
        {
          "TABLE IV": "0.9076"
        },
        {
          "TABLE IV": "0.9511"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "MSK-GCN (ours)\n0.9565\n0.9185\n0.9348",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "0.9511\n0.9402\n0.8074\n0.7726\n0.7896"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "Happy and Angry, which is about 0.005 higher than the other",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "best\nresults. Although it does not achieve the best\nresults on",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "Sad and Neutral, the accuracy is also high. Neutral is also only",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "about 0.005 lower than the best method. Our average accuracy",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "is also the highest among all methods, which proves that our",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "method ﬁnds more emotion-focused mapping than modeling",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "with\naction\nrecognition.\nIn\nprecision,\nrecall,\nand\nF1,\nour",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "method has good results and all reaching above 0.8. Especially",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "The label"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "compared with 2AS-GCN [26], although this method has the",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "highest accuracy in the Sad class in Table II, and mAP is not",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "much lower\nthan our method. So,\nthe classiﬁcation ability of",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "our method is better,\nand it\nis\nless\naffected by imbalanced",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "data than 2AS-GCN.\nIt\nfully demonstrates the superiority of",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "our method.",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "Emotion-Gait-21.\nThe results of comparison with state-of-",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "Predicted label"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "the-art gait emotion recognition are shown in Table III. Our",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "proposed method has the best\nresults on sad, Angry, Neutral,",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "Fig. 5.\nConfusion matrix for MSA-GCN. MSA-GCN has > 80% accuracy"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "for each class."
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "and average accuracy, improving the accuracy by around 0.02-",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "0.03. Our method has a large improvement in precision, recall",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "and, F1, and the improvement\nis about 0.1, which means that",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "and the PoseFormer [28] method are higher than our method in"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "the classiﬁcation ability of our method on positive samples is",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "single-class accuracy but far lower than our values in precision,"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "excellent. From the Table III,\nthe accuracy of LSTM (VAnilla)",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "recall, and F1, This shows that\nthese two methods only have"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "[12] is higher than our method in happy, but F1 is about 0.17",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "a stronger preference for\nthe sentiment of\nthat category, and"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "less than our method. The high accuracy of happy maybe since",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "tend to classify the samples that are not of\nthis category into"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "this method prefers to classify samples as happy,\nrather\nthan",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "this category, resulting in high accuracy of this category, rather"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "classifying samples labeled as happy.",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": ""
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "than learning the classiﬁcation ability. Therefore, our method"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "Table IV presents the results of comparison of our method",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "has better classiﬁcation ability in positive samples, which fully"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "with the methods for action recognition. Our method achieves",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "demonstrates the superiority of our method."
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "the\nbest\nresults\non\nboth Sad\nand Angry. While\non Happy",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "From Table\nI\n-\nIV, mAP is\nnot\nhigh\non\nboth\ndatasets,"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "and Neutral,\nit does not get\nthe best\nresults, but\nthe accuracy",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "which may be because\nthe\nactions of different emotions\nin"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "is only about 0.002 lower\nthan the best method on Neutral.",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "local\njoints are extremely similar. So,\nthe improved accuracy"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "Our average accuracy is also the highest among all methods,",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "of our method conﬁrms\nthat\nit\nis necessary to consider\nthe"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "which proves\nthat\nour method ﬁnds more\nemotion-focused",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "global information. The confusion matrix is given to show the"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "mapping than modeling with action recognition. Our method",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "discriminatory ability of MSK-GCN in Figure 5. It can be seen"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "has\nthe best\nresults\nin accuracy, and F1, all\nreaching above",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "that\nthe accuracy of our classiﬁer in each class is greater than"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "0.78. Although the recall rate does not reach the highest,\nit\nis",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "80%, which means that\nthe classiﬁer can identify each class"
        },
        {
          "PoseFormer[28]\n0.9674\n0.8913\n0.9185": "also 0.006 worse than the highest. The 2AS-GCN [26] method",
          "0.9076\n0.9212\n0.6833\n0.6529\n0.6678": "equally well."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8": "(with a single temporal convolution TCN) and"
        },
        {
          "8": "It can be"
        },
        {
          "8": ""
        },
        {
          "8": "the temporal"
        },
        {
          "8": ""
        },
        {
          "8": "convolution has a great impact on the accuracy. Compared with"
        },
        {
          "8": ""
        },
        {
          "8": "the traditional ST-GCN with only a single TCN, our method"
        },
        {
          "8": ""
        },
        {
          "8": "in accuracy. This proves\nthe"
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": "ASST-GCN"
        },
        {
          "8": ""
        },
        {
          "8": "0.9351"
        },
        {
          "8": ""
        },
        {
          "8": ""
        },
        {
          "8": "Effects of SA-TCN. The ablation results of AS-TCN are"
        },
        {
          "8": "the ST-GCN network"
        },
        {
          "8": "convolution kernel 5 and a multi-scale graph"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": ""
        },
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": "the recognition performance of models with adaptive selection"
        },
        {
          "TABLE V": "Node numbers",
          "Adaptively selected temporal graph convolution (AS-TCN),": "is\nimproved, which means\nthat\nour\nproposed AS-TCN is"
        },
        {
          "TABLE V": "10\n5",
          "Adaptively selected temporal graph convolution (AS-TCN),": "effective. The convolution kernel has the highest performance"
        },
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": ""
        },
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": "when selecting 5 and 9. For\nsome convolution kernels with"
        },
        {
          "TABLE V": "X",
          "Adaptively selected temporal graph convolution (AS-TCN),": ""
        },
        {
          "TABLE V": "X",
          "Adaptively selected temporal graph convolution (AS-TCN),": "large differences, such as 5 and 75, the effect is not signiﬁcant."
        },
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": ""
        },
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": ""
        },
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": "This may be due to the receptive ﬁeld of the convolution kernel"
        },
        {
          "TABLE V": "X\nX",
          "Adaptively selected temporal graph convolution (AS-TCN),": ""
        },
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": "being too large, and invalid information is extracted instead."
        },
        {
          "TABLE V": "X",
          "Adaptively selected temporal graph convolution (AS-TCN),": ""
        },
        {
          "TABLE V": "X",
          "Adaptively selected temporal graph convolution (AS-TCN),": ""
        },
        {
          "TABLE V": "X\nX",
          "Adaptively selected temporal graph convolution (AS-TCN),": ""
        },
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": "TABLE VIII"
        },
        {
          "TABLE V": "",
          "Adaptively selected temporal graph convolution (AS-TCN),": "IMPACT OF KERNEL SIZE SELECTION ON AS-TCN."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "X\nX\nX\n1,3,4": "X\nX\nX\nX\n1,2,3,4",
          "0.9231": "0.9183"
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": "Effects of CSFM. The ablation results of CSFM are shown"
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "in Table VI. The baseline is a single-scale graph (16 joints)",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "through ST-GCN networks with channels 32, 64, 128, and 256",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "without CSFM module and adaptive attention fusion module",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "(ATT). Compared with\nthe\nbackbone",
          "0.9231": "network without"
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "CSFM,\nthe model\nrecognition",
          "0.9231": "performance with CSFM is"
        },
        {
          "X\nX\nX\n1,3,4": "",
          "0.9231": ""
        },
        {
          "X\nX\nX\n1,3,4": "signiﬁcantly improved. And ATT (i.e.,",
          "0.9231": "spatial"
        },
        {
          "X\nX\nX\n1,3,4": "channel attention) adopted in the fusion process proves to be",
          "0.9231": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE VI": "THE EFFECT OF THE NUMBER OF SCALE FUSION LAYERS ON THE",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": ""
        },
        {
          "TABLE VI": "",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": "Comparing between MSK-GCN and G-GCSN (top),"
        },
        {
          "TABLE VI": "ACCURACY.",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": ""
        },
        {
          "TABLE VI": "",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": "are fewer samples classiﬁed correctly by G-GCSN and wrong"
        },
        {
          "TABLE VI": "Method",
          "of": "by MSK-GCN,",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": "so only the\nsamples"
        },
        {
          "TABLE VI": "MSK-GCN without CSFM, ATT(Baseline)",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": ""
        },
        {
          "TABLE VI": "",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": "MSA-GCN and wrongly classiﬁed by G-GCSN are listed. The"
        },
        {
          "TABLE VI": "ATT",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": ""
        },
        {
          "TABLE VI": "",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": "samples given in the upper part have an obvious feature. The"
        },
        {
          "TABLE VI": "level\n1\n2\n3\n4",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": ""
        },
        {
          "TABLE VI": "X",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": "collected samples are not facing the camera directly, but have"
        },
        {
          "TABLE VI": "X\nX\nBaseline with CSFMs",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": ""
        },
        {
          "TABLE VI": "",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": "an oblique angle (except\nfor\nthe ﬁrst"
        },
        {
          "TABLE VI": "X\nX\nX",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": ""
        },
        {
          "TABLE VI": "X\nX\nX\nX",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": "are much more difﬁcult\nto classify than normal"
        },
        {
          "TABLE VI": "",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": ""
        },
        {
          "TABLE VI": "",
          "of": "",
          "the wrong of MSA-GCN (bottom) are shown in Figure 6.": "samples. Because the joint points are more likely to overlap"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "Fig. 6.\nVisual\nresult\nanalysis. The top row shows 4 gaits",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "from the Emotion-Gait dataset where the predicted labels of our network exactly matched the"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "annotated input\nlabels and the predicted labels of G-GCSN did not matched the annotated input",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "labels. The bottom row shows 4 gaits where the predicted"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "labels did not match any of\nthe input\nlabels. Each gait",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "is represented by 3 poses in temporal sequence from left\nto right."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "are basically the\nsame,\nand the upper\nlimbs\ntend to be\nthe",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "ACKNOWLEDGMENTS"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "same due to the overlapping joint points of walking, which",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "This work was supported by the Natural Science Foundation"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "may be the reason why G-GCSN classiﬁes the angry samples",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "of China under Grant 61962038, Grant 61962006, and by the"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "as\nthe label\nsad. This also proves\nthat we mentioned in the",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "Guangxi Bagui Teams for\nInnovation and Research."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "introduction that when the actions of\nlocal\njoints are similar,",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "and if only focusing on the relationship of\nlocal\njoints may",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "confuse the conjecture of\nthe classiﬁcation results.",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "REFERENCES"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "For\nthe analysis of MSA-GCN misclassiﬁcation (bottom),",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[1] V. Narayanan, B. M. Manoghar, V. S. Dorbala, D. Manocha, and A. Bera,"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "the misclassiﬁed samples is selected from the class that is more",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "“Proxemo: Gait-based emotion learning and multi-view proxemic fusion"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "prone\nto misclassiﬁcation, which is more\nconducive\nto the",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "International\nfor\nsocially-aware\nrobot navigation,”\nin 2020 IEEE/RSJ"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "Conference on Intelligent Robots and Systems (IROS).\nIEEE, 2020, pp."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "analysis of why the classiﬁcation error occurs. It can be seen",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "8200–8207."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "that when misclassiﬁed,\nthe samples\ntend to be classiﬁed as",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[2]\nS. A. Denham, E. Workman, P. M. Cole, C. Weissbrod, K. T. Kendziora,"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "happy, sad, and neutral. Angry was less likely to be classiﬁed,",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "and C. ZAHN-WAXLER, “Prediction of\nexternalizing\nbehavior prob-"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "lems from early to middle childhood: The role of parental socialization"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "it should be that the limbs swayed more and faster when angry.",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "and emotion expression,” Development and psychopathology,\nvol. 12,"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "For\nthe four\nsamples\nthat were misclassiﬁed,\nthe reason for",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "no. 1, pp. 23–45, 2000."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "the misclassiﬁcation may be\nthat\nthe gait of\nthese\nsamples",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[3]\nJ. Arunnehru\nand M. Kalaiselvi Geetha,\n“Automatic\nhuman emotion"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "techniques\nin signal\nrecognition\nin surveillance\nvideo,”\nin Intelligent"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "expressing emotions is\nrelatively restrained, and the reaction",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "processing for multimedia security.\nSpringer, 2017, pp. 321–342."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "is\nthat\nthe degree of posture\ncollapse\nand the\namplitude of",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[4]\nS. Starke, H. Zhang, T. Komura, and J. Saito, “Neural state machine for"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "hand swing are\nsmall on the\nskeleton,\nso few discriminant",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "character-scene\ninteractions.” ACM Trans. Graph., vol. 38, no. 6, pp."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "209–1, 2019."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "features are extracted by the network,\nleading to classiﬁcation",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[5] W. Nie, M. Ren,\nJ. Nie,\nand\nS. Zhao,\n“C-gcn:\ncorrelation\nbased"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "is incorrect.",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "graph convolutional network for audio-video emotion recognition,” IEEE"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "Transactions on Multimedia, vol. 23, pp. 3793–3804, 2020."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[6]\nE. M. Schmidt and Y. E. Kim, “Learning emotion-based acoustic features"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "V. CONCLUSION",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "with deep belief networks,” in 2011 IEEE workshop on applications of"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "In this work, we proposed the multiscale\nadaptive graph",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "signal processing to audio and acoustics\n(Waspaa).\nIEEE, 2011, pp."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "65–68."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "convolution network for gait-based emotion recognition. Con-",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[7] R. Xia\nand Z. Ding,\n“Emotion-cause\npair\nextraction: A new task to"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "sidering that\nthe emotion of gait\nis presented by the overall",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "emotion analysis in texts,” arXiv preprint arXiv:1906.01267, 2019."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "state\nand the\nemotion information of\nevery sample\nin time",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[8] H. Zhang and M. Xu, “Multiscale\nemotion representation learning for"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "affective image recognition,” IEEE Transactions on Multimedia, 2022."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "domain is different, we obtain emotional mapping through a",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[9] M. Mohammadpour, H. Khaliliardali, S. M. R. Hashemi,\nand M. M."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "multiscale adaptive graoh convolution network we proposed.",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "AlyanNezhadi,\n“Facial\nemotion\nrecognition\nusing\ndeep\nconvolutional"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "We conducted extensive experiments on two public datasets.",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "conference on knowledge-\nnetworks,”\nin 2017 IEEE 4th international"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "based engineering and innovation (KBEI).\nIEEE, 2017, pp. 0017–0021."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "As a result, we showed the effectiveness of our module and",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[10]\nF. W. Smith and M. L. Smith, “Decoding the dynamic\nrepresentation"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "showed that\nthe model\nin which our module is\nimplemented",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "of\nfacial\nexpressions\nof\nemotion\nin\nexplicit\nand\nincidental\ntasks,”"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "achieves state-ofthe-art performance compared with previous",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "Neuroimage, vol. 195, pp. 261–271, 2019."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "[11] R. Santhoshkumar\nand M. K. Geetha,\n“Deep\nlearning\napproach\nfor"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "state-of-the-art methods on both of them. In the future, we will",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "emotion\nrecognition\nfrom human\nbody movements with\nfeedforward"
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "extend our method to noisy label\nlearning [45] and federated",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": ""
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "deep\nconvolution\nneural\nnetworks,” Procedia Computer\nScience,\nvol."
        },
        {
          "MSK-GCN(cid:966)Sad\nMSK-GCN(cid:966)Happy": "learning [46] with gait-based emotion recognition.",
          "MSK-GCN(cid:966)Neutral\nMSK-GCN(cid:966)Sad": "152, pp. 158–165, 2019."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "[35]\nJ. Zheng, S. Zhang, Z. Wang, X. Wang, and Z. Zeng, “Multi-channel"
        },
        {
          "10": "weight-sharing autoencoder based on cascade multi-head attention for"
        },
        {
          "10": "IEEE Transactions\nmultimodal\nemotion\nrecognition,”\non Multimedia,"
        },
        {
          "10": "2022."
        },
        {
          "10": "[36] B. Li, C. Zhu, S. Li, and T. Zhu, “Identifying emotions from non-contact"
        },
        {
          "10": "IEEE Transactions\non\ngaits\ninformation based on microsoft kinects,”"
        },
        {
          "10": "Affective Computing, vol. 9, no. 4, pp. 585–591, 2016."
        },
        {
          "10": "[37]\nS. Li, L. Cui, C. Zhu, B. Li, N. Zhao, and T. Zhu, “Emotion recognition"
        },
        {
          "10": "using kinect motion capture data of human gaits,” PeerJ, vol. 4, p. e2364,"
        },
        {
          "10": "2016."
        },
        {
          "10": "[38] A. Crenn, R. A. Khan, A. Meyer, and S. Bouakaz,\n“Body expression"
        },
        {
          "10": "recognition from animated 3d skeleton,” in 2016 International Confer-"
        },
        {
          "10": "ence on 3D Imaging (IC3D).\nIEEE, 2016, pp. 1–7."
        },
        {
          "10": "[39] U. Bhattacharya, T. Mittal, R. Chandra, T. Randhavane, A. Bera, and"
        },
        {
          "10": "D. Manocha, “Step: Spatial\ntemporal graph convolutional networks\nfor"
        },
        {
          "10": "the AAAI Conference\nemotion perception from gaits,” in Proceedings of"
        },
        {
          "10": "on Artiﬁcial\nIntelligence, vol. 34, no. 02, 2020, pp. 1342–1350."
        },
        {
          "10": "[40] W. Sheng, X. Lu, and X. Li, “Mldt: Multi-task learning with denoising"
        },
        {
          "10": "2021\n4th\ntransformer\nfor\ngait\nidentity\nand\nemotion\nrecognition,”\nin"
        },
        {
          "10": "Artiﬁcial\nIntelligence and Cloud Computing Conference, 2021, pp. 47–"
        },
        {
          "10": "52."
        },
        {
          "10": "[41] Y. Bhatia, A. H. Bari, and M. Gavrilova, “A lstm-based approach for"
        },
        {
          "10": "gait emotion recognition,” in 2021 IEEE 20th International Conference"
        },
        {
          "10": "on Cognitive Informatics & Cognitive Computing (ICCI* CC).\nIEEE,"
        },
        {
          "10": "2021, pp. 214–221."
        },
        {
          "10": "[42] M. Li, S. Chen, Y. Zhao, Y. Zhang, Y. Wang, and Q. Tian, “Dynamic"
        },
        {
          "10": "multiscale graph neural networks\nfor 3d skeleton based human motion"
        },
        {
          "10": "the IEEE/CVF Conference on Computer\nprediction,” in Proceedings of"
        },
        {
          "10": "Vision and Pattern Recognition, 2020, pp. 214–223."
        },
        {
          "10": "[43]\nS. Rendle, Z. Gantner, C. Freudenthaler, and L. Schmidt-Thieme, “Fast"
        },
        {
          "10": "context-aware\nrecommendations with factorization machines,”\nin Pro-"
        },
        {
          "10": "ceedings of\nthe 34th international ACM SIGIR conference on Research"
        },
        {
          "10": "and development\nin Information Retrieval, 2011, pp. 635–644."
        },
        {
          "10": "[44]\nL. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. V. Gool,"
        },
        {
          "10": "“Temporal\nsegment networks: Towards good practices\nfor deep action"
        },
        {
          "10": "recognition,”\nin European conference on computer\nvision.\nSpringer,"
        },
        {
          "10": "2016, pp. 20–36."
        },
        {
          "10": "[45]\nZ. Wang,\nJ.\nJiang, B. Han, L. Feng, B. An, G. Niu,\nand G. Long,"
        },
        {
          "10": "“SemiNLL: A framework\nof noisy-label\nlearning\nby semi-supervised"
        },
        {
          "10": "learning,” Transactions on Machine Learning Research, 2022. [Online]."
        },
        {
          "10": "Available: https://openreview.net/forum?id=qzM1Tw5i7N"
        },
        {
          "10": "[46]\nZ. Wang, T. Zhou, G. Long, B. Han, and J. Jiang, “Fednoil: A simple"
        },
        {
          "10": "two-level\nsampling method for\nfederated\nlearning with noisy labels,”"
        },
        {
          "10": "arXiv preprint arXiv:2205.10110, 2022."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "VI. BIOGRAPHY SECTION"
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "Yunfei Yin received the B.S. degree\nin Computer"
        },
        {
          "10": ""
        },
        {
          "10": "Science\nfrom Peking University, Beijing City, R.P."
        },
        {
          "10": ""
        },
        {
          "10": "China\nin 2002,\nthe M.S. degree\nin Computer En-"
        },
        {
          "10": ""
        },
        {
          "10": "gineering from Guangxi Normal University, Guilin"
        },
        {
          "10": ""
        },
        {
          "10": "City, R.P. China,\nin\n2005\nand\nthe\nPh.D.\ndegree"
        },
        {
          "10": ""
        },
        {
          "10": "in Control\nScience & Engineering\nfrom Beijing"
        },
        {
          "10": ""
        },
        {
          "10": "University of Aeronautics and Astronautics, Beijing"
        },
        {
          "10": ""
        },
        {
          "10": "City, R.P. China, in 2010. From 2010 to 2013, he was"
        },
        {
          "10": ""
        },
        {
          "10": "a Research Assistant with the Chongqing University."
        },
        {
          "10": ""
        },
        {
          "10": "Since\n2013,\nhe\nhas\nbeen\nan Associate\nProfessor"
        },
        {
          "10": ""
        },
        {
          "10": "with the College of Computer Science, Chongqing"
        },
        {
          "10": ""
        },
        {
          "10": "University. He\nis\na\nreviewer\nfor\nJournal\nof Software\n(Chinese), Artiﬁcial"
        },
        {
          "10": ""
        },
        {
          "10": "Intelligence,\nIEEE International Conference\non Data Mining\n(ICDM),\nand"
        },
        {
          "10": ""
        },
        {
          "10": "other journals and conferences. He is mainly engaged in Artiﬁcial Intelligence"
        },
        {
          "10": ""
        },
        {
          "10": "(Data Mining), System Modeling, Computer Simulation and Unmanned Aerial"
        },
        {
          "10": ""
        },
        {
          "10": "Vehicle\nresearches.\nIn recent years, he has\ninvolved in the National Natu-"
        },
        {
          "10": ""
        },
        {
          "10": "ral Science Foundation of China,\nInternational Large Research Foundation,"
        },
        {
          "10": ""
        },
        {
          "10": "provincial and ministerial\nlevel\nfoundations and other projects, a total of 20."
        },
        {
          "10": ""
        },
        {
          "10": "He has published more than 40 SCI/EI/ISTP-cited refereed papers."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "10": "[35]\nJ. Zheng, S. Zhang, Z. Wang, X. Wang, and Z. Zeng, “Multi-channel"
        },
        {
          "10": "weight-sharing autoencoder based on cascade multi-head attention for"
        },
        {
          "10": "IEEE Transactions\nmultimodal\nemotion\nrecognition,”\non Multimedia,"
        },
        {
          "10": "2022."
        },
        {
          "10": "[36] B. Li, C. Zhu, S. Li, and T. Zhu, “Identifying emotions from non-contact"
        },
        {
          "10": "IEEE Transactions\non\ngaits\ninformation based on microsoft kinects,”"
        },
        {
          "10": "Affective Computing, vol. 9, no. 4, pp. 585–591, 2016."
        },
        {
          "10": "[37]\nS. Li, L. Cui, C. Zhu, B. Li, N. Zhao, and T. Zhu, “Emotion recognition"
        },
        {
          "10": "using kinect motion capture data of human gaits,” PeerJ, vol. 4, p. e2364,"
        },
        {
          "10": "2016."
        },
        {
          "10": "[38] A. Crenn, R. A. Khan, A. Meyer, and S. Bouakaz,\n“Body expression"
        },
        {
          "10": "recognition from animated 3d skeleton,” in 2016 International Confer-"
        },
        {
          "10": "ence on 3D Imaging (IC3D).\nIEEE, 2016, pp. 1–7."
        },
        {
          "10": "[39] U. Bhattacharya, T. Mittal, R. Chandra, T. Randhavane, A. Bera, and"
        },
        {
          "10": "D. Manocha, “Step: Spatial\ntemporal graph convolutional networks\nfor"
        },
        {
          "10": "the AAAI Conference\nemotion perception from gaits,” in Proceedings of"
        },
        {
          "10": "on Artiﬁcial\nIntelligence, vol. 34, no. 02, 2020, pp. 1342–1350."
        },
        {
          "10": "[40] W. Sheng, X. Lu, and X. Li, “Mldt: Multi-task learning with denoising"
        },
        {
          "10": "2021\n4th\ntransformer\nfor\ngait\nidentity\nand\nemotion\nrecognition,”\nin"
        },
        {
          "10": "Artiﬁcial\nIntelligence and Cloud Computing Conference, 2021, pp. 47–"
        },
        {
          "10": "52."
        },
        {
          "10": "[41] Y. Bhatia, A. H. Bari, and M. Gavrilova, “A lstm-based approach for"
        },
        {
          "10": "gait emotion recognition,” in 2021 IEEE 20th International Conference"
        },
        {
          "10": "on Cognitive Informatics & Cognitive Computing (ICCI* CC).\nIEEE,"
        },
        {
          "10": "2021, pp. 214–221."
        },
        {
          "10": "[42] M. Li, S. Chen, Y. Zhao, Y. Zhang, Y. Wang, and Q. Tian, “Dynamic"
        },
        {
          "10": "multiscale graph neural networks\nfor 3d skeleton based human motion"
        },
        {
          "10": "the IEEE/CVF Conference on Computer\nprediction,” in Proceedings of"
        },
        {
          "10": "Vision and Pattern Recognition, 2020, pp. 214–223."
        },
        {
          "10": "[43]\nS. Rendle, Z. Gantner, C. Freudenthaler, and L. Schmidt-Thieme, “Fast"
        },
        {
          "10": "context-aware\nrecommendations with factorization machines,”\nin Pro-"
        },
        {
          "10": "ceedings of\nthe 34th international ACM SIGIR conference on Research"
        },
        {
          "10": "and development\nin Information Retrieval, 2011, pp. 635–644."
        },
        {
          "10": "[44]\nL. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. V. Gool,"
        },
        {
          "10": "“Temporal\nsegment networks: Towards good practices\nfor deep action"
        },
        {
          "10": "recognition,”\nin European conference on computer\nvision.\nSpringer,"
        },
        {
          "10": "2016, pp. 20–36."
        },
        {
          "10": "[45]\nZ. Wang,\nJ.\nJiang, B. Han, L. Feng, B. An, G. Niu,\nand G. Long,"
        },
        {
          "10": "“SemiNLL: A framework\nof noisy-label\nlearning\nby semi-supervised"
        },
        {
          "10": "learning,” Transactions on Machine Learning Research, 2022. [Online]."
        },
        {
          "10": "Available: https://openreview.net/forum?id=qzM1Tw5i7N"
        },
        {
          "10": "[46]\nZ. Wang, T. Zhou, G. Long, B. Han, and J. Jiang, “Fednoil: A simple"
        },
        {
          "10": "two-level\nsampling method for\nfederated\nlearning with noisy labels,”"
        },
        {
          "10": "arXiv preprint arXiv:2205.10110, 2022."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "VI. BIOGRAPHY SECTION"
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": "Yunfei Yin received the B.S. degree\nin Computer"
        },
        {
          "10": ""
        },
        {
          "10": "Science\nfrom Peking University, Beijing City, R.P."
        },
        {
          "10": ""
        },
        {
          "10": "China\nin 2002,\nthe M.S. degree\nin Computer En-"
        },
        {
          "10": ""
        },
        {
          "10": "gineering from Guangxi Normal University, Guilin"
        },
        {
          "10": ""
        },
        {
          "10": "City, R.P. China,\nin\n2005\nand\nthe\nPh.D.\ndegree"
        },
        {
          "10": ""
        },
        {
          "10": "in Control\nScience & Engineering\nfrom Beijing"
        },
        {
          "10": ""
        },
        {
          "10": "University of Aeronautics and Astronautics, Beijing"
        },
        {
          "10": ""
        },
        {
          "10": "City, R.P. China, in 2010. From 2010 to 2013, he was"
        },
        {
          "10": ""
        },
        {
          "10": "a Research Assistant with the Chongqing University."
        },
        {
          "10": ""
        },
        {
          "10": "Since\n2013,\nhe\nhas\nbeen\nan Associate\nProfessor"
        },
        {
          "10": ""
        },
        {
          "10": "with the College of Computer Science, Chongqing"
        },
        {
          "10": ""
        },
        {
          "10": "University. He\nis\na\nreviewer\nfor\nJournal\nof Software\n(Chinese), Artiﬁcial"
        },
        {
          "10": ""
        },
        {
          "10": "Intelligence,\nIEEE International Conference\non Data Mining\n(ICDM),\nand"
        },
        {
          "10": ""
        },
        {
          "10": "other journals and conferences. He is mainly engaged in Artiﬁcial Intelligence"
        },
        {
          "10": ""
        },
        {
          "10": "(Data Mining), System Modeling, Computer Simulation and Unmanned Aerial"
        },
        {
          "10": ""
        },
        {
          "10": "Vehicle\nresearches.\nIn recent years, he has\ninvolved in the National Natu-"
        },
        {
          "10": ""
        },
        {
          "10": "ral Science Foundation of China,\nInternational Large Research Foundation,"
        },
        {
          "10": ""
        },
        {
          "10": "provincial and ministerial\nlevel\nfoundations and other projects, a total of 20."
        },
        {
          "10": ""
        },
        {
          "10": "He has published more than 40 SCI/EI/ISTP-cited refereed papers."
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        },
        {
          "10": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Li\nJing\nis": "degree\nwith",
          "currently working\ntoward\nthe M.S.": "the\nSchool\nof\nComputer\nScience"
        },
        {
          "Li\nJing\nis": "and Technology, Chongqing University, Chongqing,",
          "currently working\ntoward\nthe M.S.": ""
        },
        {
          "Li\nJing\nis": "China.\nHer",
          "currently working\ntoward\nthe M.S.": "research\ninterests\ninclude\nemotion"
        },
        {
          "Li\nJing\nis": "recognition",
          "currently working\ntoward\nthe M.S.": "and\ncomputer\nvision. Contact\nher\nat"
        },
        {
          "Li\nJing\nis": "202114021068t@cqu.edu.cn",
          "currently working\ntoward\nthe M.S.": ""
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This figure \"ygc.jpg\" is available in \"jpg\"(cid:10) format from:": "http://arxiv.org/ps/2209.08988v1"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Proxemo: Gait-based emotion learning and multi-view proxemic fusion for socially-aware robot navigation",
      "authors": [
        "V Narayanan",
        "B Manoghar",
        "V Dorbala",
        "D Manocha",
        "A Bera"
      ],
      "year": "2020",
      "venue": "2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)"
    },
    {
      "citation_id": "2",
      "title": "Prediction of externalizing behavior problems from early to middle childhood: The role of parental socialization and emotion expression",
      "authors": [
        "S Denham",
        "E Workman",
        "P Cole",
        "C Weissbrod",
        "K Kendziora",
        "C Zahn-Waxler"
      ],
      "year": "2000",
      "venue": "Development and psychopathology"
    },
    {
      "citation_id": "3",
      "title": "Automatic human emotion recognition in surveillance video",
      "authors": [
        "J Arunnehru",
        "M Geetha"
      ],
      "year": "2017",
      "venue": "Intelligent techniques in signal processing for multimedia security"
    },
    {
      "citation_id": "4",
      "title": "Neural state machine for character-scene interactions",
      "authors": [
        "S Starke",
        "H Zhang",
        "T Komura",
        "J Saito"
      ],
      "year": "2019",
      "venue": "ACM Trans. Graph"
    },
    {
      "citation_id": "5",
      "title": "C-gcn: correlation based graph convolutional network for audio-video emotion recognition",
      "authors": [
        "W Nie",
        "M Ren",
        "J Nie",
        "S Zhao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "6",
      "title": "Learning emotion-based acoustic features with deep belief networks",
      "authors": [
        "E Schmidt",
        "Y Kim"
      ],
      "year": "2011",
      "venue": "2011 IEEE workshop on applications of signal processing to audio and acoustics"
    },
    {
      "citation_id": "7",
      "title": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "authors": [
        "R Xia",
        "Z Ding"
      ],
      "year": "2019",
      "venue": "Emotion-cause pair extraction: A new task to emotion analysis in texts",
      "arxiv": "arXiv:1906.01267"
    },
    {
      "citation_id": "8",
      "title": "Multiscale emotion representation learning for affective image recognition",
      "authors": [
        "H Zhang",
        "M Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Facial emotion recognition using deep convolutional networks",
      "authors": [
        "M Mohammadpour",
        "H Khaliliardali",
        "S Hashemi",
        "M Alyannezhadi"
      ],
      "year": "2017",
      "venue": "Facial emotion recognition using deep convolutional networks"
    },
    {
      "citation_id": "10",
      "title": "Decoding the dynamic representation of facial expressions of emotion in explicit and incidental tasks",
      "authors": [
        "F Smith",
        "M Smith"
      ],
      "year": "2019",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "11",
      "title": "Deep learning approach for emotion recognition from human body movements with feedforward deep convolution neural networks",
      "authors": [
        "R Santhoshkumar",
        "M Geetha"
      ],
      "year": "2019",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "12",
      "title": "Identifying emotions from walking using affective and deep features",
      "authors": [
        "T Randhavane",
        "U Bhattacharya",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2019",
      "venue": "Identifying emotions from walking using affective and deep features",
      "arxiv": "arXiv:1906.11884"
    },
    {
      "citation_id": "13",
      "title": "Siamese denoising autoencoders for joints trajectories reconstruction and robust gait recognition",
      "authors": [
        "W Sheng",
        "X Li"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "14",
      "title": "The identification of emotions from gait information",
      "authors": [
        "J Montepare",
        "S Goldstein",
        "A Clausen"
      ],
      "year": "1987",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "15",
      "title": "Not all is noticed: Kinematic cues of emotionspecific gait",
      "authors": [
        "S Halovic",
        "C Kroos"
      ],
      "year": "2018",
      "venue": "Human movement science"
    },
    {
      "citation_id": "16",
      "title": "Critical features for the perception of emotion from gait",
      "authors": [
        "C Roether",
        "L Omlor",
        "A Christensen",
        "M Giese"
      ],
      "year": "2009",
      "venue": "Journal of vision"
    },
    {
      "citation_id": "17",
      "title": "Recognition of affect based on gait patterns",
      "authors": [
        "M Karg",
        "K Kühnlenz",
        "M Buss"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
    },
    {
      "citation_id": "18",
      "title": "Cbam: Convolutional block attention module",
      "authors": [
        "S Woo",
        "J Park",
        "J.-Y Lee",
        "I Kweon"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "19",
      "title": "Take an emotion walk: Perceiving emotions from gaits using hierarchical attention pooling and affective mapping",
      "authors": [
        "U Bhattacharya",
        "C Roncal",
        "T Mittal",
        "R Chandra",
        "K Kapsaskis",
        "K Gray",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Multi-task learning for gait-based identity recognition and emotion recognition using attention enhanced temporal graph convolutional network",
      "authors": [
        "W Sheng",
        "X Li"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "21",
      "title": "G-gcsn: Global graph convolution shrinkage network for emotion perception from gait",
      "authors": [
        "Y Zhuang",
        "L Lin",
        "R Tong",
        "J Liu",
        "Y Iwamot",
        "Y.-W Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the Asian Conference on Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Spatial temporal graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "S Yan",
        "Y Xiong",
        "D Lin"
      ],
      "year": "2018",
      "venue": "Thirty-second AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "23",
      "title": "Channelwise topology refinement graph convolution for skeleton-based action recognition",
      "authors": [
        "Y Chen",
        "Z Zhang",
        "C Yuan",
        "B Li",
        "Y Deng",
        "W Hu"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "24",
      "title": "Mining actionlet ensemble for action recognition with depth cameras",
      "authors": [
        "J Wang",
        "Z Liu",
        "Y Wu",
        "J Yuan"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Spatio-temporal lstm with trust gates for 3d human action recognition",
      "authors": [
        "J Liu",
        "A Shahroudy",
        "D Xu",
        "G Wang"
      ],
      "year": "2016",
      "venue": "Spatio-temporal lstm with trust gates for 3d human action recognition"
    },
    {
      "citation_id": "26",
      "title": "Two-stream adaptive graph convolutional networks for skeleton-based action recognition",
      "authors": [
        "L Shi",
        "Y Zhang",
        "J Cheng",
        "H Lu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "27",
      "title": "Disentangling and unifying graph convolutions for skeleton-based action recognition",
      "authors": [
        "Z Liu",
        "H Zhang",
        "Z Chen",
        "Z Wang",
        "W Ouyang"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "28",
      "title": "3d human pose estimation with spatial and temporal transformers",
      "authors": [
        "C Zheng",
        "S Zhu",
        "M Mendieta",
        "T Yang",
        "C Chen",
        "Z Ding"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "Action recognition via posebased graph convolutional networks with intermediate dense supervision",
      "authors": [
        "L Shi",
        "Y Zhang",
        "J Cheng",
        "H Lu"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition for human-robot interaction: Recent advances and future perspectives",
      "authors": [
        "M Spezialetti",
        "G Placidi",
        "S Rossi"
      ],
      "year": "2020",
      "venue": "Frontiers in Robotics and AI"
    },
    {
      "citation_id": "31",
      "title": "Going deeper in facial expression recognition using deep neural networks",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter conference on applications of computer vision"
    },
    {
      "citation_id": "32",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "35",
      "title": "Multi-channel weight-sharing autoencoder based on cascade multi-head attention for multimodal emotion recognition",
      "authors": [
        "J Zheng",
        "S Zhang",
        "Z Wang",
        "X Wang",
        "Z Zeng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "36",
      "title": "Identifying emotions from non-contact gaits information based on microsoft kinects",
      "authors": [
        "B Li",
        "C Zhu",
        "S Li",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition using kinect motion capture data of human gaits",
      "authors": [
        "S Li",
        "L Cui",
        "C Zhu",
        "B Li",
        "N Zhao",
        "T Zhu"
      ],
      "year": "2016",
      "venue": "PeerJ"
    },
    {
      "citation_id": "38",
      "title": "Body expression recognition from animated 3d skeleton",
      "authors": [
        "A Crenn",
        "R Khan",
        "A Meyer",
        "S Bouakaz"
      ],
      "year": "2016",
      "venue": "2016 International Conference on 3D Imaging (IC3D)"
    },
    {
      "citation_id": "39",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Mldt: Multi-task learning with denoising transformer for gait identity and emotion recognition",
      "authors": [
        "W Sheng",
        "X Lu",
        "X Li"
      ],
      "year": "2021",
      "venue": "2021 4th Artificial Intelligence and Cloud Computing Conference"
    },
    {
      "citation_id": "41",
      "title": "A lstm-based approach for gait emotion recognition",
      "authors": [
        "Y Bhatia",
        "A Bari",
        "M Gavrilova"
      ],
      "year": "2021",
      "venue": "2021 IEEE 20th International Conference on Cognitive Informatics & Cognitive Computing (ICCI* CC)"
    },
    {
      "citation_id": "42",
      "title": "Dynamic multiscale graph neural networks for 3d skeleton based human motion prediction",
      "authors": [
        "M Li",
        "S Chen",
        "Y Zhao",
        "Y Zhang",
        "Y Wang",
        "Q Tian"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "43",
      "title": "Fast context-aware recommendations with factorization machines",
      "authors": [
        "S Rendle",
        "Z Gantner",
        "C Freudenthaler",
        "L Schmidt-Thieme"
      ],
      "year": "2011",
      "venue": "Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval"
    },
    {
      "citation_id": "44",
      "title": "Temporal segment networks: Towards good practices for deep action recognition",
      "authors": [
        "L Wang",
        "Y Xiong",
        "Z Wang",
        "Y Qiao",
        "D Lin",
        "X Tang",
        "L Gool"
      ],
      "year": "2016",
      "venue": "Temporal segment networks: Towards good practices for deep action recognition"
    },
    {
      "citation_id": "45",
      "title": "SemiNLL: A framework of noisy-label learning by semi-supervised learning",
      "authors": [
        "Z Wang",
        "J Jiang",
        "B Han",
        "L Feng",
        "B An",
        "G Niu",
        "G Long"
      ],
      "year": "2022",
      "venue": "Transactions on Machine Learning Research"
    },
    {
      "citation_id": "46",
      "title": "Fednoil: A simple two-level sampling method for federated learning with noisy labels",
      "authors": [
        "Z Wang",
        "T Zhou",
        "G Long",
        "B Han",
        "J Jiang"
      ],
      "year": "2010",
      "venue": "Artificial Intelligence, IEEE International Conference on Data Mining (ICDM), and other journals and conferences. He is mainly engaged in Artificial Intelligence (Data Mining), System Modeling, Computer Simulation and Unmanned Aerial Vehicle researches. In recent years, he has involved in the National Natural Science Foundation of China",
      "arxiv": "arXiv:2205.10110"
    }
  ]
}