{
  "paper_id": "2507.15765v2",
  "title": "Learning From Heterogeneity: Generalizing Dynamic Facial Expression Recognition Via Distributionally Robust Optimization",
  "published": "2025-07-21T16:21:47Z",
  "authors": [
    "Feng-Qi Cui",
    "Anyang Tong",
    "Jinyang Huang",
    "Jie Zhang",
    "Dan Guo",
    "Zhi Liu",
    "Meng Wang"
  ],
  "keywords": [
    "Dynamic facial expression recognition",
    "distributionally robust optimization",
    "contrastive learning",
    "time-frequency analysis"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Dynamic Facial Expression Recognition (DFER) plays a critical role in affective computing and human-computer interaction. Although existing methods achieve comparable performance, they inevitably suffer from performance degradation under sample heterogeneity caused by multi-source data and individual expression variability. To address these challenges, we propose a novel framework, called Heterogeneity-aware Distributional Framework (HDF), and design two plug-and-play modules to enhance timefrequency modeling and mitigate optimization imbalance caused by hard samples. Specifically, the Time-Frequency Distributional Attention Module (DAM) captures both temporal consistency and frequency robustness through a dual-branch attention design, improving tolerance to sequence inconsistency and visual style shifts. Then, based on gradient sensitivity and information bottleneck principles, an adaptive optimization module Distributionaware Scaling Module (DSM) is introduced to dynamically balance classification and contrastive losses, enabling more stable and discriminative representation learning. Extensive experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF significantly improves both recognition accuracy and robustness. Our method achieves superior weighted average recall (WAR) and unweighted average recall (UAR) while maintaining strong generalization across diverse and imbalanced scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial expression recognition (FER) can automatically perceive human emotions and has been widely applied in real-world scenarios such as psychological evaluation  [10, 11, 35] , and human-computer interaction  [6, 12, 44] . According to the data type, FER approaches can be divided into two categories: Static FER (SFER)  [26]  and Dynamic FER (DFER)  [47] . SFER methods focus on analyzing single still images to classify emotions  [29] , which limits their capacity to capture the dynamic progression and contextual transitions of expressions across time  [24] , leading to degraded performance in realistic settings that involve temporal complexity and motion variance. In contrast, DFER techniques utilize video sequences to model how facial expressions evolve over time, offering a more faithful representation of emotional dynamics. However, it is challenging to understand the spatiotemporal expressions in real scenarios.\n\nConsidering that facial expressions are usually subtle, transitory, and subject to spatio-temporal variations in the real world, pioneer works have been done to capture the subtle changes in expressions through temporal modeling. For example, recent DFER methods aim to model the temporal evolution and structural dynamics of facial expressions by integrating spatiotemporal cues through CNN-RNN pipelines  [18, 41] , unified 3D CNNs like I3D  [1]  and M3DFEL  [37] , or transformer-based attention mechanisms as in Former-DFER  [49]  and STT  [28] . Although these temporal modeling capabilities enhance the robustness and adaptability of DFER in real-world environments and further achieve impressive performances  [38] , several problems still need to be seriously addressed before widespread application. As illustrated in Fig.  1 , due to the lack of mechanisms to handle hard samples arising from real-world sample heterogeneity, which is extremely common and harmful, the biggest problem with existing DFER methods is poor generalization. The heterogeneity basically caused by data source differences, individual differences, and training dynamics. 1) Overlooking source-level heterogeneity undermines robustness to visual variation. DFER datasets are typically built from heterogeneous video sources such as movies, news, and interviews, which introduce substantial variations in visual style, resolution, compression, and motion continuity. These source-level discrepancies manifest as sample-level distributional variations, which make it difficult to learn consistent and discriminative representations. Without explicitly modeling the heterogeneity in visual styles and quality, models tend to overfit dominant patterns and struggle with underrepresented or atypical samples, ultimately degrading generalization under real-world conditions. 2) Individualinduced heterogeneity leads to hard samples and representation inconsistency. Variations in identity, expression patterns, age, and emotional intensity lead to significant individual-induced heterogeneity, where the same emotion is expressed in diverse and often inconsistent ways. This results in increased intra-category variation and a higher prevalence of hard or ambiguous samples during training, which undermines representation stability and degrades the reliability of learned decision boundaries. 3) Overlooking training dynamics under sample heterogeneity hinders optimization stability.\n\nIn the presence of sample heterogeneity, data-driven deep models often encounter optimization difficulties, such as fluctuating gradients and unstable convergence. Hard samples dominate training signals and may overwhelm classification loss, further leading to brittle representations.\n\nTo address the above challenges in DFER, inspired by the principles of Distributionally Robust Optimization, we propose a novel framework named Heterogeneity-aware Distributional Framework (HDF), which incorporates two plug-and-play DFER-enhancing modules that can be seamlessly integrated into existing generalpurpose video backbone networks. First, to construct more stable and generalizable expression representations under sample heterogeneity, we design the Time-Frequency Distributional Attention Module (DAM). By leveraging wasserstein-regularized temporal attention and frequency-domain adversarial perturbation to capture highly uncertain expression dynamics and suppress distributional variations in style and resolution, DAM adopts a dual-branch architecture to perform adaptive and robust modeling in both temporal and frequency domains. To further mitigate the optimization instability and representation degradation caused by hard samples, we introduce a novel Distribution-aware Scaling Module (DSM). By combining an information-constrained and reweighted supervised contrastive loss with a sharpness-aware loss scaling mechanism driven by gradient norm, DSM constructs a dual-enhanced optimization strategy that adaptively balances classification and contrastive learning objectives, thereby improving both the robustness and discriminability of learned representations under heterogeneous conditions.\n\nTotally, our main contributions can be concluded as follows:\n\n‚Ä¢ To the best of our knowledge, this paper is the first attempt to introduce the principle of Distributionally Robust Optimization into DFER. By joint modeling of style variations, individual expression dynamics, and task-level distribution shifts across both feature and optimization levels, we construct HDF that explicitly addresses ubiquitous sample heterogeneity.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work 2.1 Dynamic Facial Expression Recognition",
      "text": "Dynamic Facial Expression Recognition (DFER) focuses on modeling temporal changes in facial expressions, capturing subtle emotional dynamics beyond static images. Early methods relied on handcrafted spatiotemporal features such as LBP-TOP  [48]  and HOG-TOP  [4] . However, the property of dependence on prior knowledge limited generalization. With the rise of deep learning, CNN-RNN frameworks became mainstream, which using CNNs ( e.g., VGG  [45] , ResNet  [13] ) for spatial encoding and LSTM/GRU  [20]  for temporal modeling. Unfortunately, such methods often struggle to capture long-range dependencies and are sensitive to distribution shifts, limiting their robustness in real-world scenarios. To jointly learn spatial and temporal features, 3D CNNs such as C3D  [34] , I3D  [1] , and M3DFEL  [37]  have been widely used. M3DFEL incorporates multi-instance learning to suppress noisy frames, while IAL  [21]  introduces intensity loss to enhance inter-category separability. However, these models often struggle with ambiguous boundaries and sample uncertainty.\n\nTo improve representation quality, state-of-the-art methods explore vision-language modeling  [3, 22, 27] , label-guided temporal fusion  [2] , or static-to-dynamic domain adaptation  [14, 16, 47] , aiming to leverage additional supervision. Yet, they are still constrained by reliance on large-scale pre-trained data and limited temporal reasoning capability. Transformer-based architectures have gained popularity due to their ability to capture long-range dependencies through self-attention mechanisms. For instance, former-DFER  [49]  and STT  [28]  integrate spatiotemporal attention to improve robustness against occlusion and motion variation. Nevertheless, these transformer-based DFER models inevitably face challenges in effectively handling sample heterogeneity, noisy transitions, and data distribution shifts, further limiting their applicability to complex real-world scenarios.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Distributionally Robust Optimization",
      "text": "Distributionally Robust Optimization provides a principled framework for improving generalization under sample heterogeneity  [43] . Unlike conventional robust optimization that addresses local input perturbations  [19, 42] , Distributionally Robust Optimization minimizes the worst-case expected loss over a family of plausible data distributions, thereby accounting for global distribution shifts. These uncertainty sets are commonly defined using ùúô-divergence  [7] , Wasserstein distance  [17, 33] , or structural constraints  [5] , which enable the model to handle latent distributional variations beyond observed data noise. Although recent work has extended Distributionally Robust Optimization with learnable uncertainty sets for cross-domain robustness  [31] , directly applying Distributionally Robust Optimization in high-dimensional and temporally-evolving tasks such as DFER still remains challenging.\n\nThus, in this work, we explore Distributionally Robust Optimization as a principled solution to cope with sample heterogeneity in DFER, which arises from variations in visual style, expression dynamics, and individual identity, etc. To address this, we propose heterogeneity-aware Distributionally Robust Optimization framework HDF tailored for DFER. Among HDF, DAM models sample heterogeneity in temporal dynamics and visual styles via dualbranch attention, while DSM stabilizes training under hard samples by adaptively balancing classification and representation objectives based on gradient sensitivity. Furthermore, these components form a unified heterogeneity-aware framework guided by Distributionally Robust Optimization, enabling robust learning from diverse and uncertain dynamic expression samples.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Preliminaries 3.1 Temporal-Frequency Representation",
      "text": "Temporal and frequency representations provide two complementary perspectives for understanding dynamic facial expressions. The temporal domain focuses on the evolution of facial expressions over time, highlighting how expressions change across consecutive frames. Given a video sequence {ùë• 1 , ùë• 2 , . . . , ùë• ùë° }, temporal differences between adjacent frames can be modeled as:\n\nwhich captures frame-wise motion cues and short-term expression dynamics. In parallel, frequency-domain analysis encodes facial appearance using spatial frequency components, providing a robust view of structural texture and detail variations. The Discrete Cosine Transform (DCT)  [15, 32, 51] , commonly used for frequency feature extraction, is applied to each frame ùë• ‚àà R ùêª √óùëä to obtain a frequency representation ùëã ùëì ‚àà R ùêª √óùëä defined as:\n\nwith normalization terms:\n\nThese two representations can be integrated to form a unified feature embedding that incorporates both temporal motion and frequency-based appearance information:\n\nwhere ùëß ùë° and ùëß ùëì are temporal and frequency features respectively, and ùúÜ ùë° , ùúÜ ùëì ‚àà [0, 1] are learnable weights. This formulation provides the theoretical foundation for the time-frequency modeling in our method, enabling downstream modules to capture both motion dynamics and structural variations essential for robust dynamic facial expression representation.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Supervised Contrastive Loss",
      "text": "Contrastive learning (CL)  [50]  aims to learn discriminative representations by pulling semantically similar samples closer and pushing dissimilar ones apart in the embedding space. While traditional CL is commonly unsupervised and constructs positive pairs via data augmentation, supervised contrastive learning (SCL) leverages label information to form more informative positive and negative pairs. Given a batch of ùëÅ samples, let I = {1, . . . , ùëÅ } denote the index set.\n\nFor an anchor sample ùëñ ‚àà I, let ùëÉ (ùëñ) be the set of indices (excluding ùëñ) that share the same class label with ùëñ. The SCL is defined as:\n\nwhere z ùëñ ‚àà R ùëë is the normalized representation of sample ùëñ, ùúè is a temperature scalar, and A (ùëñ) is the set of all contrastive candidates. SCL encourages compact intra-category clusters in the embedding space, reducing the need for hard sample mining and enhancing both classification and generalization.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "We adopt X3D  [8]   First, the input feature x ‚àà R ùêµ√óùëá √óùê∂ √óùêª √óùëä is passed through a DCT convolution to extract frequency features ùëì DCT . To simulate worst-case variations caused by source-level sample heterogeneity and enhance robustness to visual style shifts, adversarial perturbation introduce two types of regularized perturbations to the original frequency features. The first is a sign-based perturbation: ùõº ‚Ä¢ùúñ ‚Ä¢ sign(ùëì DCT ), where sign(‚Ä¢) denotes the element-wise sign function, generating adversarial noise along the gradient direction; ùõº is a learnable weight and ùúñ is a fixed scaling factor. The second is a wasserstein-guided global shift:\n\n, where E[ùëì DCT ] denotes the mini-batch mean of frequency features, reflecting the global distribution center; ‚à• ‚Ä¢ ‚à• 2 is the L2 norm, measuring the deviation between a sample and the global distribution; and ùõΩ is a learnable global perturbation weight. The resulting adversarial frequency feature is:\n\n(6) This design approximates the inner maximization problem in Distributionally Robust Optimization by jointly simulating input-level perturbations and feature-level distributional deviations, enabling the model to better handle frequency-domain variations caused by source-specific heterogeneity. To address activation instability from perturbations, we design a dynamic fitting function that adaptively scales activations:\n\nwhere ùõº is a learnable scaling factor, ùõæ and ùõΩ are affine transformation parameters, and ùúÄ is a small constant for numerical stability. ‚àöÔ∏Å Var(ùëì adv ) denotes the standard deviation for each sample, which reflects the dispersion of activation values. This function dynamically adjusts activation strength according to channel-wise feature statistics, mitigating inter-sample scale variation and improving robustness under visual heterogeneity.\n\nFinally, the dynamically activated feature ùëì dyn is processed by self-attention mechanism to obtain the attention map ùëì att , which is then fused with the original features:\n\nwhere ùúé (‚Ä¢) denotes the sigmoid function. In summary, the frequency branch improves feature robustness against source-induced style variation by jointly leveraging frequency decomposition, adversarial modeling, and adaptive activation fitting.\n\n4.1.2 Temporal Attention Branch: Sequence Consistency Modeling via Wasserstein Regularization. In DFER, individual expression differences exacerbate temporal inconsistencies in frame-level features, challenging consistent trajectory modeling. To mitigate this temporal heterogeneity, we introduce a temporal attention branch that enhances robustness against dynamic inconsistency and irregular expression rhythms.\n\nGiven the input feature ùë•, we compute two temporal descriptors for each frame by applying global average pooling and max pooling over the channel and spatial dimensions, denoted as ùê¥ (ùë° ) avg (global context) and ùê¥ (ùë° ) max (local saliency), respectively. To measure the deviation of the current frame from the global expression trajectory, we introduce a wasserstein-inspired regularization term, approximated as the L2 distance between the average representation of the current frame and the global mean of the sequence:\n\n, which is used to identify outlier or drifting frames that compromise the temporal consistency of the expression sequence. To prevent this wasserstein term from suppressing genuine emotional transitions, we further introduce a local temporal difference term as a compensatory measure:\n\n, which enhances the model's sensitivity to short-term dynamic variations. We then fuse the above components using a gating mechanism to generate the final temporal attention map:\n\nwhere ùõº, ùõΩ, ùõæ, ùõø are learnable fusion weights and ùúé (‚Ä¢) denotes the sigmoid activation function. This attention map adaptively adjusts the contribution of each frame to the overall sequence representation. Finally, TA ùë°,ùë° ‚Ä≤ are passed through a self-attention mechanism to generate a temporal attention map TA, which adaptively highlights consistent frames and suppresses noisy or drifting ones. The attended sequence representation is then fused with the original temporal features as:\n\nwhere ùúé (‚Ä¢) denotes the sigmoid function. In summary, the temporal branch improves sequence-level consistency under heterogeneous conditions by integrating wasserstein-based global deviation detection, local dynamic compensation, and long-range temporal attention, providing robust and coherent temporal representations for the subsequent fusion module.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Adaptive Time-Frequency Fusion: Robust Integration Of Heterogeneous Modalities.",
      "text": "To unify the complementary strengths of the temporal and frequency branches under sample heterogeneity, we introduce an adaptive time-frequency fusion mechanism that dynamically adjusts the contribution of each modality based on input-specific characteristics. Concretely, we apply global average pooling to the outputs of the temporal branch ùë• ùë° and the frequency branch ùë• ùë† , obtaining compact descriptors xùë° ùëéùëõùëë xùë† , respectively. These descriptors are fed into learnable gating functions to compute soft fusion weights:\n\nwith a normalization constraint ùúÜ ùë° + ùúÜ ùë† = 1. The fused feature representation is then computed as:\n\nThis fusion mechanism adaptively integrates temporal and frequency information based on input dynamics, allowing the model to attend more to temporal cues under sequence inconsistency and to frequency cues under style-level variations. The resulting representation is thus more robust to sample heterogeneity and better suited for expression classification.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Distribution-Aware Robust Scaling For Dynamic Expression Learning",
      "text": "We propose a novel optimization module called Distribution-aware Scaling Module (DSM), which aims to enhance model robustness and generalization in multi-source heterogeneous data scenarios. DSM integrates two complementary components: a supervised contrastive loss (SCL) with information bottleneck regularization and distributionally robust reweighting, and a gradient-driven adaptive loss scaling strategy inspired by Sharpness-Aware Minimization (SAM)  [9] . These modules work in tandem to enable the model to learn compact and discriminative representations, while dynamically adjusting the objective weights according to the optimization state throughout training.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Information-Constrained Distributionally-Robust Contrastive",
      "text": "Loss. In DFER, hard samples caused by sample heterogeneity not only increase feature distribution diversity but also significantly undermine the stability of positive-negative discrimination in contrastive learning. To address this, we propose an information bottleneck and Distributionally Robust Optimization enhanced SCL that effectively mitigates the instability caused by hard samples during training. First, to reduce the dominance of hard negative samples in the contrastive loss, we introduce a gaussian kernel-based reweighting strategy into SCL, assigning distributionally robust weights to each negative pair based on their similarity. This design suppresses the risk of overfitting to extreme samples at the distribution level.\n\nLet the normalized feature representations be ùëß ùëñ , ùëß ùëó ‚àà R ùëë , and the similarity between sample ùëñ and ùëó is defined as:\n\nùúè , where ùúè is a temperature parameter. For each negative sample pair (ùëñ, ùëó), we construct a gaussian robust weight term based on the deviation of their similarity from the ùúÇ: ùë§ ùëñ ùëó = exp -\n\n, where\n\nùúÇ is a hyperparameter that defines the center and smoothness of the gaussian weighting curve, reflecting the expected similarity between well-aligned features, and adjusts how strongly outlier negatives are down weighted. The resulting distributionally-robust SCL is defined as:\n\nwhere ùëÉ (ùëñ) denotes the set of positive samples of anchor ùëñ. This design significantly reduces the contribution of alignment errors with hard negatives in the loss function, thereby enhancing the stability of the optimization.\n\nOn the other hand, we further incorporate the information bottleneck principle by minimizing the mutual information between the representation and the input, aiming to compress redundant information and enhance the task relevance and discriminability of the learned features. To incorporate the information bottleneck constraint, we define the feature covariance matrix as:\n\nand approximate the information bottleneck regularization term as: L IB = Tr(Cov(ùëç )). The final IB-Distributionally Robust Optimization SCL is:\n\nThis strategy mitigates the influence of hard negatives and enforces compact, informative representations for robust contrastive learning under sample heterogeneity. Let the gradient norm of the total loss be: ùëî = ‚à•‚àá ùë§ L total ‚à• 2 , then compute the dynamic weights for each loss term as:\n\nwhere ùõº and ùõΩ are base scaling coefficients. This strategy allows the model to emphasize classification loss during difficult training phases (large ùëî) for stability, and focus on contrastive refinement during easier phases (small ùëî).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Overall",
      "text": "Objective. The overall objective of DSM is defined as follows:\n\nThis optimization strategy enables per-batch awareness of training status and feature structure dynamics, achieving self-regulated balancing between classification and contrastive learning. As a result, DSM significantly improves model robustness and generalization under real-world distribution shifts and individual variations.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experiments 5.1 Experimental Setup",
      "text": "5.1.1 Datasets. We conduct extensive experiments on two popular in-the-wild DFER datasets, namely DFEW  [18]  and FERV39k  [40] . DFEW is a large-scale dynamic expressions in-the-wild dataset introduced in 2020, containing over 16000 video clips. These clips are collected from more than 1500 films worldwide and include various challenging disturbances such as extreme lighting and pose changes. Each clip is annotated by ten trained annotators under professional guidance and assigned to one of seven basic expressions: happy, sad, neutral, angry, surprised, disgusted, and fearful.\n\nFERV39k is the largest available in-the-wild DFER dataset, containing 38935 video clips collected from 4 scenes, further subdivided into 22 fine-grained scenes. It is the first DFER dataset with largescale 39K clips, scene-to-scene segmentation, and cross-domain support. Each video clip in FERV39k is annotated by 30 professional annotators to ensure high-quality labeling and assigned to one of the same seven main expressions as DFEW. We use the training and test sets provided by FERV39k for fair comparison.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Metrics.",
      "text": "In all experiments, to be consistent with previous methods, we choose weighted average recall (WAR) and unweighted average recall (UAR) as metrics. In reality, WAR and UAR hold different significance, i.e., WAR reflects the model's overall effectiveness in real-world deployments, as it is influenced by major categories with a large number of samples. Since UAR can better reflect the model's performance in each category, more attention can be obtained by researchers on the model's balanced performance across various categories.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Implementation Details.",
      "text": "Our entire framework is implemented using PyTorch-GPU and trained on a single NVIDIA RTX A6000 GPU. In our experiment, all face images are resized to 160√ó160. We use augmentation techniques such as random cropping, horizontal flipping, and 0.4 color jitter. For each video, 16 frames are extracted as samples. The feature extraction network use the standard X3D model with pre-trained weights from Torchvision. The model is trained using AdamW as the base optimizer in combination with a cosine scheduler for a total of 100 epochs, including a 20-epoch warm-up period. The learning rate is set to 5e-4, with a minimum learning rate of 5e-6, and weight decay is set to 0.05. Based on prior work  [43] , the ùúè and ùúÇ in DSM are set to 0.07 and 0.2 for stable and effective representation learning.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With The State-Of-The-Art Methods",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results On Dfew.",
      "text": "Following previous works, we conduct experiments under a 5-fd cross-validation protocol. As shown in Tab. 1, HDF achieves the best performance in both WAR and UAR metrics. In particular, it achieves consistently better performance on underrepresented and challenging categories, even surpassing some vision-language models, highlighting its strong generalization to heterogeneous and hard-to-learn samples. This demonstrates the effectiveness of our method in not only improving overall recognition accuracy, but also enhancing robustness under sample imbalance and expression diversity.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Method Metrics (%)",
      "text": "WAR UAR 2ResNet18+LSTM  [40]  43.20 31.28 Former-DFER  [49]  46.85 37.20 3D-DSwin  [39]  46.98 37.66 GCA+IAL  [21]  48.54 35.82 M3DFEL  [37]  47.67 35.94 LG-DSTF  [47]  48.19 39.84 RDFER  [25]  48   3 : Ablation study of different components in HDF on DFEW fd5. in mitigating performance degradation caused by sample heterogeneity and distributional variations. Notably, we do not introduce any dataset-specific hyper-parameter tuning, applying the same training configuration to both DFEW and FERV39k. Despite the scale differences between the two datasets, our method consistently delivers strong results, highlighting its stability and ease of deployment in real-world scenarios without requiring extensive parameter adjustment.  5.4 Visualization.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Studies",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Learned Feature Maps Visualization.",
      "text": "To verify the robustness of the proposed HDF under challenging conditions, we visualize the learned facial feature maps, as shown in Fig.  3 , including cases with low illumination, occlusion, and non-frontal poses. In the first sequence, our method effectively suppresses the influence of occluded regions by focusing on informative visible areas, showcasing robustness against spatial ambiguity. In the second sequence, despite non-frontal views, our features still capture expressive regions, highlighting resilience to pose-induced heterogeneity. Furthermore, across frames with strong head motion, the model consistently attends to semantically stable regions, demonstrating robustness against temporal noise and maintaining reliable representation under complex variations.  separable. This confirms that our framework effectively mitigates representation ambiguity caused by sample heterogeneity, allowing the model to learn more robust and compact feature embeddings for DFER tasks.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Visualization Of Loss Landscape.",
      "text": "To evaluate the effect of our DSM on training dynamics, we visualize the loss landscape  [23] , as shown in Fig.  5 . Compared with the version without DSM (left), our full HDF (right) exhibits a significantly flatter and smoother loss surface. This demonstrates that DSM effectively suppresses gradient instability induced by hard or uncertain samples, guiding the model toward more stable optima. The improved smoothness highlights enhanced convergence stability and generalization capability, particularly in the presence of sample heterogeneity and hard samples caused by temporal ambiguity, style variation, or underrepresented categories in DFER.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusion",
      "text": "We propose HDF, a heterogeneity-aware optimization framework tailored for in-the-wild DFER, which explicitly addresses samplelevel variations in style, dynamics, and category distribution. It integrates two key components: a dual-branch attention module, DAM, that performs time-frequency modeling to enhance robustness against visual inconsistency and temporal ambiguity, and a dynamic optimization strategy, DSM, which adaptively balances classification and contrastive objectives based on training dynamics to further mitigate the impact of hard or uncertain samples on representation learning. Together, these modules form a unified Distributionally Robust Optimization-based solution that effectively improves representation robustness and training stability under heterogeneous data conditions. Extensive experiments on DFEW and FERV39k demonstrate the superiority and generalizability of HDF. In future work, we plan to extend this framework to multi-modal expression analysis and investigate deployment-friendly variants for real-world applications.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Due to a series of Sample Heterogeneity problems,",
      "page": 2
    },
    {
      "caption": "Figure 1: , due to the lack of mechanisms to han-",
      "page": 2
    },
    {
      "caption": "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-",
      "page": 5
    },
    {
      "caption": "Figure 3: Visualization of the learned feature maps. There are three sequences are presented, which including the issues of the",
      "page": 7
    },
    {
      "caption": "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.",
      "page": 7
    },
    {
      "caption": "Figure 3: , including cases",
      "page": 8
    },
    {
      "caption": "Figure 4: , the baseline produces scattered and overlapping fea-",
      "page": 8
    },
    {
      "caption": "Figure 5: The visualization of separate loss landscape on",
      "page": 8
    },
    {
      "caption": "Figure 5: Compared with the version without DSM (left),",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial": ""
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial": "Anyang Tong‚àó"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial": "Hefei University of"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial": "Technology"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial": "Hefei, China"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial": "tonganyang@mail.hfut.edu.cn"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Technology\nTechnology\nSingapore, Singapore": "Hefei, China\nHefei, China\nzhang_jie@cfar.a-"
        },
        {
          "Technology\nTechnology\nSingapore, Singapore": "tonganyang@mail.hfut.edu.cn\nhjy@hfut.edu.cn\nstar.edu.sg"
        },
        {
          "Technology\nTechnology\nSingapore, Singapore": "Dan Guo\nZhi Liu\nMeng Wang"
        },
        {
          "Technology\nTechnology\nSingapore, Singapore": "Hefei University of\nThe University of\nHefei University of"
        },
        {
          "Technology\nTechnology\nSingapore, Singapore": "Technology\nElectro-Communications\nTechnology"
        },
        {
          "Technology\nTechnology\nSingapore, Singapore": "Hefei, China\nTokyo, Japan\nHefei, China"
        },
        {
          "Technology\nTechnology\nSingapore, Singapore": "eric.mengwang@gmail.com\nguodan@hfut.edu.cn\nliuzhi@uec.ac.jp"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Hefei, China\nTokyo, Japan": "guodan@hfut.edu.cn\nliuzhi@uec.ac.jp",
          "Hefei, China": "eric.mengwang@gmail.com"
        },
        {
          "Hefei, China\nTokyo, Japan": "Abstract",
          "Hefei, China": "Keywords"
        },
        {
          "Hefei, China\nTokyo, Japan": "Dynamic Facial Expression Recognition (DFER) plays a critical",
          "Hefei, China": "Dynamic facial expression recognition, distributionally robust opti-"
        },
        {
          "Hefei, China\nTokyo, Japan": "role in affective computing and human-computer interaction. Al-",
          "Hefei, China": "mization, contrastive learning, time-frequency analysis"
        },
        {
          "Hefei, China\nTokyo, Japan": "though existing methods achieve comparable performance, they",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "ACM Reference Format:"
        },
        {
          "Hefei, China\nTokyo, Japan": "inevitably suffer from performance degradation under sample het-",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu,"
        },
        {
          "Hefei, China\nTokyo, Japan": "erogeneity caused by multi-source data and individual expression",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "and Meng Wang. 2025. Learning from Heterogeneity: Generalizing Dynamic"
        },
        {
          "Hefei, China\nTokyo, Japan": "variability. To address these challenges, we propose a novel frame-",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "Facial Expression Recognition via Distributionally Robust Optimization. In"
        },
        {
          "Hefei, China\nTokyo, Japan": "work, called Heterogeneity-aware Distributional Framework",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "Proceedings of the 33rd ACM International Conference on Multimedia (MM"
        },
        {
          "Hefei, China\nTokyo, Japan": "(HDF), and design two plug-and-play modules to enhance time-",
          "Hefei, China": "‚Äô25), October 27‚Äì31, 2025, Dublin, Ireland. ACM, New York, NY, USA, 10 pages."
        },
        {
          "Hefei, China\nTokyo, Japan": "frequency modeling and mitigate optimization imbalance caused by",
          "Hefei, China": "https://doi.org/10.1145/3746027.3755036"
        },
        {
          "Hefei, China\nTokyo, Japan": "hard samples. Specifically, the Time-Frequency Distributional",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "Attention Module (DAM) captures both temporal consistency",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "1\nIntroduction"
        },
        {
          "Hefei, China\nTokyo, Japan": "and frequency robustness through a dual-branch attention design,",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "Facial expression recognition (FER) can automatically perceive hu-"
        },
        {
          "Hefei, China\nTokyo, Japan": "improving tolerance to sequence inconsistency and visual style",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "man emotions and has been widely applied in real-world scenarios"
        },
        {
          "Hefei, China\nTokyo, Japan": "shifts. Then, based on gradient sensitivity and information bottle-",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "such as psychological evaluation [10, 11, 35], and human-computer"
        },
        {
          "Hefei, China\nTokyo, Japan": "neck principles, an adaptive optimization module Distribution-",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "interaction [6, 12, 44]. According to the data type, FER approaches"
        },
        {
          "Hefei, China\nTokyo, Japan": "aware Scaling Module (DSM) is introduced to dynamically bal-",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "can be divided into two categories: Static FER (SFER) [26] and Dy-"
        },
        {
          "Hefei, China\nTokyo, Japan": "ance classification and contrastive losses, enabling more stable and",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "namic FER (DFER) [47]. SFER methods focus on analyzing single"
        },
        {
          "Hefei, China\nTokyo, Japan": "discriminative representation learning. Extensive experiments on",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "still images to classify emotions [29], which limits their capacity"
        },
        {
          "Hefei, China\nTokyo, Japan": "two widely used datasets, DFEW and FERV39k, demonstrate that",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "to capture the dynamic progression and contextual transitions of"
        },
        {
          "Hefei, China\nTokyo, Japan": "HDF significantly improves both recognition accuracy and robust-",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "expressions across time [24], leading to degraded performance in"
        },
        {
          "Hefei, China\nTokyo, Japan": "ness. Our method achieves superior weighted average recall (WAR)",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "realistic settings that involve temporal complexity and motion vari-"
        },
        {
          "Hefei, China\nTokyo, Japan": "and unweighted average recall\n(UAR) while maintaining strong",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "ance. In contrast, DFER techniques utilize video sequences to model"
        },
        {
          "Hefei, China\nTokyo, Japan": "generalization across diverse and imbalanced scenarios. Codes are",
          "Hefei, China": ""
        },
        {
          "Hefei, China\nTokyo, Japan": "",
          "Hefei, China": "how facial expressions evolve over time, offering a more faithful"
        },
        {
          "Hefei, China\nTokyo, Japan": "released at https://github.com/QIcita/HDF_DFER.",
          "Hefei, China": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "modules that can be seamlessly integrated into existing general-"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "purpose video backbone networks. First, to construct more stable"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "and generalizable expression representations under sample het-"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "erogeneity, we design the Time-Frequency Distributional At-"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tention Module (DAM). By leveraging wasserstein-regularized"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "temporal attention and frequency-domain adversarial perturba-"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tion to capture highly uncertain expression dynamics and suppress"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "distributional variations in style and resolution, DAM adopts a"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "dual-branch architecture to perform adaptive and robust modeling"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "in both temporal and frequency domains. To further mitigate the"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "optimization instability and representation degradation caused by"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "hard samples, we introduce a novel Distribution-aware Scaling"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Module (DSM). By combining an information-constrained and"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "reweighted supervised contrastive loss with a sharpness-aware loss"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "scaling mechanism driven by gradient norm, DSM constructs a"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "dual-enhanced optimization strategy that adaptively balances clas-"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "sification and contrastive learning objectives, thereby improving"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "both the robustness and discriminability of learned representations"
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "under heterogeneous conditions."
        },
        {
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Totally, our main contributions can be concluded as follows:"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "both the robustness and discriminability of learned representations": "under heterogeneous conditions."
        },
        {
          "both the robustness and discriminability of learned representations": "Totally, our main contributions can be concluded as follows:"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "‚Ä¢ To the best of our knowledge, this paper is the first attempt"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "to introduce the principle of Distributionally Robust Opti-"
        },
        {
          "both the robustness and discriminability of learned representations": "mization into DFER. By joint modeling of style variations,"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "individual expression dynamics, and task-level distribution"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "shifts across both feature and optimization levels, we con-"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "struct HDF that explicitly addresses ubiquitous sample het-"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "erogeneity."
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "‚Ä¢ A novel distributionally robust attention module DAM is"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "first proposed by adopting a dual-branch structure to jointly"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "model temporal consistency and frequency robustness un-"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "der sample heterogeneity. The frequency branch integrates"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "adversarial perturbation, and dynamic activation to improve"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "generalization across diverse visual styles, while the tem-"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "poral branch leverages wasserstein-regularized attention to"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "stabilize expression evolution across individuals."
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "‚Ä¢ We introduce a new optimization strategy named DSM, which"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "combines distributionally robust contrastive learning with"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "gradient-inspired adaptive loss scaling mechanism. This dual"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "design enables dynamic balancing between classification su-"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "pervision and representation learning, improving training"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "stability and generalization under multi-source and hetero-"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "geneous sample scenarios."
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "‚Ä¢ Extensive experiments conducted on two challenging in-the-"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "wild DFER datasets, DFEW and FERV39k, demonstrate the"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "effectiveness of our approach. The proposed HDF signifi-"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "cantly outperforms state-of-the-art methods in recognition"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "accuracy, and robustness to sample heterogeneity."
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "2\nRelated work"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "2.1\nDynamic Facial Expression Recognition"
        },
        {
          "both the robustness and discriminability of learned representations": ""
        },
        {
          "both the robustness and discriminability of learned representations": "Dynamic Facial Expression Recognition (DFER) focuses on model-"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "frameworks became mainstream, which using CNNs ( e.g., VGG\n3\nPreliminaries"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[45], ResNet [13])\nfor spatial encoding and LSTM/GRU [20] for"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "3.1\nTemporal-Frequency Representation"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "temporal modeling. Unfortunately, such methods often struggle to"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Temporal and frequency representations provide two complemen-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "capture long-range dependencies and are sensitive to distribution"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tary perspectives for understanding dynamic facial expressions."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "shifts, limiting their robustness in real-world scenarios. To jointly"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "The temporal domain focuses on the evolution of facial expressions"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "learn spatial and temporal features, 3D CNNs such as C3D [34], I3D"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "over time, highlighting how expressions change across consecutive"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[1], and M3DFEL [37] have been widely used. M3DFEL incorporates"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "temporal differ-\nframes. Given a video sequence {ùë•1, ùë•2, . . . , ùë•ùë° },"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "multi-instance learning to suppress noisy frames, while IAL [21]"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ences between adjacent frames can be modeled as:"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "introduces intensity loss to enhance inter-category separability."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "However, these models often struggle with ambiguous boundaries"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(1)\nŒîùë•ùë° = ùë•ùë° ‚àí ùë•ùë° ‚àí1,"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and sample uncertainty."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "which captures frame-wise motion cues and short-term expression"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "To improve representation quality, state-of-the-art methods ex-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "dynamics.\nIn parallel, frequency-domain analysis encodes facial"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "plore vision-language modeling [3, 22, 27], label-guided temporal"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "appearance using spatial frequency components, providing a robust"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "fusion [2], or static-to-dynamic domain adaptation [14, 16, 47], aim-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "view of structural texture and detail variations. The Discrete Cosine"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ing to leverage additional supervision. Yet, they are still constrained"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Transform (DCT) [15, 32, 51], commonly used for frequency feature"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "by reliance on large-scale pre-trained data and limited temporal"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "extraction, is applied to each frame ùë• ‚àà Rùêª √óùëä to obtain a frequency"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "reasoning capability. Transformer-based architectures have gained"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "representation ùëãùëì\n‚àà Rùêª √óùëä defined as:"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "popularity due to their ability to capture long-range dependencies"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(cid:21)\n(cid:21)"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "through self-attention mechanisms. For instance, former-DFER [49]\n(cid:20) ùúã (2ùëñ + 1)ùë¢\n(cid:20) ùúã (2ùëó + 1)ùë£"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "‚àëÔ∏Å ùëñ\n,\nùëãùëì (ùë¢, ùë£) = ùõº (ùë¢)ùõº (ùë£)\nùë• (ùëñ, ùëó)¬∑cos\ncos"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and STT [28] integrate spatiotemporal attention to improve robust-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2ùêª\n2ùëä"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": ",ùëó"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ness against occlusion and motion variation. Nevertheless, these"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(2)"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "transformer-based DFER models inevitably face challenges in effec-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "with normalization terms:"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tively handling sample heterogeneity, noisy transitions, and data"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "‚àöÔ∏É 1\n‚àöÔ∏É 1\nÔ£±\nÔ£±\ndistribution shifts, further limiting their applicability to complex"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "if ùë£ = 0\nif ùë¢ = 0\nÔ£¥\nÔ£¥\nùëä ,\nùêª ,"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Ô£¥\nÔ£¥"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": ",\n.\nùõº (ùë¢) =\nùõº (ùë£) =\n(3)"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "real-world scenarios."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "‚àöÔ∏É 2\n‚àöÔ∏É 2"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Ô£≤ Ô£¥Ô£¥Ô£≥\nÔ£≤ Ô£¥Ô£¥Ô£≥\notherwise\notherwise\nùëä ,\nùêª ,"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "4\nMethodology",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "source-specific heterogeneity. To address activation instability from"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "perturbations, we design a dynamic fitting function that adaptively"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "We adopt X3D [8] as the feature extraction backbone and embed",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "scales activations:"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "the two plug-and-play modules (the Time-Frequency Distributional",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(cid:32)\n(cid:33)"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Attention Module (DAM) in Sec. 4.2 and the Distribution-aware Scal-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ùõº"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "¬∑ ùõæ + ùõΩ,"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ing Module (DSM) in Sec. 4.3) we designed. Specifically, DAM en-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(7)\nùëìdyn = tanh\n¬∑ ùëìadv"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "‚àöÔ∏ÅVar(ùëìadv) + ùúÄ"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "hances feature robustness under sample heterogeneity by model-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ing temporal consistency and frequency stability. DSM adaptively",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "where ùõº is a learnable scaling factor, ùõæ and ùõΩ are affine transforma-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "balances classification and contrastive losses based on gradient",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tion parameters, and ùúÄ is a small constant for numerical stability."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "sharpness, enabling more stable and discriminative optimization.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "‚àöÔ∏ÅVar(ùëìadv) denotes the standard deviation for each sample, which"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "The workflow of the proposed HDF is illustrated in Fig. 2.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "reflects the dispersion of activation values. This function dynami-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "cally adjusts activation strength according to channel-wise feature"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "4.1\nDistributionally-Robust Attention on",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "statistics, mitigating inter-sample scale variation and improving"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Frequency and Time",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "robustness under visual heterogeneity."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Finally, the dynamically activated feature ùëìdyn is processed by"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "We propose Time-Frequency Distributional Attention Module (DAM),",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "self-attention mechanism to obtain the attention map ùëìatt, which is"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "a dual-branch attention architecture designed to jointly enhance",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "then fused with the original features:"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "temporal consistency and style robustness in dynamic facial ex-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "pression representation. Sample heterogeneity driven by source",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(8)\nùë•ùë† = ùë• ¬∑ ùúé (ùëìatt) + ùë•,"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "diversity and individual differences disrupts stable training and",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "where ùúé (¬∑) denotes the sigmoid function. In summary, the frequency"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "degrades feature consistency, which are insufficiently handled by",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "branch improves feature robustness against source-induced style"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "conventional empirical risk minimization. To address this issue,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "variation by jointly leveraging frequency decomposition, adversar-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "DAM incorporates a temporal attention branch and a frequency",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ial modeling, and adaptive activation fitting."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "attention branch, both guided by Distributionally Robust Optimiza-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tion principles. These branches are responsible for modeling tempo-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "4.1.2\nTemporal Attention Branch: Sequence Consistency Modeling"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ral inconsistency and visual heterogeneity, respectively. A learnable",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "In DFER, individual expression dif-\nvia Wasserstein Regularization."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "fusion mechanism further adaptively integrates the two modalities,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ferences exacerbate temporal\ninconsistencies in frame-level\nfea-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "enhancing feature stability and generalization under diverse and",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tures, challenging consistent trajectory modeling. To mitigate this"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "heterogeneous expression conditions.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "temporal heterogeneity, we introduce a temporal attention branch"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "that enhances robustness against dynamic inconsistency and irreg-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "4.1.1\nFrequency Attention Branch: Distributionally Robust Modeling",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ular expression rhythms."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "via Adversarial Perturbation and Dynamic Fitting. To address sample",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Given the input feature ùë•, we compute two temporal descrip-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "heterogeneity caused by variations in visual style, image quality,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tors for each frame by applying global average pooling and max"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and illumination, DAM introduces a frequency attention branch,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "which integrates DCT-based frequency decomposition, distribution-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "pooling over the channel and spatial dimensions, denoted as ùê¥(ùë° )\navg"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "aware adversarial perturbation, and dynamic activation adjustment",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(global context) and ùê¥(ùë° )\nmax (local saliency), respectively. To mea-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "to improve robustness against source-specific visual shifts.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "sure the deviation of\nthe current\nframe from the global expres-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "First, the input feature x ‚àà Rùêµ√óùëá √óùê∂ √óùêª √óùëä is passed through a",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "sion trajectory, we introduce a wasserstein-inspired regularization"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "DCT convolution to extract frequency features ùëìDCT. To simulate",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "term, approximated as the L2 distance between the average rep-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "worst-case variations caused by source-level sample heterogene-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "resentation of\nthe current\nframe and the global mean of\nthe se-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ity and enhance robustness to visual style shifts, adversarial per-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ùê¥(ùë° )"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)\n=\nquence: W (ùë° )\n, which is used to identify\navg ‚àí Eùë° ‚Ä≤ [ùê¥(ùë° ‚Ä≤ )\navg ]\nglobal"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "turbation introduce two types of regularized perturbations to the",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "2"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "outlier or drifting frames that compromise the temporal consis-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "original frequency features. The first is a sign-based perturbation:",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tency of the expression sequence. To prevent this wasserstein term"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ùõº ¬∑ùúñ ¬∑ sign(ùëìDCT), where sign(¬∑) denotes the element-wise sign func-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "from suppressing genuine emotional transitions, we further intro-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tion, generating adversarial noise along the gradient direction; ùõº is",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "duce a local temporal difference term as a compensatory measure:"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "a learnable weight and ùúñ is a fixed scaling factor. The second is a",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ùê¥(ùë° )\nD (ùë° )"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "wasserstein-guided global shift: ùõΩ ¬∑ sign(ùëìDCT) ¬∑ ‚à•ùëìDCT ‚àí E[ùëìDCT] ‚à•2,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(cid:13)(cid:13)(cid:13)\n(cid:13)(cid:13)(cid:13)\n=\n, which enhances the model‚Äôs sensitivity\navg ‚àí ùê¥(ùë° ‚àí1)\nlocal"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "2"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "where E[ùëìDCT] denotes the mini-batch mean of frequency features,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "to short-term dynamic variations. We then fuse the above com-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "reflecting the global distribution center; ‚à• ¬∑ ‚à•2is the L2 norm, mea-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ponents using a gating mechanism to generate the final temporal"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "suring the deviation between a sample and the global distribution;",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "attention map:"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and ùõΩ is a learnable global perturbation weight. The resulting ad-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(cid:17)\n(cid:16)"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ","
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "versarial frequency feature is:",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(9)\nTAùë°,ùë° ‚Ä≤ = ùúé\nùõæ (ùõº Àúùê¥avg + ùõΩ Àúùê¥max) + ùõø ¬∑ Wglobal ‚àí Dlocal"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": ".",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ùëìadv = ùëìDCT +ùõº ¬∑ùúñ ¬∑sign(ùëìDCT) +ùõΩ ¬∑sign(ùëìDCT) ¬∑ ‚à•ùëìDCT ‚àí E[ùëìDCT] ‚à•2",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "where ùõº, ùõΩ, ùõæ, ùõø are learnable fusion weights and ùúé (¬∑) denotes the"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(6)",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "sigmoid activation function. This attention map adaptively adjusts"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "This design approximates the inner maximization problem in Dis-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "the contribution of each frame to the overall sequence representa-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tributionally Robust Optimization by jointly simulating input-level",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tion. Finally, TAùë°,ùë° ‚Ä≤ are passed through a self-attention mechanism"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "perturbations and feature-level distributional deviations, enabling",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "to generate a temporal attention map TA, which adaptively high-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "the model to better handle frequency-domain variations caused by",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "lights consistent frames and suppresses noisy or drifting ones. The"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "4.2\nDistribution-Aware Robust Scaling for"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "Dynamic Expression Learning"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "We propose a novel optimization module called Distribution-aware"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "Scaling Module (DSM), which aims to enhance model robustness"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "and generalization in multi-source heterogeneous data scenarios."
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "DSM integrates two complementary components: a supervised con-"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "trastive loss (SCL) with information bottleneck regularization and"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "distributionally robust reweighting, and a gradient-driven adaptive"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "loss scaling strategy inspired by Sharpness-Aware Minimization"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "(SAM) [9]. These modules work in tandem to enable the model to"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "learn compact and discriminative representations, while dynami-"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "cally adjusting the objective weights according to the optimization"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "state throughout training."
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "4.2.1\nInformation-Constrained Distributionally-Robust Contrastive"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "In DFER, hard samples caused by sample heterogeneity not\nLoss."
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "only increase feature distribution diversity but also significantly"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "undermine the stability of positive-negative discrimination in con-"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "trastive learning. To address this, we propose an information bottle-"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "neck and Distributionally Robust Optimization enhanced SCL that"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "effectively mitigates the instability caused by hard samples during"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "training. First, to reduce the dominance of hard negative samples in"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "the contrastive loss, we introduce a gaussian kernel-based reweight-"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "ing strategy into SCL, assigning distributionally robust weights to"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "each negative pair based on their similarity. This design suppresses"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "the risk of overfitting to extreme samples at the distribution level."
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "Let the normalized feature representations be ùëßùëñ, ùëß ùëó ‚àà Rùëë , and the"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "ùëß‚ä§"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "ùëñ ùëß ùëó"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "similarity between sample ùëñ and ùëó\nis defined as: ùë†ùëñ ùëó =\n, where\nùúè"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "ùúè is a temperature parameter. For each negative sample pair (ùëñ, ùëó),"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "we construct a gaussian robust weight term based on the devia-"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": ""
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "(cid:17)\n(cid:16)\n(ùë†ùëñ ùëó ‚àíùúÇ ) 2"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "‚àí\ntion of their similarity from the ùúÇ: ùë§ùëñ ùëó = exp\n, where"
        },
        {
          "Figure 2: An overview of the proposed HDF. (a) The overall processing flow of HDF. (b) The pipeline of The pipeline of Time-": "2ùúÇ2"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ùúÇ is a hyperparameter that defines the center and smoothness of",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "5\nExperiments"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "the gaussian weighting curve, reflecting the expected similarity",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "5.1\nExperimental Setup"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "between well-aligned features, and adjusts how strongly outlier",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "5.1.1\nDatasets. We conduct extensive experiments on two popular"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "negatives are down weighted. The resulting distributionally-robust",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "in-the-wild DFER datasets, namely DFEW [18] and FERV39k [40]."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "SCL is defined as:",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "DFEW is a large-scale dynamic expressions in-the-wild dataset"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "exp(ùë†ùëñùëù )\n1",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": ",\nL (ùëñ )\n= ‚àí\n‚àëÔ∏Å ùëù ‚àà ùëÉ (ùëñ) log\n(13)",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "introduced in 2020, containing over 16000 video clips. These clips"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "D-SC\n|ùëÉ (ùëñ)|\n(cid:205)ùëé‚â†ùëñ ùë§ùëñùëé ¬∑ exp(ùë†ùëñùëé)",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "are collected from more than 1500 films worldwide and include"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "various challenging disturbances such as extreme lighting and pose"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "where ùëÉ (ùëñ) denotes the set of positive samples of anchor ùëñ. This",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "changes. Each clip is annotated by ten trained annotators under pro-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "design significantly reduces the contribution of alignment errors",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "fessional guidance and assigned to one of seven basic expressions:"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "with hard negatives in the loss function, thereby enhancing the",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "happy, sad, neutral, angry, surprised, disgusted, and fearful."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "stability of the optimization.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "FERV39k is the largest available in-the-wild DFER dataset, con-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "On the other hand, we further incorporate the information bot-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "taining 38935 video clips collected from 4 scenes, further subdivided"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tleneck principle by minimizing the mutual information between",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "into 22 fine-grained scenes. It is the first DFER dataset with large-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "the representation and the input, aiming to compress redundant",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "scale 39K clips, scene-to-scene segmentation, and cross-domain"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "information and enhance the task relevance and discriminability",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "support. Each video clip in FERV39k is annotated by 30 profes-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "of the learned features. To incorporate the information bottleneck",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "sional annotators to ensure high-quality labeling and assigned to"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "constraint, we define the feature covariance matrix as:",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "one of\nthe same seven main expressions as DFEW. We use the"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "training and test sets provided by FERV39k for fair comparison."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "1 ùëÅ\nùëÅ‚àëÔ∏Å ùëñ\n(ùëßùëñ ‚àí ¬Øùëß)(ùëßùëñ ‚àí ¬Øùëß)‚ä§,\n(14)\nCov(ùëç ) =",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "=1",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "In all experiments, to be consistent with previous\n5.1.2\nMetrics."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and approximate the information bottleneck regularization term",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "methods, we choose weighted average recall (WAR) and unweighted"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "as: LIB = Tr(Cov(ùëç )). The final IB-Distributionally Robust Opti-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "average recall (UAR) as metrics. In reality, WAR and UAR hold differ-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "mization SCL is:",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ent significance, i.e., WAR reflects the model‚Äôs overall effectiveness"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "in real-world deployments, as it is influenced by major categories"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "1 ùëÅ\nùëÅ‚àëÔ∏Å ùëñ\nL (ùëñ )\n+ ùõΩ ¬∑ Tr(Cov(ùëç )).\n(15)\nLIB-D-SC =",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "with a large number of samples. Since UAR can better reflect the"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "D-SC",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "model‚Äôs performance in each category, more attention can be ob-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "=1",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tained by researchers on the model‚Äôs balanced performance across"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "This strategy mitigates the influence of hard negatives and enforces",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "various categories."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "compact, informative representations for robust contrastive learn-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ing under sample heterogeneity.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "5.1.3\nImplementation Details. Our entire framework is implemented"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "using PyTorch-GPU and trained on a single NVIDIA RTX A6000"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "4.2.2\nGradient-Driven Adaptive Loss Scaling. This strategy addresses",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "GPU. In our experiment, all face images are resized to 160√ó160. We"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "loss conflicts caused by varying sample difficulty in dynamic facial",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "use augmentation techniques such as random cropping, horizontal"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "expression recognition. By leveraging a gradient-sensitivity-based",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "flipping, and 0.4 color jitter. For each video, 16 frames are extracted"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "adaptive scaling mechanism, it dynamically adjusts the weights of",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "as samples. The feature extraction network use the standard X3D"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "classification and contrastive losses to improve training stability",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "model with pre-trained weights from Torchvision. The model\nis"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and convergence. Inspired by SAM, we use the norm of adversarial",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "trained using AdamW as the base optimizer in combination with"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "gradient perturbations to estimate batch-level training difficulty.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "a cosine scheduler for a total of 100 epochs, including a 20-epoch"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Let the gradient norm of the total loss be: ùëî = ‚à•‚àáùë§ Ltotal ‚à•2, then",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "warm-up period. The learning rate is set to 5e-4, with a minimum"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "compute the dynamic weights for each loss term as:",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "learning rate of 5e-6, and weight decay is set to 0.05. Based on prior"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ùëî",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "1",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "work [43], the ùúè and ùúÇ in DSM are set to 0.07 and 0.2 for stable and"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "¬∑ ùõº,\n¬∑ ùõΩ,\n(16)\nùúÜCE =\nùúÜSC =",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ùëî + 1\nùëî + 1",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "effective representation learning."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "where ùõº and ùõΩ are base scaling coefficients. This strategy allows",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "the model to emphasize classification loss during difficult training",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "5.2\nComparison with the State-of-the-art"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "phases (large ùëî) for stability, and focus on contrastive refinement",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Methods"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "during easier phases (small ùëî).",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "5.2.1\nResults on DFEW. Following previous works, we conduct"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "experiments under a 5-fd cross-validation protocol. As shown in"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "4.2.3\nOverall Objective. The overall objective of DSM is defined as",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Tab. 1, HDF achieves the best performance in both WAR and UAR"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "follows:",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "metrics. In particular, it achieves consistently better performance on"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(17)\nLtotal = ùúÜCE ¬∑ LCE + ùúÜSC ¬∑ LIB-D-SC.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "underrepresented and challenging categories, even surpassing some"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "This optimization strategy enables per-batch awareness of training",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "vision-language models, highlighting its strong generalization to"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "status and feature structure dynamics, achieving self-regulated bal-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "heterogeneous and hard-to-learn samples. This demonstrates the ef-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ancing between classification and contrastive learning. As a result,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "fectiveness of our method in not only improving overall recognition"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "DSM significantly improves model robustness and generalization",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "accuracy, but also enhancing robustness under sample imbalance"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "under real-world distribution shifts and individual variations.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "and expression diversity."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": ""
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "years",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": ""
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Fea."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "ACM MM‚Äô20",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "21.62"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "ACM MM‚Äô20",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "21.51"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "ACM MM‚Äô21",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "31.78"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "ICASSP‚Äô23",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "32.71"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "T-CSVT‚Äô24",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "N/A"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "T-CSVT‚Äô24",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "35.36"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "AAAI‚Äô23",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "26.44"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "CVPR‚Äô23",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "31.63"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "T-BIOM‚Äô25",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "28.71"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "T-MM‚Äô24",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "N/A"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "ICME‚Äô24",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "N/A"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "-",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "41.63"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization": "",
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "Metrics (%)"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "Method"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "WAR\nUAR"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "2ResNet18+LSTM [40]\n43.20\n31.28"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "Former-DFER [49]\n46.85\n37.20"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "3D-DSwin [39]\n46.98\n37.66"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "48.54\n35.82\nGCA+IAL [21]"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "M3DFEL [37]\n47.67\n35.94"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "LG-DSTF [47]\n48.19\n39.84"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "RDFER [25]\n48.60\n36.47"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "CFAN-SDA [2]\n49.48\n39.56"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "50.30\n40.49\nHDF (Ours)"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": "Results on FERV39k. As shown in Tab. 2, HDF achieves com-"
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        },
        {
          "Figure 4: Illustration of feature distribution learned by the baseline and our proposed method in HDF on DFEW fd5.": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Metric (%)\nDAM"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "5.3\nAblation Studies",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Setting"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Fre.\nTim.\nWAR\nUAR"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "5.3.1\nEffectiveness of Each Module. We conduct ablation studies",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "‚úó\n‚úó"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "a\n71.79\n59.93"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "on DFEW fd5 to verify the individual contributions of our two core",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(cid:34)\n‚úó"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "b\n72.60\n60.43"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "components in HDF: the DAM and the DSM. Results are shown in",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "‚úó\n(cid:34)"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "72.73\n60.21\nc"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Tab. 3. Setting (a) employs only the X3D backbone with standard",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "(cid:34)\n(cid:34)"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "cross-entropy loss, serving as our baseline. Introducing DAM alone",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "73.24\n61.31\nd"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(Setting b) brings noticeable improvements in both WAR and UAR,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Table 4: Ablation study of frequency and time branches in"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "validating the effectiveness of distributionally-robust attention in",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "DAM on DFEW fd5."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "suppressing noise and inconsistencies caused by hard samples.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Adding DSM alone (Setting c) leads to even higher gains, empha-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "sizing the importance of dynamically balancing the optimization",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "focus between classification and representation learning, partic-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ularly when hard examples dominate training dynamics. Finally,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "combining both modules (Setting d) enables HDF to achieve the",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "best performance‚Äîoutperforming the baseline by +4.62% WAR and",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "+3.10% UAR. These results confirm the complementary nature of",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "our modules in addressing sample heterogeneity from both the",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "feature modeling and optimization perspectives.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Figure 5: The visualization of separate loss landscape on"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "5.3.2\nEffectiveness of Frequency and Temporal Branches in DAM.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "DFEW fd5, optimized by AdamW (left) and our DSM (right)"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "We further conduct ablation studies to examine the individual con-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "respectively."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tributions of the frequency and temporal branches within DAM .",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "As shown in Tab. 4, enabling either the frequency branch (Setting b)",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "separable. This confirms that our framework effectively mitigates"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "or the temporal branch (Setting c) yields clear improvements over",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "representation ambiguity caused by sample heterogeneity, allowing"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "the baseline (Setting a), showing that each branch independently",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "the model to learn more robust and compact feature embeddings"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "enhances robustness to a different type of hard sample: frequency",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "for DFER tasks."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "attention mitigates performance drops caused by low-quality or",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "5.4.3\nVisualization of Loss Landscape. To evaluate the effect of our"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "stylistically biased samples, while temporal attention stabilizes",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "DSM on training dynamics, we visualize the loss landscape [23],"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "learning under sequences with inconsistent dynamics. When both",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "as shown in Fig. 5. Compared with the version without DSM (left),"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "branches are jointly activated (Setting d), the model achieves the",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "our full HDF (right) exhibits a significantly flatter and smoother"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "best overall performance, verifying that these components address",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "loss surface. This demonstrates that DSM effectively suppresses"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "complementary aspects of sample heterogeneity and jointly con-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "gradient instability induced by hard or uncertain samples, guiding"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tribute to DAM‚Äôs robustness under real-world distribution shifts.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "the model toward more stable optima. The improved smoothness"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "highlights enhanced convergence stability and generalization ca-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "5.4\nVisualization.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "pability, particularly in the presence of sample heterogeneity and"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "5.4.1\nLearned Feature Maps Visualization. To verify the robustness",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "hard samples caused by temporal ambiguity, style variation, or"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "of the proposed HDF under challenging conditions, we visualize",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "underrepresented categories in DFER."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "the learned facial feature maps, as shown in Fig. 3, including cases",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "with low illumination, occlusion, and non-frontal poses. In the first",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "6\nConclusion"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "sequence, our method effectively suppresses the influence of oc-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "We propose HDF, a heterogeneity-aware optimization framework"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "cluded regions by focusing on informative visible areas, showcasing",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tailored for in-the-wild DFER, which explicitly addresses sample-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "robustness against spatial ambiguity. In the second sequence, de-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "level variations in style, dynamics, and category distribution.\nIt"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "spite non-frontal views, our features still capture expressive regions,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "integrates two key components: a dual-branch attention module,"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "highlighting resilience to pose-induced heterogeneity. Furthermore,",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "DAM, that performs time-frequency modeling to enhance robust-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "across frames with strong head motion,\nthe model consistently",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ness against visual inconsistency and temporal ambiguity, and a"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "attends to semantically stable regions, demonstrating robustness",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "dynamic optimization strategy, DSM, which adaptively balances"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "against\ntemporal noise and maintaining reliable representation",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "classification and contrastive objectives based on training dynam-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "under complex variations.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "ics to further mitigate the impact of hard or uncertain samples on"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "5.4.2\nt-SNE Visualization. We employ t-SNE [36] to visualize the",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "representation learning. Together, these modules form a unified Dis-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "feature distributions learned by different model variants. As shown",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "tributionally Robust Optimization-based solution that effectively"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "in Fig. 4,\nthe baseline produces\nscattered and overlapping fea-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "improves representation robustness and training stability under het-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ture clusters,\nindicating limited discriminative capacity. Models",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "erogeneous data conditions. Extensive experiments on DFEW and"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "enhanced with DAM or DSM yield noticeably tighter intra-category",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "FERV39k demonstrate the superiority and generalizability of HDF."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "clusters and clearer inter-category boundaries, indicating stronger",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "In future work, we plan to extend this framework to multi-modal"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "discriminability. When both modules are combined in the com-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "expression analysis and investigate deployment-friendly variants"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "plete HDF framework, emotion categories become more distinctly",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "for real-world applications."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[19] Binbin Jin, Defu Lian, Zheng Liu, Qi Liu, Jianhui Ma, Xing Xie, and Enhong Chen.\nACKNOWLEDGEMENTS"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2020. Sampling-decomposable generative adversarial recommender. Advances in"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "This work is supported by Anhui Province Science Foundation for\nNeural Information Processing Systems (2020)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Jiyoung Lee, Sunok Kim, Seungryong Kim, and Kwanghoon Sohn. 2020. Multi-\n[20]\nYouths (Grant No. 2308085QF230), Fundamental Research Funds"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Modal Recurrent Attention Networks for Facial Expression Recognition.\nIEEE"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "for the Central Universities (Grant No. JZ2025HGTB0225), Major"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Transactions on Image Processing (2020)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Scientific and Technological Project of Anhui Provincial Science\n[21] Hanting Li, Hongjing Niu, Zhaoqing Zhu, and Feng Zhao. 2023.\nIntensity-aware"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "loss for dynamic facial expression recognition in the wild. AAAI."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and Technology Innovation Platform (Grant No. 202305a12020012),"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[22] Hanting Li, Hongjing Niu, Zhaoqing Zhu, and Feng Zhao. 2024. CLIPER: A Unified"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "National Natural Science Foundation of China (Grant No. 62302145),"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Vision-Language Framework for In-the-Wild Facial Expression Recognition. In"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2024 IEEE International Conference on Multimedia and Expo (ICME).\nand Students‚Äô Innovation and Entrepreneurship Foundation of Uni-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[23] Mengke Li, Ye Liu, Yang Lu, Yiqun Zhang, Yiu ming Cheung, and Hui Huang."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "versity of Science and Technology of China (Grant No. CY2024X019A)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2024.\nImproving Visual Prompt Tuning by Gaussian Neighborhood Minimization"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "We also thank the OpenI Community (https://openi.pcl.ac.cn) for\nfor Long-Tailed Visual Recognition. In The Thirty-eighth Annual Conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Neural Information Processing Systems.\nits support."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[24]\nShan Li and Weihong Deng. 2020. Deep facial expression recognition: A survey."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "IEEE transactions on affective computing (2020)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Feng Liu, Hanyang Wang, and Siyuan Shen. 2025. Robust Dynamic Facial Expres-\n[25]"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "References\nsion Recognition.\nIEEE Transactions on Biometrics, Behavior, and Identity Science"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[1]\nJoao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new\n(2025)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[26] Hanwei Liu, Huiling Cai, Qingcheng Lin, Xuefeng Li, and Hui Xiao. 2023. Learn-\nmodel and the kinetics dataset. In proceedings of the IEEE Conference on Computer"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ing from More: Combating Uncertainty Cross-multidomain for Facial Expression\nVision and Pattern Recognition."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[2] Dongliang Chen, Guihua Wen, Pei Yang, Huihui Li, Chuyun Chen, and Bao Wang.\nRecognition. In Proceedings of the 31st ACM International Conference on Multime-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2024. CFAN-SDA: Coarse-Fine Aware Network With Static-Dynamic Adaptation\ndia (MM ‚Äô23). Association for Computing Machinery."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "for Facial Expression Recognition in Videos.\n[27]\nYuanyuan Liu, Yuxuan Huang, Shuyang Liu, Yibing Zhan, Zijing Chen, and Zhe\nIEEE Transactions on Circuits and"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Chen. 2024. Open-Set Video-based Facial Expression Recognition with Human\nSystems for Video Technology (2024)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[3] Haodong Chen, Haojian Huang, Junhao Dong, Mingzhe Zheng, and Dian Shao.\nExpression-sensitive Prompting. In Proceedings of the 32nd ACM International"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2024. FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expres-\nConference on Multimedia (MM ‚Äô24)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[28]\nFuyan Ma, Bin Sun, and Shutao Li. 2022.\nSpatio-Temporal Transformer for\nsion Recognition with AdaptERs. In Proceedings of the 32nd ACM International"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Dynamic Facial Expression Recognition in the Wild.\n(2022).\nConference on Multimedia (MM ‚Äô24)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[4]\nJunkai Chen, Zenghai Chen, Zheru Chi, and Hong Fu. 2018. Facial Expression\n[29]\nFuyan Ma, Bin Sun, and Shutao Li. 2023. Facial Expression Recognition With"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Visual Transformers and Attentional Selective Fusion.\nRecognition in Video with Multiple Feature Fusion. IEEE Transactions on Affective\nIEEE Transactions on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Computing (2018).\nAffective Computing (2023)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[5] Xi Chen, Simai He, Bo Jiang, Christopher Thomas Ryan, and Teng Zhang. 2021.\n[30]\nFuyan Ma, Bin Sun, and Shutao Li. 2023.\nLogo-Former: Local-Global Spatio-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "The Discrete Moment Problem with Nonconvex Shape Constraints.\n(2021).\nTemporal Transformer for Dynamic Facial Expression Recognition.\nIEEE Interna-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[6] Wheidima Carneiro de Melo, Eric Granger, and Abdenour Hadid. 2022. A Deep\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP) (2023)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Multiscale Spatiotemporal Network for Assessing Depression From Facial Dy-\nPaul Michel, Tatsunori Hashimoto, and Graham Neubig. 2022. Distributionally\n[31]"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "namics.\nIEEE Transactions on Affective Computing (2022).\nRobust Models with Parametric Likelihood Ratios. In International Conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "John Duchi and Hongseok Namkoong. 2019. Variance-based regularization with\n[7]\nLearning Representations."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[32] Zequn Qin, Pengyi Zhang, Fei Wu, and Xi Li. 2021. FcaNet: Frequency Channel\nconvex objectives. Journal of Machine Learning Research (2019)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[8] Christoph Feichtenhofer. 2020. X3D: Expanding Architectures for Efficient Video\nAttention Networks. In IEEE/CVF International Conference on Computer Vision"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n(ICCV)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[33] Aman Sinha, Hongseok Namkoong, and John Duchi. 2018. Certifiable Distribu-\nPattern Recognition (CVPR)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. 2021.\n[9]\ntional Robustness with Principled Adversarial Training. In International Confer-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Sharpness-aware Minimization for Efficiently Improving Generalization. In In-\nence on Learning Representations."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[34] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.\nternational Conference on Learning Representations."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[10]\nYu Gu, Xiang Zhang, Huan Yan, Jingyang Huang, Zhi Liu, Mianxiong Dong, and\n2015.\nLearning spatiotemporal\nfeatures with 3d convolutional networks.\nIn"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Fuji Ren. 2023. WiFE: WiFi and Vision Based Unobtrusive Emotion Recognition\nProceedings of the IEEE international conference on computer vision. 4489‚Äì4497."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "via Gesture and Facial Expression.\n[35] Md Azher Uddin, Joolekha Bibi Joolee, and Young-Koo Lee. 2022. Depression\nIEEE Transactions on Affective Computing 14,"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "4 (2023), 2567‚Äì2581.\nLevel Prediction Using Deep Spatiotemporal Features and Multilayer Bi-LTSM."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[11] Dan Guo, Kun Li, Bin Hu, Yan Zhang, and Meng Wang. 2024. Benchmarking\nIEEE Transactions on Affective Computing (2022)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "micro-action recognition: Dataset, methods, and applications.\n[36]\nLaurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using\nIEEE Transactions"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "on Circuits and Systems for Video Technology 34, 7 (2024), 6238‚Äì6252.\nt-SNE. Journal of Machine Learning Research (2008)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[12] Dan Guo, Wengang Zhou, Houqiang Li, and Meng Wang. 2018. Hierarchical\n[37] Hanyang Wang, Bo Li, Shuang Wu, Siyuan Shen, Feng Liu, Shouhong Ding,"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and Aimin Zhou. 2023. Rethinking the Learning Paradigm for Dynamic Facial\nLSTM for sign language translation. In Proceedings of the AAAI conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "artificial intelligence, Vol. 32.\nExpression Recognition. In IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nRecognition (CVPR)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[38] Ruiqi Wang, Jinyang Huang, Jie Zhang, Xin Liu, Xiang Zhang, Zhi Liu, Peng\nLearning for Image Recognition. In Proceedings of the IEEE Conference on Computer"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Zhao, Sigui Chen, and Xiao Sun. 2024.\nFacialPulse: An Efficient RNN-based\nVision and Pattern Recognition (CVPR)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[14]\nJingjing Hu, Dan Guo, Kun Li, Zhan Si, Xun Yang, Xiaojun Chang, and Meng\nDepression Detection via Temporal Facial Landmarks. In ACM Multimedia 2024."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Wang. 2025. Unified static and dynamic network: efficient temporal filtering for\n[39] Rui Wang and Xiao Sun. 2023. Dynamic Facial Expression Recognition Based"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "video grounding.\non Vision Transformer with Deformable Module.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\nIn 2023 IEEE International"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(2025).\nConference on Systems, Man, and Cybernetics (SMC)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[15]\nJinyang Huang, Jia-Xuan Bai, Xiang Zhang, Zhi Liu, Yuanhao Feng, Jianchun Liu,\n[40]\nYan Wang, Yixuan Sun, Yiwen Huang, Zhongying Liu, Shuyong Gao, Wei Zhang,"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Xiao Sun, Mianxiong Dong, and Meng Li. 2024. Keystrokesniffer: An off-the-shelf\nWeifeng Ge, and Wenqiang Zhang. 2022. FERV39k: A Large-Scale Multi-Scene"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "smartphone can eavesdrop on your privacy from anywhere.\nIEEE Transactions\nDataset for Facial Expression Recognition in Videos. In IEEE/CVF Conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "on Information Forensics and Security (2024).\nComputer Vision and Pattern Recognition (CVPR)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[16]\nJinyang Huang, Bin Liu, Chenglin Miao, Xiang Zhang, Jiancun Liu, Lu Su, Zhi Liu,\n[41] Yanan Wang, Jianming Wu, and Keiichiro Hoashi. 2019. Multi-attention fusion"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and Yu Gu. 2023. Phyfinatt: An undetectable attack framework against phy layer\nnetwork for video-based emotion recognition.\nIn International Conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "fingerprint-based wifi authentication.\nIEEE Transactions on Mobile Computing\nMultimodal Interaction."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(2023).\n[42] Chenwang Wu, Defu Lian, Yong Ge, Zhihao Zhu, and Enhong Chen. 2023."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[17] Ruomin Huang, Jiawei Huang, Wenjie Liu, and Hu Ding. 2022. Coresets for\nInfluence-Driven Data Poisoning for Robust Recommender Systems.\nIEEE Trans-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "wasserstein distributionally robust optimization problems. Advances in Neural\nactions on Pattern Analysis and Machine Intelligence (2023)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[43]\nJunkang Wu, Jiawei Chen, Jiancan Wu, Wentao Shi, Xiang Wang, and Xiangnan\nInformation Processing Systems (2022)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[18] Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia,\nHe. 2023. Understanding Contrastive Learning via Distributionally Robust Opti-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Cheng Lu, and Jiateng Liu. 2020. DFEW: A Large-Scale Database for Recog-\nmization. In Thirty-seventh Conference on Neural Information Processing Systems."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "nizing Dynamic Facial Expressions in the Wild. In Proceedings of the 28th ACM"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[19] Binbin Jin, Defu Lian, Zheng Liu, Qi Liu, Jianhui Ma, Xing Xie, and Enhong Chen.\nACKNOWLEDGEMENTS"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2020. Sampling-decomposable generative adversarial recommender. Advances in"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "This work is supported by Anhui Province Science Foundation for\nNeural Information Processing Systems (2020)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Jiyoung Lee, Sunok Kim, Seungryong Kim, and Kwanghoon Sohn. 2020. Multi-\n[20]\nYouths (Grant No. 2308085QF230), Fundamental Research Funds"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Modal Recurrent Attention Networks for Facial Expression Recognition.\nIEEE"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "for the Central Universities (Grant No. JZ2025HGTB0225), Major"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Transactions on Image Processing (2020)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Scientific and Technological Project of Anhui Provincial Science\n[21] Hanting Li, Hongjing Niu, Zhaoqing Zhu, and Feng Zhao. 2023.\nIntensity-aware"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "loss for dynamic facial expression recognition in the wild. AAAI."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and Technology Innovation Platform (Grant No. 202305a12020012),"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[22] Hanting Li, Hongjing Niu, Zhaoqing Zhu, and Feng Zhao. 2024. CLIPER: A Unified"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "National Natural Science Foundation of China (Grant No. 62302145),"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Vision-Language Framework for In-the-Wild Facial Expression Recognition. In"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2024 IEEE International Conference on Multimedia and Expo (ICME).\nand Students‚Äô Innovation and Entrepreneurship Foundation of Uni-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[23] Mengke Li, Ye Liu, Yang Lu, Yiqun Zhang, Yiu ming Cheung, and Hui Huang."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "versity of Science and Technology of China (Grant No. CY2024X019A)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2024.\nImproving Visual Prompt Tuning by Gaussian Neighborhood Minimization"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "We also thank the OpenI Community (https://openi.pcl.ac.cn) for\nfor Long-Tailed Visual Recognition. In The Thirty-eighth Annual Conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Neural Information Processing Systems.\nits support."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[24]\nShan Li and Weihong Deng. 2020. Deep facial expression recognition: A survey."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "IEEE transactions on affective computing (2020)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Feng Liu, Hanyang Wang, and Siyuan Shen. 2025. Robust Dynamic Facial Expres-\n[25]"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "References\nsion Recognition.\nIEEE Transactions on Biometrics, Behavior, and Identity Science"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[1]\nJoao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a new\n(2025)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[26] Hanwei Liu, Huiling Cai, Qingcheng Lin, Xuefeng Li, and Hui Xiao. 2023. Learn-\nmodel and the kinetics dataset. In proceedings of the IEEE Conference on Computer"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "ing from More: Combating Uncertainty Cross-multidomain for Facial Expression\nVision and Pattern Recognition."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[2] Dongliang Chen, Guihua Wen, Pei Yang, Huihui Li, Chuyun Chen, and Bao Wang.\nRecognition. In Proceedings of the 31st ACM International Conference on Multime-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2024. CFAN-SDA: Coarse-Fine Aware Network With Static-Dynamic Adaptation\ndia (MM ‚Äô23). Association for Computing Machinery."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "for Facial Expression Recognition in Videos.\n[27]\nYuanyuan Liu, Yuxuan Huang, Shuyang Liu, Yibing Zhan, Zijing Chen, and Zhe\nIEEE Transactions on Circuits and"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Chen. 2024. Open-Set Video-based Facial Expression Recognition with Human\nSystems for Video Technology (2024)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[3] Haodong Chen, Haojian Huang, Junhao Dong, Mingzhe Zheng, and Dian Shao.\nExpression-sensitive Prompting. In Proceedings of the 32nd ACM International"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "2024. FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expres-\nConference on Multimedia (MM ‚Äô24)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[28]\nFuyan Ma, Bin Sun, and Shutao Li. 2022.\nSpatio-Temporal Transformer for\nsion Recognition with AdaptERs. In Proceedings of the 32nd ACM International"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Dynamic Facial Expression Recognition in the Wild.\n(2022).\nConference on Multimedia (MM ‚Äô24)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[4]\nJunkai Chen, Zenghai Chen, Zheru Chi, and Hong Fu. 2018. Facial Expression\n[29]\nFuyan Ma, Bin Sun, and Shutao Li. 2023. Facial Expression Recognition With"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Visual Transformers and Attentional Selective Fusion.\nRecognition in Video with Multiple Feature Fusion. IEEE Transactions on Affective\nIEEE Transactions on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Computing (2018).\nAffective Computing (2023)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[5] Xi Chen, Simai He, Bo Jiang, Christopher Thomas Ryan, and Teng Zhang. 2021.\n[30]\nFuyan Ma, Bin Sun, and Shutao Li. 2023.\nLogo-Former: Local-Global Spatio-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "The Discrete Moment Problem with Nonconvex Shape Constraints.\n(2021).\nTemporal Transformer for Dynamic Facial Expression Recognition.\nIEEE Interna-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[6] Wheidima Carneiro de Melo, Eric Granger, and Abdenour Hadid. 2022. A Deep\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP) (2023)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Multiscale Spatiotemporal Network for Assessing Depression From Facial Dy-\nPaul Michel, Tatsunori Hashimoto, and Graham Neubig. 2022. Distributionally\n[31]"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "namics.\nIEEE Transactions on Affective Computing (2022).\nRobust Models with Parametric Likelihood Ratios. In International Conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "John Duchi and Hongseok Namkoong. 2019. Variance-based regularization with\n[7]\nLearning Representations."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[32] Zequn Qin, Pengyi Zhang, Fei Wu, and Xi Li. 2021. FcaNet: Frequency Channel\nconvex objectives. Journal of Machine Learning Research (2019)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[8] Christoph Feichtenhofer. 2020. X3D: Expanding Architectures for Efficient Video\nAttention Networks. In IEEE/CVF International Conference on Computer Vision"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n(ICCV)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[33] Aman Sinha, Hongseok Namkoong, and John Duchi. 2018. Certifiable Distribu-\nPattern Recognition (CVPR)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. 2021.\n[9]\ntional Robustness with Principled Adversarial Training. In International Confer-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Sharpness-aware Minimization for Efficiently Improving Generalization. In In-\nence on Learning Representations."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[34] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.\nternational Conference on Learning Representations."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[10]\nYu Gu, Xiang Zhang, Huan Yan, Jingyang Huang, Zhi Liu, Mianxiong Dong, and\n2015.\nLearning spatiotemporal\nfeatures with 3d convolutional networks.\nIn"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Fuji Ren. 2023. WiFE: WiFi and Vision Based Unobtrusive Emotion Recognition\nProceedings of the IEEE international conference on computer vision. 4489‚Äì4497."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "via Gesture and Facial Expression.\n[35] Md Azher Uddin, Joolekha Bibi Joolee, and Young-Koo Lee. 2022. Depression\nIEEE Transactions on Affective Computing 14,"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "4 (2023), 2567‚Äì2581.\nLevel Prediction Using Deep Spatiotemporal Features and Multilayer Bi-LTSM."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[11] Dan Guo, Kun Li, Bin Hu, Yan Zhang, and Meng Wang. 2024. Benchmarking\nIEEE Transactions on Affective Computing (2022)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "micro-action recognition: Dataset, methods, and applications.\n[36]\nLaurens van der Maaten and Geoffrey Hinton. 2008. Visualizing Data using\nIEEE Transactions"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "on Circuits and Systems for Video Technology 34, 7 (2024), 6238‚Äì6252.\nt-SNE. Journal of Machine Learning Research (2008)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[12] Dan Guo, Wengang Zhou, Houqiang Li, and Meng Wang. 2018. Hierarchical\n[37] Hanyang Wang, Bo Li, Shuang Wu, Siyuan Shen, Feng Liu, Shouhong Ding,"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and Aimin Zhou. 2023. Rethinking the Learning Paradigm for Dynamic Facial\nLSTM for sign language translation. In Proceedings of the AAAI conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "artificial intelligence, Vol. 32.\nExpression Recognition. In IEEE/CVF Conference on Computer Vision and Pattern"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\nRecognition (CVPR)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[38] Ruiqi Wang, Jinyang Huang, Jie Zhang, Xin Liu, Xiang Zhang, Zhi Liu, Peng\nLearning for Image Recognition. In Proceedings of the IEEE Conference on Computer"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Zhao, Sigui Chen, and Xiao Sun. 2024.\nFacialPulse: An Efficient RNN-based\nVision and Pattern Recognition (CVPR)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[14]\nJingjing Hu, Dan Guo, Kun Li, Zhan Si, Xun Yang, Xiaojun Chang, and Meng\nDepression Detection via Temporal Facial Landmarks. In ACM Multimedia 2024."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Wang. 2025. Unified static and dynamic network: efficient temporal filtering for\n[39] Rui Wang and Xiao Sun. 2023. Dynamic Facial Expression Recognition Based"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "video grounding.\non Vision Transformer with Deformable Module.\nIEEE Transactions on Pattern Analysis and Machine Intelligence\nIn 2023 IEEE International"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(2025).\nConference on Systems, Man, and Cybernetics (SMC)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[15]\nJinyang Huang, Jia-Xuan Bai, Xiang Zhang, Zhi Liu, Yuanhao Feng, Jianchun Liu,\n[40]\nYan Wang, Yixuan Sun, Yiwen Huang, Zhongying Liu, Shuyong Gao, Wei Zhang,"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Xiao Sun, Mianxiong Dong, and Meng Li. 2024. Keystrokesniffer: An off-the-shelf\nWeifeng Ge, and Wenqiang Zhang. 2022. FERV39k: A Large-Scale Multi-Scene"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "smartphone can eavesdrop on your privacy from anywhere.\nIEEE Transactions\nDataset for Facial Expression Recognition in Videos. In IEEE/CVF Conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "on Information Forensics and Security (2024).\nComputer Vision and Pattern Recognition (CVPR)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[16]\nJinyang Huang, Bin Liu, Chenglin Miao, Xiang Zhang, Jiancun Liu, Lu Su, Zhi Liu,\n[41] Yanan Wang, Jianming Wu, and Keiichiro Hoashi. 2019. Multi-attention fusion"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "and Yu Gu. 2023. Phyfinatt: An undetectable attack framework against phy layer\nnetwork for video-based emotion recognition.\nIn International Conference on"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "fingerprint-based wifi authentication.\nIEEE Transactions on Mobile Computing\nMultimodal Interaction."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(2023).\n[42] Chenwang Wu, Defu Lian, Yong Ge, Zhihao Zhu, and Enhong Chen. 2023."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[17] Ruomin Huang, Jiawei Huang, Wenjie Liu, and Hu Ding. 2022. Coresets for\nInfluence-Driven Data Poisoning for Robust Recommender Systems.\nIEEE Trans-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "wasserstein distributionally robust optimization problems. Advances in Neural\nactions on Pattern Analysis and Machine Intelligence (2023)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[43]\nJunkang Wu, Jiawei Chen, Jiancan Wu, Wentao Shi, Xiang Wang, and Xiangnan\nInformation Processing Systems (2022)."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[18] Xingxun Jiang, Yuan Zong, Wenming Zheng, Chuangao Tang, Wanchuang Xia,\nHe. 2023. Understanding Contrastive Learning via Distributionally Robust Opti-"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Cheng Lu, and Jiateng Liu. 2020. DFEW: A Large-Scale Database for Recog-\nmization. In Thirty-seventh Conference on Neural Information Processing Systems."
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "nizing Dynamic Facial Expressions in the Wild. In Proceedings of the 28th ACM"
        },
        {
          "Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization\nMM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "International Conference on Multimedia."
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[44] Yi Wu, Shangfei Wang, and Yanan Chang. 2023. Patch-Aware Representation",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "[48] Guoying Zhao and Matti Pietikainen. 2007. Dynamic Texture Recognition Using"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Learning for Facial Expression Recognition. In Proceedings of the 31st ACM Inter-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Local Binary Patterns with an Application to Facial Expressions. IEEE Transactions"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "national Conference on Multimedia (MM ‚Äô23).",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "on Pattern Analysis and Machine Intelligence (2007)."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[45] Tong Zhang, Wenming Zheng, Zhen Cui, Yuan Zong, and Yang Li. 2019. Spa-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "[49] Zengqun Zhao and Qingshan Liu. 2021. Former-DFER: Dynamic Facial Expression"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tial‚ÄìTemporal Recurrent Neural Network for Emotion Recognition.\nIEEE Trans-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Recognition Transformer. In Proceedings of the 29th ACM International Conference"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "actions on Cybernetics (2019).",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "on Multimedia (MM ‚Äô21)."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[46] Xiaoqin Zhang, Min Li, Sheng Lin, Hang Xu, and Guobao Xiao. 2024. Transformer-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "[50]\nJinxing Zhou, Dan Guo, and Meng Wang. 2022. Contrastive positive sample"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Based Multimodal Emotional Perception for Dynamic Facial Expression Recogni-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "propagation along the audio-visual event\nline.\nIEEE Transactions on Pattern"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "tion in the Wild.\nIEEE Transactions on Circuits and Systems for Video Technology",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "Analysis and Machine Intelligence 45, 6 (2022), 7239‚Äì7257."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "(2024).",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "[51] Hao Zhu, Wenping Ma, Lingling Li, Licheng Jiao, Shuyuan Yang, and Biao Hou."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "[47] Ziyang Zhang, Xiang Tian, Yuan Zhang, Kailing Guo, and Xiangmin Xu. 2024.",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "2020. A Dual‚ÄìBranch Attention fusion deep network for multiresolution re-"
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "Label-Guided Dynamic Spatial-Temporal Fusion for Video-Based Facial Expres-",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": "mote‚ÄìSensing image classification.\nInformation Fusion 58 (2020), 116‚Äì131."
        },
        {
          "MM ‚Äô25, October 27‚Äì31, 2025, Dublin, Ireland": "sion Recognition.\nIEEE Transactions on Multimedia (2024).",
          "Feng-Qi Cui, Anyang Tong, Jinyang Huang, Jie Zhang, Dan Guo, Zhi Liu, and Meng Wang": ""
        }
      ],
      "page": 10
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Quo vadis, action recognition? a new model and the kinetics dataset",
      "authors": [
        "Joao Carreira",
        "Andrew Zisserman"
      ],
      "year": "2017",
      "venue": "proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "CFAN-SDA: Coarse-Fine Aware Network With Static-Dynamic Adaptation for Facial Expression Recognition in Videos",
      "authors": [
        "Dongliang Chen",
        "Guihua Wen",
        "Pei Yang",
        "Huihui Li",
        "Chuyun Chen",
        "Bao Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "3",
      "title": "FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs",
      "authors": [
        "Haodong Chen",
        "Haojian Huang",
        "Junhao Dong",
        "Mingzhe Zheng",
        "Dian Shao"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia (MM '24)"
    },
    {
      "citation_id": "4",
      "title": "Facial Expression Recognition in Video with Multiple Feature Fusion",
      "authors": [
        "Junkai Chen",
        "Zenghai Chen",
        "Zheru Chi",
        "Hong Fu"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "The Discrete Moment Problem with Nonconvex Shape Constraints",
      "authors": [
        "Xi Chen",
        "Simai He",
        "Bo Jiang",
        "Christopher Thomas Ryan",
        "Teng Zhang"
      ],
      "year": "2021",
      "venue": "The Discrete Moment Problem with Nonconvex Shape Constraints"
    },
    {
      "citation_id": "6",
      "title": "A Deep Multiscale Spatiotemporal Network for Assessing Depression From Facial Dynamics",
      "authors": [
        "Wheidima Carneiro De Melo",
        "Eric Granger",
        "Abdenour Hadid"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Variance-based regularization with convex objectives",
      "authors": [
        "John Duchi",
        "Hongseok Namkoong"
      ],
      "year": "2019",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "8",
      "title": "X3D: Expanding Architectures for Efficient Video Recognition",
      "authors": [
        "Christoph Feichtenhofer"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "9",
      "title": "Sharpness-aware Minimization for Efficiently Improving Generalization",
      "authors": [
        "Pierre Foret",
        "Ariel Kleiner",
        "Hossein Mobahi",
        "Behnam Neyshabur"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "10",
      "title": "WiFE: WiFi and Vision Based Unobtrusive Emotion Recognition via Gesture and Facial Expression",
      "authors": [
        "Yu Gu",
        "Xiang Zhang",
        "Huan Yan",
        "Jingyang Huang",
        "Zhi Liu",
        "Mianxiong Dong",
        "Fuji Ren"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "11",
      "title": "Benchmarking micro-action recognition: Dataset, methods, and applications",
      "authors": [
        "Dan Guo",
        "Kun Li",
        "Bin Hu",
        "Yan Zhang",
        "Meng Wang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "12",
      "title": "Hierarchical LSTM for sign language translation",
      "authors": [
        "Dan Guo",
        "Wengang Zhou",
        "Houqiang Li",
        "Meng Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "13",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "14",
      "title": "Unified static and dynamic network: efficient temporal filtering for video grounding",
      "authors": [
        "Jingjing Hu",
        "Dan Guo",
        "Kun Li",
        "Zhan Si",
        "Xun Yang",
        "Xiaojun Chang",
        "Meng Wang"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "15",
      "title": "Keystrokesniffer: An off-the-shelf smartphone can eavesdrop on your privacy from anywhere",
      "authors": [
        "Jinyang Huang",
        "Jia-Xuan Bai",
        "Xiang Zhang",
        "Zhi Liu",
        "Yuanhao Feng",
        "Jianchun Liu",
        "Xiao Sun",
        "Mianxiong Dong",
        "Meng Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Information Forensics and Security"
    },
    {
      "citation_id": "16",
      "title": "Phyfinatt: An undetectable attack framework against phy layer fingerprint-based wifi authentication",
      "authors": [
        "Jinyang Huang",
        "Bin Liu",
        "Chenglin Miao",
        "Xiang Zhang",
        "Jiancun Liu",
        "Lu Su",
        "Zhi Liu",
        "Yu Gu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Mobile Computing"
    },
    {
      "citation_id": "17",
      "title": "Coresets for wasserstein distributionally robust optimization problems",
      "authors": [
        "Ruomin Huang",
        "Jiawei Huang",
        "Wenjie Liu",
        "Hu Ding"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "DFEW: A Large-Scale Database for Recognizing Dynamic Facial Expressions in the Wild",
      "authors": [
        "Xingxun Jiang",
        "Yuan Zong",
        "Wenming Zheng",
        "Chuangao Tang",
        "Wanchuang Xia",
        "Cheng Lu",
        "Jiateng Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "19",
      "title": "Sampling-decomposable generative adversarial recommender",
      "authors": [
        "Binbin Jin",
        "Defu Lian",
        "Zheng Liu",
        "Qi Liu",
        "Jianhui Ma",
        "Xing Xie",
        "Enhong Chen"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "20",
      "title": "Multi-Modal Recurrent Attention Networks for Facial Expression Recognition",
      "authors": [
        "Jiyoung Lee",
        "Sunok Kim",
        "Seungryong Kim",
        "Kwanghoon Sohn"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "21",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "Hanting Li",
        "Hongjing Niu",
        "Zhaoqing Zhu",
        "Feng Zhao"
      ],
      "year": "2023",
      "venue": "Intensity-aware loss for dynamic facial expression recognition in the wild"
    },
    {
      "citation_id": "22",
      "title": "CLIPER: A Unified Vision-Language Framework for In-the-Wild Facial Expression Recognition",
      "authors": [
        "Hanting Li",
        "Hongjing Niu",
        "Zhaoqing Zhu",
        "Feng Zhao"
      ],
      "year": "2024",
      "venue": "2024 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "23",
      "title": "Improving Visual Prompt Tuning by Gaussian Neighborhood Minimization for Long-Tailed Visual Recognition",
      "authors": [
        "Mengke Li",
        "Ye Liu",
        "Yang Lu",
        "Yiqun Zhang",
        "Yiu Ming Cheung",
        "Hui Huang"
      ],
      "year": "2024",
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2020",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "25",
      "title": "Robust Dynamic Facial Expression Recognition",
      "authors": [
        "Feng Liu",
        "Hanyang Wang",
        "Siyuan Shen"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Biometrics, Behavior, and Identity Science"
    },
    {
      "citation_id": "26",
      "title": "Learning from More: Combating Uncertainty Cross-multidomain for Facial Expression Recognition",
      "authors": [
        "Hanwei Liu",
        "Huiling Cai",
        "Qingcheng Lin",
        "Xuefeng Li",
        "Hui Xiao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia (MM '23)"
    },
    {
      "citation_id": "27",
      "title": "Open-Set Video-based Facial Expression Recognition with Human Expression-sensitive Prompting",
      "authors": [
        "Yuanyuan Liu",
        "Yuxuan Huang",
        "Shuyang Liu",
        "Yibing Zhan",
        "Zijing Chen",
        "Zhe Chen"
      ],
      "year": "2024",
      "venue": "Proceedings of the 32nd ACM International Conference on Multimedia (MM '24)"
    },
    {
      "citation_id": "28",
      "title": "Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild",
      "authors": [
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2022",
      "venue": "Spatio-Temporal Transformer for Dynamic Facial Expression Recognition in the Wild"
    },
    {
      "citation_id": "29",
      "title": "Facial Expression Recognition With Visual Transformers and Attentional Selective Fusion",
      "authors": [
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Logo-Former: Local-Global Spatio-Temporal Transformer for Dynamic Facial Expression Recognition",
      "authors": [
        "Fuyan Ma",
        "Bin Sun",
        "Shutao Li"
      ],
      "year": "2023",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "Distributionally Robust Models with Parametric Likelihood Ratios",
      "authors": [
        "Paul Michel",
        "Tatsunori Hashimoto",
        "Graham Neubig"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "32",
      "title": "FcaNet: Frequency Channel Attention Networks",
      "authors": [
        "Zequn Qin",
        "Pengyi Zhang",
        "Fei Wu",
        "Xi Li"
      ],
      "year": "2021",
      "venue": "IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "33",
      "title": "Certifiable Distributional Robustness with Principled Adversarial Training",
      "authors": [
        "Aman Sinha",
        "Hongseok Namkoong",
        "John Duchi"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "34",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "Du Tran",
        "Lubomir Bourdev",
        "Rob Fergus",
        "Lorenzo Torresani",
        "Manohar Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings"
    },
    {
      "citation_id": "35",
      "title": "Depression Level Prediction Using Deep Spatiotemporal Features and Multilayer Bi-LTSM",
      "authors": [
        "Md Azher Uddin",
        "Joolekha Bibi Joolee",
        "Young-Koo Lee"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Visualizing Data using t-SNE",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "37",
      "title": "Rethinking the Learning Paradigm for Dynamic Facial Expression Recognition",
      "authors": [
        "Hanyang Wang",
        "Bo Li",
        "Shuang Wu",
        "Siyuan Shen",
        "Feng Liu",
        "Shouhong Ding",
        "Aimin Zhou"
      ],
      "year": "2023",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "38",
      "title": "FacialPulse: An Efficient RNN-based Depression Detection via Temporal Facial Landmarks",
      "authors": [
        "Ruiqi Wang",
        "Jinyang Huang",
        "Jie Zhang",
        "Xin Liu",
        "Xiang Zhang",
        "Zhi Liu",
        "Peng Zhao",
        "Sigui Chen",
        "Xiao Sun"
      ],
      "year": "2024",
      "venue": "ACM Multimedia"
    },
    {
      "citation_id": "39",
      "title": "Dynamic Facial Expression Recognition Based on Vision Transformer with Deformable Module",
      "authors": [
        "Rui Wang",
        "Xiao Sun"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "40",
      "title": "FERV39k: A Large-Scale Multi-Scene Dataset for Facial Expression Recognition in Videos",
      "authors": [
        "Yan Wang",
        "Yixuan Sun",
        "Yiwen Huang",
        "Zhongying Liu",
        "Shuyong Gao",
        "Wei Zhang",
        "Weifeng Ge",
        "Wenqiang Zhang"
      ],
      "year": "2022",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "41",
      "title": "Multi-attention fusion network for video-based emotion recognition",
      "authors": [
        "Yanan Wang",
        "Jianming Wu",
        "Keiichiro Hoashi"
      ],
      "year": "2019",
      "venue": "International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "42",
      "title": "Influence-Driven Data Poisoning for Robust Recommender Systems",
      "authors": [
        "Chenwang Wu",
        "Defu Lian",
        "Yong Ge",
        "Zhihao Zhu",
        "Enhong Chen"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Understanding Contrastive Learning via Distributionally Robust Optimization",
      "authors": [
        "Junkang Wu",
        "Jiawei Chen",
        "Jiancan Wu",
        "Wentao Shi",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "44",
      "title": "Patch-Aware Representation Learning for Facial Expression Recognition",
      "authors": [
        "Yi Wu",
        "Shangfei Wang",
        "Yanan Chang"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia (MM '23)"
    },
    {
      "citation_id": "45",
      "title": "Spatial-Temporal Recurrent Neural Network for Emotion Recognition",
      "authors": [
        "Tong Zhang",
        "Wenming Zheng",
        "Zhen Cui",
        "Yuan Zong",
        "Yang Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "46",
      "title": "Transformer-Based Multimodal Emotional Perception for Dynamic Facial Expression Recognition in the Wild",
      "authors": [
        "Xiaoqin Zhang",
        "Min Li",
        "Sheng Lin",
        "Hang Xu",
        "Guobao Xiao"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology"
    },
    {
      "citation_id": "47",
      "title": "Label-Guided Dynamic Spatial-Temporal Fusion for Video-Based Facial Expression Recognition",
      "authors": [
        "Ziyang Zhang",
        "Xiang Tian",
        "Yuan Zhang",
        "Kailing Guo",
        "Xiangmin Xu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "48",
      "title": "Dynamic Texture Recognition Using Local Binary Patterns with an Application to Facial Expressions",
      "authors": [
        "Guoying Zhao",
        "Matti Pietikainen"
      ],
      "year": "2007",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "49",
      "title": "Former-DFER: Dynamic Facial Expression Recognition Transformer",
      "authors": [
        "Zengqun Zhao",
        "Qingshan Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia (MM '21)"
    },
    {
      "citation_id": "50",
      "title": "Contrastive positive sample propagation along the audio-visual event line",
      "authors": [
        "Jinxing Zhou",
        "Dan Guo",
        "Meng Wang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "51",
      "title": "A Dual-Branch Attention fusion deep network for multiresolution remote-Sensing image classification",
      "authors": [
        "Wenping Hao Zhu",
        "Lingling Ma",
        "Licheng Li",
        "Shuyuan Jiao",
        "Biao Yang",
        "Hou"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    }
  ]
}