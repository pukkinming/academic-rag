{
  "paper_id": "2405.09551v3",
  "title": "Towards Bi-Hemispheric Emotion Mapping Through Eeg: A Dual-Stream Neural Network Approach",
  "published": "2024-04-06T20:42:15Z",
  "authors": [
    "David Freire-Obregón",
    "Daniel Hernández-Sosa",
    "Oliverio J. Santana",
    "Javier Lorenzo-Navarro",
    "Modesto Castrillón-Santana"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion classification through EEG signals plays a significant role in psychology, neuroscience, and humancomputer interaction. This paper addresses the challenge of mapping human emotions using EEG data in the Mapping Human Emotions through EEG Signals FG24 competition. Subjects mimic the facial expressions of an avatar, displaying fear, joy, anger, sadness, disgust, and surprise in a VR setting. EEG data is captured using a multi-channel sensor system to discern brain activity patterns. We propose a novel two-stream neural network employing a Bi-Hemispheric approach for emotion inference, surpassing baseline methods and enhancing emotion recognition accuracy. Additionally, we conduct a temporal analysis revealing that specific signal intervals at the beginning and end of the emotion stimulus sequence contribute significantly to improve accuracy. Leveraging insights gained from this temporal analysis, our approach offers enhanced performance in capturing subtle variations in the states of emotions. Code is available at https://github.com/davidfreire/FG24-EmoNeuroDB/",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion is a central aspect of human experience, profoundly shaping our interactions and perceptions. While humans naturally excel at discerning emotions in others, replicating this nuanced understanding in computers remains a formidable task  [11] . Emotion recognition serves as a crucial step towards imbuing machines with the ability to comprehend and respond to human emotions, garnering significant interest from researchers in human-machine interaction (HMI) and pattern recognition  [4] ,  [12] .\n\nTraditionally, studies in emotion recognition have primarily focused on analyzing verbal and nonverbal cues, such as speech  [10]  and facial expressions  [3] . However, recent insights from neuroscience suggest that emotions originate from various regions of the brain, including the orbital frontal cortex, ventral medial prefrontal cortex, and amygdala  [1] . This neurobiological perspective presents an intriguing opportunity to decode emotions by capturing continuous brain activity signals from these sub-cortical regions.\n\nIn the context of virtual reality (VR), where subjects are fully immersed in carefully crafted environments, understanding and recognizing emotions take on a new dimension. By utilizing electrodes placed on the scalp to perform an electroencephalogram (EEG), researchers can record neural activity within the brain while the subjects interact with virtual avatars displaying various emotion's expressions. This This work is partially funded by the Spanish Ministry of Science and Innovation under project PID2021-122402OB-C22 and by the ACIISI-Gobierno de Canarias and European FEDER funds under project ULPGC Facilities Net and Grant EIS 2021 04. approach offers a unique opportunity to directly observe the neural mechanisms underlying the processing of emotions in a controlled VR environment.\n\nAnalyzing EEG signals within the VR paradigm provides valuable insights into how the brain responds to emotion's stimuli in immersive environments. Existing EEGbased emotion recognition techniques tackle two primary challenges: feature extraction and accurate classification. EEG signals offer rich data across time, frequency, and time-frequency domains, necessitating robust feature extraction methods.\n\nPrevious works have already leveraged machine learning techniques evaluating EEG features, underscoring the complexity of this task  [6] . Subsequent challenges lie in effectively classifying these features. In this regard, some works have proposed a group of sparse canonical correlation analysis for simultaneous EEG channel selection and emotion recognition  [15] , while others integrated brain activation patterns to enhance emotion's recognition performance  [8] .\n\nWhile these methodologies have demonstrated promising results on specific EEG emotion's datasets, there remains a need for further exploration and refinement to achieve robust and generalized emotion recognition systems. The proposed methodology contributes to the field of EEG-based emotion recognition in several significant ways:\n\n• Incorporates a Bi-Hemispheric approach within a two-stream recurrent neural network, leveraging the distinct processing characteristics of each hemisphere to enhance emotion inference accuracy. • Conducts comparative analyses with one-stream neural networks, demonstrating the superiority of the Bi-Hemispheric approach in capturing subtle nuances of emotions. • Conducts a temporal analysis of EEG signals, identifying key temporal intervals (first and last intervals) that significantly contribute to emotion classification accuracy, enabling more efficient and precise emotion inference.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Human emotions can be classified in two main ways  [16] : the discrete basic emotion approach and the dimensional approach. The discrete basic emotion approach categorizes emotions into specific states, such as the six basic emotions: joy, sadness, surprise, fear, anger, and disgust  [2] . In contrast, the dimensional approach depicts emotions as continuous entities, often defined by three dimensions (valence, arousal, and dominance) or simply two dimensions (valence and arousal)  [14] .\n\nIn EEG-based emotion recognition, two fundamental components stand out: EEG feature extractors and classifiers  [6] . Former features are typically categorized into single-channel and multi-channel varieties. Statistical metrics, power spectral density (PSD), or differential entropy (DE) are among the commonly employed single-channel features  [13] . Conversely, multi-channel approaches aim to capture interchannel relationships, such as hemispheric asymmetry in PSD and functional connectivity measures  [8] . They often rely on correlation, coherence, and phase synchronization metrics to estimate brain functional connectivity. In contrast, our proposed model prioritizes dual-channel features and harnesses a recurrent neural network architecture to integrate them effectively.\n\nIn this regard, EEG classifiers can be categorized into two main types: topology-invariant and topology-aware  [5] . The former, such as Support Vector Machines (SVM) or Recurrent Neural Networks (RNN), do not consider the topological structure of EEG features during learning  [17] . In contrast, topology-aware classifiers like Convolutional Neural Networks (CNN) and Graph Neural Networks (GNN) take into account the inter-channel topological relationships  [7] . They learn EEG representations by aggregating features from neighboring channels using convolutional operations in Euclidean or non-Euclidean space  [17] . However, existing CNNs and GNNs struggle to capture dependencies between distant channels, potentially overlooking crucial emotionrelated information  [7] . More recently, RNNs have been used to learn spatial topological relations by scanning electrodes in vertical and horizontal directions  [9] . Nevertheless, these approaches fail to fully utilize the topological structure inherent in EEG channels  [17] . Topologically close channels may seem distant in their scanning sequence. Our model addresses this by partitioning both hemispheres and using heterogeneous branches (non-shared weights). Prior to RNN application, a Conv1D module handles the topological structure issue.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "The EEG signals recorded from each electrode i at a particular time t can be represented as a continuous function of time x i (t). In discrete form, the EEG signal can be represented as a sequence of samples x i [n], where n represents the sample index.\n\nPre-processing steps. Three pre-processing steps prepare the signal before extracting the frequency-domain representations.\n\n1) Re-referencing to Mastoid Channels. Re-referencing involves subtracting the average signals from mastoid channels A1 and A2 from each EEG signal. Let x i (t) denote the EEG signal from electrode i and x A1 (t), x A2 (t) denote the signals from mastoid channels A1 and A2 respectively. The re-referenced EEG signal from electrode i is denoted as\n\n2) High-pass and Low-pass Filtering. High-pass and lowpass filters are applied to the re-referenced EEG signals to remove unwanted frequency components. Let x ref i (t) denote the re-referenced EEG signal from electrode i and x filt i (t) denote the filtered signal. The filtered EEG signal from electrode i is denoted as x filt i (t):\n\n3) Filter Delay Removal. Due to the filtering process, a delay is introduced in the signals. We discard the initial samples from each filtered signal to remove this delay. Let x filt i (t) denote the filtered EEG signal from electrode i and x prep i (t) denote the pre-processed signal after delay removal. The pre-processed EEG signal from electrode i is denoted as x prep i (t):\n\nx prep i (t) = x filt i (tdelay) After pre-processing, the EEG signals are denoted as x prep i (t). The frequency-domain representation obtained through FFT is denoted as ϕ i (f ).\n\nFrequency-Domain Representations. Let's denote this representation as ϕ i (f ), where f represents frequency.\n\nThis represents the Fourier Transform of the EEG signal x prep i (t). The magnitude of ϕ i (f ) provides information about the amplitude of different frequency components present in the EEG signal, while the phase of ϕ i (f ) provides information about the timing/delay relationships between these frequency components. These representations allow for EEG data analysis in both the time and frequency domains, enabling insights into brain activity patterns and their associations with cognitive processes such as emotion. Subsequently, the resulting data is segregated into left and right hemispheres for further examination, represented as\n\nEEG data captured from electrodes situated on the left hemisphere, including F p1, F 7, C3, P 3, O1, F 3, T 3, T 5, F z, Cz, and A1, is categorized under the left hemisphere, ϕ left i (f ). Conversely, data obtained from electrodes located on the right hemisphere, comprising F p2, F 8, C4, P 4, O2, F 4, T 4, T 6, F z, Cz, and A2, is attributed to the right hemisphere, ϕ right i (f ). Classifier. The architecture comprises two parallel streams, each processing EEG signals from different hemispheres. Each stream starts with a 1D convolutional layer followed by max-pooling and dropout regularization, with a dropout rate of 0.5. The output is reshaped and fed into a Long Short-Term Memory (LSTM) layer to capture temporal dependencies. Another dropout layer is applied to prevent overfitting before flattening the output. This process is repeated for signals from both hemispheres. The processed outputs are concatenated, and the concatenated representation is passed through a fully connected layer with ReLU activation and L2 regularization. Finally, a softmax layer produces probabilities for the six emotion classes. The model is compiled with categorical cross-entropy loss and optimized using the Adam optimizer, with accuracy as the evaluation metric. This architecture allows the model to effectively capture temporal dynamics from EEG signals in order to chose between the six possible emotions. The loss function for the provided code can be expressed as the categorical cross-entropy loss, commonly used for multiclass classification tasks like emotion classification. Mathematically, the categorical cross-entropy loss L can be defined as:\n\nWhere:\n\n• N is the total number of samples,\n\n• C is the number of classes (in this case, 6 for the six emotions), • y n,c is the true label (one-hot encoded) of sample n for class c, • ŷn,c is the predicted probability of sample ϕ n (f ) belonging to class c as output by the model.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "V. Experimental Evaluation",
      "text": "The accuracy results on the validation set for each considered approach are presented in Table  I . The baseline accuracy provided by the competition organizers, denoted as Competition base , is recorded at 19.4%. Subsequent enhancements are observed with the Single Branch approach achieving 23.3%, while the Dual Branch model demonstrates the highest accuracy at 28.9%. These findings indicate the efficacy of employing a dual-branch architecture for improving emotion classification accuracy compared to both the baseline and single-branch approaches.\n\nWhen evaluating the dual-branch approach on the validation set, the confusion matrix provides valuable insights into The confusion matrix, depicted in Figure  2 , illustrates the model's predictions compared to the ground truth labels for six emotions: anger, disgust, fear, joy, sadness, and surprise. Each cell in the matrix represents the percentage of instances predicted as one emotion class (rows) that belong to another class (columns). When analyzing the confusion matrix resulting from the Bi-Hemispheric approach on the validation set (see Figure  2 ), it is important to acknowledge the limited number of samples available for training and validation, with only 20 subjects in the training set and 10 subjects in the validation set, as detailed earlier. Despite the small sample size, the matrix reveals notable strengths in the model's performance across various emotion categories. Notably, the model demonstrates commendable accuracy in identifying joy and sadness, with 53.3% and 40.0% of samples correctly classified, respectively. Additionally, the model exhibits promising capability in distinguishing between emotions such as disgust and surprise, achieving 26.7% and 33.3% accuracy rates, respectively. While there may be room for improvement, particularly in accurately classifying instances of anger and fear, the Bi-Hemispheric approach shows potential for effectively capturing the underlying patterns in EEG signals associated with different emotion's states, even within the constraints of a small dataset.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Vi. Temporal Analysis",
      "text": "When performing a temporal analysis, the EEG inputs were divided into eight intervals to classify each interval separately. This temporal segmentation allows for a more granular examination of the EEG signals, potentially capturing transient changes in brain activity over time. Formally, the division of the EEG signal into eight intervals can be expressed as follows:\n\nInterval j = j × total time 8 , (j + 1) × total time 8\n\nWhere j represents the interval index ranging from 0 to 7, and total time denotes the duration of the EEG signal.\n\nAnalyzing the training and validation accuracies of the Mono-Hemispheric and Bi-Hemispheric approaches across eight intervals (j0 to j7), as depicted in Figure  3 , reveals intriguing patterns in the models' temporal performance. Notably, both approaches exhibit higher accuracy at the initial and final intervals (j0 and j7), indicating robust learning at the beginning and end of the temporal sequence. Intervals j1 and j6 notably stand out with notable validation accuracy for both approaches. In particular, interval j1 demonstrates the highest validation accuracy, highlighting the significance of early temporal dynamics in accurately discerning emotion's states. For the Mono-Hemispheric approach, j1 achieves a validation accuracy of 25.0%, while for the Bi-Hemispheric approach, it reaches an even higher 28.9%. Similarly, interval j6 also exhibits promising validation accuracy, with the Bi-Hemispheric approach achieving 29.4% accuracy. However, a noticeable decline in accuracy is observed in the middle intervals (j2 to j5), suggesting challenges in capturing the nuanced changes in EEG signals associated with varying emotion's states during these periods. While the Mono-Hemispheric approach shows a more pronounced decline in accuracy in the middle intervals, the Bi-Hemispheric approach demonstrates relatively better performance, albeit with some fluctuations. This discrepancy highlights the effectiveness of leveraging information from both hemispheres in capturing temporal dynamics, particularly during challenging transition phases between emotion's states.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Vii. Conclusions",
      "text": "In conclusion, our study addresses the pertinent challenge of emotion classification through EEG signals within the context of the Mapping Human Emotions through EEG Signals FG24 competition. Our proposed two-stream neural network demonstrated superior performance to baseline methods, enhancing emotion recognition accuracy when utilizing a Bi-Hemispheric approach. Furthermore, our temporal analysis revealed that specific signal intervals at the beginning and end of the emotion stimulus sequence significantly contribute to improved accuracy. Our approach effectively captures subtle variations in emotion's states, considering these insights. Notably, in the test set of the competition, where labels are not publicly disclosed, our Bi-Hemispheric approach achieved an average accuracy of 22.78%, outperforming the baseline method, which achieved an average accuracy of 18.89%. Specifically, our approach exhibited improved accuracy in classifying joy and sadness emotions, indicating its efficacy in discerning nuanced emotion's states. Our proposed approach demonstrates robustness and potential applicability in real-world scenarios, promising advancements in EEG-based emotion recognition.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Diagram illustrating the three-module stages for Bi-Hemispheric emotion recognition from EEG signals: Signal Pre-processing, Hemispheric Split,",
      "page": 2
    },
    {
      "caption": "Figure 2: Confusion matrix for the dual-branch approach on the validation",
      "page": 3
    },
    {
      "caption": "Figure 3: Temporal analysis of training and validation accuracies. Perfor-",
      "page": 4
    },
    {
      "caption": "Figure 2: , illustrates the",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Approach": "Accuracy",
          "Competition\nbase": "19.4%",
          "Mono-Hemispheric": "23.3%",
          "Bi-Hemispheric": "28.9%"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "13.3": "6.7",
          "16.7": "33.3",
          "30.0": "16.7",
          "10.0": "3.3"
        },
        {
          "13.3": "33.3",
          "16.7": "13.3",
          "30.0": "23.3",
          "10.0": "10.0"
        },
        {
          "13.3": "0.0",
          "16.7": "53.3",
          "30.0": "10.0",
          "10.0": "6.7"
        },
        {
          "13.3": "10.0",
          "16.7": "23.3",
          "30.0": "40.0",
          "10.0": "3.3"
        },
        {
          "13.3": "23.3",
          "16.7": "20.0",
          "30.0": "23.3",
          "10.0": "6.7"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Mon",
          "Column_6": "o-Hemis",
          "Column_7": "pheric t",
          "Column_8": "rain",
          "Column_9": "",
          "Column_10": "",
          "Column_11": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Bi-H",
          "Column_6": "emisphe",
          "Column_7": "ric train",
          "Column_8": "",
          "Column_9": "",
          "Column_10": "",
          "Column_11": ""
        },
        {
          "Column_1": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "Mon\nBi-H",
          "Column_6": "o-Hemis\nemisphe",
          "Column_7": "pheric\nric val",
          "Column_8": "val",
          "Column_9": "",
          "Column_10": "",
          "Column_11": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural correlates of social and nonsocial emotions: An fmri study",
      "authors": [
        "J Britton",
        "K Phan",
        "S Taylor",
        "R Welsh",
        "K Berridge",
        "I Liberzon"
      ],
      "year": "2006",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "2",
      "title": "Are there basic emotions? Psychological review",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Are there basic emotions? Psychological review"
    },
    {
      "citation_id": "3",
      "title": "Towards facial expression robustness in multi-scale wild environments",
      "authors": [
        "D Freire-Obregón",
        "D Hernández-Sosa",
        "O Santana",
        "J Lorenzo-Navarro",
        "M Castrillón-Santana"
      ],
      "year": "2023",
      "venue": "Image Analysis and Processing -ICIAP 2023"
    },
    {
      "citation_id": "4",
      "title": "Smile detection using local binary patterns and support vector machines",
      "authors": [
        "D Freire-Obregón",
        "M Santana",
        "O Déniz-Suárez"
      ],
      "year": "2009",
      "venue": "International Conference on Computer Vision Theory and Applications"
    },
    {
      "citation_id": "5",
      "title": "Node-wise domain adaptation based on transferable attention for recognizing road rage via eeg",
      "authors": [
        "X Gao",
        "C Xu",
        "Y Song",
        "J Hu",
        "J Xiao",
        "Z Meng"
      ],
      "year": "2023",
      "venue": "ICASSP 2023 -2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Hierarchical convolutional neural networks for eeg-based emotion recognition",
      "authors": [
        "J Li",
        "Z Zhang",
        "H He"
      ],
      "year": "2017",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "8",
      "title": "Eeg based emotion recognition by combining functional connectivity network and local activations",
      "authors": [
        "P Li",
        "H Liu",
        "Y Si",
        "C Li",
        "F Li",
        "X Zhu",
        "X Huang",
        "Y Zeng",
        "D Yao",
        "Y Zhang",
        "P Xu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "9",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "10",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "11",
      "title": "Toward an affect-sensitive multimodal human-computer interaction",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2003",
      "venue": "Proc. IEEE"
    },
    {
      "citation_id": "12",
      "title": "Facial expression analysis in a wild sporting environment",
      "authors": [
        "O Santana",
        "D Freire-Obregón",
        "D Hernández-Sosa",
        "J Lorenzo-Navarro",
        "E Sánchez-Nielsen",
        "M Castrillón-Santana"
      ],
      "year": "2022",
      "venue": "Facial expression analysis in a wild sporting environment"
    },
    {
      "citation_id": "13",
      "title": "Differential entropy feature for eeg-based vigilance estimation",
      "authors": [
        "L.-C Shi",
        "Y Jiao",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "14",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "W Zheng"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "16",
      "title": "Emotion Recognition from Non-Frontal Facial Images",
      "authors": [
        "W Zheng",
        "H Tang",
        "T Huang"
      ],
      "year": "2015",
      "venue": "Emotion Recognition from Non-Frontal Facial Images"
    },
    {
      "citation_id": "17",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    }
  ]
}