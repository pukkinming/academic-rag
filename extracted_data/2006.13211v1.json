{
  "paper_id": "2006.13211v1",
  "title": "Meta Transfer Learning For Emotion Recognition",
  "published": "2020-06-23T00:25:28Z",
  "authors": [
    "Dung Nguyen",
    "Sridha Sridharan",
    "Duc Thanh Nguyen",
    "Simon Denman",
    "David Dean",
    "Clinton Fookes"
  ],
  "keywords": [
    "emotional knowledge transfer",
    "emotion recognition",
    "facial expression recognition",
    "speech emotion recognition",
    "transfer learning",
    "cross-domain transfer",
    "joint leaning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning has been widely adopted in automatic emotion recognition and has lead to significant progress in the field. However, due to insufficient annotated emotion datasets, pre-trained models are limited in their generalization capability and thus lead to poor performance on novel test sets. To mitigate this challenge, transfer learning performing fine-tuning on pre-trained models has been applied. However, the finetuned knowledge may overwrite and/or discard important knowledge learned from pretrained models. In this paper, we address this issue by proposing a PathNet-based transfer learning method that is able to transfer emotional knowledge learned from one visual/audio emotion domain to another visual/audio emotion domain, and transfer the emotional knowledge learned from multiple audio emotion domains into one another to improve overall emotion recognition accuracy. To show the robustness of our proposed system, various sets of experiments for facial expression recognition and speech emotion recognition task on three emotion datasets: SAVEE, EMODB, and eN-TERFACE have been carried out. The experimental results indicate that our proposed system is capable of improving the performance of emotion recognition, making its performance substantially superior to the recent proposed fine-tuning/pre-trained models based transfer learning methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotions of humans manifest in their facial expressions, voice, gestures, and posture. An accurate emotion recognition system based on one or a combination of these modalities would be useful in various applications including surveillance, medical, robotics, human computer interaction, affective computing, and automobile safety  (Nguyen et al., 2017) . Researchers in this area have focused mainly in the area of facial expression recognition to build reliable emotion recognition systems. This is still a challenging problem since very subtle emotional changes manifested in the facial expression could go undetected  (Nguyen et al., 2017) . Recently several approaches based on deep learning techniques have contributed to progressing in this area  (Fan et al., 2016; Abbasnejad et al., 2017; Hasani and Mahoor, 2017) .\n\nIn addition to facial expression stream, speech signals, which are regarded as one of the most natural media of human communication, carry both the contents of explicit linguistic and the information of implicit paralinguistic expressed by a speaker  (Zhang et al., 2018) . Due to this rich information contained in the speech, over the last two decades numerous studies and efforts have been devoted to progressing approaches, focusing on automatic and accurate detection of human emotions from speech signals  (Zhang et al., 2018) . Speech emotion recognition is presently playing an essential role in a wide range of applications such as automobile safety, surveillance, human computer interaction, and robotics, and is attracting a great deal of attention within the affective computing research community  (Nguyen et al., 2018) .\n\nTo develop solutions for speech emotion recognition, a number of methodologies have been proposed, in which researchers have primarily applied the use of hand engineered features based on the acoustic and paralinguistic information  (Shen et al., 2011) .\n\nNevertheless, such hand-designed features seem not to be discriminative enough to boost the performance of speech emotion recognition  (Zhang et al., 2018) . Recently, algorithms based on deep learning techniques, which are capable of automatically learning features and also capable of modelling high-level information, have been the focus of most recent research and are gaining prominence.\n\nAlthough the aforementioned deep learning approaches have made a great contri-bution to progressing the emotion recognition area, we pointed out a key issue that plagues the advancement of emotion recognition research; e.g., the lack of sufficient quantities of annotated emotion data. This issue has become more critical with the advent of deep learning techniques which promise major improvements in emotion recognition accuracy in both single and multi-modal settings; yet we are unable to exploit the full potential of deep learning for emotion recognition due to the scarcity of annotated emotion data; deep learning techniques require large amounts of data for training. Transfer learning method, which commonly fine-tunes pre-trained/off-theshelf CNN models on emotion dataset, have been widely investigated to overcome this problem. However, the representational features that are unrelated to emotion are still retained in off-the-shelf/pre-trained models and the extracted features are also vulnerable to identity variations in these approaches, leading to degrading the performance of the emotion recognition system fine-tuning off-the-shelf/pre-trained models on the emotion dataset.\n\nTo resolve these drawbacks,  (Gideon et al., 2017)  have exploited a progressive network originally proposed by  (Rusu et al., 2016)  which was able to potentially support transferring knowledge across sequences of tasks.  (Gideon et al., 2017)  have successfully transferred learning between three paralinguistic tasks: emotion, speaker, and gender recognition with an emphasis on speech emotion detection as the target application without the catastrophic forgetting effect. Their system outperformed the recent speech emotion recognition approaches utilizing fine-tuning pre-trained models and also performed significantly better than deep leaning models without the use of transfer learning techniques  (Rusu et al., 2016) . Nonetheless, an unavoidable limitation of this approach is that it is computationally intensive since a number of new networks keep on growing according to the demand for a increasing number of new tasks which need to be learned  (Lee et al., 2017) . More recently, in order to alleviate the aforementioned downsides,  (Fernando et al., 2017)  have proposed PathNet as an alternative novel learning algorithm for transfer learning. PathNet was designed as a neural network in which agents (e.g., pathways through different layers of the neural network) were embedded to discover which parts of the network to be re-used for new tasks  (Fernando et al., 2017) . Agents also hold an accountability for determining which subset of parameters to be used and to be updated for subsequent learning. Pathways through the neural network for replication and mutation were selected by a tournament selection genetic algorithm proposed by  (Harvey, 2011)  during learning. The parameters along an optimal path evolved on the source task were fixed and a new population of paths for the destination task is re-evolved. Such a manner of transfer learning enables the destination task to be learned faster than learning from scratch or fine-tuning, and therefore has greatly succeeded in several supervised learning classification problems  (Fernando et al., 2017) .\n\nMotivated by the success of PathNet in those applications, in this paper, we explore utilizing PathNet for the facial expression based and speech based emotion recognition tasks. In this work, we first investigate how effective Path-Net is in transferring emotional knowledge from one visual emotion domain to another visual emotion domain to improve overall performance. Based on the experience we have gained in knowledge transferring ability of PahtNet within the visual domain in our previous work  (Nguyen et al., 2018) , we next investigate whether similar techniques can be used for transferring knowledge within the speech domain. Specifically we investigate the use of PathNet for speech emotion recognition (i) by exploring how well emotional knowledge learned from one speech emotion dataset could be transferred into another speech emotion dataset, and (ii) by examining how well emotional knowledge learned from multiple speech emotion datasets could be transferred to a single speech emotion dataset.\n\nThe contributions of our paper are as follows:\n\n• We introduce a novel transfer learning approach for the emotion recognition task by utilizing PathNet to deal with the problem of insufficient annotated emotion data as well to deal with the catastrophic forgetting issue commonly experienced with traditional transfer learning techniques.\n\n• We confirm, through experimental results, that our proposed system has a significant potential to accurately detect emotions and demonstrates its substantial success in transferring learned knowledge between different emotion datasets, as well as in transferring learned emotional knowledge from multiple speech emotion datasets to a single speech emotion dataset.\n\n• We conduct various sets of within-corpus on three commonly used bench-marking emotion datasets EMODB, eNTERFACE, and SAVEE and we show that the performance of our proposed transfer learning approach for emotion recognition exceeds recent state-of-the-art transfer learning schemes based on fine-tuning/pretrained models.\n\nThe remainder of this paper is organized as follows: Section 2 describes related research; Section 3 presents our proposed system; Section 4 reports our experimental results; and Section 5 concludes the paper.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "A number of studies using transfer learning approaches have recently been proposed for the facial expression and speech emotion recognition task. Since the literature review for the facial expression recognition task utilizing deep learning and transfer learning method was reviewed and discussed in our previous work, the interested reader is referred to that work for detailed discussion and analysis, this section will only focus on reviewing the speech emotion recognition task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech Emotion Recognition Using Deep Learning Techniques",
      "text": "Deep learning techniques have emerged as powerful solutions in a wide variety of applications including natural language processing and computer vision owing to their inherent capability of directly learning a hierarchical feature representation from the input data  (LeCun et al., 2015) . Inspired by their success in multiple fields, many deep learning approaches have been investigated for the task of speech emotion recognition.  (Kim et al., 2017a)  have proposed a novel architecture for the speech emotion recognition task, in which long short-term memory (LSTM), fully convolutional neural network (FCN), and convolutional neural network (CNN) were combined aiming at extracting local invariant features from the spectral domain. By this combination, long-term dependencies have been well captured, thereby making utterance-level features more discriminative  (Kim et al., 2017a) . Moreover, by embedding identity skipconnection techniques in their temporal architecture, this proposed system avoided the over-fitting problem caused by training on small amounts of data  (Kim et al., 2017a) . In another study, in order to handle the large mismatch between training and testing data,  (Kim et al., 2017b)  have proposed utilizing multi-task learning, and then investigated gender and naturalness as auxiliary tasks of their proposed system. This can enhance significantly the capabilities of generalizing the speech emotion recognition models.\n\nThe experimental results evaluated on within-corpus and cross-corpus scenarios have shown good performance of their proposed system.\n\nIn other approaches,  (Nguyen et al., 2017 (Nguyen et al., , 2018) )  have proposed the learning of spatio-temporal features with C3Ds from audio and video for multimodal emotion recognition.  (Kim et al., 2017c)  have also proposed three dimensional convolutional neural networks (C3Ds) to address the challenge of modeling the spectro-temporal dynamics for speech emotion recognition by simultaneously extracting short-term and long-term spectral features with a moderate number of parameters.  (Sahu et al., 2017)  have exploited the adversarial auto-encoders focusing on (i) compressing high dimensional vectors encoding emotional utterances into low space vectors (referred to as code vectors) without sacrificing the discrimination during classifying the original vectors, and (ii) generating synthetic samples applying the adversarial auto-encoder, subsequently used for emotion classification. Their system has mainly concentrated on detecting emotions at utterance level features instead of frame-level ones.  (Badshah et al., 2017)  have proposed a speech emotion recognition system, in which spectrograms were initially extracted from speech signals at different frequencies. Such spectrograms were subsequently fed into a CNN for emotion prediction. Similarly,  (Chang and Scherer, 2017)  have addressed emotional valence in human speech by directly learning spectrograms of emotional speech using a CNN. In order to further improve the performance, this architecture has been extended to a deep convolutional generative neural network which was trained in an unsupervised fashion.\n\nResearchers have also explored the whispered speech emotion recognition task, where different feature transfer learning approaches have been explored by utilizing shared-hidden-layer auto-encoders, extreme learning machines auto-encoders, and denoising auto-encoders  (Deng et al., 2017a) . The key ideas of these approaches were to develop a transformation for automatically capturing useful features hidden in data and to transfer the knowledge from the target domain-testing (whispered speech) to the source domain-training (normal phonated speech), consequently leading the great benefit regarding optimizing all parameters with the support from the test set. Extensive experiments have been conducted with a focus on entirely tackling the binary classification (i.e. valence/arousal)  (Deng et al., 2017a) . In another study,  (Deng et al., 2017b)  have also pointed out that many speech emotion recognition systems usually demonstrate poor performance on speech data when there is significant differences between training and test speech arising from the variations in the linguistic content, speaker accents, and domain/environmental conditions  (Deng et al., 2017b) . To further improve such systems performing under the mismatched training and testing condition, a novel unsupervised domain adaptation algorithm has been introduced and trained by simultaneously learning discriminative information from labeled data and incorporating the prior knowledge from unlabeled data into the learning.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Speech Emotion Recognition Using Transfer Learning Techniques",
      "text": "However, recent studies into emotion recognition have been hindered by the lack of large databases for learning  (Zhang et al., 2017; Kaya et al., 2017) . To address the lack of large emotion datasets, the fine-tuning/pre-trained model has been recently widely investigated for the emotion recognition task  (Kaya et al., 2017; Zhang et al., 2018; Kim et al., 2017a,c,b; Latif et al., 2018)  in which the CNN architectures were pre-trained using the generic ImageNet dataset and fine-tuned on emotion datasets  (Ng et al., 2015; Kaya et al., 2017) . To learn audio features,  (Zhang et al., 2017)  used a pre-trained C3D models on large-scale image and video classification datasets, and then fine-tuned them on emotion recognition tasks. To improve the performance of a speech emotion recognition system on such challenging conditions as cross-corpus and cross-language scenario,  (Latif et al., 2018)  have proposed a transfer learning technique using deep belief networks (DBNs). The experimental results evaluated on five different corpora in three different languages demonstrated the robustness of their system. These results also indicated that use of a large number of languages and a small part of the target data during training could dramatically strengthen the emotion recognition accuracy.\n\nHowever, since they have attempted to validate the system on five different datasets annotated differently, their system only focused on addressing the classification for binary positive/negative valence.\n\nIn more recent studies,  (Zhang et al., 2018)  have reconfirmed that the low-level hand-engineered features seem not to be discriminative enough to recognize the subjective emotions  (Zhang et al., 2018) . To address this disadvantage,  (Zhang et al., 2018)  have proposed to extract three channels of log Mel-spectrograms (static, delta, and delta delta corresponding to red, green, and blue in the RGB model of images) from segments over all utterances, and the pre-trained AlexNet model was then finetuned on those extracted features. A discriminant temporal pyramid matching technique was subsequently combined with optimal Lp-norm pooling, before exploiting a linear support vector machine to classify the final speech emotion score. Although this architecture demonstrated sufficient performance on tackling discrete speech emotion recognition, the system was unable to address the continuous emotion recognition task  (Zhang et al., 2018) .  (Nguyen et al., 2020)  have proposed a joint deep cross-domain transfer learning for emotion recognition which was able to effectively jointly transfer the knowledge learned from rich datasets to source-poor datasets. Moreover, as discussed earlier, all of these fine-tuning approach based systems  (Zhang et al., 2018 (Zhang et al., , 2017;; Ng et al., 2015; Kaya et al., 2017)  still have the drawbacks previously alluded to such as discarding previously learned information which were detailed by  (Rusu et al., 2016) . To mitigate these drawbacks,  (Gideon et al., 2017)  have introduced a learning algorithm using the progressive networks proposed by  (Rusu et al., 2016)  to effectively transfer knowledge captured from one emotion dataset into another. Although they have handled somewhat successfully the above mentioned limitations, their expensive computation, which kept on increasing when adding new tasks to be learned, makes them less applicable in the implementation of emotion recognition for real world applications.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Proposed Methodology",
      "text": "Our main goal in this paper is to investigate techniques to improve the accuracy of deep learning based emotion recognition task which is hindered by the non avail-ability of large annotated emotion datasets. We achieve this goal by using the recently proposed innovation known as PathNet for transfer learning. We propose the use of a novel transfer learning approach by adopting PathNet to solve the facial expression emotion recognition, and the speech emotion recognition task. Our proposed system is illustrated by simple block diagram consisting of two main blocks which are an input pre-processing block (for video and speech) followed by our proposed PathNet block along with the output classifying block (as illustrated in Fig.  3 ). The video and audio stream are initially pre-processed. For video stream, we initially exploit a Viola Jonesbased algorithm  (Nguyen et al., 2017)  to extract all face regions from both SAVEE and eNTERFACE  (Martin et al., 2006)  datasets. This is described in detail in Section 3.1. For audio stream, we also initially extract three channels of log Mel-spectrograms (static, delta, and delta delta corresponding to red, green, and blue) from segments over all utterances from eNTERFACE  (Martin et al., 2006) , SAVEE  (Haq and Jackson, 2010) , and EMO-DB  (Burkhardt et al., 2005 ) and these steps are described in detail in Section 3.2. These extracted video and audio features are subsequently fed into our PathNet to classify a final facial expression and speech emotion score, respectively. To the best of our knowledge, the use of PathNet has not been previously investigated in dealing with the dearth of suitable emotion databases for the development of emotion recognition system. The procedures of feature extraction and our PathNet architecture are described in more detail in the following subsections.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Video Pre-Processing",
      "text": "All frames are initially extracted from visual signal for further steps. Since such extracted frames still contain considerable redundant information for emotion detection, we extract only the face regions using the simple algorithm  (Nguyen et al., 2017)  as follows:\n\n1. All bounding boxes containing face regions in each frame were extracted employing the Viola-Jones algorithm  (Viola and Jones, 2004 ) and a face region was then detected.\n\n2. In some cases where the Viola Jones algorithm detected no faces, or more than 1 face, the location of the previously detected face region was used.\n\nBy applying this algorithm, we have successfully extracted all face regions from all frames in both datasets (SAVEE and eNTERFACE) as input into PathNet (see in Fig.  2 (a) . as an illustration of some input samples) 3.2. Audio pre-processing  (Zhang et al., 2018)  have pointed out that the low-level hand-engineered features, which includes RASTA-PLP  (Hermansky et al., 1992) , pitch frequency features, energyrelated features  (Ververidis et al., 2004) , formant frequency  (Xiao et al., 2005) , Zero Crossing Rate (ZCR)  (Rabiner and Sambur, 1975) , Mel-Frequency Cepstrum Coefficients (MFCC) and its first derivative, Linear Prediction Cepstrum Coefficients (LPCC), Linear Prediction Coefficients (LPC)  (Pao et al., 2006; Shen et al., 2011) , seem not to be discriminative enough for recognizing the subjective emotions  (Zhang et al., 2018) .\n\nTherefore, in order to boost the performance of speech emotion recognition system, instead of exploiting such hand-crafted features,  (Zhang et al., 2018)",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Pathnets",
      "text": "Our PathNet architecture and its settings relies on PathNets  (Fernando et al., 2017)  which was used to conduct all sets of experiments on CIFAR  (Krizhevsky and Hinton, 2009)  and SVHN  (Netzer et al., 2011) . The following sections will provide in detail its architecture and explain further how to train our system, how to transfer learned emotional knowledge between emotion dataset.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Pathnet Architecture",
      "text": "Our PathNets includes a number of layers (L = 3), a number of modules (M = 20) per layer of 20 neurons in each. Each module itself functions as a neural network consisting of linear units, and followed by a transfer function (rectified linear units adopted). For each layer the outputs of the modules of this layer are averaged before being fed into the active modules of the subsequent layer. A module is active if it is shown in the path genotype and currently validated (shown in Fig.  3 (b) ). A maximum of 4 distinct modules per layer are typically allowed in a pathway. The final layer is not shared for each task which is being learned  (Fernando et al., 2017) .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Pathway Evolution And Transfer Learning Approach",
      "text": "One emotion dataset (source data) is trained for a fixed number of generations with a goal of finding an optimal pathway by adopting a binary tournament selection algorithm proposed by  (Harvey, 2011 ) (see Fig.  1 ) which takes responsibility for eliminating bad configurations and mutating good ones, and subsequently training them further. This pathway is then fixed. This means that its parameters are no longer permitted to modify and the rest of parameters, which are not shown in such best fit path, are reinitialized, and are then again trained/evolved on the another emotion dataset (destination data). Through this knowledge transferring approach, the destination data is permitted to be learned faster than learning from scratch or after fine-tuning. The performance measurement of our proposed system is the recognition accuracy achieved after such fixed training time. Evidence to confirm a positive transfer in these cases is given by a better final recognition accuracy of our proposed system achieved when trained on destination data than that achieved by learning from scratch.\n\nWhen PathNet is trained on source data, at the beginning, a population of genotypes is randomly generated (See Fig.  4 ). In each generation, two pathways are randomly selected to train on source data (see Fig.  5 ). The reason only two pathways are selected is that binary tournament selection is exploited to choose the pathways. One and the winner is then mutated with equal probability 1/(4×3) per each candidate of the genotype (see Fig.  6 , and Fig.  7 ), a new random integer from range [-2, 2] is added to the current value of the winner candidate. We repeat step 2 (Fig.  5 ) and step 3 (Fig.  6 ) for number of generations. When training on source data is completed we achieve best pathway (see Fig.  8 ).\n\nThe parameters presented in this best fit pathway are fixed and are reused for training on the destination data and the rest of parameters are randomly reinitialized. When training PathNet on the destination data, the procedures for this training stage are sim-  by only reading textual explanation that is presented in this section. Therefore, in order to make our PathNet based transfer learning approach more understandable, we have added further explanation on the progression of achieving the best pathway by visualizing every step using the corresponding figure (please see ). As we can see these Figs, to ease for readers to view and follow step by step, we simplify the architecture by drawing only ten modules in each layers, up to two modules are activated in each layer, and a population of four random pathways are initialized. Whereas the  are drawn to further explain and visualize the progression of how to achieve best pathway for one particular set of experiment. These Figs illustrate exact parameters and architectures corresponding to the progression in gaining the optimal pathway when conducting the set of experiment. We believe that this explanation is more comprehensive than done by the original PathNet paper  (Fernando et al., 2017)  as in that paper authors did not illustrate such steps using figures.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Experiments & Results",
      "text": "Dataset Details: The eNTERFACE dataset  (Martin et al., 2006 ) is an audio-visual dataset which has 44 subjects and includes a total of 1293 video sequences in which the proportion of sequences corresponding to women and men are 23% and 77%, respectively. They were asked to express 6 discrete emotions including anger, disgust, fear, happiness, sadness, and surprise  (Martin et al., 2006) .\n\nThe SAVEE dataset  (Haq and Jackson, 2010 ) is an audio-visual dataset which was recorded by higher degree researchers (aged from 27 to 31 years) at the University of Surrey, and four native male British speakers. All of them were also required to speak and express seven discrete emotions such as anger, disgust, fear, happiness, sadness, surprise, and neutral. The dataset comprises of 120 utterances per speaker, resulting in a total of 480 sentences  (Haq and Jackson, 2010) .\n\nThe EMO-DB dataset  (Burkhardt et al., 2005)  is an acted speech corpus containing 535 emotional utterances with seven different acted emotions listed as disgust, anger, neutral, sadness, boredom, and fear. These emotions were stimulated by five male and five female professional native German-speaking actors, generating five long and five short sentences German utterances used in daily communication. These actors were asked to read predefined sentences in the targeted seven emotions. The audio files are on average around 3 seconds long were recorded using an anechoic chamber with highquality recording equipment at a sampling rate of 16 kHz with a 16-bit resolution and mono channel  (Zhang et al., 2018) .\n\nPerformance Measures: We evaluate our proposed system on all three datasets: the SAVEE, the eNTERFACE, and the EMO-DB dataset. We apply k-fold crossvalidation, the original training data is randomly divided into k equal parts. Of the k-parts, one of them is fixed as the validation data for testing the model, and the other k-1 parts are used as training data. The cross-validation process is then repeated 5 times. We also apply the leave-one-subject-out cross-validation protocol which means that we have to conduct N experiments if the dataset consists of N subjects. For each experiment, N-1 subjects are included in the training set and the remaining subject is used for the testing set and all of our sets of experiments are carried out in a subject independent manner. Apart from these, Weighted Averaged Recall (WAR) has been recently adopted and has become widely a standard measure for evaluating the performance of emotion recognition systems  (Zhang et al., 2018) . In addition, Unweighted Averaged Recall (UAR)  (Eyben et al., 2016; Eyben, 2016; Zhang et al., 2018)  has been also popularly adopted to evaluate this performance with respect to reflecting unbalance between emotional classes  (Zhang et al., 2018) . Therefore, in order to fairly compare Table  1 : Different measures for multi-class classification C i . For class C i , tp i are true positive, and f p ifalse positive, f n i -false negative, and tn i -true negative counts, l -the number of classes,  (Sokolova and Lapalme, 2009) .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Measure Formula Evaluation Focus",
      "text": "Unweighted Averaged Precision (UAP) with some recent state-of-the-art speech emotion recognition schemes using the same above-mentioned measures (WAR, UAR), in this paper, we also compute and compare both Weighted Averaged Recall and Unweighted Averaged Recall to validate the performance of our proposed system. Table  1  illustrates the detail formulas on how to calculate these evaluating measures.\n\nAs our baseline systems, we use the methodology proposed by  (Zhang et al., 2018)  and a off-the-shelf CNN (AlexNet) as our baseline systems. Additionally, we also implement additional baseline systems and each of which is trained on the same emotion dataset from scratch using PathNet.\n\nSince our main focus of this paper is to address the issue of lack of emotion data for deep learning techniques. We clearly introduced this problem in the Abstract and further discussed it in the Introduction section. To make compatible with the problem we are trying to solve, we should choose small emotion datasets to show that our proposed methodology demonstrates a robust performance on such poor data. Furthermore, all large-scale datasets such as CK+, RAF-DB and AffectNet only can be used for facial expression recognition which is not the main task of this paper. It is, however, noteworthy that addressing the issue of insufficient data for speech emotion recognition task is the main work of our paper.\n\nWe conduct various sets of experiments using our proposed system in examining",
      "page_start": 16,
      "page_end": 19
    },
    {
      "section_name": "Method War (%)",
      "text": "Fine-Tuned AlexNet  (Zhang et al., 2018)  0.85",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Method War (%)",
      "text": "Fine-Tuned Alexnet  (Zhang et al., 2018)  0.88 V eNTER 0.88\n\nas follows: the parameters presented in the best fit pathway, which is evolved on the source data, are fixed and the rest of parameters are randomly reinitialized and a new population of 20 of genotypes are randomly initialized and then are trained/evolved further on the destination data.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Experimental Results",
      "text": "In this section, we report, analyze, and compare experimental results of our proposed system on aforementioned sets of experiments. In all the experiments, we have taken meticulous care to ensure that the test data is never used for training.\n\nIn the first two sets of experiments, we conduct experiments to show the robustness of our proposed system when performing under the condition of insufficient data for facial expression recognition. Our proposed system is initially trained on visual eN-TERFACE, and is then evolved on visual SAVEE and vice versa. Since SAVEE dataset consists of an additional type of emotion (neutral) compared to eNTERFACE dataset, hence to be consistent between these two datasets, only shared types of emotion including anger, surprise, disgust, fear, happiness, and sadness are detected via these two sets of experiments. The experimental results of our proposed system on this evaluation are shown in the first two rows of Table  3 .   3 , our proposed system (V eNTER→SAV ), which transfers the emotional knowledge from visual eNTER-FACE to visual SAVEE, achieves 94% of facial expression recognition accuracy in regard to WAR. This accuracy is 5% and 9% significant higher than those achieved by our facial expression recognition baseline systems: V SAV which is trained on visual SAVEE from scratch and the system fine-tuning on AlexNet proposed by  (Zhang et al., 2018) , respectively (see Table  4 ).\n\nVisual SAVEE → Visual eNTERFACE: To further show the efficiency of our proposed system in solving the issue of insufficient facial expression data, we explore transferring the emotional knowledge from visual SAVEE to visual eNTERFACE. The experimental results are illustrated in the second row of Table  3 . In this setting, our proposed system also performs significantly better than our facial expression recognition baseline systems, achieving the best facial expression recognition accuracy regarding to WAR (94%), which is 6.5% and 6.35% better than those obtained by our facial expression recognition baseline systems: V eNTER which is trained on visual eNTER from scratch and the system fine-tuning on AlexNet proposed by  (Zhang et al., 2018)  Table  6 : Results of our proposed system when transferring the emotional knowledge from audio eNTER-FACE to audio SAVEE in comparison with the best baseline speech emotion recognition system.",
      "page_start": 22,
      "page_end": 25
    },
    {
      "section_name": "Method War",
      "text": "AlexNet  (Zhang et al., 2018)  0.69 A SAV 0.81\n\nTo depict the performance of individual emotion of these two systems, the confusion matrices of their systems (V eNTER→SAV and V SAV→eNTER ) are illustrated in Fig.  14  (a), and Fig.  14  (b), respectively. We also visualize the robust performances of each emotion of these two systems, and plot their ROC curves which are illustrated in Fig.   6 . It can be seen that our proposed system (A eNTER→SAV with the best speech emotion recognition accuracy of 85%) demonstrates a significant superior performance over the speech emotion recognition system proposed by (Zhang   7  that our proposed system (97%) performs significantly better than the transfer learning approach based on pre-trained/fine-tuning models  (Zhang et al., 2018 ) by 14% and our baseline system (A EMO which have been trained on audio EMODB from scratch using PathNet) by 8% in regard to WAR.\n\nSimilarly, in order to further gain insight into the performances of individual speech emotions of both systems (A eNTER→SAV and A eNTER+SAV→EMO ), we illustrate the confusion matrix of these speech emotion recognition systems (see Fig.  18  (a), and Fig.      This transfer learning approach is also considered as one of the most recent state-of- the-art methods. However, as pointed out earlier pre-trained models are still limited in their generalization capability and thus lead to poor performance on novel test sets.\n\nThe main objective of this paper is to propose an alternative transfer learning approach to address such disadvantages. Therefore, we have primarily compared our transfer learning method using PathNet with a state-of-the-art transfer learning method relying on fine-tuning/pre-trained models as reported in Experiments & Results Section. The transfer learning method that we have compared against my PathNet based approach using the same datasets is fined-tuned AlexNet. Moreover, unfortunately we could not find any other state of the art transfer learning method which uses the same datasets to make a fair comparison. Our proposed system outperforms the state-of-the-art emotion recognition system relying on the transfer learning method which fine-tunes offthe-shelf/pre-trained models. We believe that this is due to the fact that the representational features that are unrelated to emotion are still retained in off-the-shelf/pre-trained models and the extracted features are also vulnerable to identity variations in these approaches, leading to degrading the performance of the emotion recognition system fine-tuning off-the-shelf/pre-trained models on the emotion dataset.\n\nAs presented in Proposed Methodology Section, we have designed our PathNet ar-",
      "page_start": 26,
      "page_end": 30
    },
    {
      "section_name": "Conclusion",
      "text": "Progress in emotion recognition research has been hindered by the lack of the large amounts of labeled emotion data. To overcome this problem, various studies have widely explored the use of transfer learning approach based on pre-trained/fine-tuning models, however, the proposed approaches have been still suffering from issues such as discarding prior learned information. In this paper, we have proposed utilizing an alternative transfer learning technique using PathNet which is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to reuse for new tasks, leading to successfully addressing the abovementioned challenges. To verify performance of our proposed architecture, we have conducted various sets of experiments including, transferring the emotional knowledge from one emotion dataset into another, and transferring the learned emotional knowledge from multiple emotion datasets into one another. Experimental results on three datasets: eNTERFACE, SAVEE, and EMO-DB have indicated that our proposed system performs well under the conditions of insufficient emotion data, and significantly better than the recent transfer learning techniques exploiting fine-tuning/pre-trained models.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Acknowledgement",
      "text": "This research was supported by an Australian Research Council (ARC) Discovery grant DP140100793.",
      "page_start": 33,
      "page_end": 33
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The genotypes of the population are viewed as a pool of strings. One single cycle of the Microbial",
      "page": 4
    },
    {
      "caption": "Figure 2: This ﬁgure illustrates the pre-processing steps for video and audio stream.",
      "page": 9
    },
    {
      "caption": "Figure 3: ). The video and audio",
      "page": 10
    },
    {
      "caption": "Figure 2: (a). as an illustration of some input samples)",
      "page": 11
    },
    {
      "caption": "Figure 3: Illustrates our proposed emotion recognition system",
      "page": 12
    },
    {
      "caption": "Figure 3: (b)). A maximum",
      "page": 13
    },
    {
      "caption": "Figure 1: ) which takes responsibility for eliminat-",
      "page": 13
    },
    {
      "caption": "Figure 4: ). In each generation, two pathways are randomly",
      "page": 13
    },
    {
      "caption": "Figure 5: ). The reason only two pathways are se-",
      "page": 13
    },
    {
      "caption": "Figure 4: A number of pathways is randomly initialized when pathnet learning on source data",
      "page": 14
    },
    {
      "caption": "Figure 5: Two pathways are selected to train on source data",
      "page": 14
    },
    {
      "caption": "Figure 6: , and Fig. 7), a new random integer from range [-2, 2] is added",
      "page": 14
    },
    {
      "caption": "Figure 5: ) and step 3 (Fig.",
      "page": 14
    },
    {
      "caption": "Figure 6: A pathway with bad performance (loser) is replaced by a pathway with better performance (winner)",
      "page": 15
    },
    {
      "caption": "Figure 7: Winner is mutated",
      "page": 15
    },
    {
      "caption": "Figure 8: Best pathway achieved when completed training on the source data",
      "page": 15
    },
    {
      "caption": "Figure 9: , Fig. 10,",
      "page": 15
    },
    {
      "caption": "Figure 11: ). At the beginning, a new population of genotypes is randomly initialized",
      "page": 15
    },
    {
      "caption": "Figure 9: A number of pathways are randomly initialized when pathnet trained on the destination data",
      "page": 16
    },
    {
      "caption": "Figure 10: Two random pathways are also selected to train on the destination data",
      "page": 16
    },
    {
      "caption": "Figure 11: A pathway with bad performance (loser) is replaced by a pathway with better performance (win-",
      "page": 17
    },
    {
      "caption": "Figure 12: The winner is then mutated",
      "page": 17
    },
    {
      "caption": "Figure 13: Best pathway is achieved when completed training on the destination data",
      "page": 17
    },
    {
      "caption": "Figure 14: Illustrates the confusion matrix of our proposed system evaluated on visual SAVEE and visual",
      "page": 23
    },
    {
      "caption": "Figure 15: Illustrates testing learning curves of our proposed system during learning on visual SAVEE and",
      "page": 23
    },
    {
      "caption": "Figure 16: Receiver operating characteristic (ROC) curves of our proposed system on visual SAVEE and",
      "page": 24
    },
    {
      "caption": "Figure 14: (a), and Fig. 14 (b), respectively. We also visualize the robust performances of each",
      "page": 25
    },
    {
      "caption": "Figure 16: (a) and Fig. 16 (b), respectively. Moreover, in order to further visualize the effec-",
      "page": 25
    },
    {
      "caption": "Figure 15: As demonstrated in Fig. 15, the performance of both proposed",
      "page": 25
    },
    {
      "caption": "Figure 18: (a), and Fig.",
      "page": 26
    },
    {
      "caption": "Figure 17: We illustrate receiver operating characteristic (ROC) curves of individual emotions evaluated on",
      "page": 27
    },
    {
      "caption": "Figure 18: Illustrates confusion matrix of our proposed system evaluated on audio SAVEE and audio",
      "page": 27
    },
    {
      "caption": "Figure 17: (a) and Fig. 17 (b). Specif-",
      "page": 27
    },
    {
      "caption": "Figure 19: A population of 20 pathways are randomly initialized when learning on the source data (audio",
      "page": 28
    },
    {
      "caption": "Figure 17: (a)). It can also be seen that our AeNTER+SAV→EMO",
      "page": 28
    },
    {
      "caption": "Figure 20: The optimal pathway (highlighted by red colour) is achieved when the training stage is completed",
      "page": 29
    },
    {
      "caption": "Figure 21: A new population of pathways are generated when learning on the destination data (audio SAVEE)",
      "page": 29
    },
    {
      "caption": "Figure 22: The pathway highlighted by red colour, which have been transferred from the model on the source",
      "page": 30
    },
    {
      "caption": "Figure 23: The optimal pathway highlighted by blue colour is achieved when training stage is completed on",
      "page": 31
    },
    {
      "caption": "Figure 19: , Fig. 20, Fig. 21, Fig.",
      "page": 32
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PathNet is trained from scratch on visual eNTERFACE": "PathNet is trained from scratch on visual SAVEE",
          "V eNTER": "V SAV"
        },
        {
          "PathNet is trained from scratch on visual eNTERFACE": "PathNet is trained from scratch on audio SAVEE",
          "V eNTER": "A SAV"
        },
        {
          "PathNet is trained from scratch on visual eNTERFACE": "PathNet is trained from scratch on audio EMODB",
          "V eNTER": "A EMO"
        },
        {
          "PathNet is trained from scratch on visual eNTERFACE": "From visual eNTERFACE to visual SAVEE",
          "V eNTER": "V eNTER→ SAV"
        },
        {
          "PathNet is trained from scratch on visual eNTERFACE": "From visual SAVEE to visual eNTERFACE",
          "V eNTER": "V SAV→ eNTER"
        },
        {
          "PathNet is trained from scratch on visual eNTERFACE": "From audio eNTERFACE to audio SAVEE",
          "V eNTER": "A eNTER→ SAV"
        },
        {
          "PathNet is trained from scratch on visual eNTERFACE": "From audio eNTERFACE and audio SAVEE to audio\nEMODB",
          "V eNTER": "A eNTER+SAV→\nEMO"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "VeNTER→SAV": "VSAV→eNTER",
          "0.93": "0.99",
          "0.94": "0.94",
          "0.96": "0.95",
          "0.92": "0.81",
          "0.95": "0.93"
        },
        {
          "VeNTER→SAV": "AeNTER→SAV",
          "0.93": "0.73",
          "0.94": "0.71",
          "0.96": "0.56",
          "0.92": "0.72",
          "0.95": "0.77"
        },
        {
          "VeNTER→SAV": "AeNTER+SAV→EMO",
          "0.93": "0.99",
          "0.94": "0.97",
          "0.96": "1.0",
          "0.92": "0.94",
          "0.95": "0.96"
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Using synthetic data to improve facial expression analysis with 3d convolutional networks",
      "authors": [
        "I Abbasnejad",
        "S Sridharan",
        "D Nguyen",
        "S Denman",
        "C Fookes",
        "S Lucey"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Computer Vision Workshops (ICCVW)"
    },
    {
      "citation_id": "2",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Platform Technology and Service"
    },
    {
      "citation_id": "3",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proceedings of Interspeech, Lissabon"
    },
    {
      "citation_id": "4",
      "title": "Learning representations of emotional speech with deep convolutional generative adversarial networks",
      "authors": [
        "J Chang",
        "S Scherer"
      ],
      "year": "2017",
      "venue": "Learning representations of emotional speech with deep convolutional generative adversarial networks"
    },
    {
      "citation_id": "5",
      "title": "Recognizing emotions from whispered speech based on acoustic feature transfer learning",
      "authors": [
        "J Deng",
        "S Frhholz",
        "Z Zhang",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "6",
      "title": "Universum autoencoderbased domain adaptation for speech emotion recognition",
      "authors": [
        "J Deng",
        "X Xu",
        "Z Zhang",
        "S Frhholz",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "7",
      "title": "Real-time speech and music classification by large audio feature space extraction",
      "authors": [
        "F Eyben"
      ],
      "year": "2016",
      "venue": "Real-time speech and music classification by large audio feature space extraction"
    },
    {
      "citation_id": "8",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E Andr",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Video-based emotion recognition using CNN-RNN and C3D hybrid networks",
      "authors": [
        "Y Fan",
        "X Lu",
        "D Li",
        "Y Liu"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "10",
      "title": "Pathnet: Evolution channels gradient descent in super neural networks",
      "authors": [
        "C Fernando",
        "D Banarse",
        "C Blundell",
        "Y Zwols",
        "D Ha",
        "A Rusu",
        "A Pritzel",
        "D Wierstra"
      ],
      "year": "2017",
      "venue": "Pathnet: Evolution channels gradient descent in super neural networks"
    },
    {
      "citation_id": "11",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Progressive neural networks for transfer learning in emotion recognition"
    },
    {
      "citation_id": "12",
      "title": "Machine Audition: Principles, Algorithms and Systems",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2010",
      "venue": "Machine Audition: Principles, Algorithms and Systems"
    },
    {
      "citation_id": "13",
      "title": "The microbial genetic algorithm",
      "authors": [
        "I Harvey"
      ],
      "year": "2009",
      "venue": "The microbial genetic algorithm"
    },
    {
      "citation_id": "14",
      "title": "Facial expression recognition using enhanced deep 3D convolutional neural networks",
      "authors": [
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "Facial expression recognition using enhanced deep 3D convolutional neural networks"
    },
    {
      "citation_id": "15",
      "title": "Rasta-plp speech analysis technique, in: 1992 ICASSP",
      "authors": [
        "H Hermansky",
        "N Morgan",
        "A Bayya",
        "P Kohn"
      ],
      "year": "1992",
      "venue": "Rasta-plp speech analysis technique, in: 1992 ICASSP"
    },
    {
      "citation_id": "16",
      "title": "Video-based emotion recognition in the wild using deep transfer learning and score fusion",
      "authors": [
        "H Kaya",
        "F Grpnar",
        "A Salah"
      ],
      "year": "2017",
      "venue": "Multimodal Sentiment Analysis and Mining in the Wild Image and Vision Computing"
    },
    {
      "citation_id": "17",
      "title": "Deep temporal models using identity skip-connections for speech emotion recognition",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Proceedings of ACM Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Towards speech emotion recognition 'in the wild' using aggregated corpora and deep multi-task learning",
      "authors": [
        "J Kim",
        "G Englebienne",
        "K Truong",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Proceedings of the INTERSPEECH"
    },
    {
      "citation_id": "19",
      "title": "Learning spectro-temporal features with 3DCNNs for speech emotion recognition",
      "authors": [
        "J Kim",
        "K Truong",
        "G Englebienne",
        "V Evers"
      ],
      "year": "2017",
      "venue": "Proceedings of International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "20",
      "title": "Learning multiple layers of features from tiny images. Master's thesis",
      "authors": [
        "A Krizhevsky",
        "G Hinton"
      ],
      "year": "2009",
      "venue": "Learning multiple layers of features from tiny images. Master's thesis"
    },
    {
      "citation_id": "21",
      "title": "Cross corpus speech emotion classification-an effective transfer learning technique",
      "authors": [
        "S Latif",
        "R Rana",
        "S Younis",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Cross corpus speech emotion classification-an effective transfer learning technique"
    },
    {
      "citation_id": "22",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "NATURE"
    },
    {
      "citation_id": "23",
      "title": "Overcoming catastrophic forgetting by incremental moment matching",
      "authors": [
        "S Lee",
        "J Kim",
        "J Jun",
        "J Ha",
        "B Zhang"
      ],
      "year": "2017",
      "venue": "Overcoming catastrophic forgetting by incremental moment matching"
    },
    {
      "citation_id": "24",
      "title": "The eNTERFACE' 05 audio-visual emotion database, in: Data Engineering Workshops",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proceedings. 22nd International Conference on"
    },
    {
      "citation_id": "25",
      "title": "Reading digits in natural images with unsupervised feature learning",
      "authors": [
        "Y Netzer",
        "T Wang",
        "A Coates",
        "A Bissacco",
        "B Wu",
        "A Ng"
      ],
      "year": "2011",
      "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning"
    },
    {
      "citation_id": "26",
      "title": "Deep learning for emotion recognition on small datasets using transfer learning",
      "authors": [
        "H Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "ICMI"
    },
    {
      "citation_id": "27",
      "title": "Meta transfer learning for facial emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "I Abbasnejad",
        "D Dean",
        "C Fookes"
      ],
      "year": "2018",
      "venue": "Meta transfer learning for facial emotion recognition"
    },
    {
      "citation_id": "28",
      "title": "Deep spatiotemporal feature fusion with compact bilinear pooling for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "D Dean",
        "C Fookes"
      ],
      "year": "2018",
      "venue": "Deep spatiotemporal feature fusion with compact bilinear pooling for multimodal emotion recognition"
    },
    {
      "citation_id": "29",
      "title": "Deep spatio-temporal features for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "A Ghasemi",
        "D Dean",
        "C Fookes"
      ],
      "year": "2017",
      "venue": "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "30",
      "title": "Joint deep cross-domain transfer learning for emotion recognition",
      "authors": [
        "D Nguyen",
        "S Sridharan",
        "D Nguyen",
        "S Denman",
        "S Tran",
        "R Zeng",
        "C Fookes"
      ],
      "year": "2020",
      "venue": "Joint deep cross-domain transfer learning for emotion recognition",
      "arxiv": "arXiv:2003.11136"
    },
    {
      "citation_id": "31",
      "title": "Mandarin emotional speech recognition based on SVM and NN",
      "authors": [
        "T Pao",
        "Y Chen",
        "J Yeh",
        "P Li"
      ],
      "year": "2006",
      "venue": "18th International Conference on Pattern Recognition (ICPR'06)"
    },
    {
      "citation_id": "32",
      "title": "An algorithm for determining the endpoints of isolated utterances. The Bell System Technical",
      "authors": [
        "L Rabiner",
        "M Sambur"
      ],
      "year": "1975",
      "venue": "Journal"
    },
    {
      "citation_id": "33",
      "title": "",
      "authors": [
        "A Rusu",
        "N Rabinowitz",
        "G Desjardins",
        "H Soyer",
        "J Kirkpatrick",
        "K Kavukcuoglu",
        "R Pascanu",
        "R Hadsell"
      ],
      "year": "2016",
      "venue": ""
    },
    {
      "citation_id": "34",
      "title": "Adversarial auto-encoders for speech based emotion recognition",
      "authors": [
        "S Sahu",
        "R Gupta",
        "G Sivaraman",
        "C Espy-Wilson",
        "W Abdalmageed"
      ],
      "year": "2017",
      "venue": "Adversarial auto-encoders for speech based emotion recognition"
    },
    {
      "citation_id": "35",
      "title": "Automatic speech emotion recognition using support vector machine",
      "authors": [
        "P Shen",
        "Z Changjun",
        "X Chen"
      ],
      "year": "2011",
      "venue": "Proceedings of 2011 International Conference on Electronic Mechanical Engineering and Information Technology"
    },
    {
      "citation_id": "36",
      "title": "A systematic analysis of performance measures for classification tasks",
      "authors": [
        "M Sokolova",
        "G Lapalme"
      ],
      "year": "2009",
      "venue": "Information Processing & Management"
    },
    {
      "citation_id": "37",
      "title": "Automatic emotional speech classification",
      "authors": [
        "D Ververidis",
        "C Kotropoulos",
        "I Pitas"
      ],
      "year": "2004",
      "venue": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Robust real-time face detection",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2004",
      "venue": "Int. J. Comput. Vision"
    },
    {
      "citation_id": "39",
      "title": "Features extraction and selection for emotional speech classification",
      "authors": [
        "Z Xiao",
        "E Dellandrea",
        "W Dou",
        "L Chen"
      ],
      "year": "2005",
      "venue": "Signal Based Surveillance"
    },
    {
      "citation_id": "40",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "41",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "S Zhang",
        "T Huang",
        "W Gao",
        "Q Tian"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology PP"
    }
  ]
}