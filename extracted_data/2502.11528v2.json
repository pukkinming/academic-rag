{
  "paper_id": "2502.11528v2",
  "title": "A Survey Of Personalized Large Language Models: Progress And Future Directions",
  "published": "2025-02-17T07:58:31Z",
  "authors": [
    "Jiahong Liu",
    "Zexuan Qiu",
    "Zhongyang Li",
    "Quanyu Dai",
    "Wenhao Yu",
    "Jieming Zhu",
    "Minda Hu",
    "Menglin Yang",
    "Tat-Seng Chua",
    "Irwin King"
  ],
  "keywords": [
    "github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models Large Language Models",
    "Personalization",
    "Memory",
    "Text Generation",
    "Recommendation One-Size-Fits-All Flaw General LLM One-Size-Fits-One Personalized LLM Different Personalities!"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In recent years, substantial progress has been made in Large Language Models (LLMs) such as GPT, PaLM, LLaMA, DeepSeek, and their variants  [Zhao et al., 2023] . These models have demonstrated remarkable versatility, achieving state-of-the-art performance across various natural language processing (NLP) tasks, including question answering, logical reasoning, and machine translation  [Chang et al., 2024 , Hu et al., 2024 , Zhang et al., 2024a ,b, Zhu et al., 2024a , Wang et al., 2023a , 2024a , Li et al., 2024a , Zhao et al., 2025a , Zhu et al., 2025] , with minimal task-specific adaptation.\n\nThe Necessity of Personalized LLMs (PLLMs) While LLMs excel in general knowledge and multi-domain reasoning, their lack of personalization creates challenges in situations where user-specific understanding is crucial. For instance, conversational agents need to adapt to a user's preferred tone and incorporate past interactions to deliver relevant, personalized responses. As LLMs evolve, integrating personalization capabilities has become a promising direction for advancing human-AI interaction across diverse domains such as recommendation, education, healthcare, and finance  [Wang et al., 2024b , Hu et al., 2024 , Zhang et al., 2024a ,b, Zhu et al., 2024a , Wang et al., 2023a , 2024a] . Despite its promise, personalizing LLMs presents several challenges. These include efficiently representing and integrating diverse user data, addressing privacy concerns, managing long-term user memories, inferring users' implicit preferences, etc  [Salemi et al., 2023] . Moreover, achieving personalization often requires balancing accuracy and efficiency while addressing biases and maintaining fairness in the outputs.\n\nContributions Despite growing interest, the field of PLLMs lacks a systematic review that consolidates recent advancements. This survey aims to bridge the gap by systematically organizing existing research on PLLMs and offering insights into their methodologies and future directions. The contributions of this survey are as follows:\n\n(1) A systematic formulation of personalization scenarios: We provide precise problem formulations for different personalized LLM scenarios based on query types and personalized data characteristics, establishing a unified framework that distinguishes between different personalized techniques (Section 2.2).\n\n(2) A structured taxonomy: We propose a comprehensive taxonomy, providing a technical perspective on the existing approaches to building PLLMs across three levels: input-level prompting, model-level adaptation, and objective-level alignment (Section 2.3).\n\nPersonalized Large Language Models (PLLMs)\n\nPersonalized Prompting (Input level)\n\nProfile-Augmented ( §3.1) Cue-CoT  [Wang et al., 2023b] , PAG  [Richardson et al., 2023] , ONCE  [Liu et al., 2024a] , Matryoshka  [Li et al., 2024b] , DPL  [Qiu et al., 2025a]  , RewriterSlRl  [Li et al., 2024c] , R2P  [Luo et al., 2025]  Retrieval-Augmented ( §3.2)",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Memory",
      "text": "MemPrompt  [Madaan et al., 2022] ,  [Zhang et al., 2023] , MaLP  [Zhang et al., 2024c] , TeachMe  [Dalvi et al., 2022] , MemoRAG  [Qian et al., 2024] , FERMI  [Kim and Yang, 2024]  Retriever IPA,FiD  [Salemi et al., 2023] , MSP  [Zhong et al., 2022] , AuthorPred  [Li et al., 2023] , PEARL  [Mysore et al., 2023] , ROPG  [Salemi et al., 2024] , HYDRA  [Zhuang et al., 2024] , RECAP  [Liu et al., 2023]  , CFRAG  [Shi et al., 2025] , AP-Bots  [Yazan et al., 2025] , RPM  [Kim et al., 2025a] , PersonaAgent  [Zhang et al., 2025a]  Soft-Fused ( §3.3) UEM  [Doddapaneni et al., 2024] , PERSOMA  [Hebert et al., 2024] , REGEN  [Sayana et al., 2024] , PeaPOD  [Ramos et al., 2024] , PPlug  [Liu et al., 2024b] , User-LLM  [Ning et al., 2024] , RECAP  [Liu et al., 2023] , ComMer  [Zeldes et al., 2025]  Contrastive ( §3.4) CoS  [He et al., 2024] , StyleVector  [Zhang et al., 2025b]  Personalized Adaptation (Model level)\n\nOne PEFT All Users ( §4.1)\n\nPEFT-U  [Clarke et al., 2024] , PLoRA  [Zhang et al., 2024d] , LM-P  [Woźniak et al., 2024] , Review-LLM  [Peng et al., 2024a] ,\n\nMiLP  [Zhang et al., 2024e] , RecLoRA  [Zhu et al., 2024a] , iLoRA  [Kong et al., 2024]  One PEFT Per User ( §4.2)\n\nUserAdapter  [Zhong et al., 2021] , PocketLLM  [Peng et al., 2024b] , OPPU  [Tan et al., 2024a] , PER-PCS  [Tan et al., 2024b] ,  [Wagner et al., 2024] ,\n\nFDLoRA  [Qi et al., 2024] , HYDRA  [Zhuang et al., 2024] , PROPER  [Zhang et al., 2025c]  Personalized Alignment (Objective level)\n\nData Construction ( §5.1)  [Wu et al., 2024a] , PLUM  [Magister et al., 2024] ,  [Lee et al., 2024] ,  [Qin et al., 2024] , PRISM  [Kirk et al., 2024] ,  PersonalLLM [Zollo et al., 2024]  Optimization ( §5.2)\n\nMORLHF  [Wu et al., 2023] , MODPO  [Zhou et al., 2023] ,\n\nReward Soups  [Rame et al., 2024] , Personalized Soups  [Jang et al., 2023] , MOD  [Shi et al., 2024] , BiPO  [Cao et al., 2024] , PAD  [Chen et al., 2024a] , CIPHER  [Gao et al.] , Amulet  [Zhang et al., 2025d] , PPT  [Lau et al., 2024] , VPL  [Poddar et al., 2024] , CHAMELEON  [Zhang et al., 2025e] , REST-PG  [Salemi et al., 2025a] , Drift  [Kim et al., 2025b] , RLPA  [Zhao et al., 2025b] , PROSE  [Aroca-Ouellette et al., 2025] , COPE  [Bu et al., 2025]  Others Analysis Role of User Profile  [Wu et al., 2024b , Zhao et al., 2025c] , Safety-Utility  [Vijjini et al., 2024]  , RAG vs. PEFT  [Salemi and Zamani, 2024]  Benchmark ( §6)\n\nLaMP  [Salemi et al., 2023] , LongLamp  [Kumar et al., 2024] , ALOE  [Wu et al., 2024a]  , PGraphRAG  [Au et al., 2025] ,\n\nPerLTQA  [Du et al., 2024] , PEFT-U  [Clarke et al., 2024] , REGEN  [Sayana et al., 2024]  ,  PersonalLLM [Zollo et al., 2024] ,\n\nPrefEval  [Zhao et al., 2025d] , LongMemEval  [Wu et al., 2025]  Figure  3 : A taxonomy of PLLMs with representative examples.\n\n(3) A comprehensive review: We systematically review state-of-the-art methods for PLLMs, analyzing finegrained differences among the methods and their applicability to different personalization scenarios (Section 3, Section 4, Section 5). (4) A benchmark and evaluation summarization: We provide summarization of metrics, benchmarks tailored to different query types (extraction, abstraction, and generalization) and personalization scenarios (Section 6). (  5 ) Future directions: We highlight current limitations and outline promising avenues for future research, including multimodal personalization, edge computing, lifelong updating, trustworthiness, etc (Section 7).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Preliminary 2.1 Large Language Models",
      "text": "Large Language Models (LLMs) generally refer to models that utilize the Transformer architecture and are equipped with billions of parameters trained on trillions of text tokens. These models have demonstrated substantial improvements in a myriad of tasks related to natural language understanding and generation, increasingly proving beneficial in assisting human activities. In this work, we mainly focus on autoregressive LLMs, which are based on two main architectures: decoder-only models and encoder-decoder models. Encoder-decoder models such as Flan-T5  [Chung et al., 2022]  and ChatGLM  [Zeng et al., 2022]  analyze input through the encoder for semantic representations, making them effective in language understanding in addition to generation. Decoder-only LLMs focus on left-to-right generation by predicting the next token in a sequence, with numerous instances  [Brown et al., 2020 , Chowdhery et al., 2022 , Touvron et al., 2023 , Guo et al., 2025]  under this paradigm achieving breakthroughs in advanced capabilities.\n\nHowever, these models are typically pre-trained on general-purpose data and lack an understanding of specific user information. As a result, they are unable to generate responses tailored to a user's unique tastes and expectations, limiting their effectiveness in personalized applications where user-specific adaptation is critical.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Problem Statement",
      "text": "Personalized Large Language Models (PLLMs) generate responses that align with the user's style and expectations, offering diverse answers to the same query for different users  [Clarke et al., 2024] .\n\nProblem Formulation Let U = {u i } N i=1 be a finite set of users. For each u i ∈ U:\n\nwhere n i ∈ N and Q is the query space; • C i ⊂ C is the set of u i 's personalized data, where C denotes the space of all user personalized data.\n\nLet Y be the output space. For each user u i and query q (i) j , there exists a desired output ŷ(i) j ∈ Y that aligns with u i 's preferences and expectations. We define a metric function ζ : Y × Y → R ≥0 that evaluates the quality of the LLM-generated output compared with the desired output (the summarization of metrics is shown in Section 6.1, Table  5 ). Here, we generally assume ζ(ŷ\n\n) quantifies the degree of match between the desired and model outputs, with higher values indicating better personalization performance. All notations are summarized in Table  8 .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "One-Size-Fits-All Flaw",
      "text": "Let M 0 : Q → Y be a base (non-personalized) LLM that maps any query q\n\nIn the absence of personalized data C i , given the same query q, the base model M 0 produces identical outputs for all users. Formally:\n\nThis reflects the non-personalized nature of the base LLM, which depends only on the input query and not on the users' personalized data, leading to large deviations from users' expectations, i.e., large ζ.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Goal Of Pllms: One-Size-Fits-One",
      "text": "A general framework of LLM personalization is to design the personalization operator P : (Q → Y) × C × Θ → (Q → Y), and optimize its parameters θ ⊂ Θ (if any) to effectively inject C i into the base LLM M 0 , producing personalized models M i for user u i that minimize the overall deviation:\n\nwhere, y (i) j represents the personalized output generated by PLLM, P(M 0 , C i ; θ), tailored to the user u i for query q (i) j . This formulation encompasses various approaches to information injection, including prompting, adaptation, and alignment methods.\n\nNote that θ represents the tunable parameters that can exist either within the base LLM architecture itself (when fine-tuning is permitted) or within extra modules incorporated into the LLM framework, like the LoRA module  [Yang et al., 2024] . Based on personalized data C, queries Q, and outputs (responses) Y, personalization can be further categorized into multiple scenarios (Figure  2 ).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Personalized Data",
      "text": "The personalized data C may encapsulate information about the user's preferences, history, context, and other user-specific attributes. These can include profile/relationship, historical dialogues, historical content, and predefined human preferences. A detailed description, including examples, of the classification of input data types for user u i is provided:\n\n• Profile/Relationship User profile, including attributes (e.g., name, gender, occupation), and relationships (e.g., friends, family members), such as C i = {A, 18, student, friends:{B, C, D} . . . }. • Name: Amy",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Historical Dialogues",
      "text": "@aarthycrazy @shaaqT at the end of the best if u want some strong tea, branded ones r the best! @nzdeany i cant imagine lunch without rice. Wonder how its like to have something else for lunch.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Historical Content",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Historical Interactions",
      "text": "Please provide lunch options for me. Figure  4 : Examples of the personalized data and query type. The human brain regions (hippocampus as spatiotemporal memory integrator and experience simulator, angular gyrus as conceptual processing hub, and default mode network (including medial prefrontal cortex) as creative ideation and remote association hub) serve as analogical references to understand these query types, though the correspondence may not be strictly one-to-one. • Historical Dialogues Historical dialogues, such as question-answer pairs that user u i interacts with the LLM (e.g., C i = {(q 0 , a 0 ), (q 1 , a 1 ), . . . , (q ni , a ni )}), where each q is a query and a is the answer. • Historical Content Includes documents, previous reviews, comments or feedback from user u. For example, C i = {I like Avtar because . . . , . . . }.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Maple",
      "text": "• Historical Interactions Includes historical interactions, preferences, ratings from user u i . For example, C i = {The Lord of the Rings : 5, Interstellar : 3 . . . }.\n\n• Pre-defined Human Preference: Define a set S = {d k } K k=1 containing of K preference dimensions such as \"Helpfulness\". Choose various combinations of these dimensions, form individual preferences, and incorporate them as the instruction. For example, a preference prompt could be \"Be harmless and helpful\".\n\nQuery Type Different queries q have different focal points regarding the expected responses ŷ, which have a certain influence on the method design (high-level comparison with examples is shown in Figure  4  and Table  1 ). We primarily classify them into the following categories, with further benchmark-based demonstrations provided in Section 6:\n\n• Extraction (explicit) This type refers to factual lookup, where the answer can be directly found in the user's personalized data C i . The model acts as a retriever:\n\nHere Σ(C i ) denotes the set of explicit factual tuples contained in C i . Importantly, extraction also covers simple statistical or aggregative operations (e.g., counting, aggregation) applied over the facts in Σ(C i ).\n\n• Abstraction (implicit) The model applies a summarization mapping ϕ : C → Z that condenses C i into a query-independent summary z i . This abstract profile captures long-term tendencies or preferences, enabling responses beyond explicit tuples:\n\n• Generalization (implicit) The model constructs a query-specific state h i,j , dynamically combining the user's data with the current query. The response is then generated from a conditional distribution that also leverages external knowledge K:\n\nj , h i,j , K . Unlike z i , which is global and query-independent, h i,j is dynamic and tailored to each query. External knowledge K is necessary when C i alone is insufficient. Downstream Task By incorporating personalized data, PLLMs enhance traditional LLMs, improving response generation, recommendation, and classification tasks.\n\n• Generation Generation tasks typically involve y representing a sequence of strings, such as generating answers for users based on their personalized data C u and questions or generating content according to the user's writing style to assist their writing, and so forth  [Salemi et al., 2023 , Kumar et al., 2024 , Zhao et al., 2025d , Au et al., 2025 ].\n\n• Recommendation The major difference between recommendation and generation is that recommendation requires suggesting specific items based on the user's historical interaction data, and it can provide reasons and explanations for the recommendations  [Sayana et al., 2024 , Liu et al., 2024c] .\n\n• Classification Classification tasks, including sentiment classification, involve labeling a particular entity (such as a movie, item, or description) based on the user's preferences to assist the user in categorization or summarization  [Salemi et al., 2023 , Au et al., 2025 , Zhao et al., 2025d] .\n\nNote that our survey differs significantly from role-play related LLM personalization  [Tseng et al., 2024 , Chen et al., 2024b, Zhang et al., 2024f] . While role-play focuses on mimicking characters during conversations  [Zhu et al., 2024b , Zhao et al., 2024] , PLLMs in this survey focus on understanding users' contexts and preferences to meet their specific needs. Compared to  [Zhang et al., 2024f] , which emphasizes broad categories, our work provides a quite different systematic analysis of techniques to enhance PLLM efficiency and performance, with a detailed technical classification.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Proposed Taxonomy",
      "text": "We propose a taxonomy (as illustrated in Figure  2  and Figure  3 ) from technical perspectives (i.e., regarding the personalization operation P), categorizing the methods for Personalized Large Language Models (PLLMs) into three major levels:\n\n(1) Input level: Personalized Prompting focuses on handling user-specific data outside the LLM and injecting it into the model (Section 3).\n\n(2) Model level: Personalized Adaptation emphasizes designing a framework to efficiently fine-tune or adapt model parameters for personalization (Section 4).\n\n(3) Objective Level: Personalized Alignment aims to refine model behavior to align with user preferences effectively (Section 5).",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Personalized Prompting",
      "text": "Prompt engineering acts as a bridge for interaction between users and LLMs. In this survey, prompting involves guiding an LLM to generate desired outputs using various techniques, from traditional text prompts to advanced methods like soft embedding. Soft embedding can be extended not only through input but also via cross-attention or by adjusting output logits, enabling more flexible and context-sensitive responses. For each user u i , the personalization operation P is expressed as\n\nwhere, ϕ is a function that extracts relevant context from the user's personal context C i ; ⊕ represents the combination operator that fuses the query q (i) j and the relevant personalized context ϕ (C i ), producing enriched information for the LLM. Based on different designs of the ϕ operation, we further categorize prompting-based methods into four types, as shown in Figure  5  and Table  2:  (1) profile-augmented prompting; (2) retrieval-augmented prompting; (3) soft-fused prompting; and (4) contrastive prompting.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Profile-Augmented Prompting",
      "text": "Profile-augmented prompting (Figure  5 (a) ) explicitly utilizes summarized user preferences and profiles in natural language to augment LLMs' input at the token level (ϕ is the summarizer model). The summarizer model is typically distinct from the base LLM (generator).\n\nNon-tuned Summarizer A frozen LLM can be directly used as the summarizer to summarize user profiles due to its strong language understanding capabilities, i.e., ϕ (C u ) = M (C u ). For instance, Cue-CoT  [Wang et al., 2023b]  employs chain-of-thought prompting for personalized profile augmentation, using LLMs to extract and summarize user status (e.g., emotion, personality, and psychology) from historical dialogues. PAG  [Richardson et al., 2023]  leverages instruction-tuned LLMs to pre-summarize user profiles based on historical content. The summaries are stored offline, enabling efficient personalized response generation while meeting runtime constraints. ONCE  [Liu et al., 2024a]  prompts closed-source LLMs to summarize topics and regions of interest from users' browsing history, enhancing personalized recommendations. DPL  [Qiu et al., 2025a]  enhances personalization through inter-user comparison, identifying similar users in the same cluster. Leveraging central users from the same cluster where the target user is located creates more accurate preference summaries. Table  2 : Summarization of the operation difference among different prompting methods in Equation (1). S denotes the operation that extracts inner information from LLM. Black-box refers to whether the method can be applied to black-box models, where the base model parameters are not accessible.\n\nProfile-Augmented (Section 3.1) Tuned Summarizer Black-box LLMs are sensitive to input noise, like off-topic summaries, and struggle to extract relevant information. Thus, training the summarizer to adapt to user preferences and style is essential. Matryoshka  [Li et al., 2024b ] uses a white-box LLM to summarize user histories, similar to PAG, but fine-tunes the summarizer instead of the generator LLM. RewriterSlRl  [Li et al., 2024c]  rewrites the query q instead of concatenating summaries, optimized with supervised and reinforcement learning.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Retrieval-Augmented Prompting",
      "text": "Retrieval-augmented prompting  [Gao et al., 2023 , Fan et al., 2024 , Qiu et al., 2024]  excels at extracting the most relevant records from user data to enhance PLLMs (See Figure  5 (b) ). Due to the complexity and volume of user data, many methods use an additional memory for more effective retrieval  [Tan et al., 2025] . Common retrievers include sparse (e.g.,  BM25 [Robertson et al., 1995] ), and dense retrievers (e.g., Faiss  [Johnson et al., 2019] , Contriever  [Izacard et al., 2021] ). These methods effectively manage the increasing volume of user data within the LLM's context limit, improving relevance and personalization by integrating key evidence from the user's personalized data.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Personalized Memory Construction",
      "text": "This part designs mechanisms for retaining and updating memory to enable efficient retrieval of relevant information.\n\nNon-Parametric Memory This category maintains a token-based database, storing and retrieving information in its original tokenized form without using parameterized vector representations. For example, MemPrompt  [Madaan et al., 2022]  and TeachMe  [Dalvi et al., 2022 ] maintain a dictionary-based feedback memory (key-value pairs of mistakes and user feedback). MemPrompt focuses on prompt-based improvements, whereas TeachMe emphasizes continual learning via dynamic memory that adapts over time. MaLP  [Zhang et al., 2024c]  further integrates multiple memory types, leveraging working memory for immediate processing, short-term memory (STM) for quick access, and long-term memory (LTM) for key knowledge. FERMI  [Kim and Yang, 2024]  maintains misaligned responses and creates personalized prompts for users by using LLMs to progressively refine prompting strategies based on user profiles and past opinions.\n\nParametric Memory Recent studies parameterize and project personalized user data into a learnable space, with parametric memory filtering out redundant context to reduce noise. For instance, LD-Agent  [Li et al., 2024d]  maintains memory with separate short-term and long-term banks, encoding long-term events as parametric vector representations refined by a tunable module and retrieved via an embedding-based mechanism. MemoRAG  [Qian et al., 2024] , in contrast, adopts a different approach by utilizing a lightweight LLM as memory to learn user-personalized data. Instead of maintaining a vector database for retrieval, it generates a series of tokens as a draft to further guide the retriever, offering a more dynamic and flexible method for retrieval augmentation. Inspired by cognitive theories of memory, PRIME  [Zhang et al., 2025f]  introduces episodic and semantic memory mechanisms to enhance the personalization capabilities of LLMs, and employs a slow-thinking strategy to further improve personalized reasoning.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Personalized Memory Retrieval",
      "text": "The key challenge in the personalized retriever design lies in selecting not only relevant but also representative personalized data for downstream tasks. LaMP  [Salemi et al., 2023]  investigates how retrieved personalized information affects the responses of large language models (LLMs) through two mechanisms: in-prompt augmentation (IPA) and fusion-in-decoder (FiD). PEARL  [Mysore et al., 2023]  and ROPG  [Salemi et al., 2024]  similarly aim to enhance the retriever using personalized generation-calibrated metrics, improving both the personalization and text quality of retrieved documents. Meanwhile, HYDRA  [Zhuang et al., 2024]  trains a reranker to prioritize the most relevant information additionally from top-retrieved historical records for enhanced personalization. RPM  [Kim et al., 2025a]  focuses on reasoning ability by extracting user-specific factors from user historical personalized data and creating annotated reasoning paths. At inference, it retrieves these examples to guide reasoning-aligned outputs, strengthening personalized responses. HYDRA and RPM primarily target black-box models where access to the base LLM parameters is restricted.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Soft-Fused Prompting",
      "text": "Soft prompting differs from profile-augmented prompting in that it compresses personalized data into soft embeddings rather than summarizing it into discrete tokens. These embeddings are generated by a user feature encoder ϕ.\n\nIn this survey, we generalize the concept of soft prompting, showing that soft embeddings can be integrated (combination operator ⊕) not only through the input but also via cross-attention or by adjusting output logits, allowing for more flexible and context-sensitive responses (See Figure  5  (c)).\n\nInput Prefix Soft prompting, used as an input prefix, focuses on the embedding level by concatenating the query embedding with the soft embedding, and is commonly applied in recommendation tasks. PPlug  [Liu et al., 2024b]  constructs a user-specific embedding for each individual by modeling their historical contexts using a lightweight plug-in user embedder module. This embedding is then attached to the task input. UEM  [Doddapaneni et al., 2024]  is a user embedding module (transformer network) that generates a soft prompt conditioned on the user's personalized data. PERSOMA  [Hebert et al., 2024]  enhances UEM by employing resampling, selectively choosing a subset of user interactions based on relevance and importance. REGEN  [Sayana et al., 2024]  combines item embeddings from useritem interactions via collaborative filtering and item descriptions using a soft prompt adapter to generate contextually personalized responses. PeaPOD  [Ramos et al., 2024]  personalizes soft prompts by distilling user preferences into a limited set of learnable, dynamically weighted prompts. Unlike previously mentioned methods, which focus on directly embedding user interactions or resampling relevant data, PeaPOD adapts to user interests by weighting a shared set of prompts. ComMer  [Zeldes et al., 2025]  tunes an encoder to compress historical documents into embeddings, merges them through mean pooling, then feeds the result into a frozen base LLM, distinguishing it from methods focused on historical interaction data. DEP  [Qiu et al., 2025b]  further models inter-user differences in the latent space instead of merely considering the user's own information.\n\nCross-Attention Cross-attention enables the model to process and integrate multiple input sources by allowing it to attend to personalized data and the query. User-LLM  [Ning et al., 2024]  uses an autoregressive user encoder to convert historical interactions into embeddings through self-supervised learning, which are then integrated via cross-attention.\n\nThe system employs joint training to optimize both the retriever and generator for better performance. RECAP  [Liu et al., 2023]  utilizes a hierarchical transformer retriever designed for dialogue domains to fetch personalized information. This information is integrated into response generation via a context-aware prefix encoder, improving the model's ability to generate personalized, contextually relevant responses.\n\nOutput Logits GSMN  [Wu et al., 2021]  retrieves relevant information from personalized data, encodes it into soft embeddings, and uses them in attention with the query vector. Afterward, the resulting embeddings are concatenated with the LLM-generated embeddings, modifying the final logits to produce more personalized and contextually relevant responses.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Contrastive Prompting",
      "text": "The key insight of contrastive prompting is to utilize two forward paths of LLM to generate contrast pairs with and without personalized information (See Figure  5 (d) ). By comparing model states S with and without personalization, it identifies factors influencing personalization, enabling dynamic adjustment of the model's personalization level. Two mainstream model states are utilized: hidden states (representations) and logits (predictive distributions).\n\nCoS  [He et al., 2024 ] is a special case that assumes there is a brief user profile C for each query; it amplifies its influence in LLM response generation by combining output probabilities (logits) with and without the profile, i.e., S (M 0 (q)) + α × S (M 0 (q, C)), adjusting personalization degree through hyperparameter α without fine-tuning. In contrast, StyleVector  [Zhang et al., 2025b]  obtains encoded personalized information in LLM via hidden representations. Specifically, given a user u i and the personalized data (user historical content), c\n\n∈ C i , it use GPT-3.5-Turbo M to generate a general response for comparison with the expected personalized response r (i) j , i.e., ϕ :\n\n, where S extracts hidden representations in base model M 0 's middle layer. ϕ extracts the user style-based vector, and by adding this vector with a scaling factor α, this method can steer the LLM toward personalized output to a controllable degree. CoSteer  [Lv et al., 2025]  performs decoding-time personalization through local delta steering. A local SLM produces logits with and without personal context, and their difference ∆ = log π * pers -log π * base is used to steer the cloud LLM's logits iteratively, preserving privacy while achieving controllable personalization without fine-tuning.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Discussions",
      "text": "The three prompting methods have distinct pros and cons (Table  2 ):\n\nPersonalized Prompting Method: Pros and Cons\n\n• Profile-augmented prompting improves efficiency by compressing historical data but risks information loss and reduced personalization. • Retrieval-augmented prompting offers rich, context-aware inputs and scales well for long-term memory but can suffer from computational limits and irrelevant data retrieval. • Soft prompting efficiently embeds user-specific info, capturing semantic nuances without redundancy, but is limited to black-box models and lacks explicit user preference analysis. • Contrastive prompting provides interpretability and controllable personalization by comparing model states with and without personalized information, but suffers from hyperparameter sensitivity when choosing the hyperparameter, i.e., scaling factor α.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Pros. Parameter Efficiency Scalability",
      "text": "Strong Personalization User Isolation",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Cons. Limited Personalization Persnonalized Data Dependency",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Storage Overhead Training Complexity",
      "text": "This approach centers on maintaining, selecting, and integrating external personalized data into LLMs to improve user behavior comprehension. Overall, prompting-based methods are efficient and adaptable, enabling dynamic personalization with minimal computational overhead. Especially for RAG, which reduces LLM hallucination by grounding responses in retrieved evidence. Therefore, prompting methods are particularly suitable for explicit queries that require answering specific factual information, as mentioned in Section 2.2, excelling when personalization involves retrieving concrete information from user data. However, they lack deeper personalization analysis, as they rely on predefined prompt structures to inject user-specific information and are limited in accessing global knowledge due to the narrow scope of prompts, which usually fail in tasks with implicit generation queries compared with adaptation and alignment methods  [Tan et al., 2024a] .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Personalized Adaptation",
      "text": "PLLMs require balancing fine-tuning's deep adaptability with the efficiency of prompting. Therefore, specialized methods need to be specifically designed for PLLMs to address these challenges utilizing parameter-efficient fine-tuning methods (PEFT), such as LoRA  [Hu et al., 2021 , Yang et al., 2024] , prefix-tuning  [Li and Liang, 2021] , MeZo  [Malladi et al., 2023] , etc. From an architectural perspective, these methods can be categorized into two types: (1) one PEFT for all users, where all users share a single fine-tuning module that differentiates personalized information, and (2) one PEFT per user, where each user has their own unique fine-tuning block. Details are illustrated in Figure  6  and Table  3 .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "One Peft All Users",
      "text": "This method trains on all users' data using a shared PEFT module, eliminating the need for separate modules per user. The shared module's architecture can be further categorized.\n\nSingle PEFT PLoRA  [Zhang et al., 2024d]  and LM-P  [Woźniak et al., 2024]  utilize LoRA for PEFT of LLM, injecting personalized information via user embeddings and user IDs, respectively. PLoRA is further extended and supports online training and prediction for cold-start scenarios. UserIdentifier  [Mireshghallah et al., 2021]  uses a static, non-trainable user identifier to condition the model on user-specific information, avoiding the need for trainable user-specific parameters and reducing training costs. Review-LLM  [Peng et al., 2024a]  aggregates users' historical behaviors and ratings into prompts to guide sentiment and leverages LoRA for efficient fine-tuning. However, these methods rely on a single architecture with fixed configurations (e.g., hidden size, insertion layers), making them unable to store and activate diverse information for personalization  [Zhou et al., 2024] . To solve this problem, MiLP  [Zhang et al., 2024e]  utilizes a Bayesian optimization strategy to automatically identify the optimal configuration for applying multiple LoRA modules, enabling efficient and flexible personalization.\n\nMixture of Experts (MoE) Several methods use the LoRA module, but with a static configuration for all users. This lack of parameter personalization limits adaptability to user dynamics and preference shifts, potentially resulting in suboptimal performance  [Cai et al., 2024] . RecLoRA  [Zhu et al., 2024a]  addresses this limitation by maintaining a set of parallel, independent LoRA weights and employing a soft routing method to aggregate meta-LoRA weights, enabling more personalized and adaptive results. Similarly, iLoRA  [Kong et al., 2024]  creates a diverse set of experts (LoRA) to capture specific aspects of user preferences and generates dynamic expert participation weights to adapt to user-specific behaviors.\n\nShared PEFT methods rely on a centralized approach, where user-specific data is encoded into a shared adapter by centralized LLMs. This limits the model's ability to provide deeply personalized experiences tailored to individual users. Furthermore, using a centralized model often requires users to share personal data with service providers, raising concerns about the storage, usage, and protection of this data.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "One Peft Per User",
      "text": "Equipping a user-specific PEFT module makes LLM deployment more personalized while preserving data privacy. However, the challenge lies in ensuring efficient operation in resource-limited environments, as users may lack sufficient local resources to perform fine tuning.\n\nNo Collaboration There is no collaboration or coordination between adapters or during the learning process for each use in this category. UserAdapter  [Zhong et al., 2021]  personalizes models through prefix-tuning, fine-tuning a unique prefix vector for each user while keeping the underlying transformer model shared and frozen. PocketLLM  [Peng et al., 2024b]  utilizes a derivative-free optimization approach, based on MeZo  [Malladi et al., 2023] , to fine-tune LLMs on memory-constrained mobile devices.\n\nCollaborative Efforts The \"one-PEFT-per-user\" paradigm without collaboration is computationally and storageintensive, particularly for large user bases. Additionally, individually owned PEFTs hinder community value, as personal models cannot easily share knowledge or benefit from collaborative improvements. OPPU  [Tan et al., 2024a]  frames the personalized LLMs training as a two-stage process, where a global LoRA module is learned to capture shared knowledge across users initially, followed by user-specific LoRA modules for individual adaptation. PerFit  [Liu et al., 2025]  further enhances this by replacing LoRA modules with representation fine-tuning modules, motivated by their finding that the personalization of LLM correlates with a low-rank collaborative shift and diverse personalized shifts following the collaborative one. CoPe  [Bu et al., 2025]  utilizes reward-guided decoding tailored for personalization, aiming to maximize the implicit reward signal for each user. PER-PCS  [Tan et al., 2024b]  enables efficient and collaborative PLLMs by sharing a small fraction of PEFT parameters across users. It first divides PEFT parameters into reusable pieces with routing gates and stores them in a shared pool. For each target user, pieces are autoregressively selected from other users, ensuring scalability, efficiency, and personalized adaptation without additional training. PROPER  [Zhang et al., 2025c]  introduces hierarchical user grouping with MoE-LoRA integration, enabling fine-grained personalization through staged adaptation from population-level, group-level, to individual-level models.\n\nAnother efficient collaborative strategy is based on the federated learning (FL) framework. For example,  Wagner et al. [2024]  introduces a FL framework for on-device LLM fine-tuning, using strategies to aggregate LoRA model parameters and handle data heterogeneity efficiently, outperforming purely local fine-tuning. FDLoRA  [Qi et al., 2024]  introduces a personalized FL framework using dual LoRA modules to capture personalized and global knowledge. It shares only global LoRA parameters with a central server and combines them via adaptive fusion, enhancing performance while minimizing communication and computing costs.\n\nThere are other frameworks that can be explored, such as HYDRA  [Zhuang et al., 2024] , which also employs a base model to learn shared knowledge. However, in contrast to federated learning, it assigns distinct heads to each individual user to extract personalized information.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Discussions",
      "text": "Fine-tuning methods enable deep personalization by modifying a large set of model parameters, and parameter-efficient fine-tuning methods (e.g., prefix vectors or adapters) reduce computational cost and memory requirements while maintaining high personalization levels. These methods improve task adaptation by tailoring models to specific user needs, enhancing performance in tasks like sentiment analysis and recommendations. They also offer flexibility, allowing user-specific adjustments while leveraging pre-trained knowledge. However, they still face several challenges:",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Personalized Adaptation Methods: Challenges",
      "text": "Overfitting With limited or noisy user data, models may fail to generalize and lose robustness.\n\nPerformance-Privacy-Efficiency Trade-off A per-user PEFT strategy ensures strong privacy but suffers from limited performance. Introducing collaboration improves performance yet risks privacy leakage. Sharing only parameters instead of raw user data alleviates privacy concerns, but introduces efficiency challenges in edge-cloud coordination, underscoring the difficulty of jointly optimizing all three dimensions.\n\nCold-start Adapting to new users with sparse or unseen data remains difficult.\n\nAddressing these issues calls for complementary strategies such as federated learning, synthetic data generation, and hybrid prompting-adaptation pipelines, in order to balance efficiency, personalization depth, and reliability.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Personalized Alignment",
      "text": "Alignment techniques  [Bai et al., 2022 , Rafailov et al., 2024]  typically optimize LLMs to match the generic preferences of humans. However, in reality, individuals may exhibit significant variations in their preferences for LLM responses across different dimensions like language style, knowledge depth, and values. Personalized alignment seeks to further align with individual users' unique preferences beyond generic preferences. A significant challenge in personalized alignment is creating high-quality user-specific preference datasets, which are more complex than general alignment datasets due to data sparsity. The second challenge arises from the need to refine the canonical RLHF framework  [Ouyang et al., 2022]  to handle the diversification of user preferences, which is essential for integrating personalized preferences without compromising efficiency and performance.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Data Construction",
      "text": "High-quality data construction is critical for learning PLLMs, primarily involving self-generated data through interactions with the LLM. Wu et al.  [Wu et al., 2024a ] constructs a dataset for aligning LLMs with individual preferences by initially creating a diverse pool of 3,310 user personas, which are expanded through iterative self-generation and filtering. This method is similar to PLUM [Magister et al., 2024] that both simulate dynamic interactions through multi-turn conversation trees, allowing LLMs to infer and adapt to user preferences. To enable LLMs to adapt to individual user preferences without re-training, Lee et al.  [Lee et al., 2024]  utilizes diverse system messages as meta-instructions to guide the models' behavior. To support this, the MULTIFACETED COLLECTION dataset is created, comprising 197k system messages that represent a wide range of user values. To facilitate real-time, privacy-preserving personalization on edge devices while addressing data privacy, limited storage, and minimal user disruption, Qin et al.  [Qin et al., 2024]  introduces a self-supervised method that efficiently selects and synthesizes essential user data, improving model adaptation with minimal user interaction.\n\nResearch efforts are also increasingly concentrating on developing datasets that assess models' comprehension of personalized preferences.  Kirk et al. [Kirk et al., 2024]  introduces PRISM Alignment Dataset that maps the sociodemographics and preferences of 1,500 participants from 75 countries to their feedback in live interactions with 21 LLMs, focusing on subjective and multicultural perspectives on controversial topics.  PersonalLLM [Zollo et al., 2024]  introduces a novel personalized testdb, which curates open-ended prompts and multiple high-quality responses to simulate diverse latent preferences among users. It generates simulated user bases with varied preferences from pre-trained reward models, addressing the challenge of data sparsity in personalization. ALOE  [Wu et al., 2024a]  further generates tree-structured multi-turn conversations instead of single-turn pairwise responses.\n\nPros.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Strong Personalization Efficient Inference",
      "text": "High Flexibility No Retraining",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Cons. High Training Cost Lack Flexibility",
      "text": "Storage Overhead High Inference Cost",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Personalized Alignment Optimization",
      "text": "Personalized preference alignment is usually modeled as a multi-objective reinforcement learning (MORL) problem, where personalized preference is determined as the user-specific combination of multi-preference dimensions. Based on this, a typical alignment paradigm is training-time personalization. This involves using a personalized reward derived from multiple reward models to guide the training of policy LLMs, which is illustrated on the left of Figure  7 .\n\nMORLHF  [Wu et al., 2023]  separately trains reward models for each dimension and retrains the policy language models using proximal policy optimization, guided by a linear combination of these multiple reward models. This approach allows for the reuse of the standard RLHF pipeline. MODPO  [Zhou et al., 2023]  introduces a novel RL-free algorithm extending Direct Preference Optimization (DPO) for managing multiple alignment objectives. It integrates linear scalarization into the reward modeling process, enabling the training of LMs using a margin-based cross-entropy loss as implicit collective reward functions.\n\nThe alternative paradigm is decoding-time personalization, which combines multiple trained policy LLMs during inference. Table  4  provides a detailed comparison of this approach against training-time personalization. Its main strategies, personalized weight merging and personalized model ensemble, are illustrated in Figure  7 (b) and (c), respectively. Personalized Soups  [Jang et al., 2023]  and Reward Soups  [Rame et al., 2024]  address the challenge of RL from personalized human feedback by first training multiple policy models with distinct preferences independently and then merging their parameters post-hoc during inference. Both methods allow for dynamic weighting of the networks based on user preferences, enhancing model alignment and reducing reward misspecification. Also, the personalized fusion of policy LLMs can be achieved not only through parameter merging but also through model ensembling.\n\nMOD  [Shi et al., 2024]  outputs the next token from a linear combination of all base models, allowing for precise control over different objectives by combining their predictions without the need for retraining. The method demonstrates significant effectiveness when compared to the parameter-merging baseline. PAD  [Chen et al., 2024a ] leverages a personalized reward modeling strategy to generate token-level rewards that guide the decoding process, enabling the dynamic adaptation of the base model's predictions to individual preferences.\n\nHowever, these approaches are limited by their reliance on predefined personalization dimensions and a small set of user preferences, which fail to capture the diversity of real-world personalization needs. Recent work has therefore sought to extend reward learning to better accommodate user-specific preferences. For example, VPL  [Poddar et al., 2024]  employs a variational encoder to encode a small number of preference annotations from a given user into a latent variable that captures individual taste. A reward model is then conditioned on the latent variable and trained by maximizing the evidence lower bound, enabling accurate prediction of rewards aligned with the user's unique preferences. PREF  [Shenfeld et al., 2025]     Kim et al., 2025b]  proposes a training-free method where user preference is modeled as a linear combination of interpretable attributes, and then computes a reward signal by comparing log-likelihoods under attribute-specific prompts to guide the generation of a frozen base model. In contrast to these offline methods based on static preferences, other approaches focus on real-time adaptation. AMULET  [Zhang et al., 2025g]  approaches this from another unique test-time perspective, by formulating the decoding process of each token as an independent online learning problem. It obtains the optimization direction by contrasting the model's output with and without user-provided prompts and utilizes an efficient closed-form solution for real-time iterative optimization, thus adapting to personalized user needs instantly without retraining the model. RLPA  [Zhao et al., 2025b]  simulates real users via a dynamically updated user portrait model, enabling the language model agent to engage in multi-turn interactive reinforcement learning within this simulated environment, thus continuously adapting its policy to evolving user preferences. PersonaAgent  [Zhang et al., 2025a]  extends personalization from text style to decision behaviors such as tool usage, guided by a dynamic Persona that directs the agent's decisions. Using test-time user-preference alignment, it compares the agent's responses with the user's real actions in real time, using textual loss as feedback to optimize the Persona, which ensures personalization run through every step of the tool decision.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Discussions",
      "text": "Current mainstream personalized alignment technologies mainly model personalization as multi-objective reinforcement learning problems, where personalized user preferences are taken into account during the training phase of policy LLMs via canonical RLHF, or the decoding phase of policy LLM via parameter merging or model ensembling. Typically, these methods are limited to a small number (e.g., three) of predefined preference dimensions, represented through textual user preference prompts. However, in real-world scenarios, there could be a large number of personalized users, and their preference vectors may not be known, with only their interaction history accessed. Consequently, developing more realistic alignment benchmarks to effectively assess these techniques is a critical area for future research.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Metric And Benchmark",
      "text": "As mentioned in Section 2.2, LLM personalization can be categorized into multiple types based on different inputs, query types, and tasks. Currently, numerous benchmarks are available to help us validate personalization approaches. This section primarily organizes these benchmarks according to the aforementioned aspects. We first list the commonly used metrics for various personalization tasks (Section 6.1), then review the different benchmarks and the metrics they employ (Section 6.2).",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Metric",
      "text": "We employ a comprehensive set of mainstream evaluation metrics to assess model performance across different tasks.\n\nThe detailed specifications of these metrics are presented in Table  5 . We summarize Accuracy (Acc), F1 Score (F1), Matthews Correlation Coefficient (MCC)  [Chicco and Jurman, 2020] , Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for the classification task; ROUGE-1 (R-1)  [Lin, 2004] , BLEU  [Papineni et al., 2002] , ROUGE-L (R-L)  [Lin, 2004] , METEOR (MTR)  [Banerjee and Lavie, 2005] , SBERT  [Reimers and Gurevych, 2019] , LLM-as-Evaluator (LLM-E)  [Gu et al., 2024]  metrics to evaluate the quality of generated text for the generation task; and Hit Ratio (HR), Precision, Recall, and NDCG for the recommendation task.\n\nPrecision and recall MCC  [Chicco and Jurman, 2020]  C (M)\n\nClass imbalance MAE  [Chai and Draxler, 2014]  C (O)\n\nLarge deviation penalty Perplexity  [Serban et al., 2016\n\nRouge-1 (R-1)  [Lin, 2004 ] G",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "|S∩R| |R|",
      "text": "N-gram overlap BLEU  [Papineni et al., 2002]  G\n\nRouge-L (R-L)  [Lin, 2004 ] G",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Lcs(S,R) |R|",
      "text": "Longest common subsequence METEOR (MTR)  [Banerjee and Lavie, 2005 ]\n\nLinguistic features SBERT  [Reimers and Gurevych, 2019, Zhang et al., 2020 ]\n\nVector-based signal LLM-as-Evaluator (LLM-E)  [Gu et al., 2024 ] G LLM(S, R) Automated evaluation EGISES  [Vansh et al., 2023\n\nPersonalization insensitivity P-Accuracy  [Vansh et al., 2023]  G\n\nPersonalization-aware accuracy PerSEval  [Dasgupta et al., 2024]  G DEGRESS(S, R) × EDP(S, R) Personalization evaluation Hit Ratio (HR) R 1\n\nPrecision  [Gunawardana and Shani, 2009]  R\n\nRecall  [Gunawardana and Shani, 2009]  R\n\nCoverage of user's relevant items NDCG  [Wang et al., 2013]  R\n\nPosition-aware relevance metric",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Classification Task",
      "text": "For the classification task, T P , T N , F P , and F N represent true positives, true negatives, false positives, and false negatives, respectively; y i and ŷi denote the ground truth and predicted values.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Multi-Class & Binary Classification (M)",
      "text": "In multi-class and binary classification tasks, where labels are categorical without inherent order, standard evaluation metrics such as Accuracy and F1 score are commonly employed to assess the model's ability to correctly predict class membership. MCC provides a more robust assessment under class imbalance.\n\nOrdinal Classification (O) For ordinal multi-class classification, where labels possess a natural order or ranking, performance metrics like MAE and RMSE are preferred, as they account for the magnitude of prediction errors relative to the true order, providing a more nuanced evaluation of model quality. RMSE reflects the square-root of the mean of squared errors, providing a magnitude-sensitive measure that penalizes larger deviations more heavily. MAE complements this by computing the average absolute error, offering robustness to outliers.",
      "page_start": 17,
      "page_end": 18
    },
    {
      "section_name": "Generation Task",
      "text": "For the generation task, S and R represent the generated and reference sequences, and LCS(S, R) indicates the length of the longest common subsequence between S and R. |S| and |R| denote the number of unique words in the candidate and reference texts, respectively. In the Perplexity formula, w i is the i-th token in the reference text, w <i represents all tokens that come before position i, and p(w i |w <i ) is the probability that the model assigns to token w i given the preceding tokens. In the BLEU formula, the brevity penalty is based on c and s, which are the total word counts (including repeated words) in the candidate and reference sentences, respectively. In the METEOR formula, m represents the number of matched words between the candidate text and reference text (including exact matches, stem matches, and synonym matches). ch represents the number of \"chunks\" in the matching sequence, where a chunk is a contiguous sequence of matched words. A higher number of chunks indicates less fluent or more fragmented matching between texts.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Human-Based Evaluation (Human-E)",
      "text": "Human-E is a process where human judges assess LLM outputs by rating them on accuracy, helpfulness, coherence, appropriateness, etc. In this approach, human evaluators review and rate responses based on criteria such as accuracy, helpfulness, coherence, and appropriateness. Human-E is considered the gold standard because humans can detect nuances in language, recognize factual errors, understand contextual appropriateness, and make holistic judgments that automated metrics often miss.\n\nConventional Evaluation BLEU, ROUGE-1, ROUGE-L, and METEOR to measure lexical overlap between the generated y and ground-truth ŷ responses. BLEU evaluates n-gram precision focusing on system output, whereas ROUGE-1 measures unigram recall relative to reference texts. ROUGE-L extends this by capturing the longest common subsequence structure. METEOR incorporates linguistic features, including stemming and synonym matching, for enhanced evaluation. For semantic-level assessment beyond surface-level patterns, SBERT-based cosine similarity  [Li et al., 2025a]  provides a vector-based measure of meaning preservation between generated and reference responses.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Llm-Based Evaluation (Llm-E)",
      "text": "Human-based evaluation provides the most reliable assessment of LLM response quality, but obtaining statistically significant results is costly and time-intensive. Therefore, the \"LLM-as-a-judge\" framework  [Gu et al., 2024]  is proposed that uses LLM to automatically evaluate the quality and relevance of generated text. By prompting LLMs to score or compare outputs, it offers scalable, context-aware, and semantically rich assessments with minimal human input. This approach surpasses traditional metrics in flexibility but faces challenges like model bias and consistency. It represents a promising method for automated, nuanced evaluation. ExPerT  [Salemi et al., 2025b]  introduces an explainable reference-based evaluation framework specifically designed for style-based personalized text generation. It leverages an LLM to extract atomic aspects and their evidence from the generated and reference texts, match the aspects, and evaluate their alignment based on content and writing style -two key attributes in personalized text generation.\n\nPersonalization Degree Evaluation (Per-E) The aforementioned metrics are unable to assess the personalization capability of summarization models. To address this limitation, EGISES  [Vansh et al., 2023]  is proposed as the first automatic measure for evaluating personalization in text summarization. The core Deviation function measures summary-level personalization by computing the ratio\n\naveraged across all user pairs, where X u,u ′ and Y u,u ′ represent the Jensen-Shannon divergence-weighted differences between users' expected summaries and modelgenerated summaries, respectively. The Deviation function measures personalization quality: Dev(S c,ui , R c,ui ) → 1 for good personalization, Dev(S c,ui , R c,ui ) → 0 for poor personalization, where the model fails to match user expectation differences. Building upon EGISES, P-Accuracy  [Vansh et al., 2023]  provides a personalization-aware accuracy measure that penalizes traditional accuracy scores based on the model's personalization capability, Acc(\n\n], where σ is the sigmoid function, α and β control the penalty intensity and personalization emphasis, respectively. However, EGISES suffers from the personalization-accuracy paradox, where models can achieve high responsiveness but low accuracy, leading to poor user experience. PerSEval  [Dasgupta et al., 2024]  addresses this limitation by introducing the EDP that penalizes EGISES scores when accuracy drops, computed as PerSEval = DEGRESS × EDP, where DEGRESS measures responsiveness, and EDP incorporates accuracy-based penalties to ensure that personalization evaluation considers both user preference alignment and content quality.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Recommendation Task",
      "text": "For the recommendation task, rel i denotes the relevance score of the item at rank position i, which may be binary (0 or 1) or graded (multi-level) depending on the evaluation context. In the NDCG formula, |R K | is the number of relevant items (limited to K items), rel * i represents the relevance values sorted in descending order (the ideal ordering).\n\nItem Recommendation Traditional recommendation tasks typically use Hit Ratio (HR), Recall, and Discounted Cumulative Gain (NDCG)  [Ning et al., 2024 , Ramos et al., 2024]  as standard evaluation metrics to measure the effectiveness of top-K recommendation and preference ranking.\n\nConversational Recommendation Conversational recommendation systems commonly use Recall and NDCG as evaluation metrics to measure coverage and ranking quality, employing the \"LLMs-as-a-judger\" framework  [Zhao et al., 2025c , Huang et al., 2024 , Sayana et al., 2024] . Additionally, an LLM-based user simulator-creating unique personas via zero-shot ChatGPT prompting and defining preferences using dataset attributes-is also used to assess whether outputs align with user preferences  [Huang et al., 2024] . Note that in this scenario, we still categorize it as a generation task (G), as the core objective remains text generation in context, while the input data contains interaction information.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Benchmark",
      "text": "Benchmarks for LLM personalization can be categorized based on the types of input data they utilize, the nature of the queries they address, and the specific tasks they are designed to evaluate. In terms of the personalized data format, benchmarks can be classified into four main types: Historical Content, Dialogues, Interactions, and Pre-defined Human Preference. Sometimes the user profiles are also included as auxiliary information. The mainstream benchmarks for each data type are summarized in Table  6  23 .\n\nDialogue-based Data For dialogue-based data, the benchmarks typically focus on the ability of LLMs to extract and utilize personalized information (e.g., user characteristics, schedules, preferences) from multi-turn conversations. The challenging aspect here is to accurately capture user intent and context from the long-term dialogue history, which may involve complex interactions, implicit preferences, and enormous unrelated content  [Li et al., 2025b , Zhao et al., 2025d , Mok et al., 2025] . Therefore, the mainstream techniques for this type of data are prompting-based methods, which can effectively leverage the most decisive information from the dialogue history without requiring extensive model retraining. The query types in these benchmarks often include explicit extraction of personalized factual details. Naturally, the metrics used are usually accuracy-based, such as F1 score and accuracy, with LLM-E as a preprocessing step to calculate the exact matching scores between the generated and ground-truth responses  [Xue et al., 2025 , Wu et al., 2025 , Tan et al., 2025] .\n\nHistorical Content-based Data For historical content-based data, the benchmarks often evaluate how well LLMs can incorporate and reason over user-specific documents, such as past tweets, articles, or other textual materials. The key challenge is to enable the model to understand and integrate this external knowledge into its responses, which may require sophisticated retrieval and comprehension capabilities  [Salemi et al., 2023 , Kumar et al., 2024] . Benefiting from the purer text content provided in these benchmarks, techniques like SFT are more commonly employed to intrinsically adapt the model to the user's content, while the prompting-based RAG and PAG methods are auxiliarily used to further retrieve and conclude the most relevant terms for fine-tuning  [Au et al., 2025, Salemi and Zamani, 2025] . More query types of abstraction and generalization are included in these benchmarks, which require the model to not only extract factual information but also adapt its style and tone to match user preferences  [Salemi et al., 2023] . Therefore, the metrics used are more diverse, including both accuracy-based metrics and generation quality metrics like ROUGE, BLEU, and METEOR, with LLM-E and Human-E as supplementary evaluations  [Li et al., 2025a] .\n\nPreference-based Data For preference-based data, the benchmarks typically assess how well LLMs can align their outputs with user-defined preferences, which may be explicitly stated or implicitly inferred from user behavior. The main challenge is to accurately model and incorporate these preferences into the generation process, which may involve complex reasoning and decision-making  [Kirk et al., 2024 , Wu et al., 2024a] . Techniques like RLHF and DPO are often employed to fine-tune the model based on user feedback, while prompting-based methods like ICL are also   [Mok et al., 2025 , Zollo et al., 2024]  but also align with user tastes and styles  [Kirk et al., 2024] . Consequently, the metrics used are primarily generation quality metrics, with LLM-E and Human-E as critical components to evaluate the alignment with user preferences.\n\nInteraction-based Data For interaction-based data, the benchmarks often focus on how well LLMs can leverage user interaction histories, such as clicks, likes, and other engagement signals, to personalize recommendations and responses. The key challenge is to effectively model user behavior and preferences from these interactions, which may be sparse and noisy  [Sayana et al., 2024 , Huang et al., 2025] . Techniques like fine-tuning are commonly used to adapt the model to user interaction patterns, while prompting-based methods are also employed to retrieve relevant information based on past interactions  [Cai et al., 2025] . The query types in these benchmarks often involve generalized recommendation tasks, where the model must generate personalized suggestions based on user behavior. Therefore, the metrics used are typically recommendation quality metrics, such as Hit Ratio and NDCG. Sometimes, the generation quality metrics like BLEU and ROUGE are also used when the recommendation is presented in a conversational format.",
      "page_start": 19,
      "page_end": 21
    },
    {
      "section_name": "Discussions",
      "text": "Although existing benchmarks cover a wide range of data types, query types, and tasks, there are still some limitations and gaps that need to be addressed in future research. To generalize to more realistic scenarios, future benchmarks should consider the following aspects:\n\nLLM Personalization Benchmarks: Challenges and Directions\n\nLifelong Update In real-world applications, user preferences and behaviors may change over time, requiring LLMs to continuously adapt and update their personalization strategies. Future benchmarks should include scenarios that test the model's ability to continuously update knowledge, resolve conflicts, and forget outdated information.\n\nCross-Domain Adaptation Most existing benchmarks focus on a single domain or task, which may limit the generalizability of the personalization techniques. Future benchmarks should include scenarios that test the model's ability to adapt to different domains and tasks, such as switching from news summarization to product recommendation.\n\nCross-Modal Personalization Most existing benchmarks focus on text-based data, which may not fully capture the richness and diversity of user preferences a . Future benchmarks should include scenarios that test the model's ability to integrate and utilize multi-modal data, such as images, videos, and audio.\n\nInformation Scarcity and Imbalance Due to privacy and security concerns, collecting large-scale, highquality personalized data for training and evaluation is often challenging. As a result, the gathered user data may be scarce, noisy, or imbalanced. Future benchmarks should incorporate more demanding scenarios to assess a model's ability to detect inconsistencies, refuse to answer questions beyond its knowledge, and generalize effectively from limited or biased data.\n\na Although some benchmarks like LoCoMo❀  [Maharana et al., 2024]  and MMRC❀  [Xue et al., 2025]  include multimodal data, they are still limited in scale and scope.\n\n7 Vision and Future Directions",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Vision And Challenges",
      "text": "The vision of PLLMs (Figure  8 ) is to endow models with the abilities to remember, adapt, and evolve. Remembering means grounding responses in user-specific knowledge from both short-and long-term memory. Adapting highlights the capacity to abstract and generalize from personalized data, enabling reasoning and dynamic adjustment to user preferences. Evolving emphasizes continual learning, where models update personalization without forgetting, where models update personalization without forgetting, staying aligned with users' shifting goals and their growth over time. We further analyze the vision of PLLMs from the perspective of three key techniques: prompting, adaptation, and alignment. As shown in Figure  8 , each technique contributes differently across the stages of extraction, abstraction/generalization, and evolution, and each faces distinct trade-offs in terms of efficacy, efficiency, and privacy.\n\nEfficacy At the extraction task, prompting is effective and efficient, enabling PLLMs to recall and leverage factual or personalized information with minimal overhead  [Tan et al., 2025] . However, as tasks move toward abstraction and generalization, the limitations of prompting become evident, and adaptation/alignment methods play a more central role  [Tan et al., 2024b , Bu et al., 2025] . These techniques allow models to internalize user-specific preferences, improving personalization depth, but often come at higher computational and data requirements. Finally, at the evolution stage, where continual updating and lifelong learning are essential, performance remains largely under-explored, as models struggle to grow and adapt without catastrophic forgetting.\n\nEfficiency Efficiency is a constant constraint across all goals. Extraction via prompting is highly efficient, but as adaptation and alignment become necessary for abstraction, efficiency drops sharply due to the cost of fine-tuning or maintaining additional modules  [Clarke et al., 2024] . At the evolution stage, efficiency challenges are amplified, since continual updating requires frequent synchronization between local devices and cloud servers, incurring latency, storage, and energy costs. This highlights a tension where methods that improve personalization depth often conflict with the need for scalable, lightweight deployment.\n\nPrivacy A key challenge is maintaining user privacy while still enabling personalization. Per-user PEFT strategies provide strong privacy guarantees but at the cost of reduced performance  [Peng et al., 2024b] . In contrast, collaborative or federated strategies can enhance performance by leveraging cross-user signals, but inevitably raise the risk of privacy leakage during communication or model aggregation  [Zhuang et al., 2024] . Parameter-sharing approaches offer a compromise by avoiding direct exposure of raw user data, yet they shift the bottleneck to communication and synchronization efficiency in edge-cloud coordination  [Qi et al., 2024] .\n\nOverall Trade-offs Taken together, the figure illustrates a triangular trade-off: methods that strongly guarantee privacy tend to limit performance; those that maximize performance often compromise privacy; and strategies that balance the two introduce efficiency bottlenecks. While prompting-based extraction already achieves good efficacy and efficiency, abstraction and especially lifelong evolution remain under-explored. Addressing this \"trilemma\" requires new techniques that can simultaneously enhance performance, protect privacy, and scale efficiently-a direction that we identify as a key frontier for future research.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Future Directions",
      "text": "Despite advances in Personalized Large Language Models (PLLMs), significant challenges persist, particularly in technical improvements. Current methods effectively handle basic user preferences but struggle with complex, multi-source data, especially in multimodal contexts like images and audio. Efficiently updating models on resourceconstrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult to scale. Developing small, personalized models through techniques like quantization could address these issues.\n\n1. Complex User Data While current approaches effectively handle basic user preferences, processing complex, multi-source user data remains a significant challenge. For example, methods that use user relationships in graph-like structures are still limited to retrieval augmentation  [Du et al., 2024] . How to effectively leverage this complex user information to fine-tune LLM parameters remains a significant challenge. Most methods focus on text data, while personalized foundation models for multimodal data (e.g., images, videos, audio) remain underexplored, despite their significance for real-world deployment and applications  [Wu et al., 2024c , Pi et al., 2024 , Shen et al., 2024 , Xu et al., 2025] .",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Edge Computing",
      "text": "A key challenge in edge computing is efficiently updating models on resource-constrained devices (e.g., phones), where storage and computational resources are limited. For example, fine-tuning offers deeper personalization but is resource-intensive and hard to scale, especially in real-time applications. Balancing resources with personalization needs is important. A potential solution is to build personalized small models  [Lu et al., 2024]  for edge devices, using techniques like quantization and distillation.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Edge-Cloud Collaboration",
      "text": "The deployment of PLLMs in real-world scenarios encounters significant challenges in edge-cloud computing environments. Current collaborative approaches often lack efficient synchronization between cloud and edge devices, highlighting the need to balance local computation and cloud processing  [Tian et al., 2024] .\n\n4. Efficient Adaptation to Model Updates Updating fine-tuned PEFT parameters for each user when base LLM parameters change poses a challenge due to high user data volume and limited resources. Retraining costs can be prohibitive. Future research should focus on efficient methods for updating user-specific parameters without complete retraining, such as incremental learning and transfer learning. Moreover, the emerging paradigm of small language models (SLMs)  [Liu et al., 2024d ] offers a promising direction, where lightweight models can be co-trained or specialized to handle personalization updates more efficiently while reducing memory and energy costs.\n\n5. Lifelong Updating Given the large variety of user behaviors, a key challenge is preventing catastrophic forgetting while ensuring the efficient update of long-term and short-term of memory. Future research could explore continual learning  [Wu et al., 2024d]  and knowledge editing  [Wang et al., 2024c, Zhang et al., 2024g]  to facilitate dynamic updates of user-specific information.\n\nPrivacy Privacy remains a critical concern, particularly regarding user privacy when generating personalized responses. As LLMs are not typically deployed locally, risks of privacy leakage arise. Future research should focus on privacypreserving methods, such as federated learning and differential privacy  [Yao et al., 2024, Liu et al., 2024e] , to protect user data effectively while leveraging the model's capabilities.\n\nTrustworthiness Beyond performance and efficiency, ensuring trustworthiness is critical for the adoption of personalized LLMs. Future research should address issues of interpretability, enabling users to understand why a personalized response is generated and to what extent it relies on their data  [He et al., 2024] , like neuron selection  [Zhao et al., 2025e] . Another direction is improving fairness and bias mitigation, since personalization risks amplifying stereotypes or reinforcing undesirable patterns if user-specific data is skewed. Ultimately, developing standardized evaluation protocols and benchmarks for trustworthiness in the context of personalization will be key to building systems that users not only find useful but can also confidently rely on.\n\nApplications Exploring domain-specific challenges and opportunities for personalization is essential to advance the practical adoption of PLLMs. For example, in healthcare, it promises tailored treatments but must strictly safeguard privacy  [Zhang et al., 2024c] . In education, adaptive tutoring is enabled, yet fairness and long-term modeling remain concerns  [Zhao et al., 2025f] . In creative industries, they support style-aware generation while risking overfitting and reduced diversity. In enterprise applications  [Li et al., 2025c] , personalization enhances productivity and customer support, but requires scalable, consistent, and governable solutions.\n\nTable  7 : A systematic categorization of personalization strategies for PLLMs. Methods marked with ✫ use the LaMP benchmark  [Salemi et al., 2023] , ✫ use the LongLaMP benchmark  [Kumar et al., 2024] , and ✫ use the PerLTQA benchmark  [Du et al., 2024] . (o) means optional. The overview presents four data categories (Historical Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G, Classification C, and Recommendation R), along with fine-tuning requirements for generator LLMs.   [Zhou et al., 2023]  Preference Genl.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Method",
      "text": "LLaMA-7B × × LoRA G Personalized Soups  [Jang et al., 2023]  Preference Genl. Tulu-7B × × LoRA G Reward Soups  [Rame et al., 2024]  Preference Genl.\n\nLLaMA-7B × × LoRA G MOD  [Shi et al., 2024]  Preference Genl.\n\nLLaMA-2-7B × × ✓ G PAD  [Chen et al., 2024a]  Preference Genl.\n\nLLaMA-3-8B-Instruct, Mistral-7B-Instruct × × LoRA G PPT  [Lau et al., 2024]  Preference Genl.\n\nSelf-defined × × ✓ G VPL  [Poddar et al., 2024]  Preference Genl.\n\nGPT-2, LLaMA-2-7B × × LoRA G",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Conclusions",
      "text": "This survey offers a comprehensive overview of PLLMs, focusing on personalized responses to individual user data. It presents a taxonomy categorizing approaches into three key perspectives: Personalized Prompting (Input Level), Personalized Adaptation (Model Level), and Personalized Alignment (Objective Level), with further subdivisions.\n\nA detailed method summarization is shown in Table  7 . We highlight current limitations and suggest future research directions, providing valuable insights to advance PLLM development. Beyond taxonomy, we articulate a vision for memory-centric PLLMs, highlighting three desired capabilities-remember, adapt, and evolve-and analyze their inherent trade-offs among performance, privacy, and efficiency. We further examine domain-specific challenges and discuss future directions. Together, these contributions not only summarize the state of the art but also chart promising pathways for advancing PLLM research and practical deployment.",
      "page_start": 25,
      "page_end": 25
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Comparison of General LLM vs. Personalized LLM. Different users have different preferences. General",
      "page": 1
    },
    {
      "caption": "Figure 2: Illustration of PLLM techniques for generating personalized responses through three levels: prompting (input",
      "page": 3
    },
    {
      "caption": "Figure 3: A taxonomy of PLLMs with representative examples.",
      "page": 4
    },
    {
      "caption": "Figure 4: Examples of the personalized data and query type. The human brain regions (hippocampus as spatiotemporal",
      "page": 6
    },
    {
      "caption": "Figure 4: and Table 1). We primarily",
      "page": 7
    },
    {
      "caption": "Figure 2: and Figure 3) from technical perspectives (i.e., regarding the",
      "page": 8
    },
    {
      "caption": "Figure 5: and Table 2: (1) profile-augmented prompting; (2) retrieval-augmented prompting; (3) soft-fused",
      "page": 8
    },
    {
      "caption": "Figure 5: (a)) explicitly utilizes summarized user preferences and profiles in natural",
      "page": 8
    },
    {
      "caption": "Figure 5: The illustration of personalized prompting approaches: (a) Profile-Augmented Prompting summarizes",
      "page": 9
    },
    {
      "caption": "Figure 5: (b)). Due to the complexity and volume of user data,",
      "page": 9
    },
    {
      "caption": "Figure 5: (d)). By comparing model states S with and without personalization, it",
      "page": 11
    },
    {
      "caption": "Figure 6: Illustration of personalized adaptation approaches: (a) One PEFT for All Users with shared parameters",
      "page": 12
    },
    {
      "caption": "Figure 6: and Table 3.",
      "page": 12
    },
    {
      "caption": "Figure 7: The illustration of personalized alignment approaches under the multi-objective reinforcement learning",
      "page": 15
    },
    {
      "caption": "Figure 7: MORLHF [Wu et al., 2023] separately trains reward models for each dimension and retrains the policy language models",
      "page": 15
    },
    {
      "caption": "Figure 7: (b) and (c),",
      "page": 15
    },
    {
      "caption": "Figure 8: ) is to endow models with the abilities to remember, adapt, and evolve. Remembering",
      "page": 21
    },
    {
      "caption": "Figure 8: Vision for PLLMs.The figure contrasts three capability axes—Efficacy, Efficiency, and Trustworthi-",
      "page": 22
    },
    {
      "caption": "Figure 8: , each technique contributes differently across the stages of extraction, abstrac-",
      "page": 22
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Abstract": "Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with"
        },
        {
          "Abstract": "user-specific personalization, such as understanding individual emotions, writing styles, and pref-"
        },
        {
          "Abstract": "Personalized Large Language Models (PLLMs)"
        },
        {
          "Abstract": "individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver"
        },
        {
          "Abstract": "responses that are contextually relevant and tailored to each user’s specific needs. This is a highly"
        },
        {
          "Abstract": ""
        },
        {
          "Abstract": "tions in conversational agents, recommendation systems, emotion recognition, medical assistants,"
        },
        {
          "Abstract": "and more. This survey reviews recent advancements in PLLMs from three technical perspectives:"
        },
        {
          "Abstract": "prompting for personalized context (input level), finetuning for personalized adapters (model level),"
        },
        {
          "Abstract": "and alignment for personalized preferences (objective level). To provide deeper insights, we also"
        },
        {
          "Abstract": "discuss current limitations and outline several promising directions for future research."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Different Personalities!": "I like singing"
        },
        {
          "Different Personalities!": "but hate reading."
        },
        {
          "Different Personalities!": "I like reading"
        },
        {
          "Different Personalities!": "but hate singing."
        },
        {
          "Different Personalities!": "I like going out."
        },
        {
          "Different Personalities!": "I like staying at"
        },
        {
          "Different Personalities!": "home."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "2",
          "Introduction": "Preliminary",
          "3": "4"
        },
        {
          "1": "",
          "Introduction": "2.1\nLarge Language Models",
          "3": "4"
        },
        {
          "1": "",
          "Introduction": "2.2\nProblem Statement\n.\n.",
          "3": "5"
        },
        {
          "1": "",
          "Introduction": "2.3\nProposed Taxonomy\n.",
          "3": "8"
        },
        {
          "1": "3",
          "Introduction": "Personalized Prompting",
          "3": "8"
        },
        {
          "1": "",
          "Introduction": "3.1",
          "3": "8"
        },
        {
          "1": "",
          "Introduction": "3.2",
          "3": "9"
        },
        {
          "1": "",
          "Introduction": "3.2.1",
          "3": "10"
        },
        {
          "1": "",
          "Introduction": "3.2.2",
          "3": "10"
        },
        {
          "1": "",
          "Introduction": "3.3\nSoft-Fused Prompting",
          "3": "10"
        },
        {
          "1": "",
          "Introduction": "3.4\nContrastive Prompting .",
          "3": "11"
        },
        {
          "1": "",
          "Introduction": "3.5\nDiscussions\n.\n.\n.\n.\n.\n.",
          "3": "11"
        },
        {
          "1": "4",
          "Introduction": "Personalized Adaptation",
          "3": "12"
        },
        {
          "1": "",
          "Introduction": "4.1\nOne PEFT All Users\n.",
          "3": "12"
        },
        {
          "1": "",
          "Introduction": "4.2\nOne PEFT Per User\n.",
          "3": "13"
        },
        {
          "1": "",
          "Introduction": "4.3\nDiscussions\n.\n.\n.\n.\n.\n.",
          "3": "14"
        },
        {
          "1": "5",
          "Introduction": "Personalized Alignment",
          "3": "14"
        },
        {
          "1": "",
          "Introduction": "5.1\nData Construction\n.\n.",
          "3": "14"
        },
        {
          "1": "",
          "Introduction": "5.2",
          "3": "15"
        },
        {
          "1": "",
          "Introduction": "5.3\nDiscussions\n.\n.\n.\n.\n.\n.",
          "3": "16"
        },
        {
          "1": "6 Metric and Benchmark",
          "Introduction": "",
          "3": "16"
        },
        {
          "1": "",
          "Introduction": "6.1\nMetric\n.\n.\n.\n.\n.\n.\n.\n.\n.",
          "3": "16"
        },
        {
          "1": "",
          "Introduction": "6.1.1\nClassification Task",
          "3": "17"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "",
          "Introduction": "In recent years, substantial progress has been made in Large Language Models (LLMs) such as GPT, PaLM, LLaMA,"
        },
        {
          "1": "",
          "Introduction": "DeepSeek, and their variants [Zhao et al., 2023]. These models have demonstrated remarkable versatility, achieving"
        },
        {
          "1": "",
          "Introduction": "state-of-the-art performance across various natural language processing (NLP) tasks, including question answering,"
        },
        {
          "1": "",
          "Introduction": "logical reasoning, and machine translation [Chang et al., 2024, Hu et al., 2024, Zhang et al., 2024a,b, Zhu et al., 2024a,"
        },
        {
          "1": "Wang et al., 2023a, 2024a, Li et al., 2024a, Zhao et al., 2025a, Zhu et al., 2025], with minimal task-specific adaptation.",
          "Introduction": ""
        },
        {
          "1": "",
          "Introduction": "The Necessity of Personalized LLMs (PLLMs)"
        },
        {
          "1": "",
          "Introduction": "reasoning, their lack of personalization creates challenges in situations where user-specific understanding is crucial."
        },
        {
          "1": "",
          "Introduction": "For instance, conversational agents need to adapt to a user’s preferred tone and incorporate past interactions to deliver"
        },
        {
          "1": "",
          "Introduction": "relevant, personalized responses. As LLMs evolve, integrating personalization capabilities has become a promising"
        },
        {
          "1": "",
          "Introduction": "direction for advancing human-AI interaction across diverse domains such as recommendation, education, healthcare,"
        },
        {
          "1": "",
          "Introduction": "and finance [Wang et al., 2024b, Hu et al., 2024, Zhang et al., 2024a,b, Zhu et al., 2024a, Wang et al., 2023a, 2024a]."
        },
        {
          "1": "",
          "Introduction": "Despite its promise, personalizing LLMs presents several challenges. These include efficiently representing and"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Cue-CoT [Wang et al., 2023b], PAG [Richardson et al., 2023], ONCE [Liu et al., 2024a],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Profile-Augmented"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Matryoshka [Li et al., 2024b], DPL [Qiu et al., 2025a] , RewriterSlRl [Li et al., 2024c],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(§3.1)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "R2P [Luo et al., 2025]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "MemPrompt\n[Madaan et al., 2022],\n[Zhang et al., 2023],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Memory\nMaLP [Zhang et al., 2024c], TeachMe [Dalvi et al., 2022],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "MemoRAG [Qian et al., 2024], FERMI [Kim and Yang, 2024]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Retrieval-Augmented"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "IPA,FiD [Salemi et al., 2023], MSP [Zhong et al., 2022],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(§3.2)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "AuthorPred [Li et al., 2023], PEARL [Mysore et al., 2023],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "ROPG [Salemi et al., 2024], HYDRA [Zhuang et al., 2024],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Retriever"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "RECAP [Liu et al., 2023]\n, CFRAG [Shi et al., 2025],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "AP-Bots [Yazan et al., 2025], RPM [Kim et al., 2025a],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PersonaAgent [Zhang et al., 2025a]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "UEM [Doddapaneni et al., 2024], PERSOMA [Hebert et al., 2024],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Soft-Fused"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "REGEN [Sayana et al., 2024], PeaPOD [Ramos et al., 2024], PPlug [Liu et al., 2024b],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(§3.3)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "User-LLM [Ning et al., 2024], RECAP [Liu et al., 2023], ComMer [Zeldes et al., 2025]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Contrastive"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "CoS [He et al., 2024], StyleVector\n[Zhang et al., 2025b]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(§3.4)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PEFT-U [Clarke et al., 2024], PLoRA [Zhang et al., 2024d],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "One PEFT All Users"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "LM-P [Wo´zniak et al., 2024], Review-LLM [Peng et al., 2024a],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(§4.1)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "MiLP [Zhang et al., 2024e], RecLoRA [Zhu et al., 2024a],\niLoRA [Kong et al., 2024]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "UserAdapter [Zhong et al., 2021], PocketLLM [Peng et al., 2024b],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "One PEFT Per User\nOPPU [Tan et al., 2024a], PER-PCS [Tan et al., 2024b],\n[Wagner et al., 2024],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(§4.2)\nFDLoRA [Qi et al., 2024], HYDRA [Zhuang et al., 2024],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PROPER [Zhang et al., 2025c]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Data Construction\n[Wu et al., 2024a], PLUM [Magister et al., 2024],\n[Lee et al., 2024],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(§5.1)\n[Qin et al., 2024], PRISM [Kirk et al., 2024], PersonalLLM [Zollo et al., 2024]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "MORLHF [Wu et al., 2023], MODPO [Zhou et al., 2023],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Reward Soups [Rame et al., 2024], Personalized Soups [Jang et al., 2023],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "MOD [Shi et al., 2024], BiPO [Cao et al., 2024], PAD [Chen et al., 2024a],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Optimization"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "CIPHER [Gao et al.], Amulet\n[Zhang et al., 2025d], PPT [Lau et al., 2024],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(§5.2)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "VPL [Poddar et al., 2024], CHAMELEON [Zhang et al., 2025e],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "REST-PG [Salemi et al., 2025a], Drift\n[Kim et al., 2025b], RLPA [Zhao et al., 2025b],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PROSE [Aroca-Ouellette et al., 2025], COPE [Bu et al., 2025]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Role of User Profile [Wu et al., 2024b, Zhao et al., 2025c],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Analysis"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Safety-Utility [Vijjini et al., 2024] , RAG vs. PEFT [Salemi and Zamani, 2024]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "LaMP [Salemi et al., 2023], LongLamp [Kumar et al., 2024],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "ALOE [Wu et al., 2024a] , PGraphRAG [Au et al., 2025],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Benchmark"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PerLTQA [Du et al., 2024], PEFT-U [Clarke et al., 2024],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(§6)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "REGEN [Sayana et al., 2024] , PersonalLLM [Zollo et al., 2024],"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PrefEval\n[Zhao et al., 2025d], LongMemEval\n[Wu et al., 2025]"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2": "2.1",
          "Preliminary": "Large Language Models"
        },
        {
          "2": "",
          "Preliminary": "Large Language Models (LLMs) generally refer to models that utilize the Transformer architecture and are equipped"
        },
        {
          "2": "with billions of parameters trained on trillions of text tokens. These models have demonstrated substantial improvements",
          "Preliminary": ""
        },
        {
          "2": "",
          "Preliminary": "in a myriad of tasks related to natural language understanding and generation, increasingly proving beneficial in assisting"
        },
        {
          "2": "",
          "Preliminary": "human activities. In this work, we mainly focus on autoregressive LLMs, which are based on two main architectures:"
        },
        {
          "2": "",
          "Preliminary": "decoder-only models and encoder-decoder models. Encoder-decoder models such as Flan-T5 [Chung et al., 2022] and"
        },
        {
          "2": "",
          "Preliminary": "ChatGLM [Zeng et al., 2022] analyze input through the encoder for semantic representations, making them effective in"
        },
        {
          "2": "",
          "Preliminary": "language understanding in addition to generation. Decoder-only LLMs focus on left-to-right generation by predicting"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "they are unable to generate responses tailored to a user’s unique tastes and expectations,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Let U = {ui}N\ni=1 be a finite set of users. For each ui ∈ U:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "j\nj"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "higher values indicating better personalization performance. All notations are summarized in Table 8."
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "One-Size-Fits-All Flaw"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "to an output ˆy(i)\n).\nLet M0 : Q → Y be a base (non-personalized) LLM that maps any query q(i)\n∈ Qi\n= M0(q(i)\nj"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "In the absence of personalized data Ci, given the same query q, the base model M0 produces identical outputs for"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "all users. Formally:"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "∀ui, uk ∈ U,\nif q ∈ Qi ∩ Qk then M0(q) = y(i) = y(k)"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "This reflects the non-personalized nature of the base LLM, which depends only on the input query and not on the"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "users’ personalized data, leading to large deviations from users’ expectations, i.e., large ζ."
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "Goal of PLLMs: One-Size-Fits-One"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "A general framework of LLM personalization is to design the personalization operator P : (Q → Y) × C × Θ →"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "(Q → Y), and optimize its parameters θ ⊂ Θ (if any) to effectively inject Ci\ninto the base LLM M0, producing"
        },
        {
          ", y(i)\nHere, we generally assume ζ(ˆy(i)\n) quantifies the degree of match between the desired and model outputs, with": "that minimize the overall deviation:\npersonalized models Mi for user ui"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Extraction": "Directly extract / count / aggregate factual information from 𝒞",
          "Generalization": ""
        },
        {
          "Extraction": "",
          "Generalization": "• Writing assistance (style inference and generalization)"
        },
        {
          "Extraction": "R: 30 minutes\nQ: How long is my commute to work?",
          "Generalization": "R:  @daaku  isnt  tweet"
        },
        {
          "Extraction": "",
          "Generalization": "Q: \nParaphrase \nthe \nfollowing"
        },
        {
          "Extraction": "",
          "Generalization": "deck a bit too huge for \ntweet: Do you think Tweet deck is"
        },
        {
          "Extraction": "",
          "Generalization": "an interface for u?\ntoo big for an interface, @daaku?"
        },
        {
          "Extraction": "Q: What is my occupation?\nR: Singer",
          "Generalization": ""
        },
        {
          "Extraction": "",
          "Generalization": "•\nConversational assistance (tone / perspective adaptation)"
        },
        {
          "Extraction": "",
          "Generalization": "R: \nQ: \nTell \nme \nabout \nI  will \nexplain \nthis \nin  a"
        },
        {
          "Extraction": "Abstraction",
          "Generalization": "c\nc"
        },
        {
          "Extraction": "",
          "Generalization": "Newton’s second law.\nsimple and clear manner……"
        },
        {
          "Extraction": "Information integration and pattern recognition",
          "Generalization": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Comparison of query types. We distinguish whether the response depends on the query itself, requires external": ""
        },
        {
          "Table 1: Comparison of query types. We distinguish whether the response depends on the query itself, requires external": "Type"
        },
        {
          "Table 1: Comparison of query types. We distinguish whether the response depends on the query itself, requires external": "Extraction"
        },
        {
          "Table 1: Comparison of query types. We distinguish whether the response depends on the query itself, requires external": ""
        },
        {
          "Table 1: Comparison of query types. We distinguish whether the response depends on the query itself, requires external": "Abstraction"
        },
        {
          "Table 1: Comparison of query types. We distinguish whether the response depends on the query itself, requires external": ""
        },
        {
          "Table 1: Comparison of query types. We distinguish whether the response depends on the query itself, requires external": "Generalization"
        },
        {
          "Table 1: Comparison of query types. We distinguish whether the response depends on the query itself, requires external": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "(such as a movie, item, or description) based on the user’s preferences to assist the user in categorization or"
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "summarization [Salemi et al., 2023, Au et al., 2025, Zhao et al., 2025d]."
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "Note that our survey differs significantly from role-play related LLM personalization [Tseng et al., 2024, Chen et al.,"
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "2024b, Zhang et al., 2024f]. While role-play focuses on mimicking characters during conversations [Zhu et al., 2024b,"
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "Zhao et al., 2024], PLLMs in this survey focus on understanding users’ contexts and preferences to meet their specific"
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "needs. Compared to [Zhang et al., 2024f], which emphasizes broad categories, our work provides a quite different"
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "systematic analysis of techniques to enhance PLLM efficiency and performance, with a detailed technical classification."
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "2.3\nProposed Taxonomy"
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "We propose a taxonomy (as illustrated in Figure 2 and Figure 3)\nfrom technical perspectives (i.e.,\nregarding the"
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "personalization operation P), categorizing the methods for Personalized Large Language Models (PLLMs) into three"
        },
        {
          "• Classification Classification tasks,\nincluding sentiment classification,\ninvolve labeling a particular entity": "major levels:"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": "ϕ"
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": "⊕"
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": "Pros."
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": "Cons."
        },
        {
          "Table 2: Summarization of the operation difference among different prompting methods in Equation (1). S denotes": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "many methods use an additional memory for more effective retrieval [Tan et al., 2025]. Common retrievers include"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "sparse (e.g., BM25 [Robertson et al., 1995]), and dense retrievers (e.g., Faiss [Johnson et al., 2019], Contriever [Izacard"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "et al., 2021]). These methods effectively manage the increasing volume of user data within the LLM’s context limit,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "improving relevance and personalization by integrating key evidence from the user’s personalized data."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "3.2.1\nPersonalized Memory Construction"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "This part designs mechanisms for retaining and updating memory to enable efficient retrieval of relevant information."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Non-Parametric Memory\nThis category maintains a token-based database, storing and retrieving information in"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "its original tokenized form without using parameterized vector representations. For example, MemPrompt [Madaan"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "et al., 2022] and TeachMe [Dalvi et al., 2022] maintain a dictionary-based feedback memory (key-value pairs of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "mistakes and user feedback). MemPrompt focuses on prompt-based improvements, whereas TeachMe emphasizes"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "continual learning via dynamic memory that adapts over time. MaLP [Zhang et al., 2024c] further integrates multiple"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "memory types, leveraging working memory for immediate processing, short-term memory (STM) for quick access,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "and long-term memory (LTM) for key knowledge. FERMI [Kim and Yang, 2024] maintains misaligned responses"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "and creates personalized prompts for users by using LLMs to progressively refine prompting strategies based on user"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "profiles and past opinions."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Parametric Memory\nRecent studies parameterize and project personalized user data into a learnable space, with"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "parametric memory filtering out redundant context to reduce noise. For instance, LD-Agent [Li et al., 2024d] maintains"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "memory with separate short-term and long-term banks, encoding long-term events as parametric vector representations"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "refined by a tunable module and retrieved via an embedding-based mechanism. MemoRAG [Qian et al., 2024],\nin"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "contrast, adopts a different approach by utilizing a lightweight LLM as memory to learn user-personalized data. Instead"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "of maintaining a vector database for retrieval, it generates a series of tokens as a draft to further guide the retriever,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "offering a more dynamic and flexible method for retrieval augmentation.\nInspired by cognitive theories of memory,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PRIME [Zhang et al., 2025f] introduces episodic and semantic memory mechanisms to enhance the personalization"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "capabilities of LLMs, and employs a slow-thinking strategy to further improve personalized reasoning."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "3.2.2\nPersonalized Memory Retrieval"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "The key challenge in the personalized retriever design lies in selecting not only relevant but also representative"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalized data for downstream tasks. LaMP [Salemi et al., 2023] investigates how retrieved personalized information"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "affects the responses of large language models (LLMs) through two mechanisms:\nin-prompt augmentation (IPA) and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "fusion-in-decoder (FiD). PEARL [Mysore et al., 2023] and ROPG [Salemi et al., 2024] similarly aim to enhance"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "the retriever using personalized generation-calibrated metrics,\nimproving both the personalization and text quality"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "of retrieved documents. Meanwhile, HYDRA [Zhuang et al., 2024] trains a reranker to prioritize the most relevant"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "information additionally from top-retrieved historical records for enhanced personalization. RPM [Kim et al., 2025a]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "focuses on reasoning ability by extracting user-specific factors from user historical personalized data and creating"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "annotated reasoning paths. At inference, it retrieves these examples to guide reasoning-aligned outputs, strengthening"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalized responses. HYDRA and RPM primarily target black-box models where access to the base LLM parameters"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "is restricted."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "3.3\nSoft-Fused Prompting"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Soft prompting differs from profile-augmented prompting in that it compresses personalized data into soft embeddings"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "rather than summarizing it into discrete tokens. These embeddings are generated by a user feature encoder ϕ."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "In this survey, we generalize the concept of soft prompting, showing that soft embeddings can be integrated (combination"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "operator ⊕) not only through the input but also via cross-attention or by adjusting output logits, allowing for more"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "flexible and context-sensitive responses (See Figure 5 (c))."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Input Prefix\nSoft prompting, used as an input prefix, focuses on the embedding level by concatenating the query"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "embedding with the soft embedding, and is commonly applied in recommendation tasks. PPlug [Liu et al., 2024b]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "constructs a user-specific embedding for each individual by modeling their historical contexts using a lightweight"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "plug-in user embedder module. This embedding is then attached to the task input. UEM [Doddapaneni et al., 2024] is"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "a user embedding module (transformer network) that generates a soft prompt conditioned on the user’s personalized"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "data. PERSOMA [Hebert et al., 2024] enhances UEM by employing resampling, selectively choosing a subset of user"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "interactions based on relevance and importance. REGEN [Sayana et al., 2024] combines item embeddings from user-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "item interactions via collaborative filtering and item descriptions using a soft prompt adapter to generate contextually"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalized responses. PeaPOD [Ramos et al., 2024] personalizes soft prompts by distilling user preferences into a"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "limited set of learnable, dynamically weighted prompts. Unlike previously mentioned methods, which focus on directly"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "embedding user interactions or resampling relevant data, PeaPOD adapts to user interests by weighting a shared set of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "prompts. ComMer [Zeldes et al., 2025] tunes an encoder to compress historical documents into embeddings, merges"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "them through mean pooling, then feeds the result into a frozen base LLM, distinguishing it from methods focused on"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "historical interaction data. DEP [Qiu et al., 2025b] further models inter-user differences in the latent space instead of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "merely considering the user’s own information."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Cross-Attention\nCross-attention enables the model to process and integrate multiple input sources by allowing it to"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "attend to personalized data and the query. User-LLM [Ning et al., 2024] uses an autoregressive user encoder to convert"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "historical interactions into embeddings through self-supervised learning, which are then integrated via cross-attention."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "The system employs joint training to optimize both the retriever and generator for better performance. RECAP [Liu"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "et al., 2023] utilizes a hierarchical transformer retriever designed for dialogue domains to fetch personalized information."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "This information is integrated into response generation via a context-aware prefix encoder,\nimproving the model’s"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "ability to generate personalized, contextually relevant responses."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Output Logits\nGSMN [Wu et al., 2021] retrieves relevant information from personalized data, encodes it into soft"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "embeddings, and uses them in attention with the query vector. Afterward, the resulting embeddings are concatenated"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "with the LLM-generated embeddings, modifying the final logits to produce more personalized and contextually relevant"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "responses."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "3.4\nContrastive Prompting"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "The key insight of contrastive prompting is to utilize two forward paths of LLM to generate contrast pairs with and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "without personalized information (See Figure 5 (d)). By comparing model states S with and without personalization, it"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "identifies factors influencing personalization, enabling dynamic adjustment of the model’s personalization level. Two"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "mainstream model states are utilized: hidden states (representations) and logits (predictive distributions)."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "CoS [He et al., 2024] is a special case that assumes there is a brief user profile C for each query;\nit amplifies its"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "influence in LLM response generation by combining output probabilities (logits) with and without\nthe profile,\ni.e.,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "S (M0(q)) + α × S (M0(q, C)), adjusting personalization degree through hyperparameter α without fine-tuning. In"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "contrast, StyleVector [Zhang et al., 2025b] obtains encoded personalized information in LLM via hidden representations."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(cid:16)\n(cid:17)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "c(i)\n, r(i)\nit use GPT-3.5-\nSpecifically, given a user ui and the personalized data (user historical content),\n∈ Ci,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "j\nj"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Turbo M to generate a general\nresponse for comparison with the expected personalized response r(i)\n,\ni.e., ϕ :\nj"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(cid:104)\n(cid:16)\n(cid:16)\n(cid:16)\n(cid:17)(cid:17)(cid:17)\n(cid:16)\n(cid:16)\n(cid:17)(cid:17)(cid:105)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(cid:80)|Ci|"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "1\nS\nc(i)\n, M\nc(i)\n− S\nc(i)\n, r(i)\n, where S extracts hidden representations in base model\nM0\nM0"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "j\nj\nj\nj\nj=1\n|Ci|"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "this\nM0’s middle layer. ϕ extracts the user style-based vector, and by adding this vector with a scaling factor α,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "method can steer the LLM toward personalized output to a controllable degree. CoSteer [Lv et al., 2025] performs"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "decoding-time personalization through local delta steering. A local SLM produces logits with and without personal"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "context, and their difference ∆ = log π∗\nis used to steer the cloud LLM’s logits iteratively, preserving\npers − log π∗"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "base"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "privacy while achieving controllable personalization without fine-tuning."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "3.5\nDiscussions"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Personalized Prompting Method: Pros and Cons": "• Profile-augmented prompting improves efficiency by compressing historical data but risks information"
        },
        {
          "Personalized Prompting Method: Pros and Cons": "loss and reduced personalization."
        },
        {
          "Personalized Prompting Method: Pros and Cons": "• Retrieval-augmented prompting offers rich, context-aware inputs and scales well for long-term memory"
        },
        {
          "Personalized Prompting Method: Pros and Cons": "but can suffer from computational limits and irrelevant data retrieval."
        },
        {
          "Personalized Prompting Method: Pros and Cons": "• Soft prompting efficiently embeds user-specific info, capturing semantic nuances without redundancy,"
        },
        {
          "Personalized Prompting Method: Pros and Cons": "but is limited to black-box models and lacks explicit user preference analysis."
        },
        {
          "Personalized Prompting Method: Pros and Cons": "• Contrastive prompting provides interpretability and controllable personalization by comparing model"
        },
        {
          "Personalized Prompting Method: Pros and Cons": "states with and without personalized information, but suffers from hyperparameter sensitivity when"
        },
        {
          "Personalized Prompting Method: Pros and Cons": "choosing the hyperparameter, i.e., scaling factor α."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Personalized \nAll Users’ \n(1) Single PEFT"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "LLM"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Data"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Personalized Data"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "……"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PEFT"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "LoRA\nAdapters"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Module"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Collaborative"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(2) Mixture of Experts (MoE)\nEfforts"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PEFT"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Module"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PEFT"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "LLM"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Module"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PEFT"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Module"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Aggregation Module"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(a) One PEFT All Users\n(b) One PEFT Per User"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Illustration of personalized adaptation approaches:\n(a) One PEFT for All Users with shared parameters"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "One PEFT for All Users\nOne PEFT Per User"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(Shared Parameters, Section 4.1)\n(Individual Parameters, Section 4.2)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "|Ci|\n|Ci|"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(cid:1)"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "N(cid:88) i\n(cid:88) j\nN(cid:88) i\n(cid:88) j\nmin\n; θ), r(i)\nmin\n; θi), r(i)\nL(cid:0)M0(c(i)\nL(cid:0)M0(c(i)\nj\nj\nj\nj"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "θ"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "{θi}N\ni=1"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "=1\n=1\n=1\n=1"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "y(i)\ny(i)\n= M0(q(i)\n, Ci; θ)\n= M0(q(i)\n; θi)\nj\nj\nj\nj"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Parameter Efficiency\nStrong Personalization"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Scalability\nUser Isolation"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Limited Personalization\nStorage Overhead"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Persnonalized Data Dependency\nTraining Complexity"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "user behavior comprehension. Overall, prompting-based methods are efficient and adaptable, enabling dynamic"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalization with minimal computational overhead. Especially for RAG, which reduces LLM hallucination by"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "grounding responses in retrieved evidence. Therefore, prompting methods are particularly suitable for explicit queries"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "that require answering specific factual information, as mentioned in Section 2.2, excelling when personalization involves"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "However, they lack deeper personalization analysis, as they rely on predefined prompt structures to inject user-specific"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "information and are limited in accessing global knowledge due to the narrow scope of prompts, which usually fail in"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Personalized Adaptation"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PLLMs require balancing fine-tuning’s deep adaptability with the efficiency of prompting. Therefore, specialized"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "methods need to be specifically designed for PLLMs to address these challenges utilizing parameter-efficient fine-tuning"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "methods (PEFT), such as LoRA [Hu et al., 2021, Yang et al., 2024], prefix-tuning [Li and Liang, 2021], MeZo [Malladi"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "et al., 2023], etc. From an architectural perspective, these methods can be categorized into two types: (1) one PEFT for"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "all users, where all users share a single fine-tuning module that differentiates personalized information, and (2) one"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PEFT per user, where each user has their own unique fine-tuning block. Details are illustrated in Figure 6 and Table 3."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "One PEFT All Users"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "This method trains on all users’ data using a shared PEFT module, eliminating the need for separate modules per user."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "12"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Single PEFT\nPLoRA [Zhang et al., 2024d] and LM-P [Wo´zniak et al., 2024] utilize LoRA for PEFT of LLM,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "injecting personalized information via user embeddings and user IDs, respectively. PLoRA is further extended and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "supports online training and prediction for cold-start scenarios. UserIdentifier [Mireshghallah et al., 2021] uses a"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "static, non-trainable user identifier to condition the model on user-specific information, avoiding the need for trainable"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "user-specific parameters and reducing training costs. Review-LLM [Peng et al., 2024a] aggregates users’ historical"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "behaviors and ratings into prompts to guide sentiment and leverages LoRA for efficient fine-tuning. However, these"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "methods rely on a single architecture with fixed configurations (e.g., hidden size, insertion layers), making them unable"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "to store and activate diverse information for personalization [Zhou et al., 2024]. To solve this problem, MiLP [Zhang"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "et al., 2024e] utilizes a Bayesian optimization strategy to automatically identify the optimal configuration for applying"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "multiple LoRA modules, enabling efficient and flexible personalization."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Mixture of Experts (MoE)\nSeveral methods use the LoRA module, but with a static configuration for all users. This"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "lack of parameter personalization limits adaptability to user dynamics and preference shifts, potentially resulting in"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "suboptimal performance [Cai et al., 2024]. RecLoRA [Zhu et al., 2024a] addresses this limitation by maintaining a set of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "parallel, independent LoRA weights and employing a soft routing method to aggregate meta-LoRA weights, enabling"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "more personalized and adaptive results. Similarly, iLoRA [Kong et al., 2024] creates a diverse set of experts (LoRA) to"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "capture specific aspects of user preferences and generates dynamic expert participation weights to adapt to user-specific"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "behaviors."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Shared PEFT methods rely on a centralized approach, where user-specific data is encoded into a shared adapter by"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "centralized LLMs. This limits the model’s ability to provide deeply personalized experiences tailored to individual"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "users. Furthermore, using a centralized model often requires users to share personal data with service providers, raising"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "concerns about the storage, usage, and protection of this data."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "4.2\nOne PEFT Per User"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Equipping a user-specific PEFT module makes LLM deployment more personalized while preserving data privacy."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "However, the challenge lies in ensuring efficient operation in resource-limited environments, as users may lack sufficient"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "local resources to perform fine tuning."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "No Collaboration\nThere is no collaboration or coordination between adapters or during the learning process for each"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "use in this category. UserAdapter [Zhong et al., 2021] personalizes models through prefix-tuning, fine-tuning a unique"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "prefix vector for each user while keeping the underlying transformer model shared and frozen. PocketLLM [Peng et al.,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2024b] utilizes a derivative-free optimization approach, based on MeZo [Malladi et al., 2023], to fine-tune LLMs on"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "memory-constrained mobile devices."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Collaborative Efforts\nThe “one-PEFT-per-user\" paradigm without collaboration is computationally and storage-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "intensive, particularly for\nlarge user bases. Additionally,\nindividually owned PEFTs hinder community value, as"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personal models cannot easily share knowledge or benefit from collaborative improvements. OPPU [Tan et al., 2024a]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "frames the personalized LLMs training as a two-stage process, where a global LoRA module is learned to capture shared"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "knowledge across users initially, followed by user-specific LoRA modules for individual adaptation. PerFit [Liu et al.,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2025] further enhances this by replacing LoRA modules with representation fine-tuning modules, motivated by their"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "finding that the personalization of LLM correlates with a low-rank collaborative shift and diverse personalized shifts"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "following the collaborative one. CoPe [Bu et al., 2025] utilizes reward-guided decoding tailored for personalization,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "aiming to maximize the implicit\nreward signal\nfor each user. PER-PCS [Tan et al., 2024b] enables efficient and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "collaborative PLLMs by sharing a small fraction of PEFT parameters across users. It first divides PEFT parameters into"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "reusable pieces with routing gates and stores them in a shared pool. For each target user, pieces are autoregressively"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "selected from other users, ensuring scalability, efficiency, and personalized adaptation without additional\ntraining."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PROPER [Zhang et al., 2025c] introduces hierarchical user grouping with MoE-LoRA integration, enabling fine-grained"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalization through staged adaptation from population-level, group-level, to individual-level models."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Another efficient collaborative strategy is based on the federated learning (FL) framework. For example, Wagner et al."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "[2024] introduces a FL framework for on-device LLM fine-tuning, using strategies to aggregate LoRA model parameters"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "and handle data heterogeneity efficiently, outperforming purely local fine-tuning. FDLoRA [Qi et al., 2024] introduces a"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalized FL framework using dual LoRA modules to capture personalized and global knowledge. It shares only"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "global LoRA parameters with a central server and combines them via adaptive fusion, enhancing performance while"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "minimizing communication and computing costs."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "There are other frameworks that can be explored, such as HYDRA [Zhuang et al., 2024], which also employs a base"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "model to learn shared knowledge. However, in contrast to federated learning, it assigns distinct heads to each individual"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "user to extract personalized information."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Survey of Personalized Large Language Models": "",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "",
          "A PREPRINT": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Personalized Adaptation Methods: Challenges": "Overfitting\nWith limited or noisy user data, models may fail to generalize and lose robustness."
        },
        {
          "Personalized Adaptation Methods: Challenges": "Performance–Privacy–Efficiency Trade-off\nA per-user PEFT strategy ensures strong privacy but suffers"
        },
        {
          "Personalized Adaptation Methods: Challenges": "from limited performance. Introducing collaboration improves performance yet risks privacy leakage. Sharing"
        },
        {
          "Personalized Adaptation Methods: Challenges": "only parameters instead of raw user data alleviates privacy concerns, but introduces efficiency challenges in"
        },
        {
          "Personalized Adaptation Methods: Challenges": "edge–cloud coordination, underscoring the difficulty of jointly optimizing all three dimensions."
        },
        {
          "Personalized Adaptation Methods: Challenges": "Cold-start\nAdapting to new users with sparse or unseen data remains difficult."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "(e.g., Factuality)": "(a) Multi-Objective RLHF\n(b) Personalized Weight Merging\n(c) Personalized Model Ensemble"
        },
        {
          "(e.g., Factuality)": "Training-Time\nDecoding-Time"
        },
        {
          "(e.g., Factuality)": "Figure 7: The illustration of personalized alignment approaches under the multi-objective reinforcement\nlearning"
        },
        {
          "(e.g., Factuality)": "paradigm.\n(a) Multi-Objective RLHF trains separate reward models for different preference dimensions and uses"
        },
        {
          "(e.g., Factuality)": "their weighted combination to guide policy optimization.\n(b) Personalized Weight Merging combines multiple"
        },
        {
          "(e.g., Factuality)": "independently trained policy models with user-specific weights during inference. (c) Personalized Model Ensemble"
        },
        {
          "(e.g., Factuality)": "generates personalized outputs by ensembling predictions from multiple specialized policy models with dynamic"
        },
        {
          "(e.g., Factuality)": "user-specific weighting."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 5: We summarize Accuracy (Acc), F1 Score",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "significant effectiveness when compared to the parameter-merging baseline. PAD [Chen et al., 2024a] leverages a"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalized reward modeling strategy to generate token-level rewards that guide the decoding process, enabling the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "dynamic adaptation of the base model’s predictions to individual preferences."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "However, these approaches are limited by their reliance on predefined personalization dimensions and a small set of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "user preferences, which fail to capture the diversity of real-world personalization needs. Recent work has therefore"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "sought to extend reward learning to better accommodate user-specific preferences. For example, VPL [Poddar"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "et al., 2024] employs a variational encoder to encode a small number of preference annotations from a given user"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "into a latent variable that captures individual\ntaste. A reward model\nis then conditioned on the latent variable and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "trained by maximizing the evidence lower bound, enabling accurate prediction of rewards aligned with the user’s unique"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preferences. PREF [Shenfeld et al., 2025] models each user’s personalized reward function as a linear combination"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "of shared base reward functions, which are first\nlearned from multi-user data via matrix factorization. Using an"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "active learning strategy,\nit can efficiently infer a new user’s combination weights with only a handful of feedback"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "examples (e.g., 10), enabling user-specific adaptation of LLMs’ outputs without retraining. PPT [Lau et al., 2024]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "leverages in-context learning for scalable personalization by generating two candidate responses for each user prompt,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "soliciting user rankings, and dynamically incorporating this feedback into the model’s context to adapt to individual"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preferences over time. Drift [Kim et al., 2025b] proposes a training-free method where user preference is modeled"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "as a linear combination of interpretable attributes, and then computes a reward signal by comparing log-likelihoods"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "under attribute-specific prompts to guide the generation of a frozen base model. In contrast to these offline methods"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "based on static preferences, other approaches focus on real-time adaptation. AMULET [Zhang et al., 2025g] approaches"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "this from another unique test-time perspective, by formulating the decoding process of each token as an independent"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "online learning problem.\nIt obtains the optimization direction by contrasting the model’s output with and without"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "user-provided prompts and utilizes an efficient closed-form solution for real-time iterative optimization, thus adapting"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "to personalized user needs instantly without retraining the model. RLPA [Zhao et al., 2025b] simulates real users via"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "a dynamically updated user portrait model, enabling the language model agent\nto engage in multi-turn interactive"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "reinforcement\nlearning within this simulated environment,\nthus continuously adapting its policy to evolving user"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preferences. PersonaAgent [Zhang et al., 2025a] extends personalization from text style to decision behaviors such as"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "tool usage, guided by a dynamic Persona that directs the agent’s decisions. Using test-time user-preference alignment, it"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "compares the agent’s responses with the user’s real actions in real time, using textual loss as feedback to optimize the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Persona, which ensures personalization run through every step of the tool decision."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "5.3\nDiscussions"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Current mainstream personalized alignment technologies mainly model personalization as multi-objective reinforcement"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "learning problems, where personalized user preferences are taken into account during the training phase of policy LLMs"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "via canonical RLHF, or the decoding phase of policy LLM via parameter merging or model ensembling. Typically,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "these methods are limited to a small number (e.g., three) of predefined preference dimensions, represented through"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "textual user preference prompts. However, in real-world scenarios, there could be a large number of personalized users,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "and their preference vectors may not be known, with only their interaction history accessed. Consequently, developing"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "more realistic alignment benchmarks to effectively assess these techniques is a critical area for future research."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "indicate multi-class / binary-class and ordinal class, respectively."
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "Task Type"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "C (M)"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "C (M)"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "C (M)"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "C (O)"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "C (O)"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "G"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "R"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "R"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "R"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": "R"
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        },
        {
          "task type, and mathematical formulation. Notes indicate the applicable scenarios and characteristics of this metric.": ""
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Ordinal Classification (O)\nFor ordinal multi-class classification, where labels possess a natural order or ranking,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "performance metrics like MAE and RMSE are preferred, as they account\nfor\nthe magnitude of prediction errors"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "relative to the true order, providing a more nuanced evaluation of model quality. RMSE reflects the square-root of the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "mean of squared errors, providing a magnitude-sensitive measure that penalizes larger deviations more heavily. MAE"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "complements this by computing the average absolute error, offering robustness to outliers."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "6.1.2\nGeneration Task"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "For\nthe generation task, S and R represent\nthe generated and reference sequences, and LCS(S, R) indicates the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "length of the longest common subsequence between S and R.\n|S| and |R| denote the number of unique words in the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "candidate and reference texts, respectively. In the Perplexity formula, wi\nis the i-th token in the reference text, w<i"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "represents all tokens that come before position i, and p(wi|w<i) is the probability that the model assigns to token wi"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "given the preceding tokens.\nIn the BLEU formula, the brevity penalty is based on c and s, which are the total word"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "counts (including repeated words) in the candidate and reference sentences, respectively. In the METEOR formula, m"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "represents the number of matched words between the candidate text and reference text (including exact matches, stem"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "matches, and synonym matches). ch represents the number of \"chunks\" in the matching sequence, where a chunk is a"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "contiguous sequence of matched words. A higher number of chunks indicates less fluent or more fragmented matching"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "between texts."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Human-based Evaluation (Human-E)\nHuman-E is a process where human judges assess LLM outputs by rating"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "them on accuracy, helpfulness, coherence, appropriateness, etc.\nIn this approach, human evaluators review and rate"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "responses based on criteria such as accuracy, helpfulness, coherence, and appropriateness. Human-E is considered"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "the gold standard because humans can detect nuances in language, recognize factual errors, understand contextual"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "appropriateness, and make holistic judgments that automated metrics often miss."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Conventional Evaluation\nBLEU, ROUGE-1, ROUGE-L, and METEOR to measure lexical overlap between the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "generated y and ground-truth ˆy responses. BLEU evaluates n-gram precision focusing on system output, whereas"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "ROUGE-1 measures unigram recall relative to reference texts. ROUGE-L extends this by capturing the longest common"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "subsequence structure. METEOR incorporates linguistic features,\nincluding stemming and synonym matching, for"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "enhanced evaluation. For semantic-level assessment beyond surface-level patterns, SBERT-based cosine similarity [Li"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "et al., 2025a] provides a vector-based measure of meaning preservation between generated and reference responses."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "LLM-based Evaluation (LLM-E)\nHuman-based evaluation provides the most reliable assessment of LLM response"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "quality, but obtaining statistically significant results is costly and time-intensive. Therefore,\nthe “LLM-as-a-judge”"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "framework [Gu et al., 2024] is proposed that uses LLM to automatically evaluate the quality and relevance of generated"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "text. By prompting LLMs to score or compare outputs,\nit offers scalable, context-aware, and semantically rich"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "assessments with minimal human input. This approach surpasses traditional metrics in flexibility but faces challenges"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "like model bias and consistency. It represents a promising method for automated, nuanced evaluation. ExPerT [Salemi"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "et al., 2025b] introduces an explainable reference-based evaluation framework specifically designed for style-based"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalized text generation. It leverages an LLM to extract atomic aspects and their evidence from the generated and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "reference texts, match the aspects, and evaluate their alignment based on content and writing style – two key attributes"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "in personalized text generation."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Personalization Degree Evaluation (Per-E)\nThe aforementioned metrics are unable to assess the personalization"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "capability of summarization models. To address this limitation, EGISES [Vansh et al., 2023]\nis proposed as the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "first automatic measure for evaluating personalization in text summarization. The core Deviation function measures"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "min(Xu,u′ ,Yu,u′ )"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "summary-level personalization by computing the ratio\nmax(Xu,u′ ,Yu,u′ ) averaged across all user pairs, where Xu,u′ and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "represent the Jensen-Shannon divergence-weighted differences between users’ expected summaries and model-\nYu,u′"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "generated summaries, respectively. The Deviation function measures personalization quality: Dev(Sc,ui, Rc,ui) → 1"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "fails to match user\nfor good personalization, Dev(Sc,ui, Rc,ui ) → 0 for poor personalization, where the model"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "expectation differences. Building upon EGISES, P-Accuracy [Vansh et al., 2023] provides a personalization-aware"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "accuracy measure that penalizes traditional accuracy scores based on the model’s personalization capability, Acc(M ) ·"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "[1 − α · σ(β·EGISES(M ))\n], where σ is the sigmoid function, α and β control the penalty intensity and personalization"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Acc(M )"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "emphasis, respectively. However, EGISES suffers from the personalization-accuracy paradox, where models can"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "achieve high responsiveness but\nlow accuracy,\nleading to poor user experience. PerSEval [Dasgupta et al., 2024]"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "addresses this limitation by introducing the EDP that penalizes EGISES scores when accuracy drops, computed as"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PerSEval = DEGRESS × EDP, where DEGRESS measures responsiveness, and EDP incorporates accuracy-based"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "penalties to ensure that personalization evaluation considers both user preference alignment and content quality."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "6.1.3\nRecommendation Task"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "For the recommendation task, reli denotes the relevance score of the item at rank position i, which may be binary (0 or"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "1) or graded (multi-level) depending on the evaluation context. In the NDCG formula, |RK| is the number of relevant"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "represents the relevance values sorted in descending order (the ideal ordering).\nitems (limited to K items), rel∗\ni"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Item Recommendation\nTraditional recommendation tasks typically use Hit Ratio (HR), Recall, and Discounted"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Cumulative Gain (NDCG)\n[Ning et al., 2024, Ramos et al., 2024] as standard evaluation metrics to measure the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "effectiveness of top-K recommendation and preference ranking."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Conversational Recommendation\nConversational recommendation systems commonly use Recall and NDCG as"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "evaluation metrics to measure coverage and ranking quality, employing the \"LLMs-as-a-judger\" framework [Zhao et al.,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2025c, Huang et al., 2024, Sayana et al., 2024]. Additionally, an LLM-based user simulator—creating unique personas"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "via zero-shot ChatGPT prompting and defining preferences using dataset attributes—is also used to assess whether"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "outputs align with user preferences [Huang et al., 2024]. Note that in this scenario, we still categorize it as a generation"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "task (G), as the core objective remains text generation in context, while the input data contains interaction information."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "6.2\nBenchmark"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Benchmarks for LLM personalization can be categorized based on the types of input data they utilize, the nature of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "the queries they address, and the specific tasks they are designed to evaluate. In terms of the personalized data format,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "benchmarks can be classified into four main types: Historical Content, Dialogues, Interactions, and Pre-defined Human"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Preference. Sometimes the user profiles are also included as auxiliary information. The mainstream benchmarks for"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "each data type are summarized in Table 6 23."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Dialogue-based Data\nFor dialogue-based data, the benchmarks typically focus on the ability of LLMs to extract and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "utilize personalized information (e.g., user characteristics, schedules, preferences) from multi-turn conversations. The"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "challenging aspect here is to accurately capture user intent and context from the long-term dialogue history, which"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "may involve complex interactions, implicit preferences, and enormous unrelated content [Li et al., 2025b, Zhao et al.,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2025d, Mok et al., 2025]. Therefore, the mainstream techniques for this type of data are prompting-based methods,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "which can effectively leverage the most decisive information from the dialogue history without requiring extensive"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "model retraining. The query types in these benchmarks often include explicit extraction of personalized factual details."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Naturally, the metrics used are usually accuracy-based, such as F1 score and accuracy, with LLM-E as a preprocessing"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "step to calculate the exact matching scores between the generated and ground-truth responses [Xue et al., 2025, Wu"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "et al., 2025, Tan et al., 2025]."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Historical Content-based Data\nFor historical content-based data, the benchmarks often evaluate how well LLMs can"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "incorporate and reason over user-specific documents, such as past tweets, articles, or other textual materials. The key"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "challenge is to enable the model to understand and integrate this external knowledge into its responses, which may"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "require sophisticated retrieval and comprehension capabilities [Salemi et al., 2023, Kumar et al., 2024]. Benefiting from"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "the purer text content provided in these benchmarks, techniques like SFT are more commonly employed to intrinsically"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "adapt the model to the user’s content, while the prompting-based RAG and PAG methods are auxiliarily used to further"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "retrieve and conclude the most relevant terms for fine-tuning [Au et al., 2025, Salemi and Zamani, 2025]. More query"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "types of abstraction and generalization are included in these benchmarks, which require the model to not only extract"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "factual information but also adapt its style and tone to match user preferences [Salemi et al., 2023]. Therefore,\nthe"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "metrics used are more diverse,\nincluding both accuracy-based metrics and generation quality metrics like ROUGE,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "BLEU, and METEOR, with LLM-E and Human-E as supplementary evaluations [Li et al., 2025a]."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Preference-based Data\nFor preference-based data, the benchmarks typically assess how well LLMs can align their"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "outputs with user-defined preferences, which may be explicitly stated or implicitly inferred from user behavior. The"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "main challenge is to accurately model and incorporate these preferences into the generation process, which may"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "involve complex reasoning and decision-making [Kirk et al., 2024, Wu et al., 2024a]. Techniques like RLHF and DPO"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "are often employed to fine-tune the model based on user feedback, while prompting-based methods like ICL are also"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2PERSONABench in 6 is the abbreviation for PERSONACONVBENCH [Li et al., 2025a]."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "3Among fact-based query benchmarks (such as PerLTQA [Du et al., 2024], LongMemEval [Wu et al., 2025], and IMPLEX-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "CONV Li et al. [2025b]), some include memory-related testing tasks, like retrieval. Since these are not the focus of this paper, which"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "investigates techniques for injecting and utilizing personalized user data to guide LLMs toward better personalized performance,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "these tasks are not listed in the summary."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table 6: Summary of personalized large language model benchmarks. Five data categories are shown: Historical",
      "data": [
        {
          "A Survey of Personalized Large Language Models": "Table 6: Summary of personalized large language model benchmarks. Five data categories are shown: Historical",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference (Section 2.2). Three task types",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "are included: Generation G, Classification C, and Recommendation R. Techniques (abbreviated as): Prompting PT",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "(Section 3), Adaptation AD (Section 4), and Alignment AL (Section 5). Three query types are included: Abstraction,",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Extraction and Generalization. Note: In the row for ’All (excl. LLM-E)’, ’All’ refers to all metrics corresponding to the",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "methods from the benchmark. ❀indicates the benchmark comprises multi-modal personalized data or output.",
          "A PREPRINT": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table 6: Summary of personalized large language model benchmarks. Five data categories are shown: Historical",
      "data": [
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "methods from the benchmark. ❀indicates the benchmark comprises multi-modal personalized data or output."
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "Benchmark"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "MemoryBank"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Xue et al., 2025]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "PerLTQA"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Du et al., 2024]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "LoCoMo ❀"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Maharana et al.,"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "2024]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "LongMemEval"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Wu et al., 2025]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "MMRC ❀"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Xue et al., 2025]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "IMPLEXCONV"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Li et al., 2025b]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "PrefEval"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Zhao et al., 2025d]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "HiCUPID"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Mok et al., 2025]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "MemBench"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Tan et al., 2025]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "PER-CHAT"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Wu et al., 2021]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "LaMP"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Salemi et al., 2023]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "LongLaMP"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Kumar et al., 2024]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "PEFT-U"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Clarke et al., 2024]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "pGraphRAG"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Au et al., 2025]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "LaMP-QA"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Salemi and Zamani, 2025]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "DPL"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Qiu et al., 2025a]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "PERSONABench"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Li et al., 2025a]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "PRISM"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Kirk et al., 2024]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "PersonalLLM"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Zollo et al., 2024]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "ALOE"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Wu et al., 2024a]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "REGEN"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Sayana et al., 2024]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "PersonalWAB"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Cai et al., 2025]"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "RecBench+"
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": ""
        },
        {
          "tasks listed in Table 5; ’excl.’ abbreviates ’excluding’. * indicates methods built following the benchmark rather than": "[Huang et al., 2025]"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "not only factually accurate [Mok et al., 2025, Zollo et al., 2024] but also align with user tastes and styles [Kirk et al.,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "For interaction-based data, the benchmarks often focus on how well LLMs can leverage"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "likes, and other engagement signals,\nto personalize recommendations and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "quality metrics like BLEU and ROUGE are also used when the recommendation is presented in a conversational format."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ""
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "should consider the following aspects:": "LLM Personalization Benchmarks: Challenges and Directions"
        },
        {
          "should consider the following aspects:": "Lifelong Update\nIn real-world applications, user preferences and behaviors may change over time, requiring"
        },
        {
          "should consider the following aspects:": "LLMs to continuously adapt and update their personalization strategies. Future benchmarks should include"
        },
        {
          "should consider the following aspects:": "scenarios that test the model’s ability to continuously update knowledge, resolve conflicts, and forget outdated"
        },
        {
          "should consider the following aspects:": "information."
        },
        {
          "should consider the following aspects:": "Cross-Domain Adaptation\nMost existing benchmarks focus on a single domain or task, which may limit the"
        },
        {
          "should consider the following aspects:": "generalizability of the personalization techniques. Future benchmarks should include scenarios that test the"
        },
        {
          "should consider the following aspects:": "model’s ability to adapt to different domains and tasks, such as switching from news summarization to product"
        },
        {
          "should consider the following aspects:": "recommendation."
        },
        {
          "should consider the following aspects:": "Cross-Modal Personalization\nMost existing benchmarks focus on text-based data, which may not fully"
        },
        {
          "should consider the following aspects:": "capture the richness and diversity of user preferences a. Future benchmarks should include scenarios that test"
        },
        {
          "should consider the following aspects:": "the model’s ability to integrate and utilize multi-modal data, such as images, videos, and audio."
        },
        {
          "should consider the following aspects:": "Information Scarcity and Imbalance\nDue to privacy and security concerns, collecting large-scale, high-"
        },
        {
          "should consider the following aspects:": "quality personalized data for training and evaluation is often challenging. As a result, the gathered user data may"
        },
        {
          "should consider the following aspects:": "be scarce, noisy, or imbalanced. Future benchmarks should incorporate more demanding scenarios to assess"
        },
        {
          "should consider the following aspects:": "a model’s ability to detect inconsistencies, refuse to answer questions beyond its knowledge, and generalize"
        },
        {
          "should consider the following aspects:": "effectively from limited or biased data."
        },
        {
          "should consider the following aspects:": "aAlthough some benchmarks like LoCoMo❀ [Maharana et al., 2024] and MMRC❀ [Xue et al., 2025] include multi-"
        },
        {
          "should consider the following aspects:": "modal data, they are still limited in scale and scope."
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Remember!\nAdapt!\nEvolve!"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Grounded in Memory\nImage beyond Memory\nLifelong Learning"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "•\nUpdating\n•\nUnderstanding"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "•\nShort-term memory"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "•\nGrowing\n•\nInspiration"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "•\nLong-term memory"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "•\nNo forgetting\n•\nReasoning"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Abstraction, Generalization\nEvolution?\nExtraction"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "| \n|  \n| \n|  \nEfficacy\n| \n|"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Efficiency\n| \n|  \n| \n|  \n| \n|"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "| \n|  \n| \n|  \n| \n|  \nPrivacy"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": ": good\n: open\n: poor\n* Prompting  | Adaptation | Alignment"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Figure 8: Vision for PLLMs.The figure contrasts\nthree capability axes—Efficacy, Efficiency, and Trustworthi-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "ness—across different dimensions: Extraction, Abstraction / Generalization, and Evolution. Within each cell, icons"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "summarize (left to right) Prompting\n(Section 3) | Adaptation\n(Section 4) | Alignment\n(Section 5). The meaning of Leg-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "end is as follows:\n(good/strong, capability already well-demonstrated);\n(poor/weak, capability not well-supported);"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "(open/under-explored, requiring further research)."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "We further analyze the vision of PLLMs from the perspective of three key techniques: prompting, adaptation, and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "alignment. As shown in Figure 8, each technique contributes differently across the stages of extraction, abstrac-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "tion/generalization, and evolution, and each faces distinct trade-offs in terms of efficacy, efficiency, and privacy."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Efficacy At the extraction task, prompting is effective and efficient, enabling PLLMs to recall and leverage factual"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "or personalized information with minimal overhead [Tan et al., 2025]. However, as tasks move toward abstraction"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "and generalization,\nthe limitations of prompting become evident, and adaptation/alignment methods play a more"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "central role [Tan et al., 2024b, Bu et al., 2025]. These techniques allow models to internalize user-specific preferences,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "improving personalization depth, but often come at higher computational and data requirements. Finally, at the evolution"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "stage, where continual updating and lifelong learning are essential, performance remains largely under-explored, as"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "models struggle to grow and adapt without catastrophic forgetting."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Efficiency\nEfficiency is a constant constraint across all goals. Extraction via prompting is highly efficient, but as"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "adaptation and alignment become necessary for abstraction, efficiency drops sharply due to the cost of fine-tuning"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "or maintaining additional modules [Clarke et al., 2024]. At the evolution stage, efficiency challenges are amplified,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "since continual updating requires frequent synchronization between local devices and cloud servers, incurring latency,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "storage, and energy costs. This highlights a tension where methods that improve personalization depth often conflict"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "with the need for scalable, lightweight deployment."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Privacy\nA key challenge is maintaining user privacy while still enabling personalization. Per-user PEFT strategies"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "provide strong privacy guarantees but at the cost of reduced performance [Peng et al., 2024b]. In contrast, collaborative"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "or\nfederated strategies can enhance performance by leveraging cross-user signals, but\ninevitably raise the risk of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "privacy leakage during communication or model aggregation [Zhuang et al., 2024]. Parameter-sharing approaches"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "offer a compromise by avoiding direct exposure of raw user data, yet they shift the bottleneck to communication and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "synchronization efficiency in edge–cloud coordination [Qi et al., 2024]."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Overall Trade-offs Taken together,\nthe figure illustrates a triangular\ntrade-off: methods that strongly guarantee"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "privacy tend to limit performance;\nthose that maximize performance often compromise privacy; and strategies that"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "balance the two introduce efficiency bottlenecks. While prompting-based extraction already achieves good efficacy and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "efficiency, abstraction and especially lifelong evolution remain under-explored. Addressing this “trilemma” requires"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "new techniques that can simultaneously enhance performance, protect privacy, and scale efficiently—a direction that we"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "identify as a key frontier for future research."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "22"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "7.2\nFuture Directions"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Despite advances in Personalized Large Language Models (PLLMs), significant challenges persist, particularly in"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "technical\nimprovements. Current methods effectively handle basic user preferences but struggle with complex,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "multi-source data, especially in multimodal contexts like images and audio. Efficiently updating models on resource-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "to scale. Developing small, personalized models through techniques like quantization could address these issues."
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": "to scale. Developing small, personalized models through techniques like quantization could address these issues."
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": "graph-like structures are still limited to retrieval augmentation"
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": "Pi et al., 2024, Shen et al., 2024, Xu et al., 2025]."
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": "small models [Lu et al., 2024] for edge devices, using techniques like quantization and distillation."
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": "processing [Tian et al., 2024]."
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": "energy costs."
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": ""
        },
        {
          "constrained edge devices is also crucial. Fine-tuning enhances personalization but can be resource-intensive and difficult": "to facilitate dynamic updates of user-specific information."
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 7: A systematic categorization of personalization strategies for PLLMs. Methods marked with ✫ use the",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Table 7: A systematic categorization of personalization strategies for PLLMs. Methods marked with ✫ use the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "LaMP benchmark [Salemi et al., 2023], ✫ use the LongLaMP benchmark [Kumar et al., 2024], and ✫ use the"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "PerLTQA benchmark [Du et al., 2024].\n(o) means optional. The overview presents four data categories (Historical"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Classification C, and Recommendation R), along with fine-tuning requirements for generator LLMs."
        }
      ],
      "page": 24
    },
    {
      "caption": "Table 7: A systematic categorization of personalization strategies for PLLMs. Methods marked with ✫ use the",
      "data": [
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Classification C, and Recommendation R), along with fine-tuning requirements for generator LLMs."
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Method"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Personalized Prompting\n§3"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Cue-CoT [Wang et al., 2023b]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PAG [Richardson et al., 2023] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Matryoshka [Li et al., 2024b] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "RewriterSlRl [Li et al., 2024c]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "R2P [Luo et al., 2025] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "DPL [Qiu et al., 2025a]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "CoS [He et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "StyleVector [Zhang et al., 2025b] ✫✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "MemPrompt [Madaan et al., 2022]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "TeachMe [Dalvi et al., 2022]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "MaLP [Zhang et al., 2024c]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "MemoRAG [Qian et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "IPA [Salemi et al., 2023] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "FiD [Salemi et al., 2023] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "MSP [Zhong et al., 2022]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "AuthorPred [Li et al., 2023]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PEARL [Mysore et al., 2023]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "ROPG [Salemi et al., 2024] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "HYDRA [Zhuang et al., 2024] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PRM [Zhuang et al., 2024] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PersonaAgent [Zhang et al., 2025a] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "UEM [Doddapaneni et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PERSOMA [Hebert et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "REGEN [Sayana et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PeaPOD [Ramos et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "ComMer [Zeldes et al., 2025] ✫✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PPlug [Liu et al., 2024b] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "User-LLM [Ning et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "RECAP [Liu et al., 2023]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "GSMN [Wu et al., 2021]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Personalized Adaptation\n§4"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PLoRA [Zhang et al., 2024d]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "UserIdentifier [Mireshghallah et al., 2021]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "LM-P [Wo´zniak et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Review-LLM [Peng et al., 2024a]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "MiLP [Zhang et al., 2024e]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "RecLoRA [Zhu et al., 2024a]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "iLoRA [Kong et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "UserAdapter [Zhong et al., 2021]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PocketLLM [Peng et al., 2024b]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "OPPU [Tan et al., 2024a] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "CoPe [Bu et al., 2025] ✫✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PER-PCS [Tan et al., 2024b] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PROPER [Zhang et al., 2025c] ✫"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "[Wagner et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "FDLoRA [Qi et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Personalized Alignment\n§5"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Wu et al. [2024a]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": ""
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PLUM [Magister et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Lee et al. [2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "MORLHF [Wu et al., 2023]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "MODPO [Zhou et al., 2023]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Personalized Soups [Jang et al., 2023]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "Reward Soups [Rame et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "MOD [Shi et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PAD [Chen et al., 2024a]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "PPT [Lau et al., 2024]"
        },
        {
          "Content, Dialogues, Interactions, User Profile, and Pre-defined Human Preference) and three task types (Generation G,": "VPL [Poddar et al., 2024]"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nConclusions": ""
        },
        {
          "8\nConclusions": ""
        },
        {
          "8\nConclusions": ""
        },
        {
          "8\nConclusions": "A detailed method summarization is shown in Table 7. We highlight current limitations and suggest future research"
        },
        {
          "8\nConclusions": "directions, providing valuable insights to advance PLLM development. Beyond taxonomy, we articulate a vision for"
        },
        {
          "8\nConclusions": "memory-centric PLLMs, highlighting three desired capabilities—remember, adapt, and evolve—and analyze their"
        },
        {
          "8\nConclusions": "inherent trade-offs among performance, privacy, and efficiency. We further examine domain-specific challenges and"
        },
        {
          "8\nConclusions": "discuss future directions. Together,"
        },
        {
          "8\nConclusions": "promising pathways for advancing PLLM research and practical deployment."
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "References"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zhang, Zican Dong, et al. A survey of large language models. arXiv:2303.18223, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yidong Wang, et al. A survey on evaluation of large language models. ACM transactions on intelligent systems and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "technology, 15(3):1–45, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Minda Hu, Licheng Zong, Hongru Wang, Jingyan Zhou, Jingjing Li, Yichen Gao, Kam-Fai Wong, Yu Li, and Irwin"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "King.\nSeRTS: Self-rewarding tree search for biomedical\nretrieval-augmented generation.\nIn Proc. of EMNLP"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Findings, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zeyu Zhang, Quanyu Dai, Luyu Chen, Zeren Jiang, Rui Li, Jieming Zhu, Xu Chen, Yi Xie, Zhenhua Dong, and Ji-Rong"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Wen. Memsim: A bayesian simulator for evaluating memory of llm-based personal assistants. arXiv:2409.20163,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2024a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. A"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "survey on the memory mechanism of large language model based agents. arXiv:2404.13501, 2024b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jiachen Zhu, Jianghao Lin, Xinyi Dai, Bo Chen, Rong Shan, Jieming Zhu, Ruiming Tang, Yong Yu, and Weinan Zhang."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Lifelong personalized low-rank adaptation of large language models for recommendation. arXiv:2408.03533, 2024a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hongru Wang, Minda Hu, Yang Deng, Rui Wang, Fei Mi, Weichao Wang, Yasheng Wang, Wai-Chung Kwan, Irwin"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "King, and Kam-Fai Wong. Large language models as source planner for personalized knowledge-grounded dialogue."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2310.08840, 2023a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hongru Wang, Huimin Wang, Lingzhi Wang, Minda Hu, Rui Wang, Boyang Xue, Yongfeng Huang, and Kam-Fai"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Wong. Tpe: Towards better compositional reasoning over cognitive tools via multi-persona collaboration.\nIn NLPCC,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "pages 281–294. Springer, 2024a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Wang, Yi Sun, et al. Personal llm agents: Insights and survey about the capability, efficiency and security. arXiv"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preprint arXiv:2401.05459, 2024a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yurui Zhao, Xiang Wang, Jiahong Liu, Irwin King, and Zhitao Huang. Towards geometry problem solving in the large"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "model era: A survey. arXiv preprint arXiv:2506.02690, 2025a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Qinglin Zhu Zhu, Runcong Zhao, Hanqi Yan, Yulan He, Yudong Chen, and Lin Gui. Soft reasoning: Navigating solution"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "spaces in large language models through controlled embedding exploration.\nIn 2025 International Conference on"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Machine Learning: ICML25. PMLR, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hongru Wang, Rui Wang, Fei Mi, Yang Deng, Zezhong Wang, Bin Liang, Ruifeng Xu, and Kam-Fai Wong. Cue-cot:"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Chain-of-thought prompting for responding to in-depth dialogue questions with llms.\nIn Proc. of EMNLP Findings,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2023b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Chris Richardson, Yao Zhang, Kellen Gillespie, Sudipta Kar, Arshdeep Singh, Zeynab Raeesy, Omar Zia Khan, and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Abhinav Sethy.\nIntegrating summarization and retrieval for enhanced personalization via large language models."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2310.20081, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. ONCE: boosting content-based recommendation with both"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "open- and closed-source large language models.\nIn Proc. of WSDM, 2024a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Changhao Li, Yuchen Zhuang, Rushi Qiang, Haotian Sun, Hanjun Dai, Chao Zhang, and Bo Dai. Matryoshka: Learning"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "to drive black-box llms with llms. arXiv:2410.20749, 2024b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yilun Qiu, Xiaoyan Zhao, Yang Zhang, Yimeng Bai, Wenjie Wang, Hong Cheng, Fuli Feng, and Tat-Seng Chua."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv\nMeasuring what makes you unique: Difference-aware user modeling for enhancing llm personalization."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preprint arXiv:2503.02450, 2025a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, and Michael Bendersky. Learning to rewrite prompts for"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalized text generation.\nIn Proc. of Web Conference, 2024c."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Sichun Luo, Guanzhi Deng, Jian Xu, Xiaojie Zhang, Hanxu Hou, and Linqi Song. Reasoning meets personalization:"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Unleashing the potential of large reasoning model for personalized generation. arXiv preprint arXiv:2505.17571,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Aman Madaan, Niket Tandon, Peter Clark, and Yiming Yang. Memory-assisted prompt editing to improve gpt-3 after"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "deployment.\nIn Proc. of EMNLP, 2022."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yi Zhang, Zhongyang Yu, Wanqi Jiang, Yufeng Shen, and Jin Li. Long-term memory for large language models through"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "topic-based vector database.\nIn Proc. of IALP, 2023."
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Kai Zhang, Yangyang Kang, Fubang Zhao, and Xiaozhong Liu. Llm-based medical assistant personalization with"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "short-and long-term memory coordination.\nIn Proc. of NAACL, 2024c."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Bhavana Dalvi, Oyvind Tafjord, and Peter Clark. Towards teachable reasoning systems: Using a dynamic memory of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "user feedback for continual system improvement.\nIn Proc. of EMNLP, 2022."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hongjin Qian, Peitian Zhang, Zheng Liu, Kelong Mao, and Zhicheng Dou. Memorag: Moving towards next-gen rag"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "via memory-inspired knowledge discovery. arXiv:2409.05591, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv preprint\nJaehyung Kim and Yiming Yang.\nFew-shot personalization of\nllms with mis-aligned responses."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2406.18678, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Alireza Salemi, Sheshera Mysore, Michael Bendersky, and Hamed Zamani. Lamp: When large language models meet"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalization. arXiv:2304.11406, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hanxun Zhong, Zhicheng Dou, Yutao Zhu, Hongjin Qian, and Ji-Rong Wen. Less is more: Learning to refine dialogue"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "history for personalized dialogue generation. arXiv:2204.08128, 2022."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Cheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing Wang, Spurthi Amba Hombaiah, Yi Liang, and Michael Bendersky."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Teach llms to personalize–an approach inspired by writing education. arXiv:2308.07968, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Sheshera Mysore, Zhuoran Lu, Mengting Wan, Longqi Yang, Steve Menezes, Tina Baghaee, Emmanuel Barajas"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Gonzalez, Jennifer Neville, and Tara Safavi. Pearl: Personalizing large language model writing assistants with"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "generation-calibrated retrievers. arXiv:2311.09180, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Alireza Salemi, Surya Kallumadi, and Hamed Zamani. Optimization methods for personalizing large language models"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "through retrieval augmentation.\nIn Proc. of SIGIR, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yuchen Zhuang, Haotian Sun, Yue Yu, Rushi Qiang, Qifan Wang, Chao Zhang, and Bo Dai. Hydra: Model factorization"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "framework for black-box llm personalization. arXiv:2406.02888, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Shuai Liu, Hyundong J Cho, Marjorie Freedman, Xuezhe Ma, and Jonathan May. Recap: Retrieval-enhanced context-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "aware prefix encoder for personalized dialogue response generation. arXiv:2306.07206, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Teng Shi, Jun Xu, Xiao Zhang, Xiaoxue Zang, Kai Zheng, Yang Song, and Han Li. Retrieval augmented generation"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "with collaborative filtering for personalized text generation. arXiv preprint arXiv:2504.05731, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Mert Yazan, Suzan Verberne, and Frederik Situmeang.\nImproving rag for personalization with author features and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "contrastive examples.\nIn European Conference on Information Retrieval, pages 408–416. Springer, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jieyong Kim, Tongyoung Kim, Soonjin Yoon, Jaehyung Kim, and Dongha Lee. Llms think, but not\nin your flow:"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Reasoning-level personalization for black-box large language models. arXiv preprint arXiv:2505.21082, 2025a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Weizhi Zhang, Xinyang Zhang, Chenwei Zhang, Liangwei Yang, Jingbo Shang, Zhepei Wei, Henry Peng Zou, Zijie"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Huang, Zhengyang Wang, Yifan Gao, et al. Personaagent: When large language model agents meet personalization"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "at test time. arXiv preprint arXiv:2506.06254, 2025a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Sumanth Doddapaneni, Krishna Sayana, Ambarish Jash, Sukhdeep Sodhi, and Dima Kuzmin. User embedding model"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "for personalized language prompting. arXiv:2401.04858, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Liam Hebert, Krishna Sayana, Ambarish Jash, Alexandros Karatzoglou, Sukhdeep S Sodhi, Sumanth Doddapaneni,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yanli Cai, and Dima Kuzmin. Persoma: Personalized soft prompt adapter architecture for personalized language"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "prompting. CoRR, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Krishna Sayana, Raghavendra Vasudeva, Yuri Vasilevski, Kun Su, Liam Hebert, Hubert Pham, Ambarish Jash, and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Sukhdeep Sodhi. Beyond retrieval: Generating narratives in conversational recommender systems. arXiv:2410.16780,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jerome Ramos, Bin Wu, and Aldo Lipani.\nPreference distillation for personalized generative recommendation."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2407.05033, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jiongnan Liu, Yutao Zhu, Shuting Wang, Xiaochi Wei, Erxue Min, Yu Lu, Shuaiqiang Wang, Dawei Yin, and Zhicheng"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Dou. Llms+ persona-plug= personalized llms. arXiv:2409.11901, 2024b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Lin Ning, Luyang Liu, Jiaxing Wu, Neo Wu, Devora Berlowitz, Sushant Prakash, Bradley Green, Shawn O’Banion,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "and Jun Xie. User-llm: Efficient llm contextualization with user embeddings. arXiv:2402.13598, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yoel Zeldes, Amir Zait, Ilia Labzovsky, Danny Karmon, and Efrat Farkash. Commer: a framework for compressing"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "and merging user data for personalization. arXiv preprint arXiv:2501.03276, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jerry Zhi-Yang He, Sashrika Pandey, Mariah L Schrum, and Anca Dragan. Cos: Enhancing personalization and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "mitigating bias with context steering. arXiv:2405.01768, 2024."
        }
      ],
      "page": 27
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jinghao Zhang, Yuting Liu, Wenjie Wang, Qiang Liu, Shu Wu, Liang Wang, and Tat-Seng Chua. Personalized text"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "generation with contrastive activation steering. arXiv preprint arXiv:2503.05213, 2025b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Christopher Clarke, Yuzhao Heng, Lingjia Tang, and Jason Mars. Peft-u: Parameter-efficient fine-tuning for user"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalization. arXiv:2407.18078, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "You Zhang,\nJin Wang, Liang-Chih Yu, Dan Xu, and Xuejie Zhang.\nPersonalized lora for human-centered text"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "understanding.\nIn Proc. of AAAI, 2024d."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Stanisław Wo´zniak, Bartłomiej Koptyra, Arkadiusz Janz, Przemysław Kazienko, and Jan Koco´n. Personalized large"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "language models. arXiv:2402.09269, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Qiyao Peng, Hongtao Liu, Hongyan Xu, Qing Yang, Minglai Shao, and Wenjun Wang. Llm: Harnessing large language"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "models for personalized review generation. arXiv:2407.07487, 2024a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Kai Zhang, Lizhi Qing, Yangyang Kang, and Xiaozhong Liu. Personalized llm response generation with parameterized"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "memory injection. arXiv:2404.03565, 2024e."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Xiaoyu Kong, Jiancan Wu, An Zhang, Leheng Sheng, Hui Lin, Xiang Wang, and Xiangnan He. Customizing language"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "models with instance-wise lora for sequential recommendation.\nIn Proc. of NeurIPS, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Wanjun Zhong, Duyu Tang, Jiahai Wang, Jian Yin, and Nan Duan. Useradapter: Few-shot user learning in sentiment"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "analysis.\nIn Proc. of ACL Findings, 2021."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Dan Peng,\nZhihui Fu,\nand\nJun Wang.\nPocketllm:\nEnabling\non-device fine-tuning\nfor\npersonalized\nllms."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2407.01031, 2024b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, and Meng Jiang. Democratizing large language"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "models via personalized parameter-efficient fine-tuning.\nIn Proc. of EMNLP, 2024a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zhaoxuan Tan, Zheyuan Liu, and Meng Jiang. Personalized pieces: Efficient personalized large language models"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "through collaborative efforts. arXiv:2406.10471, 2024b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Nicolas Wagner, Dongyang Fan, and Martin Jaggi. Personalized collaborative fine-tuning for on-device large language"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "models. arXiv:2404.09753, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jiaxing Qi, Zhongzhi Luan, Shaohan Huang, Carol Fung, Hailong Yang, and Depei Qian. Fdlora: Personalized federated"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "learning of large language model via dual lora tuning. arXiv:2406.07925, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Linhai Zhang, Jialong Wu, Deyu Zhou, and Yulan He. Proper: A progressive learning framework for personalized large"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "language models with group-level adaptation. arXiv preprint arXiv:2503.01303, 2025c."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Shujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, and Heng Ji. Aligning llms with individual"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preferences via interaction. arXiv:2410.03642, 2024a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Lucie Charlotte Magister, Katherine Metcalf, Yizhe Zhang, and Maartje ter Hoeve. On the way to llm personalization:"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Learning to remember user conversations. arXiv:2411.13405, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Seongyun Lee, Sue Hyun Park, Seungone Kim, and Minjoon Seo. Aligning to thousands of preferences via system"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "message generalization. Advances in Neural Information Processing Systems, 37:73783–73829, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, and Yiyu Shi. Enabling"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "on-device large language model personalization with self-supervised data selection and synthesis.\nIn Proc. of DAC,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hannah Rose Kirk, Alexander Whitefield, Paul Röttger, Andrew Bean, Katerina Margatina, Juan Ciro, Rafael Mosquera,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Max Bartolo, Adina Williams, He He, et al. The prism alignment project: What participatory, representative and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "individualised human feedback reveals about the subjective and multicultural alignment of large language models."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2404.16019, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Thomas P Zollo, Andrew Wei Tung Siah, Naimeng Ye, Ang Li, and Hongseok Namkoong. Personalllm: Tailoring llms"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "to individual preferences. arXiv:2409.20296, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane Suhr, Prithviraj Ammanabrolu, Noah A Smith, Mari Ostendorf,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "and Hannaneh Hajishirzi. Fine-grained human feedback gives better rewards for language model training. Proc. of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "NeurIPS, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zhanhui Zhou, Jie Liu, Chao Yang, Jing Shao, Yu Liu, Xiangyu Yue, Wanli Ouyang, and Yu Qiao. Beyond one-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preference-for-all: Multi-objective direct preference optimization. arXiv:2310.03708, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Alexandre Rame, Guillaume Couairon, Corentin Dancette, Jean-Baptiste Gaya, Mustafa Shukor, Laure Soulier, and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Matthieu Cord. Rewarded soups:\ntowards pareto-optimal alignment by interpolating weights fine-tuned on diverse"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "rewards. Proc. of NeurIPS, 2024."
        }
      ],
      "page": 28
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Choi, and Prithviraj Ammanabrolu. Personalized soups: Personalized large language model alignment via post-hoc"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "parameter merging. arXiv:2310.11564, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Ruizhe Shi, Yifang Chen, Yushi Hu, Alisa Liu, Hannaneh Hajishirzi, Noah A Smith, and Simon S Du. Decoding-time"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "language model alignment with multiple objectives. arXiv:2406.18853, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yuanpu Cao, Tianrong Zhang, Bochuan Cao, Ziyi Yin, Lu Lin, Fenglong Ma, and Jinghui Chen. Personalized steering"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "of large language models: Versatile steering vectors through bi-directional preference optimization. Advances in"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Neural Information Processing Systems, 37:49519–49551, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, and Zuozhu Liu. Pad: Personalized alignment of llms at"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "decoding-time. arXiv:2410.04070, 2024a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Ge Gao, Alexey Taymanov, Eduardo Salinas, Paul Mineiro, and Dipendra Misra. Aligning llm agents by learning latent"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preference from user edits.\nIn The Thirty-eighth Annual Conference on Neural Information Processing Systems."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, and Yaodong"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yang. Amulet: Realignment during test time for personalized preference adaptation of llms.\nIn Proc. of ICLR, 2025d."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Allison Lau, Younwoo Choi, Vahid Balazadeh, Keertana Chidambaram, Vasilis Syrgkanis, and Rahul G Krishnan."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Personalized adaptation via in-context preference learning. arXiv:2410.14001, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Sriyash Poddar, Yanming Wan, Hamish Ivison, Abhishek Gupta, and Natasha Jaques. Personalizing reinforcement"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "learning from human feedback with variational preference learning. arXiv:2408.10075, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yijing Zhang, Dyah Adila, Changho Shin, and Frederic Sala. Personalize your llm: Fake it then align it.\nIn Findings of"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "the Association for Computational Linguistics: NAACL 2025, pages 7287–7301, 2025e."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Alireza Salemi, Cheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong, Tao Chen, Zhuowan Li, Michael Bendersky,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "and Hamed Zamani. Reasoning-enhanced self-training for long-form personalized text generation. arXiv preprint"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2501.04167, 2025a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Minbeom Kim, Kang-il Lee, Seongho Joo, Hwaran Lee, and Kyomin Jung. Drift: Decoding-time personalized"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "alignments with implicit user preferences. arXiv preprint arXiv:2502.14289, 2025b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Weixiang Zhao, Xingyu Sui, Yulin Hu, Jiahe Guo, Haixiao Liu, Biye Li, Yanyan Zhao, Bing Qin, and Ting Liu."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Teaching language models to evolve with users: Dynamic profile modeling for personalized alignment. arXiv preprint"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2505.15456, 2025b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Stéphane Aroca-Ouellette, Natalie Mackraz, Barry-John Theobald, and Katherine Metcalf. Aligning llms by predicting"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preferences from user writing samples.\nIn Forty-second International Conference on Machine Learning, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hyungjune Bu, Chanjoo Jung, Minjae Kang, and Jaehyung Kim. Personalized llm decoding via contrasting personal"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preference. arXiv preprint arXiv:2506.12109, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Bin Wu, Zhengyan Shi, Hossein A Rahmani, Varsha Ramineni, and Emine Yilmaz. Understanding the role of user"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "profile in the personalization of large language models. arXiv:2406.17803, 2024b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Xiaoyan Zhao, Yang Deng, Wenjie Wang, Hong Cheng, Rui Zhang, See-Kiong Ng, Tat-Seng Chua, et al. Exploring"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "the impact of personality traits on conversational recommender systems: A simulation with large language models."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv preprint arXiv:2504.12313, 2025c."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Anvesh Rao Vijjini, Somnath Basu Roy Chowdhury, and Snigdha Chaturvedi. Exploring safety-utility trade-offs in"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalized language models. arXiv:2406.11107, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Alireza Salemi and Hamed Zamani. Comparing retrieval-augmentation and parameter-efficient fine-tuning for privacy-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preserving personalization of large language models. arXiv:2409.09510, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Ishita Kumar, Snigdha Viswanathan, Sushrita Yerra, Alireza Salemi, Ryan A Rossi, Franck Dernoncourt, Hanieh"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Deilamsalehy, Xiang Chen, Ruiyi Zhang, Shubham Agarwal, et al. Longlamp: A benchmark for personalized"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "long-form text generation. arXiv:2407.11016, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Steven Au, Cameron J Dimacali, Ojasmitha Pedirappagari, Namyong Park, Franck Dernoncourt, Yu Wang, Nikos"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Kanakaris, Hanieh Deilamsalehy, Ryan A Rossi, and Nesreen K Ahmed. Personalized graph-based retrieval for large"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "language models. arXiv:2501.02157, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yiming Du, Hongru Wang, Zhengyi Zhao, Bin Liang, Baojun Wang, Wanjun Zhong, Zezhong Wang, and Kam-Fai"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Wong. Perltqa: A personal long-term memory dataset for memory classification, retrieval, and synthesis in question"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "answering. arXiv:2402.16288, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, and Kaixiang Lin. Do llms recognize your preferences?"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "evaluating personalized preference following in llms. Proc. of ICLR, 2025d."
        }
      ],
      "page": 29
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, and Dong Yu. Longmemeval: Benchmarking chat"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "assistants on long-term interactive memory.\nIn Proc. of ICLR, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Qi Wang, Jindong Li, Shiqi Wang, Qianli Xing, Runliang Niu, He Kong, Rui Li, Guodong Long, Yi Chang, and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Chengqi Zhang. Towards next-generation llm-based recommender systems: A survey and beyond. arXiv preprint"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2410.19744, 2024b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv:2210.11416, 2022."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Xiao Xia, et al. Glm-130b: An open bilingual pre-trained model. arXiv:2210.02414, 2022."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Proc. of NeurIPS, 2020."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2204.02311, 2022."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2302.13971, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv:2501.12948,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Menglin Yang, Jialin Chen, Yifei Zhang, Jiahong Liu, Jiasheng Zhang, Qiyao Ma, Harshit Verma, Qianru Zhang, Min"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zhou, Irwin King, et al. Low-rank adaptation for foundation models: A comprehensive review. arXiv:2501.00365,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Qidong Liu, Xiangyu Zhao, Yuhao Wang, Yejing Wang, Zijian Zhang, Yuqi Sun, Xiang Li, Maolin Wang, Pengyue"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv preprint\nJia, Chong Chen, et al.\nLarge language model enhanced recommender\nsystems: A survey."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2412.13432, 2024c."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Yu-Ching Hsu, Jia-Yin Foo, Chao-Wei Huang, and Yun-Nung Chen."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Two tales of persona in llms: A survey of role-playing and personalization. arXiv:2406.01171, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jin Chen, Zheng Liu, Xu Huang, Chenwang Wu, Qi Liu, Gangwei Jiang, Yuanhao Pu, Yuxuan Lei, Xiaolong Chen,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Xingmei Wang, et al. When large language models meet personalization: Perspectives of challenges and opportunities."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "World Wide Web, 27(4):42, 2024b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zhehao Zhang, Ryan A Rossi, Branislav Kveton, Yijia Shao, Diyi Yang, Hamed Zamani, Franck Dernoncourt, Joe"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Barrow, Tong Yu, Sungchul Kim, et al. Personalization of large language models: A survey. arXiv:2411.00027,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2024f."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Qinglin Zhu, Runcong Zhao, Bin Liang, Jinhua Du, Lin Gui, and Yulan He. Player*: Enhancing llm-based multi-agent"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "communication and interaction in murder mystery games. arXiv e-prints, pages arXiv–2404, 2024b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Runcong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, and Lin Gui. Narrativeplay: An"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "automated system for crafting visual worlds in novels for role-playing.\nIn Proceedings of the AAAI Conference on"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Artificial Intelligence, volume 38, pages 23859–23861, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Retrieval-augmented generation for large language models: A survey. arXiv:2312.10997, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, and Qing Li. A survey"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "on rag meeting llms: Towards retrieval-augmented large language models.\nIn Proc. of KDD, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zexuan Qiu, Zijing Ou, Bin Wu, Jingjing Li, Aiwei Liu, and Irwin King.\nEntropy-based decoding for\nretrieval-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "augmented large language models. arXiv:2406.17519, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Haoran Tan, Zeyu Zhang, Chen Ma, Xu Chen, Quanyu Dai, and Zhenhua Dong. Membench: Towards more compre-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "hensive evaluation on the memory of llm-based agents. arXiv preprint arXiv:2506.21605, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Stephen E Robertson, Steve Walker, Susan Jones, Micheline M Hancock-Beaulieu, Mike Gatford, et al. Okapi at trec-3."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Nist Special Publication Sp, 1995."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "IEEE Transactions on Big\nJeff Johnson, Matthijs Douze, and Hervé Jégou. Billion-scale similarity search with gpus."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Data, 2019."
        }
      ],
      "page": 30
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, Armand Joulin, and Edouard"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Grave. Unsupervised dense information retrieval with contrastive learning. arXiv:2112.09118, 2021."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, and Tat-Seng Chua. Hello again!\nllm-powered"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalized agent for long-term dialogue. arXiv:2406.05925, 2024d."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Xinliang Frederick Zhang, Nick Beauchamp, and Lu Wang.\nPrime: Large language model personalization with"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "cognitive memory and thought processes. arXiv preprint arXiv:2507.04607, 2025f."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, and Fuli Feng. Latent inter-user difference modeling"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "for llm personalization. arXiv preprint arXiv:2507.20849, 2025b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yuwei Wu, Xuezhe Ma, and Diyi Yang. Personalized response generation via generative split memory network.\nIn"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Proc. of NAACL, 2021."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hang Lv, Sheng Liang, Hao Wang, Hongchao Gu, Yaxiong Wu, Wei Guo, Defu Lian, Yong Liu, and Enhong Chen."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Costeer: Collaborative decoding-time personalization via local delta steering. arXiv preprint arXiv:2507.04756,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Lora: Low-rank adaptation of large language models. arXiv:2106.09685, 2021."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv:2101.00190, 2021."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D Lee, Danqi Chen, and Sanjeev Arora. Fine-tuning"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "language models with just forward passes. Proc. of NeurIPS, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Fatemehsadat Mireshghallah, Vaishnavi Shrivastava, Milad Shokouhi, Taylor Berg-Kirkpatrick, Robert Sim, and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Dimitrios Dimitriadis. Useridentifier:\nimplicit user representations for simple and effective personalized sentiment"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "analysis. arXiv:2110.00135, 2021."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Han Zhou, Xingchen Wan, Ivan Vuli´c, and Anna Korhonen. Autopeft: Automatic configuration search for parameter-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "efficient fine-tuning. Proc. of ACL, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. A survey on mixture of experts."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2407.06204, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jiahong Liu, Wenhao Yu, Quanyu Dai, Zhongyang Li, Jieming Zhu, Menglin Yang, Tat-Seng Chua, and Irwin King."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Exploring personalization shifts in representation space of llms.\nIn Knowledgeable Foundation Models at ACL 2025,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with reinforcement learning from"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "human feedback. arXiv:2204.05862, 2022."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preference optimization: Your language model is secretly a reward model. Proc. of NuerIPS, 36, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Proc. of NeurIPS, 2022."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Idan Shenfeld, Felix Faltings, Pulkit Agrawal, and Aldo Pacchiano.\nLanguage model personalization via reward"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "factorization. arXiv preprint arXiv:2503.06358, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, and Yaodong"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv preprint\nYang.\nAmulet: Realignment during test\ntime for personalized preference adaptation of\nllms."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2502.19148, 2025g."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Davide Chicco and Giuseppe Jurman. The advantages of the matthews correlation coefficient (mcc) over f1 score and"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "accuracy in binary classification evaluation. BMC genomics, 21:1–13, 2020."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.\nIn Text summarization branches out, pages"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "74–81, 2004."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation of machine"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "translation.\nIn Proc. of ACL, pages 311–318, 2002."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "human judgments.\nIn Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "translation and/or summarization, pages 65–72, 2005."
        }
      ],
      "page": 31
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.\nIn Proceedings"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "of\nthe 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 2019."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Honghao Liu, et al. A survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Tianfeng Chai and Roland R Draxler. Root mean square error (rmse) or mean absolute error (mae)?–arguments against"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "avoiding rmse in the literature. Geoscientific model development, 7(3):1247–1250, 2014."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Timothy O Hodson. Root mean square error (rmse) or mean absolute error (mae): When to use them or not. Geoscientific"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Model Development Discussions, 2022:1–10, 2022."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Iulian Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. Building end-to-end dialogue"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "systems using generative hierarchical neural network models.\nIn Proceedings of the AAAI conference on artificial"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "intelligence, volume 30, 2016."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "with bert.\nIn International Conference on Learning Representations, 2020."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Rahul Vansh, Darsh Rank, Sourish Dasgupta, and Tanmoy Chakraborty. Accuracy is not enough: Evaluating per-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "sonalization in summarizers.\nIn Findings of the Association for Computational Linguistics: EMNLP 2023, pages"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2582–2595, 2023."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Sourish Dasgupta, Ankush Chander, Parth Borad,\nIsha Motiyani, and Tanmoy Chakraborty.\nPerseval: Assessing"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "personalization in text summarizers. arXiv preprint arXiv:2407.00453, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Asela Gunawardana and Guy Shani. A survey of accuracy evaluation metrics of recommendation tasks. JMLR, 10(12),"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2009."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yining Wang, Liwei Wang, Yuanzhi Li, Di He, and Tie-Yan Liu. A theoretical analysis of ndcg type ranking measures."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "In Conference on learning theory, pages 25–54. PMLR, 2013."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Li Li, Peilin Cai, Ryan A Rossi, Franck Dernoncourt, Branislav Kveton, Junda Wu, Tong Yu, Linxin Song, Tiankai"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Yang, Yuehan Qin, et al. A personalized conversational benchmark: Towards simulating personalized conversations."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv preprint arXiv:2505.14106, 2025a."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Alireza Salemi, Julian Killingback, and Hamed Zamani. Expert: Effective and explainable evaluation of personalized"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "long-form text generation. arXiv preprint arXiv:2501.14956, 2025b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Chen Huang, Peixin Qin, Yang Deng, Wenqiang Lei, Jiancheng Lv, and Tat-Seng Chua. Concept–an evaluation"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv preprint\nprotocol on conversational\nrecommender systems with system-centric and user-centric factors."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2404.03304, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Haochen Xue, Feilong Tang, Ming Hu, Yexin Liu, Qidong Huang, Yulong Li, Chengzhi Liu, Zhongxing Xu, Chong"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Zhang, Chun-Mei Feng, et al. Mmrc: A large-scale benchmark for understanding multimodal large language model"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "in real-world conversation. arXiv preprint arXiv:2502.11903, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. Evaluating"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753, 2024."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Xintong Li, Jalend Bantupalli, Ria Dharmani, Yuwei Zhang, and Jingbo Shang.\nToward multi-session personal-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv preprint\nized conversation: A large-scale dataset and hierarchical\ntree framework for implicit reasoning."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2503.07018, 2025b."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jisoo Mok, Ik-hwan Kim, Sangkwon Park, and Sungroh Yoon. Exploring the potential of llms as personalized assistants:"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Dataset, evaluation, and analysis. arXiv preprint arXiv:2506.01262, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Alireza Salemi and Hamed Zamani. Lamp-qa: A benchmark for personalized long-form question answering. arXiv"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "preprint arXiv:2506.00137, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Hongru Cai, Yongqi Li, Wenjie Wang, Fengbin Zhu, Xiaoyu Shen, Wenjie Li, and Tat-Seng Chua. Large language"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "models empowered personalized web agents.\nIn Proceedings of the ACM on Web Conference 2025, pages 198–215,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Jiani Huang, Shijie Wang, Liang-bo Ning, Wenqi Fan, Shuaiqiang Wang, Dawei Yin, and Qing Li. Towards next-"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "generation recommender systems: A benchmark for personalized recommendation assistant with llms. arXiv preprint"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "arXiv:2503.09382, 2025."
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Junda Wu, Hanjia Lyu, Yu Xia, Zhehao Zhang, Joe Barrow, Ishita Kumar, Mehrnoosh Mirtaheri, Hongjie Chen, Ryan A"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "Rossi, Franck Dernoncourt, et al. Personalized multimodal large language models: A survey. arXiv:2412.02142,"
        },
        {
          "A PREPRINT\nA Survey of Personalized Large Language Models": "2024c."
        }
      ],
      "page": 32
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Survey of Personalized Large Language Models": "Renjie Pi, Jianshu Zhang, Tianyang Han, Jipeng Zhang, Rui Pan, and Tong Zhang. Personalized visual instruction",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "tuning. arXiv:2410.07113, 2024.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Xiaoteng Shen, Rui Zhang, Xiaoyan Zhao, Jieming Zhu, and Xi Xiao. Pmg: Personalized multimodal generation with",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "large language models.\nIn Proceedings of the ACM on Web Conference 2024, 2024.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Yiyan Xu, Jinghao Zhang, Alireza Salemi, Xinting Hu, Wenjie Wang, Fuli Feng, Hamed Zamani, Xiangnan He, and",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Tat-Seng Chua. Personalized generation in large model era: A survey. arXiv preprint arXiv:2503.02614, 2025.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D Lane, and Mengwei Xu.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Small language models: Survey, measurements, and insights. arXiv:2409.15790, 2024.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Yuqing Tian, Zhaoyang Zhang, Yuzhi Yang, Zirui Chen, Zhaohui Yang, Richeng Jin, Tony QS Quek, and Kai-Kit Wong.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "An edge-cloud collaboration framework for generative ai service provision with synergetic big cloud model and",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "small edge models. arXiv:2401.01666, 2024.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang,",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Yangyang Shi, Raghuraman Krishnamoorthi, et al. Mobilellm: Optimizing sub-billion parameter language models",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "for on-device use cases.\nIn Forty-first International Conference on Machine Learning, 2024d.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Tongtong Wu, Linhao Luo, Yuan-Fang Li, Shirui Pan, Thuy-Trang Vu, and Gholamreza Haffari. Continual learning for",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "large language models: A survey. arXiv:2402.01364, 2024d.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Song Wang, Yaochen Zhu, Haochen Liu, Zaiyi Zheng, Chen Chen, and Jundong Li. Knowledge editing for large",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "language models: A survey. ACM Computing Surveys, 57(3):1–37, 2024c.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Zhang, Yuansheng Ni, et al. A comprehensive study of knowledge editing for large language models. arXiv preprint",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "arXiv:2401.01286, 2024g.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Yuhang Yao, Jianyi Zhang, Junda Wu, Chengkai Huang, Yu Xia, Tong Yu, Ruiyi Zhang, Sungchul Kim, Ryan Rossi,",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Ang Li, et al. Federated large language models: Current progress and future directions. arXiv:2409.15723, 2024.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Jiahong Liu, Xinyu Fu, Menglin Yang, Weixi Zhang, Rex Ying, and Irwin King. Client-specific hyperbolic federated",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "learning.\nIn FedKDD@KDD, 2024e.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Runcong Zhao, Chengyu Cao, Qinglin Zhu, Xiucheng Lv, Shun Shao, Lin Gui, Ruifeng Xu, and Yulan He. Sparse",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "activation editing for reliable instruction following in narratives. arXiv preprint arXiv:2505.16505, 2025e.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Runcong Zhao, Artem Borov, Jiazheng Li, and Yulan He. Learnlens: Llm-enabled personalised, curriculum-grounded",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "feedback with educators in the loop. arXiv preprint arXiv:2507.04295, 2025f.",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Zhiyu Li, Shichao Song, Chenyang Xi, Hanyu Wang, Chen Tang, Simin Niu, Ding Chen, Jiawei Yang, Chunyu Li,",
          "A PREPRINT": ""
        },
        {
          "A Survey of Personalized Large Language Models": "Qingchen Yu, et al. Memos: A memory os for ai system. arXiv preprint arXiv:2507.03724, 2025c.",
          "A PREPRINT": ""
        }
      ],
      "page": 33
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Notation Summary": "Table 8: Notation summary used throughout the paper. Symbols are grouped into Spaces, Functions, Models, Operations,"
        },
        {
          "Notation Summary": "and Variables. This table consolidates symbols from the survey."
        },
        {
          "Notation Summary": "Symbol"
        },
        {
          "Notation Summary": "• Spaces"
        },
        {
          "Notation Summary": "C"
        },
        {
          "Notation Summary": "Z"
        },
        {
          "Notation Summary": "H"
        },
        {
          "Notation Summary": "S = {dk}K\nk=1"
        },
        {
          "Notation Summary": "Q"
        },
        {
          "Notation Summary": "Y"
        },
        {
          "Notation Summary": "K"
        },
        {
          "Notation Summary": "U = {ui}N\ni=1"
        },
        {
          "Notation Summary": "• Functions / Mappings"
        },
        {
          "Notation Summary": "ϕ : C → Z"
        },
        {
          "Notation Summary": "ψ : (C × Q) → H"
        },
        {
          "Notation Summary": ": Q × C → Y\nfext"
        },
        {
          "Notation Summary": ": Q × Z → Y\nfabs"
        },
        {
          "Notation Summary": "pθ(y | q, h, K)"
        },
        {
          "Notation Summary": "• Models"
        },
        {
          "Notation Summary": "M0 : Q → Y"
        },
        {
          "Notation Summary": "Mi = P (M0, Ci; θ)"
        },
        {
          "Notation Summary": "P"
        },
        {
          "Notation Summary": "θ"
        },
        {
          "Notation Summary": "• Operations / Sets"
        },
        {
          "Notation Summary": "Σ(Ci)"
        },
        {
          "Notation Summary": "• Variables"
        },
        {
          "Notation Summary": "ui ∈ U"
        },
        {
          "Notation Summary": "Ci ⊂ C"
        },
        {
          "Notation Summary": "}ni\nQi = {q(i)\nj\nj=1"
        },
        {
          "Notation Summary": "q(i)\n∈ Q\nj"
        },
        {
          "Notation Summary": "y(i)\n∈ Y\nj"
        },
        {
          "Notation Summary": "y(i)\n∈ Y\nj"
        },
        {
          "Notation Summary": "(c(i)\n, r(i)\n) ∈ Ci\nj\nj"
        },
        {
          "Notation Summary": "r(i)\n∈ Y\nj"
        },
        {
          "Notation Summary": "zi = ϕ(Ci)"
        },
        {
          "Notation Summary": ")\nhi,j = ψ(Ci, q(i)\nj"
        }
      ],
      "page": 34
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A survey of large language models",
      "authors": [
        "Kun Wayne Xin Zhao",
        "Junyi Zhou",
        "Tianyi Li",
        "Xiaolei Tang",
        "Yupeng Wang",
        "Yingqian Hou",
        "Beichen Min",
        "Junjie Zhang",
        "Zican Zhang",
        "Dong"
      ],
      "year": "2023",
      "venue": "A survey of large language models",
      "arxiv": "arXiv:2303.18223"
    },
    {
      "citation_id": "2",
      "title": "A survey on evaluation of large language models",
      "authors": [
        "Yupeng Chang",
        "Xu Wang",
        "Jindong Wang",
        "Yuan Wu",
        "Linyi Yang",
        "Kaijie Zhu",
        "Hao Chen",
        "Xiaoyuan Yi",
        "Cunxiang Wang",
        "Yidong Wang"
      ],
      "year": "2024",
      "venue": "ACM transactions on intelligent systems and technology"
    },
    {
      "citation_id": "3",
      "title": "SeRTS: Self-rewarding tree search for biomedical retrieval-augmented generation",
      "authors": [
        "Minda Hu",
        "Licheng Zong",
        "Hongru Wang",
        "Jingyan Zhou",
        "Jingjing Li",
        "Yichen Gao",
        "Kam-Fai Wong",
        "Yu Li",
        "Irwin King"
      ],
      "year": "2024",
      "venue": "Proc. of EMNLP Findings"
    },
    {
      "citation_id": "4",
      "title": "Memsim: A bayesian simulator for evaluating memory of llm-based personal assistants",
      "authors": [
        "Zeyu Zhang",
        "Quanyu Dai",
        "Luyu Chen",
        "Zeren Jiang",
        "Rui Li",
        "Jieming Zhu",
        "Xu Chen",
        "Yi Xie",
        "Zhenhua Dong",
        "Ji-Rong Wen",
        "; Zhang",
        "Xiaohe Bo",
        "Chen Ma",
        "Rui Li",
        "Xu Chen",
        "Quanyu Dai",
        "Jieming Zhu",
        "Zhenhua Dong",
        "Ji-Rong Wen"
      ],
      "year": "2024",
      "venue": "A survey on the memory mechanism of large language model based agents",
      "arxiv": "arXiv:2409.20163"
    },
    {
      "citation_id": "5",
      "title": "Lifelong personalized low-rank adaptation of large language models for recommendation",
      "authors": [
        "Jiachen Zhu",
        "Jianghao Lin",
        "Xinyi Dai",
        "Bo Chen",
        "Rong Shan",
        "Jieming Zhu",
        "Ruiming Tang",
        "Yong Yu",
        "Weinan Zhang"
      ],
      "year": "2024",
      "venue": "Lifelong personalized low-rank adaptation of large language models for recommendation",
      "arxiv": "arXiv:2408.03533"
    },
    {
      "citation_id": "6",
      "title": "Large language models as source planner for personalized knowledge-grounded dialogue",
      "authors": [
        "Hongru Wang",
        "Minda Hu",
        "Yang Deng",
        "Rui Wang",
        "Fei Mi",
        "Weichao Wang",
        "Yasheng Wang",
        "Wai-Chung Kwan",
        "Irwin King",
        "Kam-Fai Wong"
      ],
      "year": "2023",
      "venue": "Large language models as source planner for personalized knowledge-grounded dialogue",
      "arxiv": "arXiv:2310.08840"
    },
    {
      "citation_id": "7",
      "title": "Tpe: Towards better compositional reasoning over cognitive tools via multi-persona collaboration",
      "authors": [
        "Hongru Wang",
        "Huimin Wang",
        "Lingzhi Wang",
        "Minda Hu",
        "Rui Wang",
        "Boyang Xue",
        "Yongfeng Huang",
        "Kam-Fai Wong"
      ],
      "year": "2024",
      "venue": "NLPCC"
    },
    {
      "citation_id": "8",
      "title": "Personal llm agents: Insights and survey about the capability, efficiency and security",
      "authors": [
        "Yuanchun Li",
        "Hao Wen",
        "Weijun Wang",
        "Xiangyu Li",
        "Yizhen Yuan",
        "Guohong Liu",
        "Jiacheng Liu",
        "Wenxing Xu",
        "Xiang Wang",
        "Yi Sun"
      ],
      "year": "2024",
      "venue": "Personal llm agents: Insights and survey about the capability, efficiency and security",
      "arxiv": "arXiv:2401.05459"
    },
    {
      "citation_id": "9",
      "title": "Towards geometry problem solving in the large model era: A survey",
      "authors": [
        "Yurui Zhao",
        "Xiang Wang",
        "Jiahong Liu",
        "Irwin King",
        "Zhitao Huang"
      ],
      "year": "2025",
      "venue": "Towards geometry problem solving in the large model era: A survey",
      "arxiv": "arXiv:2506.02690"
    },
    {
      "citation_id": "10",
      "title": "Soft reasoning: Navigating solution spaces in large language models through controlled embedding exploration",
      "authors": [
        "Qinglin Zhu Zhu",
        "Runcong Zhao",
        "Hanqi Yan",
        "Yulan He",
        "Yudong Chen",
        "Lin Gui"
      ],
      "year": "2025",
      "venue": "2025 International Conference on Machine Learning: ICML25"
    },
    {
      "citation_id": "11",
      "title": "Cue-cot: Chain-of-thought prompting for responding to in-depth dialogue questions with llms",
      "authors": [
        "Hongru Wang",
        "Rui Wang",
        "Fei Mi",
        "Yang Deng",
        "Zezhong Wang",
        "Bin Liang",
        "Ruifeng Xu",
        "Kam-Fai Wong"
      ],
      "year": "2023",
      "venue": "Proc. of EMNLP Findings"
    },
    {
      "citation_id": "12",
      "title": "Integrating summarization and retrieval for enhanced personalization via large language models",
      "authors": [
        "Chris Richardson",
        "Yao Zhang",
        "Kellen Gillespie",
        "Sudipta Kar",
        "Arshdeep Singh",
        "Zeynab Raeesy",
        "Omar Khan",
        "Abhinav Sethy"
      ],
      "year": "2023",
      "venue": "Integrating summarization and retrieval for enhanced personalization via large language models",
      "arxiv": "arXiv:2310.20081"
    },
    {
      "citation_id": "13",
      "title": "ONCE: boosting content-based recommendation with both open-and closed-source large language models",
      "authors": [
        "Qijiong Liu",
        "Nuo Chen",
        "Tetsuya Sakai",
        "Xiao-Ming Wu"
      ],
      "year": "2024",
      "venue": "Proc. of WSDM"
    },
    {
      "citation_id": "14",
      "title": "Learning to drive black-box llms with llms",
      "authors": [
        "Changhao Li",
        "Yuchen Zhuang",
        "Rushi Qiang",
        "Haotian Sun",
        "Hanjun Dai",
        "Chao Zhang",
        "Bo Dai",
        "Matryoshka"
      ],
      "year": "2024",
      "venue": "Learning to drive black-box llms with llms",
      "arxiv": "arXiv:2410.20749"
    },
    {
      "citation_id": "15",
      "title": "Measuring what makes you unique: Difference-aware user modeling for enhancing llm personalization",
      "authors": [
        "Yilun Qiu",
        "Xiaoyan Zhao",
        "Yang Zhang",
        "Yimeng Bai",
        "Wenjie Wang",
        "Hong Cheng",
        "Fuli Feng",
        "Tat-Seng Chua"
      ],
      "year": "2025",
      "venue": "Measuring what makes you unique: Difference-aware user modeling for enhancing llm personalization",
      "arxiv": "arXiv:2503.02450"
    },
    {
      "citation_id": "16",
      "title": "Learning to rewrite prompts for personalized text generation",
      "authors": [
        "Cheng Li",
        "Mingyang Zhang",
        "Qiaozhu Mei",
        "Weize Kong",
        "Michael Bendersky"
      ],
      "year": "2024",
      "venue": "Proc. of Web Conference"
    },
    {
      "citation_id": "17",
      "title": "Reasoning meets personalization: Unleashing the potential of large reasoning model for personalized generation",
      "authors": [
        "Sichun Luo",
        "Guanzhi Deng",
        "Jian Xu",
        "Xiaojie Zhang",
        "Hanxu Hou",
        "Linqi Song"
      ],
      "year": "2025",
      "venue": "Reasoning meets personalization: Unleashing the potential of large reasoning model for personalized generation",
      "arxiv": "arXiv:2505.17571"
    },
    {
      "citation_id": "18",
      "title": "Memory-assisted prompt editing to improve gpt-3 after deployment",
      "authors": [
        "Aman Madaan",
        "Niket Tandon",
        "Peter Clark",
        "Yiming Yang"
      ],
      "year": "2022",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "19",
      "title": "Long-term memory for large language models through topic-based vector database",
      "authors": [
        "Yi Zhang",
        "Zhongyang Yu",
        "Wanqi Jiang",
        "Yufeng Shen",
        "Jin Li"
      ],
      "year": "2023",
      "venue": "Proc. of IALP"
    },
    {
      "citation_id": "20",
      "title": "Llm-based medical assistant personalization with short-and long-term memory coordination",
      "authors": [
        "Kai Zhang",
        "Yangyang Kang",
        "Fubang Zhao",
        "Xiaozhong Liu"
      ],
      "year": "2024",
      "venue": "Proc. of NAACL"
    },
    {
      "citation_id": "21",
      "title": "Towards teachable reasoning systems: Using a dynamic memory of user feedback for continual system improvement",
      "authors": [
        "Bhavana Dalvi",
        "Oyvind Tafjord",
        "Peter Clark"
      ],
      "year": "2022",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "22",
      "title": "Moving towards next-gen rag via memory-inspired knowledge discovery",
      "authors": [
        "Hongjin Qian",
        "Peitian Zhang",
        "Zheng Liu",
        "Kelong Mao",
        "Zhicheng Dou",
        "Memorag"
      ],
      "year": "2024",
      "venue": "Moving towards next-gen rag via memory-inspired knowledge discovery",
      "arxiv": "arXiv:2409.05591"
    },
    {
      "citation_id": "23",
      "title": "Few-shot personalization of llms with mis-aligned responses",
      "authors": [
        "Jaehyung Kim",
        "Yiming Yang"
      ],
      "year": "2024",
      "venue": "Few-shot personalization of llms with mis-aligned responses",
      "arxiv": "arXiv:2406.18678"
    },
    {
      "citation_id": "24",
      "title": "Lamp: When large language models meet personalization",
      "authors": [
        "Alireza Salemi",
        "Sheshera Mysore",
        "Michael Bendersky",
        "Hamed Zamani"
      ],
      "year": "2023",
      "venue": "Lamp: When large language models meet personalization",
      "arxiv": "arXiv:2304.11406"
    },
    {
      "citation_id": "25",
      "title": "Less is more: Learning to refine dialogue history for personalized dialogue generation",
      "authors": [
        "Hanxun Zhong",
        "Zhicheng Dou",
        "Yutao Zhu",
        "Hongjin Qian",
        "Ji-Rong Wen"
      ],
      "year": "2022",
      "venue": "Less is more: Learning to refine dialogue history for personalized dialogue generation",
      "arxiv": "arXiv:2204.08128"
    },
    {
      "citation_id": "26",
      "title": "Teach llms to personalize-an approach inspired by writing education",
      "authors": [
        "Cheng Li",
        "Mingyang Zhang",
        "Qiaozhu Mei",
        "Yaqing Wang",
        "Amba Spurthi",
        "Yi Hombaiah",
        "Michael Liang",
        "Bendersky"
      ],
      "year": "2023",
      "venue": "Teach llms to personalize-an approach inspired by writing education",
      "arxiv": "arXiv:2308.07968"
    },
    {
      "citation_id": "27",
      "title": "Personalizing large language model writing assistants with generation-calibrated retrievers",
      "authors": [
        "Sheshera Mysore",
        "Zhuoran Lu",
        "Mengting Wan",
        "Longqi Yang",
        "Steve Menezes",
        "Tina Baghaee",
        "Emmanuel Gonzalez",
        "Jennifer Neville",
        "Tara Safavi",
        "Pearl"
      ],
      "year": "2023",
      "venue": "Personalizing large language model writing assistants with generation-calibrated retrievers",
      "arxiv": "arXiv:2311.09180"
    },
    {
      "citation_id": "28",
      "title": "Optimization methods for personalizing large language models through retrieval augmentation",
      "authors": [
        "Alireza Salemi",
        "Surya Kallumadi",
        "Hamed Zamani"
      ],
      "year": "2024",
      "venue": "Proc. of SIGIR"
    },
    {
      "citation_id": "29",
      "title": "Hydra: Model factorization framework for black-box llm personalization",
      "authors": [
        "Yuchen Zhuang",
        "Haotian Sun",
        "Yue Yu",
        "Rushi Qiang",
        "Qifan Wang",
        "Chao Zhang",
        "Bo Dai"
      ],
      "year": "2024",
      "venue": "Hydra: Model factorization framework for black-box llm personalization",
      "arxiv": "arXiv:2406.02888"
    },
    {
      "citation_id": "30",
      "title": "Recap: Retrieval-enhanced contextaware prefix encoder for personalized dialogue response generation",
      "authors": [
        "Shuai Liu",
        "J Hyundong",
        "Marjorie Cho",
        "Xuezhe Freedman",
        "Jonathan Ma"
      ],
      "year": "2023",
      "venue": "Recap: Retrieval-enhanced contextaware prefix encoder for personalized dialogue response generation",
      "arxiv": "arXiv:2306.07206"
    },
    {
      "citation_id": "31",
      "title": "Retrieval augmented generation with collaborative filtering for personalized text generation",
      "authors": [
        "Teng Shi",
        "Jun Xu",
        "Xiao Zhang",
        "Xiaoxue Zang",
        "Kai Zheng",
        "Yang Song",
        "Han Li"
      ],
      "year": "2025",
      "venue": "Retrieval augmented generation with collaborative filtering for personalized text generation",
      "arxiv": "arXiv:2504.05731"
    },
    {
      "citation_id": "32",
      "title": "Improving rag for personalization with author features and contrastive examples",
      "authors": [
        "Mert Yazan",
        "Suzan Verberne",
        "Frederik Situmeang"
      ],
      "year": "2025",
      "venue": "European Conference on Information Retrieval"
    },
    {
      "citation_id": "33",
      "title": "Llms think, but not in your flow: Reasoning-level personalization for black-box large language models",
      "authors": [
        "Jieyong Kim",
        "Tongyoung Kim",
        "Soonjin Yoon",
        "Jaehyung Kim",
        "Dongha Lee"
      ],
      "year": "2025",
      "venue": "Llms think, but not in your flow: Reasoning-level personalization for black-box large language models",
      "arxiv": "arXiv:2505.21082"
    },
    {
      "citation_id": "34",
      "title": "When large language model agents meet personalization at test time",
      "authors": [
        "Weizhi Zhang",
        "Xinyang Zhang",
        "Chenwei Zhang",
        "Liangwei Yang",
        "Jingbo Shang",
        "Zhepei Wei",
        "Henry Zou",
        "Zijie Huang",
        "Zhengyang Wang",
        "Yifan Gao"
      ],
      "year": "2025",
      "venue": "When large language model agents meet personalization at test time",
      "arxiv": "arXiv:2506.06254"
    },
    {
      "citation_id": "35",
      "title": "User embedding model for personalized language prompting",
      "authors": [
        "Sumanth Doddapaneni",
        "Krishna Sayana",
        "Ambarish Jash",
        "Sukhdeep Sodhi",
        "Dima Kuzmin"
      ],
      "year": "2024",
      "venue": "User embedding model for personalized language prompting",
      "arxiv": "arXiv:2401.04858"
    },
    {
      "citation_id": "36",
      "title": "Persoma: Personalized soft prompt adapter architecture for personalized language prompting",
      "authors": [
        "Liam Hebert",
        "Krishna Sayana",
        "Ambarish Jash",
        "Alexandros Karatzoglou",
        "S Sukhdeep",
        "Sumanth Sodhi",
        "Yanli Doddapaneni",
        "Dima Cai",
        "Kuzmin"
      ],
      "year": "2024",
      "venue": "Persoma: Personalized soft prompt adapter architecture for personalized language prompting"
    },
    {
      "citation_id": "37",
      "title": "Beyond retrieval: Generating narratives in conversational recommender systems",
      "authors": [
        "Krishna Sayana",
        "Raghavendra Vasudeva",
        "Yuri Vasilevski",
        "Kun Su",
        "Liam Hebert",
        "Hubert Pham",
        "Ambarish Jash",
        "Sukhdeep Sodhi"
      ],
      "year": "2024",
      "venue": "Beyond retrieval: Generating narratives in conversational recommender systems",
      "arxiv": "arXiv:2410.16780"
    },
    {
      "citation_id": "38",
      "title": "Preference distillation for personalized generative recommendation",
      "authors": [
        "Jerome Ramos",
        "Bin Wu",
        "Aldo Lipani"
      ],
      "year": "2024",
      "venue": "Preference distillation for personalized generative recommendation",
      "arxiv": "arXiv:2407.05033"
    },
    {
      "citation_id": "39",
      "title": "Llms+ persona-plug= personalized llms",
      "authors": [
        "Jiongnan Liu",
        "Yutao Zhu",
        "Shuting Wang",
        "Xiaochi Wei",
        "Erxue Min",
        "Yu Lu",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Zhicheng Dou"
      ],
      "year": "2024",
      "venue": "Llms+ persona-plug= personalized llms",
      "arxiv": "arXiv:2409.11901"
    },
    {
      "citation_id": "40",
      "title": "User-llm: Efficient llm contextualization with user embeddings",
      "authors": [
        "Lin Ning",
        "Luyang Liu",
        "Jiaxing Wu",
        "Neo Wu",
        "Devora Berlowitz",
        "Sushant Prakash",
        "Bradley Green",
        "Shawn 'banion",
        "Jun Xie"
      ],
      "year": "2024",
      "venue": "User-llm: Efficient llm contextualization with user embeddings",
      "arxiv": "arXiv:2402.13598"
    },
    {
      "citation_id": "41",
      "title": "Commer: a framework for compressing and merging user data for personalization",
      "authors": [
        "Yoel Zeldes",
        "Amir Zait",
        "Ilia Labzovsky",
        "Danny Karmon",
        "Efrat Farkash"
      ],
      "year": "2025",
      "venue": "Commer: a framework for compressing and merging user data for personalization",
      "arxiv": "arXiv:2501.03276"
    },
    {
      "citation_id": "42",
      "title": "Cos: Enhancing personalization and mitigating bias with context steering",
      "authors": [
        "Jerry Zhi-Yang He",
        "Sashrika Pandey",
        "Mariah Schrum",
        "Anca Dragan"
      ],
      "year": "2024",
      "venue": "Cos: Enhancing personalization and mitigating bias with context steering",
      "arxiv": "arXiv:2405.01768"
    },
    {
      "citation_id": "43",
      "title": "Personalized text generation with contrastive activation steering",
      "authors": [
        "Jinghao Zhang",
        "Yuting Liu",
        "Wenjie Wang",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang",
        "Tat-Seng Chua"
      ],
      "year": "2025",
      "venue": "Personalized text generation with contrastive activation steering",
      "arxiv": "arXiv:2503.05213"
    },
    {
      "citation_id": "44",
      "title": "Peft-u: Parameter-efficient fine-tuning for user personalization",
      "authors": [
        "Christopher Clarke",
        "Yuzhao Heng",
        "Lingjia Tang",
        "Jason Mars"
      ],
      "year": "2024",
      "venue": "Peft-u: Parameter-efficient fine-tuning for user personalization",
      "arxiv": "arXiv:2407.18078"
    },
    {
      "citation_id": "45",
      "title": "Personalized lora for human-centered text understanding",
      "authors": [
        "You Zhang",
        "Jin Wang",
        "Liang-Chih Yu",
        "Dan Xu",
        "Xuejie Zhang"
      ],
      "year": "2024",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "46",
      "title": "",
      "authors": [
        "Stanisław Woźniak",
        "Bartłomiej Koptyra",
        "Arkadiusz Janz",
        "Przemysław Kazienko",
        "Jan Kocoń"
      ],
      "year": "2024",
      "venue": "",
      "arxiv": "arXiv:2402.09269"
    },
    {
      "citation_id": "47",
      "title": "Llm: Harnessing large language models for personalized review generation",
      "authors": [
        "Qiyao Peng",
        "Hongtao Liu",
        "Hongyan Xu",
        "Qing Yang",
        "Minglai Shao",
        "Wenjun Wang"
      ],
      "year": "2024",
      "venue": "Llm: Harnessing large language models for personalized review generation",
      "arxiv": "arXiv:2407.07487"
    },
    {
      "citation_id": "48",
      "title": "Personalized llm response generation with parameterized memory injection",
      "authors": [
        "Kai Zhang",
        "Lizhi Qing",
        "Yangyang Kang",
        "Xiaozhong Liu"
      ],
      "year": "2024",
      "venue": "Personalized llm response generation with parameterized memory injection",
      "arxiv": "arXiv:2404.03565"
    },
    {
      "citation_id": "49",
      "title": "Customizing language models with instance-wise lora for sequential recommendation",
      "authors": [
        "Xiaoyu Kong",
        "Jiancan Wu",
        "An Zhang",
        "Leheng Sheng",
        "Hui Lin",
        "Xiang Wang",
        "Xiangnan He"
      ],
      "year": "2024",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "50",
      "title": "Useradapter: Few-shot user learning in sentiment analysis",
      "authors": [
        "Wanjun Zhong",
        "Duyu Tang",
        "Jiahai Wang",
        "Jian Yin",
        "Nan Duan"
      ],
      "year": "2021",
      "venue": "Proc. of ACL Findings"
    },
    {
      "citation_id": "51",
      "title": "Pocketllm: Enabling on-device fine-tuning for personalized llms",
      "authors": [
        "Dan Peng",
        "Zhihui Fu",
        "Jun Wang"
      ],
      "year": "2024",
      "venue": "Pocketllm: Enabling on-device fine-tuning for personalized llms",
      "arxiv": "arXiv:2407.01031"
    },
    {
      "citation_id": "52",
      "title": "Democratizing large language models via personalized parameter-efficient fine-tuning",
      "authors": [
        "Zhaoxuan Tan",
        "Qingkai Zeng",
        "Yijun Tian",
        "Zheyuan Liu",
        "Bing Yin",
        "Meng Jiang"
      ],
      "year": "2024",
      "venue": "Proc. of EMNLP"
    },
    {
      "citation_id": "53",
      "title": "Personalized pieces: Efficient personalized large language models through collaborative efforts",
      "authors": [
        "Zhaoxuan Tan",
        "Zheyuan Liu",
        "Meng Jiang"
      ],
      "year": "2024",
      "venue": "Personalized pieces: Efficient personalized large language models through collaborative efforts",
      "arxiv": "arXiv:2406.10471"
    },
    {
      "citation_id": "54",
      "title": "Personalized collaborative fine-tuning for on-device large language models",
      "authors": [
        "Nicolas Wagner",
        "Dongyang Fan",
        "Martin Jaggi"
      ],
      "year": "2024",
      "venue": "Personalized collaborative fine-tuning for on-device large language models",
      "arxiv": "arXiv:2404.09753"
    },
    {
      "citation_id": "55",
      "title": "Fdlora: Personalized federated learning of large language model via dual lora tuning",
      "authors": [
        "Jiaxing Qi",
        "Zhongzhi Luan",
        "Shaohan Huang",
        "Carol Fung",
        "Hailong Yang",
        "Depei Qian"
      ],
      "year": "2024",
      "venue": "Fdlora: Personalized federated learning of large language model via dual lora tuning",
      "arxiv": "arXiv:2406.07925"
    },
    {
      "citation_id": "56",
      "title": "Proper: A progressive learning framework for personalized large language models with group-level adaptation",
      "authors": [
        "Linhai Zhang",
        "Jialong Wu",
        "Deyu Zhou",
        "Yulan He"
      ],
      "year": "2025",
      "venue": "Proper: A progressive learning framework for personalized large language models with group-level adaptation",
      "arxiv": "arXiv:2503.01303"
    },
    {
      "citation_id": "57",
      "title": "Aligning llms with individual preferences via interaction",
      "authors": [
        "Shujin Wu",
        "May Fung",
        "Cheng Qian",
        "Jeonghwan Kim",
        "Dilek Hakkani-Tur",
        "Heng Ji"
      ],
      "year": "2024",
      "venue": "Aligning llms with individual preferences via interaction",
      "arxiv": "arXiv:2410.03642"
    },
    {
      "citation_id": "58",
      "title": "On the way to llm personalization: Learning to remember user conversations",
      "authors": [
        "Charlotte Lucie",
        "Katherine Magister",
        "Yizhe Metcalf",
        "Maartje Zhang",
        "Ter Hoeve"
      ],
      "year": "2024",
      "venue": "On the way to llm personalization: Learning to remember user conversations",
      "arxiv": "arXiv:2411.13405"
    },
    {
      "citation_id": "59",
      "title": "Aligning to thousands of preferences via system message generalization",
      "authors": [
        "Seongyun Lee",
        "Hyun Park",
        "Seungone Kim",
        "Minjoon Seo"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "60",
      "title": "Enabling on-device large language model personalization with self-supervised data selection and synthesis",
      "authors": [
        "Ruiyang Qin",
        "Jun Xia",
        "Zhenge Jia",
        "Meng Jiang",
        "Ahmed Abbasi",
        "Peipei Zhou",
        "Jingtong Hu",
        "Yiyu Shi"
      ],
      "year": "2024",
      "venue": "Proc. of DAC"
    },
    {
      "citation_id": "61",
      "title": "The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models",
      "authors": [
        "Rose Hannah",
        "Alexander Kirk",
        "Paul Whitefield",
        "Andrew Röttger",
        "Katerina Bean",
        "Juan Margatina",
        "Rafael Ciro",
        "Max Mosquera",
        "Adina Bartolo",
        "He Williams",
        "He"
      ],
      "year": "2024",
      "venue": "The prism alignment project: What participatory, representative and individualised human feedback reveals about the subjective and multicultural alignment of large language models",
      "arxiv": "arXiv:2404.16019"
    },
    {
      "citation_id": "62",
      "title": "Tailoring llms to individual preferences",
      "authors": [
        "Andrew Thomas P Zollo",
        "Tung Wei",
        "Naimeng Siah",
        "Ang Ye",
        "Hongseok Li",
        "Namkoong",
        "Personalllm"
      ],
      "year": "2024",
      "venue": "Tailoring llms to individual preferences",
      "arxiv": "arXiv:2409.20296"
    },
    {
      "citation_id": "63",
      "title": "Fine-grained human feedback gives better rewards for language model training",
      "authors": [
        "Zeqiu Wu",
        "Yushi Hu",
        "Weijia Shi",
        "Nouha Dziri",
        "Alane Suhr",
        "Prithviraj Ammanabrolu",
        "Noah Smith",
        "Mari Ostendorf",
        "Hannaneh Hajishirzi"
      ],
      "year": "2023",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "64",
      "title": "Beyond onepreference-for-all: Multi-objective direct preference optimization",
      "authors": [
        "Zhanhui Zhou",
        "Jie Liu",
        "Chao Yang",
        "Jing Shao",
        "Yu Liu",
        "Xiangyu Yue",
        "Wanli Ouyang",
        "Yu Qiao"
      ],
      "year": "2023",
      "venue": "Beyond onepreference-for-all: Multi-objective direct preference optimization",
      "arxiv": "arXiv:2310.03708"
    },
    {
      "citation_id": "65",
      "title": "Rewarded soups: towards pareto-optimal alignment by interpolating weights fine-tuned on diverse rewards",
      "authors": [
        "Alexandre Rame",
        "Guillaume Couairon",
        "Corentin Dancette",
        "Jean-Baptiste Gaya",
        "Mustafa Shukor",
        "Laure Soulier",
        "Matthieu Cord"
      ],
      "year": "2024",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "66",
      "title": "Personalized soups: Personalized large language model alignment via post-hoc parameter merging",
      "authors": [
        "Joel Jang",
        "Seungone Kim",
        "Bill Yuchen Lin",
        "Yizhong Wang",
        "Jack Hessel",
        "Luke Zettlemoyer",
        "Hannaneh Hajishirzi",
        "Yejin Choi",
        "Prithviraj Ammanabrolu"
      ],
      "year": "2023",
      "venue": "Personalized soups: Personalized large language model alignment via post-hoc parameter merging",
      "arxiv": "arXiv:2310.11564"
    },
    {
      "citation_id": "67",
      "title": "Decoding-time language model alignment with multiple objectives",
      "authors": [
        "Ruizhe Shi",
        "Yifang Chen",
        "Yushi Hu",
        "Alisa Liu",
        "Hannaneh Hajishirzi",
        "Noah Smith",
        "Simon Du"
      ],
      "year": "2024",
      "venue": "Decoding-time language model alignment with multiple objectives",
      "arxiv": "arXiv:2406.18853"
    },
    {
      "citation_id": "68",
      "title": "Personalized steering of large language models: Versatile steering vectors through bi-directional preference optimization",
      "authors": [
        "Yuanpu Cao",
        "Tianrong Zhang",
        "Bochuan Cao",
        "Ziyi Yin",
        "Lu Lin",
        "Fenglong Ma",
        "Jinghui Chen"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "69",
      "title": "Pad: Personalized alignment of llms at decoding-time",
      "authors": [
        "Ruizhe Chen",
        "Xiaotian Zhang",
        "Meng Luo",
        "Wenhao Chai",
        "Zuozhu Liu"
      ],
      "year": "2024",
      "venue": "Pad: Personalized alignment of llms at decoding-time",
      "arxiv": "arXiv:2410.04070"
    },
    {
      "citation_id": "70",
      "title": "Aligning llm agents by learning latent preference from user edits",
      "authors": [
        "Ge Gao",
        "Alexey Taymanov",
        "Eduardo Salinas",
        "Paul Mineiro",
        "Dipendra Misra"
      ],
      "venue": "The Thirty-eighth Annual Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "71",
      "title": "Amulet: Realignment during test time for personalized preference adaptation of llms",
      "authors": [
        "Zhaowei Zhang",
        "Fengshuo Bai",
        "Qizhi Chen",
        "Chengdong Ma",
        "Mingzhi Wang",
        "Haoran Sun",
        "Zilong Zheng",
        "Yaodong Yang"
      ],
      "year": "2025",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "72",
      "title": "Personalized adaptation via in-context preference learning",
      "authors": [
        "Allison Lau",
        "Younwoo Choi",
        "Vahid Balazadeh",
        "Keertana Chidambaram",
        "Vasilis Syrgkanis",
        "Rahul G Krishnan"
      ],
      "year": "2024",
      "venue": "Personalized adaptation via in-context preference learning",
      "arxiv": "arXiv:2410.14001"
    },
    {
      "citation_id": "73",
      "title": "Personalizing reinforcement learning from human feedback with variational preference learning",
      "authors": [
        "Sriyash Poddar",
        "Yanming Wan",
        "Hamish Ivison",
        "Abhishek Gupta",
        "Natasha Jaques"
      ],
      "year": "2024",
      "venue": "Personalizing reinforcement learning from human feedback with variational preference learning",
      "arxiv": "arXiv:2408.10075"
    },
    {
      "citation_id": "74",
      "title": "Personalize your llm: Fake it then align it",
      "authors": [
        "Yijing Zhang",
        "Dyah Adila",
        "Changho Shin",
        "Frederic Sala"
      ],
      "year": "2025",
      "venue": "Findings of the Association for Computational Linguistics: NAACL 2025"
    },
    {
      "citation_id": "75",
      "title": "Reasoning-enhanced self-training for long-form personalized text generation",
      "authors": [
        "Alireza Salemi",
        "Cheng Li",
        "Mingyang Zhang",
        "Qiaozhu Mei",
        "Weize Kong",
        "Tao Chen",
        "Zhuowan Li",
        "Michael Bendersky",
        "Hamed Zamani"
      ],
      "year": "2025",
      "venue": "Reasoning-enhanced self-training for long-form personalized text generation",
      "arxiv": "arXiv:2501.04167"
    },
    {
      "citation_id": "76",
      "title": "Drift: Decoding-time personalized alignments with implicit user preferences",
      "authors": [
        "Minbeom Kim",
        "Kang-Il Lee",
        "Seongho Joo",
        "Hwaran Lee",
        "Kyomin Jung"
      ],
      "year": "2025",
      "venue": "Drift: Decoding-time personalized alignments with implicit user preferences",
      "arxiv": "arXiv:2502.14289"
    },
    {
      "citation_id": "77",
      "title": "Teaching language models to evolve with users: Dynamic profile modeling for personalized alignment",
      "authors": [
        "Weixiang Zhao",
        "Xingyu Sui",
        "Yulin Hu",
        "Jiahe Guo",
        "Haixiao Liu",
        "Biye Li",
        "Yanyan Zhao",
        "Bing Qin",
        "Ting Liu"
      ],
      "year": "2025",
      "venue": "Teaching language models to evolve with users: Dynamic profile modeling for personalized alignment",
      "arxiv": "arXiv:2505.15456"
    },
    {
      "citation_id": "78",
      "title": "Aligning llms by predicting preferences from user writing samples",
      "authors": [
        "Stéphane Aroca-Ouellette",
        "Natalie Mackraz",
        "Barry-John Theobald",
        "Katherine Metcalf"
      ],
      "year": "2025",
      "venue": "Forty-second International Conference on Machine Learning"
    },
    {
      "citation_id": "79",
      "title": "Personalized llm decoding via contrasting personal preference",
      "authors": [
        "Hyungjune Bu",
        "Chanjoo Jung",
        "Minjae Kang",
        "Jaehyung Kim"
      ],
      "year": "2025",
      "venue": "Personalized llm decoding via contrasting personal preference",
      "arxiv": "arXiv:2506.12109"
    },
    {
      "citation_id": "80",
      "title": "Understanding the role of user profile in the personalization of large language models",
      "authors": [
        "Bin Wu",
        "Zhengyan Shi",
        "Hossein Rahmani",
        "Varsha Ramineni",
        "Emine Yilmaz"
      ],
      "year": "2024",
      "venue": "Understanding the role of user profile in the personalization of large language models",
      "arxiv": "arXiv:2406.17803"
    },
    {
      "citation_id": "81",
      "title": "Exploring the impact of personality traits on conversational recommender systems: A simulation with large language models",
      "authors": [
        "Xiaoyan Zhao",
        "Yang Deng",
        "Wenjie Wang",
        "Hong Cheng",
        "Rui Zhang",
        "See-Kiong Ng",
        "Tat-Seng Chua"
      ],
      "year": "2025",
      "venue": "Exploring the impact of personality traits on conversational recommender systems: A simulation with large language models",
      "arxiv": "arXiv:2504.12313"
    },
    {
      "citation_id": "82",
      "title": "Exploring safety-utility trade-offs in personalized language models",
      "authors": [
        "Anvesh Rao Vijjini",
        "Somnath Basu",
        "Roy Chowdhury",
        "Snigdha Chaturvedi"
      ],
      "year": "2024",
      "venue": "Exploring safety-utility trade-offs in personalized language models",
      "arxiv": "arXiv:2406.11107"
    },
    {
      "citation_id": "83",
      "title": "Comparing retrieval-augmentation and parameter-efficient fine-tuning for privacypreserving personalization of large language models",
      "authors": [
        "Alireza Salemi",
        "Hamed Zamani"
      ],
      "year": "2024",
      "venue": "Comparing retrieval-augmentation and parameter-efficient fine-tuning for privacypreserving personalization of large language models",
      "arxiv": "arXiv:2409.09510"
    },
    {
      "citation_id": "84",
      "title": "Longlamp: A benchmark for personalized long-form text generation",
      "authors": [
        "Ishita Kumar",
        "Snigdha Viswanathan",
        "Sushrita Yerra",
        "Alireza Salemi",
        "Ryan Rossi",
        "Franck Dernoncourt",
        "Hanieh Deilamsalehy",
        "Xiang Chen",
        "Ruiyi Zhang",
        "Shubham Agarwal"
      ],
      "year": "2024",
      "venue": "Longlamp: A benchmark for personalized long-form text generation",
      "arxiv": "arXiv:2407.11016"
    },
    {
      "citation_id": "85",
      "title": "Personalized graph-based retrieval for large language models",
      "authors": [
        "Steven Au",
        "Cameron Dimacali",
        "Ojasmitha Pedirappagari",
        "Namyong Park",
        "Franck Dernoncourt",
        "Yu Wang",
        "Nikos Kanakaris",
        "Hanieh Deilamsalehy",
        "Ryan Rossi",
        "Nesreen K Ahmed"
      ],
      "year": "2025",
      "venue": "Personalized graph-based retrieval for large language models",
      "arxiv": "arXiv:2501.02157"
    },
    {
      "citation_id": "86",
      "title": "Perltqa: A personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering",
      "authors": [
        "Yiming Du",
        "Hongru Wang",
        "Zhengyi Zhao",
        "Bin Liang",
        "Baojun Wang",
        "Wanjun Zhong",
        "Zezhong Wang",
        "Kam-Fai Wong"
      ],
      "year": "2024",
      "venue": "Perltqa: A personal long-term memory dataset for memory classification, retrieval, and synthesis in question answering",
      "arxiv": "arXiv:2402.16288"
    },
    {
      "citation_id": "87",
      "title": "Do llms recognize your preferences? evaluating personalized preference following in llms",
      "authors": [
        "Siyan Zhao",
        "Mingyi Hong",
        "Yang Liu",
        "Devamanyu Hazarika",
        "Kaixiang Lin"
      ],
      "year": "2025",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "88",
      "title": "Longmemeval: Benchmarking chat assistants on long-term interactive memory",
      "authors": [
        "Di Wu",
        "Hongwei Wang",
        "Wenhao Yu",
        "Yuwei Zhang",
        "Kai-Wei Chang",
        "Dong Yu"
      ],
      "year": "2025",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "89",
      "title": "Towards next-generation llm-based recommender systems: A survey and beyond",
      "authors": [
        "Qi Wang",
        "Jindong Li",
        "Shiqi Wang",
        "Qianli Xing",
        "Runliang Niu",
        "He Kong",
        "Rui Li",
        "Guodong Long",
        "Yi Chang",
        "Chengqi Zhang"
      ],
      "year": "2024",
      "venue": "Towards next-generation llm-based recommender systems: A survey and beyond",
      "arxiv": "arXiv:2410.19744"
    },
    {
      "citation_id": "90",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "Chung Hyung Won",
        "Le Hou",
        "Shayne Longpre",
        "Barret Zoph",
        "Yi Tay",
        "William Fedus",
        "Yunxuan Li",
        "Xuezhi Wang",
        "Mostafa Dehghani",
        "Siddhartha Brahma"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models",
      "arxiv": "arXiv:2210.11416"
    },
    {
      "citation_id": "91",
      "title": "Glm-130b: An open bilingual pre-trained model",
      "authors": [
        "Aohan Zeng",
        "Xiao Liu",
        "Zhengxiao Du",
        "Zihan Wang",
        "Hanyu Lai",
        "Ming Ding",
        "Zhuoyi Yang",
        "Yifan Xu",
        "Wendi Zheng",
        "Xiao Xia"
      ],
      "year": "2022",
      "venue": "Glm-130b: An open bilingual pre-trained model",
      "arxiv": "arXiv:2210.02414"
    },
    {
      "citation_id": "92",
      "title": "Language models are few-shot learners",
      "authors": [
        "Tom Brown",
        "Benjamin Mann",
        "Nick Ryder",
        "Melanie Subbiah",
        "Jared Kaplan",
        "Prafulla Dhariwal",
        "Arvind Neelakantan",
        "Pranav Shyam",
        "Girish Sastry",
        "Amanda Askell"
      ],
      "year": "2020",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "93",
      "title": "Scaling language modeling with pathways",
      "authors": [
        "Aakanksha Chowdhery",
        "Sharan Narang",
        "Jacob Devlin",
        "Maarten Bosma",
        "Gaurav Mishra",
        "Adam Roberts",
        "Paul Barham",
        "Hyung Chung",
        "Charles Sutton",
        "Sebastian Gehrmann"
      ],
      "year": "2022",
      "venue": "Scaling language modeling with pathways",
      "arxiv": "arXiv:2204.02311"
    },
    {
      "citation_id": "94",
      "title": "Open and efficient foundation language models",
      "authors": [
        "Hugo Touvron",
        "Thibaut Lavril",
        "Gautier Izacard",
        "Xavier Martinet",
        "Marie-Anne Lachaux",
        "Timothée Lacroix",
        "Baptiste Rozière",
        "Naman Goyal",
        "Eric Hambro",
        "Faisal Azhar"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "95",
      "title": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "authors": [
        "Dejian Daya Guo",
        "Haowei Yang",
        "Junxiao Zhang",
        "Ruoyu Song",
        "Runxin Zhang",
        "Qihao Xu",
        "Shirong Zhu",
        "Peiyi Ma",
        "Xiao Wang",
        "Bi"
      ],
      "year": "2025",
      "venue": "Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning",
      "arxiv": "arXiv:2501.12948"
    },
    {
      "citation_id": "96",
      "title": "Low-rank adaptation for foundation models: A comprehensive review",
      "authors": [
        "Menglin Yang",
        "Jialin Chen",
        "Yifei Zhang",
        "Jiahong Liu",
        "Jiasheng Zhang",
        "Qiyao Ma",
        "Harshit Verma",
        "Qianru Zhang",
        "Min Zhou",
        "Irwin King"
      ],
      "year": "2024",
      "venue": "Low-rank adaptation for foundation models: A comprehensive review",
      "arxiv": "arXiv:2501.00365"
    },
    {
      "citation_id": "97",
      "title": "Large language model enhanced recommender systems: A survey",
      "authors": [
        "Qidong Liu",
        "Xiangyu Zhao",
        "Yuhao Wang",
        "Yejing Wang",
        "Zijian Zhang",
        "Yuqi Sun",
        "Xiang Li",
        "Maolin Wang",
        "Pengyue Jia",
        "Chong Chen"
      ],
      "year": "2024",
      "venue": "Large language model enhanced recommender systems: A survey",
      "arxiv": "arXiv:2412.13432"
    },
    {
      "citation_id": "98",
      "title": "Two tales of persona in llms: A survey of role-playing and personalization",
      "authors": [
        "Yu-Min Tseng",
        "Yu-Chao Huang",
        "Teng-Yun Hsiao",
        "Yu-Ching Hsu",
        "Jia-Yin Foo",
        "Chao-Wei Huang",
        "Yun-Nung Chen"
      ],
      "year": "2024",
      "venue": "Two tales of persona in llms: A survey of role-playing and personalization",
      "arxiv": "arXiv:2406.01171"
    },
    {
      "citation_id": "99",
      "title": "When large language models meet personalization: Perspectives of challenges and opportunities",
      "authors": [
        "Jin Chen",
        "Zheng Liu",
        "Xu Huang",
        "Chenwang Wu",
        "Qi Liu",
        "Gangwei Jiang",
        "Yuanhao Pu",
        "Yuxuan Lei",
        "Xiaolong Chen",
        "Xingmei Wang"
      ],
      "year": "2024",
      "venue": "World Wide Web"
    },
    {
      "citation_id": "100",
      "title": "Personalization of large language models: A survey",
      "authors": [
        "Zhehao Zhang",
        "Ryan Rossi",
        "Branislav Kveton",
        "Yijia Shao",
        "Diyi Yang",
        "Hamed Zamani",
        "Franck Dernoncourt",
        "Joe Barrow",
        "Tong Yu",
        "Sungchul Kim"
      ],
      "year": "2024",
      "venue": "Personalization of large language models: A survey",
      "arxiv": "arXiv:2411.00027"
    },
    {
      "citation_id": "101",
      "title": "Player*: Enhancing llm-based multi-agent communication and interaction in murder mystery games. arXiv e-prints",
      "authors": [
        "Qinglin Zhu",
        "Runcong Zhao",
        "Bin Liang",
        "Jinhua Du",
        "Lin Gui",
        "Yulan He"
      ],
      "year": "2024",
      "venue": "Player*: Enhancing llm-based multi-agent communication and interaction in murder mystery games. arXiv e-prints"
    },
    {
      "citation_id": "102",
      "title": "Narrativeplay: An automated system for crafting visual worlds in novels for role-playing",
      "authors": [
        "Runcong Zhao",
        "Wenjia Zhang",
        "Jiazheng Li",
        "Lixing Zhu",
        "Yanran Li",
        "Yulan He",
        "Lin Gui"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "103",
      "title": "Retrieval-augmented generation for large language models: A survey",
      "authors": [
        "Yunfan Gao",
        "Yun Xiong",
        "Xinyu Gao",
        "Kangxiang Jia",
        "Jinliu Pan",
        "Yuxi Bi",
        "Yi Dai",
        "Jiawei Sun",
        "Haofen Wang"
      ],
      "year": "2023",
      "venue": "Retrieval-augmented generation for large language models: A survey",
      "arxiv": "arXiv:2312.10997"
    },
    {
      "citation_id": "104",
      "title": "A survey on rag meeting llms: Towards retrieval-augmented large language models",
      "authors": [
        "Wenqi Fan",
        "Yujuan Ding",
        "Liangbo Ning",
        "Shijie Wang",
        "Hengyun Li",
        "Dawei Yin",
        "Tat-Seng Chua",
        "Qing Li"
      ],
      "year": "2024",
      "venue": "Proc. of KDD"
    },
    {
      "citation_id": "105",
      "title": "Entropy-based decoding for retrievalaugmented large language models",
      "authors": [
        "Zexuan Qiu",
        "Zijing Ou",
        "Bin Wu",
        "Jingjing Li",
        "Aiwei Liu",
        "Irwin King"
      ],
      "year": "2024",
      "venue": "Entropy-based decoding for retrievalaugmented large language models",
      "arxiv": "arXiv:2406.17519"
    },
    {
      "citation_id": "106",
      "title": "Towards more comprehensive evaluation on the memory of llm-based agents",
      "authors": [
        "Haoran Tan",
        "Zeyu Zhang",
        "Chen Ma",
        "Xu Chen",
        "Quanyu Dai",
        "Zhenhua Dong",
        "Membench"
      ],
      "year": "2025",
      "venue": "Towards more comprehensive evaluation on the memory of llm-based agents",
      "arxiv": "arXiv:2506.21605"
    },
    {
      "citation_id": "107",
      "title": "Okapi at trec-3",
      "authors": [
        "Steve Stephen E Robertson",
        "Susan Walker",
        "Micheline Jones",
        "Mike Hancock-Beaulieu",
        "Gatford"
      ],
      "year": "1995",
      "venue": "Okapi at trec-3"
    },
    {
      "citation_id": "108",
      "title": "Billion-scale similarity search with gpus",
      "authors": [
        "Jeff Johnson",
        "Matthijs Douze",
        "Hervé Jégou"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Big Data"
    },
    {
      "citation_id": "109",
      "title": "Unsupervised dense information retrieval with contrastive learning",
      "authors": [
        "Gautier Izacard",
        "Mathilde Caron",
        "Lucas Hosseini",
        "Sebastian Riedel",
        "Piotr Bojanowski",
        "Armand Joulin",
        "Edouard Grave"
      ],
      "year": "2021",
      "venue": "Unsupervised dense information retrieval with contrastive learning",
      "arxiv": "arXiv:2112.09118"
    },
    {
      "citation_id": "110",
      "title": "Hello again! llm-powered personalized agent for long-term dialogue",
      "authors": [
        "Hao Li",
        "Chenghao Yang",
        "An Zhang",
        "Yang Deng",
        "Xiang Wang",
        "Tat-Seng Chua"
      ],
      "year": "2024",
      "venue": "Hello again! llm-powered personalized agent for long-term dialogue",
      "arxiv": "arXiv:2406.05925"
    },
    {
      "citation_id": "111",
      "title": "Prime: Large language model personalization with cognitive memory and thought processes",
      "authors": [
        "Frederick Xinliang",
        "Nick Zhang",
        "Lu Beauchamp",
        "Wang"
      ],
      "year": "2025",
      "venue": "Prime: Large language model personalization with cognitive memory and thought processes",
      "arxiv": "arXiv:2507.04607"
    },
    {
      "citation_id": "112",
      "title": "Latent inter-user difference modeling for llm personalization",
      "authors": [
        "Yilun Qiu",
        "Tianhao Shi",
        "Xiaoyan Zhao",
        "Fengbin Zhu",
        "Yang Zhang",
        "Fuli Feng"
      ],
      "year": "2025",
      "venue": "Latent inter-user difference modeling for llm personalization",
      "arxiv": "arXiv:2507.20849"
    },
    {
      "citation_id": "113",
      "title": "Personalized response generation via generative split memory network",
      "authors": [
        "Yuwei Wu",
        "Xuezhe Ma",
        "Diyi Yang"
      ],
      "year": "2021",
      "venue": "Proc. of NAACL"
    },
    {
      "citation_id": "114",
      "title": "Costeer: Collaborative decoding-time personalization via local delta steering",
      "authors": [
        "Hang Lv",
        "Sheng Liang",
        "Hao Wang",
        "Hongchao Gu",
        "Yaxiong Wu",
        "Wei Guo",
        "Defu Lian",
        "Yong Liu",
        "Enhong Chen"
      ],
      "year": "2025",
      "venue": "Costeer: Collaborative decoding-time personalization via local delta steering",
      "arxiv": "arXiv:2507.04756"
    },
    {
      "citation_id": "115",
      "title": "Low-rank adaptation of large language models",
      "authors": [
        "J Edward",
        "Yelong Hu",
        "Phillip Shen",
        "Zeyuan Wallis",
        "Yuanzhi Allen-Zhu",
        "Shean Li",
        "Lu Wang",
        "Weizhu Wang",
        "Chen",
        "Lora"
      ],
      "year": "2021",
      "venue": "Low-rank adaptation of large language models",
      "arxiv": "arXiv:2106.09685"
    },
    {
      "citation_id": "116",
      "title": "Prefix-tuning: Optimizing continuous prompts for generation",
      "authors": [
        "Lisa Xiang",
        "Percy Li",
        "Liang"
      ],
      "year": "2021",
      "venue": "Prefix-tuning: Optimizing continuous prompts for generation",
      "arxiv": "arXiv:2101.00190"
    },
    {
      "citation_id": "117",
      "title": "Fine-tuning language models with just forward passes",
      "authors": [
        "Sadhika Malladi",
        "Tianyu Gao",
        "Eshaan Nichani",
        "Alex Damian",
        "Jason Lee",
        "Danqi Chen",
        "Sanjeev Arora"
      ],
      "year": "2023",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "118",
      "title": "Useridentifier: implicit user representations for simple and effective personalized sentiment analysis",
      "authors": [
        "Fatemehsadat Mireshghallah",
        "Vaishnavi Shrivastava",
        "Milad Shokouhi",
        "Taylor Berg-Kirkpatrick",
        "Robert Sim",
        "Dimitrios Dimitriadis"
      ],
      "year": "2021",
      "venue": "Useridentifier: implicit user representations for simple and effective personalized sentiment analysis",
      "arxiv": "arXiv:2110.00135"
    },
    {
      "citation_id": "119",
      "title": "Autopeft: Automatic configuration search for parameterefficient fine-tuning",
      "authors": [
        "Han Zhou",
        "Xingchen Wan",
        "Ivan Vulić",
        "Anna Korhonen"
      ],
      "year": "2024",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "120",
      "title": "A survey on mixture of experts",
      "authors": [
        "Weilin Cai",
        "Juyong Jiang",
        "Fan Wang",
        "Jing Tang",
        "Sunghun Kim",
        "Jiayi Huang"
      ],
      "year": "2024",
      "venue": "A survey on mixture of experts",
      "arxiv": "arXiv:2407.06204"
    },
    {
      "citation_id": "121",
      "title": "Exploring personalization shifts in representation space of llms",
      "authors": [
        "Jiahong Liu",
        "Wenhao Yu",
        "Quanyu Dai",
        "Zhongyang Li",
        "Jieming Zhu",
        "Menglin Yang",
        "Tat-Seng Chua",
        "Irwin King"
      ],
      "year": "2025",
      "venue": "Knowledgeable Foundation Models at ACL 2025"
    },
    {
      "citation_id": "122",
      "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "authors": [
        "Yuntao Bai",
        "Andy Jones",
        "Kamal Ndousse",
        "Amanda Askell",
        "Anna Chen",
        "Nova Dassarma",
        "Dawn Drain",
        "Stanislav Fort",
        "Deep Ganguli",
        "Tom Henighan"
      ],
      "year": "2022",
      "venue": "Training a helpful and harmless assistant with reinforcement learning from human feedback",
      "arxiv": "arXiv:2204.05862"
    },
    {
      "citation_id": "123",
      "title": "Direct preference optimization: Your language model is secretly a reward model",
      "authors": [
        "Rafael Rafailov",
        "Archit Sharma",
        "Eric Mitchell",
        "Christopher Manning",
        "Stefano Ermon",
        "Chelsea Finn"
      ],
      "year": "2024",
      "venue": "Proc. of NuerIPS"
    },
    {
      "citation_id": "124",
      "title": "Training language models to follow instructions with human feedback",
      "authors": [
        "Long Ouyang",
        "Jeffrey Wu",
        "Xu Jiang",
        "Diogo Almeida",
        "Carroll Wainwright",
        "Pamela Mishkin",
        "Chong Zhang",
        "Sandhini Agarwal",
        "Katarina Slama",
        "Alex Ray"
      ],
      "year": "2022",
      "venue": "Proc. of NeurIPS"
    },
    {
      "citation_id": "125",
      "title": "Language model personalization via reward factorization",
      "authors": [
        "Idan Shenfeld",
        "Felix Faltings",
        "Pulkit Agrawal",
        "Aldo Pacchiano"
      ],
      "year": "2025",
      "venue": "Language model personalization via reward factorization",
      "arxiv": "arXiv:2503.06358"
    },
    {
      "citation_id": "126",
      "title": "Amulet: Realignment during test time for personalized preference adaptation of llms",
      "authors": [
        "Zhaowei Zhang",
        "Fengshuo Bai",
        "Qizhi Chen",
        "Chengdong Ma",
        "Mingzhi Wang",
        "Haoran Sun",
        "Zilong Zheng",
        "Yaodong Yang"
      ],
      "year": "2025",
      "venue": "Amulet: Realignment during test time for personalized preference adaptation of llms",
      "arxiv": "arXiv:2502.19148"
    },
    {
      "citation_id": "127",
      "title": "The advantages of the matthews correlation coefficient (mcc) over f1 score and accuracy in binary classification evaluation",
      "authors": [
        "Davide Chicco",
        "Giuseppe Jurman"
      ],
      "year": "2020",
      "venue": "BMC genomics"
    },
    {
      "citation_id": "128",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "authors": [
        "Chin-Yew Lin"
      ],
      "year": "2004",
      "venue": "Text summarization branches out"
    },
    {
      "citation_id": "129",
      "title": "Bleu: a method for automatic evaluation of machine translation",
      "authors": [
        "Kishore Papineni",
        "Salim Roukos",
        "Todd Ward",
        "Wei-Jing Zhu"
      ],
      "year": "2002",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "130",
      "title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments",
      "authors": [
        "Satanjeev Banerjee",
        "Alon Lavie"
      ],
      "year": "2005",
      "venue": "Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization"
    },
    {
      "citation_id": "131",
      "title": "Sentence-bert: Sentence embeddings using siamese bert-networks",
      "authors": [
        "Nils Reimers",
        "Iryna Gurevych"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)"
    },
    {
      "citation_id": "132",
      "title": "A survey on llm-as-a-judge",
      "authors": [
        "Jiawei Gu",
        "Xuhui Jiang",
        "Zhichao Shi",
        "Hexiang Tan",
        "Xuehao Zhai",
        "Chengjin Xu",
        "Wei Li",
        "Yinghan Shen",
        "Shengjie Ma",
        "Honghao Liu"
      ],
      "year": "2024",
      "venue": "A survey on llm-as-a-judge",
      "arxiv": "arXiv:2411.15594"
    },
    {
      "citation_id": "133",
      "title": "Root mean square error (rmse) or mean absolute error (mae)?-arguments against avoiding rmse in the literature",
      "authors": [
        "Tianfeng Chai",
        "Roland Draxler"
      ],
      "year": "2014",
      "venue": "Geoscientific model development"
    },
    {
      "citation_id": "134",
      "title": "Root mean square error (rmse) or mean absolute error (mae): When to use them or not",
      "authors": [
        "Timothy O Hodson"
      ],
      "year": "2022",
      "venue": "Geoscientific Model Development Discussions"
    },
    {
      "citation_id": "135",
      "title": "Building end-to-end dialogue systems using generative hierarchical neural network models",
      "authors": [
        "Iulian Serban",
        "Alessandro Sordoni",
        "Yoshua Bengio",
        "Aaron Courville",
        "Joelle Pineau"
      ],
      "year": "2016",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "136",
      "title": "Bertscore: Evaluating text generation with bert",
      "authors": [
        "Tianyi Zhang",
        "Varsha Kishore",
        "Felix Wu",
        "Kilian Weinberger",
        "Yoav Artzi"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "137",
      "title": "Accuracy is not enough: Evaluating personalization in summarizers",
      "authors": [
        "Rahul Vansh",
        "Darsh Rank",
        "Sourish Dasgupta",
        "Tanmoy Chakraborty"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "138",
      "title": "Parth Borad, Isha Motiyani, and Tanmoy Chakraborty. Perseval: Assessing personalization in text summarizers",
      "authors": [
        "Sourish Dasgupta",
        "Ankush Chander"
      ],
      "year": "2024",
      "venue": "Parth Borad, Isha Motiyani, and Tanmoy Chakraborty. Perseval: Assessing personalization in text summarizers",
      "arxiv": "arXiv:2407.00453"
    },
    {
      "citation_id": "139",
      "title": "A survey of accuracy evaluation metrics of recommendation tasks",
      "authors": [
        "Asela Gunawardana",
        "Guy Shani"
      ],
      "year": "2009",
      "venue": "JMLR"
    },
    {
      "citation_id": "140",
      "title": "A theoretical analysis of ndcg type ranking measures",
      "authors": [
        "Yining Wang",
        "Liwei Wang",
        "Yuanzhi Li",
        "Di He",
        "Tie-Yan Liu"
      ],
      "year": "2013",
      "venue": "Conference on learning theory"
    },
    {
      "citation_id": "141",
      "title": "A personalized conversational benchmark: Towards simulating personalized conversations",
      "authors": [
        "Li Li",
        "Peilin Cai",
        "Ryan Rossi",
        "Franck Dernoncourt",
        "Branislav Kveton",
        "Junda Wu",
        "Tong Yu",
        "Linxin Song",
        "Tiankai Yang",
        "Yuehan Qin"
      ],
      "year": "2025",
      "venue": "A personalized conversational benchmark: Towards simulating personalized conversations",
      "arxiv": "arXiv:2505.14106"
    },
    {
      "citation_id": "142",
      "title": "Expert: Effective and explainable evaluation of personalized long-form text generation",
      "authors": [
        "Alireza Salemi",
        "Julian Killingback",
        "Hamed Zamani"
      ],
      "year": "2025",
      "venue": "Expert: Effective and explainable evaluation of personalized long-form text generation",
      "arxiv": "arXiv:2501.14956"
    },
    {
      "citation_id": "143",
      "title": "Concept-an evaluation protocol on conversational recommender systems with system-centric and user-centric factors",
      "authors": [
        "Chen Huang",
        "Peixin Qin",
        "Yang Deng",
        "Wenqiang Lei",
        "Jiancheng Lv",
        "Tat-Seng Chua"
      ],
      "year": "2024",
      "venue": "Concept-an evaluation protocol on conversational recommender systems with system-centric and user-centric factors",
      "arxiv": "arXiv:2404.03304"
    },
    {
      "citation_id": "144",
      "title": "A large-scale benchmark for understanding multimodal large language model in real-world conversation",
      "authors": [
        "Haochen Xue",
        "Feilong Tang",
        "Ming Hu",
        "Yexin Liu",
        "Qidong Huang",
        "Yulong Li",
        "Chengzhi Liu",
        "Zhongxing Xu",
        "Chong Zhang",
        "Chun-Mei Feng"
      ],
      "year": "2025",
      "venue": "A large-scale benchmark for understanding multimodal large language model in real-world conversation",
      "arxiv": "arXiv:2502.11903"
    },
    {
      "citation_id": "145",
      "title": "Evaluating very long-term conversational memory of llm agents",
      "authors": [
        "Adyasha Maharana",
        "Dong-Ho Lee",
        "Sergey Tulyakov",
        "Mohit Bansal",
        "Francesco Barbieri",
        "Yuwei Fang"
      ],
      "year": "2024",
      "venue": "Evaluating very long-term conversational memory of llm agents",
      "arxiv": "arXiv:2402.17753"
    },
    {
      "citation_id": "146",
      "title": "Toward multi-session personalized conversation: A large-scale dataset and hierarchical tree framework for implicit reasoning",
      "authors": [
        "Xintong Li",
        "Jalend Bantupalli",
        "Ria Dharmani",
        "Yuwei Zhang",
        "Jingbo Shang"
      ],
      "year": "2025",
      "venue": "Toward multi-session personalized conversation: A large-scale dataset and hierarchical tree framework for implicit reasoning",
      "arxiv": "arXiv:2503.07018"
    },
    {
      "citation_id": "147",
      "title": "Exploring the potential of llms as personalized assistants: Dataset, evaluation, and analysis",
      "authors": [
        "Jisoo Mok",
        "Ik-Hwan Kim",
        "Sangkwon Park",
        "Sungroh Yoon"
      ],
      "year": "2025",
      "venue": "Exploring the potential of llms as personalized assistants: Dataset, evaluation, and analysis",
      "arxiv": "arXiv:2506.01262"
    },
    {
      "citation_id": "148",
      "title": "Lamp-qa: A benchmark for personalized long-form question answering",
      "authors": [
        "Alireza Salemi",
        "Hamed Zamani"
      ],
      "year": "2025",
      "venue": "Lamp-qa: A benchmark for personalized long-form question answering",
      "arxiv": "arXiv:2506.00137"
    },
    {
      "citation_id": "149",
      "title": "Large language models empowered personalized web agents",
      "authors": [
        "Hongru Cai",
        "Yongqi Li",
        "Wenjie Wang",
        "Fengbin Zhu",
        "Xiaoyu Shen",
        "Wenjie Li",
        "Tat-Seng Chua"
      ],
      "year": "2025",
      "venue": "Proceedings of the ACM on Web Conference 2025"
    },
    {
      "citation_id": "150",
      "title": "Towards nextgeneration recommender systems: A benchmark for personalized recommendation assistant with llms",
      "authors": [
        "Jiani Huang",
        "Shijie Wang",
        "Liang-Bo Ning",
        "Wenqi Fan",
        "Shuaiqiang Wang",
        "Dawei Yin",
        "Qing Li"
      ],
      "year": "2025",
      "venue": "Towards nextgeneration recommender systems: A benchmark for personalized recommendation assistant with llms",
      "arxiv": "arXiv:2503.09382"
    },
    {
      "citation_id": "151",
      "title": "Personalized multimodal large language models: A survey",
      "authors": [
        "Junda Wu",
        "Hanjia Lyu",
        "Yu Xia",
        "Zhehao Zhang",
        "Joe Barrow",
        "Ishita Kumar",
        "Mehrnoosh Mirtaheri",
        "Hongjie Chen",
        "Ryan Rossi",
        "Franck Dernoncourt"
      ],
      "year": "2024",
      "venue": "Personalized multimodal large language models: A survey",
      "arxiv": "arXiv:2412.02142"
    },
    {
      "citation_id": "152",
      "title": "Personalized visual instruction tuning",
      "authors": [
        "Renjie Pi",
        "Jianshu Zhang",
        "Tianyang Han",
        "Jipeng Zhang",
        "Rui Pan",
        "Tong Zhang"
      ],
      "year": "2024",
      "venue": "Personalized visual instruction tuning",
      "arxiv": "arXiv:2410.07113"
    },
    {
      "citation_id": "153",
      "title": "Pmg: Personalized multimodal generation with large language models",
      "authors": [
        "Xiaoteng Shen",
        "Rui Zhang",
        "Xiaoyan Zhao",
        "Jieming Zhu",
        "Xi Xiao"
      ],
      "year": "2024",
      "venue": "Proceedings of the ACM on Web Conference 2024"
    },
    {
      "citation_id": "154",
      "title": "Personalized generation in large model era: A survey",
      "authors": [
        "Yiyan Xu",
        "Jinghao Zhang",
        "Alireza Salemi",
        "Xinting Hu",
        "Wenjie Wang",
        "Fuli Feng",
        "Hamed Zamani",
        "Xiangnan He",
        "Tat-Seng Chua"
      ],
      "year": "2025",
      "venue": "Personalized generation in large model era: A survey",
      "arxiv": "arXiv:2503.02614"
    },
    {
      "citation_id": "155",
      "title": "Small language models: Survey, measurements, and insights",
      "authors": [
        "Zhenyan Lu",
        "Xiang Li",
        "Dongqi Cai",
        "Rongjie Yi",
        "Fangming Liu",
        "Xiwen Zhang",
        "Nicholas Lane",
        "Mengwei Xu"
      ],
      "year": "2024",
      "venue": "Small language models: Survey, measurements, and insights",
      "arxiv": "arXiv:2409.15790"
    },
    {
      "citation_id": "156",
      "title": "An edge-cloud collaboration framework for generative ai service provision with synergetic big cloud model and small edge models",
      "authors": [
        "Yuqing Tian",
        "Zhaoyang Zhang",
        "Yuzhi Yang",
        "Zirui Chen",
        "Zhaohui Yang",
        "Richeng Jin",
        "Tony Quek",
        "Kai-Kit Wong"
      ],
      "year": "2024",
      "venue": "An edge-cloud collaboration framework for generative ai service provision with synergetic big cloud model and small edge models",
      "arxiv": "arXiv:2401.01666"
    },
    {
      "citation_id": "157",
      "title": "Mobilellm: Optimizing sub-billion parameter language models for on-device use cases",
      "authors": [
        "Zechun Liu",
        "Changsheng Zhao",
        "Forrest Iandola",
        "Chen Lai",
        "Yuandong Tian",
        "Igor Fedorov",
        "Yunyang Xiong",
        "Ernie Chang",
        "Yangyang Shi",
        "Raghuraman Krishnamoorthi"
      ],
      "year": "2024",
      "venue": "Forty-first International Conference on Machine Learning"
    },
    {
      "citation_id": "158",
      "title": "Continual learning for large language models: A survey",
      "authors": [
        "Tongtong Wu",
        "Linhao Luo",
        "Yuan-Fang Li",
        "Shirui Pan",
        "Thuy-Trang Vu",
        "Gholamreza Haffari"
      ],
      "year": "2024",
      "venue": "Continual learning for large language models: A survey",
      "arxiv": "arXiv:2402.01364"
    },
    {
      "citation_id": "159",
      "title": "Knowledge editing for large language models: A survey",
      "authors": [
        "Song Wang",
        "Yaochen Zhu",
        "Haochen Liu",
        "Zaiyi Zheng",
        "Chen Chen",
        "Jundong Li"
      ],
      "year": "2024",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "160",
      "title": "A comprehensive study of knowledge editing for large language models",
      "authors": [
        "Ningyu Zhang",
        "Yunzhi Yao",
        "Bozhong Tian",
        "Peng Wang",
        "Shumin Deng",
        "Mengru Wang",
        "Zekun Xi",
        "Shengyu Mao",
        "Jintian Zhang",
        "Yuansheng Ni"
      ],
      "year": "2024",
      "venue": "A comprehensive study of knowledge editing for large language models",
      "arxiv": "arXiv:2401.01286"
    },
    {
      "citation_id": "161",
      "title": "Federated large language models: Current progress and future directions",
      "authors": [
        "Yuhang Yao",
        "Jianyi Zhang",
        "Junda Wu",
        "Chengkai Huang",
        "Yu Xia",
        "Tong Yu",
        "Ruiyi Zhang",
        "Sungchul Kim",
        "Ryan Rossi",
        "Ang Li"
      ],
      "year": "2024",
      "venue": "Federated large language models: Current progress and future directions",
      "arxiv": "arXiv:2409.15723"
    },
    {
      "citation_id": "162",
      "title": "Client-specific hyperbolic federated learning",
      "authors": [
        "Jiahong Liu",
        "Xinyu Fu",
        "Menglin Yang",
        "Weixi Zhang",
        "Rex Ying",
        "Irwin King"
      ],
      "year": "2024",
      "venue": "FedKDD@KDD"
    },
    {
      "citation_id": "163",
      "title": "Sparse activation editing for reliable instruction following in narratives",
      "authors": [
        "Runcong Zhao",
        "Chengyu Cao",
        "Qinglin Zhu",
        "Xiucheng Lv",
        "Shun Shao",
        "Lin Gui",
        "Ruifeng Xu",
        "Yulan He"
      ],
      "year": "2025",
      "venue": "Sparse activation editing for reliable instruction following in narratives",
      "arxiv": "arXiv:2505.16505"
    },
    {
      "citation_id": "164",
      "title": "Learnlens: Llm-enabled personalised, curriculum-grounded feedback with educators in the loop",
      "authors": [
        "Runcong Zhao",
        "Artem Borov",
        "Jiazheng Li",
        "Yulan He"
      ],
      "year": "2025",
      "venue": "Learnlens: Llm-enabled personalised, curriculum-grounded feedback with educators in the loop",
      "arxiv": "arXiv:2507.04295"
    },
    {
      "citation_id": "165",
      "title": "Memos: A memory os for ai system",
      "authors": [
        "Zhiyu Li",
        "Shichao Song",
        "Chenyang Xi",
        "Hanyu Wang",
        "Chen Tang",
        "Simin Niu",
        "Ding Chen",
        "Jiawei Yang",
        "Chunyu Li",
        "Qingchen Yu"
      ],
      "year": "2025",
      "venue": "Memos: A memory os for ai system",
      "arxiv": "arXiv:2507.03724"
    }
  ]
}