{
  "paper_id": "2208.03506v3",
  "title": "Multi-Task Transformer With Uncertainty Modelling For Face Based Affective Computing",
  "published": "2022-08-06T12:25:12Z",
  "authors": [
    "Gauthier Tallec",
    "Jules Bonnard",
    "Arnaud Dapogny",
    "Kévin Bailly"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Face based affective computing consists in detecting emotions from face images. It is useful to unlock better automatic comprehension of human behaviours and could pave the way toward improved human-machines interactions. However it comes with the challenging task of designing a computational representation of emotions. So far, emotions have been represented either continuously in the 2D Valence/Arousal (VA) space or in a discrete manner with Ekman's 7 basic emotions (FER). Alternatively, Ekman's Facial Action Unit (AU) system have also been used to caracterize emotions using a codebook of unitary muscular activations. ABAW3 and ABAW4 Multi-Task Challenges are the first work to provide a large scale database annotated with those three types of labels. In this paper we present a transformer based multi-task method for jointly learning to predict valence arousal, action units and basic emotions. From an architectural standpoint our method uses a taskwise token approach to efficiently model the similarities between the tasks. From a learning point of view we use an uncertainty weighted loss for modelling the difference of stochasticity between the three tasks annotations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The Affective Behavior Analysis In-The-Wild (ABAW) competitions are a serie of machine learning based affective computing challenges based on the Aff-Wild2 database  [12, 17] . In the past few years, the competition objectives have focused on learning to predict different face expression representation  [9, 15]  either jointly  [8, 10, 11, 14]  or in a separate manner  [4] . This year is no exception, as ABAW4 proposes (a) A challenge for Multi-Task learning that consists in predicting Valence-Arousal, Facial Expression, and Action Units  [5]  (b) A challenge for learning to predict the 7 basic emotions from synthetic data  [3, 6, 7, 13] .\n\nWe participate in the multi-task learning challenge because this is one of the first work to provide a large scale database (170m frames) annotated in all three emotion representations (AU, VA and FER).\n\nOne of the main challenge when learning to predict emotion representations is that the provided data are often noisy due to the intrinsic subjectivity of the annotation task. It is all the more a problem when trying to jointly predict several emotion representations as those representations may display different level of noise due to their different level of subjectivity. For example Valence Arousal annotations are slightly noisier than Action Units due to the continuous nature of the annotations. To tackle that challenge we leverage a classic multi-task transformer architecture that we supervise with an uncertainty weighted loss  [2]  that takes into account the different stochasticity levels of each task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "The lack of annotated datasets in different emotional representations make multi-task face analysis models rarer in the literature. Nonetheless, the recent ABAW challenges have increased the interest in the multi-task approach to predict the three emotion descriptors.\n\nFor the ABAW3 challenge, Didan Deng  [1]  introduced a \"Sign-and-Message\" model which uses an Emotion Transformer to predict Action Units and metric learning to predict facial expressions, valence and arousal.\n\nJeong et al., the runner-ups of the ABAW3 Multi-Task challenge jointly leveraged the video data with an LSTM and the audio data with a SoundNet. These latent representations are concatenated and fed to different task classifiers and regressors. A task discriminator is used to improve the individual task predictions.\n\nHowever, the ABAW4 challenge doesn't provide any audio data and the available video data and the important number of missing frames in each video may prevent the use from of temporal methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "In this section we build up our transformer architecture based on the background material provided in  [16] . Fur-thermore we provide ourselves with a multi-task dataset\n\ni=1 where x ∈ R H×W ×C is the input image, v ∈ R 2 is the valence arousal ground truth, a ∈ {0, 1} 12 is the AU ground truth and e ∈ {0, 1} 8 is the basic emotion where the 8 th label features the additional 'Other' class that is present in ABAW4.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "General Architecture",
      "text": "ViT-like architecture are known to be sensitive to overfitting and to therefore require a lot of reliable data to be able to overcome the performance of basic convolutional networks.\n\nFor that purpose we adopt an hybrid architecture in which we first feed the input image to a convolutional encoder which consist in a convolutional encoder in which the final pooling layer is replaced by a 1D convolution with output dimension d.\n\nEach of the H ′ × W ′ output patch representations are then added an usual cos-sin based encoding to account for its position in the image. Those patches are then passed through N x classic self attention layers resulting in a patch based image represention that we denote p ∈ R H ′ ×W ′ ×d the resulting image representation.\n\nTo predict each of the three tasks we provide ourselves with three learned task tokens. Those tokens are then fed to a sequential arrangement of N t attention modules. Each cross attention module is composed of two main block:\n\n1. The first block consist in self-attention between tokens so that each task can learn to exploit the information from the other tasks.\n\n2. The second block is a cross-attention step in which each token learns which part of the p is most useful to predict its associated task.\n\nAt the end of the N t modules, each refined token is fed to a sequence of N d task specific dense layers where all activations are relu except for the last one which is linear.\n\nThe final output of this process is a prediction v for valence arousal, and logits u a and u e for action units and emotion respectively. The predictions for actions units â and emotion e are computed using different temperature T a , T e to account for the different level of stochasticity of those two tasks:\n\nwhere σ and S denotes sigmoid and softmax function respectively.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Uncertainty Based Supervision",
      "text": "To account for task stochasticy we assigned temperatures T a and T e to the action unit and emotion prediction task respectively. Similarly we assign a variance matrix to arousal and valence prediction. To simplify we work under the assumption that this variance matrix is diagonal and that valence and arousal have the same intrinsec variance σ 2 v . The final Maximum likehood based supervision loss is then :\n\nwhere :\n\nand MSE, BCE and CCE denotes mean squared error, binary crossentropy and categorical crossentropy respectively. In practice, as all three labels are not present for each examples we mask the losses for the missing labels and reweight the total loss with the number of present labels so that each example get the same contribution in the loss.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Implementation Details",
      "text": "For all our experiments we use a Resnet50 pretrained on VGGFACE2 as the convolutional backbone. For each resized image of size 224 × 224 × 3, it outputs a 7 × 7 patch based representation that we project in d = 768 dimensions using a 1D convolution. We then apply N x = 2 self attention layers with 12 attention heads to get the final image representation p.\n\nAs far as the task-token based part is concerned it consists in N t = 2 cross attention modules with 12 attention heads followed by N d = 4 taskwise dense.\n\nThe variance σ 2 v of valence arousal is set to 1. The temperature of action units and emotion are respectively set to T a = 1 and T e = 5.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Augmentation",
      "text": "ABAW4 dataset features in the wild data, so that there is a lot of variability in images and potentially between the train, the valid and the test set. To reduce the impact of those variabilitities we use classical geometrical data augmentation : random rotations, translations and zooms. Furthermore we tried using multi-task MixUp which consist in feeding random convex combinations of input images and supervising each task with the same random convex combination of each task ground truth.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Temporal Smoothing",
      "text": "The provided test set consists in images sampled from videos. To handle the dynamic of labels we simply compute the final predictions as temporal means of predictions in windows of size S = 30. For valence/arousal, the predictions are directly smoothed. For AU and Emotions, the logits are smoothed and the appropriate activation function is applied on the smoothing output. Missing images in the videos are simply ignored in the mean computation. Table  4 .3 summarize the validation scores of our two submissions. Our third submission is simply the mean of the two first submission.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conclusion",
      "text": "In this work we made use of a multi-task transformer for jointly predicting arousal/valence, basic emotions and action units. We further refined our method by modelling the different levels of stochasticity for each task. Our method finished at the 8 th position of the ABAW4 Challenge.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ISIR\nISIR": "Abstract",
          "Datakalab\nISIR/Datakalab": "database (170m frames) annotated in all three emotion rep-"
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "resentations (AU, VA and FER)."
        },
        {
          "ISIR\nISIR": "Face based affective\ncomputing consists\nin\ndetecting",
          "Datakalab\nISIR/Datakalab": "One of the main challenge when learning to predict emo-"
        },
        {
          "ISIR\nISIR": "emotions\nfrom face images.\nIt\nis useful\nto unlock better",
          "Datakalab\nISIR/Datakalab": "tion representations is that the provided data are often noisy"
        },
        {
          "ISIR\nISIR": "automatic comprehension of human behaviours and could",
          "Datakalab\nISIR/Datakalab": "due to the intrinsic subjectivity of the annotation task.\nIt is"
        },
        {
          "ISIR\nISIR": "pave the way toward improved human-machines interac-",
          "Datakalab\nISIR/Datakalab": "all\nthe more a problem when trying to jointly predict sev-"
        },
        {
          "ISIR\nISIR": "tions. However it comes with the challenging task of de-",
          "Datakalab\nISIR/Datakalab": "eral emotion representations as those representations may"
        },
        {
          "ISIR\nISIR": "signing a computational representation of emotions. So far,",
          "Datakalab\nISIR/Datakalab": "display different level of noise due to their different level of"
        },
        {
          "ISIR\nISIR": "emotions have been represented either continuously in the",
          "Datakalab\nISIR/Datakalab": "subjectivity. For example Valence Arousal annotations are"
        },
        {
          "ISIR\nISIR": "2D Valence/Arousal (VA) space or in a discrete manner with",
          "Datakalab\nISIR/Datakalab": "slightly noisier than Action Units due to the continuous na-"
        },
        {
          "ISIR\nISIR": "Ekman’s 7 basic emotions\n(FER). Alternatively, Ekman’s",
          "Datakalab\nISIR/Datakalab": "ture of the annotations. To tackle that challenge we leverage"
        },
        {
          "ISIR\nISIR": "Facial Action Unit (AU) system have also been used to car-",
          "Datakalab\nISIR/Datakalab": "a classic multi-task transformer architecture that we super-"
        },
        {
          "ISIR\nISIR": "acterize emotions using a codebook of unitary muscular ac-",
          "Datakalab\nISIR/Datakalab": "that\ntakes into\nvise with an uncertainty weighted loss [2]"
        },
        {
          "ISIR\nISIR": "tivations. ABAW3 and ABAW4 Multi-Task Challenges are",
          "Datakalab\nISIR/Datakalab": "account the different stochasticity levels of each task."
        },
        {
          "ISIR\nISIR": "the ﬁrst work to provide a large scale database annotated",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "with those three types of\nlabels.\nIn this paper we present",
          "Datakalab\nISIR/Datakalab": "2. Related Work"
        },
        {
          "ISIR\nISIR": "a transformer based multi-task method for jointly learning",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "The lack of annotated datasets in different emotional rep-"
        },
        {
          "ISIR\nISIR": "to predict valence arousal, action units and basic emotions.",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "resentations make multi-task face analysis models rarer in"
        },
        {
          "ISIR\nISIR": "From an architectural standpoint our method uses a task-",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "the literature. Nonetheless,\nthe recent ABAW challenges"
        },
        {
          "ISIR\nISIR": "wise token approach to efﬁciently model the similarities be-",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "have increased the interest in the multi-task approach to pre-"
        },
        {
          "ISIR\nISIR": "tween the tasks. From a learning point of view we use an",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "dict the three emotion descriptors."
        },
        {
          "ISIR\nISIR": "uncertainty weighted loss\nfor modelling the difference of",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "For the ABAW3 challenge, Didan Deng [1] introduced a"
        },
        {
          "ISIR\nISIR": "stochasticity between the three tasks annotations.",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "”Sign-and-Message” model which uses an Emotion Trans-"
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "former to predict Action Units and metric learning to predict"
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "facial expressions, valence and arousal."
        },
        {
          "ISIR\nISIR": "1. Introduction",
          "Datakalab\nISIR/Datakalab": "Jeong et al.,\nthe runner-ups of the ABAW3 Multi-Task"
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "challenge jointly leveraged the video data with an LSTM"
        },
        {
          "ISIR\nISIR": "The Affective Behavior Analysis In-The-Wild (ABAW)",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "and the audio data with a SoundNet. These latent represen-"
        },
        {
          "ISIR\nISIR": "competitions are a serie of machine learning based affec-",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "tations are concatenated and fed to different task classiﬁers"
        },
        {
          "ISIR\nISIR": "tive computing challenges based on the Aff-Wild2 database",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "and regressors. A task discriminator is used to improve the"
        },
        {
          "ISIR\nISIR": "In the past few years,\nthe competition objectives\n[12, 17].",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "individual task predictions."
        },
        {
          "ISIR\nISIR": "have focused on learning to predict different\nface expres-",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "However, the ABAW4 challenge doesn’t provide any au-"
        },
        {
          "ISIR\nISIR": "sion representation [9, 15] either jointly [8, 10, 11, 14] or in",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "dio data and the available video data and the important num-"
        },
        {
          "ISIR\nISIR": "a separate manner [4]. This year is no exception, as ABAW4",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "ber of missing frames in each video may prevent\nthe use"
        },
        {
          "ISIR\nISIR": "proposes (a) A challenge for Multi-Task learning that con-",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "from of temporal methods."
        },
        {
          "ISIR\nISIR": "sists in predicting Valence-Arousal, Facial Expression, and",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "Action Units [5] (b) A challenge for learning to predict the",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "",
          "Datakalab\nISIR/Datakalab": "3. Methodology"
        },
        {
          "ISIR\nISIR": "7 basic emotions from synthetic data [3, 6, 7, 13].",
          "Datakalab\nISIR/Datakalab": ""
        },
        {
          "ISIR\nISIR": "We participate in the multi-task learning challenge be-",
          "Datakalab\nISIR/Datakalab": "In this section we build up our transformer architecture"
        },
        {
          "ISIR\nISIR": "cause this is one of the ﬁrst work to provide a large scale",
          "Datakalab\nISIR/Datakalab": "based on the background material provided in [16].\nFur-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "thermore we provide ourselves with a multi-task dataset": "D = {xi, vi, ai, ei}N\ni=1 where x ∈ RH×W ×C is the in-",
          "and valence prediction. To simplify we work under the as-": "sumption that\nthis variance matrix is diagonal and that va-"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "put\nis\nthe valence arousal ground truth,\nimage, v ∈ R2",
          "and valence prediction. To simplify we work under the as-": "lence and arousal have the same intrinsec variance σ2\nv. The"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "a ∈ {0, 1}12 is the AU ground truth and e ∈ {0, 1}8 is",
          "and valence prediction. To simplify we work under the as-": "ﬁnal Maximum likehood based supervision loss is then :"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "the basic emotion where the 8th label features the additional",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "(2)\nL = Lv + La + Le"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "’Other’ class that is present in ABAW4.",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "where :"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "3.1. General Architecture",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "1"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "(3)\nMSE(ˆv, v),\nLv ="
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "ViT-like architecture are known to be sensitive to over-",
          "and valence prediction. To simplify we work under the as-": "2σ2"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "ﬁtting and to therefore require a lot of\nreliable data to be",
          "and valence prediction. To simplify we work under the as-": "1"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "(4)\nLa =\nBCE(ua, a),"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "able to overcome the performance of basic convolutional",
          "and valence prediction. To simplify we work under the as-": "2Ta"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "networks.",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "1 T\n(5)\nLe =\nCCE(ue, e),"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "For\nthat\npurpose we\nadopt\nan\nhybrid architecture\nin",
          "and valence prediction. To simplify we work under the as-": "e"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "which we ﬁrst feed the input\nimage to a convolutional en-",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "and MSE, BCE and CCE denotes mean\nsquared error,"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "coder which consist in a convolutional encoder in which the",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "binary crossentropy and categorical crossentropy respec-"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "ﬁnal pooling layer is replaced by a 1D convolution with out-",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "tively.\nIn practice, as all\nthree labels are not present\nfor"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "put dimension d.",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "each examples we mask the losses for\nthe missing labels"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "Each of\nthe H ′ × W ′ output patch representations are",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "and reweight\nthe total\nloss with the number of present\nla-"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "then added an usual cos-sin based encoding to account for",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "bels so that each example get\nthe same contribution in the"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "its position in the image.\nThose patches are then passed",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "loss."
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "through Nx classic self attention layers resulting in a patch",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "based image represention that we denote p ∈ RH′×W ′×d",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "4. Experiments"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "the resulting image representation.",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "4.1. Implementation Details"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "To predict each of the three tasks we provide ourselves",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "with three learned task tokens. Those tokens are then fed",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "For all our experiments we use a Resnet50 pretrained on"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "to a sequential arrangement of Nt attention modules. Each",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "VGGFACE2 as the convolutional backbone.\nFor each re-"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "cross attention module is composed of two main block:",
          "and valence prediction. To simplify we work under the as-": ""
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "sized image of size 224 × 224 × 3, it outputs a 7 × 7 patch"
        },
        {
          "thermore we provide ourselves with a multi-task dataset": "",
          "and valence prediction. To simplify we work under the as-": "based representation that we project in d = 768 dimensions"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Summary of validation scores for submissions to the",
      "data": [
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "1\n0.25\n0.48\n0.39\n1.12\n×",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "ing Zhao, and Stefanos Zafeiriou. Recognition of affect\nin"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "X\n2\n0.27\n0.47\n0.38\n1.11",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "the wild using deep neural networks.\nIn Computer Vision"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "and Pattern Recognition Workshops (CVPRW), 2017 IEEE"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "Table 1. Summary of validation scores\nfor\nsubmissions\nto the",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "Conference on, pages 1972–1979. IEEE, 2017."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "ABAW4 Challenge",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "[9] D Kollias, A Schulc, E Hajiyev, and S Zafeiriou. Analysing"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "affective behavior\nin the ﬁrst abaw 2020 competition.\nIn"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "2020\n15th\nIEEE International Conference\non Automatic"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "the pre-\nin windows of size S = 30. For valence/arousal,",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "Face and Gesture Recognition (FG 2020)(FG), pages 794–"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "dictions are directly smoothed. For AU and Emotions,\nthe",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "800."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "logits are smoothed and the appropriate activation function",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "[10] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "is applied on the smoothing output. Missing images in the",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "Zafeiriou.\nFace\nbehavior\na\nla\ncarte:\nExpressions,\naf-"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "videos are simply ignored in the mean computation.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "arXiv preprint\nfect and action units\nin a single network."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "Table 4.3 summarize the validation scores of our\ntwo",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "arXiv:1910.11111, 2019."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "submissions. Our\nthird submission is simply the mean of",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "[11] Dimitrios Kollias,\nViktoriia\nSharmanska,\nand\nStefanos"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "the two ﬁrst submission.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "Zafeiriou.\nDistribution matching for heterogeneous multi-"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "arXiv\npreprint\ntask\nlearning:\na\nlarge-scale\nface\nstudy."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "arXiv:2105.03790, 2021."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "5. Conclusion",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "[12] Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A Nicolaou,"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "In this work we made use of a multi-task transformer for",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "Athanasios Papaioannou, Guoying Zhao, Bj¨orn Schuller,"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "Irene Kotsia, and Stefanos Zafeiriou. Deep affect prediction"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "jointly predicting arousal/valence, basic emotions and ac-",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "in-the-wild: Aff-wild database and challenge, deep architec-"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "tion units. We further reﬁned our method by modelling the",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "tures, and beyond. International Journal of Computer Vision,"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "different\nlevels of stochasticity for each task. Our method",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "pages 1–23, 2019."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "ﬁnished at the 8th position of the ABAW4 Challenge.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "[13] Dimitrios Kollias and Stefanos Zafeiriou. Va-stargan: Con-"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "tinuous affect generation.\nIn International Conference on"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "6. Acknowledgements",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "Advanced Concepts\nfor\nIntelligent Vision Systems,\npages"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "227–238. Springer, 2020."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "This work was granted access to the HPC resources of",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "[14] Dimitrios Kollias and Stefanos Zafeiriou.\nAffect analysis"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "IDRIS under\nthe allocation 2021-AD011013183 made by",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "in-the-wild: Valence-arousal, expressions, action units and a"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "GENCI.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "uniﬁed framework. arXiv preprint arXiv:2103.15792, 2021."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "[15] Dimitrios Kollias and Stefanos Zafeiriou. Analysing affec-"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "References",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "tive behavior in the second abaw2 competition.\nIn Proceed-"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "ings of\nthe IEEE/CVF International Conference on Com-"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "[1] Didan Deng. Multiple emotion descriptors estimation at the",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "puter Vision, pages 3652–3660, 2021."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "abaw3 challenge, 2022.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "[16] Gauthier Tallec, Edouard Yvinec, Arnaud Dapogny,\nand"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "[2] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "Kevin Bailly. Multi-label\ntransformer for action unit detec-"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "learning using uncertainty to weigh losses for scene geome-",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "tion. arXiv preprint arXiv:2203.12531, 2022."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "try and semantics. In Proceedings of the IEEE conference on",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "[17]\nStefanos Zafeiriou, Dimitrios Kollias, Mihalis A Nicolaou,"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "computer vision and pattern recognition, pages 7482–7491,",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "Athanasios Papaioannou, Guoying Zhao,\nand\nIrene Kot-"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "2018.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "sia. Aff-wild: Valence and arousal\n‘in-the-wild’challenge."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "[3] Dimitrios Kollias.\nAbaw:\nLearning\nfrom synthetic",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "and Pattern Recognition Workshops\nIn Computer Vision"
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "arXiv\npreprint\ndata & multi-task\nlearning\nchallenges.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "(CVPRW), 2017 IEEE Conference on,\npages 1980–1987."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "arXiv:2207.01138, 2022.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": "IEEE, 2017."
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "[4] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "pression\nrecognition,\naction\nunit\ndetection & multi-task",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "the IEEE/CVF Con-\nlearning challenges.\nIn Proceedings of",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "ference on Computer Vision and Pattern Recognition, pages",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "2328–2336, 2022.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "[5] Dimitrios Kollias.\nAbaw: Valence-arousal estimation, ex-",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "pression\nrecognition,\naction\nunit\ndetection & multi-task",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "learning challenges. arXiv preprint arXiv:2202.10659, 2022.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "[6] Dimitrios Kollias, Shiyang Cheng, Maja Pantic, and Stefanos",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "Zafeiriou. Photorealistic facial synthesis in the dimensional",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "affect space.\nIn Proceedings of the European Conference on",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "Computer Vision (ECCV) Workshops, pages 0–0, 2018.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "[7] Dimitrios Kollias,\nShiyang Cheng,\nEvangelos Ververas,",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "Irene Kotsia, and Stefanos Zafeiriou. Deep neural network",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "Interna-\naugmentation: Generating faces for affect analysis.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        },
        {
          "ID\nMixup\nAU F1\nEmotion F1\nVA CCC\nABAW4": "tional Journal of Computer Vision, 128(5):1455–1484, 2020.",
          "[8] Dimitrios Kollias, Mihalis A Nicolaou, Irene Kotsia, Guoy-": ""
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Multiple emotion descriptors estimation at the abaw3 challenge",
      "authors": [
        "Didan Deng"
      ],
      "year": "2022",
      "venue": "Multiple emotion descriptors estimation at the abaw3 challenge"
    },
    {
      "citation_id": "2",
      "title": "Multi-task learning using uncertainty to weigh losses for scene geometry and semantics",
      "authors": [
        "Alex Kendall",
        "Yarin Gal",
        "Roberto Cipolla"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias",
        "Abaw"
      ],
      "year": "2022",
      "venue": "Learning from synthetic data & multi-task learning challenges",
      "arxiv": "arXiv:2207.01138"
    },
    {
      "citation_id": "4",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "5",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "6",
      "title": "Photorealistic facial synthesis in the dimensional affect space",
      "authors": [
        "Dimitrios Kollias",
        "Shiyang Cheng",
        "Maja Pantic",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops"
    },
    {
      "citation_id": "7",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "Dimitrios Kollias",
        "Shiyang Cheng",
        "Evangelos Ververas",
        "Irene Kotsia",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "Dimitrios Kollias",
        "A Mihalis",
        "Irene Nicolaou",
        "Guoying Kotsia",
        "Stefanos Zhao",
        "Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "9",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "10",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "11",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "12",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "13",
      "title": "Va-stargan: Continuous affect generation",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Conference on Advanced Concepts for Intelligent Vision Systems"
    },
    {
      "citation_id": "14",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "15",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Multi-label transformer for action unit detection",
      "authors": [
        "Edouard Gauthier Tallec",
        "Arnaud Yvinec",
        "Kevin Dapogny",
        "Bailly"
      ],
      "year": "2022",
      "venue": "Multi-label transformer for action unit detection",
      "arxiv": "arXiv:2203.12531"
    },
    {
      "citation_id": "17",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    }
  ]
}