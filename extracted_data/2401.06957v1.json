{
  "paper_id": "2401.06957v1",
  "title": "Evoke: Emotion Enabled Virtual Avatar Mapping Using Optimized Knowledge Distillation",
  "published": "2024-01-13T02:52:34Z",
  "authors": [
    "Maryam Nadeem",
    "Raza Imam",
    "Rouqaiah Al-Refai",
    "Meriem Chkir",
    "Mohamad Hoda",
    "Abdulmotaleb El Saddik"
  ],
  "keywords": [
    "emotion recognition",
    "EEG signals",
    "3D avatars",
    "knowledge distillation",
    "wellbeing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "As virtual environments continue to advance, the demand for immersive and emotionally engaging experiences has grown. Addressing this demand, we introduce Emotion enabled Virtual avatar mapping using Optimized KnowledgE distillation (EVOKE), a lightweight emotion recognition framework designed for the seamless integration of emotion recognition into 3D avatars within virtual environments. Our approach leverages knowledge distillation involving multi-label classification on the publicly available DEAP dataset, which covers valence, arousal, and dominance as primary emotional classes. Remarkably, our distilled model, a CNN with only two convolutional layers and 18 times fewer parameters than the teacher model, achieves competitive results, boasting an accuracy of 87% while demanding far less computational resources. This equilibrium between performance and deployability positions our framework as an ideal choice for virtual environment systems. Furthermore, the multi-label classification outcomes are utilized to map emotions onto custom-designed 3D avatars.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Nowadays, there is a growing need for better humancomputer interactions, especially in expressing emotions in virtual environments  [1] . Emotions can be recognized using text, speech, facial expressions, gestures, and physiological signals  [2] . Utilizing EEG signals for emotion recognition has evolved from defining certain sets of emotions to recognizing a wide range of them  [3] -  [5] . Mapping these emotions to avatar expressions in virtual environments can enhance the user experience and facilitate human-computer interactions in various applications such as healthcare and education, leading to advanced digital connectivity. Different frameworks have been introduced for emotion recognition using EEG signals, ranging from traditional machine learning algorithms to more complicated deep learning approaches  [6] ,  [7] . However, deploying these approaches in real-time can be a challenging task because of the huge number of parameters, resource-constrained environments, and the computational power needed to run them. In applications where fast processing is needed, using a lightweight and accelerated approach becomes a priority. Our primary objective is to bridge the gap between emotion recognition in the world of digital avatars while focusing ‡ Authors with equal contributions. on their smooth integration into immersive virtual environments. To achieve this, we propose a knowledge distillationbased framework for multi-label classification that enables the transfer of features from a comparatively larger and more complex model (teacher model) to a smaller, more efficient one (student model). This distilled student model retains much of the teacher's accuracy but operates with significantly reduced computational resources  [8] , making it highly suitable for deployment in resource-constrained environments or real-time applications as illustrated in Fig 1 . \nEVOKE finds meaningful application in the realm of healthcare and emotional well-being. It enables the development of recognition systems that can benefit patients and therapists by establishing a baseline of a patient's typical emotional states through extended EEG data collection, serving as a valuable health indicator. Moreover, visual representations of emotions via avatars enhance communication between therapists and patients, allowing patients to express their emotional experiences more effectively, thereby improving therapy outcomes. This approach also opens doors to virtual therapy and support groups within virtual environments, offering increased accessibility to emotional well-being, particularly for individuals with social anxiety or physical limitations.\n\nThe major contributions of this paper are as follows:\n\n-Proposed lightweight distilled model for EEG-based emotion recognition called EVOKE, which significantly reduces computational parameters by a factor of 18x as compared to the originally trained model while maintaining a comparable performance. -This work introduces a combination Binary Cross En-tropy with Logits Loss and the concept of knowledge distillation for multi-label classification, which to our current understanding has not been explored previously for this task.\n\n-Personalized mapping of the multi-label classification outputs to custom made 3D avatars ready to be deployed in any virtual environment.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "EEG and Emotion Recognition. Research on EEG-based emotion recognition algorithms has been growing rapidly over the past few decades. In 2009, a hierarchical binary classification approach was tested for EEG-based emotion classification  [9] . EEG emotion recognition problems have shown improved performance when utilizing deep learning techniques. For instance, in a recent work, Xiao et al  [10]  introduced a novel method, the four-dimensional attention-based neural network (4D-aNN). It transforms raw EEG signals into 4D spatial-spectral-temporal representations and uses attention mechanisms to assign weights to brain regions and frequency bands. Liu et al.  [11]  introduced a three-dimensional convolutional attention neural network called 3DCANN which extracts dynamic relationships among multi-channel EEG signals over time and fuses spatio-temporal features with attention weights outperforming existing models. Yang et al.  [12]  introduce a 3D representation of EEG segments to amalgamate features from various frequency bands while retaining spatial information among channels using a continuous CNN. Another research highlights attention mechanisms in EEG signal analysis using vision transformer-based methods  [13] .\n\nEmotion Recognition and Digital Avatars. There are two theoretical models for emotions, one is known as the discrete model which has a set of 6 basic emotions initially introduced by  [14]  and it was later expanded to 15 emotions. In contrast, there is the dimensional model that expresses a wide range of emotional states in two or three dimensions. In a multi-dimensional space, emotions are expressed with multiple fundamental features. According to Russell et al.  [15] , emotions are mapped to two different dimensions, valence and arousal. While 2D model can span many emotions, it can struggle when the valence and arousal have the same levels. Therefore, to address this, Russell and Mehrabian  [16] ,  [17]  introduced dominance as a third dimension for differentiation. These emotion schemes were used mainly for devoloping emotion recognition datasets but they can also be utilized for connecting the research with real world applications. For instance, in 2010, a fractal dimension-based algorithm for real-time EEG-based emotion recognition visualizing emotions through Haptek avatars was introduced  [18] . While this emotion mapping to digital avatars has its applications, they lack realism, expressiveness, and adaptability to virtual environment.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Methodology",
      "text": "A. Preprocessing 1) Signal: In the preprocessing of EEG signals, several steps were conducted to enhance data quality. Initially the original data was downsampled to 128Hz and artifacts from eye movements (EOG) were eliminated following established procedures  [19] . Moreover, a bandpass frequency filter, spanning 4.0Hz to 45.0Hz, was applied to isolate relevant EEG frequencies. The data was subsequently averaged to establish a common reference baseline. To ensure consistent channel ordering, the EEG channels were rearranged according to the prescribed Geneva order. The data was then segmented into 60-second trials, with a preceding of 3-second pre-trial baseline removal (see Fig.  2  (2)). Furthermore, to enhance data coherence and relevance, the trial sequences were reorganized to align with experiments ID. In order to extract relevant features from raw EEG signals, differential entropy (DE) denoted as (h(X)) was employed to construct features in four frequency bands (alpha : 8 -14Hz, beta : 14 -31Hz, gamma : 31 -49Hz, theta : 4 -8Hz)  [20] . Assuming a signal X follows a probability Gaussian distribution f (X) ∼ N (µ, σ 2 ), then;\n\n. Further, we removed the baseline signal and noise interference that was not associated with the emotional stimulus. To retain the spatial information and ensuring model compatibility, the signals were transformed into a grid-like 3D representation to be aligned with the positions of the electrodes.\n\n2) Label: We applied a binary transformation to the emotional values using a consistent threshold of 5.0 for all categories (see Fig.  2  (1)), including valence, arousal, and dominance. This step streamlined the subsequent correlation into eight distinct emotions after the classification process is completed. Labels will be set to 1 if their value is larger than the threshold and set to 0 if it is smaller. This procedure was implemented to streamline the classification process.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Knowledge-Distillation",
      "text": "To develop a readily deployable model, we incorporated knowledge distillation into the training process. Following the foundational methodology introduced by Hinton et al.  [21] , and making certain adaptations to the implementation for our specific task.\n\nUnderstanding the similarity between instances is crucial in comprehending the knowledge acquired by neural networks, especially in the context of multi-label classification of EEG signals into valence, arousal, and dominance. This is vital because gaining insights into how EEG patterns corresponds to these emotions interrelate can significantly enhance system accuracy. In the standard sigmoid activation function, the output (σ(x)) is a hard binary decision, mapping inputs to either 0 or 1. It's a step-like function with a sharp transition at x = 0. To mitigate this, distillation incorporates a temperature parameter, denoted as T , into the sigmoid activation (g), the output is softened.\n\nWhen T is set to 1, it corresponds to the standard sigmoid operation. This temperature-based operation is represented as:\n\nHere, z i represents the logit for each class, and q i transforms the teacher logits into probabilities, with T controlling the level of smoothness. During knowledge distillation, the teacher imparts its knowledge in the form of soft targets, calculated using this modified sigmoid with T > 1. If v i represents the logits by the student model then the student probabilities are represented as p i .\n\nThe knowledge distillation process combines two distinct objective functions, L 1 and L 2 . The first objective function, L 1 , referred to as the soft target loss, captures the knowledge in the soft targets provided by the teacher model. In our case, it calculates the Binary Cross-Entropy (BCE) with logits loss between the teacher's soft targets (q i ) and the student's predictions (p i ), scaled by the square of the temperature (T ).\n\nIf N denotes the number of samples in the dataset and C denotes the number of emotions, then,\n\nThe second objective function, L 2 , aims to align the student's predictions (S i ) with the true labels (Y i ), while referencing the soft targets (q i ) from the teacher model,\n\nThe aim is to minimize the student model's distillation loss, L distill , which combines L 1 and L 2 with a weighting factor α:\n\nThis combined loss ensures that the student model effectively captures the knowledge distilled from the teacher model while maintaining alignment with the true labels. Fig.  2  (3), (4), and (5) depict the process discussed above.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Model Architectures 1) Teacher:",
      "text": "We employed Continuous Convolutional Neural Network (CCNN)  [12]  architecture as the teacher model for distillation, which consists of four convolutional layers with no pooling layers between them. The first three convolutional layers uses a 4x4 kernel size and a stride of 1. After each To fuse different feature maps and reduce computational cost a 1x1 convolutional layer with 64 feature maps was added. Following these four continuous convolutional layers, a fully connected layer to map the (64x9x9) feature maps into a final feature vector of size 1x1024 is added, followed by a final softmax layer for classification.\n\n2) Student: The student model, a lightweight neural network, designed for multi-label classification of EEG signals into valence, arousal, and dominance, employs a 2 convolution layered architecture. The input, denoted as Z, represents EEG signal data organized in a grid-like fashion with shape [n,  4, 9, 9] , where n signifies the batch size, 4 indicates the number of input channels corresponding to different electrodes, and (9, 9) denotes the grid size. The model comprises two primary components: feature extraction and classification. The feature extraction phase consists of two convolutional layers, c 1 and c 2 , followed by Rectified Linear Unit (ReLU) activation functions. These layers process the input EEG data, producing intermediate outputs Z (1) and Z (2) , respectively. The flattening operation then transforms Z (2) into a onedimensional tensor Z (3) . Let G represent the classifier or the MLP, then, the feature representation, denoted as Z (3) is passed through G to predict the output labels:\n\nIn our specific case, this classifier enables multi-label classification for the 3 umbrella classes, namely valence (y valence ), arousal (y arousal ), and dominance (y dominance ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiments And Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Dataset",
      "text": "The DEAP dataset, developed by Koelstra et al.  [19] , was employed for our study. It comprises EEG signals from 32 subjects watching a series of 40 one-minute music videos, each accompanied by emotional response ratings on scales of arousal, valence, dominance, liking, and familiarity. In our study, we focus exclusively on arousal, valence, and dominance, as liking and familiarity are more related to individual perspectives  [19] . DEAP's 32-channel EEG data collection during emotional stimuli distinguishes it in EEG emotion recognition research, offering richer features for improved emotion state differentiation. Consequently, it serves as the exclusive dataset in our research.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Implementation Details",
      "text": "The optimization process employed the Adam optimizer with a learning rate set to 1 × 10 -3 . ReLU activation layers were used for the student model. A consistent batch size of 128 was applied across all models. The training phase extended for 100 epochs and encompassed a 5-fold cross-validation strategy. The entire experimentation process was conducted within the PyTorch framework  [23] , utilizing a single Nvidia RTX A6000 GPU with 40 GB of memory.\n\nFor creating 3D avatars, we used Character Creator 4's 1  Headshot plugin to turn a character's image into a 3D face mask. This involved adjusting expressions, hair color, and skin tone. Then, in Blender  [24] , we created clothing based on the 3D avatar. Importing it back to Character Creator ensured proper rigging. After fine-tuning, we had a lifelike 3D avatar, blending real-world aesthetics with virtual artistry. Headshot, a key feature of Character Creator, analyzed real-life photos to generate a detailed 3D face, accounting for contours, skin texture, and expressions.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Model Selection And Pretraining",
      "text": "We conducted a meticulous evaluation of three state-of-theart Emotion-Recognition models: Arjun ViT  [13] , ViT  [22] , and CCNN  [12] . We started with a clean slate, training these models entirely from scratch without the application of any knowledge distillation techniques. Our objective was to select the optimal candidate from these pretrained models to serve as the teacher model for our framework. For model evaluation, we employed accuracy and F1-score, which offer comprehensive insights into a model's performance in predicting multiple labels  [25] . These metrics were derived from the mean values obtained through five-fold cross-validation.\n\nThe models were first evaluated under two distinct EEGbased feature extraction techniques: Differential Entropy (DE) and Power Spectral Density (PSD) as presented in Table  I . Notably, the results demonstrate the superior performance of CCNN across both the evaluation metrics. This can be attributed to several key factors; firstly, the transformer models, although meticulously customized for EEG data, are inherently data-hungry and struggled to harness the full potential of the dataset and exhibited comparatively limited generalization capabilities. Secondly, the unique nature of EEG data, which includes spatial information pertaining to the arrangement of electrodes on the scalp, presents a significant advantage for CCNN. This spatial awareness empowers CCNN to extract fine-grained details from the EEG data, effectively capturing the nuances associated with emotional states. All subsequent experiments were conducted with CCNN as the teacher. For student model, we experimented various architectures for our custom CNN model, discovering that an 8-layer design with two convolutional layers was the most suitable.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Computation And Performance",
      "text": "Our distilled model, EVOKE, stands out with significantly fewer parameters (18x lesser than the teacher model) while achieving the fastest inference time of 0.33 ms and highest throughput of 80176 compared to other models (see Table  I ). It's performance surpasses that of ViT and ArjunViT, comparable with the performance of the teacher model. This compelling balance between performance and deployability makes this framework suitable for virtual environment systems. Note that we omitted the Power Spectral Density (PSD) feature extraction technique during EVOKE's training due to inferior results compared to those of differential entropy as shown in Table  I . Additionally, we conducted experiments involving various temperature parameter (T ) values, as shown in Figure  4",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Emotion Mapping To 3D Avatars",
      "text": "The three-dimensional emotions model namely Valence-Arousal-Dominance or VAD model for short, includes the basic emotions  [14]  defined by the rating of each dimension. For instance, the closer the rating of each dimension to zero the lower the emotion distinction. Which means as shown in Fig.  3  when all categories are low or zero then the emotion is neutral. Hence, in this paper, we have developed a set of combinations and their corresponding emotion mappings that bridge the gap between emotion classification and its representation in 3D avatars. Unlike some prior approaches  [18] , which utilized the 2D Valence-Arousal emotion model, our work incorporates an additional dimension of dominance. This additional dimension allows for a broader range of emotions, including neutral and excited states, alongside the six fundamental emotions. The number of emotions is limited intentionally to maintain focus and manageability, especially concerning the emotion mapping onto avatars. The combinations and their emotion mapping are shown in Fig.  3  where 0 indicates the low level of the primary emotion and 1 indicate having a high level for that emotion.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion And Future Work",
      "text": "In response to the practical deployment of real-time EEG emotion classification, we introduced EVOKE, a knowledge distillation-based lightweight model for emotion recognition and integration into virtual environments. The framework maps multi-label classification results to eight distinct emotions and links them to custom-created 3D avatars. When tested on the publicly available DEAP EEG dataset, our proposed model achieved competitive performance along with significantly fewer parameters as compared to other state-ofthe-art models. This work paves the way for enhanced emotional communication in virtual settings, offering numerous applications in healthcare, therapy, and beyond, ultimately making emotional well-being more accessible and immersive.\n\nOne notable application is in Virtual Therapy Sessions:\n\n-Scenario: In virtual therapy sessions, EVOKE can be utilized to enhance the emotional interaction between therapists and clients by mapping real-time emotional states onto virtual avatars. -Application: As clients express their feelings and emotions, EVOKE processes the emotional cues through its knowledge-distilled model, providing therapists with a visual representation of emotional changes in the virtual avatars. This visual feedback aids therapists in understanding and responding empathetically to the client's emotional state. In the future, we aim to integrate real-time EEG signals into a healthcare virtual environment system, making it readily accessible for use by healthcare professionals.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Inference speed of teacher and student models. Distilled student model",
      "page": 1
    },
    {
      "caption": "Figure 1: EVOKE finds meaningful application in the realm of health-",
      "page": 1
    },
    {
      "caption": "Figure 2: (2)). Furthermore, to enhance data",
      "page": 2
    },
    {
      "caption": "Figure 2: (1)), including valence, arousal, and",
      "page": 2
    },
    {
      "caption": "Figure 2: Our proposed framework, EVOKE. (1) We input the raw EEG signals to the framework which initially goes for preprocessing and label thresholding.",
      "page": 3
    },
    {
      "caption": "Figure 3: Multi-label classification results are mapped to eight emotions based",
      "page": 4
    },
    {
      "caption": "Figure 4: Performance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T) (a) and (c) parameter and weight factor",
      "page": 5
    },
    {
      "caption": "Figure 4: (a) and (c). Our findings indicate that the model performs",
      "page": 5
    },
    {
      "caption": "Figure 4: (b) and (d)). It was observed that excessively",
      "page": 5
    },
    {
      "caption": "Figure 4: , the plots illustrate the model’s performance during",
      "page": 5
    },
    {
      "caption": "Figure 3: when all categories are low or zero then the emotion is neutral.",
      "page": 5
    },
    {
      "caption": "Figure 3: where 0 indicates the low level of the primary",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "Fig. 1.\nInference speed of teacher and student models. Distilled student model"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "can be deployed for\nreal-time applications, as smaller models are faster with"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "lower\ninference times."
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "on their\nsmooth integration into immersive virtual\nenviron-"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "ments. To achieve this, we propose a knowledge distillation-"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "based framework for multi-label classification that enables the"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "transfer\nof\nfeatures\nfrom a\ncomparatively\nlarger\nand more"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "complex model (teacher model) to a smaller, more efficient one"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "(student model). This distilled student model\nretains much of"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "the teacher’s accuracy but operates with significantly reduced"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "computational\nresources\n[8], making\nit\nhighly\nsuitable\nfor"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "deployment\nin resource-constrained environments or real-time"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "applications as illustrated in Fig 1."
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "EVOKE finds meaningful application in the realm of health-"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "care and emotional well-being.\nIt enables the development of"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "recognition systems that can benefit patients and therapists by"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "establishing a baseline of a patient’s typical emotional states"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "through extended EEG data collection, serving as a valuable"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "health indicator. Moreover, visual representations of emotions"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "via\navatars\nenhance\ncommunication\nbetween\ntherapists\nand"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "patients, allowing patients\nto express\ntheir emotional experi-"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "ences more effectively,\nthereby improving therapy outcomes."
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "This approach also opens doors to virtual\ntherapy and support"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "groups within virtual environments, offering increased accessi-"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "bility to emotional well-being, particularly for individuals with"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "social anxiety or physical\nlimitations."
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "The major contributions of\nthis paper are as follows:"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "– Proposed lightweight distilled model for EEG-based emo-"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "tion recognition called EVOKE, which significantly re-"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "duces\ncomputational parameters by a\nfactor of 18x as"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "compared to the originally trained model while maintain-"
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "ing a comparable performance."
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": ""
        },
        {
          "{maryam.nadeem, raza.imam, rouqaiah.al-refai, meriem.chkir, mohamad.hoda, a.elsaddik}@mbzuai.ac.ae": "– This work introduces\na\ncombination Binary Cross En-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "distillation\nfor multi-label\nclassification, which\nto\nour",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "A. Preprocessing"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "current understanding has not been explored previously",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "1)\nSignal:\nIn\nthe\npreprocessing\nof EEG signals,\nseveral"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "for\nthis task.",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "steps were\nconducted\nto\nenhance\ndata\nquality.\nInitially\nthe"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "– Personalized mapping\nof\nthe multi-label\nclassification",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "original data was downsampled to 128Hz and artifacts\nfrom"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "outputs to custom made 3D avatars ready to be deployed",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "eye movements (EOG) were eliminated following established"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "in any virtual environment.",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "procedures [19]. Moreover, a bandpass frequency filter, span-"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "ning 4.0Hz\nto 45.0Hz, was\napplied to isolate\nrelevant EEG"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "frequencies. The data was subsequently averaged to establish"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "a\ncommon reference baseline. To ensure\nconsistent\nchannel"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "II. RELATED WORKS",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "ordering,\nthe EEG channels were\nrearranged\naccording\nto"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "the prescribed Geneva order. The data was\nthen segmented"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "EEG and Emotion Recognition. Research on EEG-based",
          "III. METHODOLOGY": "into 60-second trials, with a preceding of 3-second pre-trial"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "emotion\nrecognition\nalgorithms\nhas\nbeen\ngrowing\nrapidly",
          "III. METHODOLOGY": "baseline removal (see Fig. 2 (2)). Furthermore, to enhance data"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "over\nthe\npast\nfew decades.\nIn\n2009,\na\nhierarchical\nbinary",
          "III. METHODOLOGY": "coherence and relevance,\nthe trial sequences were reorganized"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "classification\napproach was\ntested\nfor EEG-based\nemotion",
          "III. METHODOLOGY": "to\nalign with\nexperiments\nID.\nIn\norder\nto\nextract\nrelevant"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "classification\n[9]. EEG emotion\nrecognition\nproblems\nhave",
          "III. METHODOLOGY": "features\nfrom raw EEG signals,\ndifferential\nentropy\n(DE)"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "shown\nimproved\nperformance when\nutilizing\ndeep\nlearning",
          "III. METHODOLOGY": "denoted\nas\n(h(X)) was\nemployed\nto\nconstruct\nfeatures\nin"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "techniques. For instance,\nin a recent work, Xiao et al [10] in-",
          "III. METHODOLOGY": "four\nfrequency bands\n(alpha : 8 − 14Hz, beta : 14 − 31Hz,"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "troduced a novel method, the four-dimensional attention-based",
          "III. METHODOLOGY": "gamma : 31 − 49Hz,\ntheta : 4 − 8Hz)\n[20]. Assuming\na"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "neural network (4D-aNN). It\ntransforms raw EEG signals into",
          "III. METHODOLOGY": "signal X follows a probability Gaussian distribution f (X) ∼"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "4D spatial-spectral-temporal representations and uses attention",
          "III. METHODOLOGY": "N (µ, σ2),\nthen;"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "mechanisms to assign weights to brain regions and frequency",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "(cid:90) ∞"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "bands. Liu et al. [11] introduced a three-dimensional convolu-",
          "III. METHODOLOGY": "h(X) = −\nf (X) log(f (X))dx"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "tional attention neural network called 3DCANN which extracts",
          "III. METHODOLOGY": "−∞"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "√"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "dynamic relationships among multi-channel EEG signals over",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "2πσ2) exp(−(x − µ)2/2σ2). Further, we\nwhere f (X) = (1/"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "time and fuses spatio-temporal features with attention weights",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "removed the baseline signal and noise interference that was not"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "outperforming existing models. Yang et al. [12] introduce a 3D",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "associated with the emotional\nstimulus. To retain the spatial"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "representation of EEG segments to amalgamate features from",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "information and ensuring model compatibility, the signals were"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "various\nfrequency bands while\nretaining spatial\ninformation",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "transformed into a grid-like 3D representation to be aligned"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "among channels using a continuous CNN. Another\nresearch",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "with the positions of\nthe electrodes."
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "highlights attention mechanisms in EEG signal analysis using",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "2) Label: We applied a binary transformation to the emo-"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "vision transformer-based methods [13].",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "tional\nvalues\nusing\na\nconsistent\nthreshold\nof\n5.0\nfor\nall"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "Emotion Recognition\nand Digital Avatars. There\nare",
          "III. METHODOLOGY": "categories\n(see Fig.\n2\n(1)),\nincluding\nvalence,\narousal,\nand"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "two\ntheoretical models\nfor\nemotions,\none\nis\nknown\nas\nthe",
          "III. METHODOLOGY": "dominance. This\nstep streamlined the subsequent correlation"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "discrete model which has a set of 6 basic emotions\ninitially",
          "III. METHODOLOGY": "into eight distinct emotions after\nthe classification process is"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "introduced by [14] and it was later expanded to 15 emotions.",
          "III. METHODOLOGY": "completed. Labels will be set\nto 1 if their value is larger than"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "In contrast,\nthere is\nthe dimensional model\nthat expresses a",
          "III. METHODOLOGY": "the threshold and set\nto 0 if\nit\nis smaller. This procedure was"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "wide\nrange of\nemotional\nstates\nin two or\nthree dimensions.",
          "III. METHODOLOGY": "implemented to streamline the classification process."
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "In\na multi-dimensional\nspace,\nemotions\nare\nexpressed with",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "",
          "III. METHODOLOGY": "B. Knowledge-Distillation"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "multiple fundamental features. According to Russell et al. [15],",
          "III. METHODOLOGY": ""
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "emotions are mapped to two different dimensions, valence and",
          "III. METHODOLOGY": "To develop a\nreadily deployable model, we\nincorporated"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "arousal. While\n2D model\ncan\nspan many\nemotions,\nit\ncan",
          "III. METHODOLOGY": "knowledge distillation into the training process. Following the"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "struggle when the valence and arousal have the same levels.",
          "III. METHODOLOGY": "foundational methodology introduced by Hinton et\nal.\n[21],"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "Therefore,\nto address\nthis, Russell and Mehrabian [16],\n[17]",
          "III. METHODOLOGY": "and making certain adaptations to the implementation for our"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "introduced dominance as a third dimension for differentiation.",
          "III. METHODOLOGY": "specific task."
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "These\nemotion\nschemes were\nused mainly\nfor\ndevoloping",
          "III. METHODOLOGY": "Understanding the similarity between instances is crucial\nin"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "emotion\nrecognition\ndatasets\nbut\nthey\ncan\nalso\nbe\nutilized",
          "III. METHODOLOGY": "comprehending the knowledge acquired by neural networks,"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "for connecting the research with real world applications. For",
          "III. METHODOLOGY": "especially in the context of multi-label classification of EEG"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "instance,\nin\n2010,\na\nfractal\ndimension-based\nalgorithm for",
          "III. METHODOLOGY": "signals\ninto\nvalence,\narousal,\nand\ndominance. This\nis\nvital"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "real-time EEG-based\nemotion\nrecognition\nvisualizing\nemo-",
          "III. METHODOLOGY": "because gaining insights\ninto how EEG patterns corresponds"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "tions\nthrough Haptek\navatars was\nintroduced\n[18]. While",
          "III. METHODOLOGY": "to these emotions interrelate can significantly enhance system"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "this emotion mapping to digital avatars has\nits applications,",
          "III. METHODOLOGY": "accuracy.\nIn\nthe\nstandard\nsigmoid\nactivation\nfunction,\nthe"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "they lack realism, expressiveness, and adaptability to virtual",
          "III. METHODOLOGY": "output\n(σ(x))\nis\na hard binary decision, mapping inputs\nto"
        },
        {
          "tropy with Logits Loss\nand the\nconcept of knowledge": "environment.",
          "III. METHODOLOGY": "either 0 or 1. It’s a step-like function with a sharp transition at"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "(2) Preprocessing includes stages of",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "feature extraction using differential entropy to have 4 channel bands followed by noise reduction resulting into 3D grid"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "input.\n(3) Features input\nto teacher CCNN generates soft",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "labels with Temperature (T ) > 1 which calculates the Binary Cross-Entropy with logits loss (L1)."
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "(4) Final distillation loss (Ldistill) is a weighted combination of soft",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "target\ntrained on the\nloss (L1) and final\nloss (L2) (refer Eq. 1). (5) The distilled model"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "soft predictions of\nteacher model",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "is then (6) deployed for\nfast\ninference and real-time applications."
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "x = 0. To mitigate this, distillation incorporates a temperature",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "If N denotes\nthe number of\nsamples\nin the dataset\nand C"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "parameter, denoted as T ,",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "into the sigmoid activation (g),\nthe\ndenotes the number of emotions,\nthen,"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "output\nis softened.",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": ""
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "1 N\nN(cid:88) i\nC(cid:88) j\nL1 = −\n[qi,j log(pi,j) + (1 − qi,j) log(1 − pi,j)]·T 2"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "(cid:40)",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": ""
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "0",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "=1\n=1\nif x/T → 0+"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "g(x) =",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": ""
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "1",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "if x/T → 0−"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "The second objective function, L2, aims to align the student’s"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "predictions (Si) with the true labels (Yi), while referencing the"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "When T is set\nto 1,",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": "it corresponds to the standard sigmoid\nsoft\nfrom the teacher model,\ntargets (qi)"
        },
        {
          "Fig. 2. Our proposed framework, EVOKE. (1) We input": "operation. This temperature-based operation is represented as:",
          "the raw EEG signals to the framework which initially goes for preprocessing and label\nthresholding.": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": ""
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "Inference (ms)"
        },
        {
          "TABLE I": "6.96"
        },
        {
          "TABLE I": "1.6691"
        },
        {
          "TABLE I": "0.6414"
        },
        {
          "TABLE I": "0.3300"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "CCNN [12]\n12\n6.235M\n79.961M\n23.79MB\n0.6414"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "EVOKE (Ours)\n8\n353.363K\n1.991M\n1.35MB\n0.3300"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "convolution operation, ReLU activation function was applied."
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "To fuse different\nfeature maps and reduce computational cost"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "a 1x1 convolutional\nlayer with 64 feature maps was added."
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "Following these four continuous convolutional\nlayers, a fully"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "connected layer to map the (64x9x9) feature maps into a final"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "feature vector of\nsize 1x1024 is\nadded,\nfollowed by a final"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "softmax layer\nfor classification."
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "2)\nStudent:\nThe\nstudent model,\na\nlightweight neural net-"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "work, designed for multi-label classification of EEG signals"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "into valence, arousal, and dominance, employs a 2 convolution"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "layered\narchitecture.\nThe\ninput,\ndenoted\nas Z,\nrepresents"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "EEG signal data organized in a grid-like fashion with shape"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "[n, 4, 9, 9], where n signifies\nthe batch size, 4 indicates\nthe"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "number\nof\ninput\nchannels\ncorresponding\nto\ndifferent\nelec-"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "trodes, and (9, 9) denotes the grid size. The model comprises"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "two primary components: feature extraction and classification."
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "The\nfeature\nextraction\nphase\nconsists\nof\ntwo\nconvolutional"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "followed by Rectified Linear Unit\n(ReLU)\nlayers, c1 and c2,"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "activation functions. These layers process the input EEG data,"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "producing intermediate outputs Z (1)\nand Z (2),\nrespectively."
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "The\nflattening\noperation\nthen\ntransforms Z (2)\ninto\na\none-"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "dimensional\ntensor Z (3). Let G represent\nthe\nclassifier\nor"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "the MLP,\nthen,\nthe feature representation, denoted as Z (3)\nis"
        },
        {
          "Arjun ViT [13]\n41\n144.356K\n767.592K\n587.35KB\n1.6691": "passed through G to predict\nthe output\nlabels:"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "yarousal = G(Z (3)) ∈ {0, 1}"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "ydominance = G(Z (3)) ∈ {0, 1}"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "In our specific case,\nthis classifier enables multi-label classi-"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "fication for\nthe 3 umbrella classes, namely valence (yvalence),"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "arousal\n(yarousal), and dominance (ydominance)."
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "IV. EXPERIMENTS AND RESULTS"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "A. Dataset"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "The DEAP dataset, developed by Koelstra et al.\n[19], was"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "employed for our\nstudy.\nIt\ncomprises EEG signals\nfrom 32"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "subjects watching a\nseries of 40 one-minute music videos,"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "each accompanied by emotional\nresponse\nratings on scales"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "of arousal, valence, dominance,\nliking, and familiarity.\nIn our"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "study, we\nfocus\nexclusively on arousal, valence,\nand domi-"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "nance, as liking and familiarity are more related to individual"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "perspectives\n[19]. DEAP’s\n32-channel EEG data\ncollection"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "during\nemotional\nstimuli\ndistinguishes\nit\nin EEG emotion"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "recognition\nresearch,\noffering\nricher\nfeatures\nfor\nimproved"
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": ""
        },
        {
          "yvalence = G(Z (3)) ∈ {0, 1}": "emotion state differentiation. Consequently,\nit\nserves\nas\nthe"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "α (see Eq. 1) (b) and (d). Note that\nthe accuracy and F1 scores presented in the figures are based on the mean values obtained from a 5-fold cross-validation"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "evaluation."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "the 3D avatar. Importing it back to Character Creator ensured\nachieving the fastest\ninference time of 0.33 ms and highest"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "proper\nrigging. After fine-tuning, we had a lifelike 3D avatar,\nthroughput of 80176 compared to other models (see Table I)."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "blending real-world aesthetics with virtual artistry. Headshot,\nIt’s performance surpasses that of ViT and ArjunViT, compa-"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "a key feature of Character Creator, analyzed real-life photos\nrable with the performance of\nthe teacher model. This com-"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "pelling balance between performance and deployability makes\nto generate a detailed 3D face, accounting for contours, skin"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "this framework suitable for virtual environment systems. Note\ntexture, and expressions."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "that we\nomitted\nthe Power Spectral Density\n(PSD)\nfeature"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "C. Model Selection and Pretraining"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "extraction technique during EVOKE’s training due to inferior"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "We conducted a meticulous evaluation of three state-of-the-"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "results\ncompared\nto\nthose\nof\ndifferential\nentropy\nas\nshown"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "art Emotion-Recognition models: Arjun ViT [13], ViT [22],"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "in Table I. Additionally, we conducted experiments involving"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "and CCNN [12]. We started with a clean slate,\ntraining these"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "various temperature parameter (T ) values, as shown in Figure"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "models entirely from scratch without\nthe application of any"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "4 (a) and (c). Our findings\nindicate that\nthe model performs"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "knowledge distillation techniques. Our objective was to select"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "optimally when T is set\nto 1.25. Notably, performance starts"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "the optimal candidate from these pretrained models to serve as"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "to decline gradually for T\nvalues\nexceeding 2. Further, we"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "the teacher model for our framework. For model evaluation, we"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "also explored different values of α,\nthe weight factor in Eq. 1."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "employed accuracy and F1-score, which offer comprehensive"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "The model achieved its best performance when α was set\nto"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "insights\ninto\na model’s\nperformance\nin\npredicting multiple"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "0.25 (see Fig 4 (b) and (d)).\nIt was observed that excessively"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "labels [25]. These metrics were derived from the mean values"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "large or small α values resulted in a decreased performance."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "obtained through five-fold cross-validation."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "In Fig. 4,\nthe plots illustrate the model’s performance during"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "The models were first evaluated under\ntwo distinct EEG-"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "the initial 50 epochs."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "based feature extraction techniques: Differential Entropy (DE)"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "and Power Spectral Density (PSD)\nas presented in Table\nI."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "E. Emotion Mapping to 3D Avatars"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "Notably,\nthe\nresults\ndemonstrate\nthe\nsuperior\nperformance"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "The\nthree-dimensional\nemotions model\nnamely Valence-"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "of CCNN across\nboth\nthe\nevaluation metrics. This\ncan\nbe"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "Arousal-Dominance\nor VAD model\nfor\nshort,\nincludes\nthe"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "attributed to several key factors; firstly, the transformer models,"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "basic emotions [14] defined by the rating of each dimension."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "although meticulously customized for EEG data, are inherently"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "For instance, the closer the rating of each dimension to zero the"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "data-hungry\nand\nstruggled\nto\nharness\nthe\nfull\npotential\nof"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "lower the emotion distinction. Which means as shown in Fig. 3"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "the dataset and exhibited comparatively limited generalization"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "when all categories are low or zero then the emotion is neutral."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "capabilities. Secondly,\nthe unique nature of EEG data, which"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "Hence,\nin this paper, we have developed a set of combinations"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "includes spatial\ninformation pertaining to the arrangement of"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "and their corresponding emotion mappings that bridge the gap"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "electrodes on the scalp, presents a significant advantage for"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "between emotion classification and its\nrepresentation in 3D"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "CCNN. This\nspatial\nawareness\nempowers CCNN to extract"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "avatars. Unlike some prior approaches [18], which utilized the"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "fine-grained details\nfrom the EEG data, effectively capturing"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "2D Valence-Arousal emotion model, our work incorporates an"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "the nuances associated with emotional states. All subsequent"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "additional dimension of dominance. This additional dimension"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "experiments were conducted with CCNN as\nthe teacher. For"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "allows for a broader range of emotions,\nincluding neutral and"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "student model, we experimented various architectures for our"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "excited states,\nalongside\nthe\nsix fundamental\nemotions. The"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "custom CNN model, discovering that an 8-layer design with"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "number of emotions is limited intentionally to maintain focus"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "two convolutional\nlayers was the most suitable."
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "and manageability, especially concerning the emotion mapping"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "D. Computation and Performance"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "onto avatars. The combinations and their emotion mapping are"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "shown in Fig. 3 where 0 indicates the low level of the primary\nOur distilled model, EVOKE, stands out with significantly"
        },
        {
          "Fig. 4.\nPerformance analysis in terms of accuracy and F1 score, respectively, across various values of temperature (T ) (a) and (c) parameter and weight factor": "emotion and 1 indicate having a high level\nfor\nthat emotion.\nfewer parameters\n(18x lesser\nthan the\nteacher model) while"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Quansheng Ren.\n4d attention-based neural network for\neeg emotion"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "In response to the practical deployment of\nreal-time EEG",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "recognition. Cognitive Neurodynamics, pages 1–14, 2022."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "emotion classification, we introduced EVOKE, a knowledge",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[11]\nShuaiqi Liu, Xu Wang, Ling Zhao, Bing Li, Weiming Hu, Jie Yu, and"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Yu-Dong Zhang. 3dcann: A spatio-temporal convolution attention neural"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "distillation-based lightweight model\nfor\nemotion recognition",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "IEEE Journal of Biomedical and\nnetwork for eeg emotion recognition."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "and\nintegration\ninto\nvirtual\nenvironments.\nThe\nframework",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Health Informatics, 26(11):5321–5331, 2021."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "maps multi-label classification results\nto eight distinct emo-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[12] Yilong Yang, Qingfeng Wu, Yazhen Fu, and Xiaowei Chen. Continuous"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "convolutional\nneural\nnetwork with\n3d\ninput\nfor\neeg-based\nemotion"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "tions\nand\nlinks\nthem to\ncustom-created\n3D avatars. When",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Information Processing:\n25th\nInternational\nrecognition.\nIn Neural"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "tested\non\nthe\npublicly\navailable DEAP EEG dataset,\nour",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Conference,\nICONIP 2018, Siem Reap, Cambodia, December 13–16,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "proposed model achieved competitive performance along with",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "2018, Proceedings, Part VII 25, pages 433–443. Springer, 2018."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[13] Arjun Arjun, Aniket Singh Rajpoot, and Mahesh Raveendranatha Pan-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "significantly fewer parameters as compared to other state-of-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "icker.\nIntroducing attention mechanism for eeg signals: Emotion recog-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "the-art models. This work paves\nthe way for enhanced emo-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "2021\n43rd Annual\nInternational\nnition with\nvision\ntransformers.\nIn"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "tional\ncommunication in virtual\nsettings, offering numerous",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Conference of\nthe\nIEEE Engineering in Medicine & Biology Society"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "(EMBC), pages 5723–5726.\nIEEE, 2021."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "applications\nin\nhealthcare,\ntherapy,\nand\nbeyond,\nultimately",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[14]\nPaul Ekman, E Richard Sorenson, and Wallace V Friesen. Pan-cultural"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "making emotional well-being more accessible and immersive.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "elements in facial displays of emotion. Science, 164(3875):86–88, 1969."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "One notable application is in Virtual Therapy Sessions:",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[15]\nJames A Russell. A circumplex model of affect. Journal of personality"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "and social psychology, 39(6):1161, 1980."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "– Scenario:\nIn\nvirtual\ntherapy\nsessions, EVOKE can\nbe",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[16]\nJames A Russell\nand Albert Mehrabian.\nEvidence\nfor\na\nthree-factor"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "utilized\nto\nenhance\nthe\nemotional\ninteraction\nbetween",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "theory of emotions.\nJournal of research in Personality, 11(3):273–294,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "1977."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "therapists\nand\nclients\nby mapping\nreal-time\nemotional",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[17] Albert Mehrabian.\nPleasure-arousal-dominance: A general\nframework"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "states onto virtual avatars.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "for\ndescribing\nand measuring\nindividual\ndifferences\nin\ntemperament."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "– Application: As clients express their\nfeelings and emo-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Current Psychology, 14:261–292, 1996."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[18] Yisi Liu, Olga Sourina, and Minh Khoa Nguyen. Real-time eeg-based"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "tions, EVOKE processes\nthe emotional cues\nthrough its",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "human emotion recognition and visualization.\nIn 2010 international"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "knowledge-distilled model,\nproviding\ntherapists with\na",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "conference on cyberworlds, pages 262–269.\nIEEE, 2010."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "visual\nrepresentation of emotional changes in the virtual",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[19]\nSander Koelstra, Christian Muhl, Mohammad Soleymani,\nJong-Seok"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "avatars. This\nvisual\nfeedback\naids\ntherapists\nin\nunder-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "and Ioannis Patras. Deap: A database for emotion analysis; using physi-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "standing\nand\nresponding\nempathetically\nto\nthe\nclient’s",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "ological signals.\nIEEE transactions on affective computing, 3(1):18–31,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "emotional state.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "2011."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[20] Ruo-Nan Duan,\nJia-Yi Zhu,\nand Bao-Liang Lu.\nDifferential\nentropy"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "In the\nfuture, we\naim to integrate\nreal-time EEG signals",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "feature for eeg-based emotion classification.\nIn 2013 6th International"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "into a healthcare virtual environment system, making it readily",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "IEEE/EMBS Conference on Neural Engineering (NER), pages 81–84."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "IEEE, 2013."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "accessible for use by healthcare professionals.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "in a neural network. arXiv preprint arXiv:1503.02531, 2015."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "REFERENCES",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "senborn,\nXiaohua\nZhai,\nThomas\nUnterthiner, Mostafa\nDehghani,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[1] Roddy Cowie, Ellen Douglas-Cowie, Nicolas Tsapatsoulis, George Vot-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Matthias Minderer, Georg Heigold, Sylvain Gelly,\net\nal.\nAn\nimage"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "sis, Stefanos Kollias, Winfried Fellenz,\nand John G Taylor.\nEmotion",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "is worth\n16x16 words: Transformers\nfor\nimage\nrecognition\nat\nscale."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "IEEE Signal\nprocessing\nrecognition\nin\nhuman-computer\ninteraction.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "International Conference on Learning Representations (ICLR), 2021."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "magazine, 18(1):32–80, 2001.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[2] Yan Wang, Wei Song, Wei Tao, Antonio Liotta, Dawei Yang, Xinlei Li,",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Shuyong Gao, Yixuan Sun, Weifeng Ge, Wei Zhang, et al. A systematic",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Luca Antiga, et al. Pytorch: An imperative style, high-performance deep"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "review on affective computing: Emotion models, databases, and recent",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "learning library. Advances in neural\ninformation processing systems, 32,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "advances.\nInformation Fusion, 83:19–52, 2022.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "2019."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "The influence\n[3] Danny Oude Bos et al. Eeg-based emotion recognition.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[24]\nTony Mullen. Mastering blender.\nJohn Wiley & Sons, 2011."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "of visual and auditory stimuli, 56(3):1–17, 2006.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[25] Gabriel B´en´edict, Vincent Koops, Daan Odijk, and Maarten de Rijke."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[4] Nazmi Sofian Suhaimi,\nJames Mountstephens,\nJason Teo, et al.\nEeg-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Sigmoidf1: A smooth f1 score surrogate loss for multilabel classification."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "based emotion recognition: A state-of-the-art\nreview of current\ntrends",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "arXiv preprint arXiv:2108.10566, 2021."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "and opportunities. Computational\nintelligence and neuroscience, 2020,",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "2020.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[5] Yisi Liu, Olga Sourina, and Minh Khoa Nguyen. Real-time eeg-based",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "emotion recognition and its applications. Transactions on Computational",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Science XII: Special\nIssue on Cyberworlds, pages 256–277, 2011.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[6] Xiao-Wei Wang, Dan Nie, and Bao-Liang Lu. Emotional state classifi-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "cation from eeg data using machine learning approach. Neurocomputing,",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "129:94–106, 2014.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[7] Omid Bazgir, Zeynab Mohammadi,\nand Seyed Amir Hassan Habibi.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Emotion recognition with machine learning using eeg signals.\nIn 2018",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "25th national and 3rd international\niranian conference on biomedical",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "engineering (ICBME), pages 1–5.\nIEEE, 2018.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[8] Raza Imam,\nIbrahim Almakky, Salma Alrashdi, Baketah Alrashdi, and",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Mohammad Yaqub. Seda: Self-ensembling vit with defensive distillation",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "and adversarial\ntraining for\nrobust chest x-rays classification, 2023.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[9] Yuan-Pin Lin, Chi-Hong Wang, Tien-Lin Wu, Shyh-Kang\nJeng,\nand",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Jyh-Horng Chen.\nEeg-based emotion recognition in music\nlistening:",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Quansheng Ren.\n4d attention-based neural network for\neeg emotion"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "In response to the practical deployment of\nreal-time EEG",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "recognition. Cognitive Neurodynamics, pages 1–14, 2022."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "emotion classification, we introduced EVOKE, a knowledge",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[11]\nShuaiqi Liu, Xu Wang, Ling Zhao, Bing Li, Weiming Hu, Jie Yu, and"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Yu-Dong Zhang. 3dcann: A spatio-temporal convolution attention neural"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "distillation-based lightweight model\nfor\nemotion recognition",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "IEEE Journal of Biomedical and\nnetwork for eeg emotion recognition."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "and\nintegration\ninto\nvirtual\nenvironments.\nThe\nframework",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Health Informatics, 26(11):5321–5331, 2021."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "maps multi-label classification results\nto eight distinct emo-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[12] Yilong Yang, Qingfeng Wu, Yazhen Fu, and Xiaowei Chen. Continuous"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "convolutional\nneural\nnetwork with\n3d\ninput\nfor\neeg-based\nemotion"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "tions\nand\nlinks\nthem to\ncustom-created\n3D avatars. When",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Information Processing:\n25th\nInternational\nrecognition.\nIn Neural"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "tested\non\nthe\npublicly\navailable DEAP EEG dataset,\nour",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Conference,\nICONIP 2018, Siem Reap, Cambodia, December 13–16,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "proposed model achieved competitive performance along with",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "2018, Proceedings, Part VII 25, pages 433–443. Springer, 2018."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[13] Arjun Arjun, Aniket Singh Rajpoot, and Mahesh Raveendranatha Pan-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "significantly fewer parameters as compared to other state-of-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "icker.\nIntroducing attention mechanism for eeg signals: Emotion recog-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "the-art models. This work paves\nthe way for enhanced emo-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "2021\n43rd Annual\nInternational\nnition with\nvision\ntransformers.\nIn"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "tional\ncommunication in virtual\nsettings, offering numerous",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Conference of\nthe\nIEEE Engineering in Medicine & Biology Society"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "(EMBC), pages 5723–5726.\nIEEE, 2021."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "applications\nin\nhealthcare,\ntherapy,\nand\nbeyond,\nultimately",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[14]\nPaul Ekman, E Richard Sorenson, and Wallace V Friesen. Pan-cultural"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "making emotional well-being more accessible and immersive.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "elements in facial displays of emotion. Science, 164(3875):86–88, 1969."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "One notable application is in Virtual Therapy Sessions:",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[15]\nJames A Russell. A circumplex model of affect. Journal of personality"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "and social psychology, 39(6):1161, 1980."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "– Scenario:\nIn\nvirtual\ntherapy\nsessions, EVOKE can\nbe",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[16]\nJames A Russell\nand Albert Mehrabian.\nEvidence\nfor\na\nthree-factor"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "utilized\nto\nenhance\nthe\nemotional\ninteraction\nbetween",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "theory of emotions.\nJournal of research in Personality, 11(3):273–294,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "1977."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "therapists\nand\nclients\nby mapping\nreal-time\nemotional",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[17] Albert Mehrabian.\nPleasure-arousal-dominance: A general\nframework"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "states onto virtual avatars.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "for\ndescribing\nand measuring\nindividual\ndifferences\nin\ntemperament."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "– Application: As clients express their\nfeelings and emo-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Current Psychology, 14:261–292, 1996."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[18] Yisi Liu, Olga Sourina, and Minh Khoa Nguyen. Real-time eeg-based"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "tions, EVOKE processes\nthe emotional cues\nthrough its",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "human emotion recognition and visualization.\nIn 2010 international"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "knowledge-distilled model,\nproviding\ntherapists with\na",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "conference on cyberworlds, pages 262–269.\nIEEE, 2010."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "visual\nrepresentation of emotional changes in the virtual",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[19]\nSander Koelstra, Christian Muhl, Mohammad Soleymani,\nJong-Seok"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Lee, Ashkan Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "avatars. This\nvisual\nfeedback\naids\ntherapists\nin\nunder-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "and Ioannis Patras. Deap: A database for emotion analysis; using physi-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "standing\nand\nresponding\nempathetically\nto\nthe\nclient’s",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "ological signals.\nIEEE transactions on affective computing, 3(1):18–31,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "emotional state.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "2011."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[20] Ruo-Nan Duan,\nJia-Yi Zhu,\nand Bao-Liang Lu.\nDifferential\nentropy"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "In the\nfuture, we\naim to integrate\nreal-time EEG signals",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "feature for eeg-based emotion classification.\nIn 2013 6th International"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "into a healthcare virtual environment system, making it readily",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "IEEE/EMBS Conference on Neural Engineering (NER), pages 81–84."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "IEEE, 2013."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "accessible for use by healthcare professionals.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[21] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "in a neural network. arXiv preprint arXiv:1503.02531, 2015."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "REFERENCES",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "senborn,\nXiaohua\nZhai,\nThomas\nUnterthiner, Mostafa\nDehghani,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[1] Roddy Cowie, Ellen Douglas-Cowie, Nicolas Tsapatsoulis, George Vot-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Matthias Minderer, Georg Heigold, Sylvain Gelly,\net\nal.\nAn\nimage"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "sis, Stefanos Kollias, Winfried Fellenz,\nand John G Taylor.\nEmotion",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "is worth\n16x16 words: Transformers\nfor\nimage\nrecognition\nat\nscale."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "IEEE Signal\nprocessing\nrecognition\nin\nhuman-computer\ninteraction.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "International Conference on Learning Representations (ICLR), 2021."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "magazine, 18(1):32–80, 2001.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Brad-"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[2] Yan Wang, Wei Song, Wei Tao, Antonio Liotta, Dawei Yang, Xinlei Li,",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "bury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Shuyong Gao, Yixuan Sun, Weifeng Ge, Wei Zhang, et al. A systematic",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Luca Antiga, et al. Pytorch: An imperative style, high-performance deep"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "review on affective computing: Emotion models, databases, and recent",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "learning library. Advances in neural\ninformation processing systems, 32,"
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "advances.\nInformation Fusion, 83:19–52, 2022.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "2019."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "The influence\n[3] Danny Oude Bos et al. Eeg-based emotion recognition.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[24]\nTony Mullen. Mastering blender.\nJohn Wiley & Sons, 2011."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "of visual and auditory stimuli, 56(3):1–17, 2006.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "[25] Gabriel B´en´edict, Vincent Koops, Daan Odijk, and Maarten de Rijke."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[4] Nazmi Sofian Suhaimi,\nJames Mountstephens,\nJason Teo, et al.\nEeg-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "Sigmoidf1: A smooth f1 score surrogate loss for multilabel classification."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "based emotion recognition: A state-of-the-art\nreview of current\ntrends",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": "arXiv preprint arXiv:2108.10566, 2021."
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "and opportunities. Computational\nintelligence and neuroscience, 2020,",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "2020.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[5] Yisi Liu, Olga Sourina, and Minh Khoa Nguyen. Real-time eeg-based",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "emotion recognition and its applications. Transactions on Computational",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Science XII: Special\nIssue on Cyberworlds, pages 256–277, 2011.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[6] Xiao-Wei Wang, Dan Nie, and Bao-Liang Lu. Emotional state classifi-",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "cation from eeg data using machine learning approach. Neurocomputing,",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "129:94–106, 2014.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[7] Omid Bazgir, Zeynab Mohammadi,\nand Seyed Amir Hassan Habibi.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Emotion recognition with machine learning using eeg signals.\nIn 2018",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "25th national and 3rd international\niranian conference on biomedical",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "engineering (ICBME), pages 1–5.\nIEEE, 2018.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[8] Raza Imam,\nIbrahim Almakky, Salma Alrashdi, Baketah Alrashdi, and",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Mohammad Yaqub. Seda: Self-ensembling vit with defensive distillation",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "and adversarial\ntraining for\nrobust chest x-rays classification, 2023.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "[9] Yuan-Pin Lin, Chi-Hong Wang, Tien-Lin Wu, Shyh-Kang\nJeng,\nand",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "Jyh-Horng Chen.\nEeg-based emotion recognition in music\nlistening:",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "A comparison of\nschemes\nfor multiclass\nsupport vector machine.\nIn",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "2009 IEEE international\nconference on acoustics,\nspeech and signal",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        },
        {
          "V. CONCLUSION AND FUTURE WORK": "processing, pages 489–492.\nIEEE, 2009.",
          "[10] Guowen Xiao, Meng Shi, Mengwen Ye, Bowen Xu, Zhendi Chen, and": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "2",
      "title": "A systematic review on affective computing: Emotion models, databases, and recent advances",
      "authors": [
        "Yan Wang",
        "Wei Song",
        "Wei Tao",
        "Antonio Liotta",
        "Dawei Yang",
        "Xinlei Li",
        "Shuyong Gao",
        "Yixuan Sun",
        "Weifeng Ge",
        "Wei Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "Eeg-based emotion recognition. The influence of visual and auditory stimuli",
      "authors": [
        "Danny Oude"
      ],
      "year": "2006",
      "venue": "Eeg-based emotion recognition. The influence of visual and auditory stimuli"
    },
    {
      "citation_id": "4",
      "title": "Eegbased emotion recognition: A state-of-the-art review of current trends and opportunities. Computational intelligence and neuroscience",
      "authors": [
        "James Nazmi Sofian Suhaimi",
        "Jason Mountstephens",
        "Teo"
      ],
      "year": "2020",
      "venue": "Eegbased emotion recognition: A state-of-the-art review of current trends and opportunities. Computational intelligence and neuroscience"
    },
    {
      "citation_id": "5",
      "title": "Real-time eeg-based emotion recognition and its applications",
      "authors": [
        "Yisi Liu",
        "Olga Sourina",
        "Minh Nguyen"
      ],
      "year": "2011",
      "venue": "Transactions on Computational Science"
    },
    {
      "citation_id": "6",
      "title": "Emotional state classification from eeg data using machine learning approach",
      "authors": [
        "Xiao-Wei Wang",
        "Dan Nie",
        "Bao-Liang Lu"
      ],
      "year": "2014",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition with machine learning using eeg signals",
      "authors": [
        "Omid Bazgir",
        "Zeynab Mohammadi",
        "Seyed Amir",
        "Hassan Habibi"
      ],
      "year": "2018",
      "venue": "2018 25th national and 3rd international iranian conference on biomedical engineering (ICBME)"
    },
    {
      "citation_id": "8",
      "title": "Seda: Self-ensembling vit with defensive distillation and adversarial training for robust chest x-rays classification",
      "authors": [
        "Raza Imam",
        "Ibrahim Almakky",
        "Salma Alrashdi",
        "Baketah Alrashdi",
        "Mohammad Yaqub"
      ],
      "year": "2023",
      "venue": "Seda: Self-ensembling vit with defensive distillation and adversarial training for robust chest x-rays classification"
    },
    {
      "citation_id": "9",
      "title": "Eeg-based emotion recognition in music listening: A comparison of schemes for multiclass support vector machine",
      "authors": [
        "Yuan-Pin Lin",
        "Chi-Hong Wang",
        "Tien-Lin Wu",
        "Shyh-Kang Jeng",
        "Jyh-Horng Chen"
      ],
      "year": "2009",
      "venue": "2009 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "10",
      "title": "4d attention-based neural network for eeg emotion recognition",
      "authors": [
        "Guowen Xiao",
        "Meng Shi",
        "Mengwen Ye",
        "Bowen Xu",
        "Zhendi Chen",
        "Quansheng Ren"
      ],
      "year": "2022",
      "venue": "Cognitive Neurodynamics"
    },
    {
      "citation_id": "11",
      "title": "dcann: A spatio-temporal convolution attention neural network for eeg emotion recognition",
      "authors": [
        "Shuaiqi Liu",
        "Xu Wang",
        "Ling Zhao",
        "Bing Li",
        "Weiming Hu",
        "Jie Yu",
        "Yu-Dong Zhang"
      ],
      "year": "2021",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "12",
      "title": "Continuous convolutional neural network with 3d input for eeg-based emotion recognition",
      "authors": [
        "Yilong Yang",
        "Qingfeng Wu",
        "Yazhen Fu",
        "Xiaowei Chen"
      ],
      "year": "2018",
      "venue": "Neural Information Processing: 25th International Conference"
    },
    {
      "citation_id": "13",
      "title": "Aniket Singh Rajpoot, and Mahesh Raveendranatha Panicker. Introducing attention mechanism for eeg signals: Emotion recognition with vision transformers",
      "authors": [
        "Arjun Arjun"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "14",
      "title": "Pan-cultural elements in facial displays of emotion",
      "authors": [
        "Paul Ekman",
        "Richard Sorenson",
        "Wallace Friesen"
      ],
      "year": "1969",
      "venue": "Science"
    },
    {
      "citation_id": "15",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "16",
      "title": "Evidence for a three-factor theory of emotions",
      "authors": [
        "A James",
        "Albert Russell",
        "Mehrabian"
      ],
      "year": "1977",
      "venue": "Journal of research in Personality"
    },
    {
      "citation_id": "17",
      "title": "Pleasure-arousal-dominance: A general framework for describing and measuring individual differences in temperament",
      "authors": [
        "Albert Mehrabian"
      ],
      "year": "1996",
      "venue": "Current Psychology"
    },
    {
      "citation_id": "18",
      "title": "Real-time eeg-based human emotion recognition and visualization",
      "authors": [
        "Yisi Liu",
        "Olga Sourina",
        "Minh Nguyen"
      ],
      "year": "2010",
      "venue": "2010 international conference on cyberworlds"
    },
    {
      "citation_id": "19",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "20",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "Jia-Yi Ruo-Nan Duan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "21",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "Geoffrey Hinton",
        "Oriol Vinyals",
        "Jeff Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "22",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "venue": "International Conference on Learning Representations (ICLR)"
    },
    {
      "citation_id": "23",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "24",
      "title": "Mastering blender",
      "authors": [
        "Tony Mullen"
      ],
      "year": "2011",
      "venue": "Mastering blender"
    },
    {
      "citation_id": "25",
      "title": "Sigmoidf1: A smooth f1 score surrogate loss for multilabel classification",
      "authors": [
        "Gabriel Bénédict",
        "Vincent Koops",
        "Daan Odijk",
        "Maarten De Rijke"
      ],
      "year": "2021",
      "venue": "Sigmoidf1: A smooth f1 score surrogate loss for multilabel classification",
      "arxiv": "arXiv:2108.10566"
    }
  ]
}