{
  "paper_id": "2105.09406v1",
  "title": "Speech & Song Emotion Recognition Using Multilayer Perceptron And Standard Vector Machine",
  "published": "2021-05-19T21:28:05Z",
  "authors": [
    "Behzad Javaheri"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "herein, we have compared the performance of SVM and MLP in emotion recognition using speech and song channels of the RAVDESS dataset. We have undertaken a journey to extract various audio features, identify optimal scaling strategy and hyperparameter for our models. To increase sample size, we have performed audio data augmentation and addressed data imbalance using SMOTE. Our data indicate that optimised SVM outperforms MLP with an accuracy of 82 compared to 75%. Following data augmentation, the performance of both algorithms was identical at ~79%, however, overfitting was evident for the SVM. Our final exploration indicated that the performance of both SVM and MLP were similar in which both resulted in lower accuracy for the speech channel compared to the song channel. Our findings suggest that both SVM and MLP are powerful classifiers for emotion recognition in a vocal-dependent manner.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Svm And Mlp:",
      "text": "SVM is a powerful deterministic supervised learning algorithm for both classification and regression with good generalisation and able to recognise patterns of non-linear relationships. It constructs an optimal hyperplane in which the margin between the classes is maximised  [9] . It is a two-layer neural network employing a hidden layer of radial units and one output neuron. The construct of this network and its parameters is governed by kernel functions instead of direct processing of hidden unit signals  [10] . SVM is considered a \"shallow\" architecture algorithm with few parameters to tune and therefore suffer from overreliance on kernel function with very limited flexibility. Some of the advantages of SVM over MLP linked to the complexity of networks. SVMs use a large number of learning problem formulations leading to solving a quadratic optimisation problem  [11, 12] . SVM can perform high by utilising structural risk minimisation rather than empirical risk minimisation inductive principle  [13] .\n\nMLP is non-deterministic and universal in the sense that it can approximate any continuous nonlinear function sufficiently well on a compact interval and is used for various tasks including classification and regression  [14, 15] . In MLP the neurons are arranged in layers, counting from the input layer to hidden layers ultimately to the output layer. The two neighbouring layers can connect and the network is feedforward  [16] . Essentially, MLP contains several highly interconnected processing neurons that can be used in parallel to detect a solution for a given problem as an input. However, one of the MLP's drawbacks is that it may underperform when it tries to solve a nonlinear optimisation problem with many local minima. Furthermore, it may overfit on small datasets and as such sensitive to size of data  [16, 17] .\n\nIn this study, the robustness of MLP and SVM were evaluated for speech and song emotion recognition. Additionally, we examined whether their performances are dependent on vocal channels.\n\nHypothesis: that the SVM and MLP will produce highly accurate emotion classification in a vocalindependent manner.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Description And Choice Of Training And Evaluation Methodology:",
      "text": "data distribution was checked to identify whether scaling was required and find significant variations in data distribution. It was therefore decided to scale the data. Initially, the predictors (195) and the target variable (emotions) were defined. From the sklearn library, the train_test_split function was used to split predictors and target into 80 and 20% for train and test subsets, respectively. Subsequently, to identify the most appropriate scaling approach, two available scaling methods within the sklearn library were used. StandardScaler (standard) achieves scaling by producing predictors with mean zero and scaling data to unit variance, that is, variance and standard deviation of 1. Min-Max scaling will transform predictors into a range between [0, 1] or [-1, 1]. Furthermore, the target multiclass variable (emotions) was in categorical form, therefore encoded using LabelEncoder. The following process was followed: 1) To identify optimal scaling method SVML and ML (non-optimised) were tested on non-scaled, standard and MinMax scaled. This step resulted in 7 models (3 for SVM and 4 for MLP with/without early stopping). 2) Hyperparameter search to identify optimal parameters were performed and the performance of this optimal SVM and MLP models were evaluated on the entire training as well as testing datasets. This step resulted in 2 models. 3) Data augmentation (to increase sample size and introduce noise) and SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset performed. Hyperparameter search performed and performance of optimal SVM and MLP were examined. This resulted in 2 models. 4) The data split into \"speech\" and \"song\" channels, hyperparameter tuning performed and performance of optimal SVM and MLP models evaluated on both vocal channels. This resulted in 4 models.\n\nTo visualise and compare the performance of these models, cross-validation score, learning curve, classification report, class prediction error, precision-recall, ROC/AUC graph and confusion matrix were plotted using Yellowbrick library. Additionally, for both SVM and MLP search, the computing time was recorded and the models in the search were visualised using Matplotlib's Axes3D, by plotting a 3D graph.\n\nChoice of parameters: for hyperparameter tuning exhaustive grid search was performed using sklearn's GridSearchCV, however, this was terminated as after 48 hours without completion. Alternatively, sklearn's RandomizedSearchCV was performed for kernel (rbf, poly and linear), C (uniform distribution 2, 50), gamma (uniform distribution 0.01, 1) and 3 StratifiedKfold CV. For MLP the grid included search for hidden_layer_size [  (8,) , (180,), (300,), (100,50,),  (10, 10, 10) ], activation ['tanh', 'relu', 'logistic'], solver ['sgd', 'adam'], alpha [0.0001, 0.001, 0.01], epsilon [1e-08, 0.1], and learning rate ['adaptive', 'constant']. The optimal parameters based on accuracy for both SVM and MLP (with/without early stopping) used for emotion recognition on entire training dataset and performance of these models plotted using 6 graphs in total as described above.\n\nResults, analysis and critical evaluation: SVM (non-optimised) on non-scaled data produced an accuracy of 33.2 and 28.7% on training/testing datasets respectively, with complete misclassification for angry class, with an average 0.59 precision and 0.69 AUC. The accuracy of SVM with MinMax scaling increased to 67.5 and 67.3% for training and testing datasets, with an average precision of 0.74 and 0.90 AUC. The performance of SVM on Standard scaled data show an accuracy of 75.76 and 70.77% for training and testing datasets. The result for grid search suggests (computing time of 53.2s) to use 41.8 for C, 0.03 for gamma and rbf as the kernel. The performance of optimal SVM on entire training and testing datasets indicate an accuracy of 100 and 82.3%, respectively with a mean CV score of 0.742, average precision of 0.91 and AUC of 0.96. Our data indicate that neutral and calm classes had high correct classification evident from precision, recall, F and confusion matrix (Fig.  1A -C top panels). The class imbalance for angry emotion is evident.\n\nWe then used a similar approach for MLP. The performance of MLP on non-scaled data indicate accuracies of 65.1 and 53.3 for training and testing datasets. The average precision was 0.70 with AUC of 0.89. The performance was equally low across all emotions with the lowest in sad and happy emotions. MLP's performance on Min-Max scaled data result in 79.8 and 68.6% accuracy for training and testing with an average precision of 0.78 and AUC of 0.94. In addition, MLP's performance on the standard scaled data produced 100% accuracy for the training 75% for the testing dataset. The average precision was 0.87 and AUC was reported as 0.96. Visualisation of class prediction error suggests that happy and fearful emotions have the highest misclassifications. Grid search computed in 260.7 seconds suggested using \"adam\" as solver, a learning rate of \"constant, hidden layer size of (100, 50) epsilon of 1e-08, alpha of 0.001 and \"relu\" as activation function. The optimal MLP leads to 100% accuracy for training and 76.33% for testing dataset with a mean cross-validation score of 0.737. Visualisation of the learning curve indicates overfitting. Precision was reported as 0.86 and AUC as 0.96 with happy emotion with the lowest and neutral emotion with the highest precision, recall and F1. To address overfitting MLP was re-run with earl stopping which resulted in accuracy of 87 and 74.64% for training and testing datasets with an average precision of 0.83 and 0.95 for AUC. Our data indicate that happy emotion has the lowest performance with 0.66 for precision, recall and F1 and having 24 out of 71 samples misclassified mostly within sad and fearful emotions. Neutral and calm classes enjoyed the higher true positive and lower true negatives (Fig.  1A -C bottom panels). To increase sample number, introduce noise and overcome overfitting and class imbalance observed particularly for angry emotion, two approaches were employed. To increase the sample size and introduce noise, pydiogment library was employed to generate new audio files for each emotion based on the properties of audio files within each emotion. Two approaches of \"fade in and out\" and \"change tone\" were applied. To overcome class imbalance, SMOTE function of the imblearn library was utilised.\n\nThe computation time for the SVM hyperparameter search was 5203s with 44.9 for C, 0.155 gamma and poly as the kernel. The SVM performance was 100 and 79.1% on the entire training and testing datasets with a cross-validation score of 0.744, the precision of 0.78 and 0.93 for AUC (Figure  3 ). Examination of the learning curve indicates a straight line (score 1) across all training instances suggesting overfitting. Class prediction error show that data belonging to happy and fearful emotions were most contributory to misclassified emotions and neutral and angry were least contributory (Fig.  2  A-C top panels).\n\nSimilarly, hyperparameter tuning for MLP suggested utilising adam as the solver, adaptive learning rate and 300 hidden layer size, epsilon of 1e-8, alpha of 0.0001 and activation function of relu with a computation time of 703.2s. The performance of optimal MLP showed an accuracy of 93.5 and 78.8% for the training and testing dataset respectively. Examination of the learning curve did not suggest overfitting. Our data indicate an average cross-validation score of 0.725, average precision of 0.85 and AUC of 0.96. Classification reports suggest that neutral, angry and calm had the highest accurate emotion recognition with happy emotion being the least accurate with 0.67 precision, 0.680 recall and 0.675 f1 (Fig.  2  A-C bottom panel).  To examine whether data split by vocal channel affects classification, data split into speech and song. Following hyperparameter tuning, optimised SVM model of the speech channel resulted in 100 and 71.24% accuracy for training/testing datasets with 0.632 average cross-validation score, average precision of 0.67 and AUC of 0.90. The learning curve suggests overfitting with a score 1 across all training instances. Classification report indicates sad emotion with 0.657 precision, 0.5 recall and 0.568 F1 had the lowest positive prediction (Fig.  3A -C top panel). For the song channel, accuracy of SVM on training/testing datasets were 100 and 88.9 respectively with learning curve score of 1 similarly indicating overfitting. The average CV score was 0.847, with an average precision of 0.92 and 0.96 for AUC. Angry emotion scored high with 1 for precision, 0.991 for recall and 0.995 for F1. The least accurate was for happy with 0.778, 0.840 and 0.808 for precision, recall and F1 (Fig.  4A-C ). Furthermore, emotion recognition in speech channel using optimised MLP resulted in 95 and 71.8% accuracy for train/test datasets. The average CV score was 0.620 with average precision of 0.67 and 0.94 AUC. Classification report & class prediction error indicate that neutral emotion had the highest true positive & lowest false positive with a precision of 0.841, recall of 0.848 and 0.844 F1. Angry emotion performed similarly well. In contrast, fearful emotion had the lowest true positives & highest false positives with 0.534 precision, 0.562 recall and 0.548 F1 (Fig.  3A-C ). In contrast, the performance of optimised MLP on the song channel was higher with 96.45 and 85.07% accuracy for train/test datasets with 0.804 average CV score, average precision of 0.88 and 0.98 AUC. The angry emotion had the highest true positive and lowest false positives with 0.991 precision, 0.972 recall and 0.981 F1 (Fig.  4A -C bottom panels).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions:",
      "text": "In the present work we have performed a comparative study on the performance of SVM and MLP to classify 6 emotions from audio file datasets. We have taken a holistic approach by extracting 5 predictors (many inputs from each) from the audio files and have undertaken a journey whereby we compared the performance of a) SVM and MLP on non-scaled, MinMax and standard scaled dataset; b) optimised SVM and MLP on a standard scaled dataset; c) optimised SVM and MLP on augmented and balanced (SMOTE) dataset and d) optimised SVM and MLP on each speech and song channels. This resulted in 8 grid searches to find optimal hyperparameters and 15 models in total. Our data show that hyperparameter tuning increased SVM's performance and that augmentation and addressing data imbalance does not seem to further increase this performance. Optimisation addressed MLP's overfitting but did not further enhance accuracy for the test subset and that data augmentation and addressing imbalance enhances MLP's accuracy. Interestingly, when data split by vocal channel, both SVM and MLP performed similarly for both speech and song channels but SVM exhibited overfitting as evident by learning curve whereas this was not observed for MLP. Additionally, both SVM and MLP performed better in emotion classification using the song compared to speech channel.\n\nPrevious studies on emotion recognition from audio files have used a variety of datasets and approaches. For example, Gideon et al., (2017) performed a comparative study of the performance of progressive neural networks to conventional deep neural network using the IEMOCAP and MSP-IMPROV datasets aiming to devise a multi-task learning approach for emotion, gender and speaker recognition  [18] . In addition,  Buyukyilmaz and  Cibikdiken (2016) used the MLP model on a dataset (3,168 files of male and female voices) for gender classification and reported 96.74% accuracy  [19] . Furthermore,  Shegokar and Sircar (2016)  utilised SVM for the classification of male speech samples from the RAVDESS dataset and obtained an accuracy of 60.1%  [20] .  Zhao et al., (2019)  report 95.8% accuracy using a subsection of the dataset and convolutional neural network/long short-term memory  [21] . More relevant to this work,  Popova et al., (2017)  reported 71% accuracy in emotion recognition using a convolutional neural network on the RAVDESS dataset  [22] . Similar to this work, Joy et al., (2020) compared SVM and MLP's performance in emotion recognition and reported accuracy of 70 and 58%, respectively  [23] . Our best-performing model reports higher accuracy in emotion recognition with the directly comparable studies. It also outperforms a recent study in which 71.6% accuracy in emotion recognition using the RAVDESS dataset was reported  [8] .\n\nLessons learned: both SVM and MLP are powerful classifiers for emotion recognition with vocal-dependent performance in which better classification is observed in the song compared to speech channel.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Future Work:",
      "text": "The task of emotion recognition from voice predictors differ between males and females  [24, 25] , therefore the effect of gender on classification performance can be further explored. In addition, ageing also affects the acoustical voice characteristics  [26]  and thus this important contributor has to be considered for refinement of classification strategies.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A-C top panels). The class imbalance for angry emotion is evident.",
      "page": 3
    },
    {
      "caption": "Figure 1: A-C bottom panels).",
      "page": 3
    },
    {
      "caption": "Figure 1: Comparative performance of SVM and MLP emotion classification. A) Classification report; B) prediction",
      "page": 3
    },
    {
      "caption": "Figure 3: ). Examination of the learning",
      "page": 3
    },
    {
      "caption": "Figure 2: A-C top panels).",
      "page": 4
    },
    {
      "caption": "Figure 2: A-C bottom panel).",
      "page": 4
    },
    {
      "caption": "Figure 2: Comparative performance of SVM and MLP emotion classification on augmented data. A) Classification",
      "page": 4
    },
    {
      "caption": "Figure 3: Comparative performance of SVM and MLP classification on speech channel. A) Classification report;",
      "page": 4
    },
    {
      "caption": "Figure 3: A-C top panel). For the song channel, accuracy of SVM on training/testing datasets",
      "page": 4
    },
    {
      "caption": "Figure 4: A-C). Furthermore, emotion recognition in speech channel using optimised",
      "page": 5
    },
    {
      "caption": "Figure 3: A-C). In contrast, the performance of",
      "page": 5
    },
    {
      "caption": "Figure 4: A-C bottom panels).",
      "page": 5
    },
    {
      "caption": "Figure 4: Comparative performance of SVM and MLP # classification on song channel. A) Classification report;",
      "page": 5
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech emotion from with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "S Baik"
      ],
      "year": "2017",
      "venue": "2017 international conference on platform technology and service (PlatCon)"
    },
    {
      "citation_id": "2",
      "title": "Emotion in human-computer interaction",
      "authors": [
        "S Brave",
        "C Nass"
      ],
      "year": "2009",
      "venue": "Human-computer interaction fundamentals"
    },
    {
      "citation_id": "3",
      "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis"
      ],
      "year": "2016",
      "venue": "2016 IEEE international conference on acoustics, speech and signal processing"
    },
    {
      "citation_id": "4",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "5",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "W Lim",
        "D Jang",
        "T Lee"
      ],
      "year": "2016",
      "venue": "Speech emotion recognition using convolutional and recurrent neural networks"
    },
    {
      "citation_id": "6",
      "title": "Asia-Pacific signal and information processing association annual summit and conference (APSIPA)",
      "year": "2016",
      "venue": "Asia-Pacific signal and information processing association annual summit and conference (APSIPA)"
    },
    {
      "citation_id": "7",
      "title": "Toward an affect-sensitive multimodal human-computer interaction",
      "authors": [
        "M Pantic",
        "L Rothkrantz"
      ],
      "year": "2003",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "8",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "10",
      "title": "Support vector machines (SVM) as a technique for solvency analysis",
      "authors": [
        "L Auria",
        "R Moro"
      ],
      "year": "2008",
      "venue": "Support vector machines (SVM) as a technique for solvency analysis"
    },
    {
      "citation_id": "11",
      "title": "Mlp and svm networks-a comparative study",
      "authors": [
        "S Osowski",
        "K Siwek",
        "T Markiewicz"
      ],
      "year": "2004",
      "venue": "Proceedings of the 6th Nordic Signal Processing Symposium"
    },
    {
      "citation_id": "12",
      "title": "Alfalfa Yield Prediction Using UAV-Based Hyperspectral Imagery and Ensemble Learning",
      "authors": [
        "L Feng"
      ],
      "year": "2020",
      "venue": "Remote Sensing"
    },
    {
      "citation_id": "13",
      "title": "Development of support vector machine-based model and comparative analysis with artificial neural network for modeling the plant tissue culture procedures: effect of plant growth regulators on somatic embryogenesis of chrysanthemum, as a case study",
      "authors": [
        "M Hesami",
        "R Naderi",
        "M Tohidfar",
        "M Yoosefzadeh-Najafabadi"
      ],
      "year": "2020",
      "venue": "Plant Methods"
    },
    {
      "citation_id": "14",
      "title": "Long-term SPI drought forecasting in the Awash River Basin in Ethiopia using wavelet neural network and wavelet support vector regression models",
      "authors": [
        "A Belayneh",
        "J Adamowski",
        "B Khalil",
        "B Ozga-Zielinski"
      ],
      "year": "2014",
      "venue": "Journal of Hydrology"
    },
    {
      "citation_id": "15",
      "title": "Multilayer perceptron, fuzzy sets, classifiaction",
      "authors": [
        "S Pal",
        "S Mitra"
      ],
      "year": "1992",
      "venue": "Multilayer perceptron, fuzzy sets, classifiaction"
    },
    {
      "citation_id": "16",
      "title": "Learning representations by back-propagating errors",
      "authors": [
        "D Rumelhart",
        "G Hinton",
        "R Williams"
      ],
      "year": "1986",
      "venue": "nature"
    },
    {
      "citation_id": "17",
      "title": "A comparative study of artificial neural network (MLP, RBF) and support vector machine models for river flow prediction",
      "authors": [
        "M Ghorbani",
        "H Zadeh",
        "M Isazadeh",
        "O Terzi"
      ],
      "year": "2016",
      "venue": "Environmental Earth Sciences"
    },
    {
      "citation_id": "18",
      "title": "Forecasting the Crop Yield Production in Trichy District Using Fuzzy C-Means Algorithm and Multilayer Perceptron (MLP)",
      "authors": [
        "M Geetha"
      ],
      "year": "2020",
      "venue": "International Journal of Knowledge and Systems Science (IJKSS)"
    },
    {
      "citation_id": "19",
      "title": "Progressive neural networks for transfer learning in emotion recognition",
      "authors": [
        "J Gideon",
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Progressive neural networks for transfer learning in emotion recognition",
      "arxiv": "arXiv:1706.03256"
    },
    {
      "citation_id": "20",
      "title": "Voice gender recognition using deep learning",
      "authors": [
        "M Buyukyilmaz",
        "A Cibikdiken"
      ],
      "year": "2016",
      "venue": "2016 International Conference on Modeling, Simulation and Optimization Technologies and Applications (MSOTA2016)"
    },
    {
      "citation_id": "21",
      "title": "Continuous wavelet transform based speech emotion recognition",
      "authors": [
        "P Shegokar",
        "P Sircar"
      ],
      "year": "2016",
      "venue": "2016 10th International Conference on Signal Processing and Communication Systems (ICSPCS)"
    },
    {
      "citation_id": "22",
      "title": "Speech emotion recognition using deep 1D & 2D CNN LSTM networks",
      "authors": [
        "J Zhao",
        "X Mao",
        "L Chen"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition in sound",
      "authors": [
        "A Popova",
        "A Rassadin",
        "A Ponomarenko"
      ],
      "year": "2017",
      "venue": "International Conference on Neuroinformatics"
    },
    {
      "citation_id": "24",
      "title": "Speech Emotion Recognition using Neural Network and MLP Classifier",
      "authors": [
        "J Joy",
        "Aparna; Ram",
        "; Shreya",
        "S Rama"
      ],
      "year": "2020",
      "venue": "IJESC"
    },
    {
      "citation_id": "25",
      "title": "Analysis, synthesis, and perception of voice quality variations among female and male talkers",
      "authors": [
        "D Klatt",
        "L Klatt"
      ],
      "year": "1990",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "26",
      "title": "Physiologic and acoustic differences between male and female voices",
      "authors": [
        "I Titze"
      ],
      "year": "1989",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "27",
      "title": "Age and voice quality in professional singers",
      "authors": [
        "J Sundberg",
        "M Thörnvik",
        "A Söderström"
      ],
      "year": "1998",
      "venue": "Logopedics Phoniatrics Vocology"
    }
  ]
}