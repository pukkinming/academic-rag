{
  "paper_id": "2108.10152v1",
  "title": "Ieee Signal Processing Magazine 1",
  "published": "2021-08-18T21:55:20Z",
  "authors": [
    "Sicheng Zhao",
    "Guoli Jia",
    "Jufeng Yang",
    "Guiguang Ding",
    "Kurt Keutzer"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Emotion Recognition From Multiple Modalities: Fundamentals And Methodologies",
      "text": "Sicheng Zhao, Senior Member, IEEE, Guoli Jia, Jufeng Yang, Guiguang Ding, Kurt Keutzer, Life Fellow, IEEE\n\nHumans are emotional creatures. Multiple modalities are often involved when we express emotions, whether we do so explicitly (e.g., facial expression, speech) or implicitly (e.g., text, image). Enabling machines to have emotional intelligence, i.e., recognizing, interpreting, processing, and simulating emotions, is becoming increasingly important. In this tutorial, we discuss several key aspects of multi-modal emotion recognition (MER). We begin with a brief introduction on widely used emotion representation models and affective modalities. We then summarize existing emotion annotation strategies and corresponding computational tasks, followed by the description of main challenges in MER. Furthermore, we present some representative approaches on representation learning of each affective modality, feature fusion of different affective modalities, classifier optimization for MER, and domain adaptation for MER. Finally, we outline several realworld applications and discuss some future directions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion is present everywhere in human daily life and can influence or even determine our judgment and decision making  [1] . For example, in marketing, a widely advertised brand can generate a mental representation of a product in the consumers' mind and influence their preference and action; inducing sadness and disgust during a shopping trip would respectively increase and decrease consumers' willingness to pay 1 . In driving, drivers experiencing strong emotions, such as sadness, anger, agitation, and even happiness, are much more likely to be involved in an accident 2 . In education -especially current online classes during the COVID-19 epidemic period -students' emotional experiences and interactions with teachers have a big impact on their learning ability, interest, engagement, and even career choices 3 .\n\nThe importance of emotions in artificial intelligence was recognized decades ago. Minsky, a Turing Award winner in 1970, once claimed that \"The question is not whether intelligent machines can have any emotions, but whether machines can be intelligent without emotions.\"  [2] . Enabling machines to have emotional intelligence i.e., recognizing, interpreting, processing, and simulating emotions has recently become increasingly important with wide potential applications involving human-computer interaction  [3] . On the one hand, emotionally intelligent machines can provide more harmonious and personal services for human beings, especially the elderly, disabled, and children. For example, the companion robots that can work with emotions can better meet the psychological and emotional needs of the elderly and help them stay comfortable. On the other hand, by recognizing humans' emotions automatically and in real-time, intelligent machines can better identify humans' abnormal behaviors, send reminders to their relatives and friends, and prevent extreme behaviors to themselves and even to the rest of society. For example, an emotional driving monitoring system can automatically play some soothing music to relax angry drivers who might be dissatisfied with a traffic jam and can remind them to focus on driving safely.\n\nThe first step for intelligent machines to express humanlike emotions is to recognize and understand humans' emotions typically through two groups of affective modalities: explicit affective cues and implicit affective stimuli. Explicit affective cues correspond to specific physical and psychological changes in humans that can be directly observed and recorded, such as facial expression, eye movement, speech, action, and physiological signals. These signals can be either easily suppressed or masked, or difficult and impractical to capture. Meanwhile, the popularity of mobile devices and social networks enables humans to habitually share their experiences and express their opinions online using text, image, audio, and video. Implicit affective stimuli correspond to these commonly-used digital media, the analysis of which provides an implicit way to infer humans' emotions  [4] .\n\nRegardless of whether emotions are expressed explicitly or implicitly, there are generally multiple modalities that can contribute to the emotion recognition task, as shown in Fig.  1 . As compared to uni-modal emotion recognition, multi-modal emotion recognition (MER) has several advantages. First, data complementarity. Cues from different modalities can augment or complement each other. For example, if we see one post from a good friend, \"What great weather!\", it is of high probability that the friend is expressing a positive emotion; but if there is also an auxiliary image of a storm, we can infer that the text is actually a sarcasm and that a negative emotion is intended to be expressed. Second, model robustness. Due to the influence of many normally occurring factors in data collection, such as sensor device failure, some data modalities might be unavailable, which is especially prevalent in the wild. For example, in the CALLAS dataset containing speech, facial expression, and gesture modalities, the gesture stream is missing for some momentarily motionless users  [5] . In such cases, the learned MER model can still work with the help of other available modalities. Finally, performance superiority. Joint consideration of the complementary information of different modalities can result in better recognition performance. Meta-analysis indicates that as compared to the best uni-modal counterparts, MER achieves 9.83% performance improvement on average  [6] .\n\nIn this article, we will give a comprehensive tutorial on different aspects of MER, including psychological models, affective modalities, data collections and emotion annotations, computational tasks, challenges, computational methodologies, applications, and future directions. There have been several reviews/surveys on MER related topics  [4, 6, 7, 8, 9] . In particular:  [7]  and  [9]  cover different aspects of general multi-modal machine learning with few efforts on emotion recognition;  [6]  focuses on the quantitative review and metaanalysis of existing MER systems;  [4]  and  [8]  are surveystyle MER articles with the technical emphasis on multimodal fusion. Differently, this tutorial-style article aims to give a quick and comprehensive MER introduction that is also suitable for non-specialists.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Psychological Models",
      "text": "In psychology, categorical emotion states (CES) and dimensional emotion space (DES) are two representative types of models to measure emotion  [10] . CES models define emotions as a few basic categories, such as binary sentiment (positive and negative, sometimes including neutral), Ekman's six basic emotions (positive happiness, surprise and negative anger, disgust, fear, sadness), Mikels's eight emotions (positive amusement, awe, contentment, excitement, and negative anger, disgust, fear, sadness), Plutchik's emotion wheel (eight basic emotion categories by three intensities), and Parrott's tree hierarchical grouping (primary, secondary and tertiary categories). The development of psychological theories motivates CES to be increasingly diverse and fine-grained. DES models employ continuous 2D, 3D, or higher dimensional Cartesian spaces to represent emotions; the most widely used DES model is valence-arousal-dominance (VAD), where valence, arousal, and dominance represent the pleasantness, intensity, and control degree of emotion, respectively.\n\nCES models agree better with humans' intuition, but no consensus has been reached by psychologists on how many discrete emotion categories should be included. Further, emotion is complex and subtle, which cannot be well reflected by limited discrete categories. DES models can theoretically measure all emotions as different coordinate points in the continuous Cartesian space, but the absolute continuous values are beyond users' understanding. These two types of definitions of emotions are related, with possible transformation from CES to DES. For example, anger relates to negative valence, high arousal, and high dominance.\n\nBesides emotion, there are several other widely used concepts in affective computing, such as mood, affect, and sentiment. Emotions can be expected, induced, or perceived. We do not aim distinguishing them in this article. Please refer to  [11]  for more details on the differences or correlations between these concepts.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Affective Modalities",
      "text": "In the area of MER, multiple modalities are employed to recognize and predict human emotions. The affective modalities in MER can be roughly divided into two groups based on whether emotions are recognized from humans' physical body changes or from external digital media: explicit affective cues and implicit affective stimuli. The former group includes facial expression, eye movement, speech, action, gait, and electroencephalogram, all of which can be directly observed, recorded, or collected from an individual. Meanwhile, the latter group indicates the commonly-used digital media types such as text, audio, image, and video. We use these data types to store information and knowledge as well as transfer them between digital devices. In this way, emotions may be implicitly involved and evoked. Although the efficacy of one specific modality as a reliable channel to express emotions cannot be guaranteed, jointly considering multiple modalities would significantly improve the reliability and robustness  [12] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Explicit Affective Cues",
      "text": "A facial expression is an isolated motion of one or more human face regions/units, or a combination of such motions. It is commonly agreed that facial expressions can carry informative affective cues and are recognized as one of the most natural and powerful signals to convey the emotional states and intentions of human  [12] . Facial expression is also a form of nonverbal communication conveying social information between humans. We can deduce how an individual is feeling by observing his/her eyes movement  4  . The eyes are often viewed as important cues of emotions. For example, if a person is nervous or lying, the blinking rate of his/her eyes may become slower than normal 4 . Eyes movement signals can be easily collected via an eye tracker system, and have been widely used in human-computer interaction research. Speech is a significant vocal modal to carry emotions  [13, 14] .\n\nSpeakers may express their intentions like asking or declaring by using various intonations, degrees of loudness, and tempo. Specifically, emotions can be revealed when people talk with each other, or just mutter to themselves. As an important part of human body language, action, also conveys massive emotional information. For instance, an air punch is an act of thrusting one's clenched fist up into the air, typically as a gesture of triumph or elation. Similar to action, emotions can be perceived from a person's gait, i.e., their walking style. Psychology literature has proven that participants can identify the emotions of a subject by observing the subject's posture, including long strides, collapsed upper body, etc.  5 Body movement (e.g., walking speed) also plays an important role in the perception of different emotions. High arousal emotions such as anger and excitement are more associated with rapid movements than low arousal emotions, such as sadness and contentment. Last but not least, electroencephalogram (EEG), as one representative psychological signal, is another important method to record the electrical and emotional activity of the brain  [15] . Compared to other aforementioned explicit cues, the collection of EEG signals is typically more difficult and unnatural, regardless of whether electrodes are placed noninvasively along the scalp, or invasively using electrocorticography.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Implicit Affective Stimuli",
      "text": "Text is a form to record the natural language of human beings, which can implicitly carry informative emotions  [16, 17] . Text has different levels of linguistic components, including word, sentence, paragraph, and article, which are well studied; many off-the-shelf algorithms have been developed to segment text into small pieces. Then, the affective attribute of each linguistic piece is recognized with the help of a publicly available dictionary like SentiWordNet, and the emotion evoked by the text can be deduced. A digital audio signal is a representation of sound, typically stored and transferred using a series of binary numbers  [12] . Audio signals may be synthesized directly or may originate at a transducer such as a microphone or a musical instrument. Different from speech that mainly focuses on human vocal information and whose content may be translated into natural language, audio is more general including any sound like music or birdsong. An image is a distribution of colored dots over space  6  . It is well known that \"a picture is worth a thousand words\". It has been demonstrated in psychology that emotions can be evoked in humans by images  [18] . The explosive growth of images shared online and the powerful descriptive ability of scenes enable images to become crucial affective stimuli with extensive research efforts attracted  [10] . Video naturally contains multiple modalities at the same time, such as visual, audio, and textual information  [19] . That means temporal, spatial, and multi-channel representations can be learned and utilized to recognize the emotions in videos.\n\nIV. DATA COLLECTIONS AND EMOTION ANNOTATIONS Two steps are usually involved in constructing an MER dataset: data collection and emotion annotation. The collected data can be roughly divided into two categories: selecting from existing data and new recording in specific environments. On the one hand, some data is selected from movies, reviews, videos, and TV shows in online social networks, such as YouTube and Weibo. For example, the review videos in ICT-MMMO and MOUD are collected from YouTube; audiovisual clips are extracted from the TV series in MELD; online reviews from the Food and Restaurants categories are crawled in Yelp; the video-blogs or vlogs typically with one speaker looking at the camera from YouTube are collected in CMU-MOSI to capture the speakers' information. Some collected data provides a transcription of speech either manually (e.g., CMU-MOSI, CH-SMIS) or automatically (e.g., ICT-MMMO, MELD). On the other hand, some data is newly recorded with different sensors in specifically designed environments. For example, the participants' physiological signals and frontal facial changes induced by music videos are recorded in DEAP.\n\nThere are different kinds of emotion annotation strategies. Some datasets have target emotions and do not need to be annotated. For example, in EMODB, each sentence performed by actors corresponds to a target emotion. For some datasets, the emotion annotations are obtained automatically. For example, in Multi-ZOL, the integer sentiment score for each review, ranging from 1 to 10, is regarded as the sentiment label. Several workers are employed to annotate the emotions in some datasets, such as VideoEmotion-8. The datasets with recorded data are usually annotated by participants' selfreporting, such as MAHNOB-HCI. Besides, the emotion labels of most datasets are obtained by major voting. For DES model, 'FeelTrace' and 'SAM' are often used for annotation. The former one is based on activation-evaluation space, which allows observers to track the emotional content of stimulus as they perceive it over time. The latter one is a tool that accomplishes emotion rating based on different Likert scales. Some commonly used datasets are summarized in Table  I .\n\nV. COMPUTATIONAL TASKS Given multi-modal affective signals, we can conduct different MER tasks, including classification, regression, detection, and retrieval. In this section, we will briefly introduce what these tasks do.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Emotion Classification",
      "text": "In the emotion classification task, we assume that one instance can only belong to one or a fixed number of emotion categories, and the goal is to discover class boundaries or class distributions in the data space  [16] . Current works mainly focus on the manual design of multi-modal features and classifiers or employing deep neural networks in an endto-end manner. As defined as a single label learning (SLL) problem, MER assigns a single dominant emotion label to each sample. However, emotion may be a mixture of all components from different regions or sequences rather than a single representative emotion. Meanwhile, different people may have Modalities: t-text, EEG, PPS, GSR, RA, ST, ECG, BVP, EMG, EOG, SCL, and HR are short for transcript text, Electroencephalogram, Peripheral-Physiological-Signal, Galvanic-Skin-Response, Respiration-Rmplitude, Skin Temperature, Electrocardiogram, Blood-Volume-Pressure, Electromyogram, Electrooculogram, Skin-Conductance-Level, and Heart-Rate, respectively. Emotion labels: ang, sad, hap, dis, fea, sur, fru, exc, neu, pos, neg, joy, amu, anx, ero, hor, sce, obm, ant, and tru are short for angry, sadness, happiness, disgust, fear, surprise, frustration, excited, neutral, positive, negative, joy, amusement, anxiety, erotic, horror, scenery, object-manipulation, anticipation, and trust, respectively; -L, F, and -P are short for liking, familiarity, and predictability, respectively.\n\ndifferent emotional reactions to the same stimulus, which is caused by a variety of elements like personality. Thus, multilabel learning (MLL) has been utilized to study the problem where one instance is associated with multiple emotion labels.\n\nRecently, to address the problem that MLL does not fit some real applications well where the overall distribution of different labels' importance matters, label distribution learning (LDL) is proposed to cover a certain number of labels, representing the degree to which each emotion label describes the instance  [20] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Emotion Regression",
      "text": "Emotion regression aims to learn a mapping function that can effectively associate one instance with continuous emotion values in a Cartesian space. The most common regression algorithms for MER aim to assign the average dimension values to the source data. To deal with the inherent subjectivity characteristic of emotions, researchers propose to predict the continuous probability distribution of emotions which are represented in dimensional valence-arousal (VA) space. Specifically, VA emotion labels can be represented by a Gaussian mixture model (GMM), and then the emotion distribution prediction can be formalized as a parameter learning problem  [21] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Emotion Detection",
      "text": "As the raw data does not ensure carrying emotions, or only part of the data can evoke emotional reactions, emotion detection aims to find out which kind of emotion lies where in the source data. For example, a restaurant review on Yelp might be \"This location is conveniently located across the street from where I work, being walkable is a huge plus for me! Food wise, it's the same as almost every location I've visited so there's nothing much to say there. I do have to say that the customer service is hit or miss.\" Meanwhile, the overall rating score is three stars out of five. This review contains different emotions and attitudes: positive in the first sentence, neutral in the second sentence, and negative in the last sentence. As such, it is crucial for the system to detect which sentence corresponds to which emotion. Another example is affective region detection in images  [22] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "D. Emotion Retrieval",
      "text": "How to search affective content based on human perception is another meaningful task. The existing framework first detects local interest patches or sequences in the query and candidate data sources. Then, it discovers all matched pairs by determining whether the distance between two patches or sequences is less than a given fixed threshold. The similarity score between the query and each candidate is calculated as the quantity of matched components, followed by ranking the candidates of this query accordingly. While an affective retrieval system is useful for obtaining online content with desired emotions from a massive repository  [10] , again the abstract and subjective characteristics make the task challenging and difficult to evaluate.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Challenges",
      "text": "As stated in Section I, multi-modal emotion recognition (MER) has several advantages as compared to uni-model emotion recognition but it also faces more challenges.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Affective Gap",
      "text": "The affective gap is one main challenge for MER, which measures the inconsistency between extracted features and perceived high-level emotions. The affective gap is even more challenging than the semantic gap in objective multimedia analysis. Even if the semantic gap is bridged, there might still exist an affective gap. For example, a blooming rose and a faded rose both contain a rose but can evoke different emotions. For the same sentence, different voice intonations may correspond to totally different emotions. Extracting discriminative high-level features and especially those related to emotions can help to bridge the affective gap. The main difficulty lies in how to evaluate whether the extracted features are related to emotions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Perception Subjectivity",
      "text": "Due to many personal, contextual, and psychological factors, such as the cultural background, personality, and social context, different people might have different emotional responses to the same stimuli  [10] . Even if the emotion is the same, their physical and psychological changes can also be quite different. For example, all the 36 videos in the ASCERTAIN dataset for MER are labeled with at least four out of seven different valence and arousal scales by 58 subjects  [15] . This clearly indicates that some subjects have the opposite emotional reactions to the same stimuli. Take a short video with storm and thunder for instance, some people may feel in awe because they have never seen such extreme weather, some may feel fear because of the loud thunder noise, some may feel excited to capture such rare scenes, some may feel sad because they have to cancel their travel plans, etc. Even for the same emotion (e.g., excitement), there are different reactions, such as facial expression, gait, action, and speech. For the subjectivity challenge, one direct solution is to learn personalized MER models for each subject. From the perspective of stimuli, we can also predict the emotion distribution when a certain number of subjects are involved. Besides the content of the stimuli and direct physical and psychological changes, jointly modeling the above-mentioned personal, contextual, and psychological factors would also contribute to the MER task.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Data Incompleteness",
      "text": "Because of the presence of many inevitable factors in data collection, such as sensor device failure, the information in specific modalities might be corrupted, which results in missing or incomplete data. Data incompleteness is a common phenomenon in real-world MER tasks. For example, for explicit affective cues, an EEG headset might record contaminated signals or even fail to record any signal; at night, the cameras cannot capture clear facial expressions. For implicit affective stimuli, one user might post a tweet only containing an image (without text); for some videos, the audio channel does not change much. In such cases, the simplest feature fusion method, i.e., early fusion, does not work, because we cannot extract any features given no captured signal. Designing effective fusion methods that can deal with data incompleteness is a widely employed strategy.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Cross-Modality Inconsistency",
      "text": "Different modalities of the same sample may conflict with each other and thus express different emotions. For example, facial expression and speech can be easily suppressed or masked to avoid being detected, but EEG signals that are controlled by the central nervous systems can reflect humans' unconscious body changes. When people post tweets on social media, it is very common that the images are not semantically correlated to the text. In such cases, an effective MER method is expected to automatically evaluate which modalities are more reliable, such as by assigning a weight to each modality.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Cross-Modality Imbalance",
      "text": "In some MER applications, different modalities may contribute unequally to the evoked emotion. For example, online news plays an important role in our daily lives, and in addition to understanding the preferences of readers, predicting their emotional reactions is of great value in various applications, such as personalized advertising. However, a piece of online news usually includes imbalanced texts and images, i.e., the length of the article may be very long with lots of detailed information, while only one or two illustrations are inserted into the news. Potentially more problematic, the editor of the news may select a neutral image for an article with an obvious sentiment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "F. Label Noise And Absence",
      "text": "Existing MER methods, especially the ones based on deep learning, require large-scale labeled data for training. However, in real-world applications, labeling the emotions in the groundtruth generation is not only prohibitively expensive and timeconsuming but also highly inconsistent, which results in a large amount of data but with few or even no emotion labels. With the increasingly diverse and fine-grained emotion requirement, we might have enough training data for some emotion categories but not for others. One alternate solution to manual annotation is to leverage the tags or keywords of social tweets as emotion labels, but such labels are incomplete and noisy. As such, designing effective algorithms for unsupervised/weaklysupervised learning and few/zero shot learning can provide potential solutions.\n\nMeanwhile, we might have sufficient labeled affective data in one domain, such as synthetic facial expression and speech. The problem turns to how to effectively transfer the trained MER model on the labeled source domain to another unlabeled target domain. The presence of domain shift causes significant performance decay when a direct transfer is used  [23] . Multimodal domain adaptation and domain generalization can help to mitigate such domain gap. Practical settings, such as multiple source domains, should also be considered.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Vii. Computational Methodologies",
      "text": "Generally, there are three components in an MER framework with sufficient labeled training data in the target domain: representation learning, feature fusion, and classifier optimization, as shown in Fig.  2 . In this section, we will introduce these components. Further, we will describe domain adaptation when there is no labeled training data in the target domain and when sufficient labeled data is available in another related source domain.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Representation Learning Of Each Affective Modality",
      "text": "To represent the text as a form that can be understood by computers, the following aspects are required: first, representing the symbolic words as real numbers for the next computation; second, modeling the semantic relationships; and finally, obtaining a unified representation for the whole text  [16] . In the beginning, words are represented by one-hot vectors with the length of vocabulary size, where for the tth word in the vocabulary, w t , only the position t is 1 and the other positions are 0. As the scale of the data increases, the dimension of this one-hot vector increases dramatically. Later, researchers use language models to train word vectors by predicting context, obtaining word vectors with vectors of fixed dimension. Popular word vector representation models include word2vec, GLOVE, BERT, XLNet, and so on. The text feature extraction methods have developed from simple ones to complex ones as well. Text features can be obtained by simply averaging word vectors. A recurrent neural network (RNN) is used to model the sequential relations of words in the text. A convolutional neural network (CNN) which has been widely used in the computer vision community, is also used to extract contextual relations between words.\n\nTo date, plenty of methods have been developed to design representative features for emotion stimuli in audios  [13, 14] . It has been found that audio features such as pitch, log energy, zero-crossing rate, spectral features, voice quality, and jitter, are useful in emotion recognition. The ComParE acoustic feature set is commonly used as the baseline set for the ongoing Computation Paralinguistics Challenge series since 2013. However, because of possible high similarities in certain emotions, single type of audio feature is not discriminative enough to classify emotions. To solve this problem, some approaches propose to combine different types of features. Recently, with the development of deep learning, CNN is shown to achieve state-of-the-art performance on large-scale tasks in many domains dealing with natural data, and audio emotion recognition is of course also included. Audio is typically transferred into a graphical representation, such as a spectrogram, to be fed into a CNN. Since CNN uses shared weight filters and pooling to give the model better spectral and temporal invariant properties, it typically yields better generalized and more robust models for emotion recognition.\n\nResearchers have designed informative representations for emotional stimuli in images. In general, images can be divided into two types, non-restrictive images and facial expression images. For the former, e.g., natural images, various handcrafted features including color, texture, shape, composition, etc., are developed to represent image emotion in the early years  [10] . These low-level features are developed with inspiration from psychology and art theory. Later, mid-level features based on the visual concepts are presented to bridge the gap between the pixels in images and the emotion labels. The most representative engine is SentiBank, which is composed of 1, 200 adjective-noun pairs and shows remarkable and robust recognition performance among all the hand-engineering features. In the era of deep learning, CNN is regarded as a strong feature extractor in an end-to-end manner. Specifically, to integrate various representations of different levels, features are extracted from multiple layers of CNN. Meanwhile, an attention mechanism is employed to learn better emotional representations of specific local affective regions  [22] . For the facial expression images, firstly the human face is detected and aligned, and then the face landmarks are encoded for the recognition task. Note that for those non-restrictive images that contain human faces by chance, facial expression can be treated as an important mid-level cue.\n\nIn the above, we have mentioned how to identify emotions in the isolated modalities. Here, we first focus on perceiving emotions from successive frames. Then, we introduce how to build joint representation for videos. Compared to a single image, a video contains a series of images with temporal information  [19] . To build representations of videos, a wide range of methods has been proposed. Early methods mainly utilize hand-crafted local representations in this field, which include color, motion, and shot cut rate. With the advent of deep learning, recent methods extract discriminative representations by adopting a 3D CNN that captures the temporal information encoded in multiple adjacent frames. After extracting modality-specific features in videos, integrating different types of features could obtain more promising results and improve the performance.\n\nTo perceive emotions, there are mainly two aspects of ways to learn the representations of gait  [24] . For one thing, we can explicitly model the posture and movement information that is related to the emotions. To model this information, we first extract the skeletal structure of a person and then represent each joint of the human body using the 3D coordinate system. After getting these coordinates, the angles, distance, or area among different joints (posture information), velocity/acceleration (movement information), their co-variance descriptors, etc. can be easily extracted. For another thing, highlevel emotional representations can be modeled from gait by long short-term memory (LSTM), deep convolutional neural networks, or graph convolutional networks. Some methods extract optical flow from gait videos and then extract sequence representations using these networks. Other methods learn skeletal structures of the gait and then feed them into multiple networks to extract discriminate representations.\n\nSince various information about emotions, such as frequency band, electrodeposition, and temporal information, can be explored from the brain's response to emotional stimuli, EEG signals are widely used in emotion analysis  [15] . To extract discriminative features for EEG emotion recognition, differential entropy features from frequency band or electrodeposition relationship are very popular in previous work. Besides hand-crafted features, we can also directly apply endto-end deep learning neural networks such as CNN and RNN on the raw EEG signals to obtain powerful deep features  [25] . Inspired by the learning pattern of humans, spatial-wise attention mechanisms are successfully applied to extract more discriminative spatial information. Furthermore, considering that EEG signals contain multiple channels, a channel-wise attention mechanism can also be integrated into CNN to exploit the inter-channel relationship among feature maps.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "B. Feature Fusion Of Different Affective Modalities",
      "text": "Feature fusion, as one key research topic in MER, aims to integrate the representations from multiple modalities to predict either a specific category or a continuous value of emotions. Generally, there are two strategies: model-free fusion and model-based fusion  [7, 9] . Model-free fusion that is not directly dependent on specific learning algorithms has been widely used for decades. We can divide it into early fusion, late fusion, and hybrid fusion  [5] . All these fusion methods can be extended from existing uni-modal emotion recognition classifiers. Early fusion, also named feature-level fusion, directly concatenates the feature representations from different modalities as a single representation. It is the most intuitive method to fuse different representations by exploiting the interactions between different modalities at an early stage and only requires training a single model. But since the representations from different modalities might significantly differ, we have to consider the time synchronization problem to transform these representations into the same format before fusion. When one or more modalities are missing, such early fusion would fail. Late fusion, also named decision-level fusion, instead integrates the prediction results from each single modality. Some popular mechanisms include averaging, voting, and signal variance. The advantages of late fusion include (1) flexibility and superiority -the optimal classifiers can be selected for different modalities; and (2) robustness -when some modalities are missing, late fusion can still work. However, the correlations between different modalities before decision are ignored. Hybrid fusion combines early fusion and late fusion to exploit their advantages in a unified framework but with higher computational cost.\n\nModel-based fusion that explicitly performs fusion during the construction of the learning models has been paid more attention  [7, 9] , as shown in Fig.  3 , since model-free fusion is based on some simple techniques that are not specifically designed for multimodal data. For shallow models, kernelbased fusion and graph-based fusion are two representative methods; for recent popular deep models, neural networkbased fusion, attention-based fusion, and tensor-based fusion are often used.\n\nKernel-based fusion is extended based on classifiers that contain kernels, such as SVM. For different modalities, different kernels are used. The flexibility in kernel selection and convexity of the loss functions make multiple kernel learning fusion popular in many applications, including MER. However, during testing, these fusion methods rely on the support vectors in the training data, which results in large memory cost and inefficient reference. Graph-based fusion constructs separate graphs or hypergraphs for each modality, combines these graphs into a fused one, and learns the weights of different edges and modalities by graph-based learning. It can well deal with the data incompleteness problem simply by constructing graphs based on available data. Besides the extracted feature representations, we can also incorporate prior human knowledge into the models by corresponding edges. However, the computational cost would increase exponentially when more training samples are available.\n\nNeural network-based fusion employs a direct and intuitive strategy to fuse the feature representations or predicted results of different modalities by a neural network. Attentionbased fusion uses some attention mechanisms to obtain the weighted sum of a set of feature representations with scalar weights that are dynamically learned by an attention module. Different attention mechanisms correspond to fusing different components. For example, spatial image attention measures the importance of different image regions. Image and text coattention employs symmetric attention mechanisms to generated both attended visual and attended textual representations. Parallel co-attention and alternating co-attention methods can be used to respectively generate attention for different modalities simultaneously and one by one. Recently, a Multimodal Adaptation Gate (MAG) is designed to enable Transformerbased contextual word representations, such as BERT and XLNet, to accept multi-modal nonverbal data  [17] . Based on the attention conditioned on the nonverbal behaviors, MAG can essentially map the informative multiple modalities to a vector with a trajectory and magnitude. Tensor-based fusion tries to exploit the correlations of different representations by some specific tensor operations, such as outer product and polynomial tensor pooling. These fusion methods for deep models are capable of learning from a large amount of data in an end-to-end manner with good performance but suffer from low interpretability.\n\nOne important property of the above-mentioned feature fusion methods is whether they support temporal modeling for MER in videos. It is obvious that early fusion can while late fusion and hybrid fusion cannot, since the predicted results based on each modality are already known before late fusion. For model-based fusion, excluding kernel-based fusion, all others can be used for temporal modeling, such as hidden Markov models (HMM) and conditional random fields (CRF) for graph-based fusion methods, and RNN and LSTM networks for neural network-based fusion.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Classifier Optimization For Multi-Modal Emotion Recognition",
      "text": "For the text represented as a sequence of word embeddings, the most popular approaches to leverage the semantics among words are RNN and CNN. LSTM, as a typical RNN, contains a series of cells with the same structure. Every cell takes a word embedding and the hidden state from the last cell as input, computes the output, and updates the hidden state for the following cell. The hidden state records the semantics of previous words. CNN computes local contextual features among consecutive words through convolution operations. And average pooling or max-pooling layers are used to further integrate the obtained features for the following sentiment classification. Recently, researchers begin to use Transformerbased methods, e.g., BERT and GPT-3. Transformer is implemented as a series of modules containing a multi-head self-attention layer followed by a normalization layer, a feedforward network, and another normalization layer. The order of words in the text is also represented by another position embedding layer. Compared with RNN, transformer does not require sequential processing of words, which improves the parallelizability. And compared with CNN, transformer can model relationships between more distant words.\n\nThe classification approaches used in audio emotion recognition generally include the following two options: traditional methods and deep learning-based methods. For traditional methods, HMM is a representative method because of its capability of capturing dynamic characteristics of sequential data. SVM is also widely utilized in audio emotion recognition. Deep learning-based methods have become more and more popular since they are not restricted by the classical independence assumptions of HMM models. Among these methods, sequence-to-sequence models with attention have shown success in an end-to-end manner. Recently, some approaches significantly extend the state of the art in this area by developing deep hybrid convolutional and recurrent models  [14] .\n\nIn the early years, similar to this task in other modalities, multiple hand-crafted image features are integrated and input into SVM to train classifiers. Then, based on deep learning, the classifier and feature extractor are connected and optimized in an end-to-end manner by corresponding loss functions like cross-entropy loss  [26] . Besides, popular metric losses such as triplet loss and N-pair loss also take part in the network optimization to obtain more discriminative features. With the above learning paradigm, each image is predicted as a single dominant emotion category. However, based on the theories of psychology, an image may evoke multiple emotions in viewers, which leads to an ambiguous problem. To address the problem, label distribution learning is employed to predict a concrete relative degree for each emotion category, where Kullback-Leibler divergence is the most popular loss function. Some informative and attractive regions of an image always determine the emotion of the image. Therefore, a series of architecture with extra attention or detection branch is constructed. With the optimization for multiple tasks including attention and original task, a more robust and discriminative   The numbers on the left side and the right side of \"/\" are the MER results based on BERT and XLNet, respectively.\n\nmodel is obtained. Most existing methods employ a two-stage pipeline to recognize video emotion, i.e., extracting visual and/or audio features and training classifiers. For training classifiers, many machine learning methods have been investigated to model the mapping between video features and discrete emotion categories, including SVM, GMM, HMM, dynamic Bayesian networks (DBNs), and conditional random fields (CRF). Despite the above methods have contributed to the development of emotion recognition in videos, recent methods are proposed to recognize video emotions in an end-to-end manner based on deep neural networks due to their superior capability  [27] . CNN-based methods first employ 3D convolutional neural networks to extract high-level spatio-temporal features which contain affective information, and then use fully connected layers to classify emotions. Finally, the models are followed by the loss function to optimize the whole network. Inspired by the human process of perceiving emotions, CNN-based methods employ the attention mechanism to emphasize emotionally relevant regions of frames or segments in each video. Furthermore, considering the polarity-emotion hierarchy constraint, recent methods propose polarity-consistent cross-entropy loss, to guide the attention generation.\n\nThe gait of a person can be represented as a sequence of 2D or 3D joint coordinates for each frame in the walking videos.\n\nTo leverage the inherent affective cues in the coordinates of joints, many classifiers or architectures have been used to extract affective features in the gait. LSTM networks contain many special units, i.e., memory cells, and can store the joint coordinate information from particular time steps in a longtime data sequence. Thus, it is used in some early work of gait emotion recognition. The hidden features of the LSTM can be further concatenated with the hand-crafted affective features and are then fed into a classifier (e.g., SVM or random forest (RF)) to predict emotions. Recently, another popular network used in gait emotion prediction is the spatial-temporal graph convolutional network (ST-GCN). ST-GCN is initially proposed for action recognition from human skeletal graphs. 'Spatial' represents the spatial edges in the skeletal structure, which are the limbs that connect the body joints. 'Temporal' refers to temporal edges, and they connect the positions of each joint across different frames. ST-GCN can be easily implemented as a spatial convolution followed by a temporal convolution, which is similar to the deep convolutional networks. EEG-based emotion recognition usually employs various classifiers such as SVM, decision trees, and k-nearest neighbor to classify hand-crafted features in the early stage. Later, since CNN and RNN are good at extracting spatial information and temporal information of EEG signals, respectively, end-to-end structures such as cascade convolutional recurrent network (which combines CNN and RNN), LSTM-RNN, and parallel convolutional recurrent neural networks are successfully designed and applied to emotion recognition tasks.\n\n1) Quantitative Comparison of Representative MER Methods: To give readers an impression on the performances of state-of-the-art MER methods, we conduct experiments to fairly compare some representative methods based on the released codes of CMU-Multimodal SDK  7  and Multimodal Adaptation Gate  8  . Specifically, the compared non-deep meth- From the results in Table  II  and Table  III , we have the following observations: First, the performances of deep models are generally better than non-deep ones. Second, for different datasets, the methods with the best performances are different. For example, RF achieves the best performance among nondeep models except CMU-MOSI, which demonstrates its good generalization ability, while the performance of SVM is much better than RF and THMM on CMU-MOSI. Third, multiclass classification is more difficult than binary classification, such as 77.1 vs. 34.7 of MARN on CMU-MOSI. Fourth, comparing the same method in the two tables on CMU-MOSI, we can conclude that BERT and XLNet can provide better word embeddings than GLOVE and XLNet is generally better than BERT. Finally, although XLNet-based MAG achieves near-human level performance on CMU-MOSI, there is still some gap and more efforts are expected to achieve even better performance than humans.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Domain Adaptation For Multi-Modal Emotion Recognition",
      "text": "Domain adaptation aims to learn a transferable MER model from labeled source domains that can perform well on unlabeled target domains  [23] . Recent efforts have been dedicated to deep unsupervised domain adaptation  [23] , which employs a two-streams architecture. One stream is used to train an MER model on the labeled source domains, while the other is used to align the source and target domains. Based on the alignment strategy, existing uni-modal domain adaptation methods can be classified into different categories  [23] , such as discrepancybased, adversarial discriminative, adversarial generative, and self-supervision-based methods.\n\nDiscrepancy-based methods employ some distance metrics to explicitly measure the discrepancy between the source and target domains on corresponding activation layers of the two network streams. Commonly used discrepancy loss include maximum mean discrepancy, correlation alignment, geodesic distance, central moment discrepancy, Wasserstein discrepancy, contrastive Domain discrepancy, and higher-order moment matching. Besides the used discrepancy loss, there are some other differences between existing methods, such as whether the loss is domain-level or class-level, which layer the loss is operated on, whether the backbone networks share weights or not, and whether the aligned distribution is marginal or joint. Adversarial discriminative models usually align the source and target domains with a domain discriminator by adversarially making different domains indistinguishable. The input to the discriminator ranges from original data to extracted features and the adversarial alignment can be global or class-wise. We can also consider using shared or unshared feature extractors. Adversarial generative models usually employ a generator to generate fake source or target data to make the domain discriminator indistinguishable from the generated and real domains. The generator is typically based on generative adversarial network (GAN) and its variants, such as CoGAN, SimGAN, and CycleGAN. The input to the generator and discriminator can be different in different methods. Self-supervision-based methods combine some auxiliary self-supervised learning tasks, such as reconstruction, image rotation prediction, jigsaw prediction, and masking, with the original task network to bring the source and target domains closer. We can compare these four types of domain adaptation methods from the perspectives of theory guarantee, efficiency, task scalability, data scalability, data dependency, optimizability, and performance. We can combine some of these methods to jointly exploit their advantages.\n\nThe main difficulty in domain adaptation for MER lies in the alignment of multiple modalities between the source and target domains simultaneously. There are some simple but effective ways to extend uni-modal domain adaptation to multi-modal settings, as shown in Fig.  4 . For example, we can use discrepancy loss or discriminator to align the fused feature representations. The correspondence between different modalities can be used as a self-supervised alignment. Extending adversarial generative models from uni-modal to multi-modal would be more difficult. Unlike image, other generated modalities, such as text and speech, might have confused semantics, although they can make the discriminator indistinguishable. Generating intermediate feature representations instead of raw data can provide a feasible solution.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Viii. Applications",
      "text": "Recognizing emotions from multiple explicit cues and implicit stimuli is of great significance in a broad range of realworld applications. Generally speaking, emotion is the most important aspect of the quality and meaning of our existence, which makes life worth living. The emotional impact of digital data lies in that it can improve the user experience of existing techniques and then strengthen the knowledge transfer between people and computers  [18] .\n\nMany people tend to post texts, images, and videos on social networks to express their daily life feelings. Inspired by this, we can mine people's opinions and sentiments towards topics and events happening in the real world  [28] . For instance, user-generated content in Facebook or Instagram can be used to derive the attitudes of people from different countries and regions when they face epidemics like COVID-19  [29] . Researchers also try to detect sentiment in social networks and apply the results to predict political elections. Note that when the personalized emotion of an individual is detected, we can further group these emotions, which may contribute to predicting the tendencies of society.\n\nAnother important application of multi-modal emotion recognition is business intelligence, especially marketing and consumer behavior analysis  [30] . Nowadays, most apparel eretailers use human models to present products. The model's face presentation is proved to have a significant effect on consumer approach behavior. To be specific, for participants whose emotional receptivity is high, smiling facial expression tends to lead to the highest approach behavior. Besides, researchers examine how online store specialization influences consumer pleasure and arousal, based on the stimulusorganism-response framework. Emotion recognition can also be used in call centers, the goal of which is to detect the emotional states of both the caller and the operator. The system recognizes the involved emotions through the intonation and tempo, as well as the texts translated from the corresponding speech. Based on this, we can receive feedback on the quality of the service.\n\nMeanwhile, emotion recognition plays an important role in the field of medical treatment and psychological health. With the popularity of social media, some people prefer to sharing their emotions on the Internet rather than with others. If a user is observed to be sharing negative information (e.g., sadness) frequently and continuously, it is necessary to track her/his mental status to prevent the occurrence of psychological illness and even suicide. Emotional states can also be used to monitor and predict fatigue states of a variety of people like drivers, pilots, workers in assembly lines, and students in classrooms. This technique both prevents dangerous situations and benefits the evaluation of work/study efficiency. Further, emotional states can be incorporated into various security applications, such as systems for monitoring public spaces (e.g., bus/train/subway stations, football stadiums) for potential aggression. Recently, an effective auxiliary system is introduced in the diagnosis and treatment process of autism spectrum disorder (ASD) of children, to assist in collecting the pathological information. To help professional clinicians better and faster make a diagnosis and give treatment to ASD patients, this system characterizes facial expressions and eye gaze attention which are considered to be remarkable indicators for early screening of autism.\n\nMulti-modal emotion recognition is used to improve the personal entertainment experience. For example, a recent work in brainwave-music interface maps EEG characteristics to musical structures (note, intensity, and pitch). Similarly, efforts have been made to understand the emotion-centric correlation between different modalities that are essential for various applications. Affective image-music matching provides a good chance to append a sequence of music to a given image, where they may evoke the same emotion. This helps generate emotion-aware music playlists from one's personal album photos in mobile devices.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Ix. Future Directions",
      "text": "Existing methods have achieved promising performances on various MER settings, such as visual-audio, facial-textualspeech, and textual-visual tasks. However, all the summarized challenges have not been fully addressed. For example, how to extract discriminative features that are more related to emotion, how to balance between common and personalized emotion reactions, and how to emphasize the more important modalities are still open. To help improve the performances of MER methods and make them fit special requirements in real world, we provide some potential future directions.\n\nNew Methodologies for MER. 1) Contextual and prior knowledge modeling. The experienced emotion of a user can be significantly influenced by the contextual information, such as the conversational and social environments. The prior knowledge of users, such as personality and age, can also contribute to emotion perception. For example, an optimistic user and a pessimistic viewer are likely to see different aspects of the same stimuli. Jointly considering these important contextual and prior knowledge is expected to improve the MER performance. Graph-related methods, such as graph convolutional networks, are possible solutions to model the relationships between factors and emotions. 2) Learning from unlabeled, unreliable, and unmatched affective signals. In the big data era, the affective data might be sparsely labeled or even unlabeled, the raw data or labels can be reliable, and the test and training data might be unmatched. Exploring advanced machine learning techniques, such as unsupervised representation learning, dynamic data selection and balancing, and domain adaptation, and embedding the special properties of emotions, can help to address these challenges. 3) Explainable, robust, and secure deep learning for MER. Due to the black-box nature, it is difficult to understand why existing deep neural networks perform well for MER and the trained deep networks are vulnerable to adversarial attacks and inevitable noises that might cause erraticism. Essentially explaining the decision-making process of deep learning can help to design robust and secure MER systems. 4) Combination of explicit and implicit signals. Both explicit and implicit signals are demonstrated to be useful for MER but they also suffer from some limitations. For example, explicit signals can be easily suppressed or are difficult to capture, while implicit signals might not reflect the emotions in real-time. Jointly combining them to explore the complementary information during viewermultimedia interaction would boost the MER performance. 5) Incorporation of emotion theory into MER. Different theories have been proposed in psychology, physiology, neurology, and cognitive sciences. These theories can help to understand how humans produce emotion but have not been employed in the computational MER task. We believe such incorporation would make more sense to recognize emotions.\n\nMore Practical MER Settings. 1) MER in the wild. Current MER methods mainly focus on neat lab settings. However, MER problems in the real world are much more complex. For example, the collected data might contain much noise that is unrelated to emotion; the users in the test set are from different cultures and languages from those in the training set, which results in different ways of emotion expression; different emotion label spaces are employed across various settings; training data is incrementally available. Designing an effective MER model that is generalizable to these practical settings is worth investigating. 2) MER on the edge. When deploying MER models in edge devices, such as mobile phones and security cameras, we have to consider the computing limitation and data privacy. Techniques like auto pruning, neural architecture search, invertible neural network, and softwarehardware co-design are believed to be beneficial for efficient on-device training. 3) Personalized and group MER. Because of the emotion's subjectivity, simply recognizing the dominant emotion of different individuals is insufficient. It is ideal but impractical to collect enough data for each individual to train personalized MER models. Adapting the well-trained MER models for dominant emotions to each individual with a small amount of labeled data is a possible alternate solution. On the other hand, it would make more sense to predict emotions for groups of individuals who share similar tastes or interests and have a similar background. Group emotion recognition is essential in many applications, such as recommendation systems, but how to classify users into different groups is still challenging.\n\nReal Applications Based on MER. 1) Implementation of MER in real-world applications. Although emotion recognition has been emphasized to be important for decades, it has rarely been applied to real scenarios due to relatively low performance. With the recent rapid progress of MER, we can begin incorporating emotion into different applications in marketing, education, health care, and service sectors. The feedback from the applications can in turn promote the development of MER. Together with emotion generation, we believe an age of artificial emotional intelligence is coming. 2) Wearable, simple, and accurate affective data collection. To conduct MER tasks, the first step is to collect accurate affective data. Developing wearable, simple and even contactless sensors to capture such data would make users more acceptable.\n\n3) Security, privacy, ethics, and fairness of MER. During data collection, it is possible to extract users' confidential information, such as identity, age, etc. Protecting the security and privacy of users and avoiding any chance of misuse must be taken into consideration. Emotion recognition in real applications might have a negative and even dangerous impact on a person, such as emotional pressure. Methods to eliminate such impact should also be considered from the perspectives of ethics and fairness.",
      "page_start": 11,
      "page_end": 13
    },
    {
      "section_name": "X. Conclusion",
      "text": "In this article, we provided a comprehensive tutorial on multi-modal emotion recognition (MER). We briefly introduced emotion representation models, both explicit and implicit affective modalities, emotion annotations, and corresponding computational tasks. We summarized the main challenges of MER in detail, and then we emphatically introduced different computational methodologies, including representation learning of each affective modality, feature fusion of different affective modalities, classifier optimization for MER, and domain adaptation for MER. We ended this tutorial with discussions on real-world applications and future directions. We hope this tutorial can motivate novel techniques to facilitate the development of MER, and we believe that MER will continue to attract significant research efforts.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Illustration of multiple modalities for emotion recognition: explicit",
      "page": 1
    },
    {
      "caption": "Figure 1: As compared to uni-modal emotion recognition, multi-modal",
      "page": 2
    },
    {
      "caption": "Figure 2: Illustration of a widely used MER framework, which consists of three components: representation learning to extract feature representations, feature",
      "page": 6
    },
    {
      "caption": "Figure 2: In this section, we will introduce",
      "page": 6
    },
    {
      "caption": "Figure 3: Illustration of different model-based fusion strategies, where n is the",
      "page": 7
    },
    {
      "caption": "Figure 3: , since model-free fusion",
      "page": 7
    },
    {
      "caption": "Figure 4: A generalized framework for multi-modal domain adaptation with one labeled source domain and one unlabeled target domain. The gray-scale",
      "page": 10
    },
    {
      "caption": "Figure 4: For example,",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "What an exciting day!": "I will never forget it."
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "Text"
        },
        {
          "What an exciting day!": "Image\nVideo"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "Fig. 1.\nIllustration of multiple modalities\nfor emotion recognition: explicit"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "affective cues (top) and implicit affective stimuli\n(bottom)."
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "can be intelligent without emotions. [2]. Enabling machines"
        },
        {
          "What an exciting day!": "to have emotional\nintelligence i.e.,\nrecognizing,\ninterpreting,"
        },
        {
          "What an exciting day!": "processing,\nand\nsimulating\nemotions\nhas\nrecently\nbecome"
        },
        {
          "What an exciting day!": "increasingly\nimportant with wide\npotential\napplications\nin-"
        },
        {
          "What an exciting day!": "volving\nhuman-computer\ninteraction\n[3]. On\nthe\none\nhand,"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "emotionally intelligent machines can provide more harmonious"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "and personal services for human beings, especially the elderly,"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "disabled, and children. For example, the companion robots that"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "can work with emotions can better meet the psychological and"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "emotional needs of the elderly and help them stay comfortable."
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "On the other hand, by recognizing humans emotions automat-"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "ically and in real-time,\nintelligent machines can better identify"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "humans abnormal behaviors, send reminders to their relatives"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "and friends, and prevent extreme behaviors to themselves and"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "even to the rest of\nsociety. For example, an emotional driv-"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "ing monitoring system can automatically play some soothing"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "music to relax angry drivers who might be dissatised with a"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "trafc jam and can remind them to focus on driving safely."
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "The rst\nstep for\nintelligent machines\nto express human-"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "like emotions\nis\nto recognize and understand humans emo-"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "tions\ntypically\nthrough\ntwo\ngroups\nof\naffective modalities:"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "explicit affective cues and implicit affective stimuli. Explicit"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "affective cues correspond to specic physical and psycholog-"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "ical\nchanges\nin\nhumans\nthat\ncan\nbe\ndirectly\nobserved\nand"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "recorded,\nsuch as\nfacial\nexpression,\neye movement,\nspeech,"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "action, and physiological signals. These signals can be either"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "easily suppressed or masked, or difcult\nand impractical\nto"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "capture. Meanwhile,\nthe\npopularity\nof mobile\ndevices\nand"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "social networks enables humans\nto habitually share their ex-"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "periences and express their opinions online using text,\nimage,"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "audio, and video. Implicit affective stimuli correspond to these"
        },
        {
          "What an exciting day!": ""
        },
        {
          "What an exciting day!": "commonly-used digital media,\nthe analysis of which provides"
        },
        {
          "What an exciting day!": "an implicit way to infer humans emotions [4]."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Regardless\nof whether\nemotions\nare\nexpressed\nexplicitly",
          "2": "arousal, and dominance represent\nthe pleasantness,\nintensity,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "or\nimplicitly,\nthere are generally multiple modalities that can",
          "2": "and control degree of emotion,\nrespectively."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "contribute to the emotion recognition task, as shown in Fig. 1.",
          "2": "CES models\nagree better with humans\nintuition, but no"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "As compared to uni-modal emotion recognition, multi-modal",
          "2": "consensus has been reached by psychologists on how many"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "emotion recognition (MER) has several advantages. First, data",
          "2": "discrete emotion categories should be included. Further, emo-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "complementarity. Cues from different modalities can augment",
          "2": "tion is\ncomplex and subtle, which cannot be well\nreected"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "or complement each other. For example,\nif we see one post",
          "2": "by limited discrete categories. DES models can theoretically"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "from a\ngood\nfriend,\n\"What\ngreat weather!\",\nit\nis\nof\nhigh",
          "2": "measure all emotions as different coordinate points in the con-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "probability that\nthe\nfriend is\nexpressing a positive\nemotion;",
          "2": "tinuous Cartesian space, but the absolute continuous values are"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "but\nif there is also an auxiliary image of a storm, we can infer",
          "2": "beyond users understanding. These two types of denitions of"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "that\nthe text\nis actually a sarcasm and that a negative emotion",
          "2": "emotions are related, with possible transformation from CES"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "is\nintended to be expressed. Second, model\nrobustness. Due",
          "2": "to DES. For example, anger\nrelates to negative valence, high"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "to the inuence of many normally occurring factors\nin data",
          "2": "arousal, and high dominance."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "collection, such as sensor device failure, some data modalities",
          "2": "Besides emotion,\nthere are several other widely used con-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "might\nbe\nunavailable, which\nis\nespecially\nprevalent\nin\nthe",
          "2": "cepts in affective computing, such as mood, affect, and senti-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "wild. For example,\nin the CALLAS dataset containing speech,",
          "2": "ment. Emotions can be expected, induced, or perceived. We do"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "facial expression, and gesture modalities,\nthe gesture stream is",
          "2": "not aim distinguishing them in this article. Please refer to [11]"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "missing for\nsome momentarily motionless users\n[5].\nIn such",
          "2": "for more details on the differences or\ncorrelations between"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "cases,\nthe learned MER model can still work with the help",
          "2": "these concepts."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of other available modalities. Finally, performance superiority.",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Joint consideration of\nthe complementary information of dif-",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "III. AFFECTIVE MODALITIES"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ferent modalities can result\nin better recognition performance.",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "In the area of MER, multiple modalities are employed to"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Meta-analysis indicates that as compared to the best uni-modal",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "recognize and predict human emotions. The affective modal-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "counterparts, MER achieves 9.83% performance improvement",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "ities\nin MER can be roughly divided into two groups based"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "on average [6].",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "on whether emotions are recognized from humans physical"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "In this\narticle, we will give\na\ncomprehensive\ntutorial on",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "body changes or from external digital media: explicit affective"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "different\naspects\nof MER,\nincluding\npsychological models,",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "cues and implicit affective stimuli. The former group includes"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "affective modalities, data collections and emotion annotations,",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "facial\nexpression,\neye movement,\nspeech,\naction,\ngait,\nand"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "computational\ntasks,\nchallenges,\ncomputational methodolo-",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "electroencephalogram, all of which can be directly observed,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "gies,\napplications,\nand\nfuture\ndirections. There\nhave\nbeen",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "recorded,\nor\ncollected\nfrom an\nindividual. Meanwhile,\nthe"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "several\nreviews/surveys on MER related topics [4, 6, 7, 8, 9].",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "latter group indicates the commonly-used digital media types"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "In particular:\n[7]\nand [9]\ncover different\naspects of general",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "such\nas\ntext,\naudio,\nimage,\nand\nvideo. We\nuse\nthese\ndata"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "multi-modal machine\nlearning with few efforts on emotion",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "types to store information and knowledge as well as transfer"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "recognition;\n[6]\nfocuses on the quantitative review and meta-",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "them between digital devices.\nIn this way, emotions may be"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "analysis of\nexisting MER systems;\n[4]\nand [8]\nare\nsurvey-",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "implicitly involved and evoked. Although the efcacy of one"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "style MER articles with\nthe\ntechnical\nemphasis\non multi-",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "specic modality as\na\nreliable\nchannel\nto express\nemotions"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "modal\nfusion. Differently,\nthis\ntutorial-style\narticle\naims\nto",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "cannot be guaranteed,\njointly considering multiple modalities"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "give a quick and comprehensive MER introduction that\nis also",
          "2": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "2": "would signicantly improve the reliability and robustness [12]."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "suitable for non-specialists.",
          "2": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Speakers may express their intentions like asking or declaring",
          "3": "IV. DATA COLLECTIONS AND EMOTION ANNOTATIONS"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "by using various intonations, degrees of loudness, and tempo.",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "Two\nsteps\nare\nusually\ninvolved\nin\nconstructing\nan MER"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Specically, emotions can be revealed when people talk with",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "dataset: data collection and emotion annotation. The collected"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "each\nother,\nor\njust mutter\nto\nthemselves. As\nan\nimportant",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "data can be roughly divided into two categories: selecting from"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "part of human body language, action, also conveys massive",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "existing data and new recording in specic environments. On"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "emotional\ninformation. For\ninstance,\nan air punch is\nan act",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "the one hand,\nsome data\nis\nselected from movies,\nreviews,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of\nthrusting ones\nclenched st up into the\nair,\ntypically as",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "videos,\nand TV shows\nin\nonline\nsocial\nnetworks,\nsuch\nas"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "a gesture of\ntriumph or elation. Similar\nto action, emotions",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "YouTube and Weibo. For example,\nthe review videos in ICT-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "can\nbe\nperceived\nfrom a\npersons\ngait,\ni.e.,\ntheir walking",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "MMMO and MOUD are\ncollected\nfrom YouTube;\naudio-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "style. Psychology literature has proven that participants\ncan",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "visual clips are extracted from the TV series in MELD; online"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "identify the emotions of a subject by observing the subjects",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "reviews from the Food and Restaurants categories are crawled"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "posture,\nincluding\nlong\nstrides,\ncollapsed\nupper\nbody,\netc.5",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "in Yelp;\nthe video-blogs or vlogs\ntypically with one speaker"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Body movement (e.g., walking speed) also plays an important",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "looking at\nthe camera from YouTube are collected in CMU-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "role\nin\nthe\nperception\nof\ndifferent\nemotions. High\narousal",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "MOSI\nto capture\nthe\nspeakers\ninformation. Some\ncollected"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "emotions\nsuch as anger and excitement are more associated",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "data provides a transcription of speech either manually (e.g.,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "with rapid movements than low arousal emotions, such as sad-",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "CMU-MOSI, CH-SMIS) or automatically (e.g., ICT-MMMO,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ness and contentment. Last but not least, electroencephalogram",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "MELD). On the other hand, some data is newly recorded with"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "(EEG), as one representative psychological signal,\nis another",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "different\nsensors\nin specically designed environments. For"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "important method to record the electrical and emotional ac-",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "example,\nthe\nparticipants\nphysiological\nsignals\nand\nfrontal"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "tivity of\nthe brain [15]. Compared to other\naforementioned",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "facial changes induced by music videos are recorded in DEAP."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "explicit cues,\nthe collection of EEG signals is typically more",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "There are different kinds of emotion annotation strategies."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "difcult\nand unnatural,\nregardless of whether\nelectrodes\nare",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "Some datasets have\ntarget\nemotions\nand do not need to be"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "placed\nnoninvasively\nalong\nthe\nscalp,\nor\ninvasively\nusing",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "annotated. For example,\nin EMODB, each sentence performed"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "electrocorticography.",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "by actors corresponds to a target emotion. For some datasets,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "the\nemotion annotations\nare obtained automatically. For\nex-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "ample,\nin Multi-ZOL,\nthe\ninteger\nsentiment\nscore\nfor\neach"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "B.\nImplicit Affective Stimuli",
          "3": "review,\nranging from 1 to 10,\nis\nregarded as\nthe\nsentiment"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "label. Several workers are employed to annotate the emotions"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Text is a form to record the natural\nlanguage of human be-",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "in some datasets, such as VideoEmotion-8. The datasets with"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ings, which can implicitly carry informative emotions [16, 17].",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "recorded\ndata\nare\nusually\nannotated\nby\nparticipants\nself-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Text has different\nlevels of\nlinguistic components,\nincluding",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "reporting, such as MAHNOB-HCI. Besides, the emotion labels"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "word,\nsentence, paragraph, and article, which are well\nstud-",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "of most datasets are obtained by major voting. For DES model,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ied; many\noff-the-shelf\nalgorithms\nhave\nbeen\ndeveloped\nto",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "FeelTrace\nand SAM\nare often used for\nannotation. The"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "segment\ntext\ninto small pieces. Then,\nthe\naffective\nattribute",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "former\none\nis\nbased\non\nactivation-evaluation\nspace, which"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of each linguistic piece is recognized with the help of a pub-",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "allows observers\nto track the\nemotional\ncontent of\nstimulus"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "licly available dictionary like SentiWordNet, and the emotion",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "as\nthey perceive\nit over\ntime. The\nlatter one\nis\na\ntool\nthat"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "evoked by the\ntext\ncan be deduced. A digital audio signal",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "accomplishes emotion rating based on different Likert scales."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "is a representation of\nsound,\ntypically stored and transferred",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "Some commonly used datasets are summarized in Table I."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "using a series of binary numbers [12]. Audio signals may be",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "synthesized directly or may originate at a transducer such as",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "V. COMPUTATIONAL TASKS"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "a microphone or a musical\ninstrument. Different\nfrom speech",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "Given multi-modal affective signals, we can conduct differ-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "that mainly focuses on human vocal\ninformation and whose",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "ent MER tasks,\nincluding classication,\nregression, detection,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "content may\nbe\ntranslated\ninto\nnatural\nlanguage,\naudio\nis",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "and retrieval.\nIn this\nsection, we will briey introduce what"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "more\ngeneral\nincluding\nany\nsound\nlike music\nor\nbirdsong.",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "these tasks do."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "An image\nis\na distribution\nof\ncolored dots over\nspace6.\nIt",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "is well known that\na picture\nis worth a\nthousand words.",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "A. Emotion Classication"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "It\nhas\nbeen\ndemonstrated\nin\npsychology\nthat\nemotions\ncan",
          "3": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "be evoked in humans by images\n[18]. The explosive growth",
          "3": "In\nthe\nemotion\nclassication\ntask, we\nassume\nthat\none"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of\nimages\nshared online and the powerful descriptive ability",
          "3": "instance can only belong to one or a xed number of emotion"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of\nscenes enable images\nto become crucial affective stimuli",
          "3": "categories,\nand\nthe\ngoal\nis\nto\ndiscover\nclass\nboundaries\nor"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "with extensive research efforts attracted [10]. Video naturally",
          "3": "class\ndistributions\nin\nthe\ndata\nspace\n[16]. Current works"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "contains multiple modalities at\nthe same time, such as visual,",
          "3": "mainly focus on the manual design of multi-modal\nfeatures"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "audio,\nand\ntextual\ninformation\n[19]. That means\ntemporal,",
          "3": "and classiers or employing deep neural networks in an end-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "spatial, and multi-channel\nrepresentations can be learned and",
          "3": "to-end manner. As dened as\na\nsingle\nlabel\nlearning (SLL)"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "utilized to recognize the emotions in videos.",
          "3": "problem, MER assigns a single dominant emotion label to each"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "3": "sample. However, emotion may be a mixture of all compo-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I": "BRIEF SUMMARY OF RELEASED DATASETS FOR MER."
        },
        {
          "TABLE I": "data sources"
        },
        {
          "TABLE I": "recording"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "YouTube"
        },
        {
          "TABLE I": "YouTube"
        },
        {
          "TABLE I": "Youtube, ExpoTV"
        },
        {
          "TABLE I": "News"
        },
        {
          "TABLE I": "YouTube"
        },
        {
          "TABLE I": "YouTube"
        },
        {
          "TABLE I": "TV series Friends"
        },
        {
          "TABLE I": "movies, TV series"
        },
        {
          "TABLE I": "variety shows"
        },
        {
          "TABLE I": "recording"
        },
        {
          "TABLE I": "recording"
        },
        {
          "TABLE I": "lms"
        },
        {
          "TABLE I": "recording"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "recording"
        },
        {
          "TABLE I": ""
        },
        {
          "TABLE I": "ZOL"
        },
        {
          "TABLE I": "Yelp"
        },
        {
          "TABLE I": "WeiBo"
        },
        {
          "TABLE I": "movies"
        },
        {
          "TABLE I": "YouTube, Flickr"
        },
        {
          "TABLE I": "YouTube, Flickr"
        },
        {
          "TABLE I": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "different emotional\nreactions\nto the same stimulus, which is\ndetection aims to nd out which kind of emotion lies where"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "caused by a variety of elements like personality. Thus, multi-\nin the source data. For example, a restaurant\nreview on Yelp"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "label\nlearning (MLL) has been utilized to study the problem\nmight\nbe\nThis\nlocation\nis\nconveniently\nlocated\nacross\nthe"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "where one instance is associated with multiple emotion labels.\nstreet\nfrom where I work, being walkable is a huge plus\nfor"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "Recently,\nto address the problem that MLL does not t some\nme! Food wise,\nits\nthe\nsame\nas\nalmost\nevery location Ive"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "real applications well where the overall distribution of different\nvisited so theres nothing much to say there.\nI do have\nto"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "labels importance matters, label distribution learning (LDL) is\nsay that\nthe customer service is hit or miss. Meanwhile,\nthe"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "proposed to cover a certain number of labels, representing the\noverall\nrating\nscore\nis\nthree\nstars\nout\nof ve. This\nreview"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "degree to which each emotion label describes the instance [20].\ncontains different emotions and attitudes: positive in the rst"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "sentence,\nneutral\nin\nthe\nsecond\nsentence,\nand\nnegative\nin"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "B. Emotion Regression\nthe\nlast\nsentence. As\nsuch,\nit\nis\ncrucial\nfor\nthe\nsystem to"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "detect which sentence corresponds to which emotion. Another\nEmotion regression aims\nto learn a mapping function that"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "example is affective region detection in images [22].\ncan effectively associate one instance with continuous emotion"
        },
        {
          "for\nliking,\nfamiliarity, and predictability,\nrespectively.": "values\nin\na Cartesian\nspace. The most\ncommon\nregression"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and subjective characteristics make the task challenging and",
          "5": "for\nexplicit\naffective\ncues,\nan EEG headset might\nrecord"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "difcult\nto evaluate.",
          "5": "contaminated\nsignals\nor\neven\nfail\nto\nrecord\nany\nsignal;\nat"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "night,\nthe\ncameras\ncannot\ncapture\nclear\nfacial\nexpressions."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "VI. CHALLENGES",
          "5": "For\nimplicit\naffective\nstimuli,\none\nuser might\npost\na\ntweet"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "only\ncontaining\nan\nimage\n(without\ntext);\nfor\nsome\nvideos,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "As\nstated\nin Section\nI, multi-modal\nemotion\nrecognition",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "the\naudio\nchannel\ndoes\nnot\nchange much.\nIn\nsuch\ncases,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "(MER)\nhas\nseveral\nadvantages\nas\ncompared\nto\nuni-model",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "the\nsimplest\nfeature\nfusion method,\ni.e.,\nearly\nfusion,\ndoes"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "emotion recognition but\nit also faces more challenges.",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "not work, because we\ncannot\nextract\nany features given no"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "captured signal. Designing effective fusion methods\nthat can"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "A. Affective Gap",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "deal with data incompleteness is a widely employed strategy."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "The affective gap is one main challenge for MER, which",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "measures\nthe\ninconsistency\nbetween\nextracted\nfeatures\nand",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "D. Cross-modality Inconsistency"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "perceived high-level emotions. The affective gap is even more",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "Different modalities of\nthe same sample may conict with"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "challenging\nthan\nthe\nsemantic\ngap\nin\nobjective multimedia",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "each other and thus express different emotions. For example,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "analysis. Even\nif\nthe\nsemantic\ngap\nis\nbridged,\nthere might",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "facial\nexpression\nand\nspeech\ncan\nbe\neasily\nsuppressed\nor"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "still\nexist\nan\naffective\ngap. For\nexample,\na\nblooming\nrose",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "masked\nto\navoid\nbeing\ndetected,\nbut EEG signals\nthat\nare"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and a faded rose both contain a rose but can evoke different",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "controlled by the central nervous systems can reect humans"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "emotions. For\nthe same sentence, different voice intonations",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "unconscious body changes. When people post tweets on social"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "may correspond to totally different emotions. Extracting dis-",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "media,\nit\nis very common that\nthe images are not semantically"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "criminative\nhigh-level\nfeatures\nand\nespecially\nthose\nrelated",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "correlated to the text. In such cases, an effective MER method"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "to emotions can help to bridge the affective gap. The main",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "is\nexpected\nto\nautomatically\nevaluate which modalities\nare"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "difculty lies in how to evaluate whether the extracted features",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "5": "more reliable, such as by assigning a weight\nto each modality."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "are related to emotions.",
          "5": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "B. Perception Subjectivity",
          "5": "E. Cross-modality Imbalance"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Due to many personal, contextual, and psychological\nfac-",
          "5": "In some MER applications, different modalities may con-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "tors,\nsuch\nas\nthe\ncultural\nbackground,\npersonality,\nand\nso-",
          "5": "tribute unequally to the evoked emotion. For example, online"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "cial context, different people might have different emotional",
          "5": "news plays an important role in our daily lives, and in addition"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "responses\nto\nthe\nsame\nstimuli\n[10]. Even\nif\nthe\nemotion",
          "5": "to understanding the preferences of\nreaders, predicting their"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "is\nthe\nsame,\ntheir\nphysical\nand\npsychological\nchanges\ncan",
          "5": "emotional\nreactions\nis of great value in various applications,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "also\nbe\nquite\ndifferent. For\nexample,\nall\nthe\n36\nvideos\nin",
          "5": "such as personalized advertising. However, a piece of online"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "the ASCERTAIN dataset\nfor MER are labeled with at\nleast",
          "5": "news usually includes\nimbalanced texts and images,\ni.e.,\nthe"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "four out of\nseven different valence and arousal\nscales by 58",
          "5": "length of\nthe article may be very long with lots of detailed"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "subjects\n[15]. This clearly indicates\nthat\nsome subjects have",
          "5": "information, while only one or\ntwo illustrations are inserted"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "the opposite emotional\nreactions\nto the same stimuli. Take a",
          "5": "into the news. Potentially more problematic,\nthe editor of\nthe"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "short video with storm and thunder for instance, some people",
          "5": "news may select a neutral image for an article with an obvious"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "may feel\nin awe because they have never seen such extreme",
          "5": "sentiment."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "weather,\nsome may\nfeel\nfear\nbecause\nof\nthe\nloud\nthunder",
          "5": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature extractor n": "Tensor-based fusion"
        },
        {
          "Feature extractor n": "Detection\nRetrieval\nModality n"
        },
        {
          "Feature extractor n": "Multi-modal"
        },
        {
          "Feature extractor n": "Representation learning\nFeature fusion\nClassifier optimization"
        },
        {
          "Feature extractor n": "affective signals"
        },
        {
          "Feature extractor n": "Fig. 2.\nIllustration of a widely used MER framework, which consists of\nthree components:\nrepresentation learning to extract\nfeature representations,\nfeature"
        },
        {
          "Feature extractor n": "fusion to combine features from different modalities, and classier optimization to learn specic task models (e.g., classication,\nregression, detection, and"
        },
        {
          "Feature extractor n": "retrieval). n is the number of different modalities."
        },
        {
          "Feature extractor n": "target domain. The presence of domain shift causes signicant\nTo date, plenty of methods have been developed to design"
        },
        {
          "Feature extractor n": "performance decay when a direct\ntransfer is used [23]. Multi-\nrepresentative features for emotion stimuli\nin audios [13, 14]."
        },
        {
          "Feature extractor n": "modal domain adaptation and domain generalization can help\nIt has been found that audio features such as pitch,\nlog energy,"
        },
        {
          "Feature extractor n": "to mitigate such domain gap. Practical settings, such as mul-\nzero-crossing rate,\nspectral\nfeatures, voice quality, and jitter,"
        },
        {
          "Feature extractor n": "tiple source domains, should also be considered.\nare\nuseful\nin\nemotion\nrecognition. The ComParE acoustic"
        },
        {
          "Feature extractor n": "feature\nset\nis\ncommonly\nused\nas\nthe\nbaseline\nset\nfor\nthe"
        },
        {
          "Feature extractor n": "ongoing Computation Paralinguistics Challenge\nseries\nsince"
        },
        {
          "Feature extractor n": "VII. COMPUTATIONAL METHODOLOGIES"
        },
        {
          "Feature extractor n": "2013. However, because of possible high similarities in certain"
        },
        {
          "Feature extractor n": "Generally,\nthere are three components\nin an MER frame-"
        },
        {
          "Feature extractor n": "emotions,\nsingle\ntype of\naudio feature\nis not discriminative"
        },
        {
          "Feature extractor n": "work with sufcient labeled training data in the target domain:"
        },
        {
          "Feature extractor n": "enough\nto\nclassify\nemotions. To\nsolve\nthis\nproblem,\nsome"
        },
        {
          "Feature extractor n": "representation learning, feature fusion, and classier optimiza-"
        },
        {
          "Feature extractor n": "approaches\npropose\nto\ncombine\ndifferent\ntypes\nof\nfeatures."
        },
        {
          "Feature extractor n": "tion,\nas\nshown in Fig. 2.\nIn this\nsection, we will\nintroduce"
        },
        {
          "Feature extractor n": "Recently, with\nthe\ndevelopment\nof\ndeep\nlearning, CNN is"
        },
        {
          "Feature extractor n": "these components. Further, we will describe domain adaptation"
        },
        {
          "Feature extractor n": "shown to achieve state-of-the-art performance on large-scale"
        },
        {
          "Feature extractor n": "when there\nis no labeled training data\nin the\ntarget domain"
        },
        {
          "Feature extractor n": "tasks\nin many domains dealing with natural data, and audio"
        },
        {
          "Feature extractor n": "and when sufcient\nlabeled data is available in another related"
        },
        {
          "Feature extractor n": "emotion\nrecognition\nis\nof\ncourse\nalso\nincluded. Audio\nis"
        },
        {
          "Feature extractor n": "source domain."
        },
        {
          "Feature extractor n": "typically transferred into a graphical\nrepresentation,\nsuch as"
        },
        {
          "Feature extractor n": "a spectrogram,\nto be fed into a CNN. Since CNN uses shared"
        },
        {
          "Feature extractor n": "weight lters\nand pooling to give\nthe model better\nspectral"
        },
        {
          "Feature extractor n": "A. Representation Learning of Each Affective Modality"
        },
        {
          "Feature extractor n": "and\ntemporal\ninvariant\nproperties,\nit\ntypically\nyields\nbetter"
        },
        {
          "Feature extractor n": "To\nrepresent\nthe\ntext\nas\na\nform that\ncan\nbe\nunderstood"
        },
        {
          "Feature extractor n": "generalized and more robust models for emotion recognition."
        },
        {
          "Feature extractor n": "by computers,\nthe\nfollowing aspects\nare\nrequired: rst,\nrep-"
        },
        {
          "Feature extractor n": "Researchers have designed informative representations\nfor\nresenting the\nsymbolic words\nas\nreal numbers\nfor\nthe next"
        },
        {
          "Feature extractor n": "emotional stimuli in images. In general, images can be divided\ncomputation;\nsecond, modeling\nthe\nsemantic\nrelationships;"
        },
        {
          "Feature extractor n": "images\nexpression\nand nally, obtaining a unied representation for\nthe whole\ninto two types, non-restrictive\nand facial"
        },
        {
          "Feature extractor n": "images. For\nthe\nformer,\ne.g., natural\nimages, various hand-\ntext\n[16].\nIn the beginning, words are represented by one-hot"
        },
        {
          "Feature extractor n": "crafted features\nincluding color,\ntexture,\nshape, composition,\nvectors with the length of vocabulary size, where for\nthe t-"
        },
        {
          "Feature extractor n": "etc.,\nare developed to represent\nimage\nemotion in the\nearly\nis 1 and\nth word in the vocabulary, wt, only the position t"
        },
        {
          "Feature extractor n": "years [10]. These low-level features are developed with inspi-\nthe other positions are 0. As\nthe scale of\nthe data increases,"
        },
        {
          "Feature extractor n": "ration from psychology and art\ntheory. Later, mid-level\nfea-\nthe dimension of\nthis one-hot vector\nincreases dramatically."
        },
        {
          "Feature extractor n": "Later,\nresearchers use language models to train word vectors\ntures based on the visual concepts are presented to bridge the"
        },
        {
          "Feature extractor n": "gap between the pixels in images and the emotion labels. The\nby predicting context, obtaining word vectors with vectors of"
        },
        {
          "Feature extractor n": "most\nrepresentative engine is SentiBank, which is composed\nxed dimension. Popular word vector\nrepresentation models"
        },
        {
          "Feature extractor n": "of 1, 200 adjective-noun pairs and shows remarkable and ro-\ninclude word2vec, GLOVE, BERT, XLNet,\nand so on. The"
        },
        {
          "Feature extractor n": "bust\nrecognition performance among all\nthe hand-engineering\ntext\nfeature extraction methods have developed from simple"
        },
        {
          "Feature extractor n": "features.\nIn the era of deep learning, CNN is\nregarded as a\nones to complex ones as well. Text\nfeatures can be obtained"
        },
        {
          "Feature extractor n": "by simply averaging word vectors. A recurrent neural network\nstrong feature extractor in an end-to-end manner. Specically,"
        },
        {
          "Feature extractor n": "to integrate various representations of different\nlevels, features\n(RNN)\nis used to model\nthe sequential\nrelations of words in"
        },
        {
          "Feature extractor n": "are\nextracted from multiple\nlayers of CNN. Meanwhile,\nan\nthe\ntext. A convolutional neural network (CNN) which has"
        },
        {
          "Feature extractor n": "attention mechanism is\nemployed to learn better\nemotional\nbeen widely used in the computer vision community,\nis also"
        },
        {
          "Feature extractor n": "representations of specic local affective regions [22]. For the\nused to extract contextual\nrelations between words."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "of\nfeatures could obtain more promising results and improve": ""
        },
        {
          "of\nfeatures could obtain more promising results and improve": ""
        },
        {
          "of\nfeatures could obtain more promising results and improve": "the performance."
        },
        {
          "of\nfeatures could obtain more promising results and improve": "To perceive emotions,\nthere are mainly two aspects of ways"
        },
        {
          "of\nfeatures could obtain more promising results and improve": ""
        },
        {
          "of\nfeatures could obtain more promising results and improve": "to learn the\nrepresentations of gait\n[24]. For one\nthing, we"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "can explicitly model\nthe posture and movement\ninformation"
        },
        {
          "of\nfeatures could obtain more promising results and improve": ""
        },
        {
          "of\nfeatures could obtain more promising results and improve": "that\nis related to the emotions. To model\nthis information, we"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "rst extract\nthe skeletal structure of a person and then repre-"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "sent each joint of\nthe human body using the 3D coordinate"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "system. After getting these coordinates,\nthe angles, distance,"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "or\narea\namong different\njoints\n(posture\ninformation), veloc-"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "ity/acceleration (movement\ninformation),\ntheir co-variance de-"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "scriptors, etc. can be easily extracted. For another thing, high-"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "level emotional\nrepresentations can be modeled from gait by"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "long short-term memory (LSTM), deep convolutional neural"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "networks,\nor\ngraph\nconvolutional\nnetworks. Some methods"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "extract optical ow from gait videos and then extract sequence"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "representations\nusing\nthese\nnetworks. Other methods\nlearn"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "skeletal structures of the gait and then feed them into multiple"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "networks to extract discriminate representations."
        },
        {
          "of\nfeatures could obtain more promising results and improve": "Since\nvarious\ninformation\nabout\nemotions,\nsuch\nas\nfre-"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "quency band, electrodeposition, and temporal information, can"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "be\nexplored from the brains\nresponse\nto emotional\nstimuli,"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "EEG signals\nare widely used in emotion analysis\n[15]. To"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "extract discriminative features\nfor EEG emotion recognition,"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "differential\nentropy\nfeatures\nfrom frequency\nband\nor\nelec-"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "trodeposition relationship are very popular\nin previous work."
        },
        {
          "of\nfeatures could obtain more promising results and improve": "Besides hand-crafted features, we can also directly apply end-"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "to-end deep learning neural networks such as CNN and RNN"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "on the raw EEG signals to obtain powerful deep features [25]."
        },
        {
          "of\nfeatures could obtain more promising results and improve": "Inspired by the\nlearning pattern of humans,\nspatial-wise\nat-"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "tention mechanisms are successfully applied to extract more"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "discriminative\nspatial\ninformation. Furthermore,\nconsidering"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "that EEG signals\ncontain multiple\nchannels,\na\nchannel-wise"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "attention mechanism can\nalso\nbe\nintegrated\ninto CNN to"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "exploit\nthe inter-channel\nrelationship among feature maps."
        },
        {
          "of\nfeatures could obtain more promising results and improve": ""
        },
        {
          "of\nfeatures could obtain more promising results and improve": ""
        },
        {
          "of\nfeatures could obtain more promising results and improve": "B. Feature Fusion of Different Affective Modalities"
        },
        {
          "of\nfeatures could obtain more promising results and improve": ""
        },
        {
          "of\nfeatures could obtain more promising results and improve": "Feature\nfusion,\nas one key research topic\nin MER,\naims"
        },
        {
          "of\nfeatures could obtain more promising results and improve": "to integrate\nthe\nrepresentations\nfrom multiple modalities\nto"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "facial\nexpression images, rstly the human face\nis detected",
          "7": "Feature"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Modality 1\nKernel 1"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and aligned, and then the face landmarks are encoded for\nthe",
          "7": "extractor 1"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Kernelized"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "recognition task. Note\nthat\nfor\nthose non-restrictive\nimages",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "classifier"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "that contain human faces by chance,\nfacial expression can be",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Feature"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "treated as an important mid-level cue.",
          "7": "Modality n\nKernel n"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "extractor n"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "In the above, we have mentioned how to identify emotions",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Kernel-based fusion"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "in the isolated modalities. Here, we rst\nfocus on perceiving",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "emotions from successive frames. Then, we introduce how to",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Feature"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Modality 1\nGraph 1"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "build joint\nrepresentation for videos. Compared to a\nsingle",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "extractor 1"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "image,\na\nvideo\ncontains\na\nseries\nof\nimages with\ntemporal",
          "7": "Graph"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "information [19]. To build representations of videos, a wide",
          "7": "learning"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "range of methods has been proposed. Early methods mainly",
          "7": "Feature"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Modality n\nGraph n"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "extractor n"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "utilize hand-crafted local\nrepresentations\nin this eld, which",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "include color, motion, and shot cut\nrate. With the advent of",
          "7": "Graph-based fusion"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "deep learning, recent methods extract discriminative represen-",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Feature"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "tations by adopting a 3D CNN that captures the temporal infor-",
          "7": "Modality 1"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "extractor 1"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "mation encoded in multiple adjacent\nframes. After extracting",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Fully connected"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "modality-specic features in videos, integrating different types",
          "7": "Neural network"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "layers"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of\nfeatures could obtain more promising results and improve",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Feature"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Modality n"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "the performance.",
          "7": "extractor n"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "To perceive emotions,\nthere are mainly two aspects of ways",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Neural network-based fusion"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "to learn the\nrepresentations of gait\n[24]. For one\nthing, we",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "can explicitly model\nthe posture and movement\ninformation",
          "7": "Fig. 3.\nIllustration of different model-based fusion strategies, where n is the"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "number of different modalities."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "that\nis related to the emotions. To model\nthis information, we",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "rst extract\nthe skeletal structure of a person and then repre-",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "sent each joint of\nthe human body using the 3D coordinate",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "system. After getting these coordinates,\nthe angles, distance,",
          "7": "predict either a specic category or a continuous value of emo-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "or\narea\namong different\njoints\n(posture\ninformation), veloc-",
          "7": "tions. Generally,\nthere\nare\ntwo strategies: model-free\nfusion"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ity/acceleration (movement\ninformation),\ntheir co-variance de-",
          "7": "and model-based fusion [7, 9]."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "scriptors, etc. can be easily extracted. For another thing, high-",
          "7": "Model-free fusion that is not directly dependent on specic"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "level emotional\nrepresentations can be modeled from gait by",
          "7": "learning algorithms has been widely used for decades. We can"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "long short-term memory (LSTM), deep convolutional neural",
          "7": "divide it\ninto early fusion,\nlate fusion, and hybrid fusion [5]."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "networks,\nor\ngraph\nconvolutional\nnetworks. Some methods",
          "7": "All\nthese\nfusion methods\ncan\nbe\nextended\nfrom existing"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "extract optical ow from gait videos and then extract sequence",
          "7": "uni-modal emotion recognition classiers. Early fusion, also"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "representations\nusing\nthese\nnetworks. Other methods\nlearn",
          "7": "named feature-level\nfusion, directly concatenates\nthe\nfeature"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "skeletal structures of the gait and then feed them into multiple",
          "7": "representations\nfrom different modalities\nas\na\nsingle\nrepre-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "networks to extract discriminate representations.",
          "7": "sentation.\nIt\nis\nthe most\nintuitive method\nto\nfuse\ndifferent"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Since\nvarious\ninformation\nabout\nemotions,\nsuch\nas\nfre-",
          "7": "representations by exploiting the interactions between different"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "quency band, electrodeposition, and temporal information, can",
          "7": "modalities at an early stage and only requires training a single"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "be\nexplored from the brains\nresponse\nto emotional\nstimuli,",
          "7": "model. But since the representations from different modalities"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "EEG signals\nare widely used in emotion analysis\n[15]. To",
          "7": "might\nsignicantly differ, we have to consider\nthe time syn-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "extract discriminative features\nfor EEG emotion recognition,",
          "7": "chronization problem to transform these representations\ninto"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "differential\nentropy\nfeatures\nfrom frequency\nband\nor\nelec-",
          "7": "the same format before fusion. When one or more modalities"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "trodeposition relationship are very popular\nin previous work.",
          "7": "are missing,\nsuch early fusion would fail. Late\nfusion,\nalso"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Besides hand-crafted features, we can also directly apply end-",
          "7": "named decision-level\nfusion,\ninstead integrates the prediction"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "to-end deep learning neural networks such as CNN and RNN",
          "7": "results from each single modality. Some popular mechanisms"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "on the raw EEG signals to obtain powerful deep features [25].",
          "7": "include averaging, voting, and signal variance. The advantages"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Inspired by the\nlearning pattern of humans,\nspatial-wise\nat-",
          "7": "of\nlate\nfusion\ninclude\n(1) exibility\nand\nsuperiority\n\nthe"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "tention mechanisms are successfully applied to extract more",
          "7": "optimal classiers can be selected for different modalities; and"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "discriminative\nspatial\ninformation. Furthermore,\nconsidering",
          "7": "(2) robustness  when some modalities are missing, late fusion"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "that EEG signals\ncontain multiple\nchannels,\na\nchannel-wise",
          "7": "can\nstill work. However,\nthe\ncorrelations\nbetween\ndifferent"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "attention mechanism can\nalso\nbe\nintegrated\ninto CNN to",
          "7": "modalities before decision are\nignored. Hybrid fusion com-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "exploit\nthe inter-channel\nrelationship among feature maps.",
          "7": "bines early fusion and late fusion to exploit\ntheir advantages"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "in a unied framework but with higher computational cost."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "Model-based fusion that explicitly performs fusion during"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "B. Feature Fusion of Different Affective Modalities",
          "7": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "7": "the construction of\nthe learning models has been paid more"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Feature\nfusion,\nas one key research topic\nin MER,\naims",
          "7": "attention [7, 9], as\nshown in Fig. 3,\nsince model-free fusion"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "to integrate\nthe\nrepresentations\nfrom multiple modalities\nto",
          "7": "is based on some simple techniques\nthat are not\nspecically"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "designed\nfor multimodal\ndata. For\nshallow models,\nkernel-",
          "8": "C. Classier Optimization for Multi-modal Emotion Recogni-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "based fusion and graph-based fusion are\ntwo representative",
          "8": "tion"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "methods;\nfor\nrecent\npopular\ndeep models,\nneural\nnetwork-",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "For the text represented as a sequence of word embeddings,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "based fusion, attention-based fusion, and tensor-based fusion",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "the most popular approaches to leverage the semantics among"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "are often used.",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "words are RNN and CNN. LSTM, as a typical RNN, contains"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Kernel-based fusion is\nextended based on classiers\nthat",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "a series of cells with the same structure. Every cell\ntakes a"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "contain kernels,\nsuch as SVM. For different modalities, dif-",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "word embedding and the hidden state\nfrom the\nlast\ncell\nas"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ferent\nkernels\nare\nused. The exibility\nin\nkernel\nselection",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "input, computes\nthe output, and updates\nthe hidden state for"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and\nconvexity\nof\nthe\nloss\nfunctions make multiple\nkernel",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "the\nfollowing\ncell. The\nhidden\nstate\nrecords\nthe\nsemantics"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "learning fusion popular in many applications,\nincluding MER.",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "of previous words. CNN computes\nlocal contextual\nfeatures"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "However,\nduring\ntesting,\nthese\nfusion methods\nrely\non\nthe",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "among consecutive words through convolution operations. And"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "support vectors\nin the\ntraining data, which results\nin large",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "average\npooling\nor max-pooling\nlayers\nare\nused\nto\nfurther"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "fusion\nmemory\ncost\nand\ninefcient\nreference. Graph-based",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "integrate\nthe\nobtained\nfeatures\nfor\nthe\nfollowing\nsentiment"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "constructs separate graphs or hypergraphs for each modality,",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "classication. Recently, researchers begin to use Transformer-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "combines these graphs into a fused one, and learns the weights",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "based methods,\ne.g., BERT and GPT-3. Transformer\nis\nim-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of different edges and modalities by graph-based learning.\nIt",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "plemented\nas\na\nseries\nof modules\ncontaining\na multi-head"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "can well deal with the data incompleteness problem simply",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "self-attention layer\nfollowed by a normalization layer, a feed-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "by constructing graphs based on available data. Besides\nthe",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "forward network, and another normalization layer. The order"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "extracted feature representations, we can also incorporate prior",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "of words\nin the text\nis also represented by another position"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "human knowledge\ninto the models by corresponding edges.",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "embedding layer. Compared with RNN,\ntransformer does not"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "However, the computational cost would increase exponentially",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "require\nsequential processing of words, which improves\nthe"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "when more training samples are available.",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "parallelizability. And compared with CNN,\ntransformer\ncan"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Neural network-based fusion employs a direct and intuitive",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "8": "model\nrelationships between more distant words."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "strategy to fuse\nthe\nfeature\nrepresentations or predicted re-",
          "8": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "sults of different modalities by a neural network. Attention-",
          "8": "The classication approaches used in audio emotion recog-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "based fusion uses\nsome\nattention mechanisms\nto obtain the",
          "8": "nition generally include the following two options:\ntraditional"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "weighted sum of a set of\nfeature representations with scalar",
          "8": "methods\nand\ndeep\nlearning-based methods.\nFor\ntraditional"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "weights that are dynamically learned by an attention module.",
          "8": "methods, HMM is\na\nrepresentative method\nbecause\nof\nits"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Different attention mechanisms correspond to fusing different",
          "8": "capability of capturing dynamic characteristics of\nsequential"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "components. For\nexample,\nspatial\nimage\nattention measures",
          "8": "data. SVM is\nalso widely utilized in audio emotion recog-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "the importance of different\nimage regions. Image and text co-",
          "8": "nition. Deep learning-based methods have become more and"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "attention employs\nsymmetric attention mechanisms\nto gener-",
          "8": "more\npopular\nsince\nthey\nare\nnot\nrestricted\nby the\nclassical"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ated both attended visual and attended textual representations.",
          "8": "independence\nassumptions\nof HMM models. Among\nthese"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Parallel co-attention and alternating co-attention methods can",
          "8": "methods,\nsequence-to-sequence models with\nattention\nhave"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "be used to respectively generate attention for different modal-",
          "8": "shown\nsuccess\nin\nan\nend-to-end manner. Recently,\nsome"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ities simultaneously and one by one. Recently, a Multimodal",
          "8": "approaches\nsignicantly\nextend\nthe\nstate\nof\nthe\nart\nin\nthis"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Adaptation Gate (MAG)\nis designed to enable Transformer-",
          "8": "area by developing deep hybrid convolutional\nand recurrent"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "based\ncontextual word\nrepresentations,\nsuch\nas BERT and",
          "8": "models [14]."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "XLNet,\nto accept multi-modal nonverbal data [17]. Based on",
          "8": "In the early years,\nsimilar\nto this\ntask in other modalities,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "the\nattention conditioned on the nonverbal behaviors, MAG",
          "8": "multiple hand-crafted image features are integrated and input"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "can essentially map the informative multiple modalities\nto a",
          "8": "into SVM to train classiers. Then, based on deep learning,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "vector with a trajectory and magnitude. Tensor-based fusion",
          "8": "the classier and feature extractor are connected and optimized"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "tries to exploit\nthe correlations of different\nrepresentations by",
          "8": "in\nan\nend-to-end manner\nby\ncorresponding\nloss\nfunctions"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "some\nspecic\ntensor operations,\nsuch as outer product\nand",
          "8": "like\ncross-entropy loss\n[26]. Besides, popular metric\nlosses"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "polynomial\ntensor\npooling. These\nfusion methods\nfor\ndeep",
          "8": "such\nas\ntriplet\nloss\nand N-pair\nloss\nalso\ntake\npart\nin\nthe"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "models are capable of learning from a large amount of data in",
          "8": "network optimization to obtain more discriminative features."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "an end-to-end manner with good performance but suffer from",
          "8": "With the\nabove\nlearning paradigm,\neach image\nis predicted"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "low interpretability.",
          "8": "as\na\nsingle dominant\nemotion category. However, based on"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "One\nimportant\nproperty\nof\nthe\nabove-mentioned\nfeature",
          "8": "the\ntheories\nof\npsychology,\nan\nimage may\nevoke multiple"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "fusion methods\nis whether\nthey support\ntemporal modeling",
          "8": "emotions in viewers, which leads to an ambiguous problem. To"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "for MER in videos.\nIt\nis obvious that early fusion can while",
          "8": "address the problem, label distribution learning is employed to"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "late\nfusion\nand\nhybrid\nfusion\ncannot,\nsince\nthe\npredicted",
          "8": "predict a concrete relative degree for each emotion category,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "results\nbased\non\neach modality\nare\nalready\nknown\nbefore",
          "8": "where KullbackLeibler divergence is\nthe most popular\nloss"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "late\nfusion. For model-based fusion,\nexcluding kernel-based",
          "8": "function. Some informative and attractive regions of an image"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "fusion, all others can be used for\ntemporal modeling, such as",
          "8": "always determine the emotion of the image. Therefore, a series"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "hidden Markov models (HMM) and conditional random elds",
          "8": "of\narchitecture with\nextra\nattention\nor\ndetection\nbranch\nis"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "(CRF)\nfor graph-based fusion methods, and RNN and LSTM",
          "8": "constructed. With the optimization for multiple tasks including"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "networks for neural network-based fusion.",
          "8": "attention and original\ntask, a more robust and discriminative"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dataset": "train : val\n:\ntest",
          "CMU-MOSI": "1284 : 229 : 686",
          "YouTube": "30 : 5 : 11",
          "ICT-MMMO": "11 : 2 : 4",
          "MOUD": "49 : 10 : 20",
          "IEMOCAP": "3 : 1 : 1"
        },
        {
          "dataset": "metric",
          "CMU-MOSI": "",
          "YouTube": "",
          "ICT-MMMO": "",
          "MOUD": "",
          "IEMOCAP": ""
        },
        {
          "dataset": "",
          "CMU-MOSI": "A7",
          "YouTube": "F1",
          "ICT-MMMO": "F1",
          "MOUD": "F1",
          "IEMOCAP": "CV"
        },
        {
          "dataset": "method",
          "CMU-MOSI": "",
          "YouTube": "",
          "ICT-MMMO": "",
          "MOUD": "",
          "IEMOCAP": ""
        },
        {
          "dataset": "SVM",
          "CMU-MOSI": "26.5",
          "YouTube": "37.9",
          "ICT-MMMO": "68.7",
          "MOUD": "45.5",
          "IEMOCAP": "0.060"
        },
        {
          "dataset": "RF",
          "CMU-MOSI": "21.3",
          "YouTube": "49.2",
          "ICT-MMMO": "69.8",
          "MOUD": "63.3",
          "IEMOCAP": "-"
        },
        {
          "dataset": "THMM",
          "CMU-MOSI": "17.8",
          "YouTube": "27.9",
          "ICT-MMMO": "53.0",
          "MOUD": "52.7",
          "IEMOCAP": "-"
        },
        {
          "dataset": "MV-LSTM",
          "CMU-MOSI": "33.2",
          "YouTube": "43.3",
          "ICT-MMMO": "72.3",
          "MOUD": "48.2",
          "IEMOCAP": "0.020"
        },
        {
          "dataset": "BC-LSTM",
          "CMU-MOSI": "28.7",
          "YouTube": "47.3",
          "ICT-MMMO": "71.1",
          "MOUD": "72.9",
          "IEMOCAP": "0.070"
        },
        {
          "dataset": "TFN",
          "CMU-MOSI": "28.7",
          "YouTube": "41.0",
          "ICT-MMMO": "72.6",
          "MOUD": "61.7",
          "IEMOCAP": "0.040"
        },
        {
          "dataset": "MARN",
          "CMU-MOSI": "34.7",
          "YouTube": "52.9",
          "ICT-MMMO": "85.9",
          "MOUD": "81.2",
          "IEMOCAP": "0.100"
        },
        {
          "dataset": "MFN",
          "CMU-MOSI": "34.1",
          "YouTube": "60.7",
          "ICT-MMMO": "87.1",
          "MOUD": "80.4",
          "IEMOCAP": "0.111"
        },
        {
          "dataset": "Evaluation metrics: AN means emotion classication accuracy where N denotes the number of emotion classes, AN and F1",
          "CMU-MOSI": "",
          "YouTube": "",
          "ICT-MMMO": "",
          "MOUD": "",
          "IEMOCAP": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "77.1\n77.0\n34.7\n0.968\n0.625\n54.2\n52.9\n86.3\n85.9\n81.1\n81.2\n37.0\n35.9\n0.242\n0.100\n0.497\n0.650\nMARN"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "MFN\n77.4\n77.3\n34.1\n0.965\n0.632\n61.0\n60.7\n87.5\n87.1\n81.1\n80.4\n36.5\n34.9\n0.236\n0.111\n0.482\n0.645"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "Evaluation metrics: AN means emotion classication accuracy where N denotes the number of emotion classes, AN and F1"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "are in percentage, M is short\nfor mean absolute error, C indicates the Pearson correlation, and V, A correspond to the results"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "of valence and arousal\n(the same for Table III)."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "TABLE III\nor 3D joint coordinates for each frame in the walking videos."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "QUANTITATIVE COMPARISON OF SOME REPRESENTATIVE METHODS FOR"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "To leverage the inherent affective cues\nin the coordinates of"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "MULTI-MODAL EMOTION RECOGNITION ON THE CMU-MOSI DATASET"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "joints, many\nclassiers\nor\narchitectures\nhave\nbeen\nused\nto"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "USING BERT OR XLNET AS WORD EMBEDDINGS."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "extract affective features in the gait. LSTM networks contain"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "metric\nmany special units,\ni.e., memory cells, and can store the joint"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "F1\nM\nC\nA2"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "method"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "coordinate information from particular\ntime steps\nin a long-"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "TFN\n74.8/78.2\n74.1/78.2\n0.955/0.914\n0.649/0.713"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "time data sequence. Thus,\nit\nis used in some early work of"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "MARN\n77.7/78.3\n77.9/78.8\n0.938/0.921\n0.691/0.707"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "gait emotion recognition. The hidden features of\nthe LSTM"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "78.2/78.3\n78.1/78.4\n0.911/0.898\n0.699/0.713\nMFN"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "FT\n83.5/84.7\n83.4/84.6\n0.739/0.676\n0.782/0.812\ncan be\nfurther\nconcatenated with the hand-crafted affective"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "MAG\n84.2/85.7\n84.1/85.6\n0.712/0.675\n0.796/0.821"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "features and are then fed into a classier (e.g., SVM or random"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "Human\n85.7\n87.5\n0.710\n0.820\nforest\n(RF))\nto predict\nemotions. Recently,\nanother popular"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "The numbers on the left side and the right side of / are the MER"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "network used in gait emotion prediction is the spatial-temporal"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "results based on BERT and XLNet,\nrespectively."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "graph convolutional network (ST-GCN). ST-GCN is\ninitially"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "proposed for action recognition from human skeletal graphs."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "Spatial\nrepresents the spatial edges in the skeletal structure,"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "model\nis obtained."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "which are the limbs that connect\nthe body joints.\nTemporal"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "Most\nexisting methods\nemploy\na\ntwo-stage\npipeline\nto"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "refers\nto temporal\nedges,\nand they connect\nthe positions of"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "recognize video emotion,\ni.e., extracting visual and/or audio"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "each\njoint\nacross\ndifferent\nframes.\nST-GCN can\nbe\neasily"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "features and training classiers. For\ntraining classiers, many"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "implemented as\na\nspatial\nconvolution followed by a\ntempo-"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "machine\nlearning methods have been investigated to model"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "ral\nconvolution, which is\nsimilar\nto the deep convolutional"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "the mapping\nbetween\nvideo\nfeatures\nand\ndiscrete\nemotion"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "networks."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "categories,\nincluding SVM, GMM, HMM, dynamic Bayesian"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "EEG-based\nemotion\nrecognition\nusually\nemploys\nvarious"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "networks (DBNs), and conditional\nrandom elds (CRF). De-"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "classiers such as SVM, decision trees, and k-nearest neighbor"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "spite the above methods have contributed to the development"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "to classify hand-crafted features in the early stage. Later, since"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "of emotion recognition in videos, recent methods are proposed"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "CNN and RNN are good at extracting spatial\ninformation and"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "to recognize video emotions\nin an end-to-end manner based"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "temporal\ninformation of EEG signals, respectively, end-to-end"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "on deep neural networks due to their superior capability [27]."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "structures\nsuch\nas\ncascade\nconvolutional\nrecurrent\nnetwork"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "CNN-based methods\nrst\nemploy\n3D convolutional\nneural"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "(which combines CNN and RNN), LSTM-RNN,\nand paral-"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "networks to extract high-level spatio-temporal\nfeatures which"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "lel\nconvolutional\nrecurrent neural networks\nare\nsuccessfully"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "contain affective\ninformation,\nand then use\nfully connected"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "designed and applied to emotion recognition tasks."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "layers\nto classify emotions. Finally,\nthe models are followed"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "1) Quantitative Comparison of Representative MER Meth-"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "by the loss function to optimize the whole network. Inspired by"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "ods: To give readers an impression on the performances of"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "the human process of perceiving emotions, CNN-based meth-"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "state-of-the-art MER methods, we\nconduct\nexperiments\nto"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "ods employ the attention mechanism to emphasize emotionally"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "fairly\ncompare\nsome\nrepresentative methods\nbased\non\nthe"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "relevant regions of frames or segments in each video. Further-"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "released codes of CMU-Multimodal SDK7\nand Multimodal"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "more,\nconsidering the polarity-emotion hierarchy constraint,"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "Adaptation Gate8. Specically,\nthe compared non-deep meth-"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "recent methods propose polarity-consistent cross-entropy loss,"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "to guide the attention generation."
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "7https://github.com/A2Zadeh/CMU-MultimodalSDK"
        },
        {
          "TFN\n74.6\n74.5\n28.7\n1.040\n0.587\n47.5\n41.0\n72.5\n72.6\n63.2\n61.7\n36.0\n34.5\n0.251\n0.040\n0.521\n0.550": "8https://github.com/WasifurRahman/MAG\nThe gait of a person can be represented as a sequence of 2D"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "rectangles with text\nin bold represent different alignment": "different component details, enforcing some constraints, or slightly changing the architecture.",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "ods\ninclude SVM, RF,\nand Tri-modal HMM (THMM);\nthe",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "word embeddings than GLOVE and XLNet\nis generally better"
        },
        {
          "rectangles with text\nin bold represent different alignment": "compared\ndeep methods\ninclude Multi-View LSTM (MT-",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "than BERT. Finally,\nalthough XLNet-based MAG achieves"
        },
        {
          "rectangles with text\nin bold represent different alignment": "LSTM), Bi-Directional Contextual LSTM (BC-LSTM), Ten-",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "near-human level performance on CMU-MOSI,\nthere is\nstill"
        },
        {
          "rectangles with text\nin bold represent different alignment": "sor Fusion Network\n(TFN), Multi-attention Recurrent Net-",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "some gap and more efforts are expected to achieve even better"
        },
        {
          "rectangles with text\nin bold represent different alignment": "work (MARN), Memory Fusion Network (MFN), ne-tuning",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "performance than humans."
        },
        {
          "rectangles with text\nin bold represent different alignment": "(FT), and Multi-modal Adaptation Gate (MAG). We conduct",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "experiments\non\nve\ndatasets: CMU-MOSI, YouTube,\nICT-",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "D. Domain Adaptation for Multi-modal Emotion Recognition"
        },
        {
          "rectangles with text\nin bold represent different alignment": "MMMO, MOUD,\nand\nIEMOCAP. All\nthe\ndatasets\ncontain",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "Domain adaptation aims to learn a transferable MER model"
        },
        {
          "rectangles with text\nin bold represent different alignment": "three modalities:\nface,\nspeech, and transcript\ntext. For visual",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "from labeled source domains that can perform well on unla-"
        },
        {
          "rectangles with text\nin bold represent different alignment": "features, Facet is used to extract per-frame basic and advanced",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "beled target domains [23]. Recent efforts have been dedicated"
        },
        {
          "rectangles with text\nin bold represent different alignment": "emotions and facial action units as indicators of facial muscle",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "to deep unsupervised domain adaptation [23], which employs a"
        },
        {
          "rectangles with text\nin bold represent different alignment": "movement.\nFor\nacoustic\nfeatures, COVAREP\nis\nemployed",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "two-streams architecture. One stream is used to train an MER"
        },
        {
          "rectangles with text\nin bold represent different alignment": "to\nextract\n12 Mel-frequency\ncepstral\ncoefcients\n(MFCCs),",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "model on the labeled source domains, while the other\nis used"
        },
        {
          "rectangles with text\nin bold represent different alignment": "pitch tracking and voiced/unvoiced segmenting features, glottal",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "to align the source and target domains. Based on the alignment"
        },
        {
          "rectangles with text\nin bold represent different alignment": "source parameters, peak slope parameters,\nand maxima dis-",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "strategy, existing uni-modal domain adaptation methods can be"
        },
        {
          "rectangles with text\nin bold represent different alignment": "persion quotients. For\nlinguistic features,\nthree different pre-",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "classied into different categories\n[23],\nsuch as discrepancy-"
        },
        {
          "rectangles with text\nin bold represent different alignment": "trained word embeddings,\ni.e., GLOVE, BERT, and XLNet,",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "based, adversarial discriminative, adversarial generative, and"
        },
        {
          "rectangles with text\nin bold represent different alignment": "are employed to obtain the word vector. The input\nto the non-",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "self-supervision-based methods."
        },
        {
          "rectangles with text\nin bold represent different alignment": "deep methods is the early fusion of multi-modal features. For",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "Discrepancy-based methods employ some distance metrics"
        },
        {
          "rectangles with text\nin bold represent different alignment": "emotion classication, we use accuracy (A) and F1 as metrics;",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "to\nexplicitly measure\nthe\ndiscrepancy\nbetween\nthe\nsource"
        },
        {
          "rectangles with text\nin bold represent different alignment": "for emotion regression, we use mean absolute error\n(M) and",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "and\ntarget\ndomains\non\ncorresponding\nactivation\nlayers\nof"
        },
        {
          "rectangles with text\nin bold represent different alignment": "the Pearson correlation (C) as metrics. Higher values indicate",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "the\ntwo network streams. Commonly used discrepancy loss"
        },
        {
          "rectangles with text\nin bold represent different alignment": "better performance for all\nthe metrics, except M where lower",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "include maximum mean\ndiscrepancy,\ncorrelation\nalignment,"
        },
        {
          "rectangles with text\nin bold represent different alignment": "values denote better performance.",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": ""
        },
        {
          "rectangles with text\nin bold represent different alignment": "",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "geodesic distance,\ncentral moment discrepancy, Wasserstein"
        },
        {
          "rectangles with text\nin bold represent different alignment": "From the\nresults\nin Table\nII\nand Table\nIII, we have\nthe",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "discrepancy, contrastive Domain discrepancy, and higher-order"
        },
        {
          "rectangles with text\nin bold represent different alignment": "following observations: First, the performances of deep models",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "moment matching. Besides\nthe used discrepancy loss,\nthere"
        },
        {
          "rectangles with text\nin bold represent different alignment": "are generally better\nthan non-deep ones. Second,\nfor different",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "are some other differences between existing methods, such as"
        },
        {
          "rectangles with text\nin bold represent different alignment": "datasets,\nthe methods with the best performances are different.",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "whether\nthe\nloss\nis domain-level or\nclass-level, which layer"
        },
        {
          "rectangles with text\nin bold represent different alignment": "For example, RF achieves\nthe best performance among non-",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "the loss is operated on, whether\nthe backbone networks share"
        },
        {
          "rectangles with text\nin bold represent different alignment": "deep models except CMU-MOSI, which demonstrates its good",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "weights or not, and whether the aligned distribution is marginal"
        },
        {
          "rectangles with text\nin bold represent different alignment": "generalization ability, while the performance of SVM is much",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "or\njoint. Adversarial discriminative models usually align the"
        },
        {
          "rectangles with text\nin bold represent different alignment": "better\nthan RF and THMM on CMU-MOSI. Third, multi-",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "source\nand target domains with a domain discriminator by"
        },
        {
          "rectangles with text\nin bold represent different alignment": "class classication is more difcult\nthan binary classication,",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "adversarially making different domains indistinguishable. The"
        },
        {
          "rectangles with text\nin bold represent different alignment": "such\nas\n77.1\nvs.\n34.7\nof MARN on CMU-MOSI. Fourth,",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "input\nto the discriminator\nranges\nfrom original data\nto ex-"
        },
        {
          "rectangles with text\nin bold represent different alignment": "comparing the same method in the two tables on CMU-MOSI,",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "tracted features and the adversarial alignment can be global"
        },
        {
          "rectangles with text\nin bold represent different alignment": "we\ncan conclude\nthat BERT and XLNet\ncan provide better",
          "strategies. Most existing multi-modal domain adaptation methods can be obtained by employing": "or class-wise. We can also consider using shared or unshared"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "feature extractors. Adversarial generative models usually em-",
          "11": "consumer approach behavior. To be specic,\nfor participants"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ploy\na\ngenerator\nto\ngenerate\nfake\nsource\nor\ntarget\ndata\nto",
          "11": "whose emotional receptivity is high, smiling facial expression"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "make\nthe\ndomain\ndiscriminator\nindistinguishable\nfrom the",
          "11": "tends\nto\nlead\nto\nthe\nhighest\napproach\nbehavior. Besides,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "generated and real domains. The generator\nis typically based",
          "11": "researchers\nexamine\nhow online\nstore\nspecialization\ninu-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "on\ngenerative\nadversarial\nnetwork\n(GAN)\nand\nits\nvariants,",
          "11": "ences consumer pleasure and arousal, based on the stimulus-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "such\nas CoGAN, SimGAN,\nand CycleGAN. The\ninput\nto",
          "11": "organism-response framework. Emotion recognition can also"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "the generator and discriminator can be different\nin different",
          "11": "be used in call\ncenters,\nthe goal of which is\nto detect\nthe"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "methods. Self-supervision-based methods combine some aux-",
          "11": "emotional states of both the caller and the operator. The system"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "iliary self-supervised learning tasks,\nsuch as\nreconstruction,",
          "11": "recognizes\nthe involved emotions\nthrough the intonation and"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "image\nrotation\nprediction,\njigsaw prediction,\nand masking,",
          "11": "tempo, as well as the texts translated from the corresponding"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "with the original\ntask network to bring the source and target",
          "11": "speech. Based on this, we can receive feedback on the quality"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "domains closer. We can compare these four\ntypes of domain",
          "11": "of\nthe service."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "adaptation methods from the perspectives of theory guarantee,",
          "11": "Meanwhile,\nemotion\nrecognition\nplays\nan\nimportant\nrole"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "efciency,\ntask scalability, data scalability, data dependency,",
          "11": "in the eld of medical\ntreatment\nand psychological health."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "optimizability,\nand\nperformance. We\ncan\ncombine\nsome\nof",
          "11": "With the popularity of\nsocial media,\nsome people prefer\nto"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "these methods to jointly exploit\ntheir advantages.",
          "11": "sharing their emotions on the Internet rather than with others."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "The main\ndifculty\nin\ndomain\nadaptation\nfor MER lies",
          "11": "If\na\nuser\nis\nobserved\nto\nbe\nsharing\nnegative\ninformation"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "in the\nalignment of multiple modalities between the\nsource",
          "11": "(e.g.,\nsadness)\nfrequently\nand\ncontinuously,\nit\nis\nnecessary"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and\ntarget\ndomains\nsimultaneously. There\nare\nsome\nsimple",
          "11": "to\ntrack\nher/his mental\nstatus\nto\nprevent\nthe\noccurrence\nof"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "but\neffective ways\nto\nextend\nuni-modal\ndomain\nadaptation",
          "11": "psychological\nillness and even suicide. Emotional\nstates can"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "to multi-modal\nsettings,\nas\nshown\nin Fig.\n4. For\nexample,",
          "11": "also be used to monitor and predict fatigue states of a variety of"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "we\ncan\nuse\ndiscrepancy\nloss\nor\ndiscriminator\nto\nalign\nthe",
          "11": "people like drivers, pilots, workers in assembly lines, and stu-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "fused\nfeature\nrepresentations. The\ncorrespondence\nbetween",
          "11": "dents in classrooms. This technique both prevents dangerous"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "different modalities can be used as a self-supervised alignment.",
          "11": "situations and benets the evaluation of work/study efciency."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Extending adversarial generative models\nfrom uni-modal\nto",
          "11": "Further,\nemotional\nstates\ncan\nbe\nincorporated\ninto\nvarious"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "multi-modal would\nbe more\ndifcult. Unlike\nimage,\nother",
          "11": "security applications,\nsuch as\nsystems\nfor monitoring public"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "generated modalities,\nsuch\nas\ntext\nand\nspeech, might\nhave",
          "11": "spaces (e.g., bus/train/subway stations,\nfootball stadiums)\nfor"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "confused semantics, although they can make the discriminator",
          "11": "potential aggression. Recently, an effective auxiliary system is"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "indistinguishable. Generating intermediate feature representa-",
          "11": "introduced in the diagnosis and treatment process of autism"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "tions instead of\nraw data can provide a feasible solution.",
          "11": "spectrum disorder\n(ASD) of\nchildren,\nto assist\nin collecting"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "11": "the pathological\ninformation. To help professional clinicians"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "11": "better\nand\nfaster make\na\ndiagnosis\nand\ngive\ntreatment\nto"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "VIII. APPLICATIONS",
          "11": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "11": "ASD patients,\nthis system characterizes facial expressions and"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Recognizing emotions from multiple explicit cues and im-",
          "11": "eye\ngaze\nattention which\nare\nconsidered\nto\nbe\nremarkable"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "plicit stimuli\nis of great signicance in a broad range of\nreal-",
          "11": "indicators for early screening of autism."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "world applications. Generally speaking, emotion is\nthe most",
          "11": "Multi-modal\nemotion\nrecognition\nis\nused\nto\nimprove\nthe"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "important aspect of the quality and meaning of our existence,",
          "11": "personal entertainment experience. For example, a recent work"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "which makes life worth living. The emotional impact of digital",
          "11": "in\nbrainwavemusic\ninterface maps EEG characteristics\nto"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "data lies in that\nit can improve the user experience of existing",
          "11": "musical structures (note, intensity, and pitch). Similarly, efforts"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "techniques and then strengthen the knowledge transfer between",
          "11": "have been made to understand the emotion-centric correlation"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "people and computers [18].",
          "11": "between\ndifferent modalities\nthat\nare\nessential\nfor\nvarious"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Many people tend to post texts, images, and videos on social",
          "11": "applications. Affective image-music matching provides a good"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "networks to express their daily life feelings.\nInspired by this,",
          "11": "chance\nto\nappend\na\nsequence\nof music\nto\na\ngiven\nimage,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "we can mine peoples opinions and sentiments towards topics",
          "11": "where they may evoke the same emotion. This helps generate"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and events happening in the\nreal world [28]. For\ninstance,",
          "11": "emotion-aware music\nplaylists\nfrom ones\npersonal\nalbum"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "user-generated content\nin Facebook or\nInstagram can be used",
          "11": "photos in mobile devices."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "to\nderive\nthe\nattitudes\nof\npeople\nfrom different\ncountries",
          "11": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "11": "IX. FUTURE DIRECTIONS"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and regions when they face epidemics\nlike COVID-19 [29].",
          "11": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Researchers\nalso try to detect\nsentiment\nin social networks",
          "11": "Existing methods\nhave\nachieved\npromising\nperformances"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and apply the results\nto predict political elections. Note that",
          "11": "on various MER settings, such as visual-audio,\nfacial-textual-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "when the personalized emotion of an individual\nis detected,",
          "11": "speech, and textual-visual\ntasks. However, all\nthe summarized"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "we can further group these emotions, which may contribute to",
          "11": "challenges have not been fully addressed. For example, how to"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "predicting the tendencies of society.",
          "11": "extract discriminative features that are more related to emotion,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Another\nimportant\napplication\nof multi-modal\nemotion",
          "11": "how to balance between common and personalized emotion"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "recognition is business intelligence, especially marketing and",
          "11": "reactions, and how to emphasize the more important modalities"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "consumer behavior analysis [30]. Nowadays, most apparel e-",
          "11": "are\nstill\nopen. To\nhelp\nimprove\nthe\nperformances\nof MER"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "retailers use human models to present products. The models",
          "11": "methods and make them t special requirements in real world,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "face\npresentation\nis\nproved\nto\nhave\na\nsignicant\neffect\non",
          "11": "we provide some potential\nfuture directions."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "New Methodologies\nfor MER. 1) Contextual and prior",
          "12": "emotion of different\nindividuals is insufcient.\nIt\nis ideal but"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "knowledge modeling.\nThe\nexperienced\nemotion\nof\na\nuser",
          "12": "impractical\nto collect enough data for each individual\nto train"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "can be signicantly inuenced by the contextual\ninformation,",
          "12": "personalized MER models. Adapting the well-trained MER"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "such as the conversational and social environments. The prior",
          "12": "models for dominant emotions to each individual with a small"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "knowledge\nof\nusers,\nsuch\nas\npersonality\nand\nage,\ncan\nalso",
          "12": "amount of\nlabeled data\nis\na possible\nalternate\nsolution. On"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "contribute to emotion perception. For example, an optimistic",
          "12": "the other hand,\nit would make more sense to predict emotions"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "user\nand\na\npessimistic\nviewer\nare\nlikely\nto\nsee\ndifferent",
          "12": "for groups of\nindividuals who share similar\ntastes or\ninterests"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "aspects of the same stimuli. Jointly considering these important",
          "12": "and have\na\nsimilar background. Group emotion recognition"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "contextual\nand prior knowledge\nis\nexpected to improve\nthe",
          "12": "is\nessential\nin many\napplications,\nsuch\nas\nrecommendation"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "MER performance. Graph-related methods,\nsuch\nas\ngraph",
          "12": "systems, but how to classify users into different groups is still"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "convolutional networks,\nare possible\nsolutions\nto model\nthe",
          "12": "challenging."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "relationships between factors and emotions. 2) Learning from",
          "12": "Implementation of\nReal Applications Based on MER. 1)"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "unlabeled, unreliable, and unmatched affective signals. In the",
          "12": "MER in real-world applications. Although emotion recogni-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "big data era,\nthe affective data might be sparsely labeled or",
          "12": "tion has been emphasized to be important\nfor decades,\nit has"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "even unlabeled,\nthe\nraw data or\nlabels\ncan be\nreliable,\nand",
          "12": "rarely\nbeen\napplied\nto\nreal\nscenarios\ndue\nto\nrelatively\nlow"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "the\ntest\nand\ntraining\ndata might\nbe\nunmatched. Exploring",
          "12": "performance. With\nthe\nrecent\nrapid\nprogress\nof MER, we"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "advanced machine learning techniques, such as unsupervised",
          "12": "can\nbegin\nincorporating\nemotion\ninto\ndifferent\napplications"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "representation learning, dynamic data selection and balancing,",
          "12": "in marketing,\neducation,\nhealth\ncare,\nand\nservice\nsectors."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and domain adaptation, and embedding the special properties",
          "12": "The feedback from the applications can in turn promote the"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of emotions, can help to address these challenges. 3) Explain-",
          "12": "development of MER. Together with emotion generation, we"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "able,\nrobust, and secure deep learning for MER. Due to the",
          "12": "believe an age of articial emotional intelligence is coming. 2)"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "black-box nature, it is difcult to understand why existing deep",
          "12": "Wearable,\nsimple, and accurate affective data collection. To"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "neural networks perform well\nfor MER and the trained deep",
          "12": "conduct MER tasks, the rst step is to collect accurate affective"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "networks are vulnerable to adversarial attacks and inevitable",
          "12": "data. Developing wearable,\nsimple and even contactless\nsen-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "noises that might cause erraticism. Essentially explaining the",
          "12": "sors to capture such data would make users more acceptable."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "decision-making process of deep learning can help to design",
          "12": "3)\nSecurity,\nprivacy,\nethics,\nand\nfairness\nof MER. During"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "robust and secure MER systems. 4) Combination of explicit",
          "12": "data\ncollection,\nit\nis\npossible\nto\nextract\nusers\ncondential"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and\nimplicit\nsignals. Both\nexplicit\nand\nimplicit\nsignals\nare",
          "12": "information, such as identity, age, etc. Protecting the security"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "demonstrated to be useful\nfor MER but\nthey also suffer\nfrom",
          "12": "and\nprivacy\nof\nusers\nand\navoiding\nany\nchance\nof misuse"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "some limitations. For example, explicit\nsignals can be easily",
          "12": "must be taken into consideration. Emotion recognition in real"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "suppressed or are difcult\nto capture, while implicit\nsignals",
          "12": "applications might have a negative and even dangerous impact"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "might not reect\nthe emotions in real-time. Jointly combining",
          "12": "on a person, such as emotional pressure. Methods to eliminate"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "them to explore the complementary information during viewer-",
          "12": "such impact should also be considered from the perspectives"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "multimedia interaction would boost\nthe MER performance. 5)",
          "12": "of ethics and fairness."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Incorporation of emotion theory into MER. Different\ntheories",
          "12": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "have\nbeen\nproposed\nin\npsychology,\nphysiology,\nneurology,",
          "12": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "12": "X. CONCLUSION"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and cognitive sciences. These theories can help to understand",
          "12": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "how humans produce emotion but have not been employed in",
          "12": "In\nthis\narticle, we\nprovided\na\ncomprehensive\ntutorial\non"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "the computational MER task. We believe such incorporation",
          "12": "multi-modal\nemotion\nrecognition\n(MER). We\nbriey\nintro-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "would make more sense to recognize emotions.",
          "12": "duced emotion representation models, both explicit\nand im-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "More Practical MER Settings. 1) MER in the wild. Current",
          "12": "plicit\naffective modalities,\nemotion\nannotations,\nand\ncorre-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "MER methods mainly focus on neat\nlab settings. However,",
          "12": "sponding computational\ntasks. We summarized the main chal-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "MER problems in the real world are much more complex. For",
          "12": "lenges of MER in detail, and then we emphatically introduced"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "example,\nthe collected data might contain much noise that\nis",
          "12": "different computational methodologies,\nincluding representa-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "unrelated to emotion; the users in the test set are from different",
          "12": "tion\nlearning\nof\neach\naffective modality,\nfeature\nfusion\nof"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "cultures and languages\nfrom those in the training set, which",
          "12": "different affective modalities, classier optimization for MER,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "results\nin\ndifferent ways\nof\nemotion\nexpression;\ndifferent",
          "12": "and domain adaptation for MER. We ended this tutorial with"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "emotion label\nspaces\nare\nemployed across various\nsettings;",
          "12": "discussions on real-world applications and future directions."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "training data is incrementally available. Designing an effective",
          "12": "We hope this tutorial can motivate novel\ntechniques to facili-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "MER model\nthat\nis generalizable to these practical settings is",
          "12": "tate the development of MER, and we believe that MER will"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "worth investigating. 2) MER on the\nedge. When deploying",
          "12": "continue to attract signicant\nresearch efforts."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "MER models\nin\nedge\ndevices,\nsuch\nas mobile\nphones\nand",
          "12": "Acknowledgements: This work was supported by the National"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "security cameras, we have\nto consider\nthe\ncomputing limi-",
          "12": "Key Research and Development Program of China Grant (No."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "tation and data privacy. Techniques like auto pruning, neural",
          "12": "2018AAA0100403),\nthe National Natural Science Foundation"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "architecture\nsearch,\ninvertible neural network,\nand software-",
          "12": "of China (Nos. 61701273, 61876094, U1933114, 61925107,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "hardware co-design are believed to be benecial\nfor efcient",
          "12": "U1936202),\nthe Natural Science Foundation of Tianjin, China"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "on-device training. 3) Personalized and group MER. Because",
          "12": "(Nos.20JCJQJC00020, 18JCYBJC15400, 18ZXZNGX00110),"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of the emotions subjectivity, simply recognizing the dominant",
          "12": "and Berkeley DeepDrive."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "REFERENCES",
          "13": "[24] U. Bhattacharya, T. Mittal, R. Chandra, T. Randhavane, A. Bera, and"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "13": "D. Manocha,\nStep:\nSpatial\ntemporal\ngraph\nconvolutional\nnetworks"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[1] D. Kahneman, Thinking,\nfast and slow.\nMacmillan, 2011.",
          "13": "for\nemotion perception from gaits.\nin AAAI Conference on Articial"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[2] M. Minsky, The Society of mind.\nSimon and Schuster, 1986.",
          "13": "Intelligence, 2020, pp. 13421350."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[3] D.\nSchuller\nand B. W.\nSchuller,\nThe\nage\nof\narticial\nemotional",
          "13": "[25] Y.-J. Liu, M. Yu, G. Zhao,\nJ. Song, Y. Ge,\nand Y. Shi,\nReal-time"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "intelligence, Computer, vol. 51, no. 9, pp. 3846, 2018.",
          "13": "IEEE\nmovie-induced discrete\nemotion recognition from eeg signals,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[4] M. Soleymani, D. Garcia, B. Jou, B. Schuller, S.-F. Chang, and M. Pan-",
          "13": "Transactions on Affective Computing, vol. 9, no. 4, pp. 550562, 2018."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Image\nand Vision\ntic,\nA survey\nof multimodal\nsentiment\nanalysis,",
          "13": "[26] A. Hu and S. Flaxman, Multimodal sentiment analysis to explore the"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Computing, vol. 65, pp. 314, 2017.",
          "13": "structure of emotions, in ACM International Conference on Knowledge"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[5]\nJ. Wagner, E. Andre, F. Lingenfelser,\nand J. Kim,\nExploring fusion",
          "13": "Discovery & Data Mining, 2018, pp. 350358."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "methods for multimodal emotion recognition with missing data, IEEE",
          "13": "[27]\nS. E. Kahou, V. Michalski, K. Konda, R. Memisevic,\nand C.\nPal,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Transactions on Affective Computing, vol. 2, no. 4, pp. 206218, 2011.",
          "13": "Recurrent neural networks for emotion recognition in video, in ACM"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[6]\nS. K. Dmello and J. Kory, A review and meta-analysis of multimodal",
          "13": "International Conference on Multimodal Interaction, 2015, pp. 467474."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "affect detection systems, ACM Computing Surveys, vol. 47, no. 3, p. 43,",
          "13": "[28] R.\nJi, F. Chen, L. Cao,\nand Y. Gao,\nCross-modality microblog sen-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "2015.",
          "13": "timent prediction via bi-layer multimodal hypergraph learning, IEEE"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[7] D. Ramachandram and G. W.\nTaylor,\nDeep multimodal\nlearning:",
          "13": "Transactions on Multimedia, vol. 21, no. 4, pp. 10621075, 2019."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "IEEE Signal Processing\nA survey\non\nrecent\nadvances\nand\ntrends,",
          "13": "[29] H.\nLyu,\nL. Chen, Y. Wang,\nand\nJ.\nLuo,\nSense\nand\nsensibility:"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Magazine, vol. 34, no. 6, pp. 96108, 2017.",
          "13": "Characterizing\nsocial media\nusers\nregarding\nthe\nuse\nof\ncontroversial"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[8]\nS. K. DMello, N. Bosch, and H. Chen, Multimodal-multisensor affect",
          "13": "terms\nfor covid-19, IEEE Transactions on Big Data, 2020.\n[Online]."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "detection, in The Handbook of Multimodal-Multisensor Interfaces: Sig-",
          "13": "Available: https://doi.org/10.1109/TBDATA.2020.2996401"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "nal Processing, Architectures, and Detection of Emotion and Cognition-",
          "13": "[30] R. Wu and C. L. Wang, The asymmetric impact of other-blame regret"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Volume 2, 2018, pp. 167202.",
          "13": "versus self-blame regret on negative word of mouth: Empirical evidence"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[9]\nT. Baltruaitis, C. Ahuja,\nand L.-P. Morency,\nMultimodal machine",
          "13": "from china, European Journal of Marketing, vol. 51, no. 11/12, pp."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "IEEE\nTransactions\non\nPattern\nlearning: A survey\nand\ntaxonomy,",
          "13": "17991816, 2017."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Analysis and Machine Intelligence, vol. 41, no. 2, pp. 423443, 2019.",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[10]\nS.\nZhao, X. Yao,\nJ. Yang, G.\nJia, G. Ding,\nT.-S. Chua, B. W.",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Schuller,\nand K. Keutzer,\nAffective\nimage\ncontent\nanalysis:\nTwo",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "IEEE\nTransactions\non\ndecades\nreview\nand\nnew\nperspectives,",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Pattern Analysis and Machine Intelligence, 2021.\n[Online]. Available:",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "https://doi.org/10.1109/TPAMI.2021.3094362",
          "13": "Sicheng Zhao (SM19)\nreceived the Ph.D. degree from Harbin Institute of"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[11] M. D. Munezero, C. S. Montero, E. Sutinen, and J. Pajunen, Are they",
          "13": "Technology, Harbin, China,\nin 2016. He was a Visiting Scholar at National"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "different? affect,\nfeeling, emotion,\nsentiment, and opinion detection in",
          "13": "University of Singapore from July 2013 to June 2014, a Research Fellow at"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "text, IEEE Transactions on Affective Computing, vol. 5, no. 2, pp. 101",
          "13": "Tsinghua University from September 2016 to September 2017, and a Research"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "111, 2014.",
          "13": "Fellow at University of California, Berkeley from September 2017 to Septem-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[12]\nZ. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang, A survey of affect",
          "13": "ber 2020. He is currently a Postdoc Research Scientist at Columbia University,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "recognition methods: Audio, visual, and spontaneous expressions, IEEE",
          "13": "USA. His\nresearch\ninterests\ninclude\naffective\ncomputing, multimedia,\nand"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Transactions\non Pattern Analysis\nand Machine\nIntelligence,\nvol.\n31,",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "",
          "13": "computer vision."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "no. 1, pp. 3958, 2009.",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[13]\nZ. Zhang, N. Cummins, and B. Schuller, Advanced data exploitation",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "in speech analysis: An overview, IEEE Signal Processing Magazine,",
          "13": "Guoli Jia will work toward the Masters degree at\nthe College of Computer"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "vol. 34, no. 4, pp. 107129, 2017.",
          "13": "Science, Nankai University, Tianjin, China. His\nresearch\ninterests\ninclude"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[14] M. B. Akay and K. Oguz,\nSpeech emotion recognition: Emotional",
          "13": "computer vision and pattern recognition."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "models, databases, features, preprocessing methods, supporting modali-",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "ties, and classiers, Speech Communication, vol. 116, pp. 5676, 2020.",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[15] R. Subramanian,\nJ. Wache, M. K. Abadi, R. L. Vieriu, S. Winkler,",
          "13": "Jufeng Yang\nreceived\nthe Ph.D.\ndegree\nfrom Nankai University, Tianjin,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and N. Sebe,\nAscertain: Emotion\nand\npersonality\nrecognition\nusing",
          "13": "China,\nin 2009. He is currently a full professor\nin the College of Computer"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "commercial sensors, IEEE Transactions on Affective Computing, vol. 9,",
          "13": "Science, Nankai University and was a visiting scholar with the Vision and"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "no. 2, pp. 147160, 2018.",
          "13": "Learning Lab, University of California, Merced, USA,\nfrom 2015 to 2016."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[16] A. Giachanou\nand F. Crestani,\nLike\nit\nor\nnot: A survey\nof\ntwitter",
          "13": "His\nresearch\nfalls\nin\nthe eld\nof\ncomputer\nvision, machine\nlearning\nand"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "sentiment analysis methods, ACM Computing Surveys, vol. 49, no. 2,",
          "13": "multimedia. His recent\ninterests include affective computing,\nimage retrieval,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "pp. 141, 2016.",
          "13": "ne-grained classication, and medical\nimage recognition."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[17] W. Rahman, M. K. Hasan, S. Lee, A. Zadeh, C. Mao, L.-P. Morency,",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "and E. Hoque, Integrating multimodal\ninformation in large pretrained",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "the Association for Computational\ntransformers, in Annual Meeting of",
          "13": "Guiguang Ding received his Ph.D. degree\nfrom Xidian University, China,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Linguistics, 2020, pp. 23592369.",
          "13": "in 2004. He is currently a full professor with School of Software, Tsinghua"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[18] D. Joshi, R. Datta, E. Fedorovskaya, Q.-T. Luong, J. Z. Wang, J. Li, and",
          "13": "University. Before joining school of software in 2006, he has been a postdoc-"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "J. Luo, Aesthetics and emotions\nin images, IEEE Signal Processing",
          "13": "toral\nresearch fellow in the Department of Automation, Tsinghua University."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Magazine, vol. 28, no. 5, pp. 94115, 2011.",
          "13": "His current\nresearch centers on the area of multimedia information retrieval,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[19]\nS. Wang and Q. Ji, Video affective content analysis: a survey of state-",
          "13": "computer vision, and machine learning. He served as a leading guest editor of"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "of-the-art methods, IEEE Transactions on Affective Computing, vol. 6,",
          "13": "NPL and MTAP, a special session chair of\nICASSP 2021,\nICME 2020/2019,"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "no. 4, pp. 410430, 2015.",
          "13": "PCM 2017, and a reviewer\nfor over 20 prestigious international\njournals and"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[20]\nJ. Yang, M. Sun, and S. Xiaoxiao, Learning visual sentiment distribu-",
          "13": "conferences. He has published over 100 scientic papers\nin major\njournals"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "tions via augmented conditional probability neural network, in AAAI",
          "13": "and conferences."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Conference on Articial\nIntelligence, 2017, pp. 224230.",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[21]\nS. Zhao, H. Yao, Y. Gao, R. Ji, and G. Ding, Continuous probability",
          "13": ""
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "distribution prediction of\nimage emotions via multi-task shared sparse",
          "13": "Kurt Keutzer received his Ph.D. degree in Computer Science from Indiana"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "regression, IEEE Transactions on Multimedia, vol. 19, no. 3, pp. 632",
          "13": "University\nin\n1984\nand\nthen\njoined\nthe\nresearch\ndivision\nof AT&T Bell"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "645, 2017.",
          "13": "Laboratories.\nIn 1991 he joined Synopsys,\nInc. where he ultimately became"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[22] D. She,\nJ. Yang, M.-M. Cheng, Y.-K. Lai, P. L. Rosin, and L. Wang,",
          "13": "Chief Technical Ofcer and Senior Vice-President of Research. In 1998, Kurt"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "Wscnet: Weakly\nsupervised\ncoupled\nnetworks\nfor\nvisual\nsentiment",
          "13": "became Professor\nof Electrical Engineering\nand Computer Science\nat\nthe"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "classication and detection, IEEE Transactions on Multimedia, vol. 22,",
          "13": "University of California at Berkeley. Kurts research group is currently focused"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "no. 5, pp. 13581371, 2020.",
          "13": "on using parallelism to accelerate the training and deployment of Deep Neural"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[23]\nS. Zhao, X. Yue,\nS. Zhang, B. Li, H. Zhao, B. Wu, R. Krishna,",
          "13": "Networks for applications in computer vision, speech recognition, multi-media"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "J. E. Gonzalez, A. L. Sangiovanni-Vincentelli, S. A. Seshia et al., A",
          "13": "analysis, and computational nance. Kurt has published six books, over 250"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "review of\nsingle-source deep unsupervised visual domain adaptation,",
          "13": "refereed articles, and is among the most highly cited authors in Hardware and"
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "IEEE Transactions on Neural Networks and Learning Systems, 2020.",
          "13": "Design Automation. Kurt\nis a Life Fellow of\nthe IEEE."
        },
        {
          "IEEE SIGNAL PROCESSING MAGAZINE": "[Online]. Available: https://doi.org/10.1109/TNNLS.2020.3028503",
          "13": ""
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Thinking, fast and slow",
      "authors": [
        "D Kahneman"
      ],
      "year": "2011",
      "venue": "Thinking, fast and slow"
    },
    {
      "citation_id": "2",
      "title": "The Society of mind",
      "authors": [
        "M Minsky"
      ],
      "year": "1986",
      "venue": "The Society of mind"
    },
    {
      "citation_id": "3",
      "title": "The age of artificial emotional intelligence",
      "authors": [
        "D Schuller",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Computer"
    },
    {
      "citation_id": "4",
      "title": "A survey of multimodal sentiment analysis",
      "authors": [
        "M Soleymani",
        "D Garcia",
        "B Jou",
        "B Schuller",
        "S.-F Chang"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "5",
      "title": "Exploring fusion methods for multimodal emotion recognition with missing data",
      "authors": [
        "J Wagner",
        "E Andre",
        "F Lingenfelser",
        "J Kim"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "A review and meta-analysis of multimodal affect detection systems",
      "authors": [
        "J Kory"
      ],
      "year": "2015",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "7",
      "title": "Deep multimodal learning: A survey on recent advances and trends",
      "authors": [
        "D Ramachandram",
        "G Taylor"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "8",
      "title": "Multimodal-multisensor affect detection",
      "authors": [
        "S D'mello",
        "N Bosch",
        "H Chen"
      ],
      "year": "2018",
      "venue": "The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition"
    },
    {
      "citation_id": "9",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "T Baltruaitis",
        "C Ahuja",
        "L.-P Morency"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Affective image content analysis: Two decades review and new perspectives",
      "authors": [
        "S Zhao",
        "X Yao",
        "J Yang",
        "G Jia",
        "G Ding",
        "T.-S Chua",
        "B Schuller",
        "K Keutzer"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2021.3094362"
    },
    {
      "citation_id": "11",
      "title": "Are they different? affect, feeling, emotion, sentiment, and opinion detection in text",
      "authors": [
        "M Munezero",
        "C Montero",
        "E Sutinen",
        "J Pajunen"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions",
      "authors": [
        "Z Zeng",
        "M Pantic",
        "G Roisman",
        "T Huang"
      ],
      "year": "2009",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Advanced data exploitation in speech analysis: An overview",
      "authors": [
        "Z Zhang",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers",
      "authors": [
        "M Akay",
        "K Oguz"
      ],
      "year": "2020",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "15",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "R Subramanian",
        "J Wache",
        "M Abadi",
        "R Vieriu",
        "S Winkler",
        "N Sebe"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Like it or not: A survey of twitter sentiment analysis methods",
      "authors": [
        "A Giachanou",
        "F Crestani"
      ],
      "year": "2016",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "17",
      "title": "Integrating multimodal information in large pretrained transformers",
      "authors": [
        "W Rahman",
        "M Hasan",
        "S Lee",
        "A Zadeh",
        "C Mao",
        "L.-P Morency",
        "E Hoque"
      ],
      "year": "2020",
      "venue": "Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "18",
      "title": "Aesthetics and emotions in images",
      "authors": [
        "D Joshi",
        "R Datta",
        "E Fedorovskaya",
        "Q.-T Luong",
        "J Wang",
        "J Li",
        "J Luo"
      ],
      "year": "2011",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "19",
      "title": "Video affective content analysis: a survey of stateof-the-art methods",
      "authors": [
        "S Wang",
        "Q Ji"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Learning visual sentiment distributions via augmented conditional probability neural network",
      "authors": [
        "J Yang",
        "M Sun",
        "S Xiaoxiao"
      ],
      "year": "2017",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "21",
      "title": "Continuous probability distribution prediction of image emotions via multi-task shared sparse regression",
      "authors": [
        "S Zhao",
        "H Yao",
        "Y Gao",
        "R Ji",
        "G Ding"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "22",
      "title": "Wscnet: Weakly supervised coupled networks for visual sentiment classification and detection",
      "authors": [
        "D She",
        "J Yang",
        "M.-M Cheng",
        "Y.-K Lai",
        "P Rosin",
        "L Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "A review of single-source deep unsupervised visual domain adaptation",
      "authors": [
        "S Zhao",
        "X Yue",
        "S Zhang",
        "B Li",
        "H Zhao",
        "B Wu",
        "R Krishna",
        "J Gonzalez",
        "A Sangiovanni-Vincentelli",
        "S Seshia"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems",
      "doi": "10.1109/TNNLS.2020.3028503"
    },
    {
      "citation_id": "24",
      "title": "Step: Spatial temporal graph convolutional networks for emotion perception from gaits",
      "authors": [
        "U Bhattacharya",
        "T Mittal",
        "R Chandra",
        "T Randhavane",
        "A Bera",
        "D Manocha"
      ],
      "year": "2020",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Real-time movie-induced discrete emotion recognition from eeg signals",
      "authors": [
        "Y.-J Liu",
        "M Yu",
        "G Zhao",
        "J Song",
        "Y Ge",
        "Y Shi"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Multimodal sentiment analysis to explore the structure of emotions",
      "authors": [
        "A Hu",
        "S Flaxman"
      ],
      "year": "2018",
      "venue": "ACM International Conference on Knowledge Discovery & Data Mining"
    },
    {
      "citation_id": "27",
      "title": "Recurrent neural networks for emotion recognition in video",
      "authors": [
        "S Kahou",
        "V Michalski",
        "K Konda",
        "R Memisevic",
        "C Pal"
      ],
      "year": "2015",
      "venue": "ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "28",
      "title": "Cross-modality microblog sentiment prediction via bi-layer multimodal hypergraph learning",
      "authors": [
        "R Ji",
        "F Chen",
        "L Cao",
        "Y Gao"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Sense and sensibility: Characterizing social media users regarding the use of controversial terms for covid-19",
      "authors": [
        "H Lyu",
        "L Chen",
        "Y Wang",
        "J Luo"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Big Data",
      "doi": "10.1109/TBDATA.2020.2996401"
    },
    {
      "citation_id": "30",
      "title": "The asymmetric impact of other-blame regret versus self-blame regret on negative word of mouth: Empirical evidence from china",
      "authors": [
        "R Wu",
        "C Wang"
      ],
      "year": "2017",
      "venue": "European Journal of Marketing"
    }
  ]
}