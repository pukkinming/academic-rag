{
  "paper_id": "2510.13862v1",
  "title": "Ensembling Large Language Models To Characterize Affective Dynamics In Student-Ai Tutor Dialogues",
  "published": "2025-10-13T04:43:56Z",
  "authors": [
    "Chenyu Zhang",
    "Sharifa Alghowinem",
    "Cynthia Breazeal"
  ],
  "keywords": [
    "affective computing",
    "intelligent tutoring systems",
    "large language models",
    "emotion detection",
    "ensemble learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While recent studies have examined the leaning impact of large language model (LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring remain insufficiently understood. This work introduces the first ensemble-LLM framework for large-scale affect sensing in tutoring dialogues, advancing the conversation on responsible pathways for integrating generative AI into education by attending to learners' evolving affective states. To achieve this, we analyzed two semesters' worth of 16,986 conversational turns exchanged between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across three U.S. institutions. To investigate learners' emotional experiences, we generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o, Claude), including scalar ratings of valence, arousal, and learning-helpfulness, along with free-text emotion labels. These estimates are fused through rankweighted intra-model pooling and plurality consensus across models to produce robust emotion profiles. Our analysis shows that during interaction with the AI tutor, students typically report mildly positive affect and moderate arousal. Yet learning is not uniformly smooth: confusion and curiosity are frequent companions to problem solving, and frustration, while less common, still surfaces in ways that can derail progress. Emotional states are short-lived-positive moments last slightly longer than neutral or negative ones, but they are fragile and easily disrupted. Encouragingly, negative emotions often resolve quickly, sometimes rebounding directly into positive states. Neutral moments frequently act as turning points, more often steering students upward than downward, suggesting opportunities for tutors to intervene at precisely these junctures.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Large language model (LLM)-based tutors promise ondemand hints, dialogue-level adaptation, and scalable personalization. Yet recent classroom studies report mixed or even negative effects on learning gains  [1] ,  [2] . Most evaluations still judge success solely by post-test scores or neurophysiological proxies such as EEG-derived cognitive load, leaving the felt emotional experience of learners virtually unmeasured, even though emotion steers attention  [3] , memory  [4] ,  [5] , and reasoning  [6] . In practice, students often quit when frustration mounts; an AI tutor's real success may lie in navigating those emotions through adaptive tone, timely feedback, and encouragement.\n\nAffective science offers two complementary lenses. One maps emotions to a continuous valence-arousal-dominance (VAD) space  [7] ; the other lists biologically primed \"basic\" categories  [8] . Hybrid accounts bridge the views  [9] . Extending them to education, Kort's learning-unlearning spiral adds a progress axis that rates how constructive an emotion is for mastery  [10] . Building on models of affective dynamics that link learner emotions to comprehension progress  [11] , we operationalize a third axis, learning-helpfulness, using a 1-9 Likert scale to index how much each conversational turn advances a student's understanding.\n\nYet the corpora driving most emotion models were tagged by only a few annotators (typically 3), neglect cultural variance, and force single-label decisions-choices at odds with mixed-emotion findings  [12]  and liable to propagate bias downstream. LLMs, trained on vast and diverse corpora, can dilute some of that bias, yet introduce their own idiosyncrasies  [13] . Despite such advances, large-scale evidence on the affective impact of LLM tutors remains scarce; most studies still track purely cognitive metrics. We therefore adopt a threemodel ensemble: independent inferences from Gemini, GPT-4o, and Claude are fused via hierarchical consensus, reducing both annotator-and model-specific errors. We ask: RQ1 Which affective states, both scalar and categorical, dominate student-AI tutor interactions? RQ2 How do these states evolve over time, especially with respect to valence transitions?\n\nWe answer them by analyzing",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Method",
      "text": "Building on RQ1 and RQ2, this section details the dataset, preprocessing, and ensemble-annotation pipeline that yield turn-level affect labels. We then outline the temporal analyses used to trace valence transitions and answer our research questions.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Dataset",
      "text": "The dataset for this study comes from PyTutor, a GPT-4o-based Socratic AI tutor deployed during the Fall 2024 and Spring 2025 semesters at MIT, GSU, and QCC. PyTutor provides on-demand hints, withholds complete solutions, and prompts learners to debug their own code via metacognitive questions. The deployment involved 261 undergraduates enrolled in introductory Python or computing courses, all working in English.\n\nAcross two 15-week semesters these learners produced 16,986 human-AI conversational turns. A \"session\" is defined as a contiguous block of activity separated by 60 min of inactivity. Table  I  summarizes key statistics for participants, sessions, turns, and tokens.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Automatic Emotion Annotation & Ensemble Fusion",
      "text": "Manual affect annotation is notoriously burdensome: a single crowdsourced label set can cost tens of hours, typically relies on only a few annotators, and inherits their cultural and taxonomic biases while omitting learning-specific constructs altogether. These constraints make it impractical to obtain turn-level ground truth at semester scale. We therefore opt for automatic annotation via an ensemble of frontier LLMs, leveraging their broad pre-training to minimize singleannotator bias while still acknowledging model idiosyncrasies. We generate turn-level affect annotations through a two-step pipeline that combines (i) zero-shot emotion inference from multiple LLMs with (ii) hierarchical ensemble fusion.\n\nZero-shot annotation.: Every student-tutor utterance is processed by three frontier models: Gemini 2.0 Flash (Google), GPT-4o mini (OpenAI), and Claude 3.5 Sonnet  (Anthropic) , under an identical system prompt. Each model returns two outputs: (i) a ranked list of up to K = 5 discrete emotion labels, and (ii) three 1-9 Likert ratings for valence (v), arousal (a), and learning-helpfulness (ℓ). Apart from the placeholder ''neutral'', no candidate labels are hardcoded, enabling the models to surface domain-specific states such as puzzlement or anticipation.\n\nHierarchical ensemble fusion.: Let m ∈ {1, 2, 3} index models and r ∈ {1, . . . , K m } the rank within a model's output. Fusion proceeds in three stages:\n\nStage 3: Label consensus. For discrete labels, let f t (e) be the number of models that emit label e. The primary ensemble label is the plurality winner e ⋆ t = arg max e f t (e), with ties broken by higher vt and, if still tied, lexicographic order. Each conversational turn t thus receives a fused VAL triplet vt , āt , lt and one consensus emotion label, which underpin the analyses in Sections III-IV.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "C. Temporal Analyses",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Markov Transition Dynamics.:",
      "text": "To analyze inter-state flow, we model discrete affect transitions as a first-order Markov chain with state space {positive, neutral, negative}, based on valence tertiles computed globally across all student turns. Transitions are computed within sessions (separated by ≥60 minutes of inactivity), with the first turn of each session excluded to avoid initialization bias. We estimate the 3 × 3 transition matrix P via frequency counts with Laplace smoothing (β = 1), and compute expected dwell time in each state as dwell(s) = 1/(1 -P ss ).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Results",
      "text": "With the overarching goal of characterizing human affective dynamics, all main-text analyses below focus on student turns only.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Descriptive Affective Landscape",
      "text": "Scalar VAL distributions (students only).: Figure  1a  overlays the ensemble-fused valence (v), arousal (a), and learning-helpfulness (ℓ) scores for all 16,986 student turns. The distributions are tightly centered: median v = 5 (IQR 4-5), median a = 5 (IQR 5-6), and median ℓ = 6 (IQR 5-6). Thus, learners experience mildly positive affect, moderate arousal, and above-neutral perceptions of learning benefit during most interactions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dominant Discrete Emotions (Students Only).:",
      "text": "The ten most frequent ensemble labels for students (Fig.  1b ) cover 97.02 % of the corpus. Three epistemic or low-polarity states dominate: NEUTRAL appears in 3,910 turns (45.8 %), followed by CONFUSION in 1,891 (22.15 %) and CURIOSITY in 1,351 (15.83 %). Strongly negative emotions are comparatively rare: FRUSTRATION surfaces in 736 turns (8.62 %) and ANXIETY in just 35 (0.41 %).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Temporal & Transition Dynamics",
      "text": "Markov state-transition structure.: Figure  2  visualizes the first-order Markov transition matrix P for student turns only. Each turn is binned into negative, neutral, or positive valence using global tertiles computed across all sessions. Self-loop probabilities on the diagonal quantify emotional inertia, the tendency for affect to persist once established. Offdiagonal entries capture cross-state shifts.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Discussion",
      "text": "Our results offer preliminary yet compelling insights into the affective contours of LLM-mediated tutoring. We interpret the findings through the lens of our research questions.\n\n(RQ1) Ensemble-derived patterns.: The scalar valence-arousal-learning (VAL) distributions show that most student turns cluster around mildly positive valence (ṽ = 5), moderate arousal (ã = 5-6), and above-neutral learning appraisal ( l = 6). However, this central tendency belies substantial emotional diversity: confusion appears in 22.15% of turns and frustration in 8.62%, underscoring that affective friction persists even in scaffolded tutoring environments. These results nuance overly optimistic views of AI tutors and suggest that affect-regulation scaffolds may still be needed.\n\n(RQ2) Affective transitions and persistence.: Our analysis reveals several notable patterns, outlined below.\n\n1. Emotional inertia in AI-student interaction. Within tutoring dialogues, learners are most likely to sustain a positive state (P pos→pos = 0.57), followed by negative (0.49), with neutral least stable (0.29). The corresponding expected dwell times 1/(1 -P ss ) are 2.33 (pos), 1.96 (neg), and 1.41 (neu) turns. This suggests that once a learner reaches a positive affective band in interaction with the AI tutor, they tend to remain there longer than in neutral or negative bands. 2. Escape from negative affect is common and often direct. Learners leave the negative band in 51% of turns (1 -0.49). Direct rebounds to positive are slightly more frequent than moves to neutral (P neg→pos = 0.29 vs. P neg→neu = 0.22), suggesting that improvements are often immediate rather than passing through neutrality. 3. Outer-state asymmetry with a neutral tipping point.\n\nTransitions between the extremes are not symmetric: P pos→neg = 0.22 is lower than P neg→pos = 0.29, consistent with greater stability in the positive band. Meanwhile, neutral behaves as a gateway with a slight positivity bias (P neu→pos = 0.38 vs. P neu→neg = 0.33) rather than a destination state (P neu→neu = 0.29).\n\nThe following section synthesizes these findings and outlines concrete directions for future work.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "V. Conclusion",
      "text": "This study presents one of the first large-scale analyses of affective dynamics in LLM-mediated tutoring. By leveraging ensemble affect annotations from three leading models: GPT-4o, Claude, and Gemini, we uncover how students' emotional trajectories unfold over 16,986 conversational turns. Our results indicate that while most learners exhibit mildly positive valence and moderate arousal, epistemic emotions like confusion and curiosity remain prevalent. Our temporal analysis shows that students' affective states in AI tutoring unfold in short bursts with rapid rebounds but fragile persistence, highlighting the need for tutor designs that provide timely scaffolds to repair negative affect and consolidate positive momentum.\n\nThe current analysis has several limitations that point to a clear agenda for future work. Most importantly, the absence of human-annotated gold data means we cannot yet determine how faithfully ensemble-derived labels reflect learners' true affective states; a stratified, human-coded reference set will be essential for validation. In addition, while three frontier LLMs were used for annotation, we did not conduct ablation studies to quantify each model's individual contribution or correctness-analyses that could guide more efficient and accurate ensemble design. Finally, our temporal modeling relied on a first-order Markov chain, which may miss longer-range dependencies and state-duration patterns; applying higherorder Markov models, hidden semi-Markov models, or neural sequence models could capture richer affective dynamics.\n\nIn closing, this work advances the conversation on how to responsibly integrate generative AI into education-not merely by improving test scores, but by attending to the emotional realities of learning.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This study is a secondary analysis of PyTutor chat logs and was approved by the Institutional Review Boards of all three partner institutions (Protocol 2405001323 -Investigating Scaling a Personalized GenAI Tutor in Intro to Computing Courses). Students provided informed, opt-in consent and received no compensation.\n\nEnsemble affect scores are proxies, not ground truth; we prohibit their use in high-stakes decisions without human review and will ship any deployed system with an opt-out toggle and visible affect-sensing indicator.\n\nAll code, prompts, and analysis notebooks are available at https://github.com/CharlieChenyuZhang/llm-ensembleaffective-tutoring under an MIT licence, though raw logs cannot be shared due to FERPA constraints. Misclassifications may mislead, so our metrics should complement-not replace-human judgment.",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: visualizes",
      "page": 3
    },
    {
      "caption": "Figure 1: Affective profile of PyTutor student turns.",
      "page": 4
    },
    {
      "caption": "Figure 2: Markov transition matrix (3 × 3) for valence tertiles",
      "page": 4
    }
  ],
  "tables": [],
  "citations": [
    {
      "citation_id": "1",
      "title": "Generative AI Can Harm Learning",
      "authors": [
        "H Bastani",
        "O Bastani",
        "A Sungu",
        "H Ge",
        "Ö Kabakcı",
        "R Mariman"
      ],
      "year": "2024",
      "venue": "Generative AI Can Harm Learning",
      "doi": "10.2139/ssrn.4895486"
    },
    {
      "citation_id": "2",
      "title": "Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task",
      "authors": [
        "N Kosmyna",
        "E Hauptmann",
        "Y Yuan",
        "J Situ"
      ],
      "venue": "Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task"
    },
    {
      "citation_id": "3",
      "title": "How brains beware: neural mechanisms of emotional attention",
      "authors": [
        "P Vuilleumier"
      ],
      "year": "2005",
      "venue": "Trends in cognitive sciences"
    },
    {
      "citation_id": "4",
      "title": "Human emotion and memory: interactions of the amygdala and hippocampal complex",
      "authors": [
        "E Phelps"
      ],
      "year": "2004",
      "venue": "Current opinion in neurobiology"
    },
    {
      "citation_id": "5",
      "title": "Emotional design in multimedia learning",
      "authors": [
        "E Um",
        "J Plass",
        "E Hayward",
        "B Homer"
      ],
      "year": "2012",
      "venue": "Journal of educational psychology"
    },
    {
      "citation_id": "6",
      "title": "How emotions affect logical reasoning: evidence from experiments with moodmanipulated participants, spider phobics, and people with exam anxiety",
      "authors": [
        "N Jung",
        "C Wranke",
        "K Hamburger",
        "M Knauff"
      ],
      "year": "2014",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "7",
      "title": "Basic dimensions for a general psychological theory: Implications for personality, social, environmental, and developmental studies",
      "authors": [
        "A Mehrabian"
      ],
      "year": "1980",
      "venue": "Basic dimensions for a general psychological theory: Implications for personality, social, environmental, and developmental studies"
    },
    {
      "citation_id": "8",
      "title": "Basic emotions",
      "authors": [
        "P Ekman",
        "T Dalgleish",
        "M Power"
      ],
      "year": "1999",
      "venue": "Basic emotions"
    },
    {
      "citation_id": "9",
      "title": "Categorical versus Dimensional Models of Affect",
      "authors": [
        "R Ellis",
        "P Zachar"
      ],
      "year": "2012",
      "venue": "Categorical versus Dimensional Models of Affect"
    },
    {
      "citation_id": "10",
      "title": "An affective model of interplay between emotions and learning: reengineering educational pedagogybuilding a learning companion",
      "authors": [
        "B Kort",
        "R Reilly",
        "R Picard"
      ],
      "year": "2001",
      "venue": "Proceedings IEEE International Conference on Advanced Learning Technologies",
      "doi": "10.1109/ICALT.2001.943850"
    },
    {
      "citation_id": "11",
      "title": "Dynamics of affective states during complex learning",
      "authors": [
        "S Mello",
        "A Graesser"
      ],
      "year": "2012",
      "venue": "Learning and Instruction"
    },
    {
      "citation_id": "12",
      "title": "Eliciting mixed emotions: A meta-analysis comparing models, types, and measures",
      "authors": [
        "R Berrios",
        "P Totterdell",
        "S Kellett"
      ],
      "year": "2015",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "13",
      "title": "Bias and fairness in large language models: A survey",
      "authors": [
        "I Gallegos"
      ],
      "year": "2024",
      "venue": "Computational Linguistics"
    },
    {
      "citation_id": "14",
      "title": "PyTutor: Empowering Equitable Education Pathways in Computing with Generative AI,\" media.mit.edu",
      "authors": [
        "Lab Mit Media"
      ],
      "year": "2025",
      "venue": "PyTutor: Empowering Equitable Education Pathways in Computing with Generative AI,\" media.mit.edu"
    },
    {
      "citation_id": "15",
      "title": "Gemini: A Family of Highly Capable Multimodal Models",
      "authors": [
        "G Team"
      ],
      "year": "2025",
      "venue": "Gemini: A Family of Highly Capable Multimodal Models",
      "doi": "10.48550/arXiv.2312.11805",
      "arxiv": "arXiv:arXiv:2312.11805"
    },
    {
      "citation_id": "16",
      "title": "GPT-4o System Card",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": "GPT-4o System Card",
      "doi": "10.48550/arXiv.2410.21276",
      "arxiv": "arXiv:arXiv:2410.21276"
    },
    {
      "citation_id": "17",
      "title": "The Claude 3 Model Family: Opus, Sonnet, Haiku (Model Card)",
      "authors": [
        "Anthropic"
      ],
      "year": "2024",
      "venue": "The Claude 3 Model Family: Opus, Sonnet, Haiku (Model Card)"
    },
    {
      "citation_id": "18",
      "title": "The effect of ChatGPT on students' learning performance, learning perception, and higher-order thinking: insights from a meta-analysis",
      "authors": [
        "J Wang",
        "W Fan"
      ],
      "year": "2025",
      "venue": "Humanit Soc Sci Commun",
      "doi": "10.1057/s41599-025-04787-y"
    },
    {
      "citation_id": "19",
      "title": "Weighted kappa: Nominal scale agreement provision for scaled disagreement or partial credit",
      "authors": [
        "J Cohen"
      ],
      "year": "1968",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "20",
      "title": "Intraclass correlations: uses in assessing rater reliability",
      "authors": [
        "P Shrout",
        "J Fleiss"
      ],
      "year": "1979",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "21",
      "title": "Measuring nominal scale agreement among many raters",
      "authors": [
        "J Fleiss"
      ],
      "year": "1971",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "22",
      "title": "Computing Krippendorff's alpha-reliability",
      "authors": [
        "K Krippendorff"
      ],
      "year": "2011",
      "venue": "Computing Krippendorff's alpha-reliability"
    }
  ]
}