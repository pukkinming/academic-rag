{
  "paper_id": "2005.04618v1",
  "title": "Mombat: Heart Rate Monitoring From Face Video Using Pulse Modeling And Bayesian Tracking",
  "published": "2020-05-10T09:41:16Z",
  "authors": [
    "Puneet Gupta",
    "Brojeshwar Bhowmick",
    "Arpan Pal"
  ],
  "keywords": [
    "Heart rate monitoring",
    "Face video",
    "Remote PPG",
    "Heart rate tracking",
    "Pulse modeling"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "A non-invasive yet inexpensive method for heart rate (HR) monitoring is of great importance in many real-world applications including healthcare, psychology understanding, affective computing and biometrics. Face videos are currently utilized for such HR monitoring, but unfortunately this can lead to errors due to the noise introduced by facial expressions, out-of-plane movements, camera parameters (like focus change) and environmental factors. We alleviate these issues by proposing a novel face video based HR monitoring method M OM BAT , that is, MOnitoring using Modeling and BAyesian Tracking. We utilize out-of-plane face movements to define a novel quality estimation mechanism. Subsequently, we introduce a Fourier basis based modeling to reconstruct the cardiovascular pulse signal at the locations containing the poor quality, that is, the locations affected by out-of-plane face movements. Furthermore, we design a Bayesian decision theory based HR tracking mechanism to rectify the spurious HR estimates. Experimental results reveal that our proposed method, M OM BAT outperforms state-of-the-art HR monitoring methods and performs HR monitoring with an average absolute error of 1.329 beats per minute and the Pearson correlation between estimated and actual heart rate is 0.9746. Moreover, it demonstrates that HR monitoring is significantly improved by incorporating the pulse modeling and HR tracking.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Heart rate (HR) is given by the total number of times a heart contracts or beats per minute. It can assess the human pathological and physiological parameters  [30] , thus it has attracted the fields of: i) healthcare; ii) psychology understanding of stress and mental state; iii) affective computing for understanding human emotion; and iv) biometrics for liveness and spoof detection. These fields can be benefited if HR monitoring is accurate and acquired from an inexpensive sensor in a user-friendly and non-contact manner. This motivates us to propose an accurate HR monitoring method using face videos in this paper.\n\nHR can be measured using contact or non-contact mechanisms. Contact mechanisms require the sensors like electrocardiography (ECG) or photo-plethysmography (PPG). They can mitigate illumination artifacts and provide synchronized multi-modal physiological data, but they should be properly placed on the body. In real-world scenarios, motion can change the contact area between the sensor and human skin, which eventually results in spurious HR monitoring  [55] . Since these methods require the contact between the user and the sensor for large duration, they restrict unobtrusive monitoring and require a dedicated sensor for single user monitoring. Also, maintaining the sensor contact is cumbersome for: i) neonates surveillance; ii) analyzing sleep quality; iii) exercise monitoring during rehabilitation etc.  [43]    [9]  ; and iv) observing skin damaged patients. These issues can be handled by performing HR monitoring using non-contact mechanisms, which allow the monitoring anytime and anywhere with minimal user involvement. These non-contact mechanisms can also be used for covert monitoring and thereby utilized for sleep monitoring, lie detection  [31]  and stress monitoring  [41] . Due to these advantages, non-contact based HR monitoring is proliferating.\n\nTraditional non-contact mechanisms require bulky, expensive and dedicated sensors like Microwave Doppler and laser for HR monitoring  [23] . Modern non-contact mechanisms employ inexpensive and portable camera sensors for HR estimation. They are based on the phenomenon that heart beats generate the cardiovascular pulse which propagates in the entire human body. It introduces color variations in the reflected light  [14]  and micro-movements in the face  [1] . Both these contain the cardiovascular pulse information and are imperceptible to the human eye, but they can be analyzed using the camera for estimating the HR.\n\nExisting face based HR methods analyze the micro-motion or color variations across time and refer to them as temporal signals  [14] . The cardiovascular pulse is estimated from the temporal signals and it is eventually used for HR estimation. Along with the subtle pulse signal, the temporal signal constitutes prominent noise originated from: i) facial expression; ii) eye blinking; iii) face movements; iv) respiration; v) camera parameters (for example, change in focus); and vi) environmental factors (for example, illumination variations). Extraction of HR signal from such a noisy temporal signal is thus a challenging problem. In this paper, we alleviate these issues to improve the face videos based HR monitoring by introducing a novel method M OM BAT , that is, MOnitoring using Modeling and BAyesian Tracking. The main research contributions of our proposed method, M OM BAT are:i) it introduces a novel quality estimation mechanism that adapts according to the out-of-plane face movements and provide quality of each frame, unlike existing mechanisms that provide single quality for the entire video; ii) it initiates the utilization of Fourier basis based pulse modeling for reconstructing the pulse signals at the poor quality video frames using the pulse signals at the good quality video frames; and iii) it presents a novel Bayesian decision framework for rectifying the spurious HR estimates.\n\nThe paper is organized as follows.The preliminaries required for better understanding of our method, M OM BAT are discussed in Section 2 and M OM BAT is presented in Section 3. The experimental results are analyzed in Section 4 followed by conclusions in the last section.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preliminaries",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Face Hr Estimation",
      "text": "Typically, any face videos based HR estimation method consists of the following three stages; preprocessing, HR estimation, and post-processing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Preprocessing",
      "text": "During preprocessing, a region of interest (ROI) containing useful pulse information is detected. Skin pixels contain pulse information, thus face detection followed by removing non-skin pixels are performed for ROI extraction  [37] . Usually, a face is detected using Viola-Jones  [49]  or model based  [3]  face detectors. Subsequently, non-skin pixels due to background and hairs, are removed by applying skin color discrimination techniques. Inevitable movements (like eye blinking) near the eye areas can degrade the HR estimation  [15] . Thus, the eye areas are detected by employing facial geometry heuristics or trained classifiers  [51]  and then these eye areas are removed for the better estimation. The remaining face area is used to define the region of interest (ROI). Some commonly used ROI are full face, forehead region or cheek areas  [37] . ROI locations can be shifted by the facial movements in z-direction known as out-of-plane transformations or movements in x and y-dimensions known as in-plane transformations. Both these transformations can result in spurious HR estimation due to ROI shifting. Hence, these transformations are minimized using face registration for improving the HR estimation  [17] . One can use mobile based 3D depth estimation also to get the depth of landmark  [6]  [28] for compensating out-of-plane movements.\n\nWe use simple distance between the eyes is used by  [17]  for the registration, but it can be spurious due to eye-blinking. These transformations can be accurately measured by wearables  [7] , but it requires human contact and thus, avoided for non-contact face video based HR.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Temporal Signal Extraction",
      "text": "Micro-motion and subtle color variations in the face video can be determined using Lagrangian  [1]  and Eulerian techniques  [34]  respectively.These variations across different frames provide temporal signals.In Lagrangian techniques, discriminating features are extracted from the ROI and they are explicitly tracked in the subsequent frames for determining the temporal signals  [45] . This tracking is not only time-consuming, but also spurious due to improper illumination. Alternatively, temporal signals can be determined using Eulerian techniques, where color variations are examined in the fixed ROI  [34] . The Eulerian techniques are less time-consuming than the Lagrangian tech-niques, but they are applicable only when small variations are present  [53] . It requires fixed ROI and hence, altered tremendously even if the face is slightly moved  [16] .\n\nEulerian temporal signals are given by the color variations in the face video, having RGB color channels. Amongst these channels, the green channel contains the strongest photo-plethysmographic signal because: i) haemoglobin absorbs green light better than red, which makes green light less susceptible to motion noise as compared to red light; and ii) green light penetrates sufficiently deeper into the skin as compared to blue light  [48] . It is apparent that better performance can be expected by fusing all RGB color channels. Model based methods utilize optical and physiological properties of skin reflection to perform such a fusion. Unfortunately, such methods are not applicable in all possible scenarios. For example, well known model based methods, CHROM  [11]  and POS  [52]  do not provide correct HR estimation when pulse signal and noise share similar amplitudes  [52] . Furthermore, POS fails when face videos are acquired under inhomogeneous illumination conditions, that is, when faces are illuminated by multiple light sources  [52] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Hr Estimation",
      "text": "Pulse signal is estimated from the temporal signal using statistical learning. As an instance, periodicity analysis and blind source separation (BSS) techniques are used for the pulse signal estimation by [29] and  [17]  respectively. Usually, Fast Fourier Transform (FFT) is applied to the pulse signal and the frequency corresponding to the maximum amplitude in the pulse spectrum corresponds to the HR  [8] . But when the temporal signal is contaminated with noise, several spurious peaks are generated and the actual HR may not correspond to the maximum amplitude peak. An example is shown in Figure  1  where several spurious peaks are generated due to facial movements.\n\nSeveral filtering techniques can be employed to remove the noise in the temporal signals and thereby improve HR estimation. For example, Detrending filter is applied to alleviate the non-stationary trend in the pulse signal  [46] . Some spectrum subtraction methods that mitigate the noise from the pulse signal are proposed by  [22, 27] ; and  [26] . The noise due to motion artifact is estimated by  [22]  using facial boundary tracking. Such tracking is spurious due to facial pose variations. Similarly, background variations and brightness are estimated by [27] and  [26]  respectively, for estimating the noise due to illumination variations. But they are highly dependent on the background characteristics and the distance between the background and face  [25] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Hr Monitoring",
      "text": "HR monitoring continuously performs HR estimations at different small time intervals and, usually, facial deformations affect a small number of frames. Due to this, some HR estimates in the monitoring can be spurious due to the inevitable facial deformations. Better estimation can be expected when a large number of frames are considered  [47] . But it results in the loss of HR variations which is highly useful for medical purposes  [5] . Furthermore, it restricts user-friendliness due to high wait time.\n\nTypically, the number of frames in a time interval is chosen such that the cardiovascular pulse wave can complete at least two cycles. HR monitoring is performed by  [39]  using green channel variations and band-pass filtering. Likewise, methods  [47]  and  [35]  perform HR monitoring using matrix completion and convolution neural network (CNN) respectively. Erroneous HR estimates are rectified by  [17]  to improve the HR monitoring using image registration and global HR. The global HR is estimated from all the video frames and thus, it can be spurious when temporal signals contain noise.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Constrained Local Neural Field (Clnf)",
      "text": "Detecting the discriminatory facial features is an extensively studied research topic.\n\nThese are referred to as facial landmark points  [50] . Usually, they are located around face boundaries, eyes, eyebrows, mouth and nose. Constrained Local Model (CLM) is highly useful for landmark detection. It consists of: i) point distribution model (PDM) that uses rigid and non-rigid shape transformations for modelling the global location of discriminatory points; ii) patch experts which models the behaviour of a landmark by analysing the appearance around its local neighbourhood; and iii) joint optimization which aims to fit PDM and the experts in the best possible way  [42] . The unknown shape parameter p is estimated by the joint optimization, which is given by\n\nwhere R is the regularization term which restricts the introduction of unlikely shapes; D i is the misalignment in the location of i th landmark in the image I; and x i is the i th landmark location in 3-D which is given by\n\nwhere xi denotes the mean value of i th feature given by PDM; φ i is the component matrix; and vector q is used to control the non-rigid shape  [2] . Remaining parameters scaling s, translation t and rotation R controls the rigid shape. In essence, shape parameters are given by p = [s, t, R, q]. The performance of CLM heavily relies on PDM, patch expert and joint optimization. In CLNF  [3] , patch experts are given by local neural field for modeling spatial relationships between pixels, while non-uniform regularized landmark mean-shift is proposed for joint optimization by taking into account the reliability of patch experts.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Pulse Extraction Using Kurtosis Optimization",
      "text": "Each temporal signal contains a pulse signal along with the noise. In case of multiple temporal signals, the pulse signal is extracted using blind source separation by estimating the individual source components  [34] . Amplitudes of pulse signal and noise in the temporal signals depend on the facial structure, user characteristics (like skin color) and environmental settings (like illumination). Hence, z-score normalization  [40]  is applied to normalize the temporal signals. Moreover, the temporal signal, F i contains noise, η and actual pulse signal, X a but modified by the facial structure.\n\nThat is,\n\nwhere n and A denote the time instant and matrix incorporating the effects of facial structure respectively. Further, the actual pulse signal is not known and it requires estimation from the temporal signal, that is,\n\nwhere X e and B denote the estimated actual pulse and the transformation matrix respectively. It can be observed from Equations (  3 ) and (  4 ) that:\n\nsuch that T = BA and η = Bη.\n\nFor accurate HR monitoring, X e should be similar to X a . That is, magnitude of T should be 1 and appropriate shape constraints should be imposed on the estimated pulse spectrum. Such shape constraints are imposed using higher order cumulants  [38] . The highest order of cumulant is restricted to 4 because higher-order cumulants are easily affected by the tail of the distribution which makes them sensitive to outliers and they are slightly independent in the middle of the distribution containing useful information  [24] . It is proved in [32] that constraints on cumulant similarities till fourth order can be fulfilled by defining the objective function as:\n\nwhere",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Quality Estimation Of Pulse Signal",
      "text": "Quality of pulse signal can be estimated using the peak signal to noise ratio (PSNR)  [54] . Typically, the amplitude of the pulse spectrum obtained after converting the pulse signal into the frequency domain, should contain a peak at the HR frequency and negligible values at other frequencies. Unfortunately, in the pulse spectrum, the noise increases the amplitude at other frequencies. Thus, PSNR can be defined such that the signal can be interpreted as the amplitudes corresponding to HR frequency while noise can be thought of the amplitudes at the remaining frequencies. Mathematically, the quality given by PSNR, q is given by:\n\nwhere S p denotes the spectrum of the estimated pulse signal; sum performs the sum over all the frequencies; n p represent the neighbourhood size; and maxLoc returns the position containing the maximum value (thus, the location of HR frequency is given by maxLoc(S p )). In equation  (7) , signal (or numerator) is obtained by adding the amplitude of HR frequency and its few neighbourhoods while noise (or denominator) is obtained by adding the amplitude of the remaining frequencies.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Proposed Method",
      "text": "Our proposed face based HR monitoring method, M OM BAT is presented in this section. It consists of the following three stages: window extraction, window analysis and HR tracking. In the first stage, we divide the face video into several overlapping windows. In the next stage, we estimate the cardiovascular pulse and quality for each window. Subsequently, we introduce pulse signal modeling to obtain better HR estimates. In the last stage, we propose Bayesian tracking to improve the HR monitoring.\n\nFigure  2  illustrates the flow-graph of the proposed method, M OM BAT .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Window Extraction",
      "text": "HR monitoring requires the estimation of multiple HR at various time intervals and eventually concatenation of all HR estimates. Hence, just like the existing HR monitoring methods, we divide the face video into multiple overlapping windows  [17] .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Window Analysis",
      "text": "In this section, we analyze each extracted window to estimate the corresponding cardiovascular pulse, HR and quality. Initially, we detect ROIs from the window and mitigate in-plane face movements. Then, we extract the cardiovascular pulse from the ROIs using Eulerian technique followed by FFT based analysis. Subsequently, we estimate the quality of the pulse according to their out-of-plane deformations and utilize it to rectify the pulse using pulse modeling.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Roi Detection",
      "text": "The facial skin area contains useful pulse information, hence we utilize it to define ROI. Initially, we detect the facial areas and landmarks using Constrained Local Neural Field (CLNF) model proposed by  [4] . HR can be spurious when non-skin pixels (like beard) and eye areas are utilized for HR estimation  [14] . Thus, we detect these areas and remove them. We utilize skin detection proposed by  [33]  to detect the non-skin pixels and we obtain the eye area by the convex hull of facial landmarks corresponding to the eyes and eyebrows. Furthermore, subtle motion in facial boundaries can significantly alter the temporal signals and thereby result in spurious HR. Hence, we remove the boundary pixels by performing morphological erosion  [18] . Figure  3  illustrates these steps.\n\nThe translation and rotation of face in x and y-dimensions, known as in-plane transformations, can shift the location of the ROI in subsequent frames and thereby alter the Eulerian temporal signals and results in the spurious HR estimation. It motivates us to perform image registration between subsequent frames, so as to mitigate the in-plane transformations. An example depicting the applicability of image registration is shown in Figure  4 . It shows the pulse spectrum before and after applying the image registration in Figures  4(a ) and 4(b) respectively. It can be observed that HR can be correctly estimated after employing image registration. We perform the registration between subsequent frames by minimizing the deviation between nose landmark points because nose area is least affected by the facial expressions. Figure  3(b)  shows the chosen landmark points in blue color. Mathematically, we first estimate the transformation matrix, T between the current and previous frames using:\n\nwhere M i and F i denote the positions of i th nose landmark point in current and pre-vious video frames respectively; q is the total number of nose landmark points; and T is the transformation matrix consisting of translations and rotation in 2-D, that is:\n\nwhere θ is the rotation angle while t x and t y are the translations in x and y directions respectively. It is important to note that M i and F i denotes the feature points positions in homogeneous coordinates, that is, the feature at\n\nWe utilize Gradient Descent optimization to solve the Equation (  8 )  [18] . The in-plane transformation is minimized by registering the current image using:\n\nwhere R is the registered image and I M is the current video frame. Thereafter, facial expressions can also result in spurious HR estimation. It can be mitigated by considering several face areas as different ROIs rather than considering full face as one ROI  [14] . Hence, we utilize the method proposed by  [14]  for ROI extraction. For brevity, it divides the resultant registered face area into non-overlapping square blocks and considers them as ROIs. Also, it chooses the block-size such that the detected area should contain 10 blocks in the horizontal direction. An example is shown in Figure  3 (e).",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Pulse Extraction",
      "text": "We estimate the cardiovascular pulse using the method proposed by  [14] . For brevity, it first extracts the Eulerian temporal signals from each ROI using the variations introduced in the average green channel intensities because the green channel contains the strongest plethysmographic signal amongst RGB color channels. Mathematically, the temporal signal S i corresponding to i th ROI is given by:\n\nwhere f is the total number of frames and s i k representing the variations in k th frame for i th ROI is given by:\n\nwhere B i represents the i th ROI; (x, y) denotes a pixel location; and F g k stores green channel intensities in k th frame. The extracted temporal signals contain noise which is mitigated by utilizing a band-pass filter and a Detrending filter  [14] . The cardiovascular pulse, Xe is eventually extracted by applying the kurtosis based optimization proposed in  [14] .",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Quality Estimation",
      "text": "Just like in-plane deformations, out-of-plane deformations caused by facial movements in z-direction, can shift the ROI and result in the spurious HR estimation. We introduce a novel quality measure which incorporates these out-of-plane movements to measure the confidence in the correct estimation of pulse signal at each frame. It is defined using the 3-D facial landmarks that we have detected by applying Constrained Local Neural Field (CLNF) model  [4]  in Section 3.2.1. Amongst these, we utilize only the 3-D facial landmark points corresponding to the face boundary for detecting the out-of-plane movements because the face boundary is highly affected by the motion in the z-direction. These selected landmark points are shown in red color in Figure  3(b) .\n\nOut of these chosen points, the points containing the largest deviation in z-direction are used for the quality estimation. It can be observed that yaw head motion can move some boundary points in positive and some in negative z-directions, thus we evaluate the deviation using maximum absolute change in z-direction. In essence, the deviation in the k th frame, d (k-1) is given by:\n\nwhere max is the maximum operator; |•| is the absolute operator; while l j k and l j\n\ndenote the z-coordinate of j th landmark in k th and (k -1) th frames respectively. After evaluating the deviations for all the frames, except the last frame, we compute the quality at (k -1) th frame, q(k-1) using:\n\nwhere min is the minimum operator and d stores all the computed deviations, that is where f is the number of frames. In essence, d (k-1) in Equation (  14 ) is first normalized to [0, 1] and then modified to define quality such that low and high deviations corresponds to high and low quality values respectively. Thus, the quality due to outof-plane movements, Q is given by:\n\nAn example of the quality estimation using out-of-plane movements is shown in Figure  5 (a).",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Pulse Modeling",
      "text": "The frames affected by out-of-plane movements provide spurious temporal signals and thereby results in an incorrect estimation of cardiovascular pulse, Xe . An example of such pulse is shown in Figure  5 (b) along with its corresponding pulse spectrum in Figure  5 (c). It can be observed from Figure  5 (c) that the predicted HR is deviated significantly from the actual HR. To improve the efficacy of the pulse signal, we introduce Fourier basis based modeling that aims to reconstruct the pulse signal at those frames which are affected by out-of-plane movements. We formulate the problem of noise reduction as a data fitting problem  [20] . It consists of the following steps: i) defining appropriate basis functions; ii) parameter fitting; and iii) signal reconstruction. Mathematically, Xe can be decomposed as:\n\nwhere α is the number of basis; a i denotes the model parameter for i th parameter; x denote the frame number; and φ i (x) is the i th basis function. Parameter α plays a crucial role in the modeling. Pulse reconstruction is spurious when α is small and if it is set to high value, then even noise can be modeled. We describe the parameter selection of α in Section 4.3. For simplicity, Equation (  17 ) can be written in matrix form using:\n\nwhere\n\nand\n\nWe define basis functions, Φ, using the well known Fourier basis  [20] . It is because i) these basis are orthogonal which is required to provide stability in the optimization by assuring low residual error; and ii) their amplitude lies in the range of [-1, 1] which helps in avoiding the problem of overflowing integer with polynomial basis. The Fourier basis for the order n are given by:\n\nThis represents an overdetermined system of linear equations because the small number of unknown parameters, α needs to be estimated from a large number of observations, (f -1). Furthermore, we aim to reconstruct the pulse at the frames containing large out-of-plane movements by utilizing the pulse information at the frames containing small out-of-plane movements. Thus, we solve this overdetermined system of linear equations using weighted least square estimation where weights are given by quality due to out-of-plane movements, Q  [20] . That is,\n\nwhere Â contains the estimated modeling parameters; • contains the norm; and Q (solved in Equation (  16 )) is the quality due to out-of-plane movements. The solution of Equation (  22 ) is given by:\n\nwhere Q is the diagonal matrix formed from Q in the following manner:\n\nModeled pulse signal, Xe is obtained by:\n\nAn example of the modeled pulse signal is shown in Figure  5 (d) along with its corresponding pulse spectrum in Figure  5 (e). It can be observed from the Figures  5(c ) and 5(e) that the HR estimation can be improved significantly after incorporating the proposed pulse modeling.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Hr Tracking",
      "text": "The spectrum of noise-free pulse signal should contain maximum amplitude at the HR frequency, but it is violated when the pulse signal contains noise. The modeled pulse signal obtained after applying the proposed pulse modeling technique, contains noise and thus, it can provide spurious HR. Usually, small HR change is observed between subsequent HR estimates. It motivates us to introduce the Bayesian framework which rectifies the spurious HR estimates. The framework consolidates the likelihood information derived from the current window and the prior information related to previously analyzed windows  [13] .\n\nTo leverage the observation that there is a small HR change between subsequent HR estimates, we want to define our prior information such that the large confidence value is provided when the fluctuation between the current HR and previous HR is small while a small confidence value is provided when the fluctuation is large. Furthermore, we need to provide low prior information about the previous HR whenever they are spurious, otherwise the error can be propagated in the subsequent HR estimates.\n\nAn important characteristic of spurious HR is that its corresponding pulse spectrum contains multiple peaks, thus the spectrum has low PSNR  [54] . These conditions are met by defining the prior information for (i + 1) th window using:\n\nwhere h i and γ i denote the HR frequency and PSNR of the modeled pulse spectrum in the previous window (that is, i th window) respectively; c is a predefined constant;\n\nθ is the set of all probable HR frequencies; and N denote the normal distribution. It can be observed that the normal distribution is used in Equation (  26 ) such that mean and variance are given by h i and c γi respectively. Thus, small PSNR results in high variance which in turn results in low prior knowledge. Also, it is suggested by  [55]  that the fluctuations between subsequent HR estimates usually lie within the range of -11bpm to +11bpm. Thus, we set c equal to 4, so that 3 times of the variance covers most of our permissible HR estimates.\n\nOur definition of prior information in Equation (  26 ) prohibits large fluctuation from the previous HR. Hence, if previous HR is spurious with low PSNR value, then the current HR values should be restricted with large range. But one should consider all the plausible HR frequencies when previous HR is spurious. To incorporate this intuition, we add a constant value in all the plausible HR frequencies, which are lying between 0.7 to 4Hz. That is, we modify the prior information using:\n\nwhere P e denotes the modified prior information; P g is the distribution described in Equation (  26 ); and P u is given by:\n\nIn essence, P u is the uniform distribution, defined in the HR frequency ranges of 0.7 to 4 Hz such that any frequency is equally probable with the value of ĉ. We describe the parameter selection of ĉ in Section 4. Furthermore, when the first window is analyzed P g is set to zero for all the possible HR frequency ranges, so that all the values are equally likely and hence no useful prior information is utilized.\n\nThe likelihood function is denoted by P l (S i+1 |θ) where S i+1 denotes the spectrum of reconstructed pulse signal Xe corresponding to the (i + 1) th window. We estimate it using:\n\nThe posterior probability, P p (θ|S i+1 ) is evaluated by applying the Bayes rule  [13] ,\n\nthat is,\n\nwhere P (S i+1 ) is the evidence factor. Equations (  27 ), (  29 ) and (  30 ) can be combined in the following manner:\n\nwhere Z 2 is a normalization coefficient given by:\n\nAn illustration of the prior information, likelihood function and their corresponding posterior probability is shown in Figure  6 . We apply maximum a posteriori estimation for HR frequency estimation which provides the minimum-error-rate classifier based on zero-one loss function  [13] . For brevity, the expected loss incurred on selecting a particular frequency, d is given by:\n\nwhere R is the incurred loss and L represent the loss function given by:\n\nFurther, it is obvious that the sum of likelihood function at all the possible values (which lies between 0.7 to 4Hz in our case) will be equal to one, that is,\n\nIt can be seen by combining Equations (  33 ), (  34 ) and (  35 ) that:\n\nHence, expected loss, R is minimized when θ is set to the value that maximizes the posterior probability P p , that is, θ is set to HR frequency. Hence, we obtain the HR frequency corresponding to (i + 1) th window, h i+1 using:\n\nThe corresponding HR is given by:\n\nwhere round operator rounds off the value to the nearest integer. Some examples depicting the usefulness of the proposed HR tracking are shown in Figure  7 . The figure depicts the actual HR monitoring along with the predicted HR monitoring when the proposed HR tracking is avoided and utilized. It demonstrates that the HR monitoring can be improved significantly when the proposed HR tracking is used.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Experimental Results",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Data Recording Of Our Dataset",
      "text": "The performance of our method, M OM BAT is evaluated on Intel i5-2400 CPU  training) and the remaining 50 videos are used for performance evaluation (or testing).\n\nThe videos are acquired from Logitech webcam C270 camera which is mounted on a laptop and the subjects are free to perform natural facial movements and head pose variations. The resolution of these acquired videos is 640×480 pixels. Furthermore, we avoid any compression mechanism and save the videos in AVI raw format. These are acquired for 1 minute at 30 frames per second. The ground truth is obtained by simultaneously acquiring the actual pulse from the right index fingertip using CMS 50D+ pulse oximeter. The percentage of distribution of ground truth HR estimation from the acquired database is shown in Figure  8 .",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Performance Measurement",
      "text": "The performance metrics used in our experiments are based on the predicted predicted HR error, P (i, j) -Ā (i, j) , where P (i, j) and Ā (i, j) denote the predicted and actual HR estimates respectively for i th subject in j th window. Accurate HR monitoring method requires that the prediction error is close to zero, alternatively, the mean µ and standard deviation σ of the prediction error should be close to zero. Likewise, the percentage of samples with absolute error less than 5 bpm, err 5 should be close to 100% for correct HR monitoring. Another metric employed for the evaluation is mean average error, M AE of all the subjects which is given by:\n\nwhere |•| is the absolute operator; n i represents the number of windows for i th subject;\n\nand z is the total number of subjects. Lower value of M AE indicates that the predicted and estimated HR estimates are close to each other. Similarly, we also use total time, t s required for HR monitoring in seconds as a performance metric. Furthermore, we used the Pearson correlation coefficient, ρ to evaluate the similarity between two variables in terms of linear relationship. It lies between -1 to 1 and is given by:\n\nwhere cov and σ are the covariance and standard deviation operator respectively. Better HR estimation requires high similarity between the predicted and actual HR, that is, high ρ.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Parameter Selection",
      "text": "Our proposed method M OM BAT requires proper selection of four parameters, which are: i) window size; ii) overlapping window size; iii) α representing the number of basis in pulse modeling; and iv) ĉ which is required for defining prior in HR tracking (refer Equation (28)). Since minimum possible heart beats can be 42bpm and the proper window should contain at least two cycles of the cardiovascular pulse, we selected the window of 4 second video where slightly more than two cycles can be observed. Similarly, overlap between the successive windows is chosen in an application specific manner. We focus on frequently updating the previous HR from the new HR, typically, twice in a second. Hence, we set the overlap between successive windows at We set α and ĉ equal to 50 and 0.4 respectively, where minimum M AE is attained on the training set. 1 NA means pulse modeling is avoided. Otherwise, the basis name is mentioned.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Performance Evaluation",
      "text": "For rigorous performance analysis, we create several other methods from our proposed method, M OM BAT by avoiding or replacing its components. The following methods are considered for the performance analysis: a) N orSysI which is obtained by avoiding image registration, pulse modeling and HR tracking in M OM BAT ; b)\n\nN orSysR which is same as N orSysI except that it uses red light instead of green light for extracting the temporal signals; c) N orSys which is obtained by avoiding pulse modeling and HR tracking in M OM BAT ; d) P ulseM odP and P ulseM odL which avoid HR tracking, but utilize polynomial and Legendre basis  [19]  respectively, instead of Fourier basis for pulse modeling in M OM BAT ; e) P ulseM od which is given by considering proposed (or Fourier basis based) pulse modeling, but avoiding HR tracking in M OM BAT ; and f) BayT rack which is given by avoiding pulse modeling, but considering HR tracking from M OM BAT . The description of these subversions of M OM BAT is provided in Table  1 . Furthermore, we compare our method with the following existing well known methods:  [1] ; CHROM  [11] ; POS  [52] ;  [22] ;\n\n[47];  [39] ;  [35] ; and  [17] . Pulse signal is extracted from the Lagrangian temporal signals using Principal Component Analysis in  [1] .  [22]  registers the face and utilizes Eulerian temporal signals. Model based methods are utilized in  [11]  and  [52]  where temporal signals are extracted by fusing RGB color channels. Optical and physiological properties of skin reflection are used to perform such a fusion. Methods  [1] ,  [22] ,\n\n[11] and  [52]  provide one HR value. To conform these methods with M OM BAT , we extract the window and then analyze each window using these methods for HR monitoring. We are unable to conduct the comparative analysis with [27] for HR monitoring because it requires large window size as described in  [47] . In  [35] , CNN trained on several windows is used for HR monitoring. The training and test sets contain different windows of the same subjects. For more rigorous analysis, we also perform the experimentation with another method M odCN N where  [35]  is used, except that its training and testing sets do not contain windows of the same subjects.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Performance Analysis On Our Dataset",
      "text": "Our experimental results on our dataset are presented in Table  2 . It can be inferred from the table that  [1]  provides the most spurious HR monitoring because it requires the tracking of facial features, which is easily affected by expressions. Likewise,  [22]  exhibits lower performance than the other methods except  [1]  because it averages all the temporal signals for pulse extraction. This is error-prone because large noise in few temporal signals due to facial expressions, can tremendously affect the cardiovascular pulse after averaging. Both  [1]  and  [22]  employ highly time consuming feature tracking and BSS. In contrast,  [39]  extracts the pulse signal using only green channel intensity differences of full face and avoiding computationally expensive BSS step and feature tracking. It enables  [39]  to perform in the most computationally efficient manner, but such a method performs spuriously because it is easily affected by facial deformation, as shown in  [17] .\n\nN orSysR and N orSysI are different only in the way that they utilize red and green light respectively for the temporal signal extraction. It can be observed from Table 2 that N orSysI performs better than N orSysR, which indicates that green light is more effective in photo-plethysmographic imaging than red light. This observation is also mentioned in  [48] . Furthermore, N orSysI performs better than CHROM  [11]  and POS  [52] , which utilize optical and physiological properties of skin reflection to con- POS do not provide correct HR estimation when pulse signal and noise share similar amplitudes  [52] . In addition, POS fails when face videos are illuminated by multiple light sources  [52] .\n\nN orSysI is the same as N orSys except that N orSysI avoids image registration and Table  2  points out that N orSys performs better than N orSysI. It indicates that performance can be increased by utilizing image registration. Average computational time of N orSysI and N orSys are 6.8 second and 6.84 second, respectively, out of which, BSS is the most computationally expensive step requiring 5.13 seconds.\n\nMethod N orSys also performs better than  [47]  due to better ROI selection, image registration and proper BSS technique.  [47]  utilizes matrix completion to mitigate the noise, which increases the computation time significantly. Also, it can be observed from the table that P ulseM odP , P ulseM odL, P ulseM od and BayT rack perform better HR monitoring than N orSys. P ulseM odP , P ulseM odL and P ulseM od mitigate the problems of out-of-plane movements in N orSys by modeling the pulse signal (refer Figure  5  for example) and in return, they incur an additional average time of 0.31 and 2.48 sec for the modeling and quality estimation respectively. P ulseM od performs better than P ulseM odL and P ulseM odP which demonstrate that Fourier basis is better suited for pulse signal modeling. Similarly, BayT rack performs better monitoring than N orSys because it rectifies the HR estimates by incorporating the prior knowledge of the HR estimates. Some of its examples are shown in Figure  7 . It incurs an additional average time of 0.22 sec than N orSys due to PSNR estimation.\n\nTable  2  indicates that  [35]  exhibits good HR monitoring when the training and test sets contain different windows of the same subjects. But when the training and testing sets do not contain windows of the same subjects (that is, M odCN N ) then there is significant performance degradation. It points out that CNN employed by  [35] , leverages the facial texture and skin color for HR monitoring. This is obviously a wrong way of performing HR monitoring. Likewise,  [17]  relying on only face reconstruction is incompetent to handle out-of-plane movements and hence, provide spurious HR estimates. But our method, M OM BAT handles most of the spurious cases by utilizing the pulse modeling and Bayesian tracking. Furthermore, it provides the best HR",
      "page_start": 24,
      "page_end": 26
    },
    {
      "section_name": "Performance Analysis On Cohface",
      "text": "One major factor that hampers the progress in this realm of HR analysis using face video is the lack of appropriate datasets  [10] . It is stated by  [10]  that several existing publicly available datasets that are extensively used in the literature, are not appropriate for HR estimation using face videos. One such example is MAHNOB-HCI dataset  [44]  which involves negligible illumination variations induced by the movie and thus, unable to cater complex real-world scenarios. COHFACE dataset is regarded as a more challenging dataset to cater more realistic conditions than MAHNOB-HCI by  [21] . Thus, we have conducted our experiments on COHFACE dataset as well. But it lacks significant motion variations and thus, we create and conduct experiments on our dataset for better evaluation of our proposed method, M OM BAT .\n\nThe COHFACE dataset contains 160 face videos acquired from 40 subjects. Experiments are conducted on this dataset using the performance metrics, parameter selection and methods described in Section 4.2, Section 4.3 and Section 4.4 respectively.\n\nThe corresponding results are shown in Table  3 . It can be observed that these results are similar to the results on our dataset, that is, M OM BAT performs best amongst the considered methods. Furthermore, it can be observed that the efficacy of M OM BAT reduces slightly when COHFACE dataset is considered rather than our dataset. It is because the COHFACE dataset contains compressed videos which deteriorate the HR analysis  [36] .",
      "page_start": 27,
      "page_end": 28
    },
    {
      "section_name": "Conclusions",
      "text": "This paper has proposed an HR monitoring method, M OM BAT , that is, MOni- Experimental results have demonstrated that HR monitoring can be significantly improved when both pulse modeling and HR tracking are incorporated. Further, they have indicated that our M OM BAT perform the monitoring in near real-time, with an average absolute error of 1.3293 bpm and the Pearson correlation of 0.9746 between predicted and actual HR. This indicates that our method, M OM BAT can be effectively used for HR monitoring.\n\nOur method M OM BAT can perform spuriously when the face video contains motion that persists for long duration. Our future work will investigate the possibilities to handle this issue by fusing it with Lagrangian techniques. Our method requires some parameter selection. Amongst them, the number of basis depends on the sampling rate.\n\nWe will be collecting a larger database at different sampling rates using different video compression techniques. It will be used to explore the efficacy of convolutional neural networks  [35]  and mitigate the compression artifacts for better HR analysis.\n\nIn [33] Son Lam Phung, Abdesselam Bouzerdoum, and Douglas Chai. A novel skin color model in ycbcr color space and its application to human face detection. In",
      "page_start": 29,
      "page_end": 33
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: where several spurious peaks are generated due to facial movements.",
      "page": 5
    },
    {
      "caption": "Figure 1: Spectrum of the temporal signal containing noise. HR (in bpm) and their corresponding amplitude",
      "page": 6
    },
    {
      "caption": "Figure 2: Flow-graph of our Proposed Method, MOMBAT",
      "page": 9
    },
    {
      "caption": "Figure 3: Steps in ROI Detection: a) Video frame; b) Landmarks on the face using CLNF; c) Detected skin",
      "page": 10
    },
    {
      "caption": "Figure 2: illustrates the ﬂow-graph of the proposed method, MOMBAT .",
      "page": 10
    },
    {
      "caption": "Figure 4: Usefulness of Image Registration: a) Pulse obtained without frame registration; and b) Pulse",
      "page": 11
    },
    {
      "caption": "Figure 3: illustrates",
      "page": 11
    },
    {
      "caption": "Figure 4: It shows the pulse spectrum before and after applying the image registra-",
      "page": 11
    },
    {
      "caption": "Figure 3: (b) shows the chosen land-",
      "page": 11
    },
    {
      "caption": "Figure 5: Steps in post-processing: a) Quality, ˆQ; pulse signal, ¯",
      "page": 14
    },
    {
      "caption": "Figure 5: (b) along with its corresponding pulse spectrum in",
      "page": 15
    },
    {
      "caption": "Figure 5: (c). It can be observed from Figure 5(c) that the predicted HR is deviated sig-",
      "page": 15
    },
    {
      "caption": "Figure 5: (d) along with its cor-",
      "page": 16
    },
    {
      "caption": "Figure 5: (e). It can be observed from the Figures 5(c)",
      "page": 16
    },
    {
      "caption": "Figure 6: Example of pulse spectrum during tracking: a) Prior, Pe (for hi = 1.5 and",
      "page": 17
    },
    {
      "caption": "Figure 6: We apply maximum a posteriori estimation",
      "page": 19
    },
    {
      "caption": "Figure 7: Examples of HR monitoring with and without tracking. X-axis denotes the window number and",
      "page": 20
    },
    {
      "caption": "Figure 7: The ﬁgure",
      "page": 20
    },
    {
      "caption": "Figure 8: Distribution of HR in the database.",
      "page": 21
    },
    {
      "caption": "Figure 8: 4.2. Performance Measurement",
      "page": 21
    },
    {
      "caption": "Figure 9: Performance analysis for: a) α and b) ˆc. X and Y-axes denote the parameter value and MAE (in",
      "page": 22
    },
    {
      "caption": "Figure 9: We set α and ˆc equal to 50 and 0.4 respectively, where minimum MAE is attained on",
      "page": 22
    },
    {
      "caption": "Figure 5: for example) and in return, they incur an additional average time",
      "page": 26
    },
    {
      "caption": "Figure 10: HR Monitoring by Our MOMBAT under Large HR Fluctuations. X and Y-axes denote the",
      "page": 27
    },
    {
      "caption": "Figure 11: Erroneous HR Monitoring by Our MOMBAT. X and Y-axes denote the window number and",
      "page": 27
    },
    {
      "caption": "Figure 10: Just like other existing methods, MOMBAT may",
      "page": 27
    },
    {
      "caption": "Figure 11: 4.6. Performance Analysis on COHFACE",
      "page": 27
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "N orSysR\nN orSysI\nN orSys\nP ulseM odP\nP ulseM odL\nP ulseM od\nBayT rack\nM OM BAT",
          "Color\nchannel": "Red\nGreen\nGreen\nGreen\nGreen\nGreen\nGreen\nGreen",
          "Image\nregistration": "No\nNo\nYes\nYes\nYes\nYes\nYes\nYes",
          "Pulse\nmodeling1": "NA\nNA\nNA\nPolynomial\nLegendre\nFourier\nNA\nFourier",
          "Bayesian\ntracking": "No\nNo\nNo\nNo\nNo\nNo\nYes\nYes"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "[1]\n[22]\n[39]\nCHROM [11]\nPOS [52]\nN orSysR\nN orSysI\n[47]\nN orSys\nP ulseM odP\nP ulseM odL\nP ulseM od\nBayT rack\n[35]\nM odCN N\n[17]\nM OM BAT",
          "µ": "-18.4120\n-9.0745\n9.5317\n-8.1932\n-8.9827\n-10.8246\n-6.9634\n6.8242\n-6.4405\n-6.4079\n-6.3865\n-6.1783\n-0.5864\n0.9275\n-10.1539\n0.4667\n-0.1041",
          "σ": "27.6195\n20.2534\n21.0467\n19.1917\n19.7920\n21.4921\n18.7418\n18.3521\n17.4389\n17.3964\n17.3726\n17.1010\n5.7052\n7.3472\n21.6781\n4.8230\n2.6172",
          "err5": "38\n75\n70\n76\n77\n72\n80\n81\n83\n82\n83\n85\n94\n90\n31\n89\n97",
          "M AE": "22.5972\n10.0305\n11.3885\n9.843\n10.214\n10.946\n8.7382\n8.1864\n7.3813\n7.2503\n7.2057\n7.1853\n2.2154\n3.4588\n18.2169\n2.4968\n1.3293",
          "ρ": "-0.1793\n0.2515\n0.3310\n0.3015\n0.2912\n0.2847\n0.3106\n0.4256\n0.4486\n0.4504\n0.4542\n0.4627\n0.8821\n0.8226\n-0.0337\n0.8601\n0.9746",
          "ts": "25.72\n30.37\n1.24\n6.81\n6.81\n6.80\n6.80\n19.41\n6.84\n9.63\n9.63\n9.63\n7.06\n9.92\n9.92\n16.81\n9.78"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table 3: Itcanbe observedthattheseresults",
      "data": [
        {
          "Method": "[1]\n[22]\n[39]\nCHROM [11]\nPOS [52]\nN orSysR\nN orSysI\n[47]\nN orSys\nP ulseM odP\nP ulseM odL\nP ulseM od\nBayT rack\n[35]\nM odCN N\n[17]\nM OM BAT",
          "µ": "-10.1664\n9.3481\n6.8215\n-7.9135\n-7.1263\n-7.8460\n-6.2179\n6.9628\n-5.3672\n-4.6730\n-4.5203\n-4.0942\n-1.0576\n2.3874\n-4.3561\n1.4666\n-0.9832",
          "σ": "19.2326\n18.5818\n19.4267\n19.1191\n18.3902\n18.9212\n16.3592\n15.6330\n15.8627\n14.7966\n14.7171\n13.9043\n9.4852\n11.9163\n18.2781\n12.6595\n7.3823",
          "err5": "70\n74\n67\n75\n77\n72\n80\n81\n83\n85\n85\n86\n90\n89\n56\n88\n92",
          "M AE": "12.3612\n11.2493\n16.3568\n10.384\n9.621\n10.696\n9.0424\n8.9354\n8.7956\n8.2718\n8.1629\n7.9250\n6.4797\n6.8128\n14.8243\n6.5411\n5.8923",
          "ρ": "0.0897\n0.1264\n-0.1129\n0.1315\n0.1617\n0.1248\n0.1721\n0.1813\n0.2065\n0.2108\n0.2256\n0.2614\n0.5352\n0.5036\n-0.0696\n0.5252\n0.6184"
        }
      ],
      "page": 28
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Detecting pulse from head motions in video",
      "authors": [
        "Guha Balakrishnan",
        "Fredo Durand",
        "John Guttag"
      ],
      "year": "2013",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "2",
      "title": "3d constrained local model for rigid and non-rigid facial tracking",
      "authors": [
        "Tadas Baltrušaitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2012",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "3",
      "title": "Constrained local neural fields for robust facial landmark detection in the wild",
      "authors": [
        "Tadas Baltrusaitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on Computer Vision Workshops (ICCV-W)"
    },
    {
      "citation_id": "4",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "Tadas Baltrušaitis",
        "Peter Robinson",
        "Louis-Philippe Morency"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "5",
      "title": "Heart rate variability: origins, methods, and interpretive caveats",
      "authors": [
        "Thomas Gary G Berntson",
        "Dwain Bigger",
        "Paul Eckberg",
        "Grossman",
        "Marek Peter G Kaufmann",
        "Malik",
        "N Haikady",
        "Stephen Nagaraja",
        "Philip Porges",
        "Peter Saul",
        "Stone"
      ],
      "year": "1997",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "6",
      "title": "Mobiscan3d: A low cost framework for real time dense 3d reconstruction on mobile devices",
      "authors": [
        "Brojeshwar Bhowmick",
        "Apurbaa Mallik",
        "Arindam Saha"
      ],
      "year": "2014",
      "venue": "Intl Conf on Ubiquitous Intelligence and Computing"
    },
    {
      "citation_id": "7",
      "title": "Face-fromdepth for head pose estimation on depth images",
      "authors": [
        "Guido Borghi",
        "Matteo Fabbri",
        "Roberto Vezzani",
        "Rita Cucchiara"
      ],
      "year": "2018",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "8",
      "title": "Heart rate variability: standards of measurement, physiological interpretation and clinical use. task force of the european society of cardiology and the north american society of pacing and electrophysiology",
      "authors": [
        "Marek John Camm",
        "J Malik",
        "Günter Thomas Bigger",
        "Sergio Breithardt",
        "Richard Cerutti",
        "Philippe Cohen",
        "Ernest Coumel",
        "Fallen",
        "L Harold",
        "Kennedy",
        "Kleiger"
      ],
      "year": "1996",
      "venue": "Heart rate variability: standards of measurement, physiological interpretation and clinical use. task force of the european society of cardiology and the north american society of pacing and electrophysiology"
    },
    {
      "citation_id": "9",
      "title": "Quantification of balance in single limb stance using kinect",
      "authors": [
        "Kingshuk Chakravarty",
        "Suraj Suman",
        "Brojeshwar Bhowmick",
        "Aniruddha Sinha",
        "Abhijit Das"
      ],
      "year": "2016",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Video-based heart rate measurement: Recent advances and future prospects",
      "authors": [
        "Xun Chen",
        "Juan Cheng",
        "Rencheng Song",
        "Yu Liu",
        "Rabab Ward",
        "Jane Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "11",
      "title": "Robust pulse rate from chrominance-based rppg",
      "authors": [
        "Gerard De",
        "Vincent Jeanne"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "12",
      "title": "On the meaning and use of kurtosis",
      "authors": [
        "Lawrence T Decarlo"
      ],
      "year": "1997",
      "venue": "Psychological methods"
    },
    {
      "citation_id": "13",
      "title": "Pattern classification",
      "authors": [
        "Peter Richard O Duda",
        "David Hart",
        "Stork"
      ],
      "year": "2012",
      "venue": "Pattern classification"
    },
    {
      "citation_id": "14",
      "title": "Accurate heart-rate estimation from face videos using quality-based fusion",
      "authors": [
        "Puneet Gupta",
        "Brojeshwar Bhowmick",
        "Arpan Pal"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Image Processing"
    },
    {
      "citation_id": "15",
      "title": "Serial fusion of eulerian and lagrangian approaches for accurate heart-rate estimation using face videos",
      "authors": [
        "Puneet Gupta",
        "Brojeshwar Bhowmick",
        "Arpan Pal"
      ],
      "year": "2017",
      "venue": "IEEE International Conference of the Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "16",
      "title": "Exploring the feasibility of face video based instantaneous heart-rate for micro-expression spotting",
      "authors": [
        "Puneet Gupta",
        "Brojeshwar Bhowmick",
        "Arpan Pal"
      ],
      "year": "2018",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "17",
      "title": "Robust adaptive heart-rate monitoring using face videos",
      "authors": [
        "Puneet Gupta",
        "Brojeshwar Bhowmik",
        "Arpan Pal"
      ],
      "year": "2018",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "18",
      "title": "An accurate finger vein based verification system",
      "authors": [
        "Puneet Gupta",
        "Phalguni Gupta"
      ],
      "year": "2015",
      "venue": "Digital Signal Processing"
    },
    {
      "citation_id": "19",
      "title": "Fingerprint orientation modeling using symmetric filters",
      "authors": [
        "Puneet Gupta",
        "Phalguni Gupta"
      ],
      "year": "2015",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "20",
      "title": "An accurate fingerprint orientation modeling algorithm",
      "authors": [
        "Puneet Gupta",
        "Phalguni Gupta"
      ],
      "year": "2016",
      "venue": "Applied Mathematical Modelling"
    },
    {
      "citation_id": "21",
      "title": "A reproducible study on remote heart rate measurement",
      "authors": [
        "Guillaume Heusch",
        "André Anjos",
        "Sébastien Marcel"
      ],
      "year": "2017",
      "venue": "A reproducible study on remote heart rate measurement",
      "arxiv": "arXiv:1709.00962"
    },
    {
      "citation_id": "22",
      "title": "Accurate and efficient pulse measurement from facial videos on smartphones",
      "authors": [
        "Chong Huang",
        "Xin Yang",
        "Kwang-Ting Tim Cheng"
      ],
      "year": "2016",
      "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "23",
      "title": "A self-calibrating radar sensor system for measuring vital signs",
      "authors": [
        "Ming-Chun Huang",
        "Jason Liu",
        "Wenyao Xu",
        "Changzhan Gu",
        "Changzhi Li",
        "Majid Sarrafzadeh"
      ],
      "year": "2016",
      "venue": "IEEE transactions on biomedical circuits and systems"
    },
    {
      "citation_id": "24",
      "title": "Projection pursuit. The annals of Statistics",
      "authors": [
        "J Peter",
        "Huber"
      ],
      "year": "1985",
      "venue": "Projection pursuit. The annals of Statistics"
    },
    {
      "citation_id": "25",
      "title": "Robust heart rate measurement from video using select random patches",
      "authors": [
        "Antony Lam",
        "Yoshinori Kuno"
      ],
      "year": "2015",
      "venue": "International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "26",
      "title": "Heart rate estimation from facial photoplethysmography during dynamic illuminance changes",
      "authors": [
        "Dongseok Lee",
        "Jeehoon Kim",
        "Sungjun Kwon",
        "Kwangsuk Park"
      ],
      "venue": "International Conference on Image Processing"
    },
    {
      "citation_id": "27",
      "title": "Advancements in noncontact, multiparameter physiological measurements using a webcam",
      "authors": [
        "Ming-Zher Poh",
        "Daniel Mcduff",
        "Rosalind Picard"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "28",
      "title": "EVM-CNN: Real-time contactless heart rate estimation from facial video",
      "authors": [
        "Ying Qiu",
        "Yang Liu",
        "Juan Arteaga-Falconi",
        "Haiwei Dong",
        "Abdulmotaleb Saddik"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Effects of video encoding on camera based heart rate estimation",
      "authors": [
        "Michal Rapczynski",
        "Philipp Werner",
        "Ayoub Al-Hamadi"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "30",
      "title": "How the region of interest impacts contact free heart rate estimation algorithms",
      "authors": [
        "Michal Rapczynski",
        "Philipp Werner",
        "Frerk Saxen",
        "Ayoub Al-Hamadi"
      ],
      "year": "2018",
      "venue": "International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "31",
      "title": "Automatic classification of communication signals using higher order statistics",
      "authors": [
        "Juergen Reichert"
      ],
      "year": "1992",
      "venue": "International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "32",
      "title": "Video pulse rate variability analysis in stationary and motion conditions",
      "authors": [
        "Angel Melchor",
        "J Ramos-Castro"
      ],
      "year": "2018",
      "venue": "Biomedical engineering online"
    },
    {
      "citation_id": "33",
      "title": "Handbook of multibiometrics",
      "authors": [
        "A Arun",
        "Karthik Ross",
        "Anil Nandakumar",
        "Jain"
      ],
      "year": "2006",
      "venue": "Handbook of multibiometrics"
    },
    {
      "citation_id": "34",
      "title": "Ultra short term analysis of heart rate variability for monitoring mental stress in mobile settings",
      "authors": [
        "Lizawati Salahuddin",
        "Jaegeol Cho",
        "Myeong Gi",
        "Desok Kim"
      ],
      "year": "2007",
      "venue": "International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
    },
    {
      "citation_id": "35",
      "title": "Deformable model fitting by regularized landmark mean-shift",
      "authors": [
        "Jason Saragih",
        "Simon Lucey",
        "Jeffrey Cohn"
      ],
      "year": "2011",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "36",
      "title": "Accurate upper body rehabilitation system using kinect",
      "authors": [
        "Sanjana Sinha",
        "Brojeshwar Bhowmick",
        "Kingshuk Chakravarty",
        "Aniruddha Sinha",
        "Abhijit Das"
      ],
      "year": "2016",
      "venue": "International Conference of the IEEE Engineering in Medicine and Biology Society"
    },
    {
      "citation_id": "37",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "Mohammad Soleymani",
        "Jeroen Lichtenauer",
        "Maja Thierry Pun",
        "Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Removing drift from carotid arterial pulse waveforms: A comparison of motion correction and high-pass filtering",
      "authors": [
        "Emily J Lam Po Tang",
        "Amir Hajirassouliha",
        "Martyn Nash",
        "Andrew Taberner",
        "Poul Nielsen",
        "Yusuf Cakmak"
      ],
      "year": "2019",
      "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)"
    },
    {
      "citation_id": "39",
      "title": "An advanced detrending method with application to HRV analysis",
      "authors": [
        "Mika P Tarvainen",
        "O Perttu",
        "Ranta-Aho",
        "Pasi A Karjalainen"
      ],
      "year": "2002",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "40",
      "title": "Self-adaptive matrix completion for heart rate estimation from face videos under realistic conditions",
      "authors": [
        "Sergey Tulyakov",
        "Xavier Alameda-Pineda",
        "Elisa Ricci",
        "Lijun Yin",
        "Jeffrey Cohn",
        "Nicu Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "41",
      "title": "Remote plethysmographic imaging using ambient light",
      "authors": [
        "Wim Verkruysse",
        "Lars Svaasand",
        "J Stuart"
      ],
      "year": "2008",
      "venue": "Optics express"
    },
    {
      "citation_id": "42",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "Paul Viola",
        "Michael Jones"
      ],
      "year": "2001",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "43",
      "title": "Facial feature point detection: A comprehensive survey",
      "authors": [
        "Nannan Wang",
        "Xinbo Gao",
        "Dacheng Tao",
        "Xuelong Li"
      ],
      "year": "2014",
      "venue": "Facial feature point detection: A comprehensive survey",
      "arxiv": "arXiv:1410.1037"
    },
    {
      "citation_id": "44",
      "title": "Automatic eye detection and its validation",
      "authors": [
        "Peng Wang",
        "Qiang Matthew B Green",
        "James Ji",
        "Wayman"
      ],
      "year": "2005",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition-Workshops (CVPRW)"
    },
    {
      "citation_id": "45",
      "title": "Algorithmic principles of remote ppg",
      "authors": [
        "Wenjin Wang",
        "Albertus C Den",
        "Sander Brinker",
        "Gerard Stuijk",
        "De Haan"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "46",
      "title": "Eulerian video magnification for revealing subtle changes in the world",
      "authors": [
        "Hao-Yu Wu",
        "Michael Rubinstein",
        "Eugene Shih",
        "John Guttag",
        "Frédo Durand",
        "William Freeman"
      ],
      "year": "2012",
      "venue": "ACM Transactions on Graphics"
    },
    {
      "citation_id": "47",
      "title": "A method for automatic identification of reliable heart rates calculated from ECG and PPG waveforms",
      "authors": [
        "Chenggang Yu",
        "Zhenqiu Liu",
        "Thomas Mckenna",
        "Andrew Reisner",
        "Jaques Reifman"
      ],
      "year": "2006",
      "venue": "Journal of the American Medical Informatics Association"
    },
    {
      "citation_id": "48",
      "title": "TROIKA: A general framework for heart rate monitoring using wrist-type photoplethysmographic signals during intensive physical exercise",
      "authors": [
        "Zhilin Zhang",
        "Zhouyue Pi",
        "Benyuan Liu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on biomedical engineering"
    }
  ]
}