{
  "paper_id": "2207.10293v2",
  "title": "Affective Behavior Analysis Using Action Unit Relation Graph And Multi-Task Cross Attention",
  "published": "2022-07-21T04:07:07Z",
  "authors": [
    "Dang-Khanh Nguyen",
    "Sudarshan Pant",
    "Ngoc-Huynh Ho",
    "Guee-Sang Lee",
    "Soo-Huyng Kim",
    "Hyung-Jeong Yang"
  ],
  "keywords": [
    "multi-task learning",
    "cross attention",
    "action unit detection",
    "facial expression recognition",
    "valence and arousal estimation",
    "graph convolution network"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial behavior analysis is a broad topic with various categories such as facial emotion recognition, age, and gender recognition. Many studies focus on individual tasks while the multi-task learning approach is still an open research issue and requires more research. In this paper, we present our solution and experiment result for the Multi-Task Learning challenge of the Affective Behavior Analysis in-the-wild competition. The challenge is a combination of three tasks: action unit detection, facial expression recognition, and valance-arousal estimation. To address this challenge, we introduce a cross-attentive module to improve multi-task learning performance. Additionally, a facial graph is applied to capture the association among action units. As a result, we achieve the evaluation measure of 128.8 on the validation data provided by the organizers, which outperforms the baseline result of 30.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In affective computing, emotion recognition is a fundamental research topic and our face is an obvious indicator to analyze the human affect. With the development of computer vision and deep learning, there are numerous studies and modern applications related to facial behavior analysis  [10] ,  [8] ,  [6] . The ABAW 4th Workshop organized a competition with two challenges which are the multitask learning (MTL) challenge involving the development of a multi-task model using facial images  [13]  and the learning from synthetic data (LSD) challenge involving the use of synthetic data  [7] ,  [12] . We only participated in the MTL challenge and the LSD challenge is beyond the scope of this work.\n\nThe MTL challenge aims to design a model performing the following three tasks with facial image data as input:\n\n1. Action unit detection (AU detection): a task involving a multi-label classification with 12 classes of action units that represent various movements on the subject's face.\n\n2. Facial expression recognition (FER): a multi-class classification task, which involves identifying the emotion of the subjects among 8 categories: happiness, sadness, anger, fear, surprise, disgust, neutral and other state. This paper proposes utilizing an attention mechanism for MTL problem. By attending to the output of one specific task, the model can learn to boost the result of other related tasks. In addition, we leverage the graph-based neural network to learn the relation among action units (AUs) in the AU detection task.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Based on the VGG-Face, Kollias  [13]  devised a multi-task CNN network to jointly learn the VA estimation, AU detection, and FER. The MT-VGG model was created by adapting the original VGG-Face for multi-tasking purposes with three different types of outputs. The author also proposed the recurrent version to adapt to temporal affect variations and the multi-modal version to exploit the audio information.\n\nSavchenko  [16]  introduced a multi-head model with a CNN backbone to resolve the facial expression and attributes recognition. The model was sequentially trained with typical face corpora  [18] ,  [2] ,  [15]  for various facial analysis tasks. In ABAW 3rd competition, Savchenko  [17]  also developed a lightweight model using EfficientNet  [19]  to effectively learn the facial features and achieved the top 3 best performances in the MTL challenge.\n\nKollias  [9]  showed an association between emotions and AUs distribution. Each emotion has its prototypical AUs, which are always active along with it; and observational AUs, which are frequently present with the emotion at a certain rate. From this assumption, the authors proposed co-annotation and distribution matching to couple the emotions and AUs. Luo  [14]  introduced a graph-based method with multi-dimensional edge features to learn the association among AUs. The AU detection block in our model is inspired by the node feature learning in  [14] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "Our method is based on two observations: (1) there are informative connections among AU activations  [14]  and (2) the presence of the AUs is related to the facial expression  [9] . Following these statements, we proposed a model with a graph convolution network (GCN) to exploit the AUs' connections and a cross attention module to learn the influence of AUs' presence on facial emotion expression. The architecture of the proposed model is illustrated in Fig 1 . We used a pre-trained EfficientNet  [16]  to obtain the facial feature vector from the input image. The image feature is then fed to three blocks corresponding to three tasks. Regarding the AU detection task, we utilized the AU relation graph to create the AU-specific features. For expression recognition and valance-arousal estimation, we used two fully connected layers to generate the predictions. In addition, we devised an attention-based module to learn the effect of facial AUs on the prediction of the emotion recognition task.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Au Relation Graph",
      "text": "To learn the representation of AUs, we adopted the Action unit Node feature learning (ANFL) module proposed by Luo  [14] . The ANFL module consists of N fully connected layers corresponding to N AUs. These layers generate the AU-specific feature vectors {v i } N i=1 = V using the extracted image feature X. Mathematically, the AU-specific feature vectors are given by:\n\nAfterward, the Facial Graph Generator (FGG) constructs a fully connected graph with N nodes corresponding to N AU-specific feature vectors. The edge weight of two nodes is the inner dot product of the two corresponding vectors. The graph is simplified by removing edges with low weights. We chose k-nearest neighbors algorithm to keep valuable connections between nodes. We used the simplified topology to create the adjacency matrix of a GCN. The GCN is used to enrich the connection information among AU-specific feature vectors. Generally, the AU-specific feature vectors learned by the GCN network are denoted by:\n\nFinally, we calculate the similarity between the v F GG i and s i to get the probability of each AU activation using the node features from the GCN. The similarity function is defined by:\n\nwhere s i is trainable vector having same dimension as v F GG i . The operation of ANFL is illustrated in Fig 2 . More detail about ANFL is in  [14] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Fer And Va Estimation Heads",
      "text": "To estimate FER and VA, we simply put the image feature X into two fully connected layers in parallel. We used Batch Normalization and Tanh activation function to produce the valance-arousal value ŷV A . Meanwhile, FER head generates unweighted logit prediction ỹEX without any activation function. With trainable parameters W V A , b V A , W EX , b EX , the formulas of ŷV A and ỹEX are given below:\n\nInspired by the additive attention of Bahdanau  [1] , we devised a multi-task cross attention module to discover the relationship between AU prediction and facial expression. Given an AU prediction ŷAU (query) and FER unweighted prediction ỹEX (key), the attention module generates the attention scores as the formula below:\n\nwhere W q , W k , W v are trainable parameters.\n\nThe attention weights a are computed as the softmax output of the attention scores h. This process is formulated as:\n\nFinally, the weighted FER prediction is the element-wise product of attention weight a and the FER unweighted prediction ỹEX (value): ŷEX = a * ỹEX (8)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loss Function",
      "text": "We used the weighted asymmetric loss proposed by Luo  [14]  for AU detection task training process:\n\nEach binary entropy loss of an AU is multiplied with a coefficient w i . These factors are used to balance the contribution of each AU in the loss function because their appearance frequencies of AUs are different from each other as in Fig 3 . The factor w i is computed from occurrence rate r i of AU i th in the dataset:\n\nAs the ratio of 8 expressions are imbalance, the loss we use for FER task is weighted cross entropy function, which is given by:\n\nwhere ρ i (ŷ EX ) represents the softmax function, P i is the refactor weight calculated from training set and C is the number of facial expressions.\n\nFor VA estimation task, the loss function is obtained as the sum of individual CCC loss of valence and arousal. The formula is given by:\n\nCCC i is the concordance correlation coefficient (CCC) and i could be V (valence) or A (arousal). CCC is a metric measures the relation of two distributions, denoted by:\n\nwhere x and y are the mean values of ground truth and predicted values, respectively, s x and s x are corresponding variances and s xy is the covariance value.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Dataset",
      "text": "The experiment is conducted on the s-Aff-Wild2 dataset  [5] , which is a static version of Aff-Wild  [20]  and Aff-Wild2  [11]  databases. s-Aff-Wild2 is a collection of 221,928 images annotated with 12 action units, 6 basic expressions, and 2 continuous emotion labels in valence and arousal dimensions. In the s-Aff-Wild2 dataset, some samples may lack annotations for one of the three mentioned labels. Such missing labels may cause an imbalance in the number of valid annotations among the batches split by the data sampler. For simplicity, instead of implementing a dedicated data sampler, we decided to train three tasks separately.\n\nThe organizers provided the facial cropped images extracted from the videos and corresponding cropped-aligned images. We only used the cropped-aligned version to exploit the facial features. We resized all images from 112×112 to 224×224 pixel, normalized, applied random cropping and random horizontal flipping before the feature extraction step.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "Following  [4] , the evaluation metric of the MTL challenge P M T L is the summation of three uni-task performance measure:\n\nwhere P AU is the average F1 score of the 12 AUs in AU detection task, P EX is the average F1 score of 8 expression categories in FER task and P V A is the average of CCC indexes of valance and arousal in VA estimation task. The performance metrics of individual tasks can be formulated as:\n\nwhere CCC V , CCC A are the CCC of valence and arousal, respectively; F exp,i 1 is the F1 score of the emotion label and F AU,i 1 is the F1 score of the action unit.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Experiment Setting",
      "text": "We implemented our solution using Pytorch framework and conducted the experiments on NVIDIA RTX 2080 Ti GPU. Stochastic Gradient Descent was applied following with Sharpness-aware Minimization Optimizer  [3]  to minimize the loss function. The model was trained with an initial learning rate of 10 -3 and the cosine decay learning rate scheduler was also used. Because the CCC lost function requires a sufficient sequence of predictions  [9]  so we chose the batch size of 256 for all tasks' training process. The weight decay was applied to prevent overfitting.\n\nThe pre-trained EffecientNet in  [16]  can capture facial features efficiently. Therefore, we decide not to train it in our experiment. Firstly, we train the ANFL module with AU detection tasks so that the model can learn the knowledge of AUs activation. Subsequently, we continue training the model with the FER task to take advantage of AU detection results to support the prediction of emotion. The VA estimation head can be trained independently.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Result",
      "text": "We attain the evaluation measure of 128.8% compared to 30.0% of the baseline model  [4] . A model which uses pre-trained EfficientNet and three fully connected layers accomplishes the performance score of 121.5%. By utilizing the multi-task cross attention and GCN, we improve the prediction of three tasks as shown in Table  1 . Our best model obtains the score of 111.35% on the test set of the s-Aff-Wild2 database. All metrics in this section are in percentage (%).\n\nBy discovering the association of AUs, the model increases the average F1 score to 49.9, better than the measure of 47.9 when using a fully connected layer followed by a sigmoid activation. The SAM optimizer improves model generalization and yields better performance. The detailed results are listed in Table  2 .\n\nIn the VA estimation task, our assumption is that the batch normalization can boost the accuracy of the prediction. The batch normalization layer can change the mean and variance of a batch, respectively, to the new values β and γ, which are learnable parameters. By using CCC for the loss function as in section 3.3, the network can learn to adapt the parameters β, γ to the mean and variance of ground truth distribution. Thus, the performance of the VA estimation task can be increased. We conducted the experiments with and without the batch normalization layer to test its operation. As the result, the batch normalization layer can improve P V A by more than 4%. The CCC indexes of valence, arousal, and their average value are described in Table  3 . In the FER network, there is a noticeable improvement when we use cross attention. We obtain the average F1 score of 33.3 which is considerably higher than the case of not using multi-task cross attention. To evaluate the contribution of the attention module, we compared the F1 scores for individual emotion labels by excluding the attention mechanism. As shown in Table  4 , the use of attention increased the performance for individual labels except for happy and surprise. Although there are enhancements in multi-task inference, our model can be further improved and tuned for individual tasks. The operation of the AU graph is overfitting to the training set without the support of SAM optimizer. It is less efficient than the FC-Sig module on the validation set when we remove SAM. Regarding the FER task, the prediction of happy label, which is a common emotion, is not improved when the attention mechanism is applied.",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we introduced the attention-based approach to the multi-task learning problem. The idea is to exploit the output of one task to enhance the performance of another task. In particular, our model attends to the AU detection task's result to achieve better output in the FER task. Our result is outstanding compared to the baseline and the cross attention module improves the evaluation metric on the FER task. The experiment demonstrates that facial AUs have a strong relationship with facial expression and this relation can be leveraged to recognize human emotion more efficiently. Additionally, we take advantage of the GCN and the batch normalization to accomplish the AU detection and VA estimation task, respectively, with considerable advancement.\n\nHowever, in our model, the progress to generate valence and arousal is completely independent of other tasks. In our architecture, except for the image feature, there is no common knowledge between VA estimation and remaining heads. The relation between valence-arousal and other facial attributes is not exploited in this paper. In the future, we plan to study the influence between valence-arousal and other facial behavior tasks.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Acknowledgement",
      "text": "This work was supported by a National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT). (NRF-2020R1A4A1019191).",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Block diagram of our multi-task cross attention model",
      "page": 3
    },
    {
      "caption": "Figure 1: We used a",
      "page": 3
    },
    {
      "caption": "Figure 2: More detail about ANFL is in [14].",
      "page": 4
    },
    {
      "caption": "Figure 2: AUs Relationship-aware Node Feature Learning module",
      "page": 4
    },
    {
      "caption": "Figure 3: . The factor wi is computed from occurrence rate ri of AU ith in the",
      "page": 5
    },
    {
      "caption": "Figure 3: Distribution of facial attributes in s-Aff-Wild2 database",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Our best model obtains the score of 111.35% on the test set of the",
      "page": 7
    },
    {
      "caption": "Table 2: In the VA estimation task, our assumption is that the batch normalization",
      "page": 7
    },
    {
      "caption": "Table 1: The uni-task and multi-task performance of our model and other options.",
      "page": 8
    },
    {
      "caption": "Table 2: The comparison F1 score of each AU prediction on validation set between",
      "page": 8
    },
    {
      "caption": "Table 3: Table 3. The VA evaluation metrics on validation set of the model with and without",
      "page": 8
    },
    {
      "caption": "Table 4: , the use of attention",
      "page": 8
    },
    {
      "caption": "Table 4: The F1 scores of emotion labels on validation set of the model with and",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Neural machine translation by jointly learning to align and translate",
      "arxiv": "arXiv:1409.0473"
    },
    {
      "citation_id": "2",
      "title": "Vggface2: A dataset for recognising faces across pose and age",
      "authors": [
        "Q Cao",
        "L Shen",
        "W Xie",
        "O Parkhi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "3",
      "title": "Sharpness-aware minimization for efficiently improving generalization",
      "authors": [
        "P Foret",
        "A Kleiner",
        "H Mobahi",
        "B Neyshabur"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "4",
      "title": "Abaw: Learning from synthetic data & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Learning from synthetic data & multi-task learning challenges",
      "arxiv": "arXiv:2207.01138"
    },
    {
      "citation_id": "5",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "6",
      "title": "Photorealistic facial synthesis in the dimensional affect space",
      "authors": [
        "D Kollias",
        "S Cheng",
        "M Pantic",
        "S Zafeiriou"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV) Workshops"
    },
    {
      "citation_id": "7",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "8",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "9",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "D Kollias",
        "V Sharmanska",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "10",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "12",
      "title": "Va-stargan: Continuous affect generation",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Conference on Advanced Concepts for Intelligent Vision Systems"
    },
    {
      "citation_id": "13",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "14",
      "title": "Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition",
      "authors": [
        "C Luo",
        "S Song",
        "W Xie",
        "L Shen",
        "H Gunes"
      ],
      "year": "2022",
      "venue": "Learning multi-dimensional edge feature-based au relation graph for facial action unit recognition",
      "arxiv": "arXiv:2205.01782"
    },
    {
      "citation_id": "15",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "16",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "Proceedings of the 19th International Symposium on Intelligent Systems and Informatics (SISY)"
    },
    {
      "citation_id": "17",
      "title": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "authors": [
        "A Savchenko"
      ],
      "year": "2022",
      "venue": "Frame-level prediction of facial expressions, valence, arousal and action units for mobile devices",
      "arxiv": "arXiv:2203.13436"
    },
    {
      "citation_id": "18",
      "title": "Automatic group level affect and cohesion prediction in videos",
      "authors": [
        "G Sharma",
        "S Ghosh",
        "A Dhall"
      ],
      "year": "2019",
      "venue": "2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos"
    },
    {
      "citation_id": "19",
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "authors": [
        "M Tan",
        "Q Le"
      ],
      "year": "2019",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "20",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops"
    }
  ]
}