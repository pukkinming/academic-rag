{
  "paper_id": "2311.16114v3",
  "title": "Learning Noise-Robust Joint Representation For Multimodal Emotion Recognition Under Incomplete Data Scenarios",
  "published": "2023-09-21T10:49:02Z",
  "authors": [
    "Qi Fan",
    "Haolin Zuo",
    "Rui Liu",
    "Zheng Lian",
    "Guanglai Gao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal emotion recognition (MER) in practical scenarios is significantly challenged by the presence of missing or incomplete data across different modalities. To overcome these challenges, researchers have aimed to simulate incomplete conditions during the training phase to enhance the system's overall robustness. Traditional methods have often involved discarding data or substituting data segments with zero vectors to approximate these incompletenesses. However, such approaches neither accurately represent real-world conditions nor adequately address the issue of noisy data availability. For instance, a blurry image cannot be simply replaced with zero vectors, while still retaining information. To tackle this issue and develop a more precise MER system, we introduce a novel noise-robust MER model that effectively learns robust multimodal joint representations from noisy data. This approach includes two pivotal components: firstly, a noise scheduler that adjusts the type and level of noise in the data to emulate various realistic incomplete situations. Secondly, a Variational AutoEncoder (VAE)-based module is employed to reconstruct these robust multimodal joint representations from the noisy inputs. Notably, the introduction of the noise scheduler enables the exploration of an entirely new type of incomplete data condition, which is impossible with existing methods. Extensive experimental evaluations on the benchmark datasets IEMOCAP and CMU-MOSEI demonstrate the effectiveness of the noise scheduler and the excellent performance of our proposed model. Our project is publicly available on https://github.com/WooyoohL/Noise-robust_MER.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal emotion recognition (MER) aims to take multimodal signals, including text, audio, and visual, as input to predict the emotion category  [41] . As a cutting-edge technology, it is widely used in various scenarios, including virtual intelligent assistants, robot customer service, and other applications, offering tailored and empathetic user experiences by responding appropriately to the emotional cues of users. In the research of MER, remarkable performance depends heavily on the complete multimodal data and robust joint representation learning  [15] . However, in realistic scenarios, data often becomes incomplete due to two main cases: either data is absolutely absence caused by sensor malfunction  [39] , or it is partially incompleteness attributed to diminished network bandwidth or various forms of noise interference  [25, 40] , etc. We collectively call these data noisy or incomplete data, which present huge challenges for multimodal emotion recognition  [39] .\n\nIn recent years, researchers have tried to enhance the robustness of MER by simulating incomplete data during training. Previous researchers proposed two methods to simulate incomplete data according to the usual intuition: 1) set feature vectors of missing data to zero: in some situations, zero is a value that signifies \"no information\" or \"lack of information\"  [39, 41] , 2) discard data randomly with a predefined probability: dropped data can be regarded as completely missing  [5, 23] . On this basis, MER work in the face of incomplete data focuses on two main areas: 1) missing data completion  [4, 9, 23, 35]  and 2) multimodal joint representation learning with available data  [13, 32] . For example, Cai et al.  [4]  proposed to use adversarial learning to generate missing modality images. Zeng et al.  [38]  proposed an ensemble learning method to use several models to solve the missing problem jointly. Zhao et al.  [39]  proposed a Missing Modality Imagination Network (MMIN) using AutoEncoder and cycle consistency construction to learn joint representations while predicting missing modalities. Zuo et al.  [41]  proposed introducing the modality invariant feature into MMIN to learn robust multimodal joint representations. Liu et al.  [26]  combined contrastive learning and invariant features to imagine the feature of the missing modality. The above work lays a solid foundation for MER work under incomplete data. Note that the framework that combines both missing data completion and multimodal joint representation learning has become the mainstream scheme in the research of this area.\n\nHowever, the above approach faces two main issues, including 1) the method of simulating incomplete data is unreasonable and impractical: traditional data setup methods set parts of the feature to zero vectors or discard some data directly at a set percentage. These methods neither reliably simulate real-world scenarios nor reserve the availability of incomplete data. Moreover, assuming the data contains some type or level of noise, is not it better to try to mine useful information from it rather than just discard it? and 2) the structure is redundant: traditional model structure learns a multimodal joint representation in the process of completing the missing data  [41] , and such a process tends to cause the errors generated in the first step to have a negative impact on the second step. Whether robust multimodal joint feature representations can be learned directly from noisy data?\n\nTo answer the above two questions, we propose a novel Noiserobust MER model, termed NMER. Specifically, we design a noise scheduler at the embedding level. It creates noisy training and testing data by adding various types and intensities of noise to the embedding, thus simulating the influence of diverse incomplete situations in realistic scenarios. It is worth mentioning that we explored an incomplete condition of three new modalities through our noise scheduler, which was impossible to achieve in previous works. Then, we present a Variational AutoEncoder (VAE)  [22, 34]  based multimodal joint representation learning network to reconstruct robust multimodal joint representations from the noisy data. In this way, we simulate incomplete data in realistic scenarios, make full use of the valuable information of the existing noisy data, and then leverage the powerful generative capabilities of VAE to reconstruct robust multimodal joint representations from noisy data and achieve multimodal emotion recognition. We conduct experiments on the benchmark dataset IEMOCAP  [3]  and CMU-MOSEI  [24] , which are widely used. Our code has been released in the supplementary material.\n\nThe main contributions of this work are as follows:\n\nâ€¢ We propose a noise-robust MER model, termed NMER, to generate robust multimodal joint representations under noisy incomplete data and proceed with multimodal emotion recognition.\n\nâ€¢ We explore a new method to simulate realistic data under various incomplete conditions utilizing a noise scheduler while enabling a new condition with the incompletion of all three modalities.\n\nâ€¢ Experimental results under various noise types and intensity conditions show that our NMER outperforms most baselines and demonstrates robustness in the face of incomplete data.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Nmer: Methodology 2.1 Overall Architecture",
      "text": "The overall architecture of NMER is illustrated in Figure  2 , which consists of 1) Noise Scheduler; 2) VAE-based Network; and 3) Classifier. Specifically, we first employ our Noise Scheduler to add configurable noise to original embeddings to get incomplete data and send them into the VAE-based network. Then, the VAE-based Network seeks to extract the useful features to reconstruct the robust multimodal joint representation. At last, the joint representation will be fed into Classifier to predict the final emotional result.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Noise Scheduler",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Rationale And Noise Selection.",
      "text": "Inspired by the previous work  [17] , we can obtain a blurred picture by gradually adding Gaussian noise to the picture. Similarly, we can obtain a \"noisy embedding\" by adding noise to the embedding  [30, 33] , which enhances the robustness of the model while simulating the modality incomplete condition. We examine two common noise types as examples: Gaussian noise, as described by C. F. Gauss in  [11] , and impulse noise, following the research of Kim et al.  [20]  Gaussian noise is extensively employed to simulate errors and disturbances occurring in natural and technical processes due to its distinct statistical characteristics, namely a well-defined mean ğœ‡ and standard deviation ğœ that conform to a normal distribution. The selection of this type of noise is grounded in a crucial observation: deviations influenced by various factors tend to approximate a normal distribution in most natural and technological systems. Consequently, the use of Gaussian noise in practical applications not only maintains statistical rigor but also ensures that models exhibit greater robustness and adaptability when confronted with the complexities of real-world data  [2, 14] .\n\nImpulse noise is another typical noise form that often appears in signal processing. Due to its ability to simulate brief and intense disturbances, it is widely used to model sudden anomalies in data transmission or sensor inputs. Additionally, impulse noise can also be employed to enhance the capability of the model to recognize and process extreme data points  [27, 29] . The integration of noise at the embedding level is a pivotal aspect of simulating realistic scenarios. Previous works have prove its effectiveness and robustness facing real situtaions. For the incomplete modality problem, we can choose various techniques for corrupting original data, such as text (masking, reversing word order), audio (adding reverberation or noise), and video (introducing frame loss or blur). However, quantifying the specific impact of such corruption on each modality is quite difficult. For instance, determining the equivalent effect of adding a certain decibel level of noise to audio, or masking corresponding words, if a frame in a video is completely missing, is particularly difficult. Therefore, we introduce noise on the embedding to strike a balance between realism and analytical feasibility.\n\nBased on the reasons above, we select those two types of noise on the embedding level, bring quantitative loss of information through control of the noise level, and simulate the negative impact on information from noise in reality. To be clear, our noise scheduler is designed to separate the processes of generating and adding noise. Users have the freedom to select or customize any type of noise, provided it can be created as a tensor using a programming language. This design ensures that the choice of noise type does not impact the addition process, offering users significant flexibility to adapt to various data processing needs.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Noisy Embedding Construction.",
      "text": "Following the diffusion process in  [17] , we construct noisy embedding through a similar process. Given complete multimodal data samples, encompassing acoustic, visual, and lexical components (denoted as ğ‘, ğ‘£, and ğ‘™ respectively), the embeddings are noted as ğ‘¬ ğ’‚ , ğ‘¬ ğ’— , and ğ‘¬ ğ’ . We use ğ‘¬ ğ’‚ as an example to illustrate the process of making incomplete data.\n\nFor constructing the noisy embedding using Gaussian noise, we build a noise schedule across time steps represented by ğ›½ s : However, this method is computationally intensive. To optimize this process for directly obtaining ğ‘¬ ğ’‚ ğ‘» from ğ‘¬ ğ’‚ 0 without iterating through each intermediate step from ğ‘¬ 1 to ğ‘¬ ğ‘» -1 , we precompute the cumulative product of 1 -ğ›½ terms, denoted as á¾±ğ‘¡ :\n\nIt significantly reduces computational complexity, which reflects the total variance retained in the data up to step ğ‘‡ . Utilizing á¾±ğ‘¡ , the noisy embedding ğ‘¬ ğ’‚ ğ‘» is obtained from ğ‘¬ ğ’‚ 0 with a single update:\n\nwhere ğ is a freshly sampled noise vector from a Gaussian distribution, ğ âˆ¼ N (0, 1). This method enables an efficient bypass of the sequential update steps, directly synthesizing the noisy vector at any time step ğ‘‡ that we desired. For impulse noise, the sampled noise vector ğ consists only of random values 1s and -1s. An appearance frequency ğ‘ is employed to adjust its values, setting 1 -ğ‘ percent of them to zero and leaving the remaining ğ‘ percent unchanged. This adjustment introduces noise to ğ‘ percent of data. Then we still use Equation 3 to get ğ‘¬ ğ’‚ ğ‘» .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Noise Intensity Calculation.",
      "text": "In the noisy embedding construction process, the ğ›½ start , ğ›½ end , and ğ‘‡ control the intensity of noise jointly. Therefore, we better quantify the noise intensity by calculating the signal-to-noise ratio (SNR) between the original and noisy data, in other words, the intensity ratio between noise and raw data.\n\nIn Equation  3 , we regard âˆš á¾±ğ‘‡ and âˆš 1 -á¾±ğ‘‡ as the power of the original data part and the noise part, separately. According to Equation 2 and 5, it's easy to know that when ğœ 2 is invariant, the SNR value shares the opposite trend with ğ‘‡ growing. Hence we can obtain a higher level of noise by increasing the value of ğ‘‡ .\n\nwhere the ğœ 2 is the variance of the original data. More intuitively, SNR is always represented in decibels(dB):\n\nAccording to Equation 2 and 5, it's easy to know that when ğœ 2 is invariant, the SNR value shares the opposite trend with ğ‘‡ growing. Hence we can obtain a higher level of noise by increasing the value of ğ‘‡ . Due to the variance differences across different datasets and modalities, we select a distinct T-value for each modality to maintain a uniform noise level. For instance, within the CMU-MOSEI dataset, we adjust the ğ‘‡ -value for ğ‘, ğ‘£, and ğ‘™ modalities to [140, 60, 5] respectively to introduce noise at -10dB. By doing so, we are able to impose a similar level of noise on different datasets and modalities, which simulate diverse levels of information loss.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vae-Based Network",
      "text": "The VAE-based Network includes Specificity and Invariance Encoders  [41]  and the VAE Module. The Specificity Encoders, employing LSTM  [18]  and TextCNN  [21]  structures, are tasked with extracting modality-specific emotion features ğ’‰ â€² from each modality, which is achieved by mapping the embeddings of different modalities into distinct vector spaces. Concurrently, the Invariance Encoder, utilizing a linear structure, extracts modality-invariant emotion features ğ‘¯ â€² across various modalities by mapping these embeddings into a unified vector space  [26, 41] . Upon sending the concatenated features ğ’‰ â€² and ğ‘¯ â€² into the VAE model, the VAE Encoder compresses and maps them, yielding mean and variance parameters within the latent space. This step effectively translocates the feature representation from the original data space to a probabilistic distribution in the latent space. A stochastic sampling process, facilitated by the reparameterization trick  [22] , then generates samples that reflect this latent space distribution. These samples embody the latent representations of the input features.\n\nSubsequently, these latent variables pass through the decoder network. The decoder reconstructs the feature, remapping it to the original data space, thereby producing a reconstructed multimodal joint representation denoted as ğ‘ª. Notably, the invariant feature ğ‘¯ â€² plays a pivotal role during decoding, guiding the model to focus on the common emotional features in the multimodal data.\n\nNoisy features are transformed into normal ones throughout the compression, sampling, and reconstruction phases, which provide the denoising effect. The final output, the multimodal joint representation ğ‘ª, is then fed into the Classifier to derive the result.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Loss Functions",
      "text": "As shown in Fig.  2 , the total loss L for NMER includes three parts: L = ğœ† 1 L gen + ğœ† 2 L inv + ğœ† 3 L cls , where ğœ† ğ‘  are the balance factors.\n\nGeneration loss L gen aims to calculate the distance between the generation result ğ‘ª (from incomplete data) and the multimodal joint representation Äˆ (from complete data).\n\nNote that the L gen consists of two items, that are L kl in Equation 6 and L mse in Equation  7 . L kl aims to make the hidden variables generated by the encoder conform to the standard normal distribution, while L mse seeks to make the generated multi-modal joint representation more similar to the target that extracted from complete data,\n\nwhere ğœ is the variance of the distribution of the latent vector while the ğœ‡ is the mean; ğ‘ is the total number of the real values, ğ‘¦ ğ‘– is the real value, and Å·ğ‘– is the predicted value.\n\nThe invariant loss L inv shares the same spirit as in  [41] . It adopts the MSE loss style to reduce the distance between the modalityinvariant feature ğ‘¯ â€² during training (under incomplete conditions) and the real modality-invariant feature ğ‘¯ . L cls adopts the Cross-Entropy loss function to measure and minimize the disparities between the predicted and actual emotion category labels.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments And Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Setup",
      "text": "We perform experiments on the Interactive Emotional Dyadic Motion Capture (IEMOCAP)  [3]  and CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI) dataset  [37] , which are both widely used in the research of MER. In the IEMOCAP dataset, training and testing conditions are consistently following the research  [39] . For the training set, each Table  2 : The results of the comparative studies on the IEMOCAP and CMU-MOSEI dataset, which employs two types of noise. The note \"-10dB(Avg)\" means the average performance across six common incomplete conditions under the noise intensity -10dB. \"WA\" stands for the weighted accuracy and \"UA\" refers to the unweighted accuracy. Bold values imply the best accuracy on that dataset. sample is subjected to a randomly selected incomplete condition.\n\nIn contrast, the test set consists of 557 unique samples. Because of the small number of test samples, each sample will be evaluated under six predefined incomplete conditions, resulting in a total of 3342 test samples. In the CMU-MOSEI dataset, both train and test samples are randomly influenced by one incomplete condition, the same as the train set in the IEMOCAP dataset. The only difference in data with previous works is that incomplete samples are created with our noise scheduler. Besides, the training and testing of the incomplete condition (ğ‘¬ ğ’‚ ğ‘» , ğ‘¬ ğ’— ğ‘» , ğ‘¬ ğ’ ğ‘» ) will be carried out individually to show the impact on the data in detail.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "New Incomplete Condition",
      "text": "We introduce one novel incomplete condition (ğ‘¬ ğ’‚ ğ‘» , ğ‘¬ ğ’— ğ‘» , ğ‘¬ ğ’ ğ‘» ) utilizing the noise scheduler, which represents the addition of noise to all three modalities. This condition was not previously considered in existing research. In prior methodologies, once one modality is regarded as incomplete or missing, they will drop the data of this modality or set it to zero vectors, rendering the analysis of all three modalities' incompletion as both impractical and meaningless. But in real-world scenarios, this kind of incompletion is not uncommon, and the incompletion level also varies. The controllable intensity of noise allows us to report results under this extreme condition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "On the IEMOCAP dataset, we follow research  [39, 41]  to extract the original embeddings ğ‘¬ ğ’‚ , ğ‘¬ ğ’— , and ğ‘¬ ğ’ . The audio, visual, and lexical embeddings are 130-dim OpenSMILE features with the configuration of IS13_ComParE  [10] , Denseface  [19]  embeddings extracted by a pre-trained DenseNet model of 342 dimensions, and 1024-dim BERT  [8]  word embeddings, respectively.\n\nOn the CMU-MOSEI dataset, we employ the feature extraction method from the work of Liang et al.  [24]  Audio features are 74 dimensions extracted using COVAREP  [6] , while visual embeddings are 35 dimensions derived from the pool5 layer of an Im-ageNet  [7] -trained ResNet-152  [16]  model for each video frame, which underwent preprocessing steps including resizing, center cropping, and normalizing. Facial expression features are obtained using the OpenFace  [10]  tool. Lexical features are represented using 300-dimensional GloVe word vectors  [31] . We adjust the balance factors ğœ† ğ‘  to 1, 10, and 1 for scaling the losses accordingly.\n\nThe hidden size of the LSTM structure is set to 128. The TextCNN contains 3 convolution blocks with kernel sizes of 3, 4, 5 and an output size of 128. The output size of the Invariance Encoder is also set to 128. The VAE Module includes a Transformer Encoder of 5 layers, 768 dimensions, and 16 heads as the encoder while Linear layers with dimensions of {128, 256, 384} as the decoder. The classifier contains three linear layers of size {384, 128, 4}. For the noise intensity, we conduct experiments on SNR dB of [-10dB, -20dB, -30dB, -40dB]. For impulse noise, the appearance frequency ğ‘ is set at 0.3, with other parameters remaining unchanged. All experiments are run on an NVIDIA A100 80GB GPU.\n\nWe utilize the AdamW  [28]  as the optimizer and use the Lambda LR  [36]  to dynamically update the learning rate. The initial learning rate is 0.0001. The batch size is 128 and the dropout rate is 0.5. We run experiments with 10-fold cross-validation, where each fold contains 60 epochs, and report the result on the test set. Each result is run three times and averaged to reduce the effect of random initialization of parameters. We employ the same evaluation metrics as those used in previous works  [39] ,  [41] , Weighted Accuracy (WA)  [1]  and Unweighted Accuracy (UA)  [12] , to assess various systems.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison And Ablation Study",
      "text": "In our study, we benchmark our NMER model against four advanced MER baselines to establish its relative performance.\n\n1) Modality Encoder Network (MEN)  [39] : This model serves as the complete-modality baseline. MEN is trained under complete modality conditions and tested on incomplete modality conditions.\n\n2) MCTN  [32] : MCTN uses translation-based method with cycle consistency loss to learn joint representations between every two modalities in multimodal data, which is used as a popular method.\n\n3) MMIN  [39] : This model employs a cascade residual AutoEncoder coupled with cycle consistency construction to learn joint representations, particularly for predicting missing modalities.\n\n4) IF-MMIN  [41] : An enhancement of MMIN. IF-MMIN integrates the modality invariant feature to learn robust joint representations and is recognized as a state-of-the-art incomplete modalities multimodal emotion recognition system. MCTN, MMIN, and IF-MMIN are categorized as incomplete-modality baselines, with training and testing both under incomplete modality conditions.\n\nOur experimental design not only evaluates the capability of the multimodal emotion recognition systems but also demonstrates the fine-grained control offered by our noise scheduler. We first conduct experiments on traditional six incomplete conditions, where the noise intensity progressively increases from -10dB to -40dB. Besides, we design the experiment on the brand new incompletion condition (ğ‘¬ ğ’‚ ğ‘» , ğ‘¬ ğ’— ğ‘» , ğ‘¬ ğ’ ğ‘» ), to explore and evaluate the potential of models under a more complex noise environment. Moreover, we launched a simple benchmark test on the IEMOCAP dataset that directly sends unimodality data with various intensities of noise to the classifier and reports the result. This test aims to establish a benchmark for the subsequent incomplete modality multimodal emotion recognition technology, that is, how much it has improved on the basis of a single modality data and its various types of incompletion.\n\nAdditionally, ablation studies are performed to ascertain the contribution of the VAE model and the importance of the invariant features to the overall model performance. In the study, labeled as \"w/o VAE\", we remove the VAE model. Instead, we directly concatenate the extracted emotion-specific feature ğ’‰ â€² and the emotion-invariant feature ğ‘¯ â€² , then feed this representation into the classifier. This study is designed to highlight the significant role of the VAE model in reconstructing the multimodal joint representation from noisy features. Another ablation study that removed the invariant loss, labeled as \"w/o L inv \", aims to demonstrate the invariant features can help guide the VAE model to generate more exact representations.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results About Models.",
      "text": "The average results of the comparative studies, across six incomplete conditions, four noise intensities and two noise types, are shown in Table  2 , including {a}, {v}, {l}, {a, v}, {a, l}, {v, l},  1  , with Gaussian and impulse noise at four levels of intensities.\n\n1) The NMER model outperforms most baseline models across various noise intensities and types. For instance, under -10dB Gaussian noise in the IEMOCAP dataset, NMER's WA is 0.7598 compared to 0.7120 for MEN, 0.7215 for MCTN, 0.7551 for MMIN, and 0.7543 for IF-MMIN. UA results show a similar pattern.\n\n2) There is a notable difference in accuracy (mainly in high-level noise conditions, for example -40dB, about 6% -10%) between the complete-modality baseline (MEN) and the incomplete-modality baselines (MCTN, MMIN, and IF-MMIN) as shown in Table  2 , which indicates the importance of using incomplete training data.\n\n3) The results of our model on both datasets under six incomplete conditions are detailed in Table  3  (taking Gaussian noise as an example), the WA values for single clean modality conditions {ğ‘} and {ğ‘£} drop by about 9% with the noise intensity increases from -10dB to -40dB, whereas in two clean modalities conditions {ğ‘, ğ‘£}, {ğ‘£, ğ‘™}, and {ğ‘£, ğ‘™}, the decrease is slower, ranging from about 2% -4%. When modality ğ‘ or ğ‘£ is noise-influenced, the performance decreases are roughly equivalent. This indicates that the resistance of the model to noise worsens significantly when only one modality is noisefree. However, the model shows relative robustness when the two modalities are noise-free. Notably, modality ğ‘™ plays a critical role in maintaining performance even when other modalities are compromised by noise. The WA value decreased about 4% when only modality l is clean, and about 2% when two modalities including ğ‘™ are noise-free (condition {ğ‘£, ğ‘™} and {ğ‘£, ğ‘™}).\n\n4) The WA and UA results on the CMU-MOSEI dataset, as shown in Table  2 , exhibit different trends and absolute values compared to those on the IEMOCAP dataset, primarily due to the imbalanced label distribution (as shown in Table  1 ). In such scenarios, classifiers are inclined to categorize samples into the more samples class because this strategy statistically enhances their accuracy performance. Because of the way the WA value is calculated, if the classification performance of the larger number samples class is good, then even if the noise increases, the weighted accuracy (WA) may not decrease significantly as long as the prediction for that class remains accurate. Because that class contributes more to the overall accuracy, the decline in the accuracy of fewer sample class is offset. These reasons result in the different variation trends of WA and UA values.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "About the Noise Scheduler.\n\n1) It is evident that the performance of all models deteriorates as noise intensity increases, which aligns with the expectation that increasing noise levels obscure the original data's information. For instance, in Table  2 , the WA of the IF-MMIN system on the IEMOCAP dataset under Gaussian noise declines from 0.7543 at -10dB to 0.7048 at -40dB. These results highlight the effectiveness of our noise scheduler, which allows for precise control over noise intensities and the simulation of various noise conditions. 2) From the accuracy declining and the noise intensity increasing process in Table  2 , it becomes crucial to recognize the shortcomings of previous studies that used zero vectors or drops data to simulate noise. These methods lack rationality and precise control over noise intensity and fail to enable meaningful comparisons across different noise types and intensities. In contrast, the experiments demonstrate the utility of our method in managing noises, achieving more comprehensive evaluations of the models.\n\n3) In Table  4 , we test the uni-modality result on the IEMOCAP dataset using Gaussian noise. The results in the first row show that different modalities have different quantities of information and difficulties of recognition. The performance of different modalities at the same noise level shows that different data types (audio, video, lexical) have different sensitivity to ambient noise. For example, the video (ğ‘£) modality has a greater performance degradation at -10dB (about 15%) than the audio (ğ‘) and lexical (ğ‘™) modalities (about 10%). With the incremental noise intensities (from 0dB to -40dB), the accuracy of each modality has its own downtrend. This downward trend highlights the difficulty of effective emotion recognition in high-noise environments, while also providing data support for the importance of multimodal fusion strategies. This table emphasizes the necessity and superiority of multimodal joint representation learning at the same time. When one mode is seriously disturbed, other modes may still be able to maintain good recognition performance, thus improving the overall emotion recognition accuracy.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Results About New Incomplete Condition.",
      "text": "The result of the new incomplete condition is listed in Table  5 .\n\n1) The introduction of scenarios with full-modality noise interference in multimodal emotion recognition presents a new challenge, reflecting complex real-world environments where multiple information sources can simultaneously experience quality degradation. This expands the boundaries of current research. Traditionally, models could rely on at least one clean modality. However, this new setup eliminates such possibilities, demanding greater robustness and denoising capability from the models themselves. With noise affecting all modalities, the accuracy significantly decreases. For instance, in Table  5 , on the IEMOCAP dataset, the WA value of the MMIN model falls by about 4% from -10dB to -20dB noise levels and further declines by approximately 13% at -40dB. It indicates that the traditional multimodal fusion strategy may not be able to effectively deal with the simultaneous modal degradation, exposing the limitations of existing techniques against cross-modal interference.\n\n2) There is a noticeable acceleration in performance degradation as noise intensity increases, which may be caused by the non-linear nature of emotion data and the complex response of models to varying noise levels. This information loss complicates the interaction between different modalities. Notably, all models demonstrate a similar trend under this condition. Future research should thus concentrate on studying how noise variations affect data and exploring more strategies to mitigate the impact of diverse noise.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ablation Results.",
      "text": "The result of the last two rows in Table  2  shows the crucial role of the VAE model and the invariant features' guidance. For example, in the IEMOCAP dataset, under Gaussian noise, the WA of two",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The incompleteness of multimodal data in realistic",
      "page": 1
    },
    {
      "caption": "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£stands the",
      "page": 3
    },
    {
      "caption": "Figure 2: , the total loss L for NMER includes three parts:",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Academy of Sciences": "Beijing, China",
          "Hohhot, China": "csggl@imu.edu.cn"
        },
        {
          "Academy of Sciences": "lianzheng2016@ia.ac.cn",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "Abstract",
          "Hohhot, China": "CCS Concepts"
        },
        {
          "Academy of Sciences": "Multimodal emotion recognition (MER) in practical scenarios is",
          "Hohhot, China": "â€¢ Human-\nâ€¢ Computing methodologies â†’ Computer vision;"
        },
        {
          "Academy of Sciences": "significantly challenged by the presence of missing or incomplete",
          "Hohhot, China": "centered computing â†’ HCI design and evaluation methods."
        },
        {
          "Academy of Sciences": "data across different modalities. To overcome these challenges, re-",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "searchers have aimed to simulate incomplete conditions during the",
          "Hohhot, China": "Keywords"
        },
        {
          "Academy of Sciences": "training phase to enhance the systemâ€™s overall robustness. Tradi-",
          "Hohhot, China": "Multimodal emotion recognition, incomplete modalities"
        },
        {
          "Academy of Sciences": "tional methods have often involved discarding data or substituting",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "ACM Reference Format:"
        },
        {
          "Academy of Sciences": "data segments with zero vectors to approximate these incomplete-",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian, and Guanglai Gao. 2024. Learning"
        },
        {
          "Academy of Sciences": "nesses. However, such approaches neither accurately represent",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "Noise-Robust Joint Representation for Multimodal Emotion Recognition"
        },
        {
          "Academy of Sciences": "real-world conditions nor adequately address the issue of noisy",
          "Hohhot, China": "under Incomplete Data Scenarios. In Proceedings of the 2nd International"
        },
        {
          "Academy of Sciences": "data availability. For instance, a blurry image cannot be simply",
          "Hohhot, China": "Workshop on Multimodal and Responsible Affective Computing (MRAC â€™24),"
        },
        {
          "Academy of Sciences": "replaced with zero vectors, while still retaining information. To",
          "Hohhot, China": "November 1, 2024, Melbourne, VIC, Australia. ACM, New York, NY, USA,"
        },
        {
          "Academy of Sciences": "tackle this issue and develop a more precise MER system, we intro-",
          "Hohhot, China": "9 pages. https://doi.org/10.1145/3689092.3689411"
        },
        {
          "Academy of Sciences": "duce a novel noise-robust MER model that effectively learns robust",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "multimodal joint representations from noisy data. This approach",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "includes two pivotal components: firstly, a noise scheduler that",
          "Hohhot, China": "Theoretical \nRealistic"
        },
        {
          "Academy of Sciences": "adjusts the type and level of noise in the data to emulate various",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "realistic incomplete situations. Secondly, a Variational AutoEncoder",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "(VAE)-based module is employed to reconstruct these robust mul-",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "timodal joint representations from the noisy inputs. Notably, the",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "introduction of the noise scheduler enables the exploration of an",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "entirely new type of incomplete data condition, which is impossi-",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "ble with existing methods. Extensive experimental evaluations on",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "the benchmark datasets IEMOCAP and CMU-MOSEI demonstrate",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "Complete Multimodal Data\nIncomplete Multimodal Data"
        },
        {
          "Academy of Sciences": "the effectiveness of the noise scheduler and the excellent perfor-",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "mance of our proposed model. Our project is publicly available on",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "Figure 1: The incompleteness of multimodal data in realistic"
        },
        {
          "Academy of Sciences": "https://github.com/WooyoohL/Noise-robust_MER.",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "scenarios poses a significant challenge to multimodal emo-"
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "tion recognition."
        },
        {
          "Academy of Sciences": "âˆ—Equal contributions.",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "â€ Corresponding Author.",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "1\nIntroduction"
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "Multimodal emotion recognition (MER) aims to take multimodal"
        },
        {
          "Academy of Sciences": "Permission to make digital or hard copies of all or part of this work for personal or",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "signals,\nincluding text, audio, and visual, as input to predict the"
        },
        {
          "Academy of Sciences": "classroom use is granted without fee provided that copies are not made or distributed",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "for profit or commercial advantage and that copies bear this notice and the full citation",
          "Hohhot, China": "emotion category [41]. As a cutting-edge technology, it is widely"
        },
        {
          "Academy of Sciences": "on the first page. Copyrights for components of this work owned by others than the",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "used in various scenarios, including virtual intelligent assistants,"
        },
        {
          "Academy of Sciences": "author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "republish, to post on servers or to redistribute to lists, requires prior specific permission",
          "Hohhot, China": "robot customer service, and other applications, offering tailored"
        },
        {
          "Academy of Sciences": "and/or a fee. Request permissions from permissions@acm.org.",
          "Hohhot, China": "and empathetic user experiences by responding appropriately to"
        },
        {
          "Academy of Sciences": "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "the emotional cues of users. In the research of MER, remarkable"
        },
        {
          "Academy of Sciences": "Â© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "",
          "Hohhot, China": "performance depends heavily on the complete multimodal data"
        },
        {
          "Academy of Sciences": "ACM ISBN 979-8-4007-1203-6/24/11",
          "Hohhot, China": ""
        },
        {
          "Academy of Sciences": "https://doi.org/10.1145/3689092.3689411",
          "Hohhot, China": "and robust joint representation learning [15]. However, in realistic"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "scenarios, data often becomes incomplete due to two main cases:",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "make full use of the valuable information of the existing noisy data,"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "either data is absolutely absence caused by sensor malfunction",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "and then leverage the powerful generative capabilities of VAE to"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[39], or it is partially incompleteness attributed to diminished",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "reconstruct robust multimodal\njoint representations from noisy"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "network bandwidth or various forms of noise interference [25, 40],",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "data and achieve multimodal emotion recognition. We conduct"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "etc. We collectively call these data noisy or incomplete data, which",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "experiments on the benchmark dataset IEMOCAP [3] and CMU-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "present huge challenges for multimodal emotion recognition [39].",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "MOSEI [24], which are widely used. Our code has been released in"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "In recent years, researchers have tried to enhance the robustness",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "the supplementary material."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "of MER by simulating incomplete data during training. Previous",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "The main contributions of this work are as follows:"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "researchers proposed two methods to simulate incomplete data",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "â€¢ We propose a noise-robust MER model, termed NMER, to gener-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "according to the usual intuition: 1) set feature vectors of missing",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "ate robust multimodal joint representations under noisy incomplete"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "data to zero: in some situations, zero is a value that signifies â€œno",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "data and proceed with multimodal emotion recognition."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "informationâ€ or â€œlack of informationâ€ [39, 41], 2) discard data ran-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "â€¢ We explore a new method to simulate realistic data under vari-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "domly with a predefined probability: dropped data can be regarded",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "ous incomplete conditions utilizing a noise scheduler while enabling"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "as completely missing [5, 23]. On this basis, MER work in the face",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "a new condition with the incompletion of all three modalities."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "of\nincomplete data focuses on two main areas: 1) missing data",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "â€¢ Experimental results under various noise types and intensity"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "completion [4, 9, 23, 35] and 2) multimodal\njoint representation",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "conditions show that our NMER outperforms most baselines and"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "learning with available data [13, 32]. For example, Cai et al. [4]",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "demonstrates robustness in the face of incomplete data."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "proposed to use adversarial learning to generate missing modality",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "images. Zeng et al. [38] proposed an ensemble learning method to",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "2\nNMER: Methodology"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "use several models to solve the missing problem jointly. Zhao et al.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "2.1\nOverall Architecture"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[39] proposed a Missing Modality Imagination Network (MMIN)",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "The overall architecture of NMER is illustrated in Figure 2, which"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "using AutoEncoder and cycle consistency construction to learn",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "consists of 1) Noise Scheduler; 2) VAE-based Network; and 3) Clas-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "joint representations while predicting missing modalities. Zuo et",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "sifier. Specifically, we first employ our Noise Scheduler to add con-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "al.[41] proposed introducing the modality invariant feature into",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "figurable noise to original embeddings to get incomplete data and"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "MMIN to learn robust multimodal joint representations. Liu et al.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "send them into the VAE-based network. Then, the VAE-based Net-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[26] combined contrastive learning and invariant features to imag-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "work seeks to extract the useful features to reconstruct the robust"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ine the feature of the missing modality. The above work lays a solid",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "multimodal joint representation. At last, the joint representation"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "foundation for MER work under incomplete data. Note that the",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "will be fed into Classifier to predict the final emotional result."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "framework that combines both missing data completion and mul-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "timodal joint representation learning has become the mainstream",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "2.2\nNoise Scheduler"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "scheme in the research of this area.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "However, the above approach faces two main issues, including",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "2.2.1\nRationale and Noise Selection."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "1) the method of simulating incomplete data is unreasonable and",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "Inspired by the previous work [17], we can obtain a blurred pic-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "impractical: traditional data setup methods set parts of the feature",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "ture by gradually adding Gaussian noise to the picture. Similarly,"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "to zero vectors or discard some data directly at a set percentage.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "we can obtain a \"noisy embedding\" by adding noise to the embed-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "These methods neither reliably simulate real-world scenarios nor",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "ding [30, 33], which enhances the robustness of the model while"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "reserve the availability of incomplete data. Moreover, assuming the",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "simulating the modality incomplete condition. We examine two"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "data contains some type or level of noise, is not it better to try to",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "common noise types as examples: Gaussian noise, as described by"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "mine useful\ninformation from it rather than just discard it? and",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "C. F. Gauss in [11], and impulse noise, following the research of"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2) the structure is redundant: traditional model structure learns a",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "Kim et al. [20]"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "multimodal joint representation in the process of completing the",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "Gaussian noise is extensively employed to simulate errors and"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "missing data [41], and such a process tends to cause the errors",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "disturbances occurring in natural and technical processes due to"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "generated in the first step to have a negative impact on the second",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "its distinct statistical characteristics, namely a well-defined mean ğœ‡"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "step. Whether robust multimodal joint feature representations can",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "and standard deviation ğœ that conform to a normal distribution. The"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "be learned directly from noisy data?",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "selection of this type of noise is grounded in a crucial observation:"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "To answer the above two questions, we propose a novel Noise-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "deviations influenced by various factors tend to approximate a"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "robust MER model, termed NMER. Specifically, we design a noise",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "normal distribution in most natural and technological systems."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "scheduler at\nthe embedding level.\nIt creates noisy training and",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "Consequently, the use of Gaussian noise in practical applications"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "testing data by adding various types and intensities of noise to the",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "not only maintains statistical rigor but also ensures that models"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "embedding, thus simulating the influence of diverse incomplete",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "exhibit greater robustness and adaptability when confronted with"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "situations in realistic scenarios.\nIt\nis worth mentioning that we",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "the complexities of real-world data [2, 14]."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "explored an incomplete condition of three new modalities through",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "Impulse noise is another typical noise form that often appears"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "our noise scheduler, which was impossible to achieve in previous",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "in signal processing. Due to its ability to simulate brief and intense"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "works. Then, we present a Variational AutoEncoder (VAE)\n[22,",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "disturbances, it is widely used to model sudden anomalies in data"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "34] based multimodal\njoint representation learning network to",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "transmission or sensor inputs. Additionally, impulse noise can also"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "reconstruct robust multimodal joint representations from the noisy",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "be employed to enhance the capability of the model to recognize"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "data. In this way, we simulate incomplete data in realistic scenarios,",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "and process extreme data points [27, 29]."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "transition of data toward a noise-dominated state, where ğ‘‡ should"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "be an integer greater than zero. The ğ›½start and ğ›½end are fixed to"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "0.001 and 0.1 generally refer to previous work [17]. The embedding"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "at time step 0. We then use Algorithm 1 to generate\nis noted as ğ‘¬ ğ’‚"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "0"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "the noisy embedding ğ‘¬ ğ’‚"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "ğ‘» ."
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "Algorithm 1: Process for Generating Noisy Embedding ğ‘¬ ğ’‚"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "ğ‘»"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "Input: Original embedding ğ‘¬ ğ’‚\n0 , number of steps ğ‘‡ , noise"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "schedule parameters ğ›½1, ğ›½2, . . . , ğ›½ğ‘‡"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "for ğ‘¡ = 1 to ğ‘‡ do"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "Sample noise vector ğğ’• âˆ¼ N (0, 1);"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "Update ğ‘¬ ğ’‚\nğ’• âˆ’1 + âˆšï¸ğ›½ğ‘¡ ğğ’• ;\nğ‘»"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "end"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "Output: Noisy embedding ğ‘¬ ğ’‚"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "ğ‘»"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "However, this method is computationally intensive. To optimize"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "0 without iterating\nğ‘» from ğ‘¬ ğ’‚"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "through each intermediate step from ğ‘¬1 to ğ‘¬ğ‘» âˆ’1, we precompute"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": "the cumulative product of 1 âˆ’ ğ›½ terms, denoted as\nğ›¼ğ‘¡ :"
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        },
        {
          "Figure 2: The structure of our NMER model, including noise scheduler, VAE-based network, and the classifier. Lğ‘–ğ‘›ğ‘£ stands the": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2.2.3\nNoise Intensity Calculation.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "Noisy features are transformed into normal ones throughout"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "In the noisy embedding construction process, the ğ›½start, ğ›½end,",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "the compression, sampling, and reconstruction phases, which pro-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "and ğ‘‡ control the intensity of noise jointly. Therefore, we better",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "vide the denoising effect. The final output, the multimodal\njoint"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "quantify the noise intensity by calculating the signal-to-noise ratio",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "representation ğ‘ª, is then fed into the Classifier to derive the result."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "in other words,\nthe\n(SNR) between the original and noisy data,",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "intensity ratio between noise and raw data.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "2.4\nLoss Functions"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "âˆš\nâˆš",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ğ›¼ğ‘‡\nIn Equation 3, we regard\nand\n1 âˆ’ Â¯ğ›¼ğ‘‡\nas the power of",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "As shown in Fig. 2, the total loss L for NMER includes three parts:"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "the original data part and the noise part, separately. According to",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "L = ğœ†1Lgen + ğœ†2Linv + ğœ†3Lcls, where ğœ†ğ‘  are the balance factors."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Equation 2 and 5, itâ€™s easy to know that when ğœ2 is invariant, the",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "Generation loss Lgen aims to calculate the distance between the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "SNR value shares the opposite trend with ğ‘‡ growing. Hence we can",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "generation result ğ‘ª (from incomplete data) and the multimodal"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "obtain a higher level of noise by increasing the value of ğ‘‡ .",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "joint representation Ë†ğ‘ª (from complete data)."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "in Equa-\nNote that the Lgen consists of two items, that are Lkl"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ğ‘ƒğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™\nğ›¼ğ‘‡ ğœ2",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "tion 6 and Lmse in Equation 7. Lkl aims to make the hidden vari-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "=\nSNR =\n(4)",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ğ‘ƒğ‘›ğ‘œğ‘–ğ‘ ğ‘’\n1 âˆ’ Â¯ğ›¼ğ‘‡",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "ables generated by the encoder conform to the standard normal"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "distribution, while Lmse seeks to make the generated multi-modal"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "where the ğœ2 is the variance of the original data. More intuitively,",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "joint representation more similar to the target that extracted from"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "SNR is always represented in decibels(dB):",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "complete data,"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "(cid:19)\n(cid:18)\n(cid:19)\n(cid:18) ğ‘ƒğ‘œğ‘Ÿğ‘–ğ‘”ğ‘–ğ‘›ğ‘ğ‘™",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "(cid:16)\n(cid:17)"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ğ›¼ğ‘‡ ğœ2",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": "Table 1: Emotion distribution and division of two datasets."
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": "Label"
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": "Happy"
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": "Angry"
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": "Sad"
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": "Neutral"
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": "Positive"
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": ""
        },
        {
          "both widely used in the research of MER.": "Negative"
        },
        {
          "both widely used in the research of MER.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "-10dB (Avg)": "Impulse noise",
          "-20dB (Avg)": "Impulse noise",
          "-30dB (Avg)": "Impulse noise",
          "-40dB (Avg)": ""
        },
        {
          "-10dB (Avg)": "WA",
          "-20dB (Avg)": "WA",
          "-30dB (Avg)": "WA",
          "-40dB (Avg)": "WA"
        },
        {
          "-10dB (Avg)": "0.7448",
          "-20dB (Avg)": "0.7026",
          "-30dB (Avg)": "0.6758",
          "-40dB (Avg)": "0.6348"
        },
        {
          "-10dB (Avg)": "0.7400",
          "-20dB (Avg)": "0.7270",
          "-30dB (Avg)": "0.7112",
          "-40dB (Avg)": "0.6978"
        },
        {
          "-10dB (Avg)": "0.7750",
          "-20dB (Avg)": "0.7521",
          "-30dB (Avg)": "0.7397",
          "-40dB (Avg)": "0.7226"
        },
        {
          "-10dB (Avg)": "0.7738",
          "-20dB (Avg)": "0.7589",
          "-30dB (Avg)": "0.7382",
          "-40dB (Avg)": "0.7343"
        },
        {
          "-10dB (Avg)": "0.7773",
          "-20dB (Avg)": "0.7651",
          "-30dB (Avg)": "0.7304",
          "-40dB (Avg)": "0.7391"
        },
        {
          "-10dB (Avg)": "0.7509",
          "-20dB (Avg)": "0.7467",
          "-30dB (Avg)": "0.7136",
          "-40dB (Avg)": "0.7133"
        },
        {
          "-10dB (Avg)": "0.7630",
          "-20dB (Avg)": "0.7489",
          "-30dB (Avg)": "0.7202",
          "-40dB (Avg)": "0.7158"
        },
        {
          "-10dB (Avg)": "0.7341",
          "-20dB (Avg)": "0.7010",
          "-30dB (Avg)": "0.6539",
          "-40dB (Avg)": "0.6383"
        },
        {
          "-10dB (Avg)": "0.7455",
          "-20dB (Avg)": "0.7309",
          "-30dB (Avg)": "0.7180",
          "-40dB (Avg)": "0.7007"
        },
        {
          "-10dB (Avg)": "0.7713",
          "-20dB (Avg)": "0.7580",
          "-30dB (Avg)": "0.7331",
          "-40dB (Avg)": "0.7277"
        },
        {
          "-10dB (Avg)": "0.7701",
          "-20dB (Avg)": "0.7522",
          "-30dB (Avg)": "0.735",
          "-40dB (Avg)": "0.7393"
        },
        {
          "-10dB (Avg)": "0.7744",
          "-20dB (Avg)": "0.7588",
          "-30dB (Avg)": "0.7361",
          "-40dB (Avg)": "0.7380"
        },
        {
          "-10dB (Avg)": "0.7621",
          "-20dB (Avg)": "0.7483",
          "-30dB (Avg)": "0.7151",
          "-40dB (Avg)": "0.6971"
        },
        {
          "-10dB (Avg)": "0.7709",
          "-20dB (Avg)": "0.7511",
          "-30dB (Avg)": "0.7272",
          "-40dB (Avg)": "0.7045"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "sample is subjected to a randomly selected incomplete condition.",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "3.3\nExperimental Setup"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "In contrast, the test set consists of 557 unique samples. Because of",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "On the IEMOCAP dataset, we follow research [39, 41] to extract the"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "the small number of test samples, each sample will be evaluated",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "original embeddings ğ‘¬ ğ’‚, ğ‘¬ ğ’—, and ğ‘¬ ğ’ . The audio, visual, and lexical"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "under six predefined incomplete conditions, resulting in a total of",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "embeddings are 130-dim OpenSMILE features with the configuration"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "3342 test samples. In the CMU-MOSEI dataset, both train and test",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "of IS13_ComParE [10], Denseface [19] embeddings extracted by a"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "samples are randomly influenced by one incomplete condition, the",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "pre-trained DenseNet model of 342 dimensions, and 1024-dim BERT"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "same as the train set in the IEMOCAP dataset.",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "[8] word embeddings, respectively."
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "The only difference in data with previous works is that incom-",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "On the CMU-MOSEI dataset, we employ the feature extraction"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "plete samples are created with our noise scheduler. Besides, the",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "method from the work of Liang et al. [24] Audio features are 74"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": ") will\nğ‘» , ğ‘¬ ğ’—\nğ‘» , ğ‘¬ ğ’\nğ‘»",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "dimensions extracted using COVAREP [6], while visual embed-"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "be carried out individually to show the impact on the data in detail.",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "dings are 35 dimensions derived from the pool5 layer of an Im-"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "for each video frame,\nageNet [7]-trained ResNet-152 [16] model"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "which underwent preprocessing steps including resizing, center"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "cropping, and normalizing. Facial expression features are obtained"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "3.2\nNew Incomplete Condition",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "using the OpenFace [10] tool. Lexical features are represented using"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "300-dimensional GloVe word vectors [31]. We adjust the balance"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": ") utilizing\nğ‘» , ğ‘¬ ğ’—\nğ‘» , ğ‘¬ ğ’",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "ğ‘»",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "factors ğœ†ğ‘ \nto 1, 10, and 1 for scaling the losses accordingly."
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "the noise scheduler, which represents the addition of noise to all",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "The hidden size of the LSTM structure is set to 128. The TextCNN"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "three modalities. This condition was not previously considered in",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "contains 3 convolution blocks with kernel sizes of 3, 4, 5 and an"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "existing research.\nIn prior methodologies, once one modality is",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "output size of 128. The output size of the Invariance Encoder is"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "regarded as incomplete or missing, they will drop the data of this",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "also set to 128. The VAE Module includes a Transformer Encoder"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "modality or set it to zero vectors, rendering the analysis of all three",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "of 5 layers, 768 dimensions, and 16 heads as the encoder while"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "modalitiesâ€™ incompletion as both impractical and meaningless. But",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "Linear layers with dimensions of\n{128, 256, 384} as the decoder."
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "in real-world scenarios, this kind of incompletion is not uncommon,",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "The classifier contains three linear layers of size {384, 128, 4}. For"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "and the incompletion level also varies. The controllable intensity",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": "the noise intensity, we conduct experiments on SNRdB of [-10dB,"
        },
        {
          "0.7521\n0.6732\n0.7709\n0.7034\n0.7387\n0.6713\nw/o Linv": "of noise allows us to report results under this extreme condition.",
          "0.7511": "",
          "0.6732\n0.7261\n0.6659\n0.7272\n0.6419\n0.7230\n0.6336\n0.7045\n0.6223": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "-20dB, -30dB, -40dB]. For impulse noise, the appearance frequency",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "3.5\nResults"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ğ‘ is set at 0.3, with other parameters remaining unchanged. All",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "3.5.1\nResults About Models."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "experiments are run on an NVIDIA A100 80GB GPU.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "The average results of the comparative studies, across six in-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "We utilize the AdamW [28] as the optimizer and use the Lambda",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "complete conditions, four noise intensities and two noise types,"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "LR [36] to dynamically update the learning rate. The initial learning",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "are shown in Table 2, including {a}, {v}, {l}, {a, v}, {a, l}, {v, l},1, with"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "rate is 0.0001. The batch size is 128 and the dropout rate is 0.5. We",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "Gaussian and impulse noise at four levels of intensities."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "run experiments with 10-fold cross-validation, where each fold",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "1) The NMER model outperforms most baseline models across"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "contains 60 epochs, and report the result on the test set. Each result",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "various noise intensities and types. For instance, under -10dB Gauss-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "is run three times and averaged to reduce the effect of random",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "ian noise in the IEMOCAP dataset, NMERâ€™s WA is 0.7598 compared"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "initialization of parameters. We employ the same evaluation metrics",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "to 0.7120 for MEN, 0.7215 for MCTN, 0.7551 for MMIN, and 0.7543"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "as those used in previous works [39], [41], Weighted Accuracy (WA)",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "for IF-MMIN. UA results show a similar pattern."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "[1] and Unweighted Accuracy (UA) [12], to assess various systems.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "2) There is a notable difference in accuracy (mainly in high-level"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "noise conditions, for example -40dB, about 6% - 10%) between the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "complete-modality baseline (MEN) and the incomplete-modality"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "3.4\nComparison and Ablation Study",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "baselines (MCTN, MMIN, and IF-MMIN) as shown in Table 2, which"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "In our study, we benchmark our NMER model against four advanced",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "indicates the importance of using incomplete training data."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "MER baselines to establish its relative performance.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "3) The results of our model on both datasets under six incomplete"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "1) Modality Encoder Network (MEN) [39]: This model serves as",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "conditions are detailed in Table 3 (taking Gaussian noise as an ex-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "the complete-modality baseline. MEN is trained under complete",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "ample), the WA values for single clean modality conditions {ğ‘} and"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "modality conditions and tested on incomplete modality conditions.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "{ğ‘£} drop by about 9% with the noise intensity increases from -10dB"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "2) MCTN [32]: MCTN uses translation-based method with cycle",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "to -40dB, whereas in two clean modalities conditions {ğ‘, ğ‘£}, {ğ‘£, ğ‘™},"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "consistency loss to learn joint representations between every two",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "and {ğ‘£, ğ‘™}, the decrease is slower, ranging from about 2% - 4%. When"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "modalities in multimodal data, which is used as a popular method.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "modality ğ‘ or ğ‘£ is noise-influenced, the performance decreases are"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "3) MMIN [39]: This model employs a cascade residual AutoEn-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "roughly equivalent. This indicates that the resistance of the model"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "coder coupled with cycle consistency construction to learn joint",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "to noise worsens significantly when only one modality is noise-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "representations, particularly for predicting missing modalities.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "free. However, the model shows relative robustness when the two"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "IF-MMIN inte-\n4) IF-MMIN [41]: An enhancement of MMIN.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "modalities are noise-free. Notably, modality ğ‘™ plays a critical role"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "grates the modality invariant feature to learn robust joint represen-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "in maintaining performance even when other modalities are com-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "tations and is recognized as a state-of-the-art incomplete modalities",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "promised by noise. The WA value decreased about 4% when only"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "multimodal emotion recognition system. MCTN, MMIN, and IF-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "is clean, and about 2% when two modalities including ğ‘™\nmodality l"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "MMIN are categorized as incomplete-modality baselines, with",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "are noise-free (condition {ğ‘£, ğ‘™} and {ğ‘£, ğ‘™})."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "training and testing both under incomplete modality conditions.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "4) The WA and UA results on the CMU-MOSEI dataset, as shown"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Our experimental design not only evaluates the capability of the",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "in Table 2, exhibit different trends and absolute values compared to"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "multimodal emotion recognition systems but also demonstrates the",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "those on the IEMOCAP dataset, primarily due to the imbalanced"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "fine-grained control offered by our noise scheduler. We first conduct",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "label distribution (as shown in Table 1).\nIn such scenarios, clas-"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "experiments on traditional six incomplete conditions, where the",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "sifiers are inclined to categorize samples into the more samples"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "noise intensity progressively increases from -10dB to -40dB. Besides,",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "class because this strategy statistically enhances their accuracy"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "we design the experiment on the brand new incompletion condition",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "performance. Because of the way the WA value is calculated, if the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "ğ‘» , ğ‘¬ ğ’—\nğ‘» , ğ‘¬ ğ’\nğ‘» ), to explore and evaluate the potential of models under a",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "classification performance of the larger number samples class is"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "more complex noise environment. Moreover, we launched a simple",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "good, then even if the noise increases, the weighted accuracy (WA)"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "benchmark test on the IEMOCAP dataset that directly sends uni-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "may not decrease significantly as long as the prediction for that"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "modality data with various intensities of noise to the classifier and",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "class remains accurate. Because that class contributes more to the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "reports the result. This test aims to establish a benchmark for the",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "overall accuracy, the decline in the accuracy of fewer sample class"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "subsequent incomplete modality multimodal emotion recognition",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "is offset. These reasons result in the different variation trends of"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "technology, that is, how much it has improved on the basis of a",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "WA and UA values."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "single modality data and its various types of incompletion.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "Additionally, ablation studies are performed to ascertain the con-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "3.5.2\nResults About the Noise Scheduler."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "tribution of the VAE model and the importance of the invariant fea-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "1) It is evident that the performance of all models deteriorates"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "tures to the overall model performance. In the study, labeled as â€œw/o",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "as noise intensity increases, which aligns with the expectation"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "VAEâ€, we remove the VAE model. Instead, we directly concatenate",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "that increasing noise levels obscure the original dataâ€™s information."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "the extracted emotion-specific feature ğ’‰â€² and the emotion-invariant",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "For instance,\nin Table 2, the WA of the IF-MMIN system on the"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "feature ğ‘¯ â€², then feed this representation into the classifier. This",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "IEMOCAP dataset under Gaussian noise declines from 0.7543 at"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "study is designed to highlight the significant role of the VAE model",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "-10dB to 0.7048 at -40dB. These results highlight the effectiveness"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "in reconstructing the multimodal joint representation from noisy",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "of our noise scheduler, which allows for precise control over noise"
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "features. Another ablation study that removed the invariant loss, la-",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "intensities and the simulation of various noise conditions."
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "beled as â€œw/o Linvâ€, aims to demonstrate the invariant features can",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": ""
        },
        {
          "MRAC â€™24, November 1, 2024, Melbourne, VIC, Australia.": "help guide the VAE model to generate more exact representations.",
          "Qi Fan, Haolin Zuo, Rui Liu, Zheng Lian & Guanglai Gao": "1{Â·} means the clean modality. The noisy modalities are omitted."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Emotion Recognition under Incomplete Data Scenarios"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Table 3: The detailed results of our NMER model under the six incomplete conditions utilizing Gaussian noise. â€œaâ€ means that"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": ""
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "conditions."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "a": "",
          "v": "",
          "l": "",
          "a, v": "",
          "a, l": "",
          "v, l": "",
          "Avg": ""
        },
        {
          "a": "WA\nUA",
          "v": "WA\nUA",
          "l": "WA\nUA",
          "a, v": "WA\nUA",
          "a, l": "WA\nUA",
          "v, l": "WA\nUA",
          "Avg": "WA\nUA"
        },
        {
          "a": "0.7275 0.7349",
          "v": "0.7440 0.7503",
          "l": "0.7750 0.7820",
          "a, v": "0.7432 0.7508",
          "a, l": "0.7813 0.7917",
          "v, l": "0.7877 0.7952",
          "Avg": "0.7598 0.7675"
        },
        {
          "a": "0.6752 0.6873",
          "v": "0.6911 0.6949",
          "l": "0.7623 0.7735",
          "a, v": "0.7068 0.7178",
          "a, l": "0.7720 0.7846",
          "v, l": "0.7771 0.7864",
          "Avg": "0.7307 0.7406"
        },
        {
          "a": "",
          "v": "",
          "l": "",
          "a, v": "",
          "a, l": "",
          "v, l": "",
          "Avg": ""
        },
        {
          "a": "0.6574 0.6626",
          "v": "0.6715 0.6688",
          "l": "0.7513 0.7592",
          "a, v": "0.7021 0.7071",
          "a, l": "0.7649 0.7778",
          "v, l": "0.7712 0.7771",
          "Avg": "0.7197 0.7254"
        },
        {
          "a": "0.6347 0.6484",
          "v": "0.6529 0.6511",
          "l": "0.7348 0.7484",
          "a, v": "0.6978 0.7024",
          "a, l": "0.7632 0.7777",
          "v, l": "0.7672 0.7773",
          "Avg": "0.7085 0.7176"
        },
        {
          "a": "0.7274 0.5905",
          "v": "0.7262 0.6204",
          "l": "0.7787 0.7362",
          "a, v": "0.7366 0.6187",
          "a, l": "0.8030 0.7507",
          "v, l": "0.7856 0.7383",
          "Avg": "0.7596 0.6760"
        },
        {
          "a": "0.7030 0.5644",
          "v": "0.6924 0.5885",
          "l": "0.7783 0.7317",
          "a, v": "0.7084 0.5965",
          "a, l": "0.7982 0.7474",
          "v, l": "0.7922 0.7395",
          "Avg": "0.7454 0.6617"
        },
        {
          "a": "",
          "v": "",
          "l": "",
          "a, v": "",
          "a, l": "",
          "v, l": "",
          "Avg": ""
        },
        {
          "a": "0.7099 0.5497",
          "v": "0.6959 0.5690",
          "l": "0.7837 0.7394",
          "a, v": "0.7094 0.5859",
          "a, l": "0.8012 0.7453",
          "v, l": "0.7924 0.7380",
          "Avg": "0.7487 0.6549"
        },
        {
          "a": "0.7120 0.5579",
          "v": "0.6872 0.5694",
          "l": "0.7813 0.7263",
          "a, v": "0.6973 0.5844",
          "a, l": "0.8096 0.7456",
          "v, l": "0.8017 0.7412",
          "Avg": "0.7482 0.6543"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "0.7120 0.5579\n0.6872 0.5694\n0.7813 0.7263\n-40dB",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "0.6973 0.5844\n0.8096 0.7456\n0.8017 0.7412\n0.7482 0.6543"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "Table 4: The WA and UA declining on uni-modality testing.",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "importance of multimodal fusion strategies. This table emphasizes"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "the necessity and superiority of multimodal joint representation"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "learning at the same time. When one mode is seriously disturbed,"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "a\nv\nl",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "Intensity",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "other modes may still be able to maintain good recognition perfor-"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "WA\nUA\nWA\nUA\nWA\nUA",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "mance, thus improving the overall emotion recognition accuracy."
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "3.5.3\nResults About New Incomplete Condition."
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "0.6693 0.6776\n0.5723 0.5560\n0.6484 0.6592\n0dB",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "The result of the new incomplete condition is listed in Table 5."
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "0.5633 0.5588\n0.4252 0.4107\n0.5421 0.5415\n-10dB",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "1) The introduction of scenarios with full-modality noise interfer-"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "ence in multimodal emotion recognition presents a new challenge,"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "0.4769 0.4879\n0.3941 0.3780\n0.4665 0.4644\n-20dB",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "reflecting complex real-world environments where multiple infor-"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "0.4483 0.4651\n0.3624 0.3443\n0.3867 0.3878\n-30dB",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "mation sources can simultaneously experience quality degradation."
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "This expands the boundaries of current research. Traditionally, mod-"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "0.4161 0.4182\n0.3502 0.3363\n0.3795 0.3333\n-40dB",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "els could rely on at least one clean modality. However, this new"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "setup eliminates such possibilities, demanding greater robustness"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "and denoising capability from the models themselves. With noise"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "2) From the accuracy declining and the noise intensity increasing",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "affecting all modalities, the accuracy significantly decreases. For"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "process in Table 2, it becomes crucial to recognize the shortcomings",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "instance, in Table 5, on the IEMOCAP dataset, the WA value of the"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "of previous studies that used zero vectors or drops data to simulate",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "MMIN model falls by about 4% from -10dB to -20dB noise levels and"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "noise. These methods lack rationality and precise control over noise",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "further declines by approximately 13% at -40dB. It indicates that"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "intensity and fail to enable meaningful comparisons across different",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "the traditional multimodal fusion strategy may not be able to effec-"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "noise types and intensities. In contrast, the experiments demon-",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "tively deal with the simultaneous modal degradation, exposing the"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "strate the utility of our method in managing noises, achieving more",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "limitations of existing techniques against cross-modal interference."
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "comprehensive evaluations of the models.",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "2) There is a noticeable acceleration in performance degradation"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "3) In Table 4, we test the uni-modality result on the IEMOCAP",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "as noise intensity increases, which may be caused by the non-linear"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "dataset using Gaussian noise. The results in the first row show that",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "nature of emotion data and the complex response of models to vary-"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "different modalities have different quantities of information and",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "ing noise levels. This information loss complicates the interaction"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "difficulties of recognition. The performance of different modalities",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "between different modalities. Notably, all models demonstrate a"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "at the same noise level shows that different data types (audio, video,",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "similar trend under this condition. Future research should thus con-"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "lexical) have different sensitivity to ambient noise. For example, the",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "centrate on studying how noise variations affect data and exploring"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "video (ğ‘£) modality has a greater performance degradation at -10dB",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "more strategies to mitigate the impact of diverse noise."
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "(about 15%) than the audio (ğ‘) and lexical (ğ‘™) modalities (about 10%).",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": ""
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "the\nWith the incremental noise intensities (from 0dB to -40dB),",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "3.5.4\nAblation Results."
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "accuracy of each modality has its own downtrend. This downward",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "The result of the last two rows in Table 2 shows the crucial role of"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "trend highlights the difficulty of effective emotion recognition in",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "the VAE model and the invariant featuresâ€™ guidance. For example,"
        },
        {
          "0.7099 0.5497\n0.6959 0.5690\n0.7837 0.7394\n-30dB": "high-noise environments, while also providing data support for the",
          "0.7094 0.5859\n0.8012 0.7453\n0.7924 0.7380\n0.7487 0.6549": "in the IEMOCAP dataset, under Gaussian noise, the WA of two"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": "Dataset"
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": "IEMOCAP"
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": "CMU-MOSEI"
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        },
        {
          "Table 5: The results of various systems utilizing Gaussian noise and adapting the new noise condition, the Impulse noise": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "Ours\n0.7651\n0.6842\n0.7570",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "0.6513\n0.7341 0.6181\n0.7266 0.5735"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "ablation studies achieves 0.7382 and 0.7401 on the noise intensity",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "Cultivation Project of Inner Mongolia University (21221505), the"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "-10dB whereas the NMER achieves 0.7598. The outcomes clearly",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "fund of Supporting the Reform and Development of Local Universi-"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "indicate the inclusion of the contributions that these parts provided",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "ties (Disciplinary Construction) and the special research project of"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "to the overall performance of our NMER model.",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "First-class Discipline of Inner Mongolia A. R. of China under Grant"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "(YLXKZX-ND-036). College of Computer Science, Inner Mongolia"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "University, National & Local Joint Engineering Research Center of"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "4\nConclusion",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": ""
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "Intelligent Information Processing Technology for Mongolian, and"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "This work proposed a Noise-robust Multimodal Emotion Recogni-",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": ""
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "tion model (NMER) that effectively mitigates the impact of incom-",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": ""
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "gence Technology provided equipment support to this work."
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "plete data and reconstructs the robust multimodal joint represen-",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": ""
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "tations from incomplete data. Experimental results show that Our",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": ""
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "References"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "noise scheduler can effectively create different types and intensities",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": ""
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": "[1]\nIshwar Baidari and Nagaraj Honnikoll. 2020. Accuracy weighted diversity-based"
        },
        {
          "0.7680 0.6893\n0.7588\nIF-MMIN": "of noisy data to simulate various noise corruptions. Our NMER",
          "0.6467\n0.7327\n0.6170\n0.7240\n0.5706": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "gence Technology provided equipment support to this work."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "References"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "[1]\nIshwar Baidari and Nagaraj Honnikoll. 2020. Accuracy weighted diversity-based"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "https://doi.org/10.1016/j.\nonline boosting. Expert Syst. Appl. 160 (2020), 113723."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "eswa.2020.113723"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "[2] Christopher M Bishop and Nasser M Nasrabadi. 2006. Pattern recognition and"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "machine learning. Vol. 4. Springer."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "[3] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower,"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "Samuel Kim, Jeannette N Chang, Sungbok Lee, and Shrikanth S Narayanan. 2008."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "IEMOCAP:\nInteractive emotional dyadic motion capture database.\nLanguage"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "resources and evaluation 42, 4 (2008), 335â€“359."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "[4]\nLei Cai, Zhengyang Wang, Hongyang Gao, Dinggang Shen, and Shuiwang Ji."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "2018. Deep adversarial\nlearning for multi-modality missing data completion."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "In Proceedings of the 24th ACM SIGKDD international conference on knowledge"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "discovery & data mining. 1158â€“1166."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "[5]\nJiayi Chen and Aidong Zhang. 2020. Hgmf: heterogeneous graph-based fusion"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "for multimodal data with incompleteness. In Proceedings of the 26th ACM SIGKDD"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "international conference on knowledge discovery & data mining. 1295â€“1305."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "[6] Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "2014. COVAREPâ€”A collaborative voice analysis repository for speech technolo-"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "gies. In 2014 ieee international conference on acoustics, speech and signal processing"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "(icassp). IEEE, 960â€“964."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "[7]\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet:"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "A large-scale hierarchical image database. In 2009 IEEE conference on computer"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "vision and pattern recognition. Ieee, 248â€“255."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "[8]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "Pre-training of Deep Bidirectional Transformers for Language Understanding. In"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "NAACL-HLT (1). Association for Computational Linguistics, 4171â€“4186."
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "[9] Changde Du, Changying Du, Hao Wang,\nJinpeng Li, Wei-Long Zheng, Bao-"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "Liang Lu, and Huiguang He. 2018. Semi-supervised deep generative modelling"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": ""
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "of incomplete multi-modality emotional data. In Proceedings of the 26th ACM"
        },
        {
          "Inner Mongolia Key Laboratory of Multilingual Artificial Intelli-": "international conference on Multimedia. 108â€“116."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Emotion Recognition under Incomplete Data Scenarios"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[10]\nFlorian Eyben, Martin WÃ¶llmer, and BjÃ¶rn Schuller. 2010. Opensmile: the munich"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "versatile and fast open-source audio feature extractor. In Proceedings of the 18th"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "ACM international conference on Multimedia. 1459â€“1462."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[11] Carl Friedrich Gauss. 1877. Theoria motus corporum coelestium in sectionibus"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "conicis solem ambientium. Vol. 7. FA Perthes."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[12]\nShruti Gupta, Md. Shah Fahad, and Akshay Deepak. 2020. Pitch-synchronous"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "single frequency filtering spectrogram for speech emotion recognition. Multim."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "https://doi.org/10.1007/s11042-020-\nTools Appl. 79, 31-32 (2020), 23347â€“23365."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "09068-1"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[13]\nJing Han, Zixing Zhang, Zhao Ren, and BjÃ¶rn Schuller. 2019.\nImplicit fusion by"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "joint audiovisual training for emotion recognition in mono modality. In ICASSP"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "(ICASSP). IEEE, 5861â€“5865."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[14] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "2009. The elements of statistical learning: data mining, inference, and prediction."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Vol. 2. Springer."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[15] Devamanyu Hazarika, Roger Zimmermann, and Soujanya Poria. 2020. Misa:"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Modality-invariant and-specific representations for multimodal sentiment analy-"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "sis. In Proceedings of the 28th ACM international conference on multimedia. 1122â€“"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "1131."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "learning for image recognition. In Proceedings of the IEEE conference on computer"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "vision and pattern recognition. 770â€“778."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\n[17]"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "models. Advances in neural information processing systems 33 (2020), 6840â€“6851."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[18]\nSepp Hochreiter and JÃ¼rgen Schmidhuber. 1997.\nLong Short-Term Memory."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Neural Comput. 9, 8 (1997), 1735â€“1780."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[19] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "2017. Densely Connected Convolutional Networks. In 2017 IEEE Conference on"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "https://doi.org/10.1109/CVPR.2017.243\n21-26, 2017. 2261â€“2269."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Seong Rag Kim and Adam Efron. 1995. Adaptive robust impulse noise filtering.\n[20]"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "IEEE Transactions on Signal Processing 43, 8 (1995), 1855â€“1866."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[21] Yoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "EMNLP. ACL, 1746â€“1751."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[22] Diederik P. Kingma and Max Welling. 2014. Auto-Encoding Variational Bayes. In"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "ICLR."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[23] Zheng Lian, Lan Chen, Licai Sun, Bin Liu, and Jianhua Tao. 2023. GCNet: graph"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "completion network for incomplete multimodal learning in conversation.\nIEEE"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Transactions on Pattern Analysis and Machine Intelligence (2023)."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Paul Pu Liang, Yiwei Lyu, Xiang Fan, Zetian Wu, Yun Cheng, Jason Wu, Leslie Yu-\n[24]"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "fan Chen, Peter Wu, Michelle A Lee, Yuke Zhu, et al. 2021. MultiBench: Multiscale"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Benchmarks for Multimodal Representation Learning. In Thirty-fifth Conference"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "on Neural Information Processing Systems Datasets and Benchmarks Track (Round"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "1)."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[25] Rui Liu, Berrak Sisman, BjÃ¶rn W. Schuller, Guanglai Gao, and Haizhou Li. 2022."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "Data-Driven Deep Learning. In Interspeech 2022, 23rd Annual Conference of the"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "International Speech Communication Association, Incheon, Korea, 18-22 September"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "2022. ISCA, 5493â€“5497."
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "[26] Rui Liu, Haolin Zuo, Zheng Lian, Bjorn W Schuller, and Haizhou Li. 2024. Con-"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "trastive Learning based Modality-Invariant Feature Acquisition for Robust Mul-"
        },
        {
          "Learning Noise-Robust Joint Representation for Multimodal": "timodal Emotion Recognition with Missing Modalities.\nIEEE Transactions on"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Accuracy weighted diversity-based online boosting",
      "authors": [
        "Ishwar Baidari",
        "Nagaraj Honnikoll"
      ],
      "year": "2020",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/j.eswa.2020.113723"
    },
    {
      "citation_id": "2",
      "title": "Pattern recognition and machine learning",
      "authors": [
        "M Christopher",
        "Bishop",
        "M Nasser",
        "Nasrabadi"
      ],
      "year": "2006",
      "venue": "Pattern recognition and machine learning"
    },
    {
      "citation_id": "3",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "4",
      "title": "Deep adversarial learning for multi-modality missing data completion",
      "authors": [
        "Lei Cai",
        "Zhengyang Wang",
        "Hongyang Gao",
        "Dinggang Shen",
        "Shuiwang Ji"
      ],
      "year": "2018",
      "venue": "Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "5",
      "title": "Hgmf: heterogeneous graph-based fusion for multimodal data with incompleteness",
      "authors": [
        "Jiayi Chen",
        "Aidong Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery & data mining"
    },
    {
      "citation_id": "6",
      "title": "COVAREP-A collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "2014 ieee international conference on acoustics, speech and signal processing (icassp)"
    },
    {
      "citation_id": "7",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "Imagenet: A large-scale hierarchical image database"
    },
    {
      "citation_id": "8",
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT (1)"
    },
    {
      "citation_id": "9",
      "title": "Semi-supervised deep generative modelling of incomplete multi-modality emotional data",
      "authors": [
        "Changde Du",
        "Changying Du",
        "Hao Wang",
        "Jinpeng Li",
        "Wei-Long Zheng",
        "Bao-Liang Lu",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Proceedings of the 26th ACM international conference on Multimedia"
    },
    {
      "citation_id": "10",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Theoria motus corporum coelestium in sectionibus conicis solem ambientium",
      "authors": [
        "Carl Friedrich"
      ],
      "year": "1877",
      "venue": "Theoria motus corporum coelestium in sectionibus conicis solem ambientium"
    },
    {
      "citation_id": "12",
      "title": "Pitch-synchronous single frequency filtering spectrogram for speech emotion recognition",
      "authors": [
        "Md Shruti Gupta",
        "Akshay Shah Fahad",
        "Deepak"
      ],
      "year": "2020",
      "venue": "Multim. Tools Appl",
      "doi": "10.1007/s11042-020-09068-1"
    },
    {
      "citation_id": "13",
      "title": "Implicit fusion by joint audiovisual training for emotion recognition in mono modality",
      "authors": [
        "Jing Han",
        "Zixing Zhang",
        "Zhao Ren",
        "BjÃ¶rn Schuller"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "The elements of statistical learning: data mining, inference, and prediction",
      "authors": [
        "Trevor Hastie",
        "Robert Tibshirani",
        "Jerome Friedman",
        "Jerome Friedman"
      ],
      "year": "2009",
      "venue": "The elements of statistical learning: data mining, inference, and prediction"
    },
    {
      "citation_id": "15",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Devamanyu Hazarika",
        "Roger Zimmermann",
        "Soujanya Poria"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "16",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "17",
      "title": "Denoising diffusion probabilistic models",
      "authors": [
        "Jonathan Ho",
        "Ajay Jain",
        "Pieter Abbeel"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Long Short-Term Memory",
      "authors": [
        "Sepp Hochreiter",
        "JÃ¼rgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Comput"
    },
    {
      "citation_id": "19",
      "title": "Densely Connected Convolutional Networks",
      "authors": [
        "Gao Huang",
        "Zhuang Liu",
        "Laurens Van Der Maaten",
        "Kilian Weinberger"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2017.243"
    },
    {
      "citation_id": "20",
      "title": "Adaptive robust impulse noise filtering",
      "authors": [
        "Seong Rag",
        "Adam Efron"
      ],
      "year": "1995",
      "venue": "IEEE Transactions on Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Convolutional Neural Networks for Sentence Classification",
      "authors": [
        "Yoon Kim"
      ],
      "year": "2014",
      "venue": "EMNLP. ACL"
    },
    {
      "citation_id": "22",
      "title": "Auto-Encoding Variational Bayes",
      "authors": [
        "P Diederik",
        "Max Kingma",
        "Welling"
      ],
      "year": "2014",
      "venue": "Auto-Encoding Variational Bayes"
    },
    {
      "citation_id": "23",
      "title": "GCNet: graph completion network for incomplete multimodal learning in conversation",
      "authors": [
        "Zheng Lian",
        "Lan Chen",
        "Licai Sun",
        "Bin Liu",
        "Jianhua Tao"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "24",
      "title": "MultiBench: Multiscale Benchmarks for Multimodal Representation Learning",
      "authors": [
        "Paul Pu Liang",
        "Yiwei Lyu",
        "Xiang Fan",
        "Zetian Wu",
        "Yun Cheng",
        "Jason Wu",
        "Leslie Yufan Chen",
        "Peter Wu",
        "Michelle Lee",
        "Yuke Zhu"
      ],
      "year": "2021",
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "25",
      "title": "Accurate Emotion Strength Assessment for Seen and Unseen Speech Based on Data-Driven Deep Learning",
      "authors": [
        "Rui Liu",
        "Berrak Sisman",
        "BjÃ¶rn Schuller",
        "Guanglai Gao",
        "Haizhou Li"
      ],
      "year": "2022",
      "venue": "Interspeech 2022, 23rd Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "26",
      "title": "Contrastive Learning based Modality-Invariant Feature Acquisition for Robust Multimodal Emotion Recognition with Missing Modalities",
      "authors": [
        "Rui Liu",
        "Haolin Zuo",
        "Zheng Lian",
        "Bjorn Schuller",
        "Haizhou Li"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Impulsive noise recovery and elimination: A sparse machine learning based approach",
      "authors": [
        "Sicong Liu",
        "Liang Xiao",
        "Lianfen Huang",
        "Xianbin Wang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Vehicular Technology"
    },
    {
      "citation_id": "28",
      "title": "Decoupled Weight Decay Regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "29",
      "title": "A comprehensive survey on impulse and Gaussian denoising filters for digital images",
      "authors": [
        "Mehdi Mafi",
        "Harold Martin",
        "Mercedes Cabrerizo",
        "Jean Andrian",
        "Armando Barreto",
        "Malek Adjouadi"
      ],
      "year": "2019",
      "venue": "Signal Processing"
    },
    {
      "citation_id": "30",
      "title": "Adversarial Training Methods for Semi-Supervised Text Classification",
      "authors": [
        "Takeru Miyato",
        "Andrew Dai",
        "Ian Goodfellow"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "31",
      "title": "Glove: Global vectors for word representation",
      "authors": [
        "Jeffrey Pennington",
        "Richard Socher",
        "Christopher Manning"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "32",
      "title": "Found in translation: Learning robust joint representations by cyclic translations between modalities",
      "authors": [
        "Hai Pham",
        "Paul Liang",
        "Thomas Manzini",
        "Louis-Philippe Morency",
        "BarnabÃ¡s PÃ³czos"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Creating artificial neural networks that generalize",
      "authors": [
        "Jocelyn Sietsma",
        "Robert Jf Dow"
      ],
      "year": "1991",
      "venue": "Neural networks"
    },
    {
      "citation_id": "34",
      "title": "Learning structured output representation using deep conditional generative models",
      "authors": [
        "Kihyuk Sohn",
        "Honglak Lee",
        "Xinchen Yan"
      ],
      "year": "2015",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "35",
      "title": "Metric Learning on Healthcare Data with Incomplete Modalities",
      "authors": [
        "Qiuling Suo",
        "Weida Zhong",
        "Fenglong Ma",
        "Ye Yuan",
        "Jing Gao",
        "Aidong Zhang"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "36",
      "title": "Deep transformer models for time series forecasting: The influenza prevalence case",
      "authors": [
        "Neo Wu",
        "Bradley Green",
        "Xue Ben",
        "Shawn O' Banion"
      ],
      "year": "2020",
      "venue": "Deep transformer models for time series forecasting: The influenza prevalence case",
      "arxiv": "arXiv:2001.08317"
    },
    {
      "citation_id": "37",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Prateek Vij",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "38",
      "title": "Mitigating Inconsistencies in Multimodal Sentiment Analysis under Uncertain Missing Modalities",
      "authors": [
        "Jiandian Zeng",
        "Jiantao Zhou",
        "Tianyi Liu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "39",
      "title": "Missing modality imagination network for emotion recognition with uncertain missing modalities",
      "authors": [
        "Jinming Zhao",
        "Ruichen Li",
        "Qin Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "40",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "Kun Zhou",
        "Berrak Sisman",
        "Rui Liu",
        "Haizhou Li"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Exploiting modality-invariant feature for robust multimodal emotion recognition with missing modalities",
      "authors": [
        "Haolin Zuo",
        "Rui Liu",
        "Jinming Zhao",
        "Guanglai Gao",
        "Haizhou Li"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}