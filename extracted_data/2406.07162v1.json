{
  "paper_id": "2406.07162v1",
  "title": "Emobox: Multilingual Multi-Corpus Speech Emotion Recognition Toolkit And Benchmark",
  "published": "2024-06-11T11:12:51Z",
  "authors": [
    "Ziyang Ma",
    "Mingjie Chen",
    "Hezhao Zhang",
    "Zhisheng Zheng",
    "Wenxi Chen",
    "Xiquan Li",
    "Jiaxin Ye",
    "Xie Chen",
    "Thomas Hain"
  ],
  "keywords": [
    "speech emotion recognition",
    "toolkit",
    "benchmark",
    "cross-corpus"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is an important part of human-computer interaction, receiving extensive attention from both industry and academia. However, the current research field of SER has long suffered from the following problems: 1) There are few reasonable and universal splits of the datasets, making comparing different models and methods difficult. 2) No commonly used benchmark covers numerous corpus and languages for researchers to refer to, making reproduction a burden. In this paper, we propose EmoBox 1 , an out-of-the-box multilingual multi-corpus speech emotion recognition toolkit, along with a benchmark for both intra-corpus and cross-corpus settings. For intra-corpus settings, we carefully designed the data partitioning for different datasets. For cross-corpus settings, we employ a foundation SER model, emotion2vec, to mitigate annotation errors and obtain a test set that is fully balanced in speakers and emotions distributions. Based on EmoBox, we present the intra-corpus SER results of 10 pre-trained speech models on 32 emotion datasets with 14 languages, and the cross-corpus SER results on 4 datasets with the fully balanced test sets. To the best of our knowledge, this is the largest SER benchmark, across language scopes and quantity scales. We hope that our toolkit and benchmark can facilitate the research of SER in the community.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In the realm of human-computer interaction (HCI), the ability of machines to understand and respond to human emotions through speech has emerged as a pivotal area of research, known as Speech Emotion Recognition (SER). The significance of SER extends across a wide array of applications, from enhancing user experience in virtual assistants  [1]  to facilitating emotional well-being in healthcare services  [2] . Despite its growing importance, the field of SER faces persistent challenges that hinder progress and innovation. Among these challenges are the scarcity of universally accepted dataset splits  [3]  and the absence of a comprehensive benchmark encompassing a diverse range of corpora and languages  [4] . These limitations complicate the comparison of models and methods, as well as the replication of research findings, thus impeding the advancement of SER technology.\n\nRecognizing existing critical gaps, this paper introduces EmoBox, a groundbreaking multilingual multi-corpus speech emotion recognition toolkit designed to streamline research in this field. EmoBox is accompanied by a meticulously curated benchmark tailored for both intra-corpus and cross-corpus evaluation settings. For intra-corpus evaluations, we have devised a systematic approach to data partitioning across various datasets, ensuring that researchers can conduct rigorous and comparable analyses of different SER models. In the cross-corpus context, we leverage a foundational SER model, emotion2vec  [5] , to address annotation discrepancies and create a test set that achieves a balance in speaker and emotion distribution, a feat previously unattained in SER research.\n\nOur contributions are manifold. Not only do we offer the SER community a powerful toolkit to easily conduct experiments on different datasets, but we also establish a new benchmark for the field. We detail our data partitioning, which we believe reduces the burden on researchers for future research. We present comprehensive intra-corpus SER results derived from the application of 10 pre-trained speech models across 32 emotion datasets in 14 languages. To our knowledge, this represents the most extensive SER benchmark to date, spanning the broadest scope of languages and the largest scale of data. Besides, we showcase the cross-corpus SER performance on 4 datasets, utilizing test sets that are fully balanced in terms of speakers and emotions. Through EmoBox, we aim to provide the SER community with a robust toolkit and benchmark that will catalyze further research, enhance model comparability, and ultimately, foster innovation in the field of speech emotion recognition. The comprehensive overview of the datasets utilized in this study is delineated in Table  1 . There are 32 emotional datasets spanning 14 distinct languages, comprising 12 in English, 3 in Mandarin, and 2 in French, German, and Italian. Additionally, there is 1 dataset each in Amharic, Bangla, Greek, Persian, Polish, Russian, Spanish, Turkish, and Urdu, as well as two datasets featuring a mixture of languages.\n\nFor analytical purposes, each dataset is systematically classified according to several criteria: Source denotes the origin of the samples; Lang indicates the language of the dataset; Emo represents the number of emotional categories encompassed; Spk specifies the number of speakers; #Utts details the total number of utterances; and #Hrs quantifies the aggregate hours of the samples.\n\nThe speech data extracted from these datasets undergoes a uniform processing protocol, being converted into a monophonic format with a sampling rate of 16, 000 Hz. Each piece of speech data is uniquely annotated with an emotion label, ensuring a precise correlation between the utterance and its emotional categorization.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Intra-Corpus Ser",
      "text": "Ensuring proper data partitioning is pivotal for leveraging a corpus efficiently, particularly when dealing with corpora of limited size. Through meticulous observation and analysis of the distribution of and emotions across the 32 datasets, as detailed in Section 2.1, we establish a set of criteria for data partitioning as follows: 1. Each dataset is divided into training and testing sets, with the division possibly encompassing single or multiple folds depending on the data distribution. 2. In cases where datasets come with officially predefined splits, these original partitions are adhered to. For instance, the IEMOCAP dataset is organized into 5 folds, featuring 2 distinct speakers per fold, whereas the MELD dataset is split into train, dev, and test splits. 3. For datasets characterized by a speaker count of fewer than 4, such as the PAVOQUE dataset which includes only a single speaker; or those with 4 or more speakers but exhibit an imbalanced distribution of emotions among speakers, such as the M3ED dataset, a speaker-dependent approach is employed. Here, 25% of data for each emotion is earmarked for testing, with the remainder allocated for training. 4. For datasets whose speaker number is greater than or equal to 4 with a balanced emotion distribution among speakers, the leave-one-out n-fold cross-validation manner is adopted. More specifically, if the number of speakers ∈ {4, 5, 6}, n is set the same as the number of speakers; If the number of speakers exceeds 6, n is taken to be 4 and multiple speakers are merged within each fold. The data partitioning of the aforementioned criteria is conducted in EmoBox and model performance on the benchmark is thoroughly examined in Section 3.2. This structured approach to data partitioning underscores our commitment to fostering robust and replicable research methodologies within the field.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Corpus Ser",
      "text": "In real scenarios, the ability of a SER model to generalize to unseen speakers and unknown recording conditions is essential. To evaluate this capability, cross-corpus zero-shot testing emerges as a profitable strategy to assess the robustness of SER To address potential errors in annotation, we leverage the fine-tuned version 2  of the emotion2vec  [5]  model, a foundation speech emotion recognition model with iterative fine-tuning on over 10, 000 hours of speech data. Our methodology involves an initial application of emotion2vec to assign pseudolabels to our datasets. Subsequently, we refine our dataset by retaining only those instances where there is congruence between the original annotations and those generated by emo-tion2vec. This step ensures the reduction of annotation discrepancies and enhances the reliability for further analysis. To establish a fully balanced test set across each dataset, we select 240 speech-emotion pairs for each dataset. Shared emotions among these datasets contain angry, happy, neutral, and sad, resulting in 60 pieces for each emotion. The composition of the test set, including the number of speakers and the allocation of speech-emotion pairs per speaker for each emotion, is detailed in Figure  3 . For the IEMOCAP and SAVEE datasets, all speakers are encompassed, including 5 male and 5 female speakers in IEMOCAP, and 4 male speakers in SAVEE. In the case of the MELD dataset, we focus on the 6 protagonists. For the RAVDESS dataset, we include a selection of 20 speakers, equally divided between 10 male and 10 female speakers. This meticulous composition of the test set is instrumental in our analysis of the models' generalization capabilities across diverse corpora, helping our assessment of the robustness and adaptability of models under cross-corpus settings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Benchmark",
      "text": "3.1. Experiments Setup 10 pre-trained models are employed to establish the benchmark of EmoBox, including self-supervised wav2vec 2.0  [38]  base, HuBERT  [39]  base/large, WavLM  [40]  base/large, data2vec  [41]  base/large, data2vec 2.0  [42]  base/large, and an additional supervised ASR encoder of Whisper  [43]  large v3.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross-Corpus Ser Results",
      "text": "Table  5  presents cross-corpus SER results of selected 10 pretrained speech models on the refined EmoBox test sets. As seen from the table, Whisper large v3 encoder still performs best on the cross-corpus settings, with top1 on 9/12 train-test pairs, while HuBERT large, WavLM large and data2vec base takes 1 each, respectively. Figure  1  illustrates the average accuracy of different models with cross-corpus settings. The formula for calculating the average accuracy Acc is given as follows:\n\nwhere Acci,j denotes training with dataset i and testing on dataset j, and n = 4 is in our settings. As seen from",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion, Limitation And Future Work",
      "text": "EmoBox offers an easy-to-use toolkit for multilingual multicorpus SER research with data preparation and partitioning, and",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Acknowledgements",
      "text": "",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 3: For the IEMOCAP and SAVEE",
      "page": 2
    },
    {
      "caption": "Figure 1: illustrates the average accuracy of different mod-",
      "page": 4
    },
    {
      "caption": "Figure 1: Average accuracy for cross-corpus settings.",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "CaFE (fr)"
        },
        {
          "Model": "wav2vec 2.0 base\nHuBERT base\nHuBERT large\nWavLM base\nWavLM large\ndata2vec base\ndata2vec large\ndata2vec 2.0 base\ndata2vec 2.0 large\nWhisper large v3",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "42.76\n42.47\n41.03\n54.16\n54.16\n53.36\n59.50\n58.73\n58.22\n52.71\n52.33\n51.66\n62.20\n61.33\n61.14\n42.18\n42.36\n41.69\n42.24\n42.85\n41.11\n51.83\n51.67\n50.52\n59.04\n58.02\n57.51\n69.43\n68.84\n68.06"
        },
        {
          "Model": "Model",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "EmoDB (de)"
        },
        {
          "Model": "wav2vec 2.0 base\nHuBERT base\nHuBERT large\nWavLM base\nWavLM large\ndata2vec base\ndata2vec large\ndata2vec 2.0 base\ndata2vec 2.0 large\nWhisper large v3",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "82.06\n83.14\n82.21\n87.73\n87.73\n87.82\n89.81\n90.26\n89.86\n87.03\n87.12\n86.76\n92.58\n92.67\n92.57\n58.12\n60.01\n58.32\n60.96\n61.95\n61.26\n75.07\n75.86\n75.49\n79.36\n80.41\n79.96\n91.26\n92.43\n91.84"
        },
        {
          "Model": "Model",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "eNTERFACE (en)"
        },
        {
          "Model": "wav2vec 2.0 base\nHuBERT base\nHuBERT large\nWavLM base\nWavLM large\ndata2vec base\ndata2vec large\ndata2vec 2.0 base\ndata2vec 2.0 large\nWhisper large v3",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "64.19\n64.12\n63.81\n79.14\n79.11\n78.97\n88.19\n88.17\n88.14\n88.30\n88.27\n88.20\n92.43\n92.42\n92.40\n91.61\n91.62\n91.64\n89.46\n89.46\n89.65\n90.81\n90.80\n90.83\n94.02\n94.02\n94.01\n97.69\n97.68\n97.68"
        },
        {
          "Model": "Model",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "M3ED (zh)"
        },
        {
          "Model": "wav2vec 2.0 base\nHuBERT base\nHuBERT large\nWavLM base\nWavLM large\ndata2vec base\ndata2vec large\ndata2vec 2.0 base\ndata2vec 2.0 large\nWhisper large v3",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "23.13\n43.20\n22.91\n23.80\n42.55\n24.03\n23.25\n44.49\n23.28\n22.76\n42.79\n22.03\n26.58\n44.86\n26.98\n19.44\n37.32\n19.24\n20.20\n38.73\n20.26\n22.82\n41.42\n22.89\n23.82\n43.02\n23.98\n32.84\n49.42\n33.76"
        },
        {
          "Model": "Model",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "MESD (es)"
        },
        {
          "Model": "wav2vec 2.0 base\nHuBERT base\nHuBERT large\nWavLM base\nWavLM large\ndata2vec base\ndata2vec large\ndata2vec 2.0 base\ndata2vec 2.0 large\nWhisper large v3",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "62.93\n62.89\n62.85\n47.52\n47.48\n46.33\n53.71\n53.67\n53.67\n43.58\n43.52\n42.94\n62.54\n62.49\n62.33\n34.37\n34.35\n33.24\n36.67\n36.61\n35.81\n44.86\n44.85\n43.6\n48.46\n48.45\n46.8\n69.78\n69.67\n69.64"
        },
        {
          "Model": "Model",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "Polish (pl)"
        },
        {
          "Model": "wav2vec 2.0 base\nHuBERT base\nHuBERT large\nWavLM base\nWavLM large\ndata2vec base\ndata2vec large\ndata2vec 2.0 base\ndata2vec 2.0 large\nWhisper large v3",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "67.35\n67.35\n66.76\n69.40\n69.40\n69.08\n70.20\n70.20\n70.74\n69.31\n69.31\n69.46\n79.29\n79.29\n79.02\n71.31\n71.31\n70.65\n68.05\n68.05\n67.07\n75.75\n75.75\n75.52\n74.00\n74.00\n74.05\n83.27\n83.27\n82.77"
        },
        {
          "Model": "Model",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "ShEMO (fa)"
        },
        {
          "Model": "wav2vec 2.0 base\nHuBERT base\nHuBERT large\nWavLM base\nWavLM large\ndata2vec base\ndata2vec large\ndata2vec 2.0 base\ndata2vec 2.0 large\nWhisper large v3",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "56.34\n78.96\n57.34\n58.31\n81.17\n63.15\n64.29\n83.35\n66.26\n60.73\n78.76\n62.06\n71.72\n87.13\n73.55\n47.61\n70.07\n49.49\n56.42\n74.09\n60.98\n60.59\n79.03\n64.03\n64.09\n82.68\n68.47\n80.23\n89.55\n82.94"
        },
        {
          "Model": "Model",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "URDU (ur)"
        },
        {
          "Model": "wav2vec 2.0 base\nHuBERT base\nHuBERT large\nWavLM base\nWavLM large\ndata2vec base\ndata2vec large\ndata2vec 2.0 base\ndata2vec 2.0 large\nWhisper large v3",
          "UA(%) ↑\nWA(%) ↑\nF1(%) ↑": "87.50\n87.50\n87.57\n88.41\n88.41\n88.40\n81.75\n81.75\n81.66\n82.82\n82.82\n82.85\n86.61\n86.61\n86.64\n65.42\n65.42\n65.35\n64.81\n64.81\n64.93\n69.97\n69.97\n69.91\n78.10\n78.10\n78.10\n82.52\n82.52\n82.41"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 4: presents intra-corpus SER results of selected 10 pre- M 22.50 31.39 35.24 51.42 47.00 36.44",
      "data": [
        {
          "I\nM\nR\nS": "Whisper large v3"
        },
        {
          "I\nM\nR\nS": "46.12\n46.14\n38.24\n51.42\n47.00\n36.44\n48.12\n40.68\n66.91\n49.30\n42.18\n49.63"
        },
        {
          "I\nM\nR\nS": "HuBERT large"
        },
        {
          "I\nM\nR\nS": "44.60\n15.03\n39.99\n43.74\n44.69\n38.22\n36.18\n25.02\n56.96\n42.81\n31.54\n31.92"
        },
        {
          "I\nM\nR\nS": "WavLM large"
        },
        {
          "I\nM\nR\nS": "48.59\n34.16\n35.53\n39.06\n23.06\n25.74\n44.03\n33.90\n63.35\n43.69\n34.10\n36.69"
        },
        {
          "I\nM\nR\nS": "data2vec large"
        },
        {
          "I\nM\nR\nS": "44.99\n39.03\n36.88\n40.62\n29.10\n31.57\n26.50\n26.73\n32.54\n26.82\n24.05\n16.96"
        },
        {
          "I\nM\nR\nS": "data2vec 2.0 large"
        },
        {
          "I\nM\nR\nS": "47.43\n17.80\n29.67\n41.75\n31.80\n29.66\n38.79\n34.21\n35.43\n36.39\n37.79\n23.58"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Advancing large language models to capture varied speaking styles and respond properly in spoken conversations",
      "authors": [
        "G.-T Lin",
        "C.-H Chiang",
        "H.-Y Lee"
      ],
      "year": "2024",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "3",
      "title": "Climate and weather: Inspecting depression detection via emotion recognition",
      "authors": [
        "W Wu",
        "M Wu",
        "K Yu"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "4",
      "title": "Designing and evaluating speech emotion recognition systems: A reality check case study with IEMOCAP",
      "authors": [
        "N Antoniou",
        "A Katsamanis",
        "T Giannakopoulos",
        "S Narayanan"
      ],
      "year": "2023",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "5",
      "title": "Serab: A multi-lingual benchmark for speech emotion recognition",
      "authors": [
        "N Scheidwasser-Clow",
        "M Kegler",
        "P Beckmann",
        "M Cernak"
      ],
      "year": "2022",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "6",
      "title": "emotion2vec: Self-supervised pre-training for speech emotion representation",
      "authors": [
        "Z Ma",
        "Z Zheng",
        "J Ye",
        "J Li",
        "Z Gao",
        "S Zhang",
        "X Chen"
      ],
      "year": "2024",
      "venue": "Proc. ACL Findings"
    },
    {
      "citation_id": "7",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas",
        "R Kotsakis",
        "A Liatsou",
        "C Dimoulas",
        "G Kalliris"
      ],
      "year": "2018",
      "venue": "Proc. AES"
    },
    {
      "citation_id": "8",
      "title": "A new Amharic speech emotion dataset and classification benchmark",
      "authors": [
        "E Retta",
        "E Almekhlafi",
        "R Sutcliffe",
        "M Mhamed",
        "H Ali",
        "J Feng"
      ],
      "year": "2023",
      "venue": "Proc. TALLIP"
    },
    {
      "citation_id": "9",
      "title": "ASVP-ESD: A dataset and its benchmark for emotion recognition using both speech and non-speech utterances",
      "authors": [
        "T Landry Dejoli",
        "Q He",
        "H Yan",
        "Y Li"
      ],
      "year": "2020",
      "venue": "Proc. GSJ"
    },
    {
      "citation_id": "10",
      "title": "A Canadian French emotional speech dataset",
      "authors": [
        "P Gournay",
        "O Lahaie",
        "R Lefebvre"
      ],
      "year": "2018",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "11",
      "title": "Design of speech corpus for Mandarin text to speech",
      "authors": [
        "J Tao",
        "F Liu",
        "M Zhang",
        "H Jia"
      ],
      "year": "2008",
      "venue": "The Blizzard Challenge Workshop"
    },
    {
      "citation_id": "12",
      "title": "CREMA-D: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "Proc. TAC"
    },
    {
      "citation_id": "13",
      "title": "EMNS/Imz/Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels",
      "authors": [
        "K Noriy",
        "X Yang",
        "J Zhang"
      ],
      "year": "2023",
      "venue": "EMNS/Imz/Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels"
    },
    {
      "citation_id": "14",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "15",
      "title": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems",
      "authors": [
        "A Adigwe",
        "N Tits",
        "K Haddad",
        "S Ostadabbas",
        "T Dutoit"
      ],
      "year": "2018",
      "venue": "The emotional voices database: Towards controlling the emotion dimension in voice generation systems"
    },
    {
      "citation_id": "16",
      "title": "EMOVO corpus: an Italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "17",
      "title": "Speech emotion recognition in Italian using Wav2Vec 2",
      "authors": [
        "F Catania"
      ],
      "year": "2023",
      "venue": "Authorea Preprints"
    },
    {
      "citation_id": "18",
      "title": "The eNTERFACE'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proc. ICDE Workshop"
    },
    {
      "citation_id": "19",
      "title": "Seen and unseen emotional style transfer for voice conversion with a new emotional speech dataset",
      "authors": [
        "K Zhou",
        "B Sisman",
        "R Liu",
        "H Li"
      ],
      "year": "2021",
      "venue": "Proc. ICASSP"
    },
    {
      "citation_id": "20",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "21",
      "title": "An open source emotional speech corpus for human robot interaction applications",
      "authors": [
        "J James",
        "L Tian",
        "C Watson"
      ],
      "year": "2018",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "M3ED: Multi-modal multi-scene multi-label emotional dialogue database",
      "authors": [
        "J Zhao",
        "T Zhang",
        "J Hu",
        "Y Liu",
        "Q Jin",
        "X Wang",
        "H Li"
      ],
      "year": "2022",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "23",
      "title": "MEAD: A large-scale audio-visual dataset for emotional talking-face generation",
      "authors": [
        "K Wang",
        "Q Wu",
        "L Song",
        "Z Yang",
        "W Wu",
        "C Qian",
        "R He",
        "Y Qiao",
        "C Loy"
      ],
      "year": "2020",
      "venue": "Proc. ECCV"
    },
    {
      "citation_id": "24",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proc. ACL"
    },
    {
      "citation_id": "25",
      "title": "MER 2023: Multi-label learning, modality robustness, and semi-supervised learning",
      "authors": [
        "Z Lian",
        "H Sun",
        "L Sun",
        "K Chen",
        "M Xu",
        "K Wang",
        "K Xu",
        "Y He",
        "Y Li",
        "J Zhao"
      ],
      "year": "2023",
      "venue": "Proc. ACM Multimedia"
    },
    {
      "citation_id": "26",
      "title": "The Mexican emotional speech database (MESD): elaboration and assessment based on machine learning",
      "authors": [
        "M Duville",
        "L Alonso-Valerdi",
        "D Ibarra-Zarate"
      ],
      "year": "2021",
      "venue": "Proc. EMBC"
    },
    {
      "citation_id": "27",
      "title": "The MSPconversation corpus",
      "authors": [
        "L Martinez-Lucas",
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "28",
      "title": "French emotional speech database-oréau",
      "authors": [
        "L Kerkeni",
        "C Cleder",
        "S.-R Youssef",
        "K Raoof"
      ],
      "year": "2020",
      "venue": "French emotional speech database-oréau"
    },
    {
      "citation_id": "29",
      "title": "The PAVOQUE corpus as a resource for analysis and synthesis of expressive speech",
      "authors": [
        "I Steiner",
        "M Schröder",
        "A Klepp"
      ],
      "year": "2013",
      "venue": "Proc. Phonetik & Phonologie"
    },
    {
      "citation_id": "30",
      "title": "Emotions in Polish speech recordings",
      "authors": [
        "M Miesikowska",
        "D Swisulski"
      ],
      "year": "2020",
      "venue": "Emotions in Polish speech recordings",
      "doi": "10.34808/h46c-hb44"
    },
    {
      "citation_id": "31",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "Proc. PloS One"
    },
    {
      "citation_id": "32",
      "title": "Aniemore",
      "authors": [
        "I Lubenets",
        "N Davidchuk",
        "A Amentes"
      ],
      "venue": "Aniemore"
    },
    {
      "citation_id": "33",
      "title": "Surrey audio-visual expressed emotion (SAVEE) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (SAVEE) database"
    },
    {
      "citation_id": "34",
      "title": "ShEMO: a large-scale validated database for Persian speech emotion detection",
      "authors": [
        "O Nezami",
        "P Lou",
        "M Karami"
      ],
      "year": "2019",
      "venue": "Proc. LREC"
    },
    {
      "citation_id": "35",
      "title": "SUST Bangla emotional speech corpus (SUBESCO): An audio-only emotional speech corpus for Bangla",
      "authors": [
        "S Sultana",
        "M Rahman",
        "M Selim"
      ],
      "year": "2021",
      "venue": "Proc. PloS One"
    },
    {
      "citation_id": "36",
      "title": "Toronto emotional speech set (TESS) -younger talker_happy",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2010",
      "venue": "Toronto emotional speech set (TESS) -younger talker_happy"
    },
    {
      "citation_id": "37",
      "title": "Turkish Emotion Voice Database (TurEV-DB)",
      "authors": [
        "S Canpolat",
        "Z Ormanoglu",
        "D Zeyrek"
      ],
      "year": "2020",
      "venue": "Proc. SLTU Workshop"
    },
    {
      "citation_id": "38",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Proc. FIT"
    },
    {
      "citation_id": "39",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Proc. NeurIPS"
    },
    {
      "citation_id": "40",
      "title": "HuBERT: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Proc. TASLP"
    },
    {
      "citation_id": "41",
      "title": "WavLM: Large-scale selfsupervised pre-training for full stack speech processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao"
      ],
      "year": "2022",
      "venue": "Proc. JSTSP"
    },
    {
      "citation_id": "42",
      "title": "data2vec: A general framework for self-supervised learning in speech, vision and language",
      "authors": [
        "A Baevski",
        "W.-N Hsu",
        "Q Xu",
        "A Babu",
        "J Gu",
        "M Auli"
      ],
      "year": "2022",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "43",
      "title": "Efficient selfsupervised learning with contextualized target representations for vision, speech and language",
      "authors": [
        "A Baevski",
        "A Babu",
        "W.-N Hsu",
        "M Auli"
      ],
      "year": "2023",
      "venue": "Proc. IMCL"
    },
    {
      "citation_id": "44",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "Proc. ICML"
    }
  ]
}