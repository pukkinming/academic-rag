{
  "paper_id": "2102.05894v1",
  "title": "Casa-Based Speaker Identification Using Cascaded Gmm-Cnn Classifier In Noisy And Emotional Talking Conditions",
  "published": "2021-02-11T08:56:12Z",
  "authors": [
    "Ali Bou Nassif",
    "Ismail Shahin",
    "Shibani Hamsa",
    "Nawel Nemmour",
    "Keikichi Hirose"
  ],
  "keywords": [
    "Computational Auditory Scene Analysis (CASA)",
    "Convolutional Neural Network",
    "Gaussian Mixture Model",
    "Speaker Identification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This work aims at intensifying text-independent speaker identification performance in real application situations such as noisy and emotional talking conditions. This is achieved by incorporating two different modules: a Computational Auditory Scene Analysis (CASA) based pre-processing module for noise reduction and \"cascaded Gaussian Mixture Model -Convolutional Neural Network (GMM-CNN) classifier for speaker identification\" followed by emotion recognition. This research proposes and evaluates a novel algorithm to improve the accuracy of speaker identification in emotional and highly-noise susceptible conditions. Experiments demonstrate that the proposed model yields promising results in comparison with other classifiers when \"Speech Under Simulated and Actual Stress (SUSAS) database, Emirati Speech Database (ESD), the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)\" database and the \"Fluent Speech Commands\" database are used in a noisy environment.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction And Literature Review",
      "text": "Research and developments in speaker recognition systems have resulted in a vast range of acceptance in several fields such as banking, forensic authorization and security applications in neutral talking condition  [1] ,  [2] . However, the system performance degrades in noisy and emotional talking conditions  [3] . The challenges of designing a system which can offer a high performance in the midst of a noisy interference signal and in emotional talking conditions have been identified as the main objectives of this paper.\n\nSpeech is considered to be the primary communication system  [4] . An effective communication system consists of both a linguistic and an emotional/stressful part  [5] . The emotional/stressful aspect of human communication systems is inevitable in any Human-Machine interaction system. Speaker identification in emotional and noisy talking conditions can be used to offer a promising future for the banking sector, smart customer support, and forensic applications  [6, 7] .\n\nIn natural conditions, speech undergoes various kinds of interference, such as surface reflections, reverberations, noise from other sources, and so on  [8] . These interferences, which are present in the dominant speech signal, may reduce the system's performance at the application level  [9] . For example, a bank security system using speaker identification may fail to work in noisy conditions. This is because the system does not have the ability to separate the dominant original signal from other noisy signals. Humans have the ability to separate the dominant signal even in the presence of noise, and this ability is referred to as Auditory Scene Analysis (ASA)  [10] . ASA is accomplished by the combined efforts of the human auditory and intelligence systems. The system performance can be improved by the incorporation of computationally developed ASA for machines. Therefore, they can separate the dominant signal from other forms of interference before verification by means of Computational Auditory Scene Analysis (CASA)  [11] . Our proposed model show promising results in comparison with other classifiers such as Support Vector Machine (SVM) and Multilayer Perceptron (MLP).\n\nThese days organizations, industries and several homes are equipped with security devices such as surveillance cameras. These devices can capture sounds of the surroundings. They may capture the voice of predators even in noisy and emotional conditions. The recorded voice can be an input to the proposed system, and this can be used by police in criminal investigations to identify a potential criminal for example. Four distinct speech datasets have been included in this work to assess the proposed model.\n\nThe implementation of emotion recognition, along with noise suppression is of great importance in the development of speaker identification techniques for successful implementation of an effective human-machine interaction system. Shao and Wang  [12]  studied acoustic features and investigated a general solution to achieve a robust speaker identification system under noisy environments. They proposed state of the art speakerdependent features obtained from auditory filtering and cepstral analysis. These auditory features were further enhanced, by means of binary time-frequency (T-F) masks produced by a CASA system, and their reconstruction uncertainties were estimated for better computation of speaker likelihood. Results demonstrated that their proposed Gammatone Frequency Cepstral Coefficients (GFCCs) features perform significantly better than the traditional Mel-Frequency Cepstral Coefficients (MFCC) features. Ghiurcau et al.  [13]  evaluated the impact of speaker emotional features upon text-independent speaker recognition systems by means of MFCCs, for feature extraction, and the SVM model, for classification. Experimental tests were performed on the Berlin emotional speech database. Results demonstrated that emotions play a vital role in minimizing the performance of speaker identification systems compared to when these same words were spoken neutrally. Zhao et al.  [14]  studied speaker identification performance under noisy environments. The authors first presented a new speaker feature, called gammatone frequency cepstral coefficient (GFCC) and demonstrated that this auditory feature picks up seizures acoustic characteristics pertinent to the speaker and performs noticeably better than the conventional speaker features under noisy conditions. At a later stage, the authors applied CASA separation and then reconstructed or marginalized the deteriorated constituents, specified by the CASA mask. They found out that both reconstruction and marginalization are effective. Li et al.  [15]  proposed a novel architecture to enhance the robustness in emotion-dependent speaker recognition/identification systems. In fact, they proposed a new learning technology to reweight the probability of test affective utterances at the pitch envelop level. Experiments were carried out upon the Mandarin Affective Speech dataset and results yielded an enhancement of 8% of identification performance rate over the conventional speaker recognition schemes. Patnala and Prasad  [16]  proposed a novel scheme in order to enhance speaker identification performance under the existence of fused effects of additive noise and room reverberations, which together present a significant challenge to building robust solutions to related systems. The authors proposed a system solution with the aim of resolving the aforementioned matter using two steps. The first step was the preprocessing of the audio signal corrupted by noise and room reverberations using binary time-frequency (T-F) masking algorithm, using a CASA approach, via a deep neural network classifier. Mansour et al.  [17]  employed the i-vector approach along with the Support Vector Machine (SVM) classifier as an attempt to boost and enhance the deteriorated performance of speaker recognition under emotional auditory environments. Results showed that the i-vector algorithm resolves the problem of training algorithm complexity that the SVM model suffers from and shows promising results in increasing speaker recognition performance in an emotional context. Islam et al.  [18]  proposed a state of the art neural-response-based method for a speaker identification system under noisy acoustic conditions using 2-D neurograms coefficients, which are coefficients built upon reactions of a physiologically-based computational model of the auditory periphery. The classification accuracies of the proposed model were compared to the performances of the traditional speaker identification methodologies using features such as \"MFCCs, GFCC) and Frequency domain linear prediction (FDLP)\". The identification results attained by the proposed method were comparable to the performance of those conventional approaches in quiet settings, but the new feature has demonstrated lower classification error rates under noisy environments.\n\nFaragallah  [19]  advocated a speaker identification system, that is resistant to noise, named MKMFCC-SVM. This system is based on the \"Multiple Kernel Weighted Mel Frequency Cepstral Coefficient (MKMFCC) and support vector machine (SVM)\". A comparison was made between the performance of the proposed \"MKMFCC-SVM and the MFCC-SVM\" speaker identification systems. Results revealed that the proposed MKMFCC-SVM system produces better identification rates in the presence of noise. Korba et al.  [20]  stated that MFCC features are deemed very sensitive in the presence of background conditions, which has a considerable negative impact on the performance of speaker identification systems. The authors combined the features they obtained with MFCC features. Their speaker identification system was implemented on the GMM using TIMIT speech corpus. The results of their method of implementation and testing were increased up to 28% accuracy at signal to noise ratio (SNR) 5 dB. Ayhan and Kwan  [21]  developed a vigorous speaker identification scheme under noisy conditions which implicates \"mask estimation, gammatone features with bounded marginalization to deal with unreliable features, and Gaussian mixture model (GMM) for speaker identification\". Evaluation and assessments were performed to determine the speaker identification performance of the proposed algorithm, and results showed that it substantially outperforms the conventional method MFCC with Cepstral Mean Normalization (MFCC-CMN) at low signal-to-noise conditions. Nasr et al.  [22]  proposed a new framework to enhance speaker identification accuracy based on \"Cepstral features and the Normalized Pitch Frequency (NPF)\". The novel approach used a neural classifier with a single hidden layer node as well as a pre-processing noise reduction step prior to the feature extraction procedure in order to enlarge and enhance the identification performance.\n\nThere are several limitations in the related work. Much of the literature on this subject attempted to propose groundbreaking approaches and pioneering methodologies with the aim of enhancing speaker identification accuracy under noisy as well as emotional environments. Some authors used the conventional MFCC features  [13] ,  [23] ; while some others introduced novel acoustic features such as GFCCs features  [12]  and 2-D neurograms coefficients  [18] . Some scholars favored examining the use of CASA modules in noisy speech, in conjunction with one of the above-mentioned acoustic features, and results showed substantial improvement in identification performance in some cases. Moreover, many studies used the conventional classifiers, such as SVMs  [13] , GMMs  [18] ,  [21]  and HMMs  [23] [24] [25]  , while many recent work explored the DNN-based classifiers  [16] ,  [26] .\n\nThe aim of this study is to introduce a novel algorithm for speaker identification in real-world applications. Speech processing modules are susceptible to noise and interference in natural environments. This reduces the system's performance in real world applications. In contrast, the proposed algorithm can identify the speaker in noisy and emotional talking conditions. The proposed algorithm incorporates a CASA pre-processing module for noise suppression and cascaded GMM-CNN classifier for emotion recognition and speaker identification.\n\nTo the best of our knowledge, none of the former studies has considered the usage of CASA preprocessing module and MFCC-based feature extraction in combination with hybrid cascaded DNNbased classifier, such as GMM-CNN classifier, in order to boost text-independent speaker identification systems under noisy and emotional talking conditions.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Our Contributions In This Work Are:",
      "text": "‚Ä¢ To the best of our knowledge, this is the first work that proposes CASA-GMM-CNN model.\n\n‚Ä¢ Implementation of emotion recognition, by means of the GMM model; along with the CNN, for final identification decisions, which results in a hybrid GMM-CNN classification model. ‚Ä¢ Implementation of the CASA pre-processing method and the MFCC based feature extraction together with the hybrid cascaded classifier, GMM-CNN. ‚Ä¢ The proposed framework is capable of separating the original speech signal from other noise and interference. ‚Ä¢ The proposed system is able to recognize the unknown speaker even in emotional/stressful talking conditions.\n\nThe remainder of the paper is structured as follows: Sections 2 presents the materials and methods used in this research. Section 3 depicts the results and provides a discussion about the results. Finally, Section 4 concludes our work.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Materials And Methods",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Speech Databases",
      "text": "In this work, four distinct datasets have been utilized to evaluate the proposed model. The datasets are Speech Under Simulated and Actual Stress (SUSAS) dataset  [27] , Arabic private Emirati Speech Database and the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [28] .\n\nThe four databases are listed as follows:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Speech Under Simulated And Actual Stress (Susas) Dataset",
      "text": "SUSAS is an English public dataset that consists of five domains which have an array of stress and emotion features  [27] . The database has two domains; one involves simulated speech under stress and is termed Simulated Domain. The second involves actual speech under stress and is termed Actual Domain. A group of thirty-two speakers including 19 males and 13 females, in the age group 22 to 76 years, were asked to pronounce more than 16,000 words. The speech tokens were sampled at a frequency of 8 kHz using 16 bits A/D converter. The signal samples were pre-emphasized and then segmented into frames of 20 ms each with 31.25% overlap between consecutive frames. The emphasized speech signals were implemented every 5 ms to a 30 ms Hamming Window. The observation vectors in each of CASA-based GMM-CNN were found using a 32-dimension feature analysis of MFCCs (sixteen delta MFCCs and sixteen static MFCCs). In this work, twenty different words, uttered twice by twenty speakers (two repetitions per word), uttered in seven stressful talking conditions were used. Out of the twenty words, ten words were used for training and twenty for testing.\n\nDuring the evaluation phase, ten different words were uttered by ten speakers twenty-five times under seven stressful talking conditions, which are neutral, angry, slow, loud, soft, Lombard and fast. These were mixed with the other speech signals in the same database in a ratio of 2:1 and 3:1 and were then used. Ten different words uttered by same ten speakers two times under six stressful talking conditions were mixed with various noise signals in the ratio 2:1 and 3:1.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Arabic Emirati Speech Database (Esd)",
      "text": "ESD is a private dataset made up of Twenty-five female and twenty-five male Emirati speakers with age range spanning from fourteen to fifty-five years old articulated the \"Arabic Emirati-emphasized speech database\". Eight common Emirati utterances, frequently used in the United Arab Emirates society, were uttered by every speaker. Every speaker expressed the eight sentences in each of neutral, happy, sad, disgusted, angry, and fearful emotions, nine times with a span of 2 -5 seconds.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fluent Speech Commands",
      "text": "The Fluent Speech Commands dataset  [29]  contains 30,043 utterances from 97 speakers. It is recorded as 16 kHz single-channel .wav files each containing a single utterance used for controlling smart-home appliances or virtual assistant. The dataset has a total of 248 phrasing mapping to 31 unique intents. The utterances are randomly divided into train, valid, and test splits in such a way that no speaker appears in more than one split. Each split contains all possible wordings for each intent.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Casa Pre-Processing For Noise Reduction",
      "text": "The proposed system incorporates a CASA-based preprocessing module for co-channel noise reduction. Figure  1  shows the CASA based speech separation block diagram. This figure consists of modulation frequency analysis, smoothing, onset-offset detection, segmentation and grouping  [30] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "T-F Decomposition",
      "text": "The speech signal that needs to be identified, is broken up into small time frame signals for segmental feature extraction and processing  [31] . Time-frequency (T-F) analysis of each time frame is computed by taking its short-time Fourier Transform (STFT) and is recorded as a matrix which can track the magnitude and phase in time-frequency domain  [32] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Modulation Transform",
      "text": "The signal X(m, k) extracted by the T-F decomposition process consists of the Modulator Signal M(m, k) and the Carrier Signal C(m, k)  [32] . This can be defined as,\n\nThe modulator of the signal X(m, k), M(m, k), can be obtained through applying an envelope detector.\n\nIt can be represented as,\n\nwhere \"ev\" denotes the envelope detection. The envelope detector used is an incoherent detector which is based on \"Hilbert\" envelope  [33]  as it is able to create a modulation spectrum with large area covered in the modulation frequency domain. In addition, it acts as a magnitude operator for complex-valued sub-bands, as given by the following equation,\n\nThen, the Discrete Short-time Modulation Transform of the signal x(n) can be expressed as,\n\nwhere I is the DFT length and i represents the modulation frequency index. ev {STFT {x(n)}, is the modulating signal part and it is denoted as M(m, k).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Onset-Offset Position Analysis",
      "text": "Many of the speech separation or noise reduction techniques using the CASA algorithm are performing some kind of speech enhancement or noise reduction. Using a low pass filter, the modulation transformed signal is smoothed. The partial derivative of this signal with respect to its modulating frequency will aid in the identification of the peaks and valleys of the signal which can be termed onset position and offset position, respectively  [32] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Binary Mask Segmentation",
      "text": "The onset-offset positions extracted from the likely originated sources are to be grouped to form segments. This can be accomplished by means of an \"Ideal Binary Mask\" (IBM), which can be expressed as  [34] ,  [35] ,\n\nwhere f d is the dominant pitch frequency of the input signal computed by autocorrelation based pitch estimation  [36] , fs is the sampling frequency and œÅ varies from -10 to 10.\n\nThen, the masked signal can be denoted as  [32] ,\n\nThe spectral energy of the dominant signal can be extracted from S IBM (t,f) and the range of the pitch of the interference is calculated from the remaining part of the mixture. Spectral energy from the dominant and interference signals in the entire pitch range can be used to design a frequency mask for separating the desired speech signal from the noise signals  [37] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Segregation Mask",
      "text": "The speech signal can be segregated by means of a frequency based separation mask. The input signal x(n) sampled at a rate of fs consists of both speech signal ùë• ùëõ (n) and interference signal ùë• ùë° (n) as,\n\nThe mean of the signal spectral energy of the speech and noise signals are estimated for designing a suitable frequency mask for noise suppression. X T (k) is the mean modulation spectral energy over the pitch frequency of the target signal and X I (k) is the mean modulation spectral energy over the pitch frequency of the interference signal  [38] ,\n\nPitch frequency range of dominent speech signal (8)\n\nFrequency mask can be designed as,",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. Features Extraction",
      "text": "The short term power spectrum of the sound can be effectively represented by Mel Frequency Cepstral Coefficients (MFCCs)  [39] . In Mel Frequency Cepstrum (MFC), filter coefficients are equally spaced in mel scale rather than linearly spaced filter coefficients in the normal scale. Hence, MFC can efficiently represent the human sound signals accurately  [35] ,  [40] ,  [41] .\n\nThe periodogram-based power spectral estimate of the target speech frame s t (m, k) for the m th frame at the k th frequency bin index can be expressed as follows  [42] ,  [43] ,\n\nwhere k and N represents the index of the frequency bin, k = 0, .., K-1 and the frame length, respectively.\n\nIn order to compute the mel-spaced filter bank, the sum of the periodogram power spectral estimate of 26 triangular filters are calculated. Log value of each of the 26 energies will give log filter bank energies. Discrete Cosine Transform (DCT) of the log filter bank energies are computed to get MFCC  [42] ,  [43] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "D. Proposed Design",
      "text": "Speaker recognition and emotion recognition use a variety of classifiers such as SVM, K-NN, GMM, HMM and Multilayer Perceptron (MLP). Among all these classifiers, many studies in speech processing used the GMM classifier since it can represent the complex distribution attributes as multiple modes  [43] . Hence, the GMM classifier is considered as a suitable selection for noise suppression, speaker identification, and emotion recognition applications.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Gmm Tag Generation",
      "text": "The Gaussian mixture density model is a weighted sum of M component densities and it can be defined as  [43] ,  [44] ,\n\nwhere x ÃÖand b i (x ÃÖ) represent the D-dimensional random vector and the component densities for i = 1, . M, respectively. The P i , for i = 1,.,M, are the mixture weights. The b i (x ÃÖ) is given by the equation,\n\nwhere Œº ÃÖ i and ‚àë i are the mean vector and the covariance matrix, respectively.\n\nThe GMM tag Œª is the collective representation of the following GMM parameters: mean Œº ÃÖ i , covariance Œ£ i , and weights P i . It is expressed by the following notation, Œª = {P i , Œº ÃÖ i , Œ£ i } , for i = 1,.., M",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Gmm Evaluation",
      "text": "The \"speaker identification\" algorithm which is based on the GMM classifier uses the features extracted from the test signal. After that, the complex feature distribution is converted into multiple modes of length T. The algorithm uses a convergence method as explained below  [44] :\n\n1. The training of the GMM model is initialized with Œª 2. The new model is computed. Thereby, 3. The process is repeated until the convergence is achieved,\n\nMixture weights are termed as,\n\n,\n\nMeans are given by,  (17)  Variance is defined as,  (18)  where œÉ i\n\nin which P(x t ÃÖ |Œª k ) is given in  (14) .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Cnn Classifier",
      "text": "Convolutional Neural Networks (CNN) classification is one of the cutting-edge classification techniques in machine learning  [45] ,  [46] ,  [47] ,  [48] . In deep learning, CNN models are part of deep neural networks (DNN). CNN classifiers are applicable in acoustic signal processing, as well as other applications. The CNN classifier is used for the precise identification of the target speaker, followed by the GMM classification. A 50-layer Convolutional Neural Network (CNN) is employed for classification. Each convolutional layer is followed by a maxpooling layer. The fully connected layers use GMM tags to tune the final result from the CNN classifier. For every input at the fully connected layers, the system evaluates the GMM tag value in order to filter the results at the output stage. The decision will be a binary 0 or 1 based on the GMM tag.\n\nThis paper uses a CNN with fifty hidden layers in addition to input and output layers. The activation function used in the hidden neurons is the \"Rectified Linear Unit ReLU activation  [49] . After training, the \"CNN model\" produces probability distribution P over all emotions. After that, the decision block selects the particular model having the highest probability value.\" The speech signal consists of linguistic part, emotional/stressful part, noise and distortions. Hence, the simple speaker identification system with feature extraction followed by classification is not sufficient to support human-machine interaction systems. This work proposes an efficient speaker identification algorithm that can identify and recognize the speaker in both emotional and noisy talking conditions. This is achieved by incorporating CASA-based pre-processing module, MFCC based feature extraction and cascaded GMM-CNN classifier. The noise of the target signal features is extracted using MFCC and are fed to the cascaded \"GMM-CNN classifier. During the evaluation phase, the log likelihood distance between the voice query and each of the GMM tags is compared for each of the emotional/stressful state and, thus, produces a recent vector of features, which is considered as the input of the CNN classifier.\" The CNN classifier provides the final decision.\n\nThe algorithm of the whole system is shown below:",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Results And Discussion",
      "text": "This work implements the proposed GMM-CNN model for effective speaker identification in emotional and noisy talking conditions. To evaluate the proposed algorithm, evaluation metrics such as Speaker Identification Performance (SID), Precision, Recall, F1 score and Area Under the Curve (AUC) metrics have been used.\n\nThe results show that every model functions almost ideally in neutral talking conditions. The proposed GMM-CNN model outperforms all other models using the SUSAS dataset and based on the performance evaluation metrics reported in Equations 20 to 23  [50] :\n\n\"Totalnumber of times the unknown speaker has been identified correctly\" SID Performance 100% \"Totalnumber of trials\" =ÔÇ¥\n\nWhere \"TP, TN, FP and FN are the True Positive, True Negative, False Positive and False Negative values, respectively are obtained from the confusion matrix.\"\n\nThe average text-independent speaker identification in each of neutral and emotional/stressful environments using the SUSAS dataset in view of each of CASA-based GMM-CNN, GMM-CNN, SVM and MLP is 84.49%, 80.45%, 76.77% and 77.24%, respectively as illustrated in Figure  4 . This shows that the CASA-based GMM-CNN model outperforms other models using SUSAS database. Moreover, the highest and lowest SIDS are reported for Neutral and Angry, respectively and this is consistent with prior work. In order to confirm our results, statistical tests should be used to check if the CASA-based GMM-CNN is statistically different from other models. Before we use a proper statistical test, we have to check the distribution of the SID Performance. Using the Kolmogorov-Smirnov normality test, we found that SID Performance is not normally distributed, so we have to use non-parametric tests  [51] . The Wilcoxon test  [51] , which is a non-parametric test was used to compare two models. Based on the results, we notice that the proposed CASA-based GMM-CNN is statistically different from other models based on 90% confidence level. Hence, we can confirm that the CASA-,\n\nbased GMM-CNN model surpasses other models and it is also statistically different from other models.\n\nIn order to generalize the validity of the proposed model, we conducted six additional experiments to assess the speaker identification performance achieved in neutral and emotional/stressful conditions using the CASA-based GMM-CNN classification algorithm. These experiments are:   Experiment 2: Another assessment of the proposed CASA-based GMM-CNN using the SUSAS database has been conducted using ten nonprofessional audience members (human judges). Overall, thirty speech samples are used in this experiment. During the testing stage, the speech samples were mixed with noise signals in a ratio 2:1. Figure  6  illustrates that the \"human listener performance\" is close to the proposed CASA-based GMM-CNN system.\n\nExperiment 3: The proposed system was also evaluated by using a private Arabic Emirati-accented dataset. In this experiment, a \"32-dimension feature analysis of MFCCs (16 static MFCCs and 16 delta MFCCs) was utilized to find the observation vectors in CASA-based GMM-CNN\".    This experiment evaluates the proposed system performance using angry, happy, neutral, sad, fearful, disgust, calm and surprise talking conditions using the RAVDESS dataset. Table  4  shows the average speaker identification performance based on each of GMM-CNN, SVM and MLP in neutral and emotional/stressful talking environments utilizing the RAVDESS database with and without CASA.\n\nThe proposed GMM-CNN model outperforms other models with and without CASA. We also notice that the performance of the models improves when CASA is used. Table  5  shows the average values of Precision, Recall, F1 Score and ROC of all emotions of GMM-CNN, SVM and MLP in neutral and emotional/stressful talking environments utilizing the RAVDESS database. Experiment 5: Proposed CASA-based GMM-CNN performance has been evaluated using a nonemotional speech corpus called Fluent Speech Command Dataset  [29] . Table  6  shows the evaluation metrics of the proposed model, as well as other models. This also confirms that the proposed model surpasses other models using this dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Table 6",
      "text": "Evaluation based on each of GMM-CNN, SVM and MLP utilizing the Fluent Speech Command database.\n\nExperiment 6: This experiment evaluates the classifiers GMM, CNN, GMM-CNN and CNN-GMM using ESD. Table  7  shows the average emotion recognition rate obtained using GMM alone, CNN alone, GMM-CNN and CNN-GMM. It is clear that the proposed GMM-CNN outperforms other classifiers. The ratio of the computational complexity with reference to GMM alone is 2, 6 and 7 respectively for CNN alone, GMM-CNN and CNN-GMM classifiers. It is evident from this experiment that hybrid classifier of GMM-CNN performs well in terms of performance with reduced computational complexity.   6 , the proposed classification method demonstrates a positive improvement rate over the literature.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Conclusions",
      "text": "Novel CASA-based GMM-CNN classifier has been introduced and evaluated to improve the performance of text-independent speaker identification in noisy emotional talking environments using four diverse corpora.\n\nIn this work, we show that the proposed CASA GMM-CNN model has higher SID, Precision, Recall, F1 Score and ROC than that of other classifiers such as SVM and MLP. All models are evaluated using four distinct datasets including SUSAS public English dataset, ESD private Arabic dataset, RAVDESS public English dataset and the Fluent Speech Command public English dataset.\n\nThe proposed system also yields higher performance in noisy speech signals. The algorithm based on \"GMM tag based-feature vector reduction\" helps to minimize the complications of the CNN classifier, thus, improving system performance with reduced computational complexity. The proposed classifier outperforms other classifiers even in the presence of interference. The performance of all models has been improved when CASA system is being used.\n\nCASA based pre-processing module makes the system more efficient in noisy talking conditions. The CASA preprocessing module segregates the dominant signal from other interference signals before performing the speaker recognition task. This leads the system to perform more efficiently even in noise susceptible real applications.\n\nThe proposed system demonstrates improvement in angry talking condition. This is achieved by the combined effects of CASA and GMM-CNN classifier systems. CASA separates the dominant signal features from the distorted input signal, which enables the classifier to perform more efficiently in such a talking condition.\n\nThe CASA based pre-processing module plays an important role in system performance. The proposed algorithm uses a STFT-based frequency mask for speech separation from the noise signal. However, there is a dilemma in Time and Frequency analysis. Larger window size offers higher accuracy in the frequency domain. Smaller window size offers better accuracy in the time domain. Accuracy in both time and frequency domains is necessary to achieve better system performance.\n\nFurther study is necessary to improve system performance. The pitch estimation method needs to be enhanced since pitch is the main cue for speech segregation and can incorporate additional preprocessing speech de-reverberation techniques to enhance the scalability in reverberant conditions.",
      "page_start": 19,
      "page_end": 19
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the CASA based speech separation block diagram. This figure consists of",
      "page": 6
    },
    {
      "caption": "Figure 1: Original speech separation from noisy input signal",
      "page": 7
    },
    {
      "caption": "Figure 2: MFCC feature extraction [39]",
      "page": 9
    },
    {
      "caption": "Figure 2: shows the basic flow diagram of MFCC extraction. In this figure, windowing involves the",
      "page": 9
    },
    {
      "caption": "Figure 3: demonstrates the basic schematic blocks of the proposed speaker identification system. It",
      "page": 11
    },
    {
      "caption": "Figure 3: Block schematic of the proposed system",
      "page": 12
    },
    {
      "caption": "Figure 4: ‚ÄúAverage speaker identification performance evaluation‚Äù based on CASA-based GMM-DNN,",
      "page": 14
    },
    {
      "caption": "Figure 5: and Table 1, the proposed CASA-based cascaded GMM-CNN classifier",
      "page": 14
    },
    {
      "caption": "Figure 5: Average speaker identification performance evaluation using SUSAS based on each of CASA-based",
      "page": 15
    },
    {
      "caption": "Figure 6: illustrates that the ‚Äúhuman listener performance‚Äù is",
      "page": 15
    },
    {
      "caption": "Figure 6: ‚ÄúSpeaker identification performance analysis based on the proposed-CASA based GMM-CNN and",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Training Set": "",
          "GMM Tags \nNeutral \nAngry \nSlow": "Loud"
        },
        {
          "Training Set": "",
          "GMM Tags \nNeutral \nAngry \nSlow": "Fast"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table 1: , the proposed CASA-based cascaded GMM-CNN classifier",
      "data": [
        {
          "Speaker identification performance (%)\n95.4\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\n0": "",
          "89.6\n86.7\n84.5\n84.3\n93.1\n71.6\n77.6\n59.6\n93.8\n81.4\n68.3\n92.6\n58.9\n79.3\nEmotional state": ""
        },
        {
          "Speaker identification performance (%)\n95.4\n100\n90\n80\n70\n60\n50\n40\n30\n20\n10\n0": "",
          "89.6\n86.7\n84.5\n84.3\n93.1\n71.6\n77.6\n59.6\n93.8\n81.4\n68.3\n92.6\n58.9\n79.3\nEmotional state": ""
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 1: , the proposed CASA-based cascaded GMM-CNN classifier",
      "data": [
        {
          "GMM-\nCNN": "0.84",
          "SVM": "0.78",
          "MLP": "0.78"
        },
        {
          "GMM-\nCNN": "0.80",
          "SVM": "0.70",
          "MLP": "0.70"
        },
        {
          "GMM-\nCNN": "0.82",
          "SVM": "0.72",
          "MLP": "0.69"
        },
        {
          "GMM-\nCNN": "0.81",
          "SVM": "0.71",
          "MLP": "0.69"
        },
        {
          "GMM-\nCNN": "0.80",
          "SVM": "0.70",
          "MLP": "0.60"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 3: shows the average values of Precision, Recall, F1 Score and ROC (AUC) of all talking",
      "data": [
        {
          "‚ÄúAverage Speaker Identification Performance‚Äù (%)": "‚ÄúGMM-CNN‚Äù"
        },
        {
          "‚ÄúAverage Speaker Identification Performance‚Äù (%)": "without \nCASA"
        },
        {
          "‚ÄúAverage Speaker Identification Performance‚Äù (%)": "76.8"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 3: Evaluation of GMM-CNN, SVM and MLP in neutral and emotional/stressful talking environments",
      "data": [
        {
          "Metric": "Precision",
          "GMM-\nCNN": "0.81",
          "SVM": "0.74",
          "MLP": "0.70"
        },
        {
          "Metric": "Recall",
          "GMM-\nCNN": "0.80",
          "SVM": "0.72",
          "MLP": "0.69"
        },
        {
          "Metric": "F1 score",
          "GMM-\nCNN": "0.80",
          "SVM": "0.73",
          "MLP": "0.69"
        },
        {
          "Metric": "ROC",
          "GMM-\nCNN": "0.80",
          "SVM": "0.70",
          "MLP": "0.60"
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 3: Evaluation of GMM-CNN, SVM and MLP in neutral and emotional/stressful talking environments",
      "data": [
        {
          "‚ÄúSpeaker Identification Performance‚Äù (%)": "‚ÄúGMM-CNN‚Äù \n‚ÄúSVM‚Äù \n‚ÄúMLP‚Äù"
        },
        {
          "‚ÄúSpeaker Identification Performance‚Äù (%)": "without \nwith CASA \nwithout \nwith CASA \nwithout \nwith CASA \nCASA \nCASA \nCASA"
        },
        {
          "‚ÄúSpeaker Identification Performance‚Äù (%)": ""
        }
      ],
      "page": 17
    },
    {
      "caption": "Table 5: Precision, Recall, F1 Score and ROC metrics based on each of GMM-CNN, SVM and MLP in",
      "data": [
        {
          "Metric": "Precision",
          "GMM-CNN": "0.82",
          "SVM": "0.64",
          "MLP": "0.67"
        },
        {
          "Metric": "Recall",
          "GMM-CNN": "0.81",
          "SVM": "0.62",
          "MLP": "0.65"
        },
        {
          "Metric": "F1 score",
          "GMM-CNN": "0.81",
          "SVM": "0.62",
          "MLP": "0.65"
        },
        {
          "Metric": "ROC",
          "GMM-CNN": "0.80",
          "SVM": "0.61",
          "MLP": "0.61"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 5: Precision, Recall, F1 Score and ROC metrics based on each of GMM-CNN, SVM and MLP in",
      "data": [
        {
          "Metric": "SID",
          "GMM-CNN": "0.89",
          "SVM": "0.83",
          "MLP": "0.85"
        },
        {
          "Metric": "Precision",
          "GMM-CNN": "0.82",
          "SVM": "0.64",
          "MLP": "0.67"
        },
        {
          "Metric": "Recall",
          "GMM-CNN": "0.81",
          "SVM": "0.62",
          "MLP": "0.65"
        },
        {
          "Metric": "F1 score",
          "GMM-CNN": "0.81",
          "SVM": "0.62",
          "MLP": "0.65"
        },
        {
          "Metric": "ROC",
          "GMM-CNN": "0.80",
          "SVM": "0.61",
          "MLP": "0.61"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 5: Precision, Recall, F1 Score and ROC metrics based on each of GMM-CNN, SVM and MLP in",
      "data": [
        {
          "GMM": "0.71",
          "CNN": "0.82",
          "GMM-CNN": "0.87",
          "CNN-GMM": "0.83"
        },
        {
          "GMM": "1",
          "CNN": "2",
          "GMM-CNN": "6",
          "CNN-GMM": "7"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table 8: Comparison between the proposed CASA GMM-CNN classifier and related work in noisy",
      "data": [
        {
          "Reference": "Zhao  et  al. \n[14]",
          "Talking \nEnvironment": "noisy",
          "Classifier": "GMM-\nUBM",
          "Features": "GFCC \ncoefficients",
          "Dataset": "2002 \nNIST \nSpeaker \nRecognition \nEvaluation \ncorpus",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "4.33"
        },
        {
          "Reference": "Patnala and \nPrasad [16]",
          "Talking \nEnvironment": "noisy",
          "Classifier": "GMM-\nUBM",
          "Features": "GFCC",
          "Dataset": "Private dataset",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "39.08"
        },
        {
          "Reference": "Islam  et  al. \n[18]",
          "Talking \nEnvironment": "noisy",
          "Classifier": "GMM-\nUBM",
          "Features": "2-D \nneurogram \ncoefficients",
          "Dataset": "TIMIT",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "59.37"
        },
        {
          "Reference": "",
          "Talking \nEnvironment": "",
          "Classifier": "",
          "Features": "",
          "Dataset": "TIDIGT",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "35.46"
        },
        {
          "Reference": "",
          "Talking \nEnvironment": "",
          "Classifier": "",
          "Features": "",
          "Dataset": "YOHO",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "36.5"
        },
        {
          "Reference": "",
          "Talking \nEnvironment": "",
          "Classifier": "",
          "Features": "",
          "Dataset": "UM (text-dependent)",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "20.91"
        },
        {
          "Reference": "Faragallah \n[19]",
          "Talking \nEnvironment": "noisy",
          "Classifier": "SVM",
          "Features": "MKMFCC",
          "Dataset": "Private Arabic dataset",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "3.5"
        },
        {
          "Reference": "Korba et al. \n[20]",
          "Talking \nEnvironment": "noisy",
          "Classifier": "GMM",
          "Features": "MVA \nmethod \napplied to the \nMFCC \nfeatures \nas \npost-\nprocessing \nstage",
          "Dataset": "TIMIT dataset",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "40.66"
        },
        {
          "Reference": "Ayhan  and \nKwan [21]",
          "Talking \nEnvironment": "noisy",
          "Classifier": "GMM",
          "Features": "GFCC  with \nBounded \nmarginalizati\non",
          "Dataset": "Private dataset 1",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "38.06"
        },
        {
          "Reference": "",
          "Talking \nEnvironment": "",
          "Classifier": "",
          "Features": "",
          "Dataset": "Private dataset 2",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "34.32"
        },
        {
          "Reference": "",
          "Talking \nEnvironment": "",
          "Classifier": "",
          "Features": "",
          "Dataset": "RM1",
          "Improvement Rate \nof the proposed \nmodel over prior \nwork (%)": "4.28"
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Evaluation of a novel fuzzy sequential pattern recognition tool (fuzzy elastic matching machine) and its applications in speech and handwriting recognition",
      "authors": [
        "S Shahmoradi",
        "S Bagheri",
        "Shouraki"
      ],
      "year": "2018",
      "venue": "Appl. Soft Comput",
      "doi": "10.1016/J.ASOC.2017.10.036"
    },
    {
      "citation_id": "2",
      "title": "Hybrid BBO_PSO and higher order spectral features for emotion and stress recognition from natural speech",
      "authors": [
        "M Hariharan",
        "R Ngadiran",
        "A Adom",
        "S Yaacob",
        "K Polat"
      ],
      "year": "2017",
      "venue": "Appl. Soft Comput",
      "doi": "10.1016/J.ASOC.2017.03.013"
    },
    {
      "citation_id": "3",
      "title": "Speech Recognition Using Deep Neural Networks: A Systematic Review",
      "authors": [
        "A Nassif",
        "I Shahin",
        "I Attili",
        "M Azzeh",
        "K Shaalan"
      ],
      "year": "2019",
      "venue": "Speech Recognition Using Deep Neural Networks: A Systematic Review",
      "doi": "10.1109/ACCESS.2019.2896880"
    },
    {
      "citation_id": "4",
      "title": "Speech recognition using a wavelet packet adaptive network based fuzzy inference system",
      "authors": [
        "E Avci",
        "Z Akpolat"
      ],
      "year": "2006",
      "venue": "Expert Syst. Appl",
      "doi": "10.1016/J.ESWA.2005.09.058"
    },
    {
      "citation_id": "5",
      "title": "Speaker Identification in Different Emotional States in Arabic and English",
      "authors": [
        "A Meftah",
        "H Mathkour",
        "S Kerrache",
        "Y Alotaibi"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.2983029"
    },
    {
      "citation_id": "6",
      "title": "Speaker-dependent-feature extraction, recognition and processing techniques",
      "authors": [
        "S Furui"
      ],
      "year": "1991",
      "venue": "Speech Commun",
      "doi": "10.1016/0167-6393(91)90054-W"
    },
    {
      "citation_id": "7",
      "title": "SpeakerGAN: Speaker identification with conditional generative adversarial network",
      "authors": [
        "L Chen",
        "Y Liu",
        "W Xiao",
        "Y Wang",
        "H Xie"
      ],
      "year": "2020",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "8",
      "title": "Opportunities and Challenges in Automatic Speech Recognition",
      "authors": [
        "R Makhijani",
        "U Shrawankar",
        "V Thakare"
      ],
      "year": "2018",
      "venue": "Proc. Int. Conf. BEATS, 2010: pp"
    },
    {
      "citation_id": "9",
      "title": "A deep learning approach for speaker recognition",
      "authors": [
        "S Hourri",
        "J Kharroubi"
      ],
      "year": "2020",
      "venue": "Int. J. Speech Technol"
    },
    {
      "citation_id": "10",
      "title": "Auditory Scene Analysis, Book",
      "authors": [
        "A Bregman"
      ],
      "year": "1990",
      "venue": "Auditory Scene Analysis, Book"
    },
    {
      "citation_id": "11",
      "title": "A computational auditory scene analysis system for speech segregation and robust speech recognition",
      "authors": [
        "Y Shao",
        "S Srinivasan",
        "Z Jin",
        "D Wang"
      ],
      "year": "2010",
      "venue": "Comput. Speech Lang",
      "doi": "10.1016/j.csl.2008.03.004"
    },
    {
      "citation_id": "12",
      "title": "Robust speaker identification using auditory features and computational auditory scene analysis",
      "authors": [
        "Y Shao",
        "D Wang"
      ],
      "year": "2008",
      "venue": "IEEE Int. Conf. Acoust. Speech Signal Process"
    },
    {
      "citation_id": "13",
      "title": "Speaker Recognition in an Emotional Environment, in: Int. Conf. Eco-Friendly Comput",
      "authors": [
        "M Vasile Ghiurcau",
        "C Rusu",
        "J Astola"
      ],
      "year": "2012",
      "venue": "Speaker Recognition in an Emotional Environment, in: Int. Conf. Eco-Friendly Comput"
    },
    {
      "citation_id": "14",
      "title": "CASA-Based Robust Speaker Identification",
      "authors": [
        "X Zhao",
        "Y Shao",
        "D Wang"
      ],
      "year": "2012",
      "venue": "IEEE Trans. Audio. Speech. Lang. Processing",
      "doi": "10.1109/TASL.2012.2186803"
    },
    {
      "citation_id": "15",
      "title": "Cost-sensitive learning for emotion robust speaker recognition",
      "authors": [
        "D Li",
        "Y Yang",
        "W Dai"
      ],
      "year": "2014",
      "venue": "Sci. World J",
      "doi": "10.1155/2014/628516"
    },
    {
      "citation_id": "16",
      "title": "A Novel Scheme For Robust Speaker Identification in Presence Of Noise and Reverberations",
      "authors": [
        "M Kumar Patnala",
        "P Prasad"
      ],
      "year": "2015",
      "venue": "Int. J. Sci. Res. Eng. Technol"
    },
    {
      "citation_id": "17",
      "title": "Emotional speaker recognition based on i-vector space model",
      "authors": [
        "A Mansour",
        "F Chenchah",
        "Z Lachiri"
      ],
      "year": "2016",
      "venue": "4th Int. Conf. Control Eng",
      "doi": "10.1109/CEIT.2016.7929127"
    },
    {
      "citation_id": "18",
      "title": "A Robust Speaker Identification System Using the Responses from a Model of the Auditory Periphery",
      "authors": [
        "M Islam",
        "W Jassim",
        "N Cheok",
        "M Zilany"
      ],
      "year": "2016",
      "venue": "PLoS One",
      "doi": "10.1371/journal.pone.0158520"
    },
    {
      "citation_id": "19",
      "title": "Robust noise MKMFCC-SVM automatic speaker identification",
      "authors": [
        "O Faragallah"
      ],
      "year": "2018",
      "venue": "Int. J. Speech Technol",
      "doi": "10.1007/s10772-018-9494-9"
    },
    {
      "citation_id": "20",
      "title": "Text-Independent Speaker Identification by Combining MFCC and MVA Features",
      "authors": [
        "M Korba",
        "H Bourouba",
        "D Rafik"
      ],
      "year": "2018",
      "venue": "Int. Conf. Signal, Image",
      "doi": "10.1109/SIVA.2018.8661138"
    },
    {
      "citation_id": "21",
      "title": "Robust Speaker Identification Algorithms and Results in Noisy Environments",
      "authors": [
        "B Ayhan",
        "C Kwan"
      ],
      "year": "2018",
      "venue": "Neural Networks",
      "doi": "10.1007/978-3-319-92537-0_51"
    },
    {
      "citation_id": "22",
      "title": "Int. J. Speech Technol",
      "authors": [
        "M Nasr",
        "M Abd-Elnaby",
        "A El-Fishawy",
        "S El-Rabaie",
        "F El-Samie"
      ],
      "year": "2018",
      "venue": "Int. J. Speech Technol"
    },
    {
      "citation_id": "23",
      "title": "Employing both gender and emotion cues to enhance speaker identification performance in emotional talking environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2013",
      "venue": "Int. J. Speech Technol",
      "doi": "10.1007/s10772-013-9188-2"
    },
    {
      "citation_id": "24",
      "title": "Emirati-accented speaker identification in each of neutral and shouted talking environments",
      "authors": [
        "I Shahin",
        "A Nassif",
        "M Bahutair"
      ],
      "year": "2018",
      "venue": "Int. J. Speech Technol",
      "doi": "10.1007/s10772-018-9502-0"
    },
    {
      "citation_id": "25",
      "title": "Employing Second-Order Circular Suprasegmental Hidden Markov Models to Enhance Speaker Identification Performance in Shouted Talking Environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2010",
      "venue": "EURASIP J. Audio, Speech, Music Process",
      "doi": "10.1155/2010/862138"
    },
    {
      "citation_id": "26",
      "title": "Novel cascaded Gaussian mixture model-deep neural network classifier for speaker identification in emotional talking environments",
      "authors": [
        "I Shahin",
        "A Nassif",
        "S Hamsa"
      ],
      "year": "2020",
      "venue": "Neural Comput. Appl",
      "doi": "10.1007/s00521-018-3760-2"
    },
    {
      "citation_id": "27",
      "title": "Getting Started with SUSAS: A Speech Under Simulated and Actual Stress Database",
      "authors": [
        "J Hansen",
        "S Bou-Ghazale"
      ],
      "year": "1997",
      "venue": "Getting Started with SUSAS: A Speech Under Simulated and Actual Stress Database"
    },
    {
      "citation_id": "28",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS One"
    },
    {
      "citation_id": "29",
      "title": "Fluent Speech Commands: A dataset for spoken language understanding research",
      "year": "2020",
      "venue": "Fluent Speech Commands: A dataset for spoken language understanding research"
    },
    {
      "citation_id": "30",
      "title": "Monaural Speech Separation",
      "authors": [
        "G Hu",
        "D Wang",
        "B Program"
      ],
      "year": "2003",
      "venue": "Monaural Speech Separation"
    },
    {
      "citation_id": "31",
      "title": "Overview of Front-end Features for Robust Speaker Recognition",
      "authors": [
        "Q Jin",
        "T Zheng"
      ],
      "year": "2011",
      "venue": "Asia-Pacific Signal Inf. Process. Assoc. Annu. Summit"
    },
    {
      "citation_id": "32",
      "title": "Single channel speech separation in modulation frequency domain based on a novel pitch range estimation method",
      "authors": [
        "A Mahmoodzadeh",
        "H Abutalebi",
        "H Soltanian-Zadeh",
        "H Sheikhzadeh"
      ],
      "year": "2012",
      "venue": "EURASIP J. Adv. Signal Process",
      "doi": "10.1186/1687-6180-2012-67"
    },
    {
      "citation_id": "33",
      "title": "Effect of temporal envelope smearing on speech reception",
      "authors": [
        "R Drullman",
        "J Festen",
        "R Plomp"
      ],
      "year": "1994",
      "venue": "J. Acoust. Soc. Am"
    },
    {
      "citation_id": "34",
      "title": "Performance analysis of ideal binary masks in speech enhancement",
      "authors": [
        "Y Jiang",
        "H Zhou",
        "Z Feng"
      ],
      "year": "2011",
      "venue": "4th Int. Congr. Image Signal Process",
      "doi": "10.1109/CISP.2011.6100732"
    },
    {
      "citation_id": "35",
      "title": "Binary mask estimation for voiced speech segregation using Bayesian method",
      "authors": [
        "S Liang",
        "W Liu"
      ],
      "year": "2011",
      "venue": "First Asian Conf. Pattern Recognit",
      "doi": "10.1109/ACPR.2011.6305053"
    },
    {
      "citation_id": "36",
      "title": "Pitch Detection Algorithm: Autocorrelation Method and Amdf, in: 3rd Int",
      "authors": [
        "L Tan",
        "M Karnjanadecha"
      ],
      "year": "2003",
      "venue": "Symp. Commun. Inf. Technol"
    },
    {
      "citation_id": "37",
      "title": "Dominant voiced speech segregation based on Onset offset analysis",
      "authors": [
        "S Hamsa"
      ],
      "year": "2013",
      "venue": "Int. J. Sci. Eng. Res"
    },
    {
      "citation_id": "38",
      "title": "Digital signal processing : principles, algorithms and system design",
      "authors": [
        "W Alexander",
        "C Williams"
      ],
      "year": "2017",
      "venue": "Digital signal processing : principles, algorithms and system design"
    },
    {
      "citation_id": "39",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "C Anagnostopoulos",
        "T Iliou",
        "I Giannoukos"
      ],
      "year": "2015",
      "venue": "Artif. Intell. Rev",
      "doi": "10.1007/s10462-012-9368-5"
    },
    {
      "citation_id": "40",
      "title": "Employing Emotion Cues to Verify Speakers in Emotional Talking Environments",
      "authors": [
        "I Shahin"
      ],
      "year": "2016",
      "venue": "J. Intell. Syst",
      "doi": "10.1515/jisys-2014-0118"
    },
    {
      "citation_id": "41",
      "title": "Emotion Recognition Using Hybrid Gaussian Mixture Model and Deep Neural Network",
      "authors": [
        "I Shahin",
        "A Nassif",
        "S Hamsa"
      ],
      "year": "2019",
      "venue": "Emotion Recognition Using Hybrid Gaussian Mixture Model and Deep Neural Network",
      "doi": "10.1109/ACCESS.2019.2901352"
    },
    {
      "citation_id": "42",
      "title": "Multi-taper MFCC Features for Speaker Verification using I-vectors, in: 2011 IEEE Work",
      "authors": [
        "M Alam",
        "T Kinnunen",
        "P Kenny",
        "P Ouellet",
        "D O'shaughnessy"
      ],
      "year": "2011",
      "venue": "Autom. Speech Recognit. Underst"
    },
    {
      "citation_id": "43",
      "title": "Spoken language processing : a guide to theory, algorithm, and system development",
      "authors": [
        "X Huang",
        "A Acero",
        "H.-W Hon"
      ],
      "year": "2001",
      "venue": "Spoken language processing : a guide to theory, algorithm, and system development"
    },
    {
      "citation_id": "44",
      "title": "Robust text-independent speaker identification using Gaussian mixture speaker models",
      "authors": [
        "D Reynolds",
        "R Rose"
      ],
      "year": "1995",
      "venue": "IEEE Trans. Speech Audio Process",
      "doi": "10.1109/89.365379"
    },
    {
      "citation_id": "45",
      "title": "Semisupervised 3D object recognition through CNN labeling",
      "authors": [
        "J Rangel",
        "J Mart√≠nez-G√≥mez",
        "C Romero-Gonz√°lez",
        "I Garc√≠a-Varea",
        "M Cazorla"
      ],
      "year": "2018",
      "venue": "Appl. Soft Comput",
      "doi": "10.1016/J.ASOC.2018.02.005"
    },
    {
      "citation_id": "46",
      "title": "Classification of multiple motor imagery using deep convolutional neural networks and spatial filters",
      "authors": [
        "B Olivas-Padilla",
        "M Chacon-Murguia"
      ],
      "year": "2019",
      "venue": "Appl. Soft Comput",
      "doi": "10.1016/J.ASOC.2018.11.031"
    },
    {
      "citation_id": "47",
      "title": "Automatic determination of digital modulation types with different noises using Convolutional Neural Network based on time-frequency information",
      "authors": [
        "N Daldal",
        "Z C√∂mert",
        "K Polat"
      ],
      "year": "2019",
      "venue": "Appl. Soft Comput",
      "doi": "10.1016/J.ASOC.2019.105834"
    },
    {
      "citation_id": "48",
      "title": "Analysis of DNN approaches to speaker identification",
      "authors": [
        "P Matejka",
        "O Glembek",
        "O Novotny",
        "O Plchot",
        "F Grezl",
        "L Burget",
        "J Cernocky"
      ],
      "year": "2016",
      "venue": "2016 IEEE Int. Conf. Acoust. Speech Signal Process",
      "doi": "10.1109/ICASSP.2016.7472649"
    },
    {
      "citation_id": "49",
      "title": "Empirical Evaluation of Rectified Activations in Convolution Network, ArXiv Prepr",
      "authors": [
        "B Xu",
        "N Wang",
        "T Chen",
        "M Li"
      ],
      "year": "2015",
      "venue": "Empirical Evaluation of Rectified Activations in Convolution Network, ArXiv Prepr"
    },
    {
      "citation_id": "50",
      "title": "The relationship between Precision-Recall and ROC curves",
      "authors": [
        "J Davis",
        "M Goadrich"
      ],
      "year": "2006",
      "venue": "Proc. 23rd Int. Conf. Mach. Learn"
    },
    {
      "citation_id": "51",
      "title": "A generalized Wilcoxon test for comparing arbitrarily singly-censored samples",
      "authors": [
        "E Gehan"
      ],
      "year": "1965",
      "venue": "Biometrika",
      "doi": "10.1093/biomet/52.1-2.203"
    }
  ]
}