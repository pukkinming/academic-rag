{
  "paper_id": "2107.10742v2",
  "title": "Multi-Modal Residual Perceptron Network For Audio-Video Emotion Recognition",
  "published": "2021-07-21T13:11:37Z",
  "authors": [
    "Xin Chang",
    "Władysław Skarbek"
  ],
  "keywords": [
    "emotion recognition",
    "deep neural network",
    "multi-modal classifier",
    "deep features fusion",
    "audio sensor",
    "video sensor"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition is an important research field for Human-Computer Interaction. Audio-Video Emotion Recognition is now attacked with Deep Neural Network modeling tools. In published papers, as a rule, the authors show only cases of the superiority in multi-modality over audio-only or video-only modality. However, there are cases superiority in uni-modality can be found. In our research, we hypothesize that for fuzzy categories of emotional events, the within-modal and inter-modal noisy information represented indirectly in the parameters of the modeling neural network impedes better performance in the existing late fusion and end-to-end multi-modal network training strategies. To take advantage and overcome the deficiencies in both solutions, we define a Multi-modal Residual Perceptron Network which performs end-to-end learning from multi-modal network branches, generalizing better multi-modal feature representation. For the proposed Multi-modal Residual Perceptron Network and the novel time augmentation for streaming digital movies, the state-of-art average recognition rate was improved to 91.4% for The Ryerson Audio-Visual Database of Emotional Speech and Song dataset and to 83.15% for Crowd-sourced Emotional multi-modal Actors dataset. Moreover, the Multi-modal Residual Perceptron Network concept shows its potential for multi-modal applications dealing with signal sources not only of optical and acoustical types.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Emotion Recognition From Face Expression And Voice Timbre",
      "text": "Intuitively, functionalities of intelligent artificial neurons are assigned with similar concepts as our brain cells are, processing information from the very raw senses independently and appropriately to their types.\n\nVisually, information captured by the camera is distributed to several frames as Figure  2  shows. Discrete information in a single frame is firstly delivered to pattern extracting intelligent sensors for features such as Fisherfaces and Eigenfaces  [1]  or the deep features from Convolution Neural Network (CNN)  [2] . To fully preserve the information from the discrete signals, some Sequence Aggregation Component (SAC), e.g., Long Short Term Memory (LSTM)  [3]  or Transformer  [4] , is then needed to further process the extracted features. Finally, a classifier such as Support Vector Machine (SVM) or some neural dense layer takes the integrated features for the classification. Raw vocal inputs are usually with 10,000 to 44,100 samples per second, while the visual frame rate is about 25-30 image frames per second. Though raw digital signals in the time domain approximate the original signal precisely, their spectral representation, e.g., Spectrogram frame, Mel-spectrogram coefficients, or Log Mel-spectrogram frame, appeared more effectively for sound recognition. The spectral converted vocal signals have shown significant improvements in many classification problems, in spite of some limitations. Expression events do not last at the same time, thus the width of the Spectrogram frames changes, which is not desirable for CNN pattern extractors. Therefore, the extracted features also need further processing from SAC which outputs integrated features. Figure  3  shows the expression events from different categories and with different time duration.",
      "page_start": 1,
      "page_end": 3
    },
    {
      "section_name": "Multi-Modal Emotion Recognition",
      "text": "AVER solution also follows the sensation of human beings, people claim they hear the sound when looking at the sheet music, smell the odor when recalling the memory from a photo or see the sea sight from the smell of the air. The multi-sensation information is processed by different areas of our cerebral cortex, movement, hearing, seeing, etc., then highly correlated by some other brain areas. Thus, the decision made depends not just on the recognition of uni-modal sensations independently but also jointly.\n\nThe learning process of neural sensors should also mimic our learning process. The neurons shape their weights just like our cerebral cortex changes from the stimulation of the environments and look for any correlation of them during the learning process in the supervised neural network sensors training. However, we claim the existing late fusion and end-to-end training strategies hold their own advantages but also deficiencies.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Paper Contribution And Structure",
      "text": "This paper shows by experiments the deficiencies of two training strategies: late fusion and end-to-end. Late fusion strategy takes trained static uni-modal networks and trains their fusing network components. End-to-end strategy trains all the multi-modal and uni-modal components together. A novel architecture is proposed to take advantage of both solutions and avoid their side effects, respectively. We demonstrate superiority in the novel end-to-end mechanism and architecture comparing with naive fusion mechanism in either late fusion or end-to-end training. The proposed DNN framework, data augmentation procedures, and network optimization strategy are discussed. The detailed analysis and discussion are presented by computing experiments on The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) and Crowd-Sourced Emotional Multimodal Actors Dataset (Crema-d) datasets. Our major contributions are concluded as follows:\n\n1.\n\nMulti-modal framework: We propose a novel within-modality Residual Perceptrons (RP) for efficient gradient blending for the neural network optimization using multi-term loss function in MRPN. The sub-networks and target loss functions produce superior parameterized multi-modal features, preserving the original knowledge of uni-modalities, which impedes inter-modal learning. The within-modality RP components reduce the side effects brought from such multi-term loss functions. As the result, we got significantly better performance over direct strategies including late fusion and end-to-end without MRPN.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "2.",
      "text": "Time Augmentation of input frames: We demonstrate data augmentation in time involving randomly slicing over input frame sequences from both modalities improved the recognition performance to the state-of-art, even without MRPN. We show also the results that time augmentation doesn't solve the cases where uni-modal solutions are better than multi-modal solutions, yet solved by MRPN.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "2. Related Work",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "2.1. Superiority In Multi-Modal Approach",
      "text": "Many have shown significant improvement of multi-modal solutions. N. Neverova et al.  [5]  suggest gradual fusion involving the random dropping of separate channels, and this method was adopted by V. Vielzeuf et al.  [6]  in the AVER solution for their best result.\n\nFusion at the early or late stage is discussed by others. R. Beard et al.  [7]  proposed multi-modal feature fusion at the late stage, while E. Ghaleb et al.  [8]  try to project features to a shared space in the early stage and provided external loss functions to minimize the distance of features from different modalities. A. Zadeh et al.  [9]  proposed Multi-view Gated Memory to gate the multi-modal knowledge from LSTM in the time series. E. Mansouri-Benssassi and J. Ye  [10]  archive early fusion by creating distinct multi-modal neuron groups.\n\nS. Zhang et al.  [11]  take features from CNN and 3D-CNN models for vocal and visual sources then make global averaging as video features. NC. Ristea et al.  [12]  take the features extracted by CNNs from both modalities and exploit the fused features for classification purposes. E. Tzinis et al.  [13]  take cross-modal and self-attention modules. Y. Wu et al.  [14]  localize events crossing modalities. E. Ghaleb et al.  [15]  suggest multi-modal emotion recognition metric learning to create a robust representation for both modalities.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Potential Failures In The Existing Solutions",
      "text": "Let's consider the human brain learning process again. Say the information is wrong in some of the sensory stimulation. A child learned an animal looks just like a dog but having the sound of the cat from the manipulated movies and this kid has never learned the dog and cat in a real-life environment. He will either see a dog and tell it's a cat or hear the cat sound and tell it's a dog. The situation can go even worse if the stimulation he learned from is also fuzzy within their own sensation.\n\nHis recognition is still intact to some extent that he can sometimes correctly recognize the visual or acoustic information pattern. But the recognized information is distorted, along with the correlation of the inter-modal information. This made the distorted uni-modal knowledge he learned having also a negative impact on the other. The same concept we address to the current multi-modal neural network solutions. The within-modal and inter-modal noisiness of the learned pattern both contribute to the wrong recognition. Despite many advantages from the multi-modal solutions which boost the recognition performance of emotion recognition tasks, we hypothesize the uncontrolled fusion strategy, adopted by  [6, 7, 9, 11, 14, [16] [17] [18]  could lead to potential deficiencies in either late fusion or end-to-end training strategy.\n\nThough many have shown superior performance of late fusion strategy  [19] [20] [21] , for instance, for audio events detection in video material, W. Wang et al.  [22]  illustrate the results of naive fusion from multi-modal features can be worse than the best uni-modal approach. They propose blending the gradient flow by multi-task loss functions, which is referred to as multi-term loss function by us, from uni-modalities and multi-modality, which help better parameterization of the whole system in many other research areas. Though they suggest benefits from blending the gradient flows, multi-tasking could make the features hard to be optimized serving both uni-modal and multi-modal purposes suggested by many researchers  [23] [24] [25] . We demonstrate how this proposal can still fail in some inferior cases but is solved by the within-modal RP component in MRPN.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Hypothesis",
      "text": "In this section, we discuss our hypothesis where fuzzy information from the uni-modalities can cause chaos in not just the uni-modal neurons but also the correlation neurons, namely the fusion component.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Within-Modal Information Can Be Missing Or Fuzzy",
      "text": "The missing or fuzzy information can be noticed in either visual or vocal modality emotion recognition solution, and then the success rate of recognition can not be increased noticeably. Missing information refers to feature data where emotion categories are confused with neutral categories in the uni-modality. Fuzzy information stands for feature data where one emotion category cannot be distinguished from another one in the uni-modality.\n\nNamely, the visual modality results from the challenge FER-2013  [26]  for single image facial recognition have only improved about 4% to 76.8% over the past eight years by W. Wang et al.  [27] .\n\nMoreover, for video frames, HW. Ng et al.  [28]  got 47.3% validation accuracy and 53.8% testing accuracy on EmotiW dataset  [29]  using transfer learning and averaged temporal deep features. Similarly, for vocal solutions, the results for Interactive Emotional Dyadic Motion Capture dataset (IEMOCAP)  [30]  with the raw inputs are reported around 76% by S. Kwon  [31]  and 64.93% by S. Latif et al.  [32] . The recognition rate for these cases is far from optimal.\n\nApart from the design, functionality, and training of the neural network, the human voting for those datasets draws our concerns. As the teacher in supervised learning, almost all datasets related to emotion recognition have unsatisfactory knowledge. The ones who understand human emotions the best, the human beings themselves, cannot make a majority of the agreement to the author's labeling. On average, the human rate of the emotional categories is 72% from IEMOCAP, human accuracy on FER-2013  [26]  is 65±5%, Crema-d  [33]  holds the accuracy of 63.6% and RAVDESS  [34]  has the results of 72.3%.\n\nAll the reports point out that in every uni-modality, the information of data is never crystal, thus the learned knowledge of a uni-modality in emotion recognition, can be corrupted and uncontrolled by the network. We can't identify or agree on which samples are wrong because the boundaries of the clusters are quite subjective.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "End-To-End Modeling For Multi-Modal Data Can Be Distorted",
      "text": "Multi-modal solutions seem to find more generalized patterns via the extension of parameters. However, the fused features have left a backdoor for distorted pattern learning, the side effects are concealed by its benefits.  Under such circumstances, we do not know which training sample is fuzzy in which modality, not just it causing the fuzzy direction of within-modal learning, but also inter-modal learning in end-to-end training. i.e. information in modality A is fuzzy while crystal in modality B, can results in correct learning for modality B yet fuzzy learning for modality A. In the end, the distribution of the wrong direction learned knowledge is unknown.\n\nFigure  5  illustrates the source of deficiency in the architecture during the gradient backpropagation, wherein the blue frame denotes the fusion component. Namely, the concatenation unit of the features from different modalities can backpropagate the gradients modifying jointly weights in each modality, potentially distort the knowledge in some modality.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Late Fusion Modeling For Multi-Modal Data Can Be Insufficient",
      "text": "The late fusion seems to prevent the inter-modal learning of the system, however, not just the distribution of the fuzzy information to each modality is unknown, but also the clean data which holds the highly correlated information between modalities are. If the samples contain clean information in all modalities, then the frozen parameters of the shadow layers cannot make proper adjustments to learn inter-modal information from the joint gradient flow.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Proposed Methods",
      "text": "Addressing the mentioned issues, we proposed a novel MRPN along with a multi-term loss function for the better parameterization of the whole network taking advantage of both late fusion and end-to-end strategies while avoiding their deficiencies. MRPN can eliminate the problems without assuming the data is noisy or clean.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Functional Description Of Analyzed Networks",
      "text": "The functional descriptions of the analyzed deep networks are presented for their training mode (see Figure  6 ). They are based on the selected functionalities of neural units and components. We use index m for inputs of any modality. In our experiments m = v or m = a.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "1.",
      "text": "F m : feature extractor for input temporal sequence x m of modality m, e.g. F v for video frames x v , F a for audio segments x a .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "2.",
      "text": "A m : aggregation component SAC for temporal feature sequence leading to temporal feature vector f m , eg. A v , A a for video and audio features, respectively.\n\n3.\n\nStandard computing units: DenseUnitaffine (a.k.a. dense, full connection), Dropout random elements dropping for model regularizing, FeatureNormnormalization for learned data regularizing (batch norm is adopted in the current implementation), and Concatenatejoining feature maps, ReLU, Sigmoidactivation units.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "4.",
      "text": "Scoringcomponent mapping feature vectors to class scores vector, usually composing the following operations:",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "5.",
      "text": "FusionComponentconcatenates its inputs g v , g a , then makes the statistical normaliza- tion, and finally produces the vector of class scores:\n\nIn our networks g v , g a are statistically normalized multi-modal features ( fv , fa ) or their residually updated form ( f v , f a ) -cf. those symbols in Figure  6 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "6.",
      "text": "SoftMaxcomputing unit for normalization of class scores to class probabilities:\n\nCrossEntropya divergence of probability distributions used as loss function. Let p is the target probability distribution. Then the following loss functions are defined:\n\nwhere L is multi-term loss function implying the gradient blending in the backpropagation stage.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "8.",
      "text": "ResPerceptron (Residual Perceptron)component performing statistical normalization for the dense unit (perceptron) computing residuals for normalized data. In our solution it transforms a modal feature vector f m into f m , as follows:\n\nThree networks N 0 , N 1 , N 2 are defined for further analysis: 1.\n\nNetwork N 0 ( f v , f a ; p) with fusion component and loss function L va :\n\n2.\n\nNetwork N 1 ( f v , f a ; p) with fusion component and fused loss function\n\n3.\n\nNetwork N 2 ( f v , f a ; p) with normalized residual perceptron, fusion component and fused loss function L For the networks N 0 , N 1 , N 2 detailed in Figure  6 , we can observe:\n\n1.\n\nAll instances of FeatureNorm unit are implemented as batch normalization units.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "2.",
      "text": "In testing mode only the central branch of networks N 1 , N 2 are active while the side branches are inactive as they are used only to compute the extra terms of the extended loss function.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "3.",
      "text": "The above facts make network architectures N 0 , N 1 equivalent in the testing mode. However, the models trained for those architectures are not the same, as weights are optimized for different loss functions. 4.\n\nIn the testing mode all Dropout units are not active, as well.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "5.",
      "text": "The architecture of FusionComponent is identical for all three networks. The difference between models of N 0 and N 1 networks follows from the different loss functions while the difference between models of N 1 and N 2 networks is implied by using ResPerceptron (RP) components in N 2 network.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "6.",
      "text": "To control the range of affine combinations computed by Residual Perceptron (RP) component, we use Sigmoid activations instead of the ReLU activations exploited in other components. The experiments confirm the advantage of this design decision. 7.\n\nThe Residual Perceptron (RP) was introduced in the network N 2 to implement better parameterization of within-modal features before their fusion.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Mrpn Components' Role In Multi-Term Optimization",
      "text": "1.\n\nAs we discussed in the hypothesis section, the late fusion strategy has the advantage of preserving the best information in each uni-modality, since the uni-modality extracts generalized deep features which suffer few from the outliers of their own modality. i.e. a small amount of wrongly labeled data in uni-modal solutions won't contribute to the generalized feature patterns, they are \"filtered out\" by the uni-modal neural network. Thus the additional term of loss functions implies the blended gradient in the shallow layers of each uni-modality, and helped for better parameterization of the features before fusion, preserving the knowledge as uni-modalities are trained respectively. The above facts make the end-to-end strategy suffer less inter-modal information as late fusion does.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "2.",
      "text": "However, the multi-term optimization can result in extracting inferior uni-modal features as the input to the fusion component. This problem was mentioned in the literature  [23] [24] [25] . RP is introduced to make modified uni-modal features, instead of storing all knowledge for uni-modal and multi-modal purposes in one unit, causing the clash of loss converging from two directions, the uni-modal and multi-modal knowledge can be stored in the original uni-modal features and modified multi-modal features, creating a new path for the gradient flow. RP can preserve the best of the uni-modal solution while the modified features from the short-cut can still fulfill the purpose of integrating new multi-modal features.\n\nThe mentioned two novel properties make MRPN free from side-effects of late fusion and end-to-end strategy while preserving their own advantages. We suggest that MRPN can be adopted in any multi-modal application that involves many multi-modal inputs and one target function or many multi-modal inputs and many multi-modal target functions as Figure  7  shows. In both cases MRPN benefits from many terms of loss functions as the numbers of uni-modalities, updating the whole system together while avoiding learning from inter-modal fuzzy information. MRPN is general to be compatible with any other proposed mechanism.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Mrpn In General Multi-Modal Applications",
      "text": "",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Pre-Processing",
      "text": "Our data pre-processing includes the procedures for both modality inputs, namely spatial and time-dependent augmentation are applied.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "1.",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Spatial Data Augmentation For Visual Frames:",
      "text": "The facial area for the visual input frames is cropped using a CNN solution from Dlib library  [35] . Once the facial area is cropped, spatial video augmentation is applied during the training phase. The same random augmentation parameters are applied for all frames of a video source illustrated in Figure  8 .",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "2.",
      "text": "Time dependent data augmentation for visual frames: Obviously, expressions from the same category do not last the same duration. To make our system robust to the inconsistent duration of the emotion events, we perform data augmentation in time by randomly slicing the original frames as Figure  9  illustrates. Such operation should also avoid too few input frames missing information of the expression events. Thus the training segments are selected to have at least one-second duration unless the original duration of the file is less than that.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Computational Experiments And Their Discussion",
      "text": "This section presents an evaluation regarding the advantages of our proposed framework and time-dependent augmentation. Two datasets, RAVDESS and Crema-d are employed for this purpose. The improvement of the time-augmentation mechanism is analyzed in the naive fusion model which brought us state-of-art results even without MRPN design. The inferior cases in such common neural multi-modal solutions are detected and discussed in the comparison. Improvement of MRPN is then presented, not just in the detected inferior sub-datasets, but also in general data samples.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Datasets",
      "text": "RAVDESS and Crema-d differ in numbers of expression categories, total files, identifies, and also video quality.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "1.",
      "text": "RAVDESS dataset includes both speech and song files. For the speech recognition proposal, we only use the speech files from the dataset. It contains 2880 files, 24 actors (12 female, 12 male), state two lexically identical statements. Speech includes calm, happy, sad, angry, fearful, surprise, and disgusted expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression, in a total of 8 categories. It is the most recent video-audio emotional dataset with the highest video quality in this research area to our best knowledge.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "2.",
      "text": "Crema-d dataset consists of visual and vocal emotional speech files in a range of basic emotional states (happy, sad, anger, fear, disgust, and neutral). 7,442 clips of 91 actors with diverse ethnic backgrounds were rated by multiple raters in three modalities: audio, visual, and audio-visual.\n\nFor both datasets, the training set and testing set are separated using similar concepts as 10-fold inter-validation. Additionally, identities of the actors are also separated in train and val sets to prevent the results from leaning on the actors. Around 10% of the actors are used for validation while the remaining 90% are used for training, male and female actors are balanced in each set. We rotate the split train/validation sub-datasets to get multiple results over the whole dataset. The crema-d dataset has fewer categories for the classification tasks, but from the report of the authors themselves, Crema-d holds the accuracy of 63.6% from human recognition for 6 categories which are less than RAVDESS at 72.3% for 8 categories. The resolution of the video source is verified not the cause of the worse performance. The better results in the RAVDESS dataset, in our opinion, are the more crystal and natural emotion information inside the RAVDESS dataset.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Model Organization And Computational Setup",
      "text": "The naive fusion model N 0 , advanced fusion network N 1 , which is equivalent to the Facebook  [22]  solution and the N 2 (MRPN) have the same CNN extractors at the initial stage of the training. To compare the impact of strategy from features fusion only, CNN extractor architecture is fixed to Resnet-18  [36] .\n\nThe CNN in visual modality is initialized from a facial image expression recognition task, the challenge FER2013  [26] . As for vocal modality, The CNN is pretrained on the voice recognition task from VoxCeleb dataset  [37] . The initialization of the CNN extractors made the whole system much easier to be optimized.\n\nAdaMW optimizer is adopted for the model optimization, with the initial learning rate at 5 • 10 -5 , decreased two times if validation loss is not dropping over ten epochs.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Data Augmentation Cannot Generalize Multi-Modal Feature Patterns",
      "text": "This subsection illustrates the improvement of time-dependent augmentation. The improvement also proves that the inferior case of multi-modal solution doesn't depend on the with-modal patterns. The single modality solutions in our experiments (shown in Table  1 ) take pretrained Resnet-18 as extractors and LSTM cells as SACs. The naive multi-modal solution takes twice of the components with an additional fusion layer as Figure  6  illustrates on the left panel. Adopting time-dependent augmentation shows overall performance improvements on either single or multi-modal solutions. The Table notations are presented in the follows:\n\nIn the variational train/val sub-datasets in Table  1 , Ax,y stands for the validation files that came from actor x and y, odd number notes for a male actor, and even number for a female actor.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Discussion On Inferior Multi-Modal Cases",
      "text": "The time augmentation shows overall improvements in either uni-modal or multi-modal approach, yet the inferior case where uni-modal solution better than multi-modal solution still exists, which suggests data augmentation cannot generalize multi-modal features. Only one inferior case is detected in Table  1  of the case A9,10, but we argue such deficiency is common in fuzzy multi-modal data. The pattern learning ability from both modalities is well enough, both solutions have performance over 85% in cases like A7,8 and A1,2. But the ratio of mismatched learned and target patterns are ranging along with the shuffling of the sub-datasets.\n\nThe degeneration of the performance became visible only because the percentage of the pattern mismatched samples has passed some kind of threshold in the training set. If so, by eliminating or reducing such side effects, overall improvements should be expected for any train and testing sub-dataset.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Improvement Of Mrpn",
      "text": "This subsection addresses the improvement of MRPN preventing the side-effects in the existing late fusion and end-to-end strategies we hypothesized as Table  2  and Table  3  illustrate. The end-to-end strategy of N 1 , which takes multi-term loss function helped the better parameterization shows improved average performance over naive end-to-end and late fusion training strategies, yet it can still fail in some cases. Our proposed MRPN on the contrary demonstrates the same performance or most improvement in any circumstance. It can been seen from the confusion matrices in Figure  10  and 11 the averaged improvements of N 2 (MRPN) over the late fusion and end-to-end N 0 models. Performance on some specific categories shows a slight decrease for MRPN, especially for the categories of calm and neutral expressions because they are naturally close to each other in the RAVDESS dataset. N 1 doesn't always perform better than the existing solutions, the almost 6% improvements of N 2 (MRPN) over N 1 suggests the level of data fuzziness can make the end-to-end multi-term optimization even harder without proposed RP components. The overall improvements suggest that multi-modal patterns are more generalized from the solution of N 2 (MRPN).",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Comparing Baseline With Sota",
      "text": "Our proposed MRPN shows stat-or-art results on both datasets. It has no conflicts with any potential advantages from another novel mechanism. Experiments regarding the pretraining of the CNN extractors and the time augmentation have made the network robust to overcome the overfitting issues regarding the small amount of the training and testing data.\n\nAdditionally, replacement of LSTM of Bidirectional LSTM and Transformer as aggregator has been conducted but no noticeable differences of them as sequence aggregators can be seen. As for Transformer, the average feature is taken from the decoded outputs, the concept follows from Vision Transformer (VIT)  [38] .   [8]  67.7% 74.00% Resnet101 + BiLSTM  [39]  77.02% X custom CNN  [12]  X 69.42% Early Cross-modal + MFCC + MEL spectrogram  [10]  83.6% X CNN + Fisher vector + Metric learning  [15]  X 66.5% custom CNN+Spectrogram  [31]  79.5% (Audio) X",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Conclusion",
      "text": "This paper focuses on explaining the potential deficiencies in the existing fusion layer of the multi-modal approach to AVER tasks using late fusion or end-to-end strategy. The proposed MPRN architecture along with the multi-term loss function makes superior fused features from multi-modal sources. We observe the elimination of inferior cases of multi-modal solutions with respect to uni-modal solutions.\n\nOur results achieve an average accuracy of 91.4% on the RAVDESS dataset and 83.15% on the Crema-d dataset. MRPN solution contributes to a better average recognition rate of approximately 2%. We have observed the maximum improvement of MRPN for a subset to be around 90% from nearly 80%.\n\nThe proposed data pre-processing by time augmentation makes general overall rate improvements for both, the uni-modal and multi-modal data. It also illustrates data augmentation cannot generalize multi-modal features due to the deficiencies in the existing multi-modal solutions.\n\nMoreover, the MRPN concept shows its potential for multi-modal classifiers dealing with signal sources not only of optical and acoustical type.\n\nfound at https://www.kaggle.com/msambare/fer2013. (accessed on 7 July 2021). The initialization of vocal CNN extractor were sampled from VoxCeleb dataset  [37] . This data was made available under the Open Database License and can be found at https://www.robots.ox.ac.uk/~vgg/data/ voxceleb/vox1.html. (accessed on 7 July 2021). The evaluation of the MRPN and time augmentation is based on RAVDESS dataset  [34]  and Crema-d  [33]  under the Open Database License and can be found at https://zenodo.org/record/1188976#.YOOv3Oj7SUk(accessed on 7 July 2021) and https: //github.com/CheyneyComputerScience/CREMA-D(accessed on 7 July 2021) respectively.",
      "page_start": 17,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed multi-modal emotion recognition system using Deep Neural Network (DNN) approach.",
      "page": 1
    },
    {
      "caption": "Figure 1: illustrates, addressing the Audio-Video Emotion Recognition (AVER) problem.",
      "page": 2
    },
    {
      "caption": "Figure 2: shows. Discrete information in a single frame is ﬁrstly delivered to pattern extracting",
      "page": 2
    },
    {
      "caption": "Figure 2: Video frames of visual facial expressions selected from RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and",
      "page": 2
    },
    {
      "caption": "Figure 3: shows the expression events",
      "page": 3
    },
    {
      "caption": "Figure 3: Mel Spectrograms of vocal timbres selected from RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)",
      "page": 3
    },
    {
      "caption": "Figure 4: Visualization (t-SNE algorithm) of deep features clustering from two different setups where",
      "page": 6
    },
    {
      "caption": "Figure 4: shows us the different clustering from shufﬂed train/validation sub-datasets in",
      "page": 6
    },
    {
      "caption": "Figure 5: illustrates the source of deﬁciency in the architecture during the gradient back-",
      "page": 7
    },
    {
      "caption": "Figure 5: Distorted gradients backpropagation in some modality since the gradients from fused layer makes impact on gradients ﬂow into",
      "page": 7
    },
    {
      "caption": "Figure 6: ). They are based on the selected functionalities of neural units and",
      "page": 7
    },
    {
      "caption": "Figure 6: Evolution of network design for multi-modal fusion (presented for training mode). N0: Fusion component (FC) only. N1 ([22]):",
      "page": 9
    },
    {
      "caption": "Figure 6: , we can observe:",
      "page": 9
    },
    {
      "caption": "Figure 7: Generalization of our MRPN fusion approach to many modalities. It could be used for either",
      "page": 11
    },
    {
      "caption": "Figure 7: shows. In both cases MRPN beneﬁts from many terms",
      "page": 11
    },
    {
      "caption": "Figure 9: illustrates. Such",
      "page": 11
    },
    {
      "caption": "Figure 8: Visual comparison of augmentation procedure for cropped video frames.",
      "page": 12
    },
    {
      "caption": "Figure 9: Examples of time dependent augmentation for visual frames.",
      "page": 12
    },
    {
      "caption": "Figure 6: illustrates on the left",
      "page": 14
    },
    {
      "caption": "Figure 10: Averaged confusion matrices of tested models for RAVDESS dataset.",
      "page": 15
    },
    {
      "caption": "Figure 10: and 11 the averaged improve-",
      "page": 15
    },
    {
      "caption": "Figure 11: Averaged confusion matrices of tested models for Crema-d dataset.",
      "page": 16
    }
  ],
  "tables": [
    {
      "caption": "Table 1: , Ax,y stands for the validation ﬁles",
      "page": 14
    },
    {
      "caption": "Table 1: Comparison of single modalities models with N0 model (RAVDESS cases): VM – Visual Modal-",
      "page": 14
    },
    {
      "caption": "Table 1: of the case A9,10, but we argue such deﬁciency is",
      "page": 14
    },
    {
      "caption": "Table 2: and Table 3",
      "page": 14
    },
    {
      "caption": "Table 2: Comparison for RAVDESS of MRPN approach (network N2) with late fusion strategy (N0),",
      "page": 15
    },
    {
      "caption": "Table 3: Comparison for Crema-d of MRPN approach (network N2) with simple fusion strategy (N0),",
      "page": 16
    },
    {
      "caption": "Table 4: Comparison of our fusion models with others recent solutions. Options used: IA – image",
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection",
      "authors": [
        "P Belhumeur",
        "J Hespanha",
        "D Kriegman"
      ],
      "year": "1997",
      "venue": "IEEE Trans. Pattern Anal. Mach. Intell",
      "doi": "10.1109/34.598228"
    },
    {
      "citation_id": "2",
      "title": "Gradient-Based Learning Applied to Document Recognition",
      "authors": [
        "Y Lecun",
        "L Bottou",
        "Y Bengio",
        "P Haffner"
      ],
      "year": "1998",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "3",
      "title": "Long Short-Term Memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "4",
      "title": "Attention Is All You Need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention Is All You Need"
    },
    {
      "citation_id": "5",
      "title": "ModDrop: adaptive multi-modal gesture recognition",
      "authors": [
        "N Neverova",
        "C Wolf",
        "G Taylor",
        "F Nebout"
      ],
      "year": "2015",
      "venue": "ModDrop: adaptive multi-modal gesture recognition"
    },
    {
      "citation_id": "6",
      "title": "Temporal Multimodal Fusion for Video Emotion Classification in the Wild",
      "authors": [
        "V Vielzeuf",
        "S Pateux",
        "F Jurie"
      ],
      "year": "2017",
      "venue": "Temporal Multimodal Fusion for Video Emotion Classification in the Wild"
    },
    {
      "citation_id": "7",
      "title": "Multi-Modal Sequence Fusion via Recursive Attention for Emotion Recognition",
      "authors": [
        "R Beard",
        "R Das",
        "R Ng",
        "P Gopalakrishnan",
        "L Eerens",
        "P Swietojanski",
        "O Miksik"
      ],
      "year": "2018",
      "venue": "Proceedings of the 22nd Conference on Computational Natural Language Learning",
      "doi": "10.18653/v1/K18-1025"
    },
    {
      "citation_id": "8",
      "title": "Multimodal and Temporal Perception of Audio-visual Cues for Emotion Recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2019",
      "venue": "th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "9",
      "title": "Memory Fusion Network for Multi-view Sequential Learning",
      "authors": [
        "A Zadeh",
        "P Liang",
        "N Mazumder",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2018",
      "venue": "Memory Fusion Network for Multi-view Sequential Learning"
    },
    {
      "citation_id": "10",
      "title": "Speech Emotion Recognition With Early Visual Cross-modal Enhancement Using Spiking Neural Networks",
      "authors": [
        "E Mansouri-Benssassi",
        "J Ye"
      ],
      "year": "2019",
      "venue": "International Joint Conference on Neural Networks (IJCNN)",
      "doi": "10.1109/IJCNN.2019.8852473"
    },
    {
      "citation_id": "11",
      "title": "Learning Affective Features With a Hybrid Deep Model for Audio-Visual Emotion Recognition",
      "authors": [
        "S Zhang",
        "S Zhang",
        "T Huang",
        "W Gao",
        "Q Tian"
      ],
      "year": "2018",
      "venue": "IEEE Trans. Cir. and Sys. for Video Technol",
      "doi": "10.1109/TCSVT.2017.2719043"
    },
    {
      "citation_id": "12",
      "title": "Emotion Recognition System from Speech and Visual Information based on Convolutional Neural Networks",
      "authors": [
        "N Ristea",
        "L Duţu",
        "A Radoi"
      ],
      "year": "2019",
      "venue": "International Conference on Speech Technology and Human-Computer Dialogue (SpeD)"
    },
    {
      "citation_id": "13",
      "title": "Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention",
      "authors": [
        "E Tzinis",
        "S Wisdom",
        "T Remez",
        "J Hershey"
      ],
      "year": "2021",
      "venue": "Improving On-Screen Sound Separation for Open Domain Videos with Audio-Visual Self-attention"
    },
    {
      "citation_id": "14",
      "title": "Dual Attention Matching for Audio-Visual Event Localization",
      "authors": [
        "Y Wu",
        "L Zhu",
        "Y Yan",
        "Y Yang"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "15",
      "title": "Metric Learning-Based Multimodal Audio-Visual Emotion Recognition",
      "authors": [
        "E Ghaleb",
        "M Popa",
        "S Asteriadis"
      ],
      "year": "2020",
      "venue": "IEEE MultiMedia",
      "doi": "10.1109/MMUL.2019.2960219"
    },
    {
      "citation_id": "16",
      "title": "Audio-Visual Emotion Recognition in Video Clips",
      "authors": [
        "F Noroozi",
        "M Marjanovic",
        "A Njegus",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2713783"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition using deep learning approach from audio-visual emotional big data",
      "authors": [
        "M Hossain",
        "G Muhammad"
      ],
      "year": "2019",
      "venue": "Information Fusion",
      "doi": "10.1016/j.inffus.2018.09.008"
    },
    {
      "citation_id": "18",
      "title": "Learning Better Representations for Audio-Visual Emotion Recognition with Common Information",
      "authors": [
        "F Ma",
        "W Zhang",
        "Y Li",
        "S Huang",
        "L Zhang"
      ],
      "year": "2020",
      "venue": "Applied Sciences",
      "doi": "10.3390/app10207239"
    },
    {
      "citation_id": "19",
      "title": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition",
      "authors": [
        "L Wang",
        "Y Xiong",
        "Z Wang",
        "Y Qiao",
        "D Lin",
        "X Tang",
        "L Gool"
      ],
      "year": "2016",
      "venue": "Temporal Segment Networks: Towards Good Practices for Deep Action Recognition"
    },
    {
      "citation_id": "20",
      "title": "Two-Stream Convolutional Networks for Action Recognition in Videos",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Two-Stream Convolutional Networks for Action Recognition in Videos"
    },
    {
      "citation_id": "21",
      "title": "Action Recognition? A New Model and the Kinetics Dataset",
      "authors": [
        "J Carreira",
        "A Zisserman",
        "Vadis"
      ],
      "year": "2018",
      "venue": "Action Recognition? A New Model and the Kinetics Dataset"
    },
    {
      "citation_id": "22",
      "title": "What Makes Training Multi-Modal Classification Networks Hard?",
      "authors": [
        "W Wang",
        "D Tran",
        "M Feiszli"
      ],
      "year": "2020",
      "venue": "IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
      "doi": "10.1109/CVPR42600.2020.01271"
    },
    {
      "citation_id": "23",
      "title": "Which Tasks Should Be Learned Together in Multi-task Learning?",
      "authors": [
        "T Standley",
        "A Zamir",
        "D Chen",
        "L Guibas",
        "J Malik",
        "S Savarese"
      ],
      "year": "2020",
      "venue": "Which Tasks Should Be Learned Together in Multi-task Learning?"
    },
    {
      "citation_id": "24",
      "title": "Mach. Learn",
      "authors": [
        "R Caruana",
        "Learning"
      ],
      "year": "1997",
      "venue": "Mach. Learn",
      "doi": "10.1023/A:1007379606734"
    },
    {
      "citation_id": "25",
      "title": "Gradient Surgery for Multi-Task Learning",
      "authors": [
        "T Yu",
        "S Kumar",
        "A Gupta",
        "S Levine",
        "K Hausman",
        "C Finn"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "26",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D Lee",
        "Y Zhou",
        "C Ramaiah",
        "F Feng",
        "R Li",
        "X Wang",
        "D Athanasakis",
        "J Shawe-Taylor",
        "M Milakov",
        "J Park",
        "R Ionescu",
        "M Popescu",
        "C Grozea",
        "J Bergstra",
        "J Xie",
        "L Romaszko",
        "B Xu",
        "Z Chuang",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "Special Issue on \"Deep Learning of Representations",
      "doi": "10.1016/j.neunet.2014.09.005"
    },
    {
      "citation_id": "27",
      "title": "Learning to Augment Expressions for Few-shot Fine-grained Facial Expression Recognition",
      "authors": [
        "W Wang",
        "Y Fu",
        "Q Sun",
        "T Chen",
        "C Cao",
        "Z Zheng",
        "G Xu",
        "H Qiu",
        "Y Jiang",
        "X Xue"
      ],
      "year": "2020",
      "venue": "Learning to Augment Expressions for Few-shot Fine-grained Facial Expression Recognition"
    },
    {
      "citation_id": "28",
      "title": "Deep Learning for Emotion Recognition on Small Datasets Using Transfer Learning",
      "authors": [
        "H Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction",
      "doi": "10.1145/2818346.2830593"
    },
    {
      "citation_id": "29",
      "title": "EmotiW 2018: Audio-Video, Student Engagement and Group-Level Affect Prediction",
      "authors": [
        "A Dhall",
        "A Kaur",
        "R Goecke",
        "T Gedeon"
      ],
      "year": "2018",
      "venue": "EmotiW 2018: Audio-Video, Student Engagement and Group-Level Affect Prediction"
    },
    {
      "citation_id": "30",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database. Language Resources and Evaluation",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "IEMOCAP: Interactive emotional dyadic motion capture database. Language Resources and Evaluation",
      "doi": "10.1007/s10579-008-9076-6"
    },
    {
      "citation_id": "31",
      "title": "A CNN-Assisted Enhanced Audio Signal Processing for Speech Emotion Recognition",
      "authors": [
        "Mustaqeem",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "Sensors",
      "doi": "10.3390/s20010183"
    },
    {
      "citation_id": "32",
      "title": "Variational Autoencoders for Learning Latent Representations of Speech Emotion",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2017",
      "venue": "Variational Autoencoders for Learning Latent Representations of Speech Emotion"
    },
    {
      "citation_id": "33",
      "title": "CREMA-D: Crowd-Sourced Emotional Multimodal Actors Dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLOS ONE",
      "doi": "10.1371/journal.pone.0196391"
    },
    {
      "citation_id": "35",
      "title": "Machine Learning Toolkit",
      "authors": [
        "D King",
        "Dlib-Ml"
      ],
      "year": "2009",
      "venue": "Machine Learning Toolkit"
    },
    {
      "citation_id": "36",
      "title": "Deep Residual Learning for Image Recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Deep Residual Learning for Image Recognition"
    },
    {
      "citation_id": "37",
      "title": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
      "authors": [
        "A Nagrani",
        "J Chung",
        "A Zisserman"
      ],
      "year": "2017",
      "venue": "VoxCeleb: A Large-Scale Speaker Identification Dataset",
      "doi": "10.21437/interspeech.2017-950"
    },
    {
      "citation_id": "38",
      "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2020",
      "venue": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
    },
    {
      "citation_id": "39",
      "title": "Clustering-Based Speech Emotion Recognition by Incorporating Learned Features and Deep BiLSTM",
      "authors": [
        "Mustaqeem",
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2020.2990405"
    }
  ]
}