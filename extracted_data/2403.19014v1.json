{
  "paper_id": "2403.19014v1",
  "title": "Thelxinoë: Recognizing Human Emotions Using Pupillometry And Machine Learning",
  "published": "2024-03-27T21:14:17Z",
  "authors": [
    "Darlene Barker",
    "Haim Levkowitz"
  ],
  "keywords": [
    "Emotion recognition",
    "emotions",
    "pupillometry",
    "machine learning",
    "ensemble learning",
    "gradient boosting"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In this study, we present a method for emotion recognition in Virtual Reality (VR) using pupillometry. We analyze pupil diameter responses to both visual and auditory stimuli via a VR headset and focus on extracting key features in the time-domain, frequency-domain, and time-frequency domain from VRgenerated data. Our approach utilizes feature selection to identify the most impactful features using Maximum Relevance Minimum Redundancy (mRMR). By applying a Gradient Boosting model, an ensemble learning technique using stacked decision trees, we achieve an accuracy of 98.8% with feature engineering, compared to 84.9% without it. This research contributes significantly to the Thelxinoë framework, aiming to enhance VR experiences by integrating multiple sensor data for realistic and emotionally resonant touch interactions. Our findings open new avenues for developing more immersive and interactive VR environments, paving the way for future advancements in virtual touch technology.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "In a poetic sense, the eyes have long been regarded as the \"window into the soul\" offering a glimpse into the depths of human emotions and experiences  [1] . In the realm of modern technology, this poetic vision transforms into a scientific reality, particularly in VR. The \"pupils\" serve as gateways not just \"to the brain\" but to the autonomic nervous system which subtly dilates and contracts in response to our emotions  [1] . This study ventures beyond the realm of the soul, focusing on how the emotional state of a person influences the diameter of their pupils, and how this physiological response can be harnessed within the immersive world of VR  [2] .\n\nIn VR, where the digital and physical worlds converge, recognizing emotions can significantly enhance the overall experience, bringing it closer to the full spectrum of human interaction. This is where pupillometry, the measurement of pupil diameter, becomes a pivotal tool. Pupillary responses, which occur spontaneously and are challenging to control voluntarily, provide a continuous measure of emotional states, independent of a person's conscious awareness or control  [1, 2, 3] . Leveraging this involuntary response, our research delves into the realm of VR, aiming to mirror real-life experiences by interpreting these subtle yet telling signs of our internal emotional landscape.\n\nThe emotions that we explored are the ones that \"are related to the most basic needs of the body\" like happiness, sadness, anger, and fear; of which the other emotions are composite  [4] . Thelxinoe was an idea that started with the desire to accomplish the creation of the ability to touch another person across great distance whether via long-distance interactions or within a virtual space, to be able to feel the touch on the other side and feel feedback to that touch.\n\nThe idea is that you can manipulate objects in VR but not feel what you touch, or feel being touched. Our research is aimed at creating a framework to support this happening with the capturing of emotions of the parties involved using sensors, including cameras, to be able to capture the emotions from the brain, face, speech, and body movement  [5] .\n\nThe objective of our research was to see if the use of the pupil diameter was enough on its own to recognize human emotions using our emotion model of the four base emotions-happy, sad, anger and fear. The novelty of this approach is that we focussed solely on the pupil diameter and creating a working machine learning (ML) model with an accuracy of 98.8% using distinct emotions, and that these measures are \"unconscious and difficult to control voluntarily\"  [6] .\n\nIn this study, we explored emotion recognition with the use of pupil diameter. The initial data was collected from watching video content that matched the emotion model was and our approach using machine learning we classified the emotions with great accuracy. In our case, the stimuli were from video clips and with a non-invasive manner we used pupillometry to recognize the emotions  [2] . We can control our emotions in the way we communicate verbally but we are not able to hide what we truly are feeling in our eyes. This is where pupillometry comes in, it provides \"involuntary response\" that we cannot control  [7] .\n\nAnger and fear are ranging from 2.5 to 5mm while happiness is ranging from 2 to 5mm, and all emotions are all ranging between 2 and 5mm. For this reason, we explored additional features during the extraction process, and selection process, in preparation for classification.\n\n\"Pupillometry is the study of such changes in pupil diameter as a function of different neurological activities in a human brain\"  [7] . We studied pupillometry in recognizing emotions because of its dynamic nature compared to other physiological methods  [8] .\n\nAnother reason to study pupillometry is that our \"learning, attention, perception, and memory is affected by our emotions\" and by studying those emotions and their connections, we can move closer to recreating touch in VR  [6] . This research is focused on the four basic emotions-fear, sadness, anger, and happiness  [4] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem",
      "text": "We can see, hear, and manipulate objects within a VR experience. What we cannot do is feel when we run into a game object, or another player. Yes, there are currently ways to produce feedback with vibrations in the controllers. We require more than vibration; we need touch.\n\nBecause we respond to what we see more so than what we hear, Pupillometry is a necessary part of any emotion recognition frameworks such as Thelxinoë  [3, 9] .\n\nRecognizing emotions in VR can bring us closer to a full body experience, and studying the senses that are already represented, we get closer to adding touch as part of the experience.\n\n\"Collecting emotions data from multiple sensors for Thelxinoë requires some aggregation of data to recognize the mental state of the user to process touch and generate the associated sensation quickly\"  [9] . At the same time, each sensor or sensor area needs to be studied separately and optimized independently of all the other sensors. In this case, we are studying the eyes as a sensor area independent to other sensor areas such as the brain, face, speech, and body movement. This research is geared towards creating the framework necessary to be able to connect haptic devices to a human being and create an experience where touch occurs, and feeling is felt. We are aiming at putting the entire human body into the equation to be able to touch and be touched in VR.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "This method of recognizing human emotions from our eyes, from the subtle yet informative changes in pupil diameter as a window into the human emotional state is crucial to bring another sense into VR. A variety of machine learning models have been employed in recent studies to decode these physiological signals, each contributing uniquely to our understanding of the complex interplay between emotions and physiological responses.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "96.5%",
      "text": "Eye-Tracking Analysis for Emotion Recognition  [13]  Support Vector Machine (SVM) 80% Table  1 : Some of the related works using Pupillometry to recognize human emotions.\n\nOne of the studies in Table  1  introduced a \"technique to detect and differentiate between positive and negative emotions\" with a 96.5% accuracy  [6] . We show in figure  1  the taxonomy of emotions showing them falling into either neutral, or non-neutral which covers all other emotions that are either positive or negative. This was the closest to what we studied but still different in that we used a distinct emotion model.\n\nA study of cognitive Pupillometry methods that provides \"a comprehensive,\" firsthand \"guide to methods in cognitive Pupillometry, with a focus on trial-based experiments in which the measure of interest is the task evoked pupil response to stimulus\" as well as expressed the need for a system for processing Pupillometry data  [15] .\n\nAnother study argues \"that Pupillometry is a powerful and non-invasive tool to investigate emotional processing in clinical populations\"  [8] . While this is not physically invasive, it is emotionally invasive. It is assumed that the users of the framework that this study is a part of consent to the exchange of information about each other.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Motivation",
      "text": "This paper is part of our research for the creation of a framework called Thelxinoë  [89] . This technology will bring the whole body into VR to achieve a close to whole-body experience. The framework will allow those in a VR world to touch another person, feel this sensation, and have the other person feel it as well. In this work, we focused on the pupils of the eyes and what emotions we can recognize using the pupil diameters during the experience of the basic emotions.\n\nReplacing in-person interactions is not the goal here but to simulate them. With the added information that one would not have in an in-person interaction, this information gotten from sensors we can get a better measure of what the person is feeling which can narrow the distance or allow for greater immersion in a virtual space. Knowing this information would analogously characterize touch as in-person touch, without seeing either as being a real-life interaction vs a manufactured experience  [12]",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Methods And Materials",
      "text": "This research includes one of the key areas of recognizing human emotions, the eyes, as a source of the data of study. Here we look at how our body responds to external stimuli to trigger the changes in pupil size. The autonomic nervous system controls the changes in your pupil whether it be light in your environment or an emotional state, dilating when happy or excited and constricting when sad. It is hard to hide our emotions when it expressed in the changes in the pupil.\n\n\"Pupillometry is a powerful and\" physically non-invasive \"tool to investigate emotional processing\" making this \"a promising tool in\" research  [1, 8] . This part of the Thelxinoë framework covers emotion recognition in our \"pupil diameter\" to better understand the emotional state of the user  [1, 3, 8, 13] . While pupil constriction and dilation are assumed to be a reaction to negative and positive stimuli, it has been found that \"pupil dilation is far more dynamic\"  [8] .\n\nDuring this research, we used an emotion model that contains the four basic emotions: anger, sadness, happiness, and fear. The data that is collected during the collection phase will go through the pre-processing phase, followed by the feature extraction, then the feature selection phase, classification, and finally the statistical analysis phase.\n\nOne thing that is key while studying pupillometry is that pupil dilation is a measure of emotion, cognitive load, and memory recognition but also the objective amount of light that the eye is exposed to  [16] . The light was controlled by maintaining a constant light source in the room where the datasets were collected-only light changes in the video content were used as stimuli. Breaks were taken between each emotional state that was triggered or type of stimuli, to reduce any potential cognitive overload.\n\nMemory recognition was not addressed since the assumption in this work is that a happy video would evoke happy emotions and somehow remind the user of happy memories, the same with the other basic emotions.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Data Collection",
      "text": "While \"eye tracking is a powerful tool that can be applied to a wide variety of research questions\" in our approach, we only used the diameter of the pupil resulting from dilation and contraction in response to stimuli, for the recognition of the emotional state of the user  [17] . \"The most primary metric used in Pupillometry is the pupil diameter\" validated our choice  [18] . We collected data using the HP Reverb G2 Omnicept Edition headset, as seen in Fig.  3 , within a VR environment with video content serving as stimuli that evoke emotions, happiness, sad, anger, and fear.\n\nAs seen in Fig.  2  our approach involves data collection, pre-processing, feature extraction, feature selection, classification, and statistical analysis of the results, to recognize the emotions from the data we collected of pupil diameter data. We used the pupil diameter of both eyes and collected for 10-minute periods for each emotion, happiness, sadness, anger, and fear. This data went through pre-processing to remove noise, feature extraction to gain more information about the dataset, feature selection to choose the optimal features for the dataset, classifying the samples into one of the emotions, and finally the statistical analysis to show how well the model performed.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sensors",
      "text": "The device used in the research was the HP Reverb G2 Omnicept Edition VR Headset shown in fig.  3 . It contains an Eye Tracking sensor. We used Unity3D 2021.  3.19f1  Personal for the environment that we created for data collection. The other software that we used in this research were MATLAB R2022b, Python 3.11.2, and HD Omnicept SDK.\n\nFor data collection, we created a space where the user could not move since all they needed to do was start an executable of the scene created in Unity 3D and put on the headset. The world included a large screen where the video clips played. We attempted to limit movement and anything that would limit results, such as changes in lighting in the room where the data was collected. We also captured results from both eyes. Data was captured from watching video clips from shows that were expected to create the desired emotions in the user. We focused solely on pupil diameter and ignore gaze and cognitive overload because the tests intended to see if it was possible to classify emotions using the pupil diameter  [6] . We ignored where the person was looking on the screen as well as saccades, which is the rapid eye movement between fixation points on the screen. Each session was stored in a file with the emotion trigger in the name.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Preprocessing",
      "text": "We created the datasets we used from the viewing in Unity3D of video content which were the source triggers for each emotion. The design matrix contained the timestamp of the data collection, left-eye pupil diameter, and right-eye pupil diameter as shown in figure  4 . As part of the preprocessing step, the dataset of all four emotions was compiled using the name of the file to create the label of each record held in the file, also as shown in Figure  4 . The pupil diameter range for the sensor is 2-8mm, any value small is reported in the data as -1 which is representative of blinks. We removed the noise in the form of blinks. We also removed samples where one eye was closed, which usually either led up to or followed a blink event  [19] . The other pre-processing that we carried us was replacing the date and time with a timestamp.\n\nIn  [3] , Nakakoga et al. used principal component analysis (PCA) to remove \"artifacts\" in the pupillary data. In our data gathered from the Omnicept SDK, blinks are recorded as -1 and we used that behavior to remove artifacts from the data. All data that contained a blink representative of the -1 on one eye or both eyes, were removed from the raw dataset before feature engineering.\n\nThey start with a clean dataset of the dilation measurements of the left and right eyes while viewing emotional stimuli within a virtual environment. The individual emotion data samples were labelled and combined to form the dataset of all four of the emotions. This raw dataset consists of a timestamp, two features (left eye, right eye), and a label, like that shown in Fig.  4  with the happy label.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Feature Extraction",
      "text": "The term \"VR-generated\" in the context of our abstract refers to data that is produced or collected within a Virtual Reality (VR) environment. It implies that the data, particularly the pupillometry data is gathered while participants are engaged in a VR setting, experiencing visual and auditory stimuli through a VR headset.\n\nIn VR, stimuli can be highly controlled and varied in ways that might not be possible in a realworld setting. This allows for the collection of detailed and specific data under a range of controlled conditions, which can be particularly valuable for research in fields like emotion recognition, behavioral studies, and human-computer interaction.\n\nIn this step, we used MATLAB to extract additional features from the two features: leftEye, and rightEye. This process included statistical features (mean, standard deviation, covariance, and kurtosis) as well as the power spectral density (PSD)  [19] . Kurtosis was used to compute the sample kurtosis for the input blocks providing an estimate of the shape of the data distribution's tail. This is useful for finding outliers, or extreme values in the dataset, and these extreme pupil dilations or constrictions are crucial in the different emotional states.\n\nThe extraction process was done on 120 block samples. The dataset included 193 additional columns 18 columns were removed because they were zeros, dropping down to 175 columns.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Feature Selection",
      "text": "The feature selection process used a combination of mRMR and GridSearchCV to go from 175 to 50 optimal features that we used for our classification model.\n\nThe initial feature set began with 175 features that were extracted from the data. This large set of features included both highly informative and less relevant or redundant features. We applied the mRMR technique to find a subset of features that would have maximum relevance as well as those features that had minimum redundancy. The goal here was to minimize the potential of overfitting.\n\nWe reduce the number of extracted features from 175 to 50 features and with further reduction use GridSearchCV to find the top optimal features  [20, 21, 22, 23, 24] . The decision to reduce the number of features to fifty was based on an analysis of feature importance, as illustrated in Figure  5 . Feature importance scores help identify which features contribute most significantly to the model's predictive ability. Here we could observe that the importance of features drops to zero around the fifties indicating that beyond this point, additional features do not meaningfully contribute to model performance.\n\nFurther experimentation, such as testing with the top thirty features, was conducted to validate the choice of fifty features. It was found that reducing to thirty features led to a drop in accuracy, indicating that while some reduction is beneficial, overly aggressive feature pruning can remove valuable information about the data. The overall goal of feature selection is to find a balance between model complexity and performance. Too many features can lead to overfitting and increased computational cost, while too few features might fail to capture important nuances in the data.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Classification",
      "text": "Before the classifier used the dataset, the samples were shuffled. For each dataset we used they were \"divided into a training set (70%) and a test set (30%)\"  [11] . Like other papers we read, we used SVM but chose a GB based off the stacking of Decision Trees with greater accuracy. We performed tuning we used \"grid search\" to achieve the \"best hyperparameters for\" the GB model, as shown in fig.  6 . We used a DecisionTreeClassifier as the base estimator of the GB model along with subsampling set to 0.8 for better generalization, meaning the model being able to adapt to previously unseen data  [24] . from sklearn.metrics import mean_squared_error gbrt = GradientBoostingClassifier(max_depth = 5, learning_rate=0.\n\n.fit(X_train,y_train) Another model that we used was the Long Short-Term Memory (LSTM) with an accuracy of 61-70% without further finetuning. This was not pursued further because the Pupillometry data will be combined with the EEG data in future works and processed together. This decision was based upon the accuracy achieved by another work that used combined signals and achieved similar accuracy using a similar model as well as the need to address a multimodal approach to process the data from multiple sensors in real-time in future research of Thelxinoë  [9, 25] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results",
      "text": "Our GB model generated 85% accuracy without feature engineering, for both training and test datasets. For datasets that underwent feature engineering, the accuracy was 95% for both training and test datasets. In addition to accuracy, we achieved the following: specificity (99%), sensitivity (98%), precision (98%), f-score (98%), and Matthew Correlation Coefficient (97%), shown in Figure  7  below: In figure  8 , shows the confusion matrix of our GB model which achieved a 98.8% accuracy.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Discussion",
      "text": "In our study, the initial feature set f or each sample comprised timestamps and pupil diameters for both the left eye (LE) and right eye (RE). Interestingly, when identifying the most crucial features, we observed a predominance of data from the LE. Out of the top features, most were derived from the LE, with only seven features representing the RE, as detailed in Fig.  9  for the top thirty features for clarity. This finding echoes the work of Rafique et al. who focused on identifying a range of emotions based on pupil diameter, using data from the left eye  [7] . Our approach expanded on this by using both eyes, yet our results similarly highlighted a greater significance in the LE data. Notably, our feature extraction process revealed that the majority of the top fifty-one features were LE-centric, with figure  6  illustrating the distribution.\n\nWe exclusively relied on pupil diameter measurements, excluding confidence levels from our data collection. This approach yielded successful results, with accuracies of 84.9% without feature engineering and 98.8% with.\n\nThe environmental setup for our data collection was intentionally controlled and minimalistic, with fixed lighting and no additional interactions in the VR space, apart from a static background. This design choice was to ensure data consistency and reduce variables that potentially influence pupil responses.\n\nOur Gradient Boosting (GD) model demonstrated impressive accuracies, consistently scoring in the mid-to-high nineties. However, the Long Short-Term Memory (LSTM) offline model showed lower performance, with accuracy in the mid-to-high sixties. In future research phases, we plan to explore the integration of LSTM in processing aggregated data. Nevertheless, the current GD model stands as a robust tool for recognizing emotions through pupillometry, offering promising results and potential for further refinement.\n\nThe GD model was able to show accuracies in the mid-to-high nineties but with offline model LSTM currently coming in in the mid-to-high sixties. We were able to achieve high eighties to high nineties with the same model on the EEG data. We decided that the use of LSTM for the aggregated data to be addressed when the aggregation phase of the research happens but for now, we have a model for recognizing pupillometry data using GD with satisfactory results.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Future Work",
      "text": "One obvious step would be to aggregate the EEG and pupillometry data in recognizing emotions  [17] . In doing this, the datasets are collected on different sample frequencies, 256Hz, and 120Hz, for EEG, and Pupillometry, respectively. The sampling change will need to be addressed. For the offline model, the LSTM model will need to be further finetuned  [26] .\n\nExpand the emotion model to include more than the basic emotions after addressing the multimodal emotion recognition.\n\nAddressing the study of dominance in eyes and if this would affect the emotion recognition process. Most of the optimal features were from the left eye extracted features although the eye gets the number one spot while the feature spread seems to change. This brings us to question whether studying both eyes is of significance to the recognition of emotions, in one of the studies we looked at, the pupil diameter was the same for both eyes but with our research that was not the case, hence the use of both  [7] .\n\nAnother would be potentially \"examining the full waveform of pupil activity, from resting baseline, to constriction, to\" re-dilation to see if it affects accuracy we have already achieved  [27] .\n\nYet another approach would use eye movement to further understand what the user is thinking. The user looking away along, combined with an emotional state could explain their behavior within the virtual world. One of the studies found that \"eye movement such as fixation and saccaded depend largely on the dynamics of the movie scene\"  [9] . So, for our research after the physical environment and other integrations this could be useful information, which will be part of body language, and a more controllable emotional response.\n\nFinally, it is important to develop safeguards to protect participants using Thelxinoë. This would mean conducting research into what measures to put in place and how to put them. In cases where the user is not experiencing the expected emotions one will need to detect if there is an \"affective deficit\"  [8] . One study suggested that \"psychopathy is associated with a problem in processing negative emotions such as sadness and fear\"  [8] .",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Conclusions",
      "text": "In this research study, we focused on recognizing emotions using pupillometry data within a VR environment. The primary goal was to determine whether using just the pupil diameters as features in response to the visual and auditory stimuli, would we be able to accurately recognize human emotions.\n\nThe research included feature extraction techniques, including time-domain, frequency-domain, and time-frequency domain composite features, extracted from datasets captured in VR. These features were processed to reduce the dimensionality and selecting the top features through feature engineering. An ensemble learning model that stacked multiple decision trees and used gradient boosting, was used to achieve emotion recognition.\n\nThe study achieved an accuracy of 98.8% after feature engineering, and model tuning. Even without feature engineering, the model achieved 84.9% accuracy. This outcome demonstrated the effectiveness of using pupillometry data for emotion recognition.\n\nThe contribution of this work is to add recognition of emotions from pupil diameters to the Thelxinoë framework, aimed at enabling touch sensations in VR interactions by collecting data from various sensors to generate meaningful emotions. This work represents a foundational step towards enhancing immersion and emotional interaction within virtual spaces.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The taxonomy of emotions from [14].",
      "page": 4
    },
    {
      "caption": "Figure 2: our approach involves data collection, pre-processing, feature",
      "page": 5
    },
    {
      "caption": "Figure 2: The flow of the emotion recognition pipeline used.",
      "page": 5
    },
    {
      "caption": "Figure 3: It contains an Eye Tracking sensor. We used Unity3D 2021.3.19f1 Personal for the",
      "page": 6
    },
    {
      "caption": "Figure 3: HP Reverb G2 Omnicept Edition",
      "page": 6
    },
    {
      "caption": "Figure 4: As part of",
      "page": 6
    },
    {
      "caption": "Figure 4: Sample of eye tracking data showing pupil sizes for happy.",
      "page": 7
    },
    {
      "caption": "Figure 4: with the happy label.",
      "page": 7
    },
    {
      "caption": "Figure 5: Feature importance scores help identify which features contribute most significantly to the",
      "page": 8
    },
    {
      "caption": "Figure 5: The feature importance graph of fifty-eight features while experimenting with GridSeasrchCV.",
      "page": 8
    },
    {
      "caption": "Figure 6: We used a DecisionTreeClassifier as the base estimator of the GB model along",
      "page": 9
    },
    {
      "caption": "Figure 6: GB model including the Hyperparameters.",
      "page": 9
    },
    {
      "caption": "Figure 7: Additional Metrics for our GB Model.",
      "page": 10
    },
    {
      "caption": "Figure 8: , shows the confusion matrix of our GB model which achieved a 98.8% accuracy.",
      "page": 10
    },
    {
      "caption": "Figure 8: Gradient Boosting Machine Classifier Confusion Matrix",
      "page": 10
    },
    {
      "caption": "Figure 9: for the top thirty features for clarity.",
      "page": 10
    },
    {
      "caption": "Figure 6: illustrating the",
      "page": 10
    },
    {
      "caption": "Figure 9: The 30 top features while experimenting with GridSearchCV.",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table 1: encapsulates a selection of seminal works in this domain, illustrating the diversity in",
      "data": [
        {
          "S\ntudy": "Four-class emotion classification in virtual \nreality using pupillometry [10]",
          "Machine Learning Model Used": "Support Vector Machine (SVM) \nK-Nearest Neighbor (KNN)",
          "Accuracy": "57.05% \n70.83%"
        },
        {
          "S\ntudy": "Understanding the role of emotion in decision \nmaking process: using machine learning to \nanalyze physiological responses to visual, \nauditory, and combined stimulation [11]",
          "Machine Learning Model Used": "K-nearest Neighbors (KNN) \nDecision trees (DT) \nLogistic Regression (LR) \nSupport Vector Machine (SVM) \nLinear Discriminant analysis (LDA)  \nRandom Forest (RF) \nAdaboost (ADB)",
          "Accuracy": "52%  \n44%  \n51%"
        },
        {
          "S\ntudy": "Machine Learning to Differentiate Between \nPositive and Negative Emotions Using Pupil \nDiameter [6]",
          "Machine Learning Model Used": "K-nearest Neighbors (KNN)",
          "Accuracy": "96.5%"
        },
        {
          "S\ntudy": "Eye-Tracking Analysis for Emotion \nRecognition [13]",
          "Machine Learning Model Used": "Support Vector Machine (SVM)",
          "Accuracy": "80%"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Eye pupila window into central autonomic regulation via emotional/cognitive processing",
      "authors": [
        "N Ferencova",
        "Z Visnovcova",
        "L Olexova",
        "I Tonhajzerova"
      ],
      "year": "2021",
      "venue": "Physiological Research"
    },
    {
      "citation_id": "2",
      "title": "Perspectives on Psychological Science",
      "authors": [
        "S Laeng",
        "\" Bruno; Sirois",
        "Pupillometry"
      ],
      "year": "2012",
      "venue": "Perspectives on Psychological Science"
    },
    {
      "citation_id": "3",
      "title": "Asymmetrical characteristics of emotional responses to pictures and sounds: Evidence from pupillometry",
      "authors": [
        "S Nakakoga",
        "H Higashi",
        "J Muramatsu",
        "S Nakauchi",
        "T Minami"
      ],
      "year": "2020",
      "venue": "PloS one"
    },
    {
      "citation_id": "4",
      "title": "A model for basic emotions using of behavior in Drosophila",
      "authors": [
        "S Gu",
        "F Wang",
        "N Patel",
        "J Bourgeois",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "5",
      "title": "Thelxinoë for Emotion Recognition and Touch Processing",
      "authors": [
        "D Barker",
        "H Levkowitz"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Advanced Systems and Emergent Technologies (IC_ASET)",
      "doi": "10.1109/IC_ASET58101.2023.10150503"
    },
    {
      "citation_id": "6",
      "title": "Machine Learning to Differentiate Between Positive and Negative Emotions Using Pupil Diameter",
      "authors": [
        "A Babiker",
        "I Faye",
        "K Prehn",
        "A Malik"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "7",
      "title": "Towards Estimation of Emotions from Eye Pupillometry with Low-Cost Devices",
      "authors": [
        "S Rafique",
        "N Kanwal",
        "I Karamat",
        "M Asghar",
        "M Fleury"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "8",
      "title": "Emotional modulation of the pupil response in psychopathy",
      "authors": [
        "D Burley",
        "N Gray",
        "R Snowden"
      ],
      "year": "2019",
      "venue": "Personality Disorders: Theory, Research, and Treatment"
    },
    {
      "citation_id": "9",
      "title": "Creation of Thelxinoë: Emotions and Touch in Virtual Reality",
      "authors": [
        "D Barker"
      ],
      "year": "2023",
      "venue": "Creation of Thelxinoë: Emotions and Touch in Virtual Reality"
    },
    {
      "citation_id": "10",
      "title": "Four-class emotion classification in virtual reality using pupillometry",
      "authors": [
        "Lim Zheng",
        "James Jia",
        "Jason Mountstephens",
        "Teo"
      ],
      "year": "2020",
      "venue": "Journal of Big Data"
    },
    {
      "citation_id": "11",
      "title": "Understanding the role of emotion in decision making process: using machine learning to analyze physiological responses to visual, auditory, and combined stimulation",
      "authors": [
        "E Polo",
        "A Farabbi",
        "M Mollura",
        "L Mainardi",
        "R Barbieri"
      ],
      "year": "2024",
      "venue": "Frontiers in human neuroscience",
      "doi": "10.3389/fnhum.2023.1286621"
    },
    {
      "citation_id": "12",
      "title": "Touch with the Future: The sense of touch from cognitive neuroscience to virtual reality",
      "authors": [
        "A Gallace",
        "C Spence"
      ],
      "year": "2014",
      "venue": "Touch with the Future: The sense of touch from cognitive neuroscience to virtual reality"
    },
    {
      "citation_id": "13",
      "title": "Eye-tracking analysis for emotion recognition",
      "authors": [
        "P Tarnowski",
        "M Kolodziej",
        "A Majkowski",
        "R Rak"
      ],
      "year": "2020",
      "venue": "Computational intelligence and neuroscience"
    },
    {
      "citation_id": "14",
      "title": "A systematic survey on multimodal emotion recognition using learning algorithms",
      "authors": [
        "N Ahmed",
        "Z Aghbari",
        "S Girija"
      ],
      "year": "2023",
      "venue": "Intelligent Systems with Applications"
    },
    {
      "citation_id": "15",
      "title": "Methods in cognitive pupillometry: Design, pre-processing, and statistical analysis",
      "authors": [
        "A Mathˆot",
        "; Sebastiaan",
        "Vilotijevi´c"
      ],
      "year": "2022",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "16",
      "title": "Towards estimation of emotions from eye pupillometry with low-cost devices",
      "authors": [
        "S Rafique",
        "N Kanwal",
        "I Karamat",
        "M Asghar",
        "M Fleury"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "17",
      "title": "Best practices in eye tracking research",
      "authors": [
        "B Carter",
        "S Luke"
      ],
      "year": "2020",
      "venue": "International Journal of Psychophysiology"
    },
    {
      "citation_id": "18",
      "title": "A review study on eye-tracking technology usage in immersive virtual reality learning environments",
      "authors": [
        "D Shadiev",
        "; Rustam",
        "Li"
      ],
      "year": "2023",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "19",
      "title": "Machine learning-based analysis of operator pupillary response to assess cognitive workload in clinical ultrasound imaging",
      "authors": [
        "H Sharma",
        "L Drukker",
        "A Papageorghiou",
        "J Noble"
      ],
      "year": "2021",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition using multimodal data and machine learning techniques: A tutorial and review",
      "authors": [
        "J Zhang",
        "Z Yin",
        "P Chen",
        "S Nichele"
      ],
      "venue": "Information Fusion"
    },
    {
      "citation_id": "21",
      "title": "EEG based emotion recognition: A tutorial and review",
      "authors": [
        "X Li",
        "Y Zhang",
        "P Tiwari",
        "D Song",
        "B Hu",
        "M Yang",
        "Z Zhao",
        "N Kumar",
        "P Marttinen"
      ],
      "year": "2022",
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "22",
      "title": "Emotion recognition from EEG signal focusing on deep learning and shallow learning techniques",
      "authors": [
        "M Islam",
        "M Moni",
        "M Islam",
        "M Rashed-Al-Mahfuz",
        "M Islam",
        "M Hasan",
        "M Hossain",
        "M Ahmad",
        "S Uddin",
        "A Azad"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "23",
      "title": "Human emotion recognition from EEG-based braincomputer interface using machine learning: a comprehensive review",
      "authors": [
        "E Houssein",
        "A Hammad",
        "A Ali"
      ],
      "year": "2022",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "24",
      "title": "Gradient boosting machines, a tutorial",
      "authors": [
        "A Natekin",
        "A Knoll"
      ],
      "year": "2013",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "25",
      "title": "BiLSTM-based Quality of Experience Prediction using Physiological Signals",
      "authors": [
        "S Vijayakumar",
        "R Flynn",
        "P Corcoran",
        "N Murray"
      ],
      "year": "2022",
      "venue": "2022 14th International Conference on Quality of Multimedia Experience (QoMEX)"
    },
    {
      "citation_id": "26",
      "title": "Deep Learning based Emotion Classification with Temporal Pupillometry Sequences",
      "authors": [
        "S Rafique",
        "N Kanwal",
        "M Ansari",
        "M Asghar",
        "Z Akhtar"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Electrical, Computer and Energy Technologies (ICECET)"
    },
    {
      "citation_id": "27",
      "title": "Enhanced emotional response to both negative and positive images in post-traumatic stress disorder: evidence from pupillometry",
      "authors": [
        "A Mckinnon",
        "N Gray",
        "R Snowden"
      ],
      "year": "2020",
      "venue": "Biological Psychology"
    }
  ]
}