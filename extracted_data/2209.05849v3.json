{
  "paper_id": "2209.05849v3",
  "title": "Weight-Based Channel-Model Matrix Framework Provides A Reasonable Solution For Eeg-Based Cross-Dataset Emotion Recognition",
  "published": "2022-09-13T09:51:33Z",
  "authors": [
    "Huayu Chen",
    "Huanhuan He",
    "Jing Zhu",
    "Shuting Sun",
    "Jianxiu Li",
    "Xuexiao Shao",
    "Junxiang Li",
    "Xiaowei Li",
    "Bin Hu"
  ],
  "keywords": [
    "Electroencephalogram(EEG)",
    "affective braincomputer interface(aBCI)",
    "emotion recognition",
    "cross-dataset {chenhy2021",
    "220220942890",
    "zhujing",
    "sunsht17",
    "lijx18",
    "shaoxx19",
    "lijunxiang19",
    "lixwei",
    "bh}@lzu.edu.cn"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Cross-dataset emotion recognition as an extremely challenging task in the field of EEG-based affective computing is influenced by many factors, which makes the universal models yield unsatisfactory results. Facing the situation that lacks EEG information decoding research, we first analyzed the impact of different EEG information(individual, session, emotion and trial) for emotion recognition by sample space visualization, sample aggregation phenomena quantification, and energy pattern analysis on five public datasets. Based on these phenomena and patterns, we provided the processing methods and interpretable work of various EEG differences. Through the analysis of emotional feature distribution patterns, the Individual Emotional Feature Distribution Difference(IEFDD) was found, which was also considered as the main factor of the stability for emotion recognition. After analyzing the limitations of traditional modeling approach suffering from IEFDD, the Weight-based Channel-model Matrix Framework(WCMF) was proposed. To reasonably characterize emotional feature distribution patterns, four weight extraction methods were designed, and the optimal was the correction T-test(CT) weight extraction method. Finally, the performance of WCMF was validated on cross-dataset tasks in two kinds of experiments that simulated different practical scenarios, and the results showed that WCMF had more stable and better emotion recognition ability.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The affective brain-computer interface  [1]  is an important component of brain-computer interface, which has suffered from the severe impact by EEG individual information, causing the universal model is unable to give stable and effective performance on cross-session  [2] -  [4] , cross-subject  [5] -  [7] , cross-dataset  [8] -  [10] , and other emotion classification tasks. In previous works, many studies demonstrated that machine learning algorithms could learn the emotional difference information delivered by EEG signals for single subject  [11] ,  [12] . However, due to the existence of EEG individual differences, it is difficult to find common emotion knowledge among different subjects to build universal emotion recognition models. Thus, the universal model usually produces extremely unstable results on unknown subjects and unknown sessions, and it is more challenging to implement stable emotion recognition between datasets. Importantly, only if affective brain-computer interfaces that could provide stable and effective performance on cross-session, cross-subject and cross-dataset tasks, it could be expected to deal with complex and variable real-life emotion recognition scenarios.\n\nIn the cross-session emotion recognition classification task, many studies have shown that there are some differences between the EEG data collected from single subject in different time periods, which makes subject-dependent models have inconsistent performance on session datasets, and the abrupt drop often occurs in classification accuracy. He et al. performed the intra-day task and cross-day task on a 12person dataset separately. The results showed that the subjectdependent model had an excellent performance on the intraday task, but it did suffer from the accuracy drop on crossday task. They tried to alleviate the problem of session sample domain differences by the transfer component analysis algorithm(TCA), which obtained some improvement  [13] . Meanwhile, Lin et al. proposed a robust principal component analysis(RPCA)-embedded transfer learning(TL) to obviate intra-and inter-individual differences  [14] . In the Multiple Day Music-listening experiment using Emotiv(MDME) dataset that included 12 subjects for the cross-session task, the PCA sample space visualization revealed that the session sample domains of different subjects had different degrees of variation, and the RPCA had the ability to close the distance between session domains, which resulted in alleviating the session differences. This cross-time EEG difference was also mentioned in other studies  [2] ,  [3] ,  [15] . Therefore, the solution of crosstime EEG differences determines the performance of subjectdependent model in a significant way.\n\nFor the cross-subject emotion recognition task, the impact of individual differences is more powerful, where the universal model often exhibits poor performance when facing unknown subjects  [16] . To overcome the problem that emotion recognition model had poor generalization across subjects, Meng et al. proposed Deep Subdomain Associate Adaptation Network(DSAAN) to build a source-to-target domain transfer network by minimizing the sum of source-domain classification loss and Subdomain Associate Loop(SAL), and the result of cross-subject positive-negative emotion classification experiment in SEED was 89.23 ± 1.93%  [17] . Zhao et al. learned the group public component and subject private component by shared encoder and private encoder respectively, this framework achieved domain transfer by feature encoding.\n\nAfter private component extraction for unknown subjects, the more similar subject private classifiers would be granted higher weights, which would influence the joint classification of private and shared classifiers, and the result of 86.7 ± 7.1% was obtained on the SEED dataset  [18] . In general, the main solutions of domain differences caused by individual information are transfer learning  [7] ,  [19] -  [21]  and deep learning  [10] ,  [15] ,  [22] . For the results, both methods can obtain excellent results, but both miss the analysis of EEG information and the interpretability of EEG individual differences, which as the basic theory of affective brain-computer interfaces is more important for EEG-based emotion recognition. In addition, the ideal results on one dataset still had limitations. Compared with the cross-subject emotion recognition task, the cross-dataset task not only needs to deal with EEG individual differences, but also needs to consider differences in acquisition devices, emotion stimulation materials and emotion modeling approaches. Therefore, the cross-dataset emotion classification task is the most challenging. If a reasonable and effective solution could be given, it would substantially improve the performance of affective brain-computer interface. Lan et al. performed positive-neutral-negative emotion classification experiments on SEED  [23] ,  [24]  and DEAP  [25]  datasets with various domain-adaptive algorithms, and there were significant differences between two datasets in terms of acquisition devices, emotion stimulation materials and emotion modeling approaches. In the cross-dataset experiments, although the domain adaptation algorithm could make some improvements compared to the baseline, the poor results were obtained with accuracies ranging from 30% to 50%  [9] .  Ni et al.  proposed new domain adaptation sparse representation classifier(DASRC) applied to the cross-domain emotion recognition task, and cross-subject tasks results were similar to other research in both the DEAP and SEED, but the accuracy drop still occured in the cross-dataset task, which fell to 50% directly  [26] . Similarly, this situation occurred in other research about cross-dataset emotion recognition, which indicated that some unknown factors contributed to universal model usually show the unsatisfactory emotion recognition ability on the cross-dataset task  [27] -  [29] .\n\nIn summary, complex EEG individual information has a huge impact on different emotion recognition tasks. Although these differences are well accepted in the EEG-based af-fective computing field and there exists solutions to deal with individual differences, the in-depth investigation about EEG information is rare, which results in deep learning and transfer learning methods have certain effects but still lack interpretable theoretical support about EEG signals. Therefore, it is necessary to clear up the components of EEG information for affective computing. Only if we know the impact of different information on universal models, it is possible to draw up the processing methods for different EEG information, and attempt to achieve more stable affective braincomputer interfaces. Based on the above ideas, we conducted an interpretability study of EEG information, analyzed the impacts of different information on EEG sample distribution, and designed solutions to cope with difficult cross-dataset tasks based on the obtained knowledge of different EEG information. The major contributions are as follows.\n\n(1) Through the analysis of feature extraction algorithm, we compared the performance and computational principles of differential entropy and fractal dimension, analyzed the meanings of two feature for EEG signals from the view of energy, and simplified the calculation process.\n\n(2) For the complex EEG information, the different sample aggregation phenomena were observed by sample space visualization based on power feature. And the difference information ratio and sample aggregation degree were quantified for each dataset, all dataset were the mixture of different EEG information, which influence the distribution of samples. Based on the EEG energy distribution patterns, we analyzed the influence of different information and concluded the cause of different sample aggregation phenomena. Finally, a reasonable explanation work for different sample aggregation phenomena was presented.\n\n(3) For individual emotional information, the individual emotional feature distribution patterns were obtained by channel emotion classification, the individual emotional feature distribution difference were found, and its impact on the construction of universal model was analyzed. In order to deal with this difference appropriately, we designed different weight extraction methods to quantify emotional feature distribution patterns, and proposed the weight-based channelmodel matrix framework, which made the universal model more stable and effective on cross-dataset tasks.\n\nThis paper is organized as follows. Section II reviews the five datasets we used in this paper, and documents the data preprocessing process, including signal preprocessing, feature extraction, and sample selection. Section III analyzes and quantifies the impact of different EEG information on sample distribution, and gives an explanation of EEG sample aggregation phenomena based on energy distribution patterns. Section IV explains the advantages of channel-model matrix modeling over traditional multidimensional modeling based on emotional feature distribution patterns, and introduces the design process of four weight extraction methods and trial correction strategy. Section V describes the details and results of two experimental protocols. Scetion VI is the discussions about the weight-based channel-model matrix framework and the EEG-based cross-dataset emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Dataset Preprocessing",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Datasets",
      "text": "Many stuides have attempted to perform cross-dataset tasks on SEED and DEAP datasets. But due to the differences in EEG acquisition devices, data specifications, emotion stimulation materials and emotion label modeling between the two datasets, the cross-dataset classification task is extremely challenging. Therefore, we selected five similar public datasets: SEED, SEED-IV  [30] , SEED-V  [31] , RCLS  [32]  and MPED  [33] . The commonalities of these datasets are as follows.\n\n(1) The EEG acquisition devices are the 62-channel ESI NeuroScan System with international 10-20 system of EEG caps.\n\n(2) Paradigms all use video clips to induce emotions, and all datasets contain positive, negative, and neutral emotions.\n\n(3) The category labels are discrete emotion modeling method rather than valence-arousal emotion modeling method.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Signal Preprocessing Process",
      "text": "Signal preprocessing plays an important role in EEG signal application, which clean the EEG signal through a series of processing steps. Firstly, we used the AutoMagic  [34]  automatic preprocessing framework to perform the following operations on all EEG signals: (1)Band-pass frequency filter between 1Hz and 50Hz. (2)Taking Fpz electrode signals as reference, EOG regression was employed to eliminate EOG  [35] ,  [36] . (3)IClabel was used to eliminate artifacts  [37] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Feature Extraction",
      "text": "Based on the feature analysis of differential entropy and fractal dimension in Appendix A, we knew how both features work in the EEG signal measurement, and we extracted the simplified features: variance(differential entropy  [24] ) and power(Katz's fractal dimension  [38] ,  [39] ). The calculation formulas are as follows, where x represents the EEG signal. The detailed analysis can be found in Appendix A.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Sample Selection",
      "text": "Due to the varying paradigm lengths between datasets, there were differences in the number of segmented samples. We would like to build balanced emotion datasets with a certain number of data according to the sample number of different datasets. The samples were selected based on the following two principles.\n\n(1) For emotion analysis, the emotions(positive, negative, neutral) are selected for analysis, which of them are the common emotions of each dataset.\n\n(2) To ensure the balance of the dataset, n samples are selected randomly on each trial's segmented dataset in different datasets.\n\nThe specification of datasets after selection is as follows. Note: Trial: the number of trial we chose. Sample: the number of samples for each trial. Total: the total number of sample for each dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iii. Investigation Of Eeg Information",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Visualization Of The Sample Space",
      "text": "For different datasets, the visualization of sample distribution were obtained by t-SNE  [40]  in intra-session and multisessions scenarios respectively. After observation, we found that there were four kinds of sample distribution patterns under variance and power features: emotional aggregation, trial aggregation, session aggregation, and individual aggregation.\n\n1) Emotional aggregation and trial aggregation in intrasession scenes: Firstly, we visualized each session dataset and analyzed the distribution of emotional samples under different session data. The details are shown in Fig.  1 .\n\nFig.  1 . The sample space visualization in the intra-session scenario under power feature. The left subplot is the sample space visualization marked by emotions(positive-red, negative-blue, neutral-green), and the right subplot is the sample space visualization marked by trials.\n\nIt was found that there existed different degrees of emotional sample aggregation under different data. In particular, on the basis of emotional sample aggregation, there are several samples clustered into small clusters among the emotional clusters. We assumed that the samples were not only aggregated by emotion, but also existed trial aggregation phenomenon. Therefore, we marked the different trial samples with different colors, and trial aggregation phenomenon were found.\n\n2) Session aggregation and individual aggregation in multisessions scenarios: Considering that individual difference is a widely-accepted attribute of EEG, we first investigated the relationship between different session data for each subject. For the multi-session dataset, we visualized the sample distribution of multiple session data belonging to a single subject and observed the distribution pattern between different session data. The visualization results are as follows.\n\nFig.  2 . The sample space visualization in the single-subject scenario under power feature, the left subplot is the sample space visualization marked by emotions(positive-red, negative-blue, neutral-green), and the right subplot is the sample space visualization marked by sessions.\n\nAs shown in Fig.  2 , the samples are clustered by session. There are different degrees of emotional aggregation in different session domains. This phenomenon indicates the following two points:\n\n(1) The subjects' brain states are stable during each session, which lead to a tendency of aggregation on each session samples.\n\n(2) Even though different sessions generated from the same subject, there also existed differences between different sessions.\n\nAfter observing the session aggregation phenomenon in the intra-subject scenario, it is necessary to investigate the influence of session aggregation phenomenon and the relationship between sessions of different subjects in multi-subjects scenarios. The visualizations of the multi-subjects sample distribution are shown in Fig.  3 .\n\nAs shown in the left subplot of Fig.  3 , samples marked by subject, there were large differences between the samples belonging to different subjects. Due to the impact of individual differences in EEG, sample space shows the individual aggregation phenomenon. As shown in the right subplot of Fig.  3 , samples marked by session, we found that the sample distribution in multi-subjects scenario still retained the session aggregation phenomenon, while the samples of different ses-Fig.  3 . The sample space visualization in the multi-subjects scenario under the power feature, the left subplot is the sample space visualization marked by subjects, and the right subplot is the sample space visualization marked by sessions.\n\nsion presented different degrees of aggregation. In most cases, each individual cluster is composed of session sub-clusters.\n\nIn conclusion, the samples present a tendency of session aggregation on the basis of individual aggregation in the multi-subjects scenario, which reveals that the EEG individual information contains session difference information. However, these individual information are extremely unhelpful to the construction of universal model. In intra-session scenario, the samples show trial aggregation on the base of emotional aggregation, which reveal that the emotional difference information includes the trial difference information. Therefore, the EEG information can be decomposed into four kinds of information(individual, session, emotion, trial), each of which causes different aggregation phenomena. The relationship of sample clusters under four aggregation phenomena is individual ⊇ session ⊇ emotion ⊇ trial.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Quantification Of Sample Aggregation Phenomena",
      "text": "To further know the degree of different aggregation phenomena for each dataset, two metrics, the ratio of difference features and the sample aggregation coefficient(SAC), were used to quantify various aggregation phenomena.\n\nThe first metric is the ratio of difference features. Taking the dataset with n-dimensional features as an example, Pearson correlation analysis is employed between each dimensional data and the label of corresponding difference, and the features with P≤0.05 are considered as difference features, the difference information of which are the main factors of sample aggregation phenomena. Finally, the ratio between the number of difference features and the total number of features is calculated to measure the difference information component of the dataset  [41] .\n\nSecondly, we combined the t-sne and k-means algorithms for the calculation of the sample aggregation coefficient, which represented the degree of intra-cluster sample aggreagation influenced by different EEG information. The details are shown in Appendix B.\n\nUnder the variance feature and inverse power feature, two metrics were calculated. The results are as follows. As shown in Table  II , the following conclusions were obtained:\n\n(1) For the ratio of difference features, there existed different numbers of features representing the differences of emotion, trial, session, and subject on all the datasets, which influenced the sample distribution.\n\n(2) Comparing the results of variance features and inverse power features under different datasets, we found that all datasets had similar results on the quantification of emotion, session and subject sample aggregation. However, the inverse power feature showed a higher ratio of difference features and a more stable trial aggregation phenomenon compared to the variance feature in the quantification of trial sample aggregation.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Analysis Of Brain Energy Patterns",
      "text": "To further investigate the causes of these aggregation phenomena at the feature level, we draw energy distribution plot with power feature for each sample to analyze the differences from different views. And the power feature values of each sample were normalized to the range of 0 and 1. Therefore, the plots represent the relative energy distribution pattern.\n\nFirstly, we observed the energy patterns in different time periods of a single trial for trial aggregation phenomenon analysis, and it was found that the power feature had the property of cross-time stable patterns such as differential entropy  [23] . As shown in Fig.  4 , although there were some differences in energy patterns between time periods, it could be found that brain energy patterns were stable during the whole trial. In addition, after observing the energy distribution of different subjects, we found that the areas with high energy of most subjects were mainly distributed in the frontal, temporal and occipital lobes. The energy in the parietal and anterior occipital lobes was lower than that in the above regions.\n\nSecondly, we visualized the energy patterns of different trials for each session, and compared the differences in energy patterns. As shown in Fig.  5 , there are more or less differences in the trial energy patterns, which might be result from the energy change of brain activity under the stimulation of different trials during the same session. But trial energy distributions are still stable in general.\n\nCombining the above results, we assumed that the trial aggregation phenomenon was due to the similar energy patterns among trial samples. The gaps between trial clusters were influenced by the differences in trial energy patterns. In addition, the differences between trials were greater than intra-trial sample differences.\n\nAfter analysis of trial energy patterns, we combined trials' energy pattern belonging to the same emotion for each session, and compared these energy patterns from the views of emotion and session. The energy pattern plots are shown in Fig.  6 . After comparison of different emotional energy patterns from a single session, it was found that the emotional differences were also caused by the differences in the energy distribution. In addition, the comparison of different sessions' energy patterns indicated that there existed greater energy pattern differences between sessions than that of emotion. Similarly, the phenomena of individual aggregation also resulted from greater energy differences.\n\nCombining the results of sample distribution visualization and energy pattern analysis, we knew that various sample aggregation phenomena were caused by different degrees of difference information, and the difference information mainly reflected the energy pattern differences.\n\nIn general, we concluded the following 3 points to explain the difference in EEG information.\n\n(1) Emotional differences, trial differences, session differences and individual differences in EEG were all the different representations of energy pattern differences.\n\n(2) Due to the various degrees of differences, emotional differences and trial differences were hard to observe in multisubjects scenes, which indicated both were the much smaller than session differences and individual differences.\n\n(3) The differences were ranked in ascending order: trial differences, emotional differences, session differences and individual differences.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. The Processing Of Different Eeg Information",
      "text": "The effects of different EEG information on emotion classification were different. Based on the above phenomena, we would like to eliminate the invalid information that hindered the work of the universal model and retained the beneficial information. The differences between individual and session domains caused by individual and session information are the major factors that hinder the generalization of the universal model. We applied the Personal-Zscore(PZ) in each session dataset to eliminate the individual differences and session differences in the dataset, which could unify different session domains into a domain with 0 mean and 1 variance to improve the model generalization ability  [41] .\n\nEmotional information and trial information, which influence the sample distribution within the session domain, are the basis of emotion recognition. Based on previous work about the quantification of emotional information, it was found that different subjects' EEG data contained different levels of emotional information, but the results in Appendix A showed that even if data contained different degrees of emotional information and the impact of domain differences was eliminated, there were still unknown factors that led to large differences between different datasets. Therefore, both emotional information and trial information need more reasonable ways to improve the effectiveness of emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Weight-Based Channel-Model Matrix Framework A. Analyzing The Distribution Patterns Of Emotional Difference Features",
      "text": "After analyzing the causes of different EEG sample aggregation phenomena, we paid more attention to the challenge of cross-dataset emotion classification task and investigate the reasons for the poor performance of universal emotion recognition model. Specially, only the inverse power feature that reasonably represented energy was used for subsequent research.\n\nFirstly, it was necessary to quantify the emotional information of different dimensions, and we performed 2-class emotion intra-session classification with Leave-One-Trial-Out(LOTO) cross-validation for each dimensional dataset. Because only the inverse power feature datasets were selected, the dimension also could be considered as channel. The accuracies of each dimension were considered as weight, the demension(channel) with high weight was called as emotional difference feature. Therefore, weights were visualized in the same way as the energy pattern to observe the degree of emotional information for each channel and the distribution of emotional difference features. As shown in Fig.  7 , there are significant differences between different subjects' emotional feature distributions. However, comparing the emotional feature distributions of sessions belonging to a single subject, these distribution patterns are similar.\n\nAlthough the brain was a three-dimensional organic organization, the emotional feature distribution also could reflected the brain activities. we presented some assumptions from the perspective of the brain mechanism. For a single subject, the similar distributions between emotional features of sessions revealed that there existed neurons for emotional information processing in the brain. When brain were stimulated by the same emotion material in different time periods, the neurons produced some energy changes during the processing of emotional information, which were delivered to the similar scalp electrodes through stable brain circuits by EEG. For the difference of emotional feature distributions between subjects, we assumed that it would be caused by the complex individual factors, which should be validated through a further experiment.\n\nNext, we analyzed the impact of individual emotion feature distribution differences on the construction of a universal emotion recognition model from the classification perspective.\n\n(1) For the training set, it contains a mixture of emotional data and invalid data in each dimension. The emotional feature distribution differences cause the chaos of the sample distribution, it is difficult to learn the consistent emotional knowledge from the training set using machine learning algorithms. Consequently, the trained universal model have unstable emotion recognition performance.\n\n(2) For the test set, there exists a case that the test set would be misclassified by the traditional mutli-dimensional universal model, whose training data contains no or less data with similar emotional feature distributions. Therefore, the test set will obtain different results from the universal models constructed on different subjects' EEG data.\n\nIn summary, it was unreasonable to build the universal model with the data of different subjects without determining different subject emotional feature distributions, and the universal model obtained from such training process was also unreliable.\n\nCombining the results in Appendix A, we could know the impact of IEFDD on different datasets. Although all datasets had the high ratios of emotional features, the higher accuracy obtained with leave-one-session-out emotion classification in SEED was due to the high overlap of different subjects' emotional feature distributions. For the other datasets with low distribution overlap rate, IEFDD would extremely influence the classification accuracy.\n\nTherefore, there exists a large randomness for unknown subjects' emotion recognition without prior knowledge of the individual emotional feature distributions. When one test set was classified by models trained with different datasets separately, the classification results depended on the similarity of emotional feature distribution between training data and test data. This randomness is unacceptable for affective braincomputer interface. Therefore, we considered that the joint classification strategy of channel models might be better than the traditional multi-dimensional model classification in the EEG-based emotion recognition scene. Hence we proposed WCMF for dealing with the impact of individual emotional feature distribution difference, which would be a solution for cross-dataset emotion recognition.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Channel-Model Matrix",
      "text": "Considering the individual emotional feature distribution difference, we employed the joint classification strategy of channel models. In this way, the training data of each channel model were selected based on the emotional feature distributions of different sessions. For each channel, the session data that showed emotional differences were retained, and invalid data were eliminated to ensure a stable emotional distribution of samples in the training set, and deep neural network(DNN) models with one hidden layer and 100 hidden-units were trained. In addition, the channel-model matrix was built up by the channel DNN model based on inverse power features, and this kind of modeling strategy could solve the problem of chaotic emotional sample distributions due to the individual emotional feature differences in traditional multidimensional modeling.\n\nHowever, there still exists the following limitations in classification with the channel-model matrix.\n\n(1) In terms of model training, it is necessary to design a method to quantify the emotional feature distribution of different subjects, which determines the quality of training data and the performances of channel models.\n\n(2) For emotion classification, although each channel model have the stable ability to recognize emotion, the test data are still composed of valid emotional information and unknown invalid information. Therefore, the classification results are influenced by the number of emotional difference features in the test set. The greater the number of emotional difference features, the greater number of channel model will judge the emotions correctly, and the greater positive contribution will be made for joint classification. On the contrary, invalid information would made greatly impacts to the joint classification, resulting in misclassification.\n\nIn summary, the classification of channel-model matrix still exists a randomness on the test set. Therefore, a efficient quantification method of emotional difference feature distribution would solve the randoness and improve the performance of channel-model matrix.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "C. Exploration Of Weight Extraction Methods",
      "text": "For channel-model matrices, weights were beneficial for the selection of training data and the classification of known subjects, so it was necessary to find methods for learning weights from data in a reasonable and efficient way. Initialize DN N weight j as zero vector.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "4:",
      "text": "data, label =DIVIDE(Data, Label, pair j ), get the j-th emotion pair data and label. hidden, weights=DNNENCODER(models, data), get the hidden samples and weights for each DNN model.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "8:",
      "text": "W , Select weights by the similar degree between hidden and label.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "9:",
      "text": "weight=FLITER(W ,wT hreshold), retain the weight value ≥ wT hreshold and average weights. data=REGROUPDATA(data), retain the data with unextracted weights.\n\n12: end while 13: end for 14: return DN N weight\n\nThe idea of the DNN weight extraction method was to train a DNN model that only contained one hidden unit. 61-channel dataset were used to train the one-unit DNN, and this DNN would learn a way of information fusion from the dataset, where the edge weights between each channel data and the hidden unit described the contribution of each channel data to the hidden unit. Then, the higher weights were extracted, and the remaining channel data were used for the next round of weight extraction until training accuracy was lower than threshold.\n\nSpecially, when performing DNN training multiple times in each round, there were two kind of opposite cases, positive and negative weight vectors. To eliminate this instability, based on the positive and negative weights, the hidden feature samples were obtained by positive and negative weight DNN model. Then, we compared the relationship between the positive and negative hidden feature sample distribution and selected the weight whose hidden feature sample relationship was similar to that of the label. In addition, the process of multi-times weight extraction still demanded a high time cost.\n\n3) Inter-group difference-based T-test weight extraction method: To address the problem of higher computational costs, the t-test method was employed to measure the degree of inter-group differences between different emotions for each channel data. The algorithm is as follows. p, q=pair j , get the index of j-th emotion pair.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "4:",
      "text": "p value=TTEST2(emotion p , emotion q ), perform ttest on two class dataset. weight j =LOGN(p value, 10, 2), apply log 10 function on p value 2 times. 7: end for 8: return weight Firstly, t-test was used to calculate the inter-group p-value of each channel data, and the smaller the p-value was, the greater degree of inter-group difference in that channel. However, because the value range of p-value was too small and the differences between the p-values of each channel were vast, which caused that directly applying the normalized p-value as weight could not measure the relationship of inter-group differences between channels effectively.\n\nTherefore, we performed a series of value range transformation operations, beginning with the transformation of the p-value into a positive integer value range by the inverse ratio operation. Facing the inverse p-values with the extremely large value range, we utilized the monotonicity and slope decreasing property of the log function to decrease the value range and retain the size relationship of p-values between each channel.\n\nFinally, the p-value after the value range transformation provided a more reasonable way to measure the degree of inter-group differences in each channel. In addition, the T-test weights had extremely low time costs.  As shown in Table  III , the results of T-test weight and C weight are similar, but both of them are lower than DNN weight. Comparing the similarities and differences of different weight extraction methods, we found that although the extracted weights could describe the differences between different emotion samples in each channel indeed, the similarities and differences of sample distribution tendencies on difference features were ignored. Taking the positive-negative emotion recognition scenario as an example, the following three sample relationships exist in dimensions.\n\n(1) Positive samples have higher feature values than negative samples.\n\n(2) Positive samples have lower feature values than negative samples.\n\n(3) There is no difference between positive samples and negative samples, and the samples are mixed up.\n\nFor the channels reflecting such invalid sample relationships as (3), the chaos sample distributions are filtered out by the three weight extraction methods. However, the two types of sample relationships (1) and (  2 ) are not determined on C weight and T-test weight extraction methods. Because these two relationships are mutually exclusive, it is reasonable for the model to classify the data with the same sample relationship as the training data, but the classification between data with different sample relationships would lead to great misclassification results.\n\nConcerning the DNN weight extraction method, the operation of selecting weights whose hidden feature samples have the same distribution with the label plays a role of sample relationship judgment. So, this is the reason why DNN weights outperformed than other weights, but the DNN weight extraction method still lacks the ability of processing the weights with the opposite relationship to the label. if label size relationship consist to sumF eature then   As shown in Fig.  8 , the higher weights are selected on all weights as the red part in all plots, but CT weight also selects high negative weight as the blue part in the plot of CT weight, and the white part in T-test and CT weight plots represents the channels with chaos information.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "D. Comparison Of Modeling And Weight Extraction Methods",
      "text": "To comprehensively analyze the impact of different modeling strategies and different weights on the classification results, we combined all classification results of different methods with the cross-dataset emotion classification task.\n\nAs shown in Table  IV , we analyzed the results from different points of view. Comparing the classification results of different datasets, it could be found that the difficulties of cross-datasets classification tasks on different datasets were varied, which was caused by the individual emotional feature distribution difference. Among datasets, the SEED dataset obtained the best performance, which revealed that the subjects' emotional feature distribution had more common parts in SEED. Comparing different modeling strategies, it could be found that the average classification results of channel-model matrix without the guidance of weights were slightly better than that of the multidimensional model (see Fig.  9 ). After observing the results of one test dataset under different training datasets, we found that the classification on the traditional multidimensional model was unstable, with higher variance, which was caused by the chaotic and complex sample space in the training set with IEFDD. In contrast, the classification accuracies were relatively stable for the channel-model matrix. The classification results are greatly influenced by the number  of emotional features in test set, the more emotional features, the more number of channel models with correct classification, and the higher accuracy of joint classification.\n\nComparing channel-model matrix classification and weightbased channel-model matrix classification, due to not all data had a great number of emotional difference features, the emotion recognition task for different subjects could not be handled by the channel-model matrix alone. Therefore, for the purpose of making the channel-model matrix work stably and effectively, the guidance of weight was necessary for the test data classification.\n\nCombining the classification results of different weights with the previous weights comparison analysis, it was found that the classification results of CT weights were the best(see Fig.  10 ), which validated the idea that sample relationship judgment in difference features was correct. Comparing the performance of different weight extraction methods, we considered that CT weight was the best extraction method, which not only had the T-test's extremely low computational cost but also had a more precise capability of learning emotional information than DNN weight.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "E. Emotion State Recognition Using Trial Information",
      "text": "Considering that EEG had a stable trial sample aggregation phenomenon under power features, we considered trial information could be helpful for emotion sample recognition. Therefore, we tried to calibrate the classification results by real trial labels, and emotion sample recognition was turned into emotion state recognition. Firstly, true trial labels were used in cross-dataset classification with different weight extraction methods to define each trial sample set. Then, the predicted labels were unified to the majority of sample prediction for each trial sample set.\n\nAs shown in Table  V , two points could be observed.\n\n(1) With the help of trial information, the emotion state classification got an improvement compared with the emotion sample classification. This phenomenon indicated that there were more or fewer misclassifications in trial datasets.\n\n(2) The improvement brought by the trial correction strategy depended on how great the emotion sample classification results were. When the majority of samples in the trial clusters were classified correctly, the misclassified samples could be corrected. Therefore, this strategy usually brought a boost to the case with high classification correct rate on the emotion sample recognition.\n\nTo solve the situation that true trial labels were not available, we applied the DBSCAN algorithm for the estimation of trial number in unknown datasets based on the trial aggregation of power features. Then, the predicted trial labels were obtained by k-means algorithm. The details of trial label estimation algorithm are shown in Appendix C.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "V. Experiments And Results",
      "text": "As mentioned in the above sections, all the weight extraction methods are based on labels, so weights extracted from the training set are reasonable. However, in practical scenarios, the labels are unknown for EEG generated in real time, and the weight extraction methods can not be employed directly. Moreover, the cross-dataset emotion classification task is still tricky and the practical scenarios of aBCI are variable. Here, we validated the performance of WCMF for the cross-dataset task in two kinds of practical scenarios.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A. Emotion Recognition In The Unknown Subject Scenario",
      "text": "In this scene, the emotion label and trial label are unknown for the test data. However, since weight extraction method required labels, we predicted the labels based on the emotional aggregation phenomenon and trial aggregation phenomenon under the inverse power feature.\n\nFirstly, the emotion cluster label was generated by the combination of t-sne and k-means methods. The result of clustering depended on the ratio of emotion features and the amount of emotional information in the data, which   VI , we found that the predicted trial labels and CT weights had different impacts for different datasets, there were different degrees of improvements in performance for SEED, RCLS, and MPED datasets. The cases of accuracy reduction occurred on SEED-IV and SEED-V datasets.\n\nCombining the results of weight-based cross-dataset experiments in ideal scenarios, it is known that the classification of unknown subjects greatly depended on the effectiveness of weights. The similarity between the predicted weights and the true weights influences the judgement of joint emotion classification directly.\n\nFor each session dataset, the performance of emotion label estimation determines the results of WCMF indirectly. When there exists strong emotional differences in the session dataset, the clustering algorithm is able to perform emotion clustering well, and the reliable emotion estimation labels were obtained. In this way, the knowledge of weights based on reliable predicted labels is closer to the knowledge learned in the ideal scenarios, which improves the emotion recognition ability on unknown subjects. In contrast, for the session dataset with poor emotional information, it is difficult to perform emotion clustering in the sample space with chaotic emotion distributions, and each emotion cluster obtained by k-means contains a certain number of other emotion samples. Therefore, weights based on chaotic predicted labels are unable to represent the differences between different emotions correctly. Regarding the results of SEED-IV and SEED-V as an example, due to the low ratio of emotional difference features, they got greater impact by invalid feature, which caused the chaos of sample space, the mistake of label estimation and invalid weight extraction. Finally, the invalid weight played a negative role in the cross-dataset emotion recognition.\n\nIn summary, the results reveals that emotion recognition for completely unknown subjects is very tricky due to IEFDD. Although the strategy of label predicteding makes some effect, it still exists some limitations, which extremely relies on the emotional information of data.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "B. Emotion Recognition Based On The Prior Weights Scenario",
      "text": "Considering the challenge of emotion recognition in unknown subject scenarios, it was unreliable to recognize emotion just relying on data. Therefore, based on the characteristic of the similar session emotional feature distributions belonging to one subject, the emotion recognition based on the prior weights was performed. The visualizations of CT weights for different subjects' sessions are as follows.\n\nAs shown in Fig.  11 , the visualization of CT weights had similar results as the previous visualization of C weight, where the different subject's weights existed large differences, and the session CT weights conducted from a single subject showed similar emotional feature distribution patterns. It was known that CT weights measured the brain activity under different emotion stimulation. However, due to the complexity of neural activity, EEG not only responded to main brain activity evoked by emotion, but also contained other unknown brain activity, which led great similarities and some differences between different session weights from a single subject.\n\nTo capture stable emotional feature distribution pattern, it is necessary to perform CT weight extraction from multiple times emotion recognition tasks, and the common parts between CT weights are maintained as a stable and valid emotional feature distribution pattern for that subject. Thus, the affective braincomputer interface would work as a system based on initial weights, which are used to analyze the data acquired through online emotion recognition tasks. After each work session, initial weights would be more stable through the updating based on the online session data constantly.\n\nFor the existing data, we picked the datasets with 3 sessions to validate the effect of weight merging strategy, and leaveone-session-out strategy was employed for the generation of unknown session's prior weight. For a subject dataset, when one session is considered as test data, the other sessions are considered as priori data, and the CT weight extraction method is employed on each priori session relatively. Then, the common parts between priori CT weights are merged as test session's weight, which simulates a process of initial weight updating. Because the subject weights are generated from the prior knowledge, the problem of information leakage would not happen. Finally, we took SEED, SEED-IV, and SEED-V as the test sets, and performed cross-dataset 2-class emotion state classification task based on the prior weights.\n\nAs shown in Table  VII , due to the similarity of CT weights for different sessions of the same subject, the prior weights served as guidance for the emotion recognition task of unknown sessions. In general, the average accuracy of the SEED, SEED-IV, and SEED-V datasets got an improvement of 13.41%, 4.96%, and 7.00% on negative-neutral, positiveneutral, and positive-negative under the guidance of prior weights.",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Vi. Discussion",
      "text": "Comparing the cross-dataset emotion classification results under two kinds of practical application scenarios, it was found that emotion recognition of priori weights had higher reliability than that of predicteding weights. Considering the challenges of EEG-based cross-dataset emotion recognition, we believes that the strategy based on prior weights and updating weights continuously is a reasonable solution for affective brain-computer interfaces in practical scenarios. In the offline phase, the initial weights are extracted through one or more emotion experiments for unknown subjects. In the online phase, the weights will guide the model matrix to perform stable emotion recognition in real time. Beside, the EEG data obtained from the online working session are used for the updating of model matrix and weights, which not only expands the data of channel model, but also makes the subjects' weight better. As the affective brain-computer interface works over and over again, an extremely stable model matrix will be obtained, and different subjects' effective emotional feature distribution pattern will provide a reference for further research about gender  [42] , behavior  [43]  and so on. Compared with the traditional multidimensional universal model, the following advantages of WCMF are found.\n\n(1) The joint classification of channel-model matrix avoids the risk of overfitting and the curse of dimensionality, and the classification principle of channel-model matrix is more reasonable, the performances are more stable under the guidance of the weights.\n\n(2) In terms of universal model training, CT weights not only learn the emotional feature distribution for different subjects reasonably and accurately, but also have lower time cost and computational cost. Under the guidance of weights, a certain amount of data with consistent emotional sample distribution is selected for the training of channel models, which not only accelerates the models' training, but also makes each channel model have stable emotional classification capability.\n\n(3) In practical application scenarios, EEG data generated in real time can be recognized by different model matrices with the help of weights, even though the model matrices built by different groups of subjects. Importantly, the storage of weights is more convenient than the storage of a priori EEG signals or EEG feature data sets.\n\nAlthough WCMF has some advantages, some limitations of WCMF and EEG-based affective computing are found through these mutli-dataset works in this article.\n\nFor the construction of datasets, due to the cross-time energy pattern during trial and the trial sample aggregation phenomenon, we consider that the number of trial is more important than the length of trial in this work. Therefore, we suggest that the construction of dataset should be goaloriented. For the purpose of multi emotion analysis, we suggest to expand the number of trial with a certain length, which could make the result more stable and conclusions of emotional differences more reliable. In addition, for the purpose of further analysis on EEG individual difference, the dataset needs more number of subject and session. For the purpose of real-time emotion degree change analysis, the length of trial and the reasonable acquisition method of realtime emotion changing label are more important.\n\nFor multi-emotion recognition, we investigated the differences between different emotion pairs in this article, although CT weight extraction method had the ability to capture differences between different emotion pair, and whether there exist stable patterns of multi-emotion relationships in different subjects still need more data and experiments for further investigation. Combining the phenomenon of few differences between negative and neutral emotions, it's necessary to validate that this phenomenon is caused by the incompletely induction of negative emotion by the emotional stimulus material, or due to the brain activity does not have significant differences between negative and neutral emotions  [44] . Therefore, we believe that emotion recognition in multi-emotion scenarios is more challenging than cross-dataset tasks, not only needs to find multi-emotion relationships among different subjects, but also needs to verify the reasonableness of multi-emotion recognition.\n\nFor application of affective brain-computer interface, it's unstable, unreliable and impossible for multi-dimensional universal model without weight guidance to deal all emotion recognition task in various application scene of emotion recognition. Although the CT-test weight-based channel-model matrix framework can handle the tricky cross-dataset emotion recognition task, this solution still exists so many limitations that framework need more improvements to fit the various affective computing application scenes. After all, there are not all scenarios need to process all emotions in real life. Considering the performance of the universal model, the challenge of emotion recognition and the complexity of application scenarios, it's necessary to specify different application scenes. Based on the emotion recognition requirements of different application scenarios, the potential emotions in different scenarios will be analyzed. Then, customized emotion subset modeling solutions will be designed to build a stable and effective emotion subset model for emotional sub-scenes. Of course, the multi-emotion universal model is still the final goal of affective brain-computer interface, but the sub-scene emotion recognition modeling can provide valuable experiences for EEG-based affective computing.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Appendix B The Calculation Of Sample Aggregation Coefficient",
      "text": "In our previous work, we combined t-SNE and k-means algorithm for the Calculation of Individual Aggregation Coefficient (CIAC) algorithm to quantify the individual aggregation phenomenon, which directly judged the two sub-clusters were close in individual cluster  [41] . Therefore, the judgement of two sub-clusters as an improvement was applied in Calculation of Sample Aggregation Coefficient for the quantification of different sample aggregation phenomena, which described the sample aggregation degree for clusters(trial, emotion, session, subject). The details are as follows. end if 12: end for 13: SAC =MEAN({rate j } n j=1 ) 14: return SAC Taking the calculation of trial sample aggregation coefficients as an example, the dataset is reduced to 2 dimensions firstly. Then, the k-means is applied for sample clustering. In i-th trial cluster, the sample number of the largest and second largest sub-clusters are counted, the distance between the two sub-clusters and the radius of both sub-cluster are considered as indicators to judge the closeness of both clusters. If two sub-clusters are judge as close, the samples of sub-clusters are considered as aggregation, the ratio of aggregation sample number and total trial sample is the i-th trial's sample cover rate. Finally, the trial sample aggregation coefficient of dataset is the mean of all trial's sample cover rates.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Appendix C Trial Label Estimation Algorithm",
      "text": "Based on the characteristics of the EEG sample aggregation phenomena, in order to cope with the situation of unknown cluster labels, we combined two clustering algorithms, DB-SCAN and k-means, to estimated the number and label of sample clusters. The specific algorithms are as follows.\n\nAlgorithm 5 The predicted of trial number and trial label Require: Data the unknown session dataset. Ensure:\n\n1: dist =DISTANCE(Data), calculation of reachable distance between each sample. 2: seq =SORT(dist), sort dist to ascending sequence. cluster number i =DBSCAN(Data, radius), perform DBSCAN with i-th reachable distance. 7: end for 8: trial number, reject invalid cluster number and get the highest occurrences cluster number. 9: embed data=T-SNE(Data,trial number) 10: trial label=K-MEANS(embed data, trial n umber) 11: return trial number, trial label Firstly, the number of sample clusters is estimated by DBSCAN. Because DBSCAN determines the reachability between samples by drawing circles based on the center of each sample, the number of clusters will be influenced by the radius. For an unknown dataset, we calculate the distance between each sample and get the reachable distance sequence in ascending order. When the radius is less than the minimum reachable distance, it cannot be clustered, and the number of clusters is 0. Therefore, the reachable distance sequence is traversed, the tendency of clustering number with radius is shown in Fig.  12 . As the radius increasing, the number of reachable samples also increasing, and the number of clusters will show a trend of growing and then decreasing until the radius is the l-th reachable distance, where all samples are reachable and the number of clusters is 1. Taking the sequence of clustering numbers from i to j, i is index that the clustering breakthrough at 0, j is the index that clustering number down to a low and stable value, the occurrences for each clustering number are statistics, the clustering number with the highest occurrences as trial estimation. Specially, the stable results of 3 clusters between j and l indicate the emotional aggregation phenomenon. Finally, predicted trial labels are generated by k-means with trial estimation number. The algorithm can also be applied to session clustering estimation scenarios.",
      "page_start": 18,
      "page_end": 18
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The sample space visualization in the intra-session scenario under",
      "page": 3
    },
    {
      "caption": "Figure 2: The sample space visualization in the single-subject scenario under",
      "page": 4
    },
    {
      "caption": "Figure 2: , the samples are clustered by session.",
      "page": 4
    },
    {
      "caption": "Figure 3: As shown in the left subplot of Fig. 3, samples marked",
      "page": 4
    },
    {
      "caption": "Figure 3: , samples marked by session, we found that the sample",
      "page": 4
    },
    {
      "caption": "Figure 3: The sample space visualization in the multi-subjects scenario under",
      "page": 4
    },
    {
      "caption": "Figure 4: Energy patterns under different time periods of a single trial.",
      "page": 5
    },
    {
      "caption": "Figure 4: , although there were some differences in",
      "page": 5
    },
    {
      "caption": "Figure 5: Different trial energy patterns under a single session.",
      "page": 5
    },
    {
      "caption": "Figure 5: , there are more or less differences in the",
      "page": 5
    },
    {
      "caption": "Figure 6: The energy patterns of different emotions for a single subject under",
      "page": 6
    },
    {
      "caption": "Figure 7: Positive-negative emotional difference feature distribution of different",
      "page": 6
    },
    {
      "caption": "Figure 7: , there are signiﬁcant differences between",
      "page": 6
    },
    {
      "caption": "Figure 8: Emotional difference feature distribution of different weight extraction methods. The value range of C, DNN and T-test weight are [0,1]. The value",
      "page": 10
    },
    {
      "caption": "Figure 8: , the higher weights are selected on all",
      "page": 10
    },
    {
      "caption": "Figure 9: The results of different modeling methods on the cross-dataset emotion",
      "page": 10
    },
    {
      "caption": "Figure 9: ). After observ-",
      "page": 10
    },
    {
      "caption": "Figure 10: The results of different weight extraction methods on the cross-dataset emotion recognition task in ideal scenarios.",
      "page": 11
    },
    {
      "caption": "Figure 10: ), which validated the idea that sample relationship",
      "page": 11
    },
    {
      "caption": "Figure 11: , the visualization of CT weights",
      "page": 12
    },
    {
      "caption": "Figure 11: Comparison of CT weights between different subjects and different",
      "page": 13
    },
    {
      "caption": "Figure 12: Changes in clustering number during the traversal of reachable",
      "page": 18
    },
    {
      "caption": "Figure 12: As the radius increasing, the number of reachable",
      "page": 18
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy\nTest\nTrain": "SEED-IV\nSEED-V\nSEED\nRCLS\nMPED",
          "Classiﬁcation weight\nNeg-Neu\nPos-Neu\nPos-Neg": "56.66%\n79.65%\n77.26%\n53.87%\n80.43%\n76.53%\n58.17%\n79.44%\n77.63%\n56.97%\n80.56%\n77.14%",
          "DNN weight\nNeg-Neu\nPos-Neu\nPos-Neg": "61.64%\n80.20%\n76.96%\n61.53%\n81.01%\n76.83%\n61.12%\n81.13%\n76.64%\n61.64%\n81.39%\n77.08%",
          "T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg": "59.98%\n79.09%\n75.68%\n47.70%\n78.26%\n74.68%\n56.55%\n77.70%\n75.33%\n56.05%\n79.73%\n76.94%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-V\nSEED-IV\nRCLS\nMPED",
          "Classiﬁcation weight\nNeg-Neu\nPos-Neu\nPos-Neg": "54.20%\n62.56%\n59.91%\n46.26%\n63.74%\n60.15%\n53.57%\n63.70%\n58.76%\n53.74%\n63.37%\n61.50%",
          "DNN weight\nNeg-Neu\nPos-Neu\nPos-Neg": "60.54%\n67.59%\n65.96%\n59.78%\n67.50%\n65.30%\n59.46%\n68.09%\n65.65%\n59.98%\n68.09%\n66.02%",
          "T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg": "53.76%\n62.67%\n59.39%\n46.72%\n62.67%\n59.63%\n52.52%\n62.93%\n59.04%\n57.48%\n63.65%\n61.06%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-IV\nSEED-V\nRCLS\nMPED",
          "Classiﬁcation weight\nNeg-Neu\nPos-Neu\nPos-Neg": "48.14%\n64.32%\n63.98%\n47.97%\n63.80%\n66.02%\n51.01%\n63.73%\n66.74%\n48.44%\n65.57%\n66.15%",
          "DNN weight\nNeg-Neu\nPos-Neu\nPos-Neg": "54.46%\n67.53%\n68.82%\n53.91%\n66.49%\n69.03%\n54.15%\n67.66%\n67.74%\n55.26%\n68.06%\n68.73%",
          "T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg": "46.51%\n64.41%\n65.28%\n47.48%\n63.96%\n66.56%\n47.90%\n63.19%\n67.81%\n50.80%\n65.82%\n68.61%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-IV\nRCLS\nSEED-V\nMPED",
          "Classiﬁcation weight\nNeg-Neu\nPos-Neu\nPos-Neg": "69.00%\n67.36%\n58.89%\n61.86%\n69.46%\n60.11%\n61.07%\n69.18%\n59.93%\n67.43%\n71.61%\n61.39%",
          "DNN weight\nNeg-Neu\nPos-Neu\nPos-Neg": "70.32%\n71.00%\n63.50%\n69.39%\n73.14%\n63.61%\n70.46%\n72.39%\n64.46%\n67.68%\n73.43%\n64.14%",
          "T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg": "67.04%\n67.18%\n58.64%\n62.00%\n69.57%\n59.29%\n42.57%\n67.00%\n59.71%\n59.25%\n70.36%\n61.57%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-IV\nMPED\nSEED-V\nRCLS",
          "Classiﬁcation weight\nNeg-Neu\nPos-Neu\nPos-Neg": "55.78%\n62.13%\n59.77%\n57.02%\n62.70%\n65.03%\n48.07%\n64.36%\n61.92%\n54.89%\n63.81%\n63.41%",
          "DNN weight\nNeg-Neu\nPos-Neu\nPos-Neg": "60.07%\n63.95%\n63.07%\n59.33%\n64.63%\n64.40%\n61.75%\n64.89%\n62.00%\n59.55%\n64.66%\n61.52%",
          "T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg": "54.59%\n61.28%\n59.46%\n56.32%\n62.46%\n61.43%\n48.38%\n61.89%\n60.16%\n54.73%\n60.47%\n61.82%"
        },
        {
          "Accuracy\nTest\nTrain": "Average",
          "Classiﬁcation weight\nNeg-Neu\nPos-Neu\nPos-Neg": "55.21%\n68.07%\n65.11%",
          "DNN weight\nNeg-Neu\nPos-Neu\nPos-Neg": "61.10%\n70.64%\n67.57%",
          "T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg": "53.42%\n67.21%\n64.60%"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy\nTest\nTrain": "SEED-IV\nSEED-V\nSEED\nRCLS\nMPED",
          "Practical Scenarios\nClassic Classiﬁcation\nMatrix Classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "53.80%\n74.40%\n67.76%\n51.48%\n64.14%\n75.97%\n57.59%\n74.41%\n58.16%\n49.62%\n69.29%\n72.21%",
          "Weight-based Channel-model Matrix In Ideal Scenarios\nClassiﬁcation weight\nDNN weight\nT-test weight\nCorrection T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "56.66%\n79.65%\n77.26%\n53.87%\n80.43%\n76.53%\n58.17%\n79.44%\n77.63%\n56.97%\n80.56%\n77.14%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-V\nSEED-IV\nRCLS\nMPED",
          "Practical Scenarios\nClassic Classiﬁcation\nMatrix Classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "52.50%\n56.28%\n58.30%\n49.96%\n63.00%\n58.20%\n55.94%\n63.98%\n57.13%\n54.00%\n61.30%\n58.28%",
          "Weight-based Channel-model Matrix In Ideal Scenarios\nClassiﬁcation weight\nDNN weight\nT-test weight\nCorrection T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "54.20%\n62.56%\n59.91%\n46.26%\n63.74%\n60.15%\n53.57%\n63.70%\n58.76%\n53.74%\n63.37%\n61.50%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-IV\nSEED-V\nRCLS\nMPED",
          "Practical Scenarios\nClassic Classiﬁcation\nMatrix Classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "52.90%\n57.22%\n66.34%\n51.08%\n61.75%\n59.74%\n54.51%\n56.11%\n55.97%\n54.20%\n61.18%\n66.56%",
          "Weight-based Channel-model Matrix In Ideal Scenarios\nClassiﬁcation weight\nDNN weight\nT-test weight\nCorrection T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "48.14%\n64.32%\n63.98%\n47.97%\n63.80%\n66.02%\n51.01%\n63.73%\n66.74%\n48.44%\n65.57%\n66.15%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-IV\nRCLS\nSEED-V\nMPED",
          "Practical Scenarios\nClassic Classiﬁcation\nMatrix Classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "62.18%\n66.64%\n53.71%\n56.14%\n62.07%\n63.18%\n51.04%\n57.86%\n60.75%\n52.86%\n63.75%\n60.68%",
          "Weight-based Channel-model Matrix In Ideal Scenarios\nClassiﬁcation weight\nDNN weight\nT-test weight\nCorrection T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "69.00%\n67.36%\n58.89%\n61.86%\n69.46%\n60.11%\n61.07%\n69.18%\n59.93%\n67.43%\n71.61%\n61.39%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-IV\nMPED\nSEED-V\nRCLS",
          "Practical Scenarios\nClassic Classiﬁcation\nMatrix Classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "52.24%\n64.72%\n68.72%\n56.82%\n64.83%\n65.40%\n55.45%\n62.60%\n66.42%\n57.64%\n65.28%\n59.63%",
          "Weight-based Channel-model Matrix In Ideal Scenarios\nClassiﬁcation weight\nDNN weight\nT-test weight\nCorrection T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "55.78%\n62.13%\n59.77%\n57.02%\n62.70%\n65.03%\n48.07%\n64.36%\n61.92%\n54.89%\n63.81%\n63.41%"
        },
        {
          "Accuracy\nTest\nTrain": "Average",
          "Practical Scenarios\nClassic Classiﬁcation\nMatrix Classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "54.10%\n63.54%\n62.66%",
          "Weight-based Channel-model Matrix In Ideal Scenarios\nClassiﬁcation weight\nDNN weight\nT-test weight\nCorrection T-test weight\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg\nNeg-Neu\nPos-Neu\nPos-Neg": "55.21%\n68.07%\n65.11%"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Accuracy\nTest\nTrain": "SEED-IV\nSEED-V\nSEED\nRCLS\nMPED",
          "Matrix classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg": "54.08%\n73.05%\n73.22%\n45.83%\n76.35%\n72.45%\n54.85%\n75.60%\n73.53%\n52.33%\n70.29%\n70.93%",
          "Weight & trial correction\nNeg-Neu\nPos-Neu\nPos-Neg": "60.18%\n80.22%\n80.64%\n60.51%\n82.94%\n80.64%\n59.93%\n82.12%\n80.76%\n60.45%\n80.42%\n80.80%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-V\nSEED-IV\nRCLS\nMPED",
          "Matrix classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg": "52.93%\n60.52%\n57.61%\n46.69%\n61.11%\n57.65%\n52.07%\n60.94%\n57.15%\n50.35%\n58.11%\n56.43%",
          "Weight & trial correction\nNeg-Neu\nPos-Neu\nPos-Neg": "52.78%\n59.70%\n56.81%\n52.46%\n59.72%\n55.83%\n53.56%\n61.70%\n56.13%\n52.26%\n61.00%\n56.11%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-IV\nSEED-V\nRCLS\nMPED",
          "Matrix classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg": "45.49%\n59.95%\n61.68%\n46.79%\n59.17%\n63.68%\n48.63%\n60.90%\n65.03%\n48.89%\n57.90%\n58.45%",
          "Weight & trial correction\nNeg-Neu\nPos-Neu\nPos-Neg": "48.19%\n58.00%\n60.45%\n46.74%\n55.28%\n60.19%\n47.95%\n57.15%\n60.17%\n47.53%\n56.56%\n60.50%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-IV\nRCLS\nSEED-V\nMPED",
          "Matrix classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg": "56.14%\n63.64%\n56.96%\n57.86%\n66.04%\n58.14%\n38.93%\n64.79%\n57.86%\n55.54%\n62.93%\n55.68%",
          "Weight & trial correction\nNeg-Neu\nPos-Neu\nPos-Neg": "69.89%\n75.07%\n61.25%\n69.89%\n77.64%\n59.79%\n68.50%\n78.46%\n62.32%\n69.18%\n76.25%\n59.86%"
        },
        {
          "Accuracy\nTest\nTrain": "SEED\nSEED-IV\nMPED\nSEED-V\nRCLS",
          "Matrix classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg": "54.87%\n59.09%\n57.06%\n53.59%\n57.68%\n59.12%\n47.16%\n59.69%\n56.53%\n53.65%\n59.23%\n58.11%",
          "Weight & trial correction\nNeg-Neu\nPos-Neu\nPos-Neg": "54.52%\n68.37%\n61.12%\n55.57%\n63.91%\n60.10%\n56.11%\n67.91%\n60.97%\n55.07%\n66.99%\n61.04%"
        },
        {
          "Accuracy\nTest\nTrain": "Average",
          "Matrix classiﬁcation\nNeg-Neu\nPos-Neu\nPos-Neg": "50.83%\n63.35%\n61.36%",
          "Weight & trial correction\nNeg-Neu\nPos-Neu\nPos-Neg": "57.06%\n68.47%\n63.77%"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "An eeg-based brain computer interface for emotion recognition and its application in patients with disorder of consciousness",
      "authors": [
        "H Huang",
        "Q Xie",
        "J Pan",
        "Y He",
        "Z Wen",
        "R Yu",
        "Y Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Eeg-based emotion recognition using simple recurrent units network and ensemble learning",
      "authors": [
        "C Wei",
        "L -L. Chen",
        "Z.-Z Song",
        "X.-G Lou",
        "D.-D Li"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "3",
      "title": "Self-weighted semi-supervised classification for joint eeg-based emotion recognition and affective activation patterns mining",
      "authors": [
        "Y Peng",
        "W Kong",
        "F Qin",
        "F Nie",
        "J Fang",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "4",
      "title": "Ms-mda: Multisource marginal distribution adaptation for cross-subject and cross-session eeg emotion recognition",
      "authors": [
        "H Chen",
        "M Jin",
        "Z Li",
        "C Fan",
        "J Li",
        "H He"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "5",
      "title": "Cross-subject eeg-based emotion recognition through neural networks with stratified normalization",
      "authors": [
        "J Fdez",
        "N Guttenberg",
        "O Witkowski",
        "A Pasquali"
      ],
      "year": "2021",
      "venue": "Frontiers in neuroscience"
    },
    {
      "citation_id": "6",
      "title": "Contrastive learning of subject-invariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "7",
      "title": "Generator-based domain adaptation method with knowledge free for cross-subject eeg emotion recognition",
      "authors": [
        "D Huang",
        "S Zhou",
        "D Jiang"
      ],
      "year": "2022",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "8",
      "title": "Investigating the use of pretrained convolutional neural network on cross-subject and cross-dataset eeg emotion recognition",
      "authors": [
        "Y Cimtay",
        "E Ekmekcioglu"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "9",
      "title": "Domain adaptation techniques for eeg-based emotion recognition: a comparative study on two public datasets",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "R Scherer",
        "G Müller-Putz"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "10",
      "title": "An adversarial discriminative temporal convolutional network for eeg-based cross-domain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "11",
      "title": "Eeg emotion recognition using fusion model of graph convolutional neural networks and lstm",
      "authors": [
        "Y Yin",
        "X Zheng",
        "B Hu",
        "Y Zhang",
        "X Cui"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "12",
      "title": "Differences first in asymmetric brain: A bi-hemisphere discrepancy convolutional neural network for eeg emotion recognition",
      "authors": [
        "D Huang",
        "S Chen",
        "C Liu",
        "L Zheng",
        "Z Tian",
        "D Jiang"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "13",
      "title": "Cross-day eeg-based emotion recognition using transfer component analysis",
      "authors": [
        "Z He",
        "N Zhuang",
        "G Bao",
        "Y Zeng",
        "B Yan"
      ],
      "year": "2022",
      "venue": "Electronics"
    },
    {
      "citation_id": "14",
      "title": "Constructing a personalized cross-day eeg-based emotionclassification model using transfer learning",
      "authors": [
        "Y.-P Lin"
      ],
      "year": "2019",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "15",
      "title": "Two-level domain adaptation neural network for eeg-based emotion recognition",
      "authors": [
        "G Bao",
        "N Zhuang",
        "L Tong",
        "B Yan",
        "J Shu",
        "L Wang",
        "Y Zeng",
        "Z Shen"
      ],
      "year": "2021",
      "venue": "Frontiers in Human Neuroscience"
    },
    {
      "citation_id": "16",
      "title": "Cross-subject emotion recognition using visibility graph and genetic algorithm-based convolution neural network",
      "authors": [
        "Q Cai",
        "J.-P An",
        "H.-Y Li",
        "J.-Y Guo",
        "Z.-K Gao"
      ],
      "year": "2022",
      "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science"
    },
    {
      "citation_id": "17",
      "title": "A deep subdomain associate adaptation network for cross-session and cross-subject eeg emotion recognition",
      "authors": [
        "M Meng",
        "J Hu",
        "Y Gao",
        "W Kong",
        "Z Luo"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "18",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "L.-M Zhao",
        "X Yan",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Wgan domain adaptation for eeg-based emotion recognition",
      "authors": [
        "Y Luo",
        "S.-Y Zhang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "20",
      "title": "Crosssubject eeg emotion recognition combined with connectivity features and meta-transfer learning",
      "authors": [
        "J Li",
        "H Hua",
        "Z Xu",
        "L Shu",
        "X Xu",
        "F Kuang",
        "S Wu"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "21",
      "title": "A deep multi-source adaptation transfer network for cross-subject electroencephalogram emotion recognition",
      "authors": [
        "F Wang",
        "W Zhang",
        "Z Xu",
        "J Ping",
        "H Chu"
      ],
      "year": "2021",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "22",
      "title": "Cross-subject eeg emotion recognition with self-organized graph neural network",
      "authors": [
        "J Li",
        "S Li",
        "J Pan",
        "F Wang"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "23",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on autonomous mental development"
    },
    {
      "citation_id": "24",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "25",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "26",
      "title": "A domain adaptation sparse representation classifier for cross-domain electroencephalogram-based emotion classification",
      "authors": [
        "T Ni",
        "Y Ni",
        "J Xue",
        "S Wang"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "27",
      "title": "Multi-source co-adaptation for eeg-based emotion recognition by mining correlation information",
      "authors": [
        "J Tao",
        "Y Dan"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "28",
      "title": "Subject independent emotion recognition system for people with facial deformity: an eeg based approach",
      "authors": [
        "P Pandey",
        "K Seeja"
      ],
      "year": "2021",
      "venue": "Journal of Ambient Intelligence and Humanized Computing"
    },
    {
      "citation_id": "29",
      "title": "Cross-subject and cross-device wearable eeg emotion recognition using frontal eeg under virtual reality scenes",
      "authors": [
        "F Kuang",
        "L Shu",
        "H Hua",
        "S Wu",
        "L Zhang",
        "X Xu",
        "Y Liu",
        "M Jiang"
      ],
      "year": "2021",
      "venue": "2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "30",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "31",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "W Liu",
        "J.-L Qiu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "32",
      "title": "Eeg emotion recognition based on graph regularized sparse linear regression",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "S Ge"
      ],
      "year": "2019",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "33",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "34",
      "title": "Automagic: Standardized preprocessing of big eeg data",
      "authors": [
        "A Pedroni",
        "A Bahreini",
        "N Langer"
      ],
      "year": "2019",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "35",
      "title": "Eog correction: which regression should we use?",
      "authors": [
        "R Croft",
        "R Barry"
      ],
      "year": "2000",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "36",
      "title": "Recipes for the linear analysis of eeg",
      "authors": [
        "L Parra",
        "C Spence",
        "A Gerson",
        "P Sajda"
      ],
      "year": "2005",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "37",
      "title": "Iclabel: An automated electroencephalographic independent component classifier, dataset, and website",
      "authors": [
        "L Pion-Tonachini",
        "K Kreutz-Delgado",
        "S Makeig"
      ],
      "year": "2019",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "38",
      "title": "Fractals and the analysis of waveforms",
      "authors": [
        "M Katz"
      ],
      "year": "1988",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "39",
      "title": "Application of fractal dimension for eeg based diagnosis of encephalopathy",
      "authors": [
        "J Jacob",
        "G Nair",
        "A Cherian",
        "T Iype"
      ],
      "year": "2019",
      "venue": "Analog Integrated Circuits and Signal Processing"
    },
    {
      "citation_id": "40",
      "title": "Visualizing data using t-sne",
      "authors": [
        "L Maaten",
        "G Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    },
    {
      "citation_id": "41",
      "title": "Personal-zscore: Eliminating individual difference for eeg-based cross-subject emotion recognition",
      "authors": [
        "H Chen",
        "S Sun",
        "J Li",
        "R Yu",
        "N Li",
        "X Li",
        "B Hu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Cross-subject and crossgender emotion classification from eeg",
      "authors": [
        "J.-Y Zhu",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "Medical Physics and Biomedical Engineering"
    },
    {
      "citation_id": "43",
      "title": "Behavior needs neural variability",
      "authors": [
        "L Waschke",
        "N Kloosterman",
        "J Obleser",
        "D Garrett"
      ],
      "year": "2021",
      "venue": "Neuron"
    },
    {
      "citation_id": "44",
      "title": "Hierarchical convolutional neural networks for eeg-based emotion recognition",
      "authors": [
        "J Li",
        "Z Zhang",
        "H He"
      ],
      "year": "2018",
      "venue": "Cognitive Computation"
    }
  ]
}