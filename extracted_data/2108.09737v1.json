{
  "paper_id": "2108.09737v1",
  "title": "A Transformer Architecture For Stress Detection From Ecg",
  "published": "2021-08-22T14:34:44Z",
  "authors": [
    "Behnam Behinaein",
    "Anubhav Bhatti",
    "Dirk Rodenburg",
    "Paul Hungler",
    "Ali Etemad"
  ],
  "keywords": [
    "Affective Computing",
    "Stress",
    "Transformers",
    "ECG",
    "Wearable"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electrocardiogram (ECG) has been widely used for emotion recognition. This paper presents a deep neural network based on convolutional layers and a transformer mechanism to detect stress using ECG signals. We perform leave-one-subject-out experiments on two publicly available datasets, WESAD and SWELL-KW, to evaluate our method. Our experiments show that the proposed model achieves strong results, comparable or better than the state-of-theart models for ECG-based stress detection on these two datasets. Moreover, our method is end-to-end, does not require handcrafted features, and can learn robust representations with only a few convolutional blocks and the transformer component.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective computing studies how machines can recognize, infer, process, and simulate human emotions  [16] , with applications in education, health care, video games, and others  [16] [17] [18] . The ubiquitous availability of consumer-grade wearable sensing devices that collect biological signals (e.g., ECG) and the availability of deep learning frameworks have facilitated affective computing technologies  [4, 11, 21] . Classical machine learning and feature engineering methods have been used to extract handcraft features and classify affect states  [1, 2, 6, 7, 21, 22] . Although handcrafted features perform well on emotion recognition, extracting them requires field expertise as such features are very application-specific. Convolutional layers have been employed to automate the feature extraction process  [9, 19, 20] . In addition, Transformer architectures  [23]  have recently emerged as a powerful solution and an alternative to recurrent neural networks for processing sequential data, and have been widely used in natural language processing  [5, 25]  and computer vision  [8, 10] .\n\nDue to the sequential nature of ECG time-series, transformers are viable candidates to learn spatio-temporal representations  [3, 24] . In this paper, we propose an architecture that uses ECG to detect  stress based on a combination of convolutional and transformer architectures. Our model uses only two convolutional blocks, which is considerably less compared to other works in the area  [14, 19] . We test our proposed model on two publicly available affective computing datasets, WESAD  [21]  and SWELL-KW  [13] , using leave-onesubject-out (LOSO) scheme. Initial results using LOSO demonstrate that a fine-tuning (calibration) step is required to yield competitive results versus prior work. We demonstrate that by fine-tuning the model on only 10% of user-specific data, strong results are achieved.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Method",
      "text": "We propose an end-to-end network comprising three subnetworks, a convolutional subnetwork, a transformer encoder, and a fully connected (FC) subnetwork. The model and architectural details are depicted in Figure  1 . The convolutional front-end subnetwork comprises two convolutional layers, each directly followed by a ReLU activation and a maxpooling layer. The convolutional layers are followed by a reshape layer to flatten the last dimension. The role of the convolutional subnetwork is to extract spatio-temporal features from raw input ECG signals and feed them to the encoder. Since using a multi-head component (which will come later) results in loss of ordering in the input sequence, a piece of information needs to be added to the embeddings to give the encoder some sense of order. Here, we use the positional encoder proposed by Vaswani et al.  [23]  and add its output to the embeddings obtained from the reshape layer before supplying them to the transformer encoder. Next, the encoder consists of a multi-head, self-attention layer, followed by a dropout and a layer normalization, then a fully connected feed-forward network, and finally a dropout and a layer normalization. We use scaled dot-product attention  [23]  for our model where a query, key, and value vectors are generated. These vectors are created for each input by multiplying the input by ùëä ùëû , ùëä ùëò , and ùëä ùë£ , which are learned weight matrices for query, key, and value, respectively. The queries, keys, and values are individually stacked to create ùëÑ, ùêæ, and ùëâ , respectively. The attention values are then computed based on  [23] , as ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùëÑ, ùêæ, ùëâ ) = ùë†ùëú ùëì ùë°ùëöùëéùë• (\n\nwhere ùëÑ ‚àà ùëÖ ùëõ√óùëë ùëû , ùêæ ‚àà ùëÖ ùëõ√óùëë ùëò , and ùëâ ‚àà ùëÖ ùëõ√óùë£ ùëò . Here, ùëõ is the length of the sequence, and ùëë ùëû , ùëë ùëò , and ùëë ùë£ are the embedding dimensions of ùëÑ, ùêæ, and ùëâ , respectively. The scaling factor ‚àöÔ∏Å ùëë ùëò is added to mitigate the softmax's small gradient when its argument is very large. We use four of these components in the transformer encoder. Embeddings generated by the transformer encoder are flattened and fed to three FC layers where each of the first two FC layers is followed by a Rectified Linear Unit (ReLU) activation function and the third one is followed by a sigmoid function for achieving binary classification. The network specifications are as follows: first convolution layer (filters = 64, kernel = 64, strides = 8), second convolution layer (filters = 128, kernel = 32, strides = 4), d ùëöùëúùëëùëíùëô = 1024, K, V, Q's dimension = 1024, d ùëì ùëì = 512, heads = 4, and finally the three FC layers' output dimensions = 512, 256, and 1, respectively. A dropout of 0.5 is used after each of the first two FC layers.\n\nWe implement the model in PyTorch on an NVIDIA TITAN RTX GPU. We use an Adam optimizer with an initial learning rate of 0.0001 and exponential decay of 0.985. We use the weighted binary cross-entropy loss to deal with the class imbalance. We train the model for 70 epochs with a batch size of 256. Due to significant intersubject variability, we use a small portion of each test subject's data for calibrating (fining-tuning) the model. In this step, each model is trained for 40 epochs and fine-tuned for 30 epochs.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Experiments And Result",
      "text": "Datasets. We use two publicly available datasets, Wearable Stress and Affect Detection (WESAD)  [21]  and SWELL knowledge work (SWELL-KW)  [13] , for evaluating our model. WESAD is a multimodal dataset collected from 15 subjects using wrist-worn and chest-worn wearable devices. The affect status of the subjects is also recorded in the dataset. We use the ECG data from the chest-worn device. In the SWELL-KW dataset, several modalities were recorded from 25 subjects. In this study, we only consider the ECG modality. Data Pre-processing. We apply a 5th-order Butterworth highpass filter with a cutoff frequency of 0.5 Hz on ECG similar to  [15] . ECG was originally sampled at 700 Hz and 2048 Hz in WESAD and SWELL-KW, respectively. We down-sample signals from both datasets to 256 Hz for our study. ECG signals are normalized using user-specific z-score normalization  [19] . In terms of the output classes, WESAD recorded three affective states, neutral, stress, and amusement. For binary classification (stress vs. non-stress), we merge the neutral and amusement states into a 'non-stress' state. For SWELL-KW, we use the 'neutral' as the non-stress state and 'time pressure' and 'interruptions' as stress state. Validation Schemes. To evaluate our model, we perform LOSO validation. We segment the data with a window size of 30 seconds and incremental steps of 1 second. In the fine-tuning step, we use 1%, 5%, and 10% of data to calibrate the model. Results. The results for stress detection on WESAD is presented in Table  1 . Our model obtains an accuracy of 80.4% and F 1 score of 69.7%, which are below the state-of-the-art results. By fine-tuning the model with only 10% of the test data, the performance is considerably boosted to an accuracy of 91.1%, outperforming other methods in Table  1 . For the SWELL-KW dataset, as can be seen from Table  1 , we achieve an accuracy of 58.1 and F 1 score of 58.8 which are comparable to  [12] . It should be noted, however, that  [12]  uses multi-modal data (facial, posture, computer interactions, ECG, and EDA) as apposed to our uni-modal approach. Similar to WESAD, we observe that fine-tuning on only 10% of data results in a considerable performance boost and an accuracy of 71.6% (see Table  1 ). Table  2  shows the performance when different percentages of test data are used for calibration. As can be seen, in WESAD, we need to use more than 1% of the data in the fine-tuning step to considerably improve the result. However, for SWELL-KW, calibrating with even 1% of data boosts the performance to outperform the baselines. While our approach yields promising results and is endto-end (does not require hand-crafted features), the results indicate that to generalize to unseen subjects better than the state-of-theart, it requires calibration with a small amount of data, which is considered a limitation of our work. Nonetheless, we believe our method demonstrates the potential for transformer architectures to be used in the area of affective computing.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Conclusion",
      "text": "We presented a model based on convolutional and transformer architectures for detecting stress versus non-stress using ECG signals.\n\nTo test our model, we used two publicly available datasets, WESAD and SWELL-KW. We showed that our model can achieve competitive results by using transformers with few convolutional layers. The results using LOSO validation showed that by fine-tuning the model with only a fraction of the test data (10%), the proposed model can outperform the baseline methods.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Transformer architecture for stress detection.",
      "page": 1
    },
    {
      "caption": "Figure 1: The convolutional front-end subnetwork",
      "page": 1
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "Queen‚Äôs University, Kingston, Canada"
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "ali.etemad@queensu.ca"
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "PosiÔøΩonal encoder"
        },
        {
          "paul.hungler@queensu.ca": "ABSTRACT",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "Electrocardiogram (ECG) has been widely used for emotion recog-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "nition. This paper presents a deep neural network based on convo-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "lutional layers and a transformer mechanism to detect stress using",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "ECG Seq\nConv\nReLU\nMaxPool\nMulÔøΩ-head\nAÔøΩenÔøΩon\nAdd & Norm\nFeed-fwd\nAdd & Norm\nLinear\nReLU\nLinear\nReLU\nLinear\nSigmoid\nStress\n+"
        },
        {
          "paul.hungler@queensu.ca": "ECG signals. We perform leave-one-subject-out experiments on",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "two publicly available datasets, WESAD and SWELL-KW, to evalu-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "ate our method. Our experiments show that the proposed model",
          "Ingenuity Labs Research Institute": "x 2\nx 4"
        },
        {
          "paul.hungler@queensu.ca": "achieves strong results, comparable or better than the state-of-the-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "art models for ECG-based stress detection on these two datasets.",
          "Ingenuity Labs Research Institute": "Figure 1: Transformer architecture for stress detection."
        },
        {
          "paul.hungler@queensu.ca": "Moreover, our method is end-to-end, does not require handcrafted",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "features, and can learn robust representations with only a few con-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "stress based on a combination of convolutional and transformer ar-"
        },
        {
          "paul.hungler@queensu.ca": "volutional blocks and the transformer component.",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "chitectures. Our model uses only two convolutional blocks, which is"
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "considerably less compared to other works in the area [14, 19]. We"
        },
        {
          "paul.hungler@queensu.ca": "KEYWORDS",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "test our proposed model on two publicly available affective comput-"
        },
        {
          "paul.hungler@queensu.ca": "Affective Computing, Stress, Transformers, ECG, Wearable",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "ing datasets, WESAD [21] and SWELL-KW [13], using leave-one-"
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "subject-out (LOSO) scheme. Initial results using LOSO demonstrate"
        },
        {
          "paul.hungler@queensu.ca": "1\nINTRODUCTION",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "that a fine-tuning (calibration) step is required to yield competitive"
        },
        {
          "paul.hungler@queensu.ca": "Affective computing studies how machines can recognize, infer,",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "results versus prior work. We demonstrate that by fine-tuning the"
        },
        {
          "paul.hungler@queensu.ca": "process, and simulate human emotions [16], with applications in",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "model on only 10% of user-specific data, strong results are achieved."
        },
        {
          "paul.hungler@queensu.ca": "education, health care, video games, and others [16‚Äì18]. The ubiq-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "uitous availability of consumer-grade wearable sensing devices that",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "2\nMETHOD"
        },
        {
          "paul.hungler@queensu.ca": "collect biological signals (e.g., ECG) and the availability of deep",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "We propose an end-to-end network comprising three subnetworks,"
        },
        {
          "paul.hungler@queensu.ca": "learning frameworks have facilitated affective computing technolo-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "a convolutional subnetwork, a transformer encoder, and a fully"
        },
        {
          "paul.hungler@queensu.ca": "gies [4, 11, 21]. Classical machine learning and feature engineering",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "connected (FC) subnetwork. The model and architectural details"
        },
        {
          "paul.hungler@queensu.ca": "methods have been used to extract handcraft features and classify",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "are depicted in Figure 1. The convolutional front-end subnetwork"
        },
        {
          "paul.hungler@queensu.ca": "affect states [1, 2, 6, 7, 21, 22]. Although handcrafted features per-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "comprises two convolutional\nlayers, each directly followed by a"
        },
        {
          "paul.hungler@queensu.ca": "form well on emotion recognition, extracting them requires field",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "ReLU activation and a maxpooling layer. The convolutional layers"
        },
        {
          "paul.hungler@queensu.ca": "expertise as such features are very application-specific. Convolu-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "are followed by a reshape layer to flatten the last dimension. The"
        },
        {
          "paul.hungler@queensu.ca": "tional layers have been employed to automate the feature extraction",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "role of the convolutional subnetwork is to extract spatio-temporal"
        },
        {
          "paul.hungler@queensu.ca": "process [9, 19, 20]. In addition, Transformer architectures [23] have",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "features from raw input ECG signals and feed them to the encoder."
        },
        {
          "paul.hungler@queensu.ca": "recently emerged as a powerful solution and an alternative to recur-",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "Since using a multi-head component (which will come later) results"
        },
        {
          "paul.hungler@queensu.ca": "rent neural networks for processing sequential data, and have been",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "in loss of ordering in the input sequence, a piece of information"
        },
        {
          "paul.hungler@queensu.ca": "widely used in natural language processing [5, 25] and computer",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "needs to be added to the embeddings to give the encoder some sense"
        },
        {
          "paul.hungler@queensu.ca": "vision [8, 10].",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "of order. Here, we use the positional encoder proposed by Vaswani"
        },
        {
          "paul.hungler@queensu.ca": "Due to the sequential nature of ECG time-series, transformers are",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "et al. [23] and add its output to the embeddings obtained from the re-"
        },
        {
          "paul.hungler@queensu.ca": "viable candidates to learn spatio-temporal representations [3, 24].",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "shape layer before supplying them to the transformer encoder. Next,"
        },
        {
          "paul.hungler@queensu.ca": "In this paper, we propose an architecture that uses ECG to detect",
          "Ingenuity Labs Research Institute": ""
        },
        {
          "paul.hungler@queensu.ca": "",
          "Ingenuity Labs Research Institute": "the encoder consists of a multi-head, self-attention layer, followed"
        },
        {
          "paul.hungler@queensu.ca": "https://doi.org/10.1145/3460421.3480427",
          "Ingenuity Labs Research Institute": "by a dropout and a layer normalization,\nthen a fully connected"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "Table 1: Classification Results. TF: Transformer, SVM: Sup-"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "port Vector Machine, LDA: Linear Discriminant Analysis,"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "QDA: Quadratic Discriminant Analysis, FT: Fine-Tuned."
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "Dataset\nRef\nMethod\nModality\nApproach\nAcc.\nF1"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "WESAD\n[2]\nQDA\nECG\nLOSO\n85.7\n-"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "[21]\nLDA\nECG\nLOSO\n85.4\n81.3"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "Ours\nTF\nECG\nLOSO\n80.4\n69.7"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "TF\nECG\nLOSO (FT)\nOurs\n91.1\n83.3"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "SWELL\n[12]\nSVM\nMulti\nLOSO\n58.9\n‚Äì"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "Ours\nTF\nECG\nLOSO\n58.1\n58.8"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "TF\nECG\nLOSO (FT)\nOurs\n71.6\n74.2"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "Table 2: Fine-tuning results. Values are in Acc (F1) format."
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "Dataset\nNo Tuning\n1%\n5%\n10%"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "WESAD\n80.4 (69.7)\n81.6 (69.8)\n89.9 (80.8)\n91.1 (83.3)"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "SWELL-KW\n58.1 (58.8)\n67.4 (69.7)\n68.3 (70.8)\n71.6 (74.2)"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": ""
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "and incremental steps of 1 second. In the fine-tuning step, we use"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "1%, 5%, and 10% of data to calibrate the model."
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "Results. The results for stress detection on WESAD is presented"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "in Table 1. Our model obtains an accuracy of 80.4% and F1 score of"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "69.7%, which are below the state-of-the-art results. By fine-tuning"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "the model with only 10% of the test data, the performance is con-"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "siderably boosted to an accuracy of 91.1%, outperforming other"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "methods in Table 1. For the SWELL-KW dataset, as can be seen"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "from Table 1, we achieve an accuracy of 58.1 and F1 score of 58.8"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "which are comparable to [12].\nIt should be noted, however, that"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "[12] uses multi-modal data (facial, posture, computer interactions,"
        },
        {
          "Behinaein, Bhatti, Rodenburg, Hungler, and Etemad": "ECG, and EDA) as apposed to our uni-modal approach. Similar to"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "A Transformer Architecture for Stress Detection from ECG": "REFERENCES"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[1] Dhananjai Bajpai and Lili He. 2020. Evaluating KNN Performance on WESAD"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Dataset. In 2020 12th International Conference on Computational Intelligence and"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Communication Networks (CICN). IEEE, 60‚Äì62."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[2]\nPatr√≠cia Bota, Chen Wang, Ana Fred, and Hugo Silva. 2020. Emotion Assessment"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Using Feature Fusion and Decision Fusion Classification Based on Physiological"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Data: Are We There Yet? Sensors 20, 17 (2020), 4723."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[3] Chao Che, Peiliang Zhang, Min Zhu, Yue Qu, and Bo Jin. 2021. Constrained"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Transformer Network for ECG Signal Processing and Arrhythmia Classification."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "BMC Medical Informatics and Decision Making 21 (2021)."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": ""
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[4]\nJuan Abdon Miranda Correa, Mojtaba Khomami Abadi, Niculae Sebe, and Ioannis"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Patras. 2018. Amigos: A Dataset for Affect, Personality and Mood Research on"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Individuals and Groups.\nIEEE Transactions on Affective Computing (2018)."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[5]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Pre-training of Deep Bidirectional Transformers for Language Understanding."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "arXiv preprint arXiv:1810.04805 (2018)."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[6] Hany Ferdinando, Tapio Sepp√§nen, and Esko Alasaarela. 2016. Comparing fea-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "tures from ECG pattern and HRV analysis for emotion recognition system. In"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "2016 IEEE Conference on Computational Intelligence in Bioinformatics and Compu-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "https://doi.org/10.1109/CIBCB.2016.7758108\ntational Biology (CIBCB). 1‚Äì6."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[7] Han-Wen Guo, Yu-Shun Huang, Chien-Hung Lin,\nJen-Chien Chien, Koichi"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Haraikawa, and Jiann-Shing Shieh. 2016. Heart rate variability signal features"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "for emotion recognition by using principal component analysis and support"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "vectors machine. In 2016 IEEE 16th International Conference on Bioinformatics and"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Bioengineering (BIBE). IEEE, 274‚Äì277."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[8] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Liu, Yehui Tang, An Xiao, Chunjing Xu, Yixing Xu, et al. 2020. A Survey on"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Visual Transformer. arXiv preprint arXiv:2012.12556 (2020)."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[9] Bosun Hwang,\nJiwoo You, Thomas Vaessen,\nInez Myin-Germeys, Cheolsoo"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Park, and Byoung-Tak Zhang. 2018.\nDeep ECGNet: An optimal deep learn-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "ing framework for monitoring mental stress using ultra short-term ECG signals."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "TELEMEDICINE and e-HEALTH 24, 10 (2018), 753‚Äì772."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[10]\nSalman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fa-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "had Shahbaz Khan, and Mubarak Shah. 2021. Transformers in Vision: A Survey."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "arXiv preprint arXiv:2101.01169 (2021)."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan\n[11]"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Yazdani, Touradj Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Patras. 2011."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Deap: A Database for Emotion Analysis; Using Physiological Signals.\nIEEE"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Transactions On Affective Computing 3, 1 (2011), 18‚Äì31."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[12]\nSaskia Koldijk, Mark A Neerincx, and Wessel Kraaij. 2016. Detecting Work Stress"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "in Offices by Combining Unobtrusive Sensors.\nIEEE Transactions on Affective"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Computing 9, 2 (2016), 227‚Äì239."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Saskia Koldijk, Maya Sappelli, Suzan Verberne, Mark A Neerincx, and Wessel\n[13]"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Kraaij. 2014. The Swell Knowledge Work Dataset for Stress and User Model-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "ing Research. In Proceedings of the 16th International Conference on Multimodal"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Interaction. 291‚Äì298."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[14]\nJionghao Lin, Shirui Pan, Cheng Siong Lee, and Sharon Oviatt. 2019. An Explain-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "able Deep Fusion Network for Affect Recognition using Physiological Signals. In"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Proceedings of the 28th ACM International Conference on Information and Knowl-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "edge Management. 2069‚Äì2072."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[15] Dominique Makowski, Tam Pham, Zen J. Lau,\nJan C. Brammer, Fran√ßois"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Lespinasse, Hung Pham, Christopher Sch√∂lzel, and S. H. Annabel Chen. 2021."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "NeuroKit2: A Python toolbox for neurophysiological signal processing. Behavior"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "https://doi.org/10.3758/s13428-020-01516-y\nResearch Methods (02 Feb 2021)."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[16] Rosalind W Picard. 2000. Affective computing. MIT press."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[17]\nSoujanya Poria, Erik Cambria, Rajiv Bajpai, and Amir Hussain. 2017. A Re-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "view Of Affective Computing: From Unimodal Analysis To Multimodal Fusion."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Information Fusion 37 (2017), 98‚Äì125."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[18] Kyle Ross, Pritam Sarkar, Dirk Rodenburg, Aaron Ruberto, Paul Hungler, Adam"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Szulewski, Daniel Howes, and Ali Etemad. 2019. Toward Dynamically Adaptive"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Simulation: Multimodal Classification of User Expertise using Wearable Devices."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Sensors 19, 19 (2019), 4270."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[19]\nPritam Sarkar and Ali Etemad. 2020. Self-supervised ecg representation learning"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "for emotion recognition.\nIEEE Transactions on Affective Computing (2020)."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[20]\nPritam Sarkar and Ali Etemad. 2020.\nSelf-supervised learning for ecg-based"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "emotion recognition. In 2020 IEEE International Conference on Acoustics, Speech"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "and Signal Processing (ICASSP). 3217‚Äì3221."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[21]\nPhilip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, and Kristof"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Van Laerhoven. 2018.\nIntroducing Wesad, A Multimodal Dataset for Wearable"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Stress and Affect Detection. In Proceedings of the 20th ACM International Confer-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "ence on Multimodal Interaction. 400‚Äì408."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[22]\nSenthil Sriramprakash, Vadana D Prasanna, and OV Ramana Murthy. 2017. Stress"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Detection in Working People. Procedia Computer Science 115 (2017), 359‚Äì366."
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "[23] Ashish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones,"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "Aidan N. Gomez, undefinedukasz Kaiser, and Illia Polosukhin. 2017. Attention is"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "All You Need. In Proceedings of the 31st International Conference on Neural Infor-"
        },
        {
          "A Transformer Architecture for Stress Detection from ECG": "mation Processing Systems (Long Beach, California, USA) (NIPS‚Äô17). 6000‚Äì6010."
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Evaluating KNN Performance on WESAD Dataset",
      "authors": [
        "Dhananjai Bajpai",
        "Lili He"
      ],
      "year": "2020",
      "venue": "2020 12th International Conference on Computational Intelligence and Communication Networks (CICN)"
    },
    {
      "citation_id": "2",
      "title": "Emotion Assessment Using Feature Fusion and Decision Fusion Classification Based on Physiological Data: Are We There Yet?",
      "authors": [
        "Patr√≠cia Bota",
        "Chen Wang",
        "Ana Fred",
        "Hugo Silva"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "3",
      "title": "Constrained Transformer Network for ECG Signal Processing and Arrhythmia Classification",
      "authors": [
        "Peiliang Chao Che",
        "Min Zhang",
        "Yue Zhu",
        "Bo Qu",
        "Jin"
      ],
      "year": "2021",
      "venue": "BMC Medical Informatics and Decision Making"
    },
    {
      "citation_id": "4",
      "title": "Amigos: A Dataset for Affect, Personality and Mood Research on Individuals and Groups",
      "authors": [
        "Juan Abdon",
        "Miranda Correa",
        "Mojtaba Khomami Abadi",
        "Niculae Sebe",
        "Ioannis Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "6",
      "title": "Comparing features from ECG pattern and HRV analysis for emotion recognition system",
      "authors": [
        "Hany Ferdinando",
        "Tapio Sepp√§nen",
        "Esko Alasaarela"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computational Intelligence in Bioinformatics and Computational Biology (CIBCB)",
      "doi": "10.1109/CIBCB.2016.7758108"
    },
    {
      "citation_id": "7",
      "title": "Heart rate variability signal features for emotion recognition by using principal component analysis and support vectors machine",
      "authors": [
        "Han-Wen Guo",
        "Yu-Shun Huang",
        "Chien-Hung Lin",
        "Jen-Chien Chien",
        "Koichi Haraikawa",
        "Jiann-Shing Shieh"
      ],
      "year": "2016",
      "venue": "Bioinformatics and Bioengineering"
    },
    {
      "citation_id": "8",
      "title": "A Survey on Visual Transformer",
      "authors": [
        "Kai Han",
        "Yunhe Wang",
        "Hanting Chen",
        "Xinghao Chen",
        "Jianyuan Guo",
        "Zhenhua Liu",
        "Yehui Tang",
        "An Xiao",
        "Chunjing Xu",
        "Yixing Xu"
      ],
      "year": "2020",
      "venue": "A Survey on Visual Transformer",
      "arxiv": "arXiv:2012.12556"
    },
    {
      "citation_id": "9",
      "title": "Deep ECGNet: An optimal deep learning framework for monitoring mental stress using ultra short-term ECG signals",
      "authors": [
        "Bosun Hwang",
        "Jiwoo You",
        "Thomas Vaessen",
        "Inez Myin-Germeys",
        "Cheolsoo Park",
        "Byoung-Tak Zhang"
      ],
      "year": "2018",
      "venue": "TELEMEDICINE and e-HEALTH"
    },
    {
      "citation_id": "10",
      "title": "Transformers in Vision: A Survey",
      "authors": [
        "Salman Khan",
        "Muzammal Naseer",
        "Munawar Hayat",
        "Fahad Syed Waqas Zamir",
        "Mubarak Shahbaz Khan",
        "Shah"
      ],
      "year": "2021",
      "venue": "Transformers in Vision: A Survey",
      "arxiv": "arXiv:2101.01169"
    },
    {
      "citation_id": "11",
      "title": "Deap: A Database for Emotion Analysis; Using Physiological Signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE Transactions On Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Detecting Work Stress in Offices by Combining Unobtrusive Sensors",
      "authors": [
        "Saskia Koldijk",
        "Mark Neerincx",
        "Wessel Kraaij"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "The Swell Knowledge Work Dataset for Stress and User Modeling Research",
      "authors": [
        "Saskia Koldijk",
        "Maya Sappelli",
        "Suzan Verberne",
        "Mark Neerincx",
        "Wessel Kraaij"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "14",
      "title": "An Explainable Deep Fusion Network for Affect Recognition using Physiological Signals",
      "authors": [
        "Jionghao Lin",
        "Shirui Pan",
        "Cheng Siong Lee",
        "Sharon Oviatt"
      ],
      "year": "2019",
      "venue": "Proceedings of the 28th ACM International Conference on Information and Knowledge Management"
    },
    {
      "citation_id": "15",
      "title": "NeuroKit2: A Python toolbox for neurophysiological signal processing",
      "authors": [
        "Dominique Makowski",
        "Tam Pham",
        "J Zen",
        "Jan Lau",
        "Fran√ßois Brammer",
        "Hung Lespinasse",
        "Christopher Pham",
        "S Sch√∂lzel",
        "Annabel Chen"
      ],
      "year": "2021",
      "venue": "Behavior Research Methods",
      "doi": "10.3758/s13428-020-01516-y"
    },
    {
      "citation_id": "16",
      "title": "Affective computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "17",
      "title": "A Review Of Affective Computing: From Unimodal Analysis To Multimodal Fusion",
      "authors": [
        "Soujanya Poria",
        "Erik Cambria",
        "Rajiv Bajpai",
        "Amir Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "18",
      "title": "Toward Dynamically Adaptive Simulation: Multimodal Classification of User Expertise using Wearable Devices",
      "authors": [
        "Kyle Ross",
        "Pritam Sarkar",
        "Dirk Rodenburg",
        "Aaron Ruberto",
        "Paul Hungler",
        "Adam Szulewski",
        "Daniel Howes",
        "Ali Etemad"
      ],
      "year": "2019",
      "venue": "Toward Dynamically Adaptive Simulation: Multimodal Classification of User Expertise using Wearable Devices"
    },
    {
      "citation_id": "19",
      "title": "Self-supervised ecg representation learning for emotion recognition",
      "authors": [
        "Pritam Sarkar",
        "Ali Etemad"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Self-supervised learning for ecg-based emotion recognition",
      "authors": [
        "Pritam Sarkar",
        "Ali Etemad"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Introducing Wesad, A Multimodal Dataset for Wearable Stress and Affect Detection",
      "authors": [
        "Philip Schmidt",
        "Attila Reiss",
        "Robert Duerichen",
        "Claus Marberger",
        "Kristof Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "22",
      "title": "Stress Detection in Working People",
      "authors": [
        "Senthil Sriramprakash",
        "Vadana D Prasanna",
        "Ov Ramana",
        "Murthy"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "23",
      "title": "Attention is All You Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Illia Kaiser",
        "Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Arrhythmia Classification with Heartbeat-Aware Transformer",
      "authors": [
        "Bin Wang",
        "Chang Liu",
        "Chuanyan Hu",
        "Xudong Liu"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Transformers: State-of-the-Art Natural Language Processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Remi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander Lhoest",
        "Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations"
    }
  ]
}