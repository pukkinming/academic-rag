{
  "paper_id": "2401.01495v1",
  "title": "A Two-Stage Multimodal Emotion Recognition Model Based On Graph Contrastive Learning",
  "published": "2024-01-03T01:58:31Z",
  "authors": [
    "Wei Ai",
    "FuChen Zhang",
    "Tao Meng",
    "YunTao Shou",
    "HongEn Shao",
    "Keqin Li"
  ],
  "keywords": [
    "graph contrastive learning",
    "graph convolutional network",
    "multimodal emotion recognition",
    "two-stage classification"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In terms of human-computer interaction, it is becoming more and more important to correctly understand the user's emotional state in a conversation, so the task of multimodal emotion recognition (MER) started to receive more attention. However, existing emotion classification methods usually perform classification only once. Sentences are likely to be misclassified in a single round of classification. Previous work usually ignores the similarities and differences between different morphological features in the fusion process. To address the above issues, we propose a two-stage emotion recognition model based on graph contrastive learning (TS-GCL). First, we encode the original dataset with different preprocessing modalities. Second, a graph contrastive learning (GCL) strategy is introduced for these three modal data with other structures to learn similarities and differences within and between modalities. Finally, we use MLP twice to achieve the final emotion classification. This staged classification method can help the model to better focus on different levels of emotional information, thereby improving the performance of the model. Extensive experiments show that TS-GCL has superior performance on IEMOCAP and MELD datasets compared with previous methods.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion is an indispensable element in human daily communication, and the goal of multimodal emotion recognition is to automatically identify and track the emotional state of speakers in a conversation  [7] . This task is receiving increasing attention in the fields of natural language processing (NLP) and multimodal processing. Multimodal emotion recognition has many potential applications, such as assisting dialogue analysis in legal trials and e-health services. Additionally, it is a critical component in creating more natural human-machine interactions, a specific example of which is shown in Fig.  1 .\n\nWith the rapid growth of conversational data on social media platforms, more and more researchers have begun to pay attention to multimodal emotion recognition. Multimodal emotion recognition analyzes complex emotional expressions more easily. By combining multiple modalities, the diversity and complexity of human emotions can be better captured.\n\nHowever, multimodal data such as video, audio, and text have differences in feature space distribution, which leads to the gap problem between different modalities in multimodal emotion recognition  [1] . To eliminate the differences between the features of each modality, the current mainstream multimodal feature fusion method is to directly map each modality into a shared feature space for representation. For example, the Tensor Fusion Network (TFN)  [8]  introduces tensor decomposition technology to decompose the fused multimodal feature representation into different weight matrices to capture the relationship between multimodal features, which helps to capture multimodal data interaction information better. Nevertheless, the above methods still suffer from indelible heterogeneity when mapping different modal features to a standard representation space. To address the above issues, we introduce a strategy of graph contrastive learning to eliminate this heterogeneity.\n\nIn addition, the current deep learning methods often perform one-time classification when dealing with emotion recognition tasks, and there is no secondary correction or reclassification mechanism, which directly reduces the model's performance. For example, Hu et al.  [9]  used Graph Convolution Network (GCN) to achieve multimodal data fusion and emotion recognition. Two-stage classification can decompose the task into two smaller subtasks, thereby reducing the complexity of the overall mission and making the model easier to train and optimize.\n\nTo address the above challenges, we propose a novel modal called a two-stage emotion recognition in conversations model based on graph contrastive learning (TS-GCL), which aims to simulate the emotion recognition process in human dialogue. Human beings can recognize the emotion of the sentence itself and modify or re-understand the feeling of a sentence according to the context. This process can be better simulated by using two classifications.\n\nThe main contributions of our study are summarized below:\n\n• We propose a novel two-stage multimodal emotion recognition model (TS-GCL) based on graph contrastive learning, which utilizes graph contrastive learning strategies to continuously update and correct the differences between samples, making the model anti-noise and antibias ability. At the same time, it effectively enhances the robustness of the model. In recent years, as an essential topic of natural language processing, MER has attracted more and more interest from researchers  [17] -  [25] . Unlike ordinary emotion recognition, MER needs to consider modeling the speaker's contextual information and the problem of semantic information fusion from multiple modalities. To this end, Poria et al.  [10]  proposed a video emotion recognition method that considers the context of video content. Specifically, they used LSTM for context modeling, and the context information was further used for emotion recognition. Hazarika et al.  [11]  proposed a Conversational Memory Network (CMM) to capture context dependencies in conversations. Although previous methods have achieved good performance on MER, most ignore the differences between different emotion categories, so we introduce a graph contrastive learning mechanism and a two-stage classification method, which can not only effectively model the emotion from three modal graph structure information. We can better learn the representation of graphs and compare graphs so that the features between graphs expressing the same emotion are more similar. The graph features of different emotions are more differentiated.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Contrastive Learning",
      "text": "Self-supervised learning (SL), as an essential part of deep learning, has received extensive attention in recent years. Contrastive representation learning (CRL) is a representative method in self-supervised learning. Its core idea is to learn the discriminative features for distinguishing samples by continuously reducing the distance between positive samples and expanding the space between positive samples.\n\nLi et al.  [12]  proposed introducing contrastive representation learning into the model, randomly sampling multiple slices on the feature sequence, maximizing the similarity between different slice representations of the same speech, and minimizing other Similarity between slice representations. Kim et al.  [13]  proposed a Contrastive Adversarial Learning (CAL) framework, which consists of a Contrastive Learning Module and an Adversarial Module, to learn representations that distinguish between different expressions. Compared with methods that directly use contrastive learning for expression recognition, contrastive adversarial learning improves the robustness of features. Although the model's performance is further improved after introducing contrastive learning in previous methods, how to model the dependencies and internal consistency among different modalities becomes a new challenge.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Preliminary Information",
      "text": "In this section, we detail the use of different preprocessing methods for other modalities, and at the same time, we define the MER task mathematically.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Feature Extraction",
      "text": "To verify the model's performance, this paper uses two multimodal emotion recognition general data sets, IEMOCAP and MELD. They use three data forms (text, audio, video) for storage. Aiming at these three different data storage forms, a targeted data preprocessing method is used for feature encoding to obtain high-quality semantic feature representations with rich semantic information. We will describe the encoding process of each mode in detail as follows.\n\n1) Text Feature Extraction: We will extract word-level text vector features from transcripts in the dataset. Specifically, influenced by previous work  [2] ,  [3] ,  [17] , we convert transcripts into text vector features using the RoBERTa  [4]  pretrained model, which is the most advanced sentence encoding model known to us and has shown its superiority in a variety of tasks. Properties, including question-answering systems, named entity recognition, and information retrieval. Using this model, we obtain excellent word embedding representations δ t .\n\n2) Audio Feature Extraction: The fluctuation of sound in the audio signal is not only the expression of sound but also reflects the ups and downs of the speaker's inner emotions. Sometimes, a person's behavior may not accurately reflect his emotional state, but his voice is an undisguised expression of real emotion. Inspired by previous research  [5] ,  [6] , we extracted audio features δ a using OpenSMILE.\n\n3) Vision Feature Extraction: We utilize 3D-CNN as a visual feature extractor. The model is a network architecture for visual representation learning. Recent work  [5] ,  [6]  has demonstrated its capability in various downstream tasks such as video analysis and medical image processing. Finally, we used 3D-CNN to extract 512-dimensional visual features δ v .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Problem Definition",
      "text": "Suppose I speakers and N sentences are in a multimodal dialogue. Then it can be expressed as two sets S and U , where S = {s 1 , s 2 , . . . , s I }, U = {u 1 , u 2 , . . . , u N }. The multimodal emotion recognition task needs to predict the emotional label of each sentence when the speaker speaks. We define a mapping function φ to represent the connection between each sentence and the speaker. For example, s φn represents the speaker corresponding to the n-th sentence. We will use the preprocessing method described above to obtain the representation features u i ∈ R d of the utterance. The representation features of each utterance contain data from three modalities, namely text, audio, and vision. In summary, it can be expressed as:\n\nIV. PROPOSED METHOD In order to enhance the performance of multimodal emotion recognition, we propose a new method called a two-stage multimodal emotion recognition model based on graph contrastive learning (TS-GCL). The overall architecture of TS-GCL is shown in Fig.  2 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Modality Encoder",
      "text": "Contextual information plays an important role in predicting emotion labels for each utterance. Therefore, converting contextual information into discourse feature expression has positive benefits. We use a bidirectional Long Short Term Memory (LSTM) for contextual information extraction on sentence sequences U = {u 1 , u 2 , . . . , u N } because it can effectively capture long-term dependencies, which is important for understanding long-distance associations in text or sequence data. The calculation process is as follows:\n\n← -\n\nwhere -→ h i and ←h i represent the cell states of two unidirectional lstm forward propagation and backward propagation respectively, h i ∈ R d represents the output of a bidirectional LSTM.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Speaker Embedding",
      "text": "Speaker information can provide clues about the speaker's social background, emotional tendencies, and attitudes. This helps to better understand the context of the text and thus predict emotion more accurately. And speaker information can enhance the modeling of dialogue context. The same piece of text may have different emotional interpretations between different speakers. Embedding speaker information helps to capture context information more accurately. Therefore, we decide to embed the original speaker's information s i into it before composing it. The speaker embedding λ i calculation process is as follows:\n\nwhere W s is the weight matrix and b λ i is the bias vector.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Prediction",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extraction",
      "text": "Graph Contrastive",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Negative Samples",
      "text": "Tell her to wear her own earrings……\n\nTell her to wear her own earrings……\n\nNegative Samples",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "C. Graph Construction",
      "text": "Inspired by previous work  [15] ,  [16] ,  [14] . we construct a directed graph G M = {V M , E M , T M } to represent a dialogue consisting of I utterances as follows, where M ∈ {t, a, v}. In this graph, the node set V(|V| = 3N ) corresponds to utterances in three different utterance modes, and the edge type set T includes the context, speaker, and utterance mode dependencies between utterances. The edge set E ⊂ V × V indicates that there is a dialog relationship between nodes. The specific way to construct the graph is as follows:\n\nNodes: Given a dialog, the number of nodes in the graph is positively related to the number of sentences in the dialog. This paper constructed a graph containing 3N nodes, where each statement contains three nodes\n\nand correspond to three modes.\n\nEdges: We assume that every utterance in the same dialogue is related to other utterances. Therefore, there will be a connection between any two nodes of the same modality in the same dialog. Furthermore, each node is also connected to nodes of the same utterance but from different modalities. For example, in the graph node v t i will be connected to v a i and v v i . Edge Weighting: We imagine that if there is a higher similarity between two nodes, then the information interaction between them will also be more important, which indicates that the edge weight between them should be larger. To capture the similarity between node expressions, we adopt angular similarity as a way to measure the edge weight of two nodes.\n\nThere are two types of edges in the graph, namely: 1) edges connecting nodes of the same modality, and 2) edges connecting nodes of different modalities. To differentiate these two cases, we employ different edge weighting strategies. For nodes of the same modality, the weights of their edges are calculated as follows:\n\nwhere v i and v j denote the feature representations of the i-th and j-th nodes in the graph. For nodes in different modalities, their edge weights are calculated as follows:\n\nwhere ω is a hyper parameter. According to the above steps, we further use GCN to encode contextual features.\n\nwhere P is the Laplacian matrix, D is the degree matrix, and I is the identity matrix. The iterative process of different layers can be represented by multiple layers of graph convolutional network (GCN) as follows:\n\nwhere κ and ϱ represent hyperparameters, φ represents the activation function, and W (l) is a learned weight matrix.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Gcl: Graph Contrastive Learning",
      "text": "GCL aims to make full use of the complementary information of different modalities by using contrastive learning, which can enhance the model's sensitivity to emotional features and improve emotion recognition accuracy by shortening the distance between positive samples and expanding the space between negative samples. For samples from the same modality, construct positive and negative pairs. Positive pairs include representations of the same modality for the same sample, and teams of negative samples have representations of the same modality for different samples. Using the Softmax function, the similarity between positive and negative examples is mapped to a specific range to distinguish intra-class and inter-class samples better, thereby providing a more accurate loss function for the training of comparative learning. The intra-class comparison loss and inter-class comparison loss are calculated as follows:\n\nwhere N p denotes the number of positive samples, N n denotes the number of negative samples, ρ M i is a positive sample in the same category, η M j is a negative sample in the same category, µ is the kernel function, the similarity between the two texts.\n\nThe three terms in the formula calculate the difference in distribution between positive samples, the difference in distribution between positive and negative samples, and the difference in distribution between negative samples. By minimizing these distribution differences, we can make the model pay more attention to the emotion's characteristics rather than the differences between datasets. This helps improve the generalization ability of emotion classifiers to perform well under different data distributions.\n\nComparable loss and other possible classification loss combinations, reaching the loss function, the calculation process is as follows:\n\nWhere ζ is the missing weight factor.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "E. Multi-Task Learning",
      "text": "After graph comparison learning processing, we get a vector χ M i after multi-modal fusion. The target category (label) for each emotion is denoted by y i ∈ {-1, 0, 1}, where (-1),  (1) , and (0) represent the neutral emotion, positive emotion, and negative emotion of the first classification in the secondary classification, respectively.\n\nThe multi-modal fusion feature vector χ M i and the corresponding target category y i will be used as the input of the multi-layer perceptron. By performing backpropagation training on samples, the multilayer perceptron will gradually learn appropriate weights and biases for better prediction in secondary classification tasks. The process is as shown in the formula:\n\nSo far, we got emotion labels ŷi for each sentence. V. EXPERIMENTS",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Implementation Details",
      "text": "In this section, we describe the implementation details of the model during training. Our experimental environment is the Windows 11 operating system, and the computer used is equipped with an Intel Core i7 13700k processor and an NVIDIA RTX 3090 graphics card. The construction of the deep learning algorithm adopts Python 3.8 and PyTorch 1.9.1 version.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Benchmark Dataset Used",
      "text": "In MER, two multimodal dialogue data sets, IEMOCAP and MELD, are usually used for comparative experiments. The following is an introduction to these two data sets:\n\nIEMOCAP: is an emotion database for studying emotional expressions and interactive behaviors. Dialogue in IEMOCAP covers a variety of emotional states, such as anger, happiness, sadness, neutral, etc., as well as different situations, such as face-to-face communication, telephone communication, etc. This allows researchers to explore the relationship between emotion, interaction, communication, etc. while examining multimodal expressions in a laboratory setting.\n\nMELD: is a multimodal dataset widely used in emotion recognition research to help researchers understand emotional expressions more comprehensively. The dialogues in the dataset come from movie clips containing text dialogue, audio, and video information. The conversations of the MELD dataset cover a variety of emotional states, such as anger, happiness, sadness, neutral, etc., as well as different situations and emotional intensities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "C. Baseline Models",
      "text": "In this section, we detail the baseline model compared with the model in this paper, which are the results achieved on two general datasets, which will be described in detail below:\n\ntext-CNN efficiently extracts critical text features through convolution pooling operation, and its end-to-end learning method also promotes the development of text classification tasks.\n\nMFN can give full play to multi-modal complementarity through multi-level fusion and end-to-end training, but its structure is complex, and training is difficult. A large amount of labeled data is required for supervised training.\n\nbc-LSTM: The bc-LSTM provides sufficient context information through the target word's forward and backward context vectors to help the model better understand the semantics of the target word, but the computational complexity is also high.\n\nCMN proposed a cross-modal contextual attention mechanism, which can learn the correlation between text and video features and perform adaptive multi-modal fusion. However, it cannot clearly distinguish the feature contribution of unimodality and multimodality, and the demand for labeled data is significant.\n\nDialogueRNN used a Conditional Random Field (CRF) in the context generator part, which can effectively model the context dependency of sentences. However, using CRF also increases the computational complexity of the model.\n\nDialogueGCN is a dialogue modeling method based on a graph convolutional network. It models dialogue as a sentence graph, and graph edges represent sentence relationships. GCN is used to learn the representation of sentence graphs. Therefore, it has achieved excellent performance on emotion",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Results And Discussion",
      "text": "We comprehensively compare our proposed emotion recognition algorithm TS-GCL with other deep learning algorithms. On the IEMOCAP and MELD datasets, Table  I  and Table  II  show the recognition accuracy and F1 value of each algorithm on each emotion category and the average accuracy and F1 value of the model as a whole. Experimental results significantly demonstrate the superior performance of our proposed algorithm.\n\nIEMOCAP: As shown in Table  I , TS-GCL has achieved excellent performance on the IEMOCAP dataset, taking the lead in four indicators, among which the accuracy rate and F1 are 70.3% and 70.2%, respectively. In addition, TS-GCL has achieved excellent performance in the happy category, and the accuracy and F1 of other categories are slightly lower than different existing algorithms. Analysis of the reasons shows that TS-GCL considers the similarities and differences between modalities and modalities during fusion and simulates human emotions for emotional classification by using graph comparison learning strategies and two emotions label classification methods for better performance.\n\nMELD: As shown in Table  II , TS-GCL has achieved excellent performance on the MELD dataset, leading in terms of indicators, among which the accuracy rate and F1 are 64.4% and 64.1%, respectively, compared with the existing comparison algorithms have achieved a small margin leading. In the four categories of surprise, fear, sadness, and Sadness, TS-GCL has achieved leading performance in accuracy and F1. Due to the severe category imbalance problem in the MELD dataset, TS-GCL generally performs on fear and disgust compared to existing algorithms, and our future work will also optimize this phenomenon.\n\nThe analysis of the aforementioned experimental results indicates that TS-GCL demonstrates superior performance, effectively capturing the similarities and differences among emotion samples. Based on these outcomes, further optimization can be pursued, along with the utilization of a secondary emotion classification approach. This approach avoids the shortcomings observed in previous methods for emotion classification.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "E. Ablation Studies",
      "text": "In this section, we choose three benchmark models, dissect the TS-GCL model step by step, present the experimental results of each part, and analyze their performance on the IEMOCAP dataset to gain insight into their impact on the overall performance. At the same time, three benchmark models are selected for comparison. We present the corresponding experimental results in Fig.  3 .\n\n• Observing and comparing the effects of GCL and TS-GCL, we can see the impact of the secondary classification method on TS-GCL. Although related to our choice of state-of-the-art feature extractors, secondary classification methods still have some effect. Compared with not using TS (Two-Stage GCN), the accuracy is improved by 1.4%. Secondary classification methods enable models to better adapt to specific domains or user emotions. • Observing and comparing the effects of TS and TS-GCL, we can see the influence of GCL on TS-GCL. Compared with not using GCL, the accuracy increased by 1.9%. Graph contrastive learning introduces richer contextual information and relationship modeling for multimodal emotion recognition, which can improve the feature learning ability of the model, thereby achieving better results in multimodal emotion recognition tasks.\n\nTS-GCL achieves the best performance when we use both TS and GCL. These partial combinations constitute our final model, and our plausible model is demonstrated experimentally.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this paper, we propose a two-stage multimodal emotion recognition model based on graph contrastive learning (TS-GCL), which achieves efficient cross-modal feature fusion and designs a novel contrastive learning strategy to reduce the distance between samples. Different emotion labels in modals enhance the semantic representation ability of nodes. Then, we propose a two-stage classification method for multi-task multi-modal emotion recognition. The two-stage classification method can facilitate the module design of the model, so that modules in different stages can be tuned and optimized independently.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An example of effective multimodal multi-emotion human-machine",
      "page": 1
    },
    {
      "caption": "Figure 1: With the rapid growth of conversational data on social",
      "page": 2
    },
    {
      "caption": "Figure 2: A. Modality Encoder",
      "page": 3
    },
    {
      "caption": "Figure 2: We propose the architecture of TS-GCL. It is mainly divided into three parts. The first part is feature extraction, using different preprocessing methods",
      "page": 4
    },
    {
      "caption": "Figure 3: Ablation experiments on the IEMOCAP dataset. We conduct experi-",
      "page": 7
    },
    {
      "caption": "Figure 3: • Observing and comparing the effects of GCL and TS-",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Methods": "",
          "IEMOCAP": "Happy\nSadness\nNeutral\nAngry\nExcitement\nFrustration\nAverage(w)"
        },
        {
          "Methods": "",
          "IEMOCAP": "Acc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1\nAcc. F1"
        },
        {
          "Methods": "bc-LSTM\nCMN\nICON\nMFN\nDialogueRNN\nA-DMN\nDialogueGCN\nCTnet\nLR-GCN\nGraphCFC\nTS-GCL",
          "IEMOCAP": "28.7 34.8\n57.7 60.3\n54.7 52.4\n56.6 57.3\n51.7 57.3\n67.7 59.3\n55.1 55.4\n25.7 30.1\n55.6 62.8\n53.2 52.7\n61.0 59.3\n55.0 60.3\n70.7 60.1\n56.3 56.1\n22.5 30.3\n59.1 64.9\n62.6 57.8\n65.1 63.4\n58.8 62.9\n67.1 60.7\n59.7 59.0\n23.7 34.6\n65.5 70.1\n55.2 52.1\n71.4 66.5\n63.8 62.1\n68.6 62.7\n60.0 59.7\n25.1 33.9\n74.8 78.3\n58.2 59.0\n65.1 65.6\n80.3 71.5\n61.0 58.8\n63.6 62.4\n43.0 50.2\n69.8 76.4\n63.0 62.9\n63.6 56.5\n87.8 77.4\n53.7 55.5\n64.9 64.2\n88.8 84.1\n40.7 42.9\n62.2 63.8\n67.2 64.0\n65.3 63.0\n64.2 66.9\n65.0 64.2\n69.3 65.5\n48.0 50.8\n78.0 79.7\n73.0 67.2\n85.6 78.7\n52.1 58.7\n68.2 67.5\n54.1 55.3\n81.9 78.8\n59.1 63.8\n69.7 69.0\n76.0 73.9\n68.3 68.6\n68.5 68.1\n43.5 54.1\n85.1 84.5\n64.3 62.0\n71.2 70.3\n78.7 73.8\n63.7 62.2\n68.9 68.4\n71.2 70.0\n70.3 70.2\n81.3 81.7\n67.4 64.2\n60.5 61.4\n74.6 76.5\n62.0 64.6"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Integrating multi-label contrastive learning with dual adversarial graph neural networks for crossmodal retrieval",
      "authors": [
        "S Qian",
        "D Xue",
        "Q Fang",
        "C Xu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "2",
      "title": "Conversational emotion recognition studies based on graph convolutional neural networks and a dependent syntactic analysis",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "S Yang",
        "K Li"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "3",
      "title": "A Lowrank Matching Attention based Cross-modal Feature Fusion Method for Conversational Emotion Recognition",
      "authors": [
        "Y Shou",
        "X Cao",
        "D Meng",
        "B Dong",
        "Q Zheng"
      ],
      "year": "2023",
      "venue": "A Lowrank Matching Attention based Cross-modal Feature Fusion Method for Conversational Emotion Recognition",
      "arxiv": "arXiv:2306.17799"
    },
    {
      "citation_id": "4",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "5",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "6",
      "title": "Emocaps: Emotion capsule based model for conversational emotion recognition",
      "authors": [
        "Z Li",
        "F Tang",
        "M Zhao",
        "Y Zhu"
      ],
      "year": "2022",
      "venue": "Findings of the Association for Computational Linguistics: ACL 2022"
    },
    {
      "citation_id": "7",
      "title": "Attentionemotion-enhanced convolutional lstm for sentiment analysis",
      "authors": [
        "F Huang",
        "X Li",
        "C Yuan",
        "S Zhang",
        "J Zhang",
        "S Qiao"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "8",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "9",
      "title": "Mmgcn: Multimodal fusion via deep graph convolution network for emotion recognition in conversation",
      "authors": [
        "J Hu",
        "Y Liu",
        "J Zhao",
        "Q Jin"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "10",
      "title": "Context-dependent sentiment analysis in user-generated videos",
      "authors": [
        "S Poria",
        "E Cambria",
        "D Hazarika",
        "N Majumder",
        "A Zadeh",
        "L Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "11",
      "title": "Conversational memory network for emotion recognition in dyadic dialogue videos",
      "authors": [
        "D Hazarika",
        "S Poria",
        "A Zadeh",
        "E Cambria",
        "L.-P Morency",
        "R Zimmermann"
      ],
      "year": "2018",
      "venue": "Proceedings of the Conference of the North American Chapter ofthe Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "12",
      "title": "Contrastive unsupervised learning for speech emotion recognition",
      "authors": [
        "M Li",
        "B Yang",
        "J Levy",
        "A Stolcke",
        "V Rozgic",
        "S Matsoukas",
        "C Papayiannis",
        "D Bone",
        "C Wang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "13",
      "title": "Contrastive adversarial learning for person independent facial emotion recognition",
      "authors": [
        "D Kim",
        "B Song"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "14",
      "title": "ReGNN: A redundancy-eliminated graph neural networks accelerator",
      "authors": [
        "C Chen",
        "K Li",
        "Y Li",
        "X Zou"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)"
    },
    {
      "citation_id": "15",
      "title": "Dygnn: Algorithm and architecture support of dynamic pruning for graph neural networks",
      "authors": [
        "C Chen",
        "K Li",
        "X Zou",
        "Y Li"
      ],
      "year": "2021",
      "venue": "2021 58th ACM/IEEE Design Automation Conference (DAC)"
    },
    {
      "citation_id": "16",
      "title": "Citywide traffic flow prediction based on multiple gated spatio-temporal convolutional neural networks",
      "authors": [
        "C Chen",
        "K Li",
        "S Teo",
        "X Zou",
        "K Li",
        "Z Zeng"
      ],
      "year": "2020",
      "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD)"
    },
    {
      "citation_id": "17",
      "title": "A multi-message passing framework based on heterogeneous graphs in conversational emotion recognition",
      "authors": [
        "T Meng",
        "Y Shou",
        "W Ai",
        "J Du",
        "H Liu",
        "K Li"
      ],
      "year": "2023",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "18",
      "title": "Object Detection in Medical Images Based on Hierarchical Transformer and Mask Mechanism",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "C Xie",
        "H Liu",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "19",
      "title": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "N Yin",
        "K Li"
      ],
      "year": "2023",
      "venue": "A comprehensive survey on multi-modal conversational emotion recognition with deep learning",
      "arxiv": "arXiv:2312.05735"
    },
    {
      "citation_id": "20",
      "title": "Prediction Model of Dow Jones Index Based on LSTM-Adaboost",
      "authors": [
        "R Ying",
        "Y Shou",
        "C Liu"
      ],
      "year": "2021",
      "venue": "2021 International Conference on Communications, Information System and Computer Engineering (CISCE)"
    },
    {
      "citation_id": "21",
      "title": "Graph information bottleneck for remote sensing segmentation",
      "authors": [
        "Y Shou",
        "W Ai",
        "T Meng"
      ],
      "year": "2023",
      "venue": "Graph information bottleneck for remote sensing segmentation",
      "arxiv": "arXiv:2312.02545"
    },
    {
      "citation_id": "22",
      "title": "CZL-CIAE: CLIP-driven Zeroshot Learning for Correcting Inverse Age Estimation",
      "authors": [
        "Y Shou",
        "Ai Meng"
      ],
      "year": "2023",
      "venue": "CZL-CIAE: CLIP-driven Zeroshot Learning for Correcting Inverse Age Estimation",
      "arxiv": "arXiv:2312.01758"
    },
    {
      "citation_id": "23",
      "title": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "authors": [
        "T Meng",
        "Y Shou"
      ],
      "year": "2023",
      "venue": "Deep imbalanced learning for multimodal emotion recognition in conversations",
      "arxiv": "arXiv:2312.06337"
    },
    {
      "citation_id": "24",
      "title": "DER-GCN: Dialogue and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Dialogue Emotion Recognition",
      "authors": [
        "W Ai",
        "Y Shou",
        "T Meng",
        "K Li"
      ],
      "year": "2023",
      "venue": "DER-GCN: Dialogue and Event Relation-Aware Graph Convolutional Neural Network for Multimodal Dialogue Emotion Recognition",
      "arxiv": "arXiv:2312.10579"
    },
    {
      "citation_id": "25",
      "title": "Adversarial Representation with Intra-Modal and Inter-Modal Graph Contrastive Learning for Multimodal Emotion Recognition",
      "authors": [
        "Y Shou",
        "T Meng",
        "W Ai",
        "K Li"
      ],
      "year": "2023",
      "venue": "Adversarial Representation with Intra-Modal and Inter-Modal Graph Contrastive Learning for Multimodal Emotion Recognition",
      "arxiv": "arXiv:2312.16778"
    }
  ]
}