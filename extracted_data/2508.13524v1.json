{
  "paper_id": "2508.13524v1",
  "title": "Eai Endorsed Transactions On Ai And Robotics",
  "published": "2025-08-19T05:33:10Z",
  "authors": [
    "Vamsi Krishna Mulukutla",
    "Sai Supriya Pavarala",
    "Srinivasa Raju Rudraraju",
    "Sridevi Bonthu"
  ],
  "keywords": [
    "Facial Emotion Detection",
    "VLMs",
    "Facial Expression Classification",
    "Phi-3.5",
    "CLIP"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Facial Emotion Recognition (FER) is crucial for applications such as human-computer interaction and mental health diagnostics. This study presents the first empirical comparison of open-source Vision-Language Models (VLMs), including Phi-3.5 Vision and CLIP, against traditional deep learning models-VGG19, ResNet-50, and EfficientNet-B0-on the challenging FER-2013 dataset, which contains 35,887 low-resolution, grayscale images across seven emotion classes. To address the mismatch between VLM training assumptions and the noisy nature of FER data, we introduce a novel pipeline that integrates GFPGAN-based image restoration with FER evaluation. Results show that traditional models, particularly EfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision (51.66%), highlighting the limitations of VLMs in low-quality visual tasks. In addition to performance evaluation using precision, recall, F1-score, and accuracy, we provide a detailed computational cost analysis covering preprocessing, training, inference, and evaluation phases, offering practical insights for deployment. This work underscores the need for adapting VLMs to noisy environments and provides a reproducible benchmark for future research in emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Facial Emotion Recognition (FER) has emerged as a critical area in artificial intelligence, with applications spanning human-computer interaction, behavioral analysis, and surveillance systems  [1] . Traditional deep learning models such as VGG19, ResNet-50, and EfficientNet-B0 have demonstrated strong performance in FER due to their ability to extract robust visual features  [2] . However, the widely used FER-2013 dataset presents unique challenges, including low resolution, class imbalance, and varying lighting conditions, which complicate model evaluation  [3] .\n\nWhile deep learning models have been successful under such conditions, recent advances in Vision-Language Models (VLMs)-such as Phi-3.5 Vision, LLaMA-3.2 Vision Instruct, and CLIP-have raised interest in their potential to enhance FER through multi-modal understanding and largescale pretraining  [4] . Despite their success in general vision tasks, VLMs remain underexplored for FER, particularly in noisy and low-resolution environments. Moreover, although face restoration techniques have been studied independently, there is limited research on combining them with VLM-based FER pipelines.\n\nThis study addresses that gap by introducing a novel pipeline that integrates GFPGAN-based image restoration with FER evaluation using open-source VLMs. We empirically compare their performance with traditional deep learning models on the FER-2013 dataset, revealing that the latter still outperform VLMs-likely due to VLMs' reliance on structured, high-quality data that struggles with real-world visual variability. In addition to performance benchmarking using accuracy, precision, recall, and F1-score, we also analyze the computational cost across preprocessing, training, and inference stages to assess real-world deployment viability.\n\nBy enhancing FER-2013 images through GFPGAN and evaluating multiple architectures, this work provides new insights into the limitations of current VLMs in FER tasks and offers a reproducible benchmark for future studies aiming to adapt large models to challenging vision applications. The key contributions of this work are as follows:\n\n• Systematically evaluated traditional deep learning models (VGG19, ResNet-50, EfficientNet-B0) and VLM-based vision models (Phi-3.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Literature Review",
      "text": "FER has been extensively studied using Convolutional Neural Networks (CNNs), which have achieved considerable success in classifying facial expressions from images  [8, 9] . For example, Jaiswal et al.  [10]  demonstrated that deep CNNs can achieve validation accuracies of 70.14% on FER-2013 and 98.65% on JAFFE, emphasizing the role of data quality in model performance. The foundational work of Krizhevsky and Hinton  [11]  on deep CNNs, although initially trained on CIFAR-10, laid the groundwork for modern FER models through innovations in feature extraction and visualization of learned filters.\n\nResearchers have also explored FER applications in other domains. Bartlett and Movellan  [12]  applied facial analysis for drowsiness detection in drivers, leveraging facial action units to monitor fatigue, which highlights the safety-critical potential of FER systems. To improve performance further, hybrid architectures have been proposed. Al-Shabi et al.  [13] , for instance, combined CNNs with SIFT features and applied extensive data augmentation and model aggregation techniques, achieving strong performance on FER-2013 and CK+ datasets.\n\nRecent works have focused on FER under real-world conditions such as occlusion, lighting variation, and low resolution. Image restoration techniques like GFPGAN  [7]  have been successfully employed to enhance low-quality facial images, thereby improving downstream classification accuracy. However, few studies have extended such preprocessing techniques to VLMs. While models like CLIP and Flamingo  [14]  have shown promise in general emotion recognition tasks, their performance degrades when applied to noisy, unstructured datasets. Other methods have explored adversarial learning and domain adaptation to improve generalization  [15, 16] . Despite these advancements, there remains a significant gap in evaluating VLMs under degraded image conditions typical of real-world FER datasets. Specifically, little research has investigated how restoration pipelines like GFPGAN can be combined with VLMs to improve robustness. This study addresses this gap by integrating GFPGAN-based preprocessing with state-of-the-art VLMs (e.g., Phi-3.5 Vision and CLIP) and benchmarking them against traditional deep learning models on the FER-2013 dataset. Our work provides new insights into the performance trade-offs between these model families under constrained visual conditions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Requirements",
      "text": "To conduct the experiments in this study, appropriate computational resources and software environments were essential. The hardware setup included systems with at least an Intel Core i5 processor or equivalent, although GPU acceleration was strongly recommended for training and inference tasks. For general experimentation, NVIDIA T4 GPUs were used via cloud-based platforms such as Google Colab. For more computationally intensive models and preprocessing operations (e.g., GFPGAN), higher-end GPUs such as NVIDIA A100 or V100 were utilized through Colab Pro.\n\nDeep learning tasks required a minimum of 12GB RAM, with 25GB or more preferred for optimal performance and stability during training. Cloud storage solutions, such as Google Drive, were used for efficient dataset and model management.\n\nThe software environment was based on Python (version 3.7 or higher), and included essential libraries such as Hugging Face Transformers (for pre-trained VLM integration and inference) and Pillow (PIL) for image processing. All model evaluations-including Phi-3.5 Vision and CLIP-were conducted using publicly available implementations hosted in open-source repositories. Google Colab provided the necessary compute infrastructure for executing and scaling these experiments.\n\nA summary of compute time, GPU usage, and memory demands across models and tasks is presented in Section 6.3, to support reproducibility and practical deployment planning.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Datasets",
      "text": "This study uses the FER-2013 dataset, a widely recognized benchmark for facial emotion recognition. It comprises 35,887 grayscale facial images, each with a resolution of 48 × 48 pixels. These images are categorized into seven emotion classes: angry, disgust, sad, happy, neutral, surprise, and fear  [23] . Collected from diverse sources, the dataset includes variations in facial expressions, poses, occlusions, and lighting conditions-factors that make it particularly challenging for automated emotion recognition. FER-2013 was chosen for its ability to simulate real-world scenarios through low-quality images, significant class imbalance, and noisy visual content. These characteristics provide a rigorous testbed for evaluating both traditional deep learning models (e.g., ResNet-50, EfficientNet-B0) and Vision-Language Models (e.g., Phi-3.5 Vision, CLIP). By assessing these models under the same challenging conditions, we aim to understand their respective capabilities in handling imprecise, low-resolution facial data.\n\nThe dataset is divided into training, validation, and test sets, ensuring a fair and standardized comparison across models. Due to the noisy and imbalanced nature of the dataset, we applied preprocessing techniques such as grayscale scaling, contrast enhancement, and noise filtering to improve feature extraction. Figure  1  shows representative sample images from the FER-2013 dataset.\n\nOverall, FER-2013 provides a comprehensive and practical benchmark for evaluating model robustness, particularly when adapting large-scale pretrained models like VLMs to specialized vision tasks such as FER in unconstrained environments.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "The methodology adopted for this study is designed to explore the impact of preprocessing techniques, particularly GFPGAN, on the performance of traditional deep learning and VLMs in FER using the FER-2013 dataset. The following subsections describe the stages involved in preprocessing, model selection, training, evaluation, and computational cost analysis.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Processing",
      "text": "The FER-2013 dataset comprises low-resolution grayscale facial images, many of which suffer from issues like blur, occlusions, poor contrast, and inconsistent lighting. These challenges impair the models' ability to extract meaningful features, reducing classification performance. To address these limitations, a robust preprocessing pipeline was established, involving both image enhancement and data filtering.\n\nThe cornerstone of the preprocessing pipeline is GFPGAN, a deep learning-based facial image restoration model. GFPGAN is designed to recover lost facial details, rectify distortions, and improve the overall clarity of facial images while maintaining identity preservation. Its architecture integrates a coarse-to-fine facial reconstruction network with a pre-trained face prior module, enabling effective restoration of degraded facial features. Figure  2  illustrates the complete methodology employed by GFPGAN.\n\nBy leveraging GFPGAN, the dataset's low-quality images were enhanced, allowing the models to detect finer facial details, thereby improving the learning process and emotion classification accuracy. Alongside enhancement, a data filtering step was introduced to remove images with severe distortions, occlusions (e.g., faces obscured by hair), or blank entries caused by data acquisition issues. Figure  3  demonstrates the transformation of low-resolution grayscale images to restored RGB outputs. A combination of automatic quality-checking tools and manual inspection was used to ensure only high-quality, relevant images remained.\n\nThis preprocessing phase significantly improved the quality and consistency of the dataset, thereby enhancing the robustness and accuracy of both deep learning and VLM models. The enriched dataset enabled the models to learn more representative features and make reliable emotion predictions under real-world conditions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Model Selection",
      "text": "To comprehensively evaluate the effectiveness of traditional deep learning models and Vision-Language Models in FER, five distinct models were selected: VGG19, ResNet-50, EfficientNet-B0, Phi-3.5 Vision, and CLIP. These models were chosen based on their architectural diversity, performance history, and capability to handle the unique challenges posed by the FER-2013 dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Selection",
      "text": "To comprehensively evaluate the effectiveness of traditional deep learning models and Vision-Language Models in FER, five distinct models were selected: VGG19, ResNet-50, EfficientNet-B0, Phi-3.5 Vision, and CLIP. These models were chosen based on their architectural diversity, performance history, and capability to handle the unique challenges posed by the FER-2013 dataset.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vgg19",
      "text": "VGG19 stands out as a deep CNN with a straightforward yet powerful design. It's built with 19 layers and leans heavily on compact 3×3 convolutional filters, which excel at picking up subtle image details  [26] . This setup has proven its worth in image classification, and it's particularly handy for facial emotion recognition, thanks to its knack for pulling out everything from basic edges and textures to complex facial features and expressions  [27] . Sure, it's a bit of a resource hog compared to sleeker modern models, but VGG19 holds its own as a reliable benchmark, especially when tackling the diverse facial expression shifts in the FER-2013 dataset with its robust feature extraction skills.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Resnet-50",
      "text": "ResNet-50 (a 50-Layered Residual Network)  [28]  is one of the commonly applied deep learning models that uses residual learning for overcoming the vanishing gradient problem of deep networks. ResNet-50 utilizes skip connections (shortcuts) to enable gradients to pass through more easily in backpropagation, hence enabling deep networks to learn more conveniently  [29] . ResNet-50 is well known for its ability to extract strong features and has also shown very high accuracy on other image classification problems, making it a strong contender for emotion recognition in faces. Due to its deep model structure, intricate features such as edges and textures and prominent features such as face structures and expressions of the FER 2013 dataset can be extracted.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Efficientnet-B0",
      "text": "EfficientNet-B0 belongs to the EfficientNet model family, which is computationally highly efficient and accurate with low computation  [30] . It proposes a compound scaling approach in which the model scales depth (layer wise), width (channels per layer), and resolution (image size) simultaneously for performance. EfficientNet-B0, in comparison to regular CNNs, provides higher accuracy with fewer parameters, which is beneficial in situations where computational resources are scarce. In FER 2013, EfficientNet-B0 was chosen since it can learn fine-grained features with low weight and speed.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Phi-3.5 Vision",
      "text": "A large multimodal model that can reason and comprehend images  [31] . Unlike normal CNNs, which are only concerned with pixel-level feature extraction, Phi-3.5 Vision uses pretrained vision knowledge from huge vision datasets such that it can recognize emotions more contextually. It is better than traditional models that utilize transfer learning and finetuned embeddings with noisy, low-quality images. This therefore makes it a powerful tool for FER 2013, especially where common deep models are not comfortable with ambiguous or unclear expressions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Clip (Contrastive Language-Image Pretraining)",
      "text": "CLIP is a refined vision-language model from OpenAI pretrained from natural language supervision rather than labeled data  [32] . It is trained on ample text-image pairs so that it can generalize across various vision-related tasks, including emotion recognition. In contrast to CNNs that are primarily based on labeled marks, CLIP can map textual explanations to images and thus is very flexible in detecting facial expressions even with limited training data. Its capacity to identify emotions from a general point of view offers richer insights compared to deep CNN-based models  [33] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Deep Learning Models",
      "text": "VGG19: VGG19 was trained for 60 epochs, taking advantage of its deep convolutional layers to extract layer-wise features from the images. The model employed ReLU activation function in the convolutional layers to bring non-linearity and enhance feature representation. Batch normalization was applied after every convolutional layer to promote stable training and quicker convergence. Dropout was added as a regularization method to avoid overfitting by randomly dropping out neurons throughout training. The weights of the model were initialized with He-uniform kernel initialization to better distribute the weights. The last classification layer utilized the softmax activation function to provide probability distributions over the seven emotion classes, and categorical cross-entropy was used as the loss function. The Adam optimizer was put to use to update the model's parameters efficiently and improve learning.\n\nResNet-50: ResNet-50 was trained for 60 epochs, utilizing its deep residual connections to strengthen feature learning while preventing vanishing gradient problems. The model employed ReLU activation in the hidden layers to inject nonlinearity, helping with improved representation learning. For stable training and quick convergence, batch normalization was implemented following convolutional layers. Dropout was also included as a regularizing method to avoid overfitting by randomly disengaging neurons throughout training. The weights of the model were initialized with He-uniform kernel initialization, which assists in effective weight allocation. The last classification layer utilized the softmax activation function to create probability distributions for the final emotion classes, while categorical cross-entropy was utilized as the loss function. The Adam optimizer was added to effectively update the model's parameters.\n\nEfficientNet-B0: EfficientNet-B0 was trained for 30 epochs, taking advantage of its compound scaling method to optimize depth and width for better computational efficiency. Like ResNet-50, it used ReLU activation, batch normalization, dropout, and He-uniform kernel initialization for amelioration of training performance. The last classification layer also used softmax activation, with categorical crossentropy for the loss function and Adam optimize to update the model's weights efficiently.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Vision-Based Language Models (Vlms)",
      "text": "Pre-trained vision-based language models like Phi-3.5 Vision and CLIP were also tested for facial emotion detection  [34] . These models, however, are already pre-trained on massive multimodal datasets and do not require further training or fine-tuning  [35] . Rather, they can be used directly to the dataset, with their zero-shot and few-shot learning ability being tested for comparison of performance with the deep learning models  [36] . Recent advancements in Large Language Models are becoming more trustworthy and responsible  [37] .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Phi-3.5 Vision Model:",
      "text": "The Microsoft's Phi-3.5 visioninstruct model, crafted by Microsoft, marks a leap forward in compact multimodal AI, blending text and image processing within a lean 4.2-billion-parameter framework. Unlike its siblings-Phi-3.5 mini-instruct and Phi-3.5 MoE-instructthese variant shines with a 128,000-token context window and excels in tasks like image comprehension, OCR, and multi-frame analysis. Built from scratch with synthetic data and curated public sources, it is fine-tuned through supervised learning and preference optimization for precision and safety.\n\nPerformance-wise, Phi-3.5-vision-instruct punches above its weight, scoring 57.0 overall in benchmarks-outpacing peers like LlaVA-Interleave-Qwen-7B (53.1) in forensic detection (92.4) and art style recognition (87.2)  [38] . Yet, it stumbles in abstract reasoning, with scores like 29.2 in functional correspondence lagging behind heavyweights like GPT-4o.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Clip:",
      "text": "CLIP is an OpenAI-developed multimodal artificial intelligence model that achieves vision-language parity. CLIP is trained on a vast dataset of text-image pairs so that it understands images and text and relates the two modalities. CLIP does this by learning a common latent space representation for images and text. This enables it to accomplish a multitude of tasks, from zero-shot image classification, object detection, and even text-based search for images, without task-specific training  [36] . From a parameter standpoint, CLIP's base model holds around 400 million parameters, which is proportionally small to newer versions like Phi-3.5 Vision, with billions of parameters. Though small, CLIP performs incredibly well, particularly in the likes of applications such as zero-shot classification, where it is able to categorize images by classes under text prompts without the need for specialized prior training. The performance of CLIP can differ depending upon the task, and it performs optimally at locations where image and text data have a high correspondence.\n\nCLIP has provided good performance  [39]  on the majority of benchmarks, especially for tasks involving matching an image with its matching text description. The most wonderful thing about CLIP is that it generalizes so well across domains, i.e., it can process an enormous variety of visual concepts without being trained for them in particular. But CLIP is not yet in the state of being able to solve abstract thinking or higher-order multi-step thinking problems compared to other models such as GPT-4 or Phi-3.5 Vision, which are solely designed for carrying out such processes. CLIP is nonetheless still a very formidable tool for a titanic scope of multimodal AI applications with respect to those restrictions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation",
      "text": "To measure the models' performance, we employed four main evaluation parameters: precision, recall, F1-score, and accuracy. These parameters provide a clear evaluation of an algorithm's performance, particularly with respect to tackling the class imbalance of the FER2013 dataset.\n\nAccuracy counts the total accuracy of an algorithm by determining the total number of correctly categorized classes to the total number of classes. Even though accuracy is a most basic measure, it may not show the actual performance of the model because a dataset may contain imbalances. Precision calculates the number of true positive samples to the total of samples predicted positive. It has application when there should be less number of false positives such that neutral or weak expressions are not mistakenly recognized as emotions such as anger or fear by the model. Recall is how well a model can classify every possible case of a given class. Higher recall essentially means that an algorithm is adequate at detecting most cases of a particular emotion, which is very much a necessity when there are fewer training samples for a particular emotion like \"disgust.\" F1-score is a balanced measurement between recall and precision, which is particularly helpful when class imbalances need to be handled. It takes both false negatives and false positives, providing a better complete understanding of the model performance.\n\nThe effectiveness of regular deep learning-based models, such as ResNet-50 and Efficientnet-B0, is compared against the capabilities of vision language models, such as Phi-3.5 Vision and CLIP, in determining facial expression recognition workloads through comparison of such metrics  [40, 41] .",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Computational Cost Overview",
      "text": "In order to assess the resource requirements of various models used in our experiments, we tracked the GPU time, batch sizes, number of epochs, and memory utilization during preprocessing, training, inference, and evaluation stages. This helps establish a compute-efficiency profile for each model and provides deployment-relevant insights.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Inference, Results And Discussion",
      "text": "We evaluated the performance of three deep learning architectures-VGG19, ResNet50, and EfficientNet-B0-on the FER2013 facial expression recognition dataset, focusing on their ability to classify input images into seven categories of emotion i.e., anger, fear, disgust, happiness, surprise, sad, and neutral. The models were trained and tested under consistent conditions, with VGG19 specifically trained for 60 epochs, as detailed in our methodology. The results from Table  1 , highlight clear variations in model's efficiency, measured through precision, recall, F1-Score, and accuracy.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vision Language Models",
      "text": "The figure  5  demonstrates the confusion matrix drawn using the inference of the Phi 3.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Resource Usage Comparison",
      "text": "In addition to evaluating model performance in terms of accuracy, precision, recall, and F1-score, we also benchmarked the computational cost associated with each stage of the pipeline-preprocessing (GFPGAN), model training/fine-tuning (for CNNs), and inference (for VLMs).\n\nThe summary presented in Table  3  shows the estimated compute units and execution time for each model and task. While EfficientNet-B0 demonstrated high accuracy with relatively low computational cost, Vision-Language Models like CLIP and Phi-3.5 Vision required more resources for inference and evaluation, even without training. GFPGAN preprocessing, though effective, also contributes significantly to the computational overhead. These findings are particularly important for practitioners considering FER deployment in edge or resourceconstrained environments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "This study presents a comprehensive evaluation of deep learning and vision-language models (VLMs) for facial emotion recognition (FER) using the FER-2013 dataset.\n\nAddressing challenges like low resolution, noise, and class imbalance, we applied GFPGAN-based image enhancement and data filtering to improve input quality. Traditional deep learning models, especially EfficientNet-B0 and ResNet-50, showed strong performance due to their ability to extract meaningful features from noisy images. In contrast, VLMs such as Phi-3.5 Vision and CLIP, evaluated in zero-shot mode, struggled with the dataset's variability, highlighting their dependence on high-quality, structured data. Evaluation metrics confirmed that while VLMs offer flexibility across tasks, deep learning models remain more reliable for specialized FER tasks. Our findings emphasize the need for improved adaptation strategies for VLMs and offer practical guidance on balancing performance with computational cost in realworld deployments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Future Direction",
      "text": "",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows representative sample images",
      "page": 3
    },
    {
      "caption": "Figure 1: Sample images from the FER-2013 dataset,",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates the complete",
      "page": 3
    },
    {
      "caption": "Figure 3: demonstrates the transformation of low-resolution grayscale",
      "page": 3
    },
    {
      "caption": "Figure 2: The Methodology of GFPGAN [7].",
      "page": 4
    },
    {
      "caption": "Figure 3: This figure demonstrates the low to high",
      "page": 4
    },
    {
      "caption": "Figure 4: The methodology of the CLIP architecture [14].",
      "page": 6
    },
    {
      "caption": "Figure 4: , CLIP employs distinct",
      "page": 6
    },
    {
      "caption": "Figure 5: demonstrates the confusion matrix drawn using",
      "page": 7
    },
    {
      "caption": "Figure 5: Confusion matrix of Phi-3.5 Vision inference.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Facial  Emotion  Recognition  (FER)  is  crucial  for  applications  such  as  human-computer  interaction  and  mental  health \ndiagnostics. This study presents the first empirical comparison of open-source Vision-Language Models (VLMs), including \nPhi-3.5  Vision  and  CLIP,  against  traditional  deep  learning  models—VGG19,  ResNet-50,  and  EfficientNet-B0—on  the \nchallenging FER-2013 dataset, which contains 35,887 low-resolution, grayscale images across seven emotion classes. To \naddress the mismatch between VLM training assumptions and the noisy nature of FER data, we introduce a novel pipeline \nthat integrates GFPGAN-based image restoration with FER evaluation. Results show that traditional models, particularly \nEfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision": "(51.66%), highlighting the limitations of VLMs in low-quality visual tasks. In addition to performance evaluation using \nprecision, recall, F1-score, and accuracy, we provide a detailed computational cost analysis covering preprocessing, training, \ninference, and evaluation phases, offering practical insights for deployment. This work underscores the need for adapting"
        },
        {
          "Facial  Emotion  Recognition  (FER)  is  crucial  for  applications  such  as  human-computer  interaction  and  mental  health \ndiagnostics. This study presents the first empirical comparison of open-source Vision-Language Models (VLMs), including \nPhi-3.5  Vision  and  CLIP,  against  traditional  deep  learning  models—VGG19,  ResNet-50,  and  EfficientNet-B0—on  the \nchallenging FER-2013 dataset, which contains 35,887 low-resolution, grayscale images across seven emotion classes. To \naddress the mismatch between VLM training assumptions and the noisy nature of FER data, we introduce a novel pipeline \nthat integrates GFPGAN-based image restoration with FER evaluation. Results show that traditional models, particularly \nEfficientNet-B0 (86.44%) and ResNet-50 (85.72%), significantly outperform VLMs like CLIP (64.07%) and Phi-3.5 Vision": "VLMs to noisy environments and provides a reproducible benchmark for future research in emotion recognition."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table 1: , highlight clear variations in model’s efficiency,",
      "data": [
        {
          "Model": "VGG19",
          "Accuracy": "60.16%",
          "Precision": "0.50",
          "Recall": "0.41",
          "F1-\nScore": "0.40"
        },
        {
          "Model": "ResNet-50",
          "Accuracy": "85.72%",
          "Precision": "0.59",
          "Recall": "0.45",
          "F1-\nScore": "0.44"
        },
        {
          "Model": "EfficientNet-\nB0",
          "Accuracy": "94.72%",
          "Precision": "0.93",
          "Recall": "0.91",
          "F1-\nScore": "0.90"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Performance of CLIP (BERT + ViT) on FER- The CLIP model performs at 64.07% accuracy with precision",
      "data": [
        {
          "Task": "GFPGAN Preprocessing",
          "GPU Type": "T4",
          "Batch \nSize": "–",
          "Epochs": "–",
          "Time \n(hrs)": "2",
          "Compute Units": "2.0 units"
        },
        {
          "Task": "Phi-3.5 Vision (Fine-tune)",
          "GPU Type": "A100",
          "Batch \nSize": "32",
          "Epochs": "30",
          "Time \n(hrs)": "–",
          "Compute Units": "20.0 units"
        },
        {
          "Task": "Phi-3.5 Vision (Inference)",
          "GPU Type": "T4",
          "Batch \nSize": "32",
          "Epochs": "–",
          "Time \n(hrs)": "1",
          "Compute Units": "1.0 unit"
        },
        {
          "Task": "CLIP ViT-B/32 (Fine-tune)",
          "GPU Type": "T4",
          "Batch \nSize": "64",
          "Epochs": "30",
          "Time \n(hrs)": "–",
          "Compute Units": "10.0 units"
        },
        {
          "Task": "CLIP (Inference)",
          "GPU Type": "T4",
          "Batch \nSize": "64",
          "Epochs": "–",
          "Time \n(hrs)": "0.5",
          "Compute Units": "0.5 units"
        },
        {
          "Task": "VGG19 (Train + Eval)",
          "GPU Type": "T4/V4-2-8",
          "Batch \nSize": "64",
          "Epochs": "60",
          "Time \n(hrs)": "7.5",
          "Compute Units": "7.5 units"
        },
        {
          "Task": "ResNet-50 (Train + Eval)",
          "GPU Type": "T4/V4-2-8",
          "Batch \nSize": "64",
          "Epochs": "60",
          "Time \n(hrs)": "8",
          "Compute Units": "8.0 units"
        },
        {
          "Task": "EfficientNet-B0",
          "GPU Type": "T4/V4-2-8",
          "Batch \nSize": "64",
          "Epochs": "30",
          "Time \n(hrs)": "6",
          "Compute Units": "6.0 units"
        },
        {
          "Task": "Model Evaluation (Precision/Recall)",
          "GPU Type": "CPU/T4",
          "Batch \nSize": "–",
          "Epochs": "–",
          "Time \n(hrs)": "2",
          "Compute Units": "2.0 units"
        },
        {
          "Task": "Confusion Matrices / Visuals",
          "GPU Type": "CPU",
          "Batch \nSize": "–",
          "Epochs": "–",
          "Time \n(hrs)": "1",
          "Compute Units": "1.0 unit"
        },
        {
          "Task": "Overheads / Mount / Logs",
          "GPU Type": "Mixed",
          "Batch \nSize": "–",
          "Epochs": "–",
          "Time \n(hrs)": "1",
          "Compute Units": "1.0 unit"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D Lee",
        "Y Zhou"
      ],
      "year": "2013",
      "venue": "InNeural information processing: 20th international conference, ICONIP 2013, daegu, korea"
    },
    {
      "citation_id": "2",
      "title": "Rethinking model scaling for convolutional neural networks. InInternational conference on machine learning",
      "authors": [
        "M Tan",
        "Le Efficientnet"
      ],
      "year": "2019",
      "venue": "Rethinking model scaling for convolutional neural networks. InInternational conference on machine learning"
    },
    {
      "citation_id": "3",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar",
        "A Rodriguez",
        "Llama"
      ],
      "year": "2023",
      "venue": "Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "5",
      "title": "Scaling vision transformers",
      "authors": [
        "X Zhai",
        "A Kolesnikov",
        "N Houlsby",
        "L Beyer"
      ],
      "year": "2022",
      "venue": "InProceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "6",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2020",
      "venue": "Deep facial expression recognition: A survey"
    },
    {
      "citation_id": "7",
      "title": "Towards real-world blind face restoration with generative facial prior",
      "authors": [
        "X Wang",
        "Y Li",
        "H Zhang",
        "Y Shan"
      ],
      "venue": "InProceedings of the IEEE/CVF conference on computer vision and pattern recognition 2021"
    },
    {
      "citation_id": "8",
      "title": "A comprehensive review of deep learning: Architectures, recent advances, and applications. Information",
      "authors": [
        "I Mienye",
        "T Swart"
      ],
      "year": "2024",
      "venue": "A comprehensive review of deep learning: Architectures, recent advances, and applications. Information"
    },
    {
      "citation_id": "9",
      "title": "A survey of deep learning: Platforms, applications and emerging research trends. IEEE access",
      "authors": [
        "W Hatcher",
        "W Yu"
      ],
      "year": "2018",
      "venue": "A survey of deep learning: Platforms, applications and emerging research trends. IEEE access"
    },
    {
      "citation_id": "10",
      "title": "Facial emotion detection using deep learning",
      "authors": [
        "A Jaiswal",
        "A Raju"
      ],
      "year": "2020",
      "venue": "2020 international conference for emerging technology (INCET)"
    },
    {
      "citation_id": "11",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "12",
      "title": "ImageNet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2017",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "13",
      "title": "Facial expression recognition using a hybrid CNN-SIFT aggregator. InInternational workshop on multi-disciplinary trends in artificial intelligence",
      "authors": [
        "T Connie",
        "M Al-Shabi",
        "W Cheah",
        "M Goh"
      ],
      "year": "2017",
      "venue": "Facial expression recognition using a hybrid CNN-SIFT aggregator. InInternational workshop on multi-disciplinary trends in artificial intelligence"
    },
    {
      "citation_id": "14",
      "title": "Phi-3 technical report: A highly capable language model locally on your phone",
      "authors": [
        "M Abdin",
        "J Aneja",
        "H Awadalla",
        "A Awadallah",
        "A Awan",
        "N Bach",
        "A Bahree",
        "A Bakhtiari",
        "J Bao",
        "H Behl",
        "A Benhaim"
      ],
      "year": "2024",
      "venue": "Phi-3 technical report: A highly capable language model locally on your phone",
      "arxiv": "arXiv:2404.14219"
    },
    {
      "citation_id": "15",
      "title": "Occlusion-Robust Facial Expression Recognition Based on Multi-Angle Feature Extraction",
      "authors": [
        "Y Li",
        "H Liu",
        "J Liang",
        "D Jiang"
      ],
      "year": "2025",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "16",
      "title": "De Albuquerque VH, Reboucas Filho PP. Performance analysis of google colaboratory as a tool for accelerating deep learning applications",
      "authors": [
        "S Karamizadeh",
        "S Chaeikar",
        "M Najafabadi",
        "T Carneiro",
        "Da Nóbrega",
        "R Nepomuceno",
        "T Bian"
      ],
      "year": "2018",
      "venue": "Ieee Access"
    },
    {
      "citation_id": "17",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "18",
      "title": "Pretraining of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2019",
      "venue": "Pretraining of deep bidirectional transformers for language understanding"
    },
    {
      "citation_id": "19",
      "title": "Framework for automation of short answer grading based on domain-specific pretraining. Engineering Applications of Artificial Intelligence",
      "authors": [
        "S Bonthu",
        "S Sree",
        "M Prasad"
      ],
      "year": "2024",
      "venue": "Framework for automation of short answer grading based on domain-specific pretraining. Engineering Applications of Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Gpt-4 technical report",
      "authors": [
        "J Achiam",
        "S Adler",
        "S Agarwal",
        "L Ahmad",
        "I Akkaya",
        "F Aleman",
        "D Almeida",
        "J Altenschmidt",
        "S Altman",
        "S Anadkat",
        "R Avila"
      ],
      "year": "2023",
      "venue": "Gpt-4 technical report",
      "arxiv": "arXiv:2303.08774"
    },
    {
      "citation_id": "21",
      "title": "Learning transferable visual models from natural language supervision. InInternational conference on machine learning",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "Ramesh Goh",
        "G Agarwal",
        "S Sastry",
        "G Askell",
        "A Mishkin",
        "P Clark",
        "J Krueger"
      ],
      "year": "2021",
      "venue": "Learning transferable visual models from natural language supervision. InInternational conference on machine learning"
    },
    {
      "citation_id": "22",
      "title": "Sentiment Analysis of Twitter Data on",
      "authors": [
        "V Mulukutla",
        "S Pavarala",
        "V Kareti",
        "S Midatani",
        "S Bonthu"
      ],
      "year": "2023",
      "venue": "The Agnipath Yojana'. InInternational Conference on Multi-disciplinary Trends in Artificial Intelligence"
    },
    {
      "citation_id": "23",
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "authors": [
        "X Glorot",
        "Y Bengio"
      ],
      "year": "2010",
      "venue": "InProceedings of the thirteenth international conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "24",
      "title": "End-to-end object detection with transformers. InEuropean conference on computer vision",
      "authors": [
        "N Carion",
        "F Massa",
        "G Synnaeve",
        "N Usunier",
        "A Kirillov",
        "S Zagoruyko"
      ],
      "year": "2020",
      "venue": "End-to-end object detection with transformers. InEuropean conference on computer vision"
    },
    {
      "citation_id": "25",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "26",
      "title": "Towards context-rich automated biodiversity assessments: deriving AI-powered insights from camera trap data",
      "authors": [
        "P Fergus",
        "C Chalmers",
        "N Matthews",
        "S Nixon",
        "A Burger",
        "O Hartley",
        "C Sutherland",
        "X Lambin",
        "S Longmore",
        "S Wich"
      ],
      "year": "2024",
      "venue": "Sensors"
    },
    {
      "citation_id": "27",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "InProceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "28",
      "title": "Deep learning made easier by linear transformations in perceptrons. InArtificial intelligence and statistics",
      "authors": [
        "T Raiko",
        "H Valpola",
        "Y Lecun"
      ],
      "year": "2012",
      "venue": "Deep learning made easier by linear transformations in perceptrons. InArtificial intelligence and statistics"
    },
    {
      "citation_id": "29",
      "title": "Rethinking model scaling for convolutional neural networks. InInternational conference on machine learning",
      "authors": [
        "M Tan",
        "Le Efficientnet"
      ],
      "year": "2019",
      "venue": "Rethinking model scaling for convolutional neural networks. InInternational conference on machine learning"
    },
    {
      "citation_id": "30",
      "title": "Scaling (down) clip: A comprehensive analysis of data, architecture, and training strategies",
      "authors": [
        "Z Li",
        "C Xie",
        "E Cubuk"
      ],
      "year": "2024",
      "venue": "Scaling (down) clip: A comprehensive analysis of data, architecture, and training strategies",
      "arxiv": "arXiv:2404.08197"
    },
    {
      "citation_id": "31",
      "title": "Image classification using convolutional neural networks",
      "authors": [
        "M Ramprasath",
        "M Anand",
        "S Hariharan"
      ],
      "year": "2018",
      "venue": "International Journal of Pure and Applied Mathematics"
    },
    {
      "citation_id": "32",
      "title": "Large language model is also an open-ended decoder for vision-centric tasks",
      "authors": [
        "W Wang",
        "Z Chen",
        "X Chen",
        "J Wu",
        "X Zhu",
        "G Zeng",
        "P Luo",
        "T Lu",
        "J Zhou",
        "Y Qiao",
        "J Dai",
        "Visionllm"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "33",
      "title": "Towards vision enhancing llms: Empowering multimodal knowledge storage and sharing in llms",
      "authors": [
        "Y Li",
        "B Hu",
        "W Wang",
        "X Cao",
        "M Zhang"
      ],
      "year": "2023",
      "venue": "Towards vision enhancing llms: Empowering multimodal knowledge storage and sharing in llms",
      "arxiv": "arXiv:2311.15759"
    },
    {
      "citation_id": "34",
      "title": "Lit: Zero-shot transfer with lockedimage text tuning",
      "authors": [
        "X Zhai",
        "X Wang",
        "B Mustafa",
        "A Steiner",
        "D Keysers",
        "A Kolesnikov",
        "L Beyer"
      ],
      "year": "2022",
      "venue": "InProceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "35",
      "title": "LLM potentiality and awareness: a position paper from the perspective of trustworthy and responsible AI modeling",
      "authors": [
        "I Sarker"
      ],
      "year": "2024",
      "venue": "Discover Artificial Intelligence"
    },
    {
      "citation_id": "36",
      "title": "A review on large language models: Architectures, applications, taxonomies, open issues and challenges. IEEE access",
      "authors": [
        "M Raiaan",
        "M Mukta",
        "K Fatema",
        "N Fahad",
        "S Sakib",
        "M Mim",
        "J Ahmad",
        "M Ali",
        "S Azam"
      ],
      "year": "2024",
      "venue": "A review on large language models: Architectures, applications, taxonomies, open issues and challenges. IEEE access"
    },
    {
      "citation_id": "37",
      "title": "Object detection via a multiregion and semantic segmentation-aware cnn model",
      "authors": [
        "S Gidaris",
        "N Komodakis"
      ],
      "year": "2015",
      "venue": "Object detection via a multiregion and semantic segmentation-aware cnn model"
    },
    {
      "citation_id": "38",
      "title": "Multiclass spoken language identification for Indian Languages using deep learning",
      "authors": [
        "L Arla",
        "S Bonthu",
        "A Dayal"
      ],
      "year": "2020",
      "venue": "2020 IEEE Bombay Section Signature Conference (IBSSC)"
    },
    {
      "citation_id": "39",
      "title": "Comparative analysis of classification methods to predict diabetes mellitus on noisy data. InMachine Learning, Image Processing",
      "authors": [
        "U Jyothi",
        "M Dabbiru",
        "S Bonthu",
        "A Dayal",
        "N Kandula"
      ],
      "year": "2021",
      "venue": "Network Security and Data Sciences: Select Proceedings of 3rd International Conference on MIND"
    },
    {
      "citation_id": "40",
      "title": "Facial emotion recognition: State of the art performance on FER2013",
      "authors": [
        "Y Khaireddin",
        "Z Chen"
      ],
      "year": "2021",
      "venue": "Facial emotion recognition: State of the art performance on FER2013",
      "arxiv": "arXiv:2105.03588"
    },
    {
      "citation_id": "41",
      "title": "Measuring robustness to natural distribution shifts in image classification",
      "authors": [
        "R Taori",
        "Dave Shankar",
        "V Carlini",
        "N Recht",
        "B Schmidt"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}