{
  "paper_id": "2412.19909v1",
  "title": "Mouth Articulation-Based Anchoring For Improved Cross-Corpus Speech Emotion Recognition",
  "published": "2024-12-27T20:00:45Z",
  "authors": [
    "Shreya G. Upadhyay",
    "Ali N. Salman",
    "Carlos Busso",
    "Chi-Chun Lee"
  ],
  "keywords": [
    "speech emotion recognition",
    "articulatory gestures",
    "cross-corpus",
    "transfer learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Cross-corpus speech emotion recognition (SER) plays a vital role in numerous practical applications. Traditional approaches to cross-corpus emotion transfer often concentrate on adapting acoustic features to align with different corpora, domains, or labels. However, acoustic features are inherently variable and error-prone due to factors like speaker differences, domain shifts, and recording conditions. To address these challenges, this study adopts a novel contrastive approach by focusing on emotion-specific articulatory gestures as the core elements for analysis. By shifting the emphasis on the more stable and consistent articulatory gestures, we aim to enhance emotion transfer learning in SER tasks. Our research leverages the CREMA-D and MSP-IMPROV corpora as benchmarks and it reveals valuable insights into the commonality and reliability of these articulatory gestures. The findings highlight mouth articulatory gesture potential as a better constraint for improving emotion recognition across different settings or domains.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Speech emotion recognition (SER) systems are essential for improved user experiences across various applications, including automated call centers, education, entertainment, and medical fields  [1] -  [4] . In cross-corpus SER, aligning corpora from different domains and settings poses a significant challenge  [5] . Existing research has introduced several techniques to address domain, label, and feature discrepancies, such as transfer learning, semi-supervised learning, and few-shot or zero-shot learning to improve model generalization  [6] -  [8] . Numerous approaches like optimizing distance metrics  [9] , adversarial training  [10] , GANs to generate synthetic data  [11] , and phonetic-based feature alignments  [12]  have also been explored. These SER methods mostly focused on handling acoustic feature mismatch between corpora, given their strong correlation with emotion and ease of recording.\n\nIn our previous work  [13] , we introduced a phonemeanchoring approach to enhance cross-corpus alignment in SER. This method focused on identifying stable sub-units by leveraging shared vowel-phoneme emotion-specific acoustic spaces to match acoustic distributions across corpora. By establishing stable phoneme-based anchors, we hypothesized that similar phonemes would yield similar acoustic features. Unlike many previous approaches that attempted to directly match acoustic feature distributions, our stable phonemeanchoring method led to improved SER performance in crosscorpus settings. However, a critical question remains: Are acoustic features the most stable anchors for cross-corpus alignment in SER tasks? While acoustic features play a key role in conveying emotion, they are also vulnerable to noise, microphone quality, and recording environment variations, which can undermine SER accuracy in cross-corpus scenarios. We believe that our previous idea can achieve further improvement by incorporating more stable anchoring units.\n\nAcoustic signals and articulatory features are intrinsically linked, offering complementary insights into emotion recognition  [14] -  [16] . Significant research has explored mapping between these modalities, such as converting acoustic to articulatory and vice versa  [15] . Emotions are closely tied to articulatory movements, particularly in facial expressions, where the mouth region is crucial due to its role in speech production  [17] . Unlike broader facial features, mouth gesture are more stable because of their limited physical range, making them valuable for emotion recognition tasks  [18] ,  [19] . Focusing on the more stable aspect of speech production (such as articulatory gesture) can offer a promising alternative. In this study, we adopt the definition of articulatory gestures (AG) as the coordinated actions of speech organs (in this case, the mouth) that produce distinct phonemes. This study aims to improve cross-corpus alignment using stable AG properties, hypothesizing that stable mouth articulation patterns should result in similar acoustic characteristics.\n\nMouth articulation data is available through methods like electromagnetic articulography (EMA)  [20]  and real-time magnetic resonance imaging (MRI)  [21] ,  [22] . However, these methods are challenging to record and have limited corpora. Inspired by past research using marker information to analyze AG  [18] ,  [23] , this work focuses on using mouth landmarks extracted from the visual modality as a representation of AG. This work introduces the concept of incorporating constraints on AG into transfer learning to improve emotion recognition accuracy across corpora. We evaluate our approach using two multimodal datasets, CREMA-D  [24]  and MSP-IMPROV  [25] . Our proposed cross-modal anchoring idea, articulatory gesture-anchored cross-corpus SER (AG-CC), shows improved performance compared to the considered baseline.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Articulatory Gesture Analysis",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Multi-Modal Affective Corpora",
      "text": "The CREMA-D  [24]  (CREMA) is a publicly available resource for emotion recognition research. It includes approximately 7.5 hours of recordings from 91 actors, each performing seven categorical emotions, and primary attributes across 12 scripts. With around 7,440 utterances averaging 3 to 4 seconds each, the corpus provides a rich source of multimodal emotional expressions through both audio and video recordings.\n\nThe MSP-IMPROV  [25]  (IMPROV) corpus includes approximately 8.5 hours of recordings, consisting of 8,438 prompted and spontaneous emotional sentences, with each utterance averaging about 4 seconds in length. This corpus is specifically designed for emotion recognition tasks and provides both audio and video recordings.\n\nWe select these corpora for their diverse emotional expressions and multimodal data. IMPROV provides naturalistic, conversational emotions with varied intensity, while CREMA offers more controlled, scripted emotional expressions that are often more intense. In this study, we only focus on four major emotions: Neutral, Anger, Happiness, and Sadness. The phoneme information for both the corpora is obtained using the Montreal Forced Aligner (MFA)  [26] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "B. Ag Feature Extraction And Prepossessing",
      "text": "Our goal in this step is to extract robust features that enable the comparison of the mouth region across different subjects and corpora. To achieve this, we first use OpenFace  [27]  to detect the face bounding box and identify 68 2D landmark points that capture consistent facial features (e.g., eyes, chin, lips). We then rotate the landmarks to align the distance between the eyes parallel to the x-axis and normalize by the inter-pupil distance, reducing speaker-specific variations and ensuring consistent results from the same speaker across different sessions  [28] ,  [29] . This work examines mouth region AG by analyzing twelve key landmarks (48 to 59) that define the outer mouth shape. Building on insights from our previous research  [13] , we focus on six vowel phonemes: {A, @, E, i, ae, u}. Phoneme-specific AG segments are extracted by segmenting frames based on phonetic boundaries.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Articulatory-Gesture Clustering",
      "text": "Mouth gestures are continuous and dynamic, making hard segmentation difficult without clear boundaries. Clustering provides a more effective approach for capturing distinct AG patterns. First, we segment the long landmark sequences into smaller, contextually relevant segments using phonetic boundaries. We then apply time-series k-means clustering  [30]  with Soft-DTW (Dynamic Time Warping) as the distance metric, grouping mouth shapes with similar articulatory patterns despite timing variations. Validation samples from all four emotion categories across both corpora are used to train the AG cluster model. Testing different k values (5 to 30), optimal cluster number is found to be 10 using the elbow method. To evaluate the clustering model, we analyze how AG pattern variations for the same vowel are captured across different clusters. Fig.  1  presents examples using the vowels /A/ and /i/. For this analysis, we focus on key points: the corner-left (48th landmark) and corner-right (54th landmark) for x-coordinate analysis and the lower mid-mouth point (57th landmark) for y-coordinate analysis. Fig.  1a  and Fig.  1b  show y-coordinate curves which is aligned with expected mouth movements. For /A/, the downward AG curve in Fig.  1a  reflects the lower mid-point moving down, while the upward trend in Fig.  1b  for /i/ shows the lower mid-point rising, both consistent with expected articulation. For the x-coordinates, examining the corner points in Fig.  1c  and Fig.  1d  aligns with expected behavior, as the x-coordinates increase during the pronunciation of /i/ in their respective direction. The plots shown in Fig.  1  reveal distinct AG patterns across two clusters but show similar AG patterns from different corpora within the same cluster. This behavior is consistent across all the plots.\n\n1) Cross-Corpus AG Cluster Overlap Analysis: In this analysis, we aim to identify overlap in AG clusters between two corpora by analyzing vowel-specific AG samples. We process these samples through an AG cluster model to determine their cluster IDs and then calculate the percentage of samples that are similarly clustered across the corpus using Equation  1 . For example, if Corpus 1 has clusters 1 and 2, and Corpus 2 includes clusters 1, 2, and 3, we focus on the common clusters for similarity evaluation. To account for potential discrepancies and ensure accuracy, we use a 25% threshold to exclude very less common clusters, averaging similarity measures only for clusters exceeding this threshold. Table  I  shows the crosscorpus AG cluster Overlap analyses results. where, K c,t represents the number of common clusters between the two corpora after thresholding. N 1,i and N 2,i denote the number of samples in cluster i for the source and target, respectively, with N 1 and N 2 being the total number of samples in the source and target, respectively.\n\nTable  I  shows clustering results in two sections: clusterspecific overlap for each vowel and average overlap across all clusters for emotion-specific analysis. Higher values indicate greater AG cluster similarity across corpora for the given vowels. From Table  I , we observe substantial overlap in certain clusters, suggesting there exist shared articulatory patterns, which is promising for cross-corpus analyses. For instance, the vowel /A/ is clustered into five groups (1, 2, 3, 7, 8), with most showing high overlap, indicating strong consistency in gestures for this vowel. We can observe this pattern in other vowels as well.\n\nTable  I  reveals varying levels of emotion-specific overlap between corpora for different vowel phonemes. For example, the vowel /A/ has a high similarity score of 80.1% for Happiness, indicating a significant overlap in AG patterns across corpora in this emotional context. Likewise, /i/ shows higher similarity scores for Happiness (78.2%) and Anger (75.6%), suggesting consistent articulatory patterns for this vowel across datasets in these emotional states. These results are consistent with previous studies  [13] ,  [18] .\n\n2) AG-Acoustic Features Association Analysis: To assess how well samples grouped by AG clusters align with their corresponding acoustic features, we model an acoustic cluster system similar to the AG clustering approach described in Section II-C, using time-series K-means with 10 clusters for consistency and comparability. This gives us two clustering models: one based on AG and the other on acoustic features. First, we cluster the samples using the AG cluster model. Then, for each AG cluster, we extract the acoustic features of the grouped samples and passed them through the acoustic cluster model. The goal is to assess how many acoustic clusters form within each AG cluster to evaluate the association between AG-based and acoustic clustering.\n\nWe visualize the relationship between AG clusters and acoustic clusters using a heatmap, with rows representing AG clusters and columns representing acoustic clusters. Each cell shows how samples from an AG cluster are distributed across acoustic clusters. Given that the AG and acoustic models are trained separately, a diagonal pattern is not expected. However, if samples from an AG cluster mostly align with fewer acoustic clusters, it indicates a stronger association between the AG and acoustic features. Fig.  2  illustrates that acoustic embeddings often cluster in less number of clusters within each AG cluster, reflecting that there is a correlation between AG gestures and their acoustic counterparts. However, no consistent pattern is observed across acoustic clusters. For example, AG cluster 1 aligns mostly with acoustic cluster 9 in Happiness but with clusters 2 in Anger. This suggests a one-to-one mapping from AG clusters to acoustic clusters, where each type of mouth articulation corresponds to a specific set of acoustic features. In contrast, the mapping from acoustic clusters to AG clusters is more one-to-many, indicating that a single type of acoustic feature can arise from various mouth articulations.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Ag-Anchored Cross-Corpus Ser",
      "text": "To improve the cross-corpus emotion transfer task, we propose an AG clusters-based cross-modal anchoring method for the 4-category SER. Here, we implement a constraint on the common AG clusters over both corpora to align their acoustic space features, called AG-anchored loss (L AG ) shown in Equation  2 . For each target sample, we form triplets using:\n\n(1) Anchor: the acoustic embedding of the target sample from the common AG clusters, (2) Positive: an embedding from the same AG cluster and emotion category but from the source dataset, and (3) Negative: an embedding from a different AG cluster within the same emotion category from the source corpus. We adjust the distance between the anchor and negative samples using a weight factor w i,n , based on cluster centroid distances, and compute the soft triplet loss accordingly as shown in Equation  2 .\n\nwhere z i , z p , and z n represent the anchor, positive, and negative sets, respectively. w i,n is soft weight, estimate using the Equation  3 . α is a margin parameter that enforces a minimum separation between the positive and negative pairs. Here the value of α is set to a constant value of 0.3.\n\nwhere d(C ki , C kn ) is the distance between the centroids of the clusters. β is a scaling parameter that controls the influence of cluster distances. Here the β value is set to 0.2. This loss function, detailed in Equation  2 , ensures that acoustic embeddings from the common AG cluster are closely aligned, while embeddings from different clusters are separated. By incorporating this loss into cross-corpus SER model training, we enhance the alignment of acoustic embeddings based on AG, improving cross-corpus SER. The total loss for each training batch combines the SER loss with the AGanchored loss, as shown in Equation  4 .\n\nwhere L ER is the conventional cross-entropy loss for 4category emotion recognition and the L AG is the soft weighted AG-anchored loss.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiment Results",
      "text": "In our experiments, we benchmark using the CREMA and IMPROV corpora. We utilize Wav2vec2.0  [31]  embeddings as pretrained features and apply a transformer with a 4-layer fully connected architecture, similar to our previous work  [13] , Back-propagated using the loss function in Equation  4 , with soft-weighted AG anchoring. The model is optimized with Adam, using a learning rate of 0.0001 and a decay factor of 0.001, and trained for up to 70 epochs with a batch size of 64 with early stopping. The performance is evaluated using the unweighted average recall (UAR) metric.\n\nIn our study, we evaluate the effectiveness of the AG-CC method by comparing it with two baseline models: phonemeanchored (PA-CC)  [13]  and layer-anchored (LA-CC)  [32] . The PA-CC model leverages vowel phonemes as references to align emotional acoustic features in cross-corpora tasks, while the LA-CC approach aligns model layers to maintain consistent emotional patterns across corpora. In contrast, our AG-CC approach aligns these acoustic features through AG clusters to enhance cross-corpus SER.\n\nThe cross-corpus SER performance of all considered models is presented in Table  II . As evident from the results, our   II .\n\nTo further validate AG-CC, we compare it against the hardsegmented AG anchoring method (Hard-AG), as outline in Table  II . In this comparison, Hard-AG refers to the use of fixed AG phonetic segments as anchors, consistent with our previous work  [13]  where we anchored on acoustic hard segments. The results, with 54.87% for C→I and 52.90% for I→C in the 4-CAT task, indicate that AG-CC's continuous clustering approach significantly outperforms the Hard-AG method. This is likely due to the continuous nature of AG, which lacks distinct boundaries, making clustering a better fit for capturing their continuous patterns.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "We introduce the mouth articulation-based anchoring (AG-CC) approach to improve cross-corpus SER by aligning acoustic features across corpora through stable articulatory gesture (AG). By focusing on AG, which is more stable than acoustic features, we aim to enhance the generalization of SER systems across different domain corpora. The AG-CC method leverages stable AG anchors for cross-modal alignment, offering a robust foundation for emotion transfer. Our model AG-CC shows the better UAR with 57.37% for the 4-category cross-corpus SER task. Future work will concentrate on applying this concept in cross-lingual settings, improving AG-clustering strategies to better handle speaker variability, extending AG-CC evaluation to additional emotional categories, and optimizing performance across diverse acoustic environments.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Clustered AG patterns for /A/ and /i/ from both corpora come from two",
      "page": 2
    },
    {
      "caption": "Figure 1: presents examples using the vowels",
      "page": 2
    },
    {
      "caption": "Figure 1: a and Fig. 1b show",
      "page": 2
    },
    {
      "caption": "Figure 1: a reflects",
      "page": 2
    },
    {
      "caption": "Figure 1: b for /i/ shows the lower mid-point rising, both",
      "page": 2
    },
    {
      "caption": "Figure 1: c and Fig. 1d aligns",
      "page": 2
    },
    {
      "caption": "Figure 1: reveal distinct AG patterns across two clusters",
      "page": 2
    },
    {
      "caption": "Figure 2: Visualization of association between AG cluster and acoustic cluster",
      "page": 3
    },
    {
      "caption": "Figure 2: illustrates that acoustic embeddings",
      "page": 3
    },
    {
      "caption": "Figure 3: Proposed mouth articulation-based anchoring architecture for cross-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Upper Bound": "PA-CC [13]",
          "C→C\nI→I": "C→I\nI→C",
          "66.36\n62.10": "55.33\n53.18",
          "89.44\n88.27\n85.35\n79.51\n87.84\n85.33\n83.68\n75.05": "75.35\n73.33\n74.82\n66.98\n75.46\n72.14\n73.64\n63.35"
        },
        {
          "Upper Bound": "LA-CC [32]",
          "C→C\nI→I": "C→I\nI→C",
          "66.36\n62.10": "56.04\n52.95",
          "89.44\n88.27\n85.35\n79.51\n87.84\n85.33\n83.68\n75.05": "78.24\n75.49\n73.50\n66.73\n77.67\n67.73\n76.52\n74.52"
        },
        {
          "Upper Bound": "AG-CC",
          "C→C\nI→I": "C→I\nI→C",
          "66.36\n62.10": "57.37\n53.83",
          "89.44\n88.27\n85.35\n79.51\n87.84\n85.33\n83.68\n75.05": "78.51\n75.03\n76.74\n68.10\n77.72\n75.30\n77.46\n65.85"
        },
        {
          "Upper Bound": "Hard-AG",
          "C→C\nI→I": "C→I\nI→C",
          "66.36\n62.10": "54.87\n52.90",
          "89.44\n88.27\n85.35\n79.51\n87.84\n85.33\n83.68\n75.05": "72.35\n74.46\n71.90\n69.41\n70.68\n71.53\n70.24\n63.24"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Behavioral signal processing: Deriving human behavioral informatics from speech and language",
      "authors": [
        "S Narayanan",
        "P Georgiou"
      ],
      "year": "2013",
      "venue": "Proceedings of the IEEE"
    },
    {
      "citation_id": "2",
      "title": "Using emotion to gain rapport in a spoken dialog system",
      "authors": [
        "J Acosta"
      ],
      "year": "2009",
      "venue": "Using emotion to gain rapport in a spoken dialog system"
    },
    {
      "citation_id": "3",
      "title": "Speech based emotion classification framework for driver assistance system",
      "authors": [
        "A Tawari",
        "M Trivedi"
      ],
      "year": "2010",
      "venue": "2010 IEEE Intelligent Vehicles Symposium"
    },
    {
      "citation_id": "4",
      "title": "Real-life emotion-related states detection in call centers: a cross-corpora study",
      "authors": [
        "L Devillers",
        "C Vaudable",
        "C Chastagnol"
      ],
      "year": "2010",
      "venue": "Eleventh Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "5",
      "title": "Deep cross-corpus speech emotion recognition: Recent advances and perspectives",
      "authors": [
        "S Zhang",
        "R Liu",
        "X Tao",
        "X Zhao"
      ],
      "year": "2021",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "6",
      "title": "Semi-supervised speech emotion recognition with ladder networks",
      "authors": [
        "S Parthasarathy",
        "C Busso"
      ],
      "year": "2020",
      "venue": "IEEE/ACM transactions on audio, speech, and language processing"
    },
    {
      "citation_id": "7",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "8",
      "title": "Zero-shot speech emotion recognition using generative learning with reconstructed prototypes",
      "authors": [
        "X Xu",
        "J Deng",
        "Z Zhang",
        "Z Yang",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Improving cross-corpus speech emotion recognition with adversarial discriminative domain generalization (ADDoG)",
      "authors": [
        "J Gideon",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "11",
      "title": "A conditional cycle emotion gan for cross corpus speech emotion recognition",
      "authors": [
        "B.-H Su",
        "C.-C Lee"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "12",
      "title": "An intelligent infrastructure toward large scale naturalistic affective speech corpora collection",
      "authors": [
        "S Upadhyay",
        "W.-S Chien",
        "B.-H Su",
        "L Goncalves",
        "Y.-T Wu",
        "A Salman",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "2023 10th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "13",
      "title": "Phonetic anchor-based transfer learning to facilitate unsupervised cross-lingual speech emotion recognition",
      "authors": [
        "S Upadhyay",
        "L Martinez-Lucas",
        "B.-H Su",
        "W.-C Lin",
        "W.-S Chien",
        "Y.-T Wu",
        "W Katz",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "14",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition based on bi-directional acoustic-articulatory conversion",
      "authors": [
        "H Li",
        "X Zhang",
        "S Duan",
        "H Liang"
      ],
      "year": "2024",
      "venue": "Speech emotion recognition based on bi-directional acoustic-articulatory conversion"
    },
    {
      "citation_id": "16",
      "title": "A study of correlation between physiological process of articulation and emotions on mandarin chinese",
      "authors": [
        "Z Zhang",
        "M Huang",
        "Z Xiao"
      ],
      "year": "2023",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "17",
      "title": "Say cheese vs. smile: Reducing speechrelated variability for facial emotion recognition",
      "authors": [
        "Y Kim",
        "E Provost"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia"
    },
    {
      "citation_id": "18",
      "title": "Expressive speech-driven lip movements with multitask learning",
      "authors": [
        "N Sadoughi",
        "C Busso"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE international conference on automatic face & gesture recognition"
    },
    {
      "citation_id": "19",
      "title": "Articulation constrained learning with application to speech emotion recognition",
      "authors": [
        "M Shah",
        "M Tu",
        "V Berisha",
        "C Chakrabarti",
        "A Spanias"
      ],
      "year": "2019",
      "venue": "EURASIP journal on audio, speech, and music processing"
    },
    {
      "citation_id": "20",
      "title": "Articulation, acoustics and perception of mandarin chinese emotional speech",
      "authors": [
        "D Erickson",
        "C Zhu",
        "S Kawahara",
        "A Suemitsu"
      ],
      "year": "2016",
      "venue": "Open Linguistics"
    },
    {
      "citation_id": "21",
      "title": "Vocal tract shaping of emotional speech",
      "authors": [
        "J Kim",
        "A Toutios",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2020",
      "venue": "Computer speech & language"
    },
    {
      "citation_id": "22",
      "title": "An approach to real-time magnetic resonance imaging for speech production",
      "authors": [
        "S Narayanan",
        "K Nayak",
        "S Lee",
        "A Sethy",
        "D Byrd"
      ],
      "year": "2004",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "23",
      "title": "Articulatory distinctiveness of vowels and consonants: A data-driven approach",
      "authors": [
        "J Wang",
        "J Green",
        "A Samal",
        "Y Yunusova"
      ],
      "year": "2013",
      "venue": "Articulatory distinctiveness of vowels and consonants: A data-driven approach"
    },
    {
      "citation_id": "24",
      "title": "Crema-d: Crowd-sourced emotional multimodal actors dataset",
      "authors": [
        "H Cao",
        "D Cooper",
        "M Keutmann",
        "R Gur",
        "A Nenkova",
        "R Verma"
      ],
      "year": "2014",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "25",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "C Busso",
        "S Parthasarathy",
        "A Burmania",
        "M Abdelwahab",
        "N Sadoughi",
        "E Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Montreal forced aligner: Trainable text-speech alignment using kaldi",
      "authors": [
        "M Mcauliffe",
        "M Socolof",
        "S Mihuc",
        "M Wagner",
        "M Sonderegger"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "27",
      "title": "Openface: an open source facial behavior analysis toolkit",
      "authors": [
        "T Baltrušaitis",
        "P Robinson",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "2016 IEEE winter conference on applications of computer vision"
    },
    {
      "citation_id": "28",
      "title": "Face alignment via regressing local binary features",
      "authors": [
        "S Ren",
        "X Cao",
        "Y Wei",
        "J Sun"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "29",
      "title": "Wing loss for robust facial landmark localisation with convolutional neural networks",
      "authors": [
        "Z.-H Feng",
        "J Kittler",
        "M Awais",
        "P Huber",
        "X.-J Wu"
      ],
      "year": "2018",
      "venue": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "30",
      "title": "Time series k-means: A new k-means type smooth subspace clustering for time series data",
      "authors": [
        "X Huang",
        "Y Ye",
        "L Xiong",
        "R Lau",
        "N Jiang",
        "S Wang"
      ],
      "year": "2016",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "31",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "A layer-anchoring strategy for enhancing cross-lingual speech emotion recognition",
      "authors": [
        "S Upadhyay",
        "C Busso",
        "C.-C Lee"
      ],
      "year": "2024",
      "venue": "A layer-anchoring strategy for enhancing cross-lingual speech emotion recognition",
      "arxiv": "arXiv:2407.04966"
    }
  ]
}