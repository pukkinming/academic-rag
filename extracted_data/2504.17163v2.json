{
  "paper_id": "2504.17163v2",
  "title": "Physiosync: Temporal And Cross-Modal Contrastive Learning Inspired By Physiological Synchronization For Eeg-Based Emotion Recognition",
  "published": "2025-04-24T00:48:03Z",
  "authors": [
    "Kai Cui",
    "Jia Li",
    "Yu Liu",
    "Xuesong Zhang",
    "Zhenzhen Hu",
    "Meng Wang"
  ],
  "keywords": [
    "EEG-based emotion recognition",
    "multi-modal fusion",
    "contrastive-learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalography (EEG) signals provide a promising and involuntary reflection of brain activity related to emotional states, offering significant advantages over behavioral cues like facial expressions. However, EEG signals are often noisy, affected by artifacts, and vary across individuals, complicating emotion recognition. While multimodal approaches have used Peripheral Physiological Signals (PPS) like GSR to complement EEG, they often overlook the dynamic synchronization and consistent semantics between the modalities. Additionally, the temporal dynamics of emotional fluctuations across different time resolutions in PPS remain underexplored. To address these challenges, we propose PhysioSync, a novel pre-training framework leveraging temporal and cross-modal contrastive learning, inspired by physiological synchronization phenomena. PhysioSync incorporates Cross-Modal Consistency Alignment (CM-CA) to model dynamic relationships between EEG and complementary PPS, enabling emotion-related synchronizations across modalities. Besides, it introduces Long-and Short-Term Temporal Contrastive Learning (LS-TCL) to capture emotional synchronization at different temporal resolutions within modalities. After pre-training, cross-resolution and cross-modal features are hierarchically fused and fine-tuned to enhance emotion recognition. Experiments on DEAP and DREAMER datasets demonstrate PhysioSync's advanced performance under unimodal and cross-modal conditions, highlighting its effectiveness for EEG-centered emotion recognition. The source code will be publicly available at https://github.com/MSA-LMC/PhysioSync.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Eeg-Based Multimodal Emotion Recognition",
      "text": "Electroencephalographic (EEG) signals, reflecting brain activity and psychological processes, contain vital psychophysiological information  [3] . Unlike other physiological signals, EEG is directly tied to the central nervous system, offering more precise emotional insights  [21] . This advantage has driven the development of various EEG-based emotion recognition methods  [22] ,  [23] . However, many methods still rely on hand-crafted features like power spectral density (PSD) and differential entropy (DE), rather than fully exploiting the potential of deep learning models. In contrast, some approaches use raw EEG data with advanced techniques like CNNs and LSTMs to capture spatial and temporal features. For instance, Yin et al.  [24]  proposed a framework combining graph CNNs and LSTMs, while Ding et al.  [25]  introduced TSception, a multi-scale CNN that integrates dynamic temporal, asymmetric spatial, and high-level fusion layers for more effective representation learning across temporal and channel dimensions.\n\nDespite these advancements, the complexity of human emotional states poses a significant challenge. A single EEG modality is insufficient to provide a comprehensive representation of the current emotional state, and achieving satisfactory recognition in terms of both accuracy and robustness remains a challenge  [26] . To address this, researchers have explored methods that combine signals from the central nervous system  [27]  (e.g., EEG) and the peripheral nervous system  [28]  (e.g., GSR, ECG, EMG, EOG, etc.). Jimenez et al.  [29]  proposed a multi-modal and multi-source Domain Adaptation (MMDA) method for addressing the multi-modal emotion recognition problem using EEG and eye movement signals. Li et al.  [30]  presented a fusion model that leverages the Cross Modal Transformer (CMT), Low Rank Fusion (LRF), and modified CMT (MCMT) to reduce incongruity and redundancy among multimodal physiological signals.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Contrastive Learning For Physiological Representation",
      "text": "Contrastive learning is a form of self-supervised learning that aims to learn representations without relying on manual labels. It has demonstrated state-of-the-art performance across various domains, including computer vision (CV)  [31] , natural language processing (NLP)  [32] , and bioinformatics  [33] . Typically, contrastive learning methods involve pre-training representations on relatively large datasets, which are then fine-tuned for downstream tasks  [34] . However, in the domain of physiological signal emotion recognition, particularly in Electroencephalogram (EEG)-based emotion recognition, the absence of large-scale datasets presents a significant challenge for pre-training. To address this, Shen et al.  [19]  proposed a contrastive learning strategy tailored for cross-subject generalization. This approach learns the similarities between samples from different subjects exposed to the same stimuli, enabling the model to generalize across subjects. Notably, their method does not rely on large external datasets; instead, it generates numerous self-supervised labels based on the alignment of experimental designs across subjects. One form of contrastive learning involves classifying samples into positive and negative pairs based on the internal relationships within the data. Using a specific loss function, this approach aims to maximize the similarity between positive pairs while minimizing the similarity between negative pairs  [31] . Kan et al.  [35]  utilized self-supervised learning alongside a novel genetics-inspired data augmentation method to achieve state-of-the-art performance in EEG-based emotion recognition. Simultaneously, contrastive learning methods have also been introduced in cross-corpus EEG-based emotion recognition. Liu et al.  [36]  introduced a cross-domain contrastive learning strategy during the pre-training phase and incorporating inter-electrode structural connectivity during the finetuning phase, the method significantly improved the accuracy of emotion recognition across different datasets. Our work introduces a multi-modal contrastive learning approach that effectively integrates both temporal and cross-modal contrastive learning to better combine multi-modal information.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Proposed Method",
      "text": "This section introduces PhysioSync, a novel contrastive learning framework with two phases: self-supervised pretraining and supervised fine-tuning (Fig.  2 ). Section A covers the data input and augmentation for multimodal physiological signals. Section B presents the temporal and cross-modal contrastive optimization mechanism. Section C describes the fine-tuning strategy, aligning pre-trained features with emotion labels through task-specific layers.\n\nA. Preliminaries 1) Long-and Short-term Clip Segmentation: To capture multi-scale emotional features, we set different time lengths of t to train encoders for long-term and short-term feature extraction, respectively. Specifically, the input is processed in mini-batches from subjects A and B as an example, the data from each trial of subject A is divided into t-second clips in temporal order, resulting in N clips denoted as X eeg i,A (i = 1, 2, 3, . . . , N ), where X eeg i,A ‚àà R C√óS , C is the number of channels, and S is the number of sampling points. Similarly, subject B's eeg data is divided into X eeg i,B (i = 1, 2, 3, . . . , N ). segments X eeg i,A and X eeg i,B from the same video stimulus form a positive pair, while X eeg i,S and X eeg j,S (j Ã∏ = i, S ‚àà {A, B}) from different video stimuli form a negative pair. For all subjects and other modalities, such segmentation is performed, then segments for K video stimuli from 2 subjects are randomly sampled to form a mini-batch\n\nHere, m can represent both eeg or pps.\n\n2) Data Augmentation: A major challenge in using physiological signals for emotion recognition is the limited number of subjects and small public datasets, which can hinder model training  [37] . To address this, data augmentation techniques, such as scaling and noise addition, can be employed to expand the dataset  [38] , and introduce more positive samples for contrastive learning. The data augmentation step generates more training samples and improves emotion-related semantic consistency learning against moderate noise. Specifically, two scaling factors are applied to the original samples, and Gaussian noise with a fixed mean and signal-to-noise ratio (SNR) is added. The process can be summarized as follows: where A represents the data augmentation operation. Through scaling, noise addition, and their combination, the data is augmented to five times its original size. Finally, the input can be represented as:\n\nB. Self-supervised Pre-training Strategy 1) Pretraining model: Previous work has shown that selfsupervised pre-training strategies can significantly aid model learning. Building upon the structure from  [39] , we have designed an encoder, as illustrated in Fig.  3 .\n\nPhysiological signals are input and transformed into z embeddings through parallel linear layers, aiming to encourage the model to focus on different views of features. Another linear layer followed by an activation function is used to gate the embeddings with information useful for emotion recognition. Subsequently, E i and ·∫º are multiplied elementwise and aggregated across the z embeddings to obtain e = (e 1 , . . . , e z ). The formula is as follows  [39] :\n\nwhere œÉ represents the sigmoid function, which constrains the output values between 0 and 1, and ‚äô denotes the Hadamard product. BN stands for batch normalization, and RELU denotes the activation function. By this method, the input features Gm are transformed into token sequences from different views, which can be further processed by subsequent layers.\n\nBefore passing to the next layers, the embedding e is prepended with a learnable class token E cls , which serves to aggregate information from the entire sequence. To integrate positional and modality information, learnable positional embeddings E pos and modality embeddings E mod are added to the input embeddings. This approach ensures that the input encompasses not only the sequence content but also positional and modality information:\n\nThe core component of the Transformer is the multi-head self-attention mechanism  [40] . The embedding ·∫Ω is transformed into query Q, key K, and value V through three linear layers, with the calculation formula as follows:\n\nwhere d e is the dimension of the embedding. Then we use h heads for self-attention, with each head represented as\n\n, where W is the weight matrix. Subsequently, after further processing by a feed-forward neural network (FFN), then repeat twice. Incorporating emotional prompt (learnable token)  [41]  boosts the model's sensitivity to emotional changes. We can simply denote the entire encoder as f m :\n\n2) Long-and Short-Term Temporal Contrastive Learning: To align the stimuli across subjects and achieve physiological synchronization among different subjects, the features extracted by the encoder are used for intra-modal temporal contrastive learning (TCL). Different values of t are used to perform contrastive learning at different time resolutions, and the corresponding encoders are pre-trained accordingly. Inspired by the SimCLR framework  [31] , a nonlinear projection head is employed between the encoder and the final contrastive loss. The structure, shown in Fig.  3 , is used to process the features extracted by the encoder:\n\nwhere W 1 , W 2 , W 3 are the weight matrices of the three linear layers; b 1 , b 2 , b 3 are the bias vectors and\n\ndenote the output features of each layer. where i denotes the number of physiological signal segments, S denotes the subject. Subsequently, the similarity between samples is calculated using the following formula:\n\nSimilar to the SimCLR, we use the normalized temperaturescaled cross-entropy to define the loss function as follows: Calculate loss L m and L cc by (  10 )-(  12 ), then obtain total loss L by  (13)  10:\n\nAbate loss L through optimizer updating parameters of Œ∏ eeg , Œ∏ pps , Œ∏ pr eeg and Œ∏ pr pps\n\nuntil all possible pairs of subjects are enumerated 12: end for 13: Output: Parameters Œ∏ eeg , Œ∏ pps Where T is the training epochs.\n\nsimilarity of negative pairs. The loss for the mini-batch L m is given by:\n\n3) Cross-Modal Consistency Alignment: To align physiological signals from different modalities, the features extracted by the encoder are also used for cross-modal contrastive learning (CM-CL). Physiological signal segments from different modalities induced by the same stimulus are treated as positive pairs, while others are treated as negative pairs. Additionally, before calculating the similarity, the features pass through a linear layer to obtain zm i,s . Then according to formulas (10)-(  12 ), the cross-modal contrastive loss L cc is computed.\n\nDuring the pre-training phase, the total loss L consists of the contrastive loss within the EEG modality L eeg , the contrastive loss within peripheral physiological signals L pps , and the cross-modal contrastive loss L cc :\n\nWe empirically set Œ± = 0.5, Œ≤ = 0.5 to balance EEG and PPS supervision, and Œ≥ = 1 to emphasize cross-modal consistency and enhance feature alignment.\n\nAfter pre-training, the encoder can effectively extract consistency features across different modalities at various time resolutions. Algorithm 1 summarizes the pre-training process.",
      "page_start": 3,
      "page_end": 5
    },
    {
      "section_name": "C. Supervised Fine-Tuning Framework",
      "text": "During the pre-training stage, we obtain modality-specific encoders at different time resolutions, which are subsequently used for supervised fine-tuning in the downstream task. Specifically, the pre-trained encoders are employed to extract consistency features for EEG and PPS at long or short time resolutions (the short-term input is derived by decomposing the long-term input into 1s ). Then their long-term and shortterm features are fused by concatenation separately into H eeg and H pps , which are then input into the modal fusion module.\n\nIn the Modal Fusion Module (Fig.  4 ), to flexibly adjust the information flow, we use Maximum Class Probability (MCP)  [11]  to compute the confidence level of each modality and assign weights to the features based on their confidence. MCP enhances the robustness and accuracy of the fusion process by measuring the confidence of each modality, ensuring more reliable decision-making in the presence of uncertain or noisy data:\n\nMCP(H m ) = max k‚àày\n\nwhere w represents the set of network parameters, m can represent both eeg or pps. Subsequently, the weighted features from both modalities are concatenated. This combined feature H joint is then fed into a classifier for final classification, and we optimize the model using cross-entropy loss.\n\nwhere P (H joint ) represents the probability that the joint feature vector belongs to a certain class, and y denotes the sample label.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments",
      "text": "Our method PhysioSync is experimented and evaluated on two widely used multimodal physiological signaling datasets: the DEAP dataset and the DREAMER  [42] ,  [43] . This section first describes the DEAP and DREAMER datasets in detail and the related implementation details. We then show the results of various experiments on both datasets.\n\nA. Dataset 1) DEAP: A multimodal physiological signal dataset used for emotion recognition, encompassing electroencephalogram (EEG) signals and several peripheral physiological signals. We primarily utilized EEG, galvanic skin response (GSR), electromyography (EMG), and electrooculography (EOG). These signals were recorded from 32 subjects as they watched 40 different short videos, with each recording session lasting 63 seconds (60 seconds for emotional arousal and 3 seconds for baseline). The Self-Assessment Manikin (SAM)  [44]  was utilized to capture the emotional ratings of the subjects, which included assessments of arousal, valence, liking, and dominance for each video, rated on a scale from 1 to 9. In this experiment, we utilized arousal and valence as the classification criteria. When the ratings are greater than or equal to 5, they are labeled as high arousal or high valence. Conversely, ratings below 5 are labeled as low arousal or low valence. This approach forms a binary classification for each dimension, and the combination of the two dimensions results in a four-class classification.\n\n2) DREAMER: The dataset comprises 14-channel EEG and 2-channel Electrocardiogram (ECG) data from 23 subjects (14 males and 9 females). Each subject viewed 18 movie clips, with durations ranging from 65 to 393 seconds. These clips were designed to elicit various emotions, including amusement, excitement, happiness, calmness, anger, disgust, fear, sadness, and surprise. Following each clip, subjects provided subjective evaluations of valence, arousal, and dominance using the Self-Assessment Manikin (SAM) on a scale from 1 to 5. The midpoint of the scale served as the threshold value: ratings of 3 or below indicated low valence or arousal, while ratings above 3 indicated high valence or arousal.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Data Pre-Processing",
      "text": "In the DEAP dataset, we used the publicly available preprocessed signals, which include downsampling to 128 Hz, EOG artifact removal  [42] , 4-45 Hz bandpass filtering, common average re-referencing, and EEG channel reordering following the Geneva scheme. Each trial contains a 60-second emotion segment and a 3-second baseline. Following  [45] , we averaged the baseline into a 1-second reference and subtracted it from the emotion segment. The corrected signals were segmented into 5-second clips, yielding 480 clips per participant. With 32 participants and 40 videos, this resulted in 15,360 segments for 10-fold cross-validation.\n\nFor the DREAMER dataset, we used the raw data provided by the official release. No artifact removal or filtering was applied, except that the ECG signals were downsampled to 128 Hz to match the sampling rate of other modalities. A similar baseline correction procedure was applied. The data was then segmented into 1-second windows for both pre-training and classification stages.\n\nDuring the pre-training phase, we organized the dataset tensor along five dimensions: video clips, subjects, 1, channels, and sampling points. In each fold, every epoch iterated through all the video clips in the current training data. In the finetuning phase, applying the same criteria, we divided the data along the video clips dimension and then reshaped the first two axes-video clips and subjects-into a sample axis. After reshaping, each axis of the dataset corresponded to samples, 1, channels, and sampling points.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "C. Experimental Setup",
      "text": "All training and experiments were conducted on an NVIDIA TITAN RTX GPU. Due to the limited number of subjects in the dataset, the results in  [11] ,  [19] ,  [25]  show significant data fluctuations when evaluated with cross-subject criteria, with some standard deviations exceeding 10, affecting the reliability. Therefore,we primarily adopt a subject-dependent evaluation using a unified ten-fold cross-validation protocol. In each fold, self-supervised pre-training and fine-tuning were performed exclusively on the corresponding training set, followed by evaluation on the corresponding test set. The best model from each fold was retained, and final results were reported as the mean and standard deviation across all folds. For completeness, we also report results under the cross-subject setting, where the leave-one-subject-out (LOSO) protocol was used. All evaluations strictly follow established practices to ensure reliable and fair comparisons.\n\nWith an input signal length of t = 1s for short-term and t = 5s for long-term segments, they are used to train encoders for each time scale during pre-training, with long-and shortterm feature extraction performed during fine-tuning. Data augmentation included scaling the samples with factors from [0.7, 0.8] and [1.2, 1.3], and adding Gaussian noise with zero mean to ensure an SNR of 5dB.\n\nTo optimize the contrastive learning model, we set the number of training epochs to 500, used the Adam optimizer  [54] , and applied a cosine annealing learning rate scheduler with triple cyclic warm restarts  [55] . The initial learning rate was set to 0.0001. During fine-tuning, the emotion classification task is simpler than feature extraction. To preserve the pretrained features and avoid overfitting on the small dataset, we set the number of epochs to 15, initial learning rate to 0.001, and batch size to 256, based on empirical observations. Other parameters remained unchanged.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "D. Subject-Dependent Performance Comparison With Sota",
      "text": "To demonstrate the superiority of the proposed method, we conducted extensive performance comparisons on the DEAP and DREAMER datasets. First, we selected several EEGonly models, results for these \"*\" models were obtained via ten-fold cross-validation using published source code and consistent experimental criteria. Additionally, we compared several multimodal fusion models.\n\nAs shown in Tables I and II, our proposed method outperforms all others. Specifically, we obtain average accuracies of 98.35%, 98.17%, and 97.99% on DEAP, and 97.01%, 96.11%, and 94.70% on DREAMER, with corresponding F1 scores closely aligned. Compared to models using EEG alone, our method achieves a recognition accuracy increase of 0.18% (arousal), 0.04% (valence), and 1.31% (four-class) on the DEAP dataset. In the DREAMER dataset, the accuracy increases by 0.62%, 0.16%, and 1.39%, demonstrating the benefit of fusing EEG with peripheral physiological signals. These gains highlight the complementary nature of the two modalities in capturing richer emotional information.\n\nFurther, against existing multimodal methods, our approach achieves additional improvements-up to 0.16% on DEAP",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Method",
      "text": "DEAP Dreamer and 1.47% on DREAMER in arousal, 0.10% and 1.28% in valence-indicating better exploitation of cross-modal information and more effective suppression of redundancy. Similar trends are observed in the F1 scores, confirming consistent performance gains. Moreover, both the accuracy and F1 scores have a standard deviation (STD) not exceeding 0.6. This low variability is primarily due to the use of subject-dependent experiments, where training and testing are performed on data from the same individual. Such settings typically yield more stable and consistent results compared to cross-subject scenarios. In summary, our approach not only enhances performance but also improves the reliability and stability of emotion recognition using multimodal physiological signals.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Cross-Subject Performance Comparison With Sota",
      "text": "To evaluate the generalization ability of our method, we conducted cross-subject experiments on the DEAP and DREAMER datasets using the Leave-One-Subject-Out (LOSO) protocol. As shown in Table  III . Specifically, it reaches 65.46% / 64.79% (Acc) and 64.33% / 64.10% (F1) on DEAP, and 64.88% / 63.24% (Acc) and 65.89% / 62.57% (F1) on DREAMER. Although the corresponding accuracy does not reach the absolute highest, the results remain highly competitive, and the consistently strong F1 scores reflect better balance between precision and recall, indicating more robust generalization across subjects. This suggests that the model is less affected by individual variability and can effectively learn subject-invariant representations.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "F. Qualitative Analysis",
      "text": "In this section, we visually analyze the challenges of emotion recognition. Fig.  5  shows the confusion matrices of our model on the DEAP and DREAMER datasets, demonstrating its ability to distinguish emotional states. Higher accuracy is observed in both arousal and valence dimensions for high values, typically linked to discrete positive emotions like happiness and excitement. These emotions correspond to distinct physiological signals-such as increased heart rate, skin con-   ductance, and brain activity-making them easier to recognize. When combining arousal and valence into four categories, the best performance occurs in the high valence/high arousal group for both datasets, consistent with previous binary classification results.\n\nTo demonstrate the model's classification ability, we applied t-SNE for dimensionality reduction to compare raw features and features after training (see Fig.  6 ). The trained features show significantly improved separation between emotion categories, with clear clustering across all dimensions. This indicates enhanced feature extraction and classification performance, allowing the model to better capture and distinguish latent emotional patterns. Additionally, the trained features exhibit a continuous band-like distribution, reflecting the smoothness of emotion labels-that is, their gradual changes over time. Although labels are binarized for classification, this pattern shows the model captures underlying temporal dynamics beyond discrete categories.\n\nTo further demonstrate the convergence and stability of the model during pre-training, we plot the training and validation loss curves for both 1-second and 5-second windows in Fig.  7 . As shown, the training and validation losses decrease steadily and plateau without signs of divergence, indicating effective learning and generalization. This supports the robustness of the proposed model across varying temporal resolutions.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "V. Ablation Experiments",
      "text": "This section presents a series of ablation experiments. Notably, to control for confounding factors, all experiments during fine-tuning utilized only long-term features, except for the ablation experiment of the long short term strategy.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Analysis Of Data Augmentation And Contrastive Learning",
      "text": "To validate the effectiveness of each module, we conducted multiple ablation experiments by sequentially adding each module (see Table  IV ). Three components were selected for ablation studies: intra-modal temporal contrastive learning (TCL), data augmentation (DA), and cross-modal contrastive learning (CM-CL). By applying intra-modal temporal contrastive learning, the encoders can extract physiological features consistent with the stimuli, facilitating the distinction of emotional states and achieving physiological synchronization across subjects. Data augmentation increases training samples and positive pairs, enhancing contrastive learning and improving model generalization. Cross-modal contrastive learning strengthens interaction and alignment between modalities, mapping signals into a shared semantic space for physiological synchronization. Results show that recognition performance steadily improves as each module is added, confirming the effectiveness of our approach.\n\nAdditionally, we explored other common data augmentation techniques beyond scaling and noise injection, including channel permutation (CP), and signal temporal flipping (TF). The ablation results are summarized in V. Compared with the baseline method (Baseline: no augmentation), each augmentation method contributes to improved performance in both arousal and valence dimensions. Notably, method All achieves competitive but not the best results, indicating that more augmentations do not necessarily lead to better performance.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "B. Ablation Of Eeg And Pps",
      "text": "To validate the complementary role of PPS to EEG, we conducted a modality ablation experiment on the DEAP dataset. We tested four modalities-EEG, Electromyography (EMG), Electrooculography (EOG), and GSR-both individually and in combination, as summarized in Table see Figure  VI . The results show that incorporating peripheral physiological signals improved recognition accuracy by 2.91% in arousal and 3.03% in valence, while F1 scores increased by 1.47% in arousal and 1.66% in valence. Specifically, the combination of EEG and GSR yielded the best results across most dimensions. The SOTA comparison experiment results are based on the fusion of EEG and GSR. Among the single modalities, EEG consistently achieved the highest accuracy and F1 scores across the Arousal, Valence, and four-category tasks, further emphasizing its critical role in emotion recognition. The relatively limited contribution from EMG and EOG may be due to their weaker correlation with emotional states-EMG is more susceptible to motion artifacts  [59] , while EOG mainly reflects eye movements, which not be strongly linked to arousal or valence  [60] . We further conducted experiments using the EEG and GSR data from each subject separately to validate the role of modality fusion. As shown in Fig.  8 , the fusion of EEG and GSR (green line) consistently outperformed the single modalities (EEG in blue and GSR in red) across all 32 subjects in the arousal, valence, and four-class classification tasks. This demonstrates the complementary nature of EEG and GSR, highlighting the strength of modality fusion in providing a richer and more robust representation of emotional states.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "C. Ablation Of Long-Term Signal Decomposition For Short-Term Fusion",
      "text": "In the final fine-tuning strategy, we combined long-term and short-term temporal features. Here, \"Long\" refers to using only long-term features,\"Long+Short\" uses a fusion of both long-and short-term features, and \"Long*\" adds two selfattention layers on top of the \"Long\" strategy. Specifically(see Table  VII ), compared to using long-term features alone, further fusion with short-term features improves recognition accuracy by 0.43% in arousal and 0.16% in valence, and F1 scores increased by 0.43% in arousal and 0.18% in valence. The performance improvement highlights the value of multi-scale feature integration in emotion recognition. In contrast, adding parameters with the \"Long*\" strategy alone does not improve performance, indicating that relying solely on long-term features, even with more parameters, limits the model's capacity to capture dynamic emotional changes.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Impact Of Time Resolutions (Windows)",
      "text": "In this section, we systematically examine the effect of time window length on model performance in emotion recognition using physiological signals (see Fig.  9 ). Signal segments ranging from 1 to 10 seconds (excluding 7s-9s) were evaluated. Results show a clear trend: performance improves notably from 1 to 5 seconds, with both accuracy and F1 scores increasing as longer windows capture richer physiological features. The best performance is achieved at 5 seconds, which offers an optimal trade-off between accuracy and computational efficiency. However, when the window length exceeds 6 seconds, performance begins to decline. This may be due to emotional drift  [61] , where the subject's emotional state changes within the segment, introducing label noise. Additionally, longer segments may contain redundant or diluted information, making it harder for the model to capture key discriminative features. Overall, a 5-second window strikes the optimal trade-off between performance and efficiency.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "E. Comparison Of Physiological Signal Pairing Strategies",
      "text": "To validate the effectiveness of our positive-negative pair selection strategy, we compared it with CLISA  [19] , the best-performing single-modality method using only EEG. In CLISA, 5-second segments are randomly selected from each trial and paired across subjects as positive pairs. Our method pre-divides each trial into chronological 5-second segments, randomly selects K segments, and generates negative pairs within the same trial. This increases the challenge of contrastive learning through more refined sample pairing. As shown in Table VIII (Our* means that our method does not use data augmentation), our construction method better helps the model distinguish between different emotional categories. Specifically, accuracy and F1 scores improved by 0.34% and 1.54% in arousal, 0.38% and 1.35% in valence, while corresponding standard deviations decreased by 0.06 ‚àº 0.41 and 0.21 ‚àº 0.37, respectively. Our intra-trial negative pairing surpasses CLISA by learning fine-grained within-subject emotional differences, boosting robustness and discriminability.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "F. Ablation Of Modal Fusion Strategies",
      "text": "In downstream tasks involving the fusion of different modalities, we compared several common fusion methods, including feature-level fusion  [62] , decision-level fusion  [63] , adaptive weighting  [64] , attention mechanisms  [65] , and cross-attention  [66]  (see Fig.  10 ). The decision-level fusion combines the final outputs of the classification head, while other methods perform fusion on the features extracted by the encoder. The results show that our fusion method outperforms others in terms of performance. By assigning a weight to each modality based on its maximum class probability before fusion, our method ensures an appropriate contribution from each modality, enabling the model to prioritize the most informative signals while still considering all modalities.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vi. Limitation",
      "text": "In this study, we primarily adopted a subject-dependent evaluation protocol due to large performance fluctuations observed under the cross-subject setting, with standard deviations exceeding 10 in some cases. This issue is largely attributed to the limited number of subjects and data in the datasets. Although we included cross-subject experimental results, a detailed analysis was not performed in this work. In future studies, we plan to explore this setting more thoroughly as an important research direction. In addition, our current model only supports fusion of two modalities. In future work, we will explore fusion of multiple modalities to better integrate complementary information from other physiological signals.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, inspired by physiological synchronization, we propose a contrastive learning framework for emotion recognition, achieving competitive results on the DEAP and DREAMER datasets. By integrating intra-modal temporal contrastive learning (TCL) and cross-modal contrastive learning (CM-CL), we make preliminary attempts at achieving physiological synchronization across both subjects and modalities. Additionally, we found that a 5-second time window performed optimally when using a single time resolution feature. However, as emotions are characterized by both instantaneous and subtle changes, further extraction and fusion of long-term and short-term features from different time resolutions have led to even better results. Experiments show that, although EEG signals generally perform well in emotion recognition, a reasonable and effective combination with peripheral physiological signals can lead to even better results. Physiological signals are typically multi-channel, with spatial relationships between channels that our model has not yet fully exploited. Therefore, in addition to improving the cross-subject experiments and exploring fusion of multiple modalities, future research will focus on better exploring and utilizing these spatial relationships to further enhance model performance.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the proposed approach compared to existing methods.",
      "page": 1
    },
    {
      "caption": "Figure 1: (a)). For example,",
      "page": 2
    },
    {
      "caption": "Figure 1: (b)). For example, Shen et al. [19] utilized convolutional",
      "page": 2
    },
    {
      "caption": "Figure 1: (c)). Our contrastive learn-",
      "page": 2
    },
    {
      "caption": "Figure 2: The proposed contrastive learning framework inspired by physiological synchronization, which consists of a self-supervised pre-training phase and a",
      "page": 3
    },
    {
      "caption": "Figure 4: illustrates the Fusion Module and Classifier.",
      "page": 3
    },
    {
      "caption": "Figure 2: ). Section A covers",
      "page": 3
    },
    {
      "caption": "Figure 3: The illustration of Encoder and Projector, where the Encoder we take",
      "page": 4
    },
    {
      "caption": "Figure 3: Physiological signals are input and transformed into z",
      "page": 4
    },
    {
      "caption": "Figure 3: , is used to process the",
      "page": 4
    },
    {
      "caption": "Figure 4: The illustration of Modal Fusion Module and Classifier. In the",
      "page": 5
    },
    {
      "caption": "Figure 4: ), to flexibly adjust the",
      "page": 5
    },
    {
      "caption": "Figure 5: shows the confusion matrices of our",
      "page": 7
    },
    {
      "caption": "Figure 5: Confusion matrices in the DEAP and DREAMER datasets. (a) DEAP-Arousal. (b) DEAP-Valence. (c) DEAP-Four. (d) DREAMER-Arousal. (e)",
      "page": 8
    },
    {
      "caption": "Figure 6: T-SNE visualization in the DEAP and DREAMER datasets. The original feature distributions of arousal, valence, and the four-class for the DEAP and",
      "page": 8
    },
    {
      "caption": "Figure 7: Training and validation loss curves over epochs for both 1-second",
      "page": 8
    },
    {
      "caption": "Figure 6: ). The trained fea-",
      "page": 8
    },
    {
      "caption": "Figure 7: As shown, the training and validation losses decrease steadily",
      "page": 8
    },
    {
      "caption": "Figure 8: , the fusion of EEG",
      "page": 9
    },
    {
      "caption": "Figure 8: The recognition accuracy of arousal, valence, and four dimensions of each subject in DEAP dataset by using EEG, GSR, and modality fusion.",
      "page": 10
    },
    {
      "caption": "Figure 9: Illustration of model performance with different time windows on the DEAP dataset. The best performance is achieved with a 5-second window.",
      "page": 10
    },
    {
      "caption": "Figure 9: ). Signal segments rang-",
      "page": 10
    },
    {
      "caption": "Figure 10: ). The decision-level fusion combines the final",
      "page": 11
    },
    {
      "caption": "Figure 10: Comparison of different modal fusion methods (Including feature-",
      "page": 11
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Projector ‚Ä¶ ùê≥ (cid:2915)(cid:2915)(cid:2917)\nEncoder\nEEG\nor\nLinear EEG Temporal\nContrastive Learning\nùê≥(cid:3556) ‚Ä¶\n(cid:2915)(cid:2915)(cid:2917)\nCM-CL\nCross-Modal TCL\nConsistency Alignment\nùê≥(cid:3556) ‚Ä¶\n(cid:2926)(cid:2926)(cid:2929)\nPPS Temporal\nEncoder Linear Contrastive Learning\nPPS\nor\n‚Ä¶\nProjector ùê≥ (cid:2926)(cid:2926)(cid:2929)": "Self-Supervised Pre-Training"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Short-Term C :Concat\nEEG Encoder\nEEG\nLong-Term\nEncoder\nC\nClassifier\nConfidence-Aware\nModal Fusion\nPPS Long-Term\nEncoder\nC Supervised\nLearning\nShort-Term\nPPS Encoder Emotion Labels": "Supervised Fine-Tuning"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5": "cedure for PhysioSync\nConfidence-Aware\nXpps Classifier\nModal Fusion\nbase encoders Œ∏eeg, Œ∏pps\n‚Ä¶ ‚Ä¶\npps Softmax\nùëØ\nùíãùíêùíäùíèùíï\nC Linear\nX X BN & RELU\nB, Random scramble after\nMax Max\n,...,N;S ‚àà{A,B}} ùë∑ ùíÜùíÜùíà ùë∑ ùíëùíëùíî Linear ‚äó\nments G = {Xm |i =\nm i,S\nSoftmax Softmax\nBN & RELU\n¬©\n1,...,5K;S ‚àà {A,B}}\nFC Layer Linear\nK;S ‚àà{A,B}} by (3)-(9)\n‚Ä¶ ‚Ä¶\nC Concat\nc by (10)-(12), then obtain ùëØ ùíÜùíÜùíà ùëØ ùíëùíëùíî X Product Operator\nmizer updating parameters Fig. 4. The illustration of Modal Fusion Module and Classifier. In the\nŒ∏pr pps ModalFusionModule,weemploytheMaximumClassProbability(MCP)to\ncalculateconfidencelevelsandassignweightstoeachmodalityaccordingly.\nbjects are enumerated\nresolutions (the short-term input is derived by decomposing\nthe long-term input into 1s ). Then their long-term and short-",
          "Column_2": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ÔºàdÔºâ ÔºàeÔºâ ÔºàfÔºâ": "10",
          "Column_2": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Arousal Acc ¬± Std Arousal F1 ¬± Std Valence Acc ¬± Std Valence F1 ¬± Std Arousal Acc ¬± Std Arousal F1 ¬± Std Valence Acc ¬± Std Valence F1 ¬± Std SimCLR",
      "venue": "Arousal Acc ¬± Std Arousal F1 ¬± Std Valence Acc ¬± Std Valence F1 ¬± Std Arousal Acc ¬± Std Arousal F1 ¬± Std Valence Acc ¬± Std Valence F1 ¬± Std SimCLR"
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Sgmc"
      ],
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "Lrescapsule"
      ],
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "Discriminative joint knowledge transfer with online updating mechanism for eegbased emotion recognition",
      "authors": [
        "Xiaowei Zhang",
        "Zhongyi Zhou",
        "Qiqi Zhao",
        "Kechen Hou",
        "Xiangyu Wei",
        "Sipo Zhang",
        "Yikun Yang",
        "Yanmeng Cui"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "5",
      "title": "Emotion recognition in eeg signals using deep learning methods: A review",
      "authors": [
        "Mahboobeh Jafari",
        "Afshin Shoeibi",
        "Marjane Khodatars",
        "Sara Bagherzadeh",
        "Ahmad Shalbaf",
        "David Garc√≠a",
        "Juan Gorriz",
        "U Rajendra"
      ],
      "year": "2023",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "6",
      "title": "Emotions recognition using eeg signals: A survey",
      "authors": [
        "M Soraia",
        "Manuel Alarcao",
        "Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "Measures of emotion: A review",
      "authors": [
        "B Iris",
        "Mauss",
        "Michael D Robinson"
      ],
      "year": "2009",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "8",
      "title": "Sstd: a novel spatio-temporal demographic network for eegbased emotion recognition",
      "authors": [
        "Rui Li",
        "Chao Ren",
        "Chen Li",
        "Nan Zhao",
        "Dawei Lu",
        "Xiaowei Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "9",
      "title": "Gusa: Graph-based unsupervised subdomain adaptation for cross-subject eeg emotion recognition",
      "authors": [
        "Xiaojun Li",
        "Bianna Cl Philip Chen",
        "Tong Chen",
        "Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Adaptive dual-space network with multigraph fusion for eeg-based emotion recognition",
      "authors": [
        "Mengqing Ye",
        "Wenming Cl Philip Chen",
        "Tong Zheng",
        "Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "11",
      "title": "Multi-source domain transfer discriminative dictionary learning modeling for electroencephalogram-based emotion recognition",
      "authors": [
        "Xiaoqing Gu",
        "Weiwei Cai",
        "Ming Gao",
        "Yizhang Jiang",
        "Xin Ning",
        "Pengjiang Qian"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "12",
      "title": "Hierarchical multimodal-fusion of physiological signals for emotion recognition with scenario adaption and contrastive alignment",
      "authors": [
        "Jiehao Tang",
        "Zhuang Ma",
        "Kaiyu Gan",
        "Jianhua Zhang",
        "Zhong Yin"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "13",
      "title": "Tensor correlation fusion for multimodal physiological signal emotion recognition",
      "authors": [
        "Jian Shen",
        "Kexin Zhu",
        "Huakang Liu",
        "Jinwen Wu",
        "Kang Wang",
        "Qunxi Dong"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "14",
      "title": "Dynamic confidence-aware multi-modal emotion recognition",
      "authors": [
        "Qi Zhu",
        "Chuhang Zheng",
        "Zheng Zhang",
        "Wei Shao",
        "Daoqiang Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "15",
      "title": "Cross-modal guiding neural network for multimodal emotion recognition from eeg and eye movement signals",
      "authors": [
        "Baole Fu",
        "Wenhao Chu",
        "Chunrui Gu",
        "Yinhua Liu"
      ],
      "year": "2024",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "16",
      "title": "Subject independent emotion recognition using eeg and physiological signals-a comparative study",
      "authors": [
        "Manju Priya",
        "Arthanarisamy Ramaswamy",
        "Suja Palaniswamy"
      ],
      "year": "2022",
      "venue": "Applied Computing and Informatics"
    },
    {
      "citation_id": "17",
      "title": "Intersubject synchronization of cortical activity during natural vision",
      "authors": [
        "Uri Hasson",
        "Yuval Nir",
        "Ifat Levy",
        "Galit Fuhrmann",
        "Rafael Malach"
      ],
      "year": "2004",
      "venue": "science"
    },
    {
      "citation_id": "18",
      "title": "Physiological synchrony in eeg, electrodermal activity and heart rate reflects shared selective auditory attention",
      "authors": [
        "Nattapong Ivo V Stuldreher",
        "Jan Bf Thammasan",
        "Anne-Marie Van Erp",
        "Brouwer"
      ],
      "year": "2020",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "19",
      "title": "Multimodal motor imagery decoding method based on temporal spatial feature alignment and fusion",
      "authors": [
        "Yukun Zhang",
        "Shuang Qiu",
        "Huiguang He"
      ],
      "year": "2023",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "20",
      "title": "Physiological synchronization in the clinical process: A research primer",
      "authors": [
        "Alessandro Johann R Kleinbub",
        "Arianna Talia",
        "Palmieri"
      ],
      "year": "2020",
      "venue": "Journal of Counseling Psychology"
    },
    {
      "citation_id": "21",
      "title": "Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems",
      "authors": [
        "Junnan Li",
        "Ramprasaath Selvaraju",
        "Akhilesh Gotmare",
        "Shafiq Joty",
        "Caiming Xiong",
        "Steven Chu",
        "Hong Hoi"
      ],
      "year": "2021",
      "venue": "Align before fuse: Vision and language representation learning with momentum distillation. Advances in neural information processing systems"
    },
    {
      "citation_id": "22",
      "title": "Contrastive learning of subject-invariant eeg representations for crosssubject emotion recognition",
      "authors": [
        "Xinke Shen",
        "Xianggen Liu",
        "Xin Hu",
        "Dan Zhang",
        "Sen Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "The relation between short-term emotion dynamics and psychological wellbeing: A meta-analysis",
      "authors": [
        "Marlies Houben",
        "Wim Van Den",
        "Peter Noortgate",
        "Kuppens"
      ],
      "year": "2015",
      "venue": "Psychological bulletin"
    },
    {
      "citation_id": "24",
      "title": "Eeg-based multimodal emotion recognition: a machine learning perspective",
      "authors": [
        "Huan Liu",
        "Tianyu Lou",
        "Yuzhe Zhang",
        "Yixiao Wu",
        "Yang Xiao",
        "Christian Jensen",
        "Dalin Zhang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "25",
      "title": "Da-capsnet: A multi-branch capsule network based on adversarial domain adaption for cross-subject eeg emotion recognition",
      "authors": [
        "Shuaiqi Liu",
        "Zeyao Wang",
        "Yanling An",
        "Bing Li",
        "Xinrui Wang",
        "Yudong Zhang"
      ],
      "year": "2024",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "26",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "Xiaobing Du",
        "Cuixia Ma",
        "Guanhua Zhang",
        "Jinyao Li",
        "Yu-Kun Lai",
        "Guozhen Zhao",
        "Xiaoming Deng",
        "Yong-Jin Liu",
        "Hongan Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Eeg emotion recognition using fusion model of graph convolutional neural networks and lstm",
      "authors": [
        "Yongqiang Yin",
        "Xiangwei Zheng",
        "Bin Hu",
        "Yuang Zhang",
        "Xinchun Cui"
      ],
      "year": "2021",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "28",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from eeg for emotion recognition",
      "authors": [
        "Yi Ding",
        "Neethu Robinson",
        "Su Zhang",
        "Qiuhao Zeng",
        "Cuntai Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "Advances in multimodal emotion recognition based on brain-computer interfaces",
      "authors": [
        "Zhipeng He",
        "Zina Li",
        "Fuzhou Yang",
        "Lei Wang",
        "Jingcong Li",
        "Chengju Zhou",
        "Jiahui Pan"
      ],
      "year": "2020",
      "venue": "Brain sciences"
    },
    {
      "citation_id": "30",
      "title": "Fganet: fnirsguided attention network for hybrid eeg-fnirs brain-computer interfaces",
      "authors": [
        "Youngchul Kwak",
        "Woo-Jin",
        "Seong-Eun Song",
        "Kim"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "31",
      "title": "Lstm-modeling of emotion recognition using peripheral physiological signals in naturalistic conversations",
      "authors": [
        "Cheul Sami Zitouni",
        "Young Park",
        "Uichin Lee",
        "Leontios Hadjileontiadis",
        "Ahsan Khandoker"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "32",
      "title": "Mmda: A multimodal and multisource domain adaptation method for cross-subject emotion recognition from eeg and eye movement signals",
      "authors": [
        "Magdiel Jim√©nez-Guarneros",
        "Gibran Fuentes-Pineda",
        "Jonas Grande-Barreto"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Computational Social Systems"
    },
    {
      "citation_id": "33",
      "title": "Incongruity-aware multimodal physiology signals fusion for emotion recognition",
      "authors": [
        "Jing Li",
        "Ning Chen",
        "Hongqing Zhu",
        "Guangqiang Li",
        "Zhangyong Xu",
        "Dingxin Chen"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "34",
      "title": "A simple framework for contrastive learning of visual representations",
      "authors": [
        "Ting Chen",
        "Simon Kornblith",
        "Mohammad Norouzi",
        "Geoffrey Hinton"
      ],
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "35",
      "title": "",
      "authors": [
        "Pmlr"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "36",
      "title": "Coda: Contrast-enhanced and diversity-promoting data augmentation for natural language understanding",
      "authors": [
        "Yanru Qu",
        "Dinghan Shen",
        "Yelong Shen",
        "Sandra Sajeev",
        "Jiawei Han",
        "Weizhu Chen"
      ],
      "year": "2020",
      "venue": "Coda: Contrast-enhanced and diversity-promoting data augmentation for natural language understanding",
      "arxiv": "arXiv:2010.08670"
    },
    {
      "citation_id": "37",
      "title": "Supervised graph co-contrastive learning for drug-target interaction prediction",
      "authors": [
        "Yang Li",
        "Guanyu Qiao",
        "Xin Gao",
        "Guohua Wang"
      ],
      "year": "2022",
      "venue": "Bioinformatics"
    },
    {
      "citation_id": "38",
      "title": "Self-supervised contrastive learning for medical time series: A systematic review",
      "authors": [
        "Ziyu Liu",
        "Azadeh Alavi",
        "Minyi Li",
        "Xiang Zhang"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "39",
      "title": "Self-supervised group meiosis contrastive learning for eeg-based emotion recognition",
      "authors": [
        "Haoning Kan",
        "Jiale Yu",
        "Jiajin Huang",
        "Zihe Liu",
        "Heqian Wang",
        "Haiyan Zhou"
      ],
      "year": "2023",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "40",
      "title": "Joint contrastive learning with feature alignment for cross-corpus eeg-based emotion recognition",
      "authors": [
        "Qile Liu",
        "Zhihao Zhou",
        "Jiyuan Wang",
        "Zhen Liang"
      ],
      "year": "2024",
      "venue": "Joint contrastive learning with feature alignment for cross-corpus eeg-based emotion recognition",
      "arxiv": "arXiv:2404.09559"
    },
    {
      "citation_id": "41",
      "title": "Subject wise data augmentation based on balancing factor for quaternary emotion recognition through hybrid deep learning model",
      "authors": [
        "Khushboo Singh",
        "Mitul Kumar Ahirwal",
        "Manish Pandey"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "42",
      "title": "Hypercomplex multimodal emotion recognition from eeg and peripheral physiological signals",
      "authors": [
        "Eleonora Lopez",
        "Eleonora Chiarantano",
        "Eleonora Grassucci",
        "Danilo Comminiello"
      ],
      "year": "2023",
      "venue": "2023 IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW)"
    },
    {
      "citation_id": "43",
      "title": "Multimodal adaptive emotion transformer with flexible modality inputs on a novel dataset with continuous labels",
      "authors": [
        "Wei-Bang Jiang",
        "Xuan-Hao Liu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "44",
      "title": "Attention is all you need",
      "authors": [
        "Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "45",
      "title": "Vision transformers need registers",
      "authors": [
        "Timoth√©e Darcet",
        "Maxime Oquab",
        "Julien Mairal",
        "Piotr Bojanowski"
      ],
      "year": "2023",
      "venue": "Vision transformers need registers",
      "arxiv": "arXiv:2309.16588"
    },
    {
      "citation_id": "46",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "47",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless lowcost off-the-shelf devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "48",
      "title": "Measuring emotion: the selfassessment manikin and the semantic differential",
      "authors": [
        "M Margaret",
        "Peter Bradley",
        "Lang"
      ],
      "year": "1994",
      "venue": "Journal of behavior therapy and experimental psychiatry"
    },
    {
      "citation_id": "49",
      "title": "Emotion recognition from multi-channel eeg through parallel convolutional recurrent neural network",
      "authors": [
        "Yilong Yang",
        "Qingfeng Wu",
        "Ming Qiu",
        "Yingdong Wang",
        "Xiaowei Chen"
      ],
      "year": "2018",
      "venue": "2018 international joint conference on neural networks (IJCNN)"
    },
    {
      "citation_id": "50",
      "title": "Light-weight residual convolution-based capsule network for eeg emotion recognition",
      "authors": [
        "Cunhang Fan",
        "Jinqin Wang",
        "Wei Huang",
        "Xiaoke Yang",
        "Guangxiong Pei",
        "Taihao Li",
        "Zhao Lv"
      ],
      "year": "2024",
      "venue": "Advanced Engineering Informatics"
    },
    {
      "citation_id": "51",
      "title": "Emotion recognition from eeg based on multi-task learning with capsule network and attention mechanism",
      "authors": [
        "Chang Li",
        "Bin Wang",
        "Silin Zhang",
        "Yu Liu",
        "Rencheng Song",
        "Juan Cheng",
        "Xun Chen"
      ],
      "year": "2022",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "52",
      "title": "Eeg emotion recognition based on ordinary differential equation graph convolutional networks and dynamic time wrapping",
      "authors": [
        "Yiyuan Chen",
        "Xiaodong Xu",
        "Xiaoyi Bian",
        "Xiaowei Qin"
      ],
      "year": "2024",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "53",
      "title": "Emotion recognition of eeg based on dualinput multi-network fusion features",
      "authors": [
        "Weitong Sun",
        "Yuping Su"
      ],
      "year": "2024",
      "venue": "2024 7th International Conference on Information Communication and Signal Processing (ICICSP)"
    },
    {
      "citation_id": "54",
      "title": "Tacoformer: Token-channel compounded cross attention for multimodal emotion recognition",
      "authors": [
        "Xinda Li"
      ],
      "year": "2023",
      "venue": "Tacoformer: Token-channel compounded cross attention for multimodal emotion recognition",
      "arxiv": "arXiv:2306.13592"
    },
    {
      "citation_id": "55",
      "title": "Emotion recognition based on multiple physiological signals",
      "authors": [
        "Qi Li",
        "Yunqing Liu",
        "Fei Yan",
        "Qiong Zhang",
        "Cong Liu"
      ],
      "year": "2023",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "56",
      "title": "Msdsanet: Multimodal emotion recognition based on multistream network and dual-scale attention network feature representation",
      "authors": [
        "Weitong Sun",
        "Xingya Yan",
        "Yuping Su",
        "Gaihua Wang",
        "Yumei Zhang"
      ],
      "year": "2025",
      "venue": "Sensors"
    },
    {
      "citation_id": "57",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "Wei Liu",
        "Jie-Lin Qiu",
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "58",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Kingma"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "59",
      "title": "Stochastic gradient descent with warm restarts",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter",
        "Sgdr"
      ],
      "year": "2016",
      "venue": "Stochastic gradient descent with warm restarts",
      "arxiv": "arXiv:1608.03983"
    },
    {
      "citation_id": "60",
      "title": "An adversarial discriminative temporal convolutional network for eeg-based cross-domain emotion recognition",
      "authors": [
        "Zhipeng He",
        "Yongshi Zhong",
        "Jiahui Pan"
      ],
      "year": "2022",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "61",
      "title": "The mitigation of heterogeneity in temporal scale among different cortical regions for eeg emotion recognition",
      "authors": [
        "Zhangyong Xu",
        "Ning Chen",
        "Guangqiang Li",
        "Jing Li",
        "Hongqing Zhu",
        "Zhiying Zhu"
      ],
      "year": "2025",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "62",
      "title": "Emotionmil: An end-to-end multiple instance learning framework for emotion recognition from eeg signals",
      "authors": [
        "Jun Xiao",
        "Feifei Qi",
        "Lingli Wang",
        "Yanbin He",
        "Jingang Yu",
        "Wei Wu",
        "Zhuliang Yu",
        "Yuanqing Li",
        "Zhenghui Gu",
        "Tianyou Yu"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "63",
      "title": "Affect dynamics of facial emg during continuous emotional experiences",
      "authors": [
        "Yulia Golland",
        "Adam Hakim",
        "Tali Aloni",
        "Stacey Schaefer",
        "Nava Levit-Binnun"
      ],
      "year": "2018",
      "venue": "Biological psychology"
    },
    {
      "citation_id": "64",
      "title": "Affective processing in natural scene viewing: Valence and arousal interactions in eye-fixation-related potentials",
      "authors": [
        "Jaana Simola",
        "Kevin Fevre",
        "Jari Torniainen",
        "Thierry Baccino"
      ],
      "year": "2015",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "65",
      "title": "Short-term emotion assessment in a recall paradigm",
      "authors": [
        "Guillaume Chanel",
        "J Joep",
        "Mohammad Kierkels",
        "Thierry Soleymani",
        "Pun"
      ],
      "year": "2009",
      "venue": "International Journal of Human-Computer Studies"
    },
    {
      "citation_id": "66",
      "title": "Feature Level Fusion. Advanced Pattern Recognition Technologies with Applications to",
      "authors": [
        "David Zhang",
        "Fengxi Song",
        "Yong Xu",
        "Zhizhen Liang"
      ],
      "year": "2009",
      "venue": "Biometrics"
    },
    {
      "citation_id": "67",
      "title": "Decision level fusion",
      "authors": [
        "David Zhang",
        "Fengxi Song",
        "Yong Xu",
        "Zhizhen Liang"
      ],
      "year": "2009",
      "venue": "Advanced pattern recognition technologies with applications to biometrics"
    },
    {
      "citation_id": "68",
      "title": "Adaptive weighted fusion: a novel fusion approach for image classification",
      "authors": [
        "Yong Xu",
        "Yuwu Lu"
      ],
      "year": "2015",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "69",
      "title": "Attentional feature fusion",
      "authors": [
        "Yimian Dai",
        "Fabian Gieseke",
        "Stefan Oehmcke",
        "Yiquan Wu",
        "Kobus Barnard"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF winter conference on applications of computer vision"
    },
    {
      "citation_id": "70",
      "title": "Cross attention network for few-shot classification",
      "authors": [
        "Ruibing Hou",
        "Hong Chang",
        "Bingpeng Ma",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    }
  ]
}