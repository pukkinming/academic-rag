{
  "paper_id": "2111.02327v1",
  "title": "Ml-Persref: A Machine Learning-Based Personalized Multimodal Fusion Approach For Referencing Outside Objects From A Moving Vehicle",
  "published": "2021-11-03T16:22:17Z",
  "authors": [
    "Amr Gomaa",
    "Guillermo Reyes",
    "Michael Feld"
  ],
  "keywords": [
    "Multimodal Fusion",
    "Pointing",
    "Eye Gaze",
    "Object Referencing",
    "Personalized Models",
    "Machine Learning",
    "Deep Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Over the past decades, the addition of hundreds of sensors to modern vehicles has led to an exponential increase in their capabilities. This allows for novel approaches to interaction with the vehicle that go beyond traditional touch-based and voice command approaches, such as emotion recognition, head rotation, eye gaze, and pointing gestures. Although gaze and pointing gestures have been used before for referencing objects inside and outside vehicles, the multimodal interaction and fusion of these gestures have so far not been extensively studied. We propose a novel learning-based multimodal fusion approach for referencing outside-the-vehicle objects while maintaining a long driving route in a simulated environment. The proposed multimodal approaches outperform single-modality approaches in multiple aspects and conditions. Moreover, we also demonstrate possible ways to exploit behavioral differences between users when completing the referencing task to realize an adaptable personalized system for each driver. We propose a personalization technique based on the transfer-of-learning concept for exceedingly small data sizes to enhance prediction and adapt to individualistic referencing behavior. Our code is publicly available at https://github.com/amr-gomaa/ML-PersRef.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "As vehicles move towards fully autonomous driving, driving assistance functionalities are exponentially increasing, such as parking assistance, blind-spot detection, and cruise control. These features make the driving experience much easier and more enjoyable, which allows car manufacturers to design and implement numerous other additional features such as detecting eyes off-road using gaze monitoring, or enhancing secondary tasks while driving, such as music and infotainment control using mid-air hand gestures and voice commands. All these driving-and non-driving-related features require numerous sensors to work, such as external and internal cameras, gaze trackers, hand and finger detectors, and motion sensors. This enables researchers to design more novel features that could make use of the full range of available sensors, such as automatic music control using emotion recognition, and referencing objects (e.g., landmarks, restaurants, and buildings) outside the car. The latter feature is the focus of this paper. Selecting and referencing objects using speech, gaze, hand, and pointing gestures have been attempted in multiple domains and situations such as human-robot interaction, Industry 4.0, and invehicle and outside-the-vehicle interaction. Only recently did car manufacturers start implementing gesture-based in-vehicle object selection, such as the BMW Natural User Interface  [1]  using pointing, and outside-the-vehicle object referencing, such as the Mercedes-Benz User Experience (MBUX) Travel Knowledge Feature  [2]  using gaze. However, these features focus on single-modality interaction or limited multimodal approach where the second modality (usually speech) is used as event trigger only. This may deteriorate the referencing task performance during high-stress dynamic situations such as high-traffic driving or lane changing  [8, 11, 48] . While multimodal interaction is more natural and provides superior performance compared to single-modality approaches, it introduces inherent complexity and possesses multiple challenges, such as fusion techniques (e.g., Early versus Late Fusion  [9, 43, 47] ), translation, co-learning, representation, and alignment  [6, 7] . This has limited its use in previous systems, especially in complex dual-task situations where several modalities can be used in both tasks at the same time, such as infotainment interaction while driving.\n\nIn this paper, we explore several fusion strategies to overcome these challenges during the referencing task. Specifically, we focus on machine-learning-based approaches for personalized referencing of outside-the-vehicle objects using gaze and pointing multimodal interaction.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "One-year-old infants learn to point at things and use hand gestures even before learning how to speak; hence, gestures are the most natural form of communication used by human beings  [49] . However, gesturing is not exclusive to hand movements; it can be done using face motion or even simple gazing. Therefore, researchers have attempted various approaches for controlling objects using symbolic hand gestures  [22, 38, 45, 52, 54] , deictic (pointing) hand gestures  [23, 25, 34] , eye gaze  [13, 24, 27, 31, 36, 37, 50, 51, 53] , and facial expressions  [3, 18, 46] . Specifically for the automotive domain, in-vehicle interaction has been attempted using hand gestures  [5, 16, 33, 40] , eye gaze  [36] , and facial expressions  [44] , while outside-the-vehicle interaction has been attempted using pointing gestures  [17, 41] , eye gaze  [24] , and head pose  [26, 30] . Although most of the previous methods focus on single-modality approaches while using a button or voice commands as event triggers, more recent work focused on multimodal fusion approaches to enhance performance. Roider et al.  [39]  used a simple rule-based multimodal fusion approach to enhance pointing gesture performance for invehicle object selection while driving by including passive gaze tracking. They were able to enhance the overall accuracy, but only when the confidence in the pointing gesture accuracy was low. When the confidence was high, their fusion approach would have a negative effect and lead to worse results. Similarly, Aftab et al.  [4]  used a more sophisticated deep-learning-based multimodal fusion approach combining gaze, pointing and head pose for the same task, but with more objects to select.\n\nWhile previous approaches, where both source and target are stationary, show great promise for in-vehicle interaction, they are not directly applicable to an outside-the-vehicle referencing task. This is because in this case, the source and the target are in constant relative motion. Moniri et al.  [32]  studied multimodal interaction in referencing objects outside the vehicle for passenger seat users using eye gaze, head pose, and pointing gestures and proposed an algorithm for the selection process. Similarly, Gomaa et al.  [19]  investigated a multimodal referencing approach using gaze and pointing gestures while driving and highlighted personal differences in behavior among drivers. They proposed a rule-based approach for fusion that exploits the differences in users' behavior by modality switching between gaze and pointing based on a specific user or situation. While both these methods align with the multimodal interaction paradigm of this paper, they do not strictly fuse the modalities together, but rather decide on which modality to use based on performance. They also do not consider time series analysis in their approaches, but rather treat referencing as a single-frame event.\n\nUnlike these previous methods, this paper focuses on utilizing both gaze and pointing modalities through explicit autonomous fusion methods. Our contributions can be summarized as follows: 1) we propose a machine-learning-based fusion approach and compare it with the rule-based fusion approaches used in previous methods; 2) we compare an early fusion approach with a late fusion one; 3) we compare single frame analysis with time series analysis; 4) we utilize individuals' different referencing behavior for a personalized fusion approach that outperforms a non-personalized one.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Design And Participants",
      "text": "We designed a within-subject counterbalanced experiment using the same setup (as seen in Figure  2 ) as Gomaa et al.  [19]  to collect data for the referencing task. We used OpenDS software  [29]  with a steering wheel, pedals, and speakers for the driving simulation; a state-of-the-art non-commercial hand tracking camera prototype specially designed for in-vehicle control for pointing tracking; and SMI Eye Tracking Glasses 1 for gaze tracking. For the driving task, we used the same 40-minute five-conditions star-shaped driving route with no traffic and a maximum driving speed of 60 km/hour as Gomaa et al.  [19] . The five counterbalanced conditions were divided into two levels of environment density (i.e., number of distractors around the Points of Interest (PoI)), two levels of distance from the road (i.e., near versus far PoI), and one autonomous driving condition. PoIs (e.g., Buildings) to reference are randomly located on the left and right of the road for all conditions and signaled to the driver in real-time. Drivers completed 24 referencing tasks per condition with a total of 124 gestures. These conditions were selected to implicitly increase the external validity of the experiment and the robustness of the learning models, however, there were not explicitly classified during model training. Finally, the PoIs' shapes and colors were selected during a pre-study to subtly contrast the surroundings and a pilot study was conducted to tune the design parameters.\n\nWe recruited 45 participants for the study. Six were excluded: one due to motion sickness and failure to complete the entire route, another due to incorrectly executing the task, and the other four due to technical problems with the tracking devices. The remaining 39 participants (17 males) had a mean age of 26 years (SD=6). Thirtytwo participants were right-handed while the rest were left-handed.  Early fusion is an end-to-end machine learning approach while late fusion requires pre-processing the pointing and gaze modalities first, and extracting final horizontal angles for both using the approach in  [19] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "Figure  3  shows our proposed fusion approaches. We compared using late and early fusion on the pointing and gaze modalities to determine the referenced object. We also performed a hybrid learning approach that combines unsupervised and supervised learning to predict the PoI's center angle; we first cluster participants based on pointing and gaze behavior and then we train our early fusion model on each cluster to boost the fusion performance per user.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Fusion Approach",
      "text": "We consider a common 1D polar coordinate system with horizontal angular coordinates only (as seen in Figure  1 ) to which both pointing and gaze tracking systems as well as the PoI ground truth (GT) are transformed (similar to  [19, 24] ). Since this is a continuous output case, we need to solve a regression problem. We consider three fusion approaches depending on the input feature type as follows.\n\n4.1.1 Late Fusion. The first approach involves pre-processing of the data, in which we transform the pointing and gaze raw data using the method in  [19] , then use these transformed horizontal angles as input features to our machine learning algorithm while using the PoI center angle as the target output (i.e., ground truth).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Early Fusion.",
      "text": "The second approach is an end-to-end solution where we use the raw 3D vectors outputted from the pointing tracker and the 1D x-axis pixel coordinates outputted from the gaze tracker as input features to the learning algorithm while the ground truth remains the same.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Hybrid Fusion.",
      "text": "The third approach is a hybrid approach that combines early fusion with a Gaussian Mixture Model (GMM)  [21]  clustering algorithm where the participants are divided into three clusters based on the accuracy of pointing and gaze (i.e., the distribution of pointing and gaze accuracy per participant as calculated in  [19] ). The three clusters are \"Participants with high pointing and high gaze accuracy\", \"Participants with high pointing and low gaze accuracy\", and \"Participants with low pointing and low gaze accuracy\"; there were no participants with high gaze and low pointing accuracy. Then, pointing and gaze input features are considered for each cluster of participants separately when applying the early fusion method to predict the GT angle. This approach can be considered as a semi-personalized approach that exploits individual referencing performance on a group of users.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Frame Selection Method",
      "text": "We use the pointing modality as the main trigger for the referencing action, and thus we have multiple frame selection approaches to fuse it with the gaze modality. We define them as follows:\n\n4.2.1 All Pointing, All Gaze. The first approach is to use all pointing and gaze frames as input modalities with the corresponding PoI center angle as GT. This produces, on average, five data points (i.e., frames) per PoI per user, resulting in an average of 620 data points per user that could be used for training or testing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mid",
      "text": "Pointing, Mid Gaze. The second approach is to use the middle frame of pointing and the middle frame of gaze as input modalities with the corresponding PoI center angle as GT. This approach produces one data point per PoI per user, resulting in 124 data points per user that could be used for training or testing.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mid",
      "text": "Pointing, First Gaze. The third and final approach is to use the middle frame of pointing as pointing modality input with the corresponding PoI center angle as GT. However, we will take the gaze modality frame at the time of the first pointing frame. The reason behind this approach is that there is evidence that drivers tend to glance at an object only at the very beginning of the pointing action then focus on the road  [19, 41] . This approach produces the same number of data points per user as the second approach.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset Split",
      "text": "The entire dataset consisting of 39 participants is split into training, validation, and test sets using nested cross-validation  [12]  approach with 4-fold inner and outer loops. In addition, the test data is further split when training and testing personalized models (see Figure  5 ). Note that all the splits are done on the participant level and not on the data point level. This way, no data from the same participant is used in training and testing for the same model, to ensure external validity and model generalization. Two models are implemented to satisfy generalization and personalization aspects as follows.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Universal Background Model (Ubm)",
      "text": ". This model follows the classic approach to machine learning: use training and validation data to create a prediction model with low generalization error reflected by its performance on the test data. A total of 29 participants are split into training and validation sets by the 4-fold inner loop of the nested cross-validation. It is then evaluated on the 10 participants selected for the test set by the 4-fold outer loop.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Personalized Model (Pm).",
      "text": "To train and test the personalization effect, we split the test data of each of the 10 participants into two halves, one that will be used as training data for personalization, and the other for personalization testing. Note that data points are shuffled before splitting for each participant. For each participant, we add the first half of the participant's samples to the training samples from the UBM (that corresponds to 29 participants) and we retrain a Personalized Model (PM) that was adapted for this participant. This trained model is tested on the second half of the same participant's data points as well as the data points of each of the other 9 test participants to verify the effect of personalization.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fusion Model",
      "text": "For each of the fusion approaches and different frame selection approaches, we use a machine learning model for learning. Specifically, we implement a Support Vector machine for Regression model (SVR)  [42] . We hypothesize that a deep learning model is not adequate in our use case due to the dataset's small size, however, we also compare different fusion models using deep learning approaches  [10, 20]  (e.g., Fully Connected Neural Network (FCNN), Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM)). Technical specifications for each model are as follows.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Machine",
      "text": "Learning Model: SVR. Support Vector Machines (SVM)  [15]  are a powerful machine learning algorithm based on the maximal margin and support vector classifier, which were first intended for linear binary classification. However, later extensions to the algorithms made it possible to train SVMs for multi-class classification and regression (SVR)  [42]  both linearly and non-linearly. In our setup, we the python-based Scikit-Learn  [35]  for SVR implementation with a non-linear radial basis function (RBF) kernel and epsilon value of two, while the remaining parameters remain default values. These values for the hyperparameters were chosen using 4-fold cross-validation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Deep",
      "text": "Learning Models: FCNN, CNN, and LSTM. Deep learning or Artificial Neural Networks  [10, 20]  are, in many regards, more powerful learning algorithms than traditional machine learning ones. However, they require much more training data to function optimally  [28] . We use the Python-based library Keras  [14]  for FCNN, CNN, and LSTM implementation. Due to the small size of the training dataset, we use a small number of layers for the neural network implementation to reduce the number of learnable parameters; the FCNN network consists of three hidden layers of size 32, 16, and 8 respectively, while non-linearity is introduced using the rectified linear unit (ReLU) activation function, and the 1D CNN network consists of one hidden convolutional layer of 64 filters and a kernel size of 3. Lastly, the LSTM network consists of one hidden layer having 50 hidden units. The output layer for all types of networks is a fully connected layer of output one and a linear activation function to work as regressors. Similar to the machine learning models, these architecture sizes (e.g., number of layers) and hyperparameters (e.g., filter and kernel sizes) were chosen using 4-fold cross-validation.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Performance Metrics",
      "text": "The most commonly used metric for regression problems is Mean Squared Error (MSE) or Root Mean Squared Error (RMSE), which would directly relate to the PoI center angle (i.e., GT). However, in our scenario, users might point (and gaze) at the edge of the PoI instead of the center, which still should be considered as correct referencing. For instance, a user might point at the edge of a wide building with a higher MSE than when pointing at the edge of a tall, narrow building. Therefore, we define another metric relating to the PoI width for performance comparison along with the RMSE.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Root Mean Square Error (Rmse).",
      "text": "Root Mean Square Error (RMSE) relates directly to the output angle. Figure  6  further illustrates this RMSE metric. The machine learning model attempts to predict a fused angle of pointing and gaze (ùúÉ ùëùùëüùëíùëëùëñùëêùë°ùëíùëë ) as close to the PoI center (i.e., ground truth) as possible. The difference between the predicted angle and the ground truth angle (ùúÉ ùëíùëüùëüùëúùëü ) is calculated for each data point and squared; then, the mean of all ùëÅ square errors is calculated, which results in the MSE. Finally, the RMSE is simply the square root of the MSE, which produces an average estimation of prediction error in degrees as seen in Equation  1\n\nFor example, if the resultant MSE of our prediction is 225, it means that the average error of the model prediction (i.e., RMSE) is 15 degrees away from the center of the PoI, so if we assume that we have a PoI's angular width of 40 degrees in visual space, it means that our model already predicts a referencing angle completely within the PoI. Note that a limitation of the RMSE metric is that it does not differentiate between positive and negative offset since it is squared during the MSE calculation. So, this 15-degree average error could be to the left or the right of the PoI center. which we, in theory, could assess our model performance. However, when interpreting this error, caution must be taken, since larger error values have a more severe impact for far PoIs versus near ones as far objects have much lower angular width (ùúÉ ùë§ùëñùëëùë°‚Ñé ) compared to closer objects. Thus, a relative metric would better describe the performance in comparison to an absolute one. Consequently, we introduced the Relative Distance-agnostic Error (RDE) metric, which divides the difference between the predicted angle and the ground truth angle (ùúÉ ùëíùëüùëüùëúùëü ) by half of the angular width ( ùúÉ ùë§ùëñùëëùë°‚Ñé 2 ) of the PoI on the screen at this time instance (since the vehicle is moving, the angular width changes with time). The resulting value then describes the error in multiples of half the PoI's width at each time instance. Thus, the MRDE is the average of the RDE for all ùëÅ time instances (i.e., data points) as seen in Equation  2 . Consequently, values less than one denote that the pointing vector lies within the PoI, while values larger than one denote that the referencing vector is outside the PoI's boundaries.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Mean Relative Distance-Agnostic Error (Mrde). The Previous Rmse Metric Returns An Average Absolute Error In Degrees With",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Middle Screen",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "L E F T S C E E N R Ig H T S C R E E N",
      "text": "While this metric allows a better interpretation of the severity of the referencing error in this study, it still does not yield a perfect absolute rating, since it now depends on the size of PoIs used in this study. For the task at hand, however, we consider this sufficient since PoI sizes did not vary greatly across the experiment.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "Results are divided into three sections. Section 5.1 shows and discusses the UBM results and the selection of the best fusion approach. We implement the UBM using all fusion approaches, frame selection methods, and fusion models to determine the optimum fusion combination that is used in the personalization approach. Section 5.2 discusses the PM results and the selection of additional hyperparameters that are concerned with personalization only. Section 5.3 discusses some further investigations on the pointing behavior quantitatively. Additionally, we illustrate the effect of multimodal fusion in comparison to single-modality approaches in all sections.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Universal Background Model (Ubm) Results",
      "text": "In this section, we first show the results of using a machine learning model, specifically SVR. Then, we compare against the FCNN and hybrid models. Lastly, we illustrate the effect of including data points in chronological order in the analysis (i.e., time series analysis) using CNN and LSTM.   [4, 19, 39, 40]  showing that while human interaction is multimodal in general, one modality would be dominant while the other modalities would be less relevant and their effect less significant in the referencing process.\n\nThe figures also illustrate the differences between possible pointing and gaze frame selection methods: while the performance difference is not statistically significant for any frame selection method over another, selecting the middle pointing frame and the first gaze frame still outperforms the other frame selection methods by an average of 2.65% and 4.64% for RMSE and MRDE respectively. This result also agrees with previous work  [19, 41]   usually glance at PoIs only at the beginning of the referencing task, while keeping their eyes on the road and pointing at the PoI during the remaining time of the referencing action.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison Against Fcnn And Hybrid Models.",
      "text": "Having chosen the \"middle pointing and first gaze\" frame selection approach as an optimum approach, Figure  8a  and Figure  8b  show the comparison between the SVR model and the FCNN model. Early fusion using the SVR model is superior to using the FCNN model for both RMSE and MRDE values with a relative average of 2.91% and 8.75% respectively. Additionally, both pointing and gaze single-modality prediction using SVR is on average more accurate than using FCNN for both RMSE and MRDE by 4.57% and 13.14% respectively. Since the entire training dataset consists of only 3596 data points (i.e., 29 participants with 124 data points per participant), we attribute the superiority of SVR over FCNN to the small dataset size in our use case. This is because deep neural networks generally require more data (even for the shallow FCNN used), but this remains to be tested in the future. Finally, the figures show a comparison between early fusion and hybrid fusion approaches. Fusion clusters (including high pointing accuracy, high gaze accuracy, or both) outperform the \"no GMM clustering\" approach for RMSE and MRDE by an average of 33.71% and 3.38% respectively, while the cluster where both modalities performed poorly (i.e., low pointing and low gaze accuracy) is degraded by 49.80% and 43.45% respectively. Moreover, for all clustering and no clustering cases, fusion models achieve better performance than single-modality ones. This means that the fusion model works better than the single best modality and does not get degraded by the other modality, unlike the rule-based fusion model in  [39]  that degrades with high pointing accuracy.\n\nNo  Fusion versus Hybrid Early fusion approach when using the SVR algorithm or FCNN against single modalities. The results are calculated using the \"middle pointing and first gaze\" frame selection method.\n\n5.1.3 Time Series Analysis using CNN and LSTM. Since both the CNN and LSTM algorithms require temporal information, the \"all pointing and all gaze\" frame selection method is used in analyses for both CNN and LSTM models as well as the previous SVR model for comparison. Figure  9a  and Figure  9b  show the results for these analyses while comparing the early fusion model to the hybrid fusion model as in the previous section. From the figures, it can be seen that while the temporal approach methods (i.e., CNN and LSTM) are almost equivalent for all cases in both RMSE and MRDE metrics, they are quite different from the non-temporal one (i.e., SVR). Although temporal models are slightly superior to the nontemporal model for RMSE values (Figure  9a ) with an average of 3.26%, they are inferior in terms of MRDE values (Figure  9b ) by an average of 7.33%, which suggests that a non-temporal approach is still the optimum approach. We hypothesize that the reason for that could also be the small dataset size, or it might be due to the fact that the temporal information for this task is too noisy or redundant to be useful for the fusion.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Personalized Model (Pm) Results",
      "text": "As seen in Figure  5  The results are calculated using the \"middle pointing and first gaze\" frame selection method.\n\nparticipants' data points' contribution to the retraining process significantly reduces the personalization effect. Therefore, we introduced a sampling weight hyperparameter that would increase the importance of newly added data points when retraining by putting more emphasis on these data points by the SVR regressor. Figure  10a  and Figure  10b  show the RMSE and MRDE results for the PM with the effect of this sampling weight, respectively, and they compare the PM to the UBM. It can be seen that the PM is superior to the UBM even if the personalized participant training data points (i.e., personalized samples) are weighted one-to-one with the other training participants' training data points (i.e., UBM samples); however, when giving more weight to the personalized participants' samples, the model adapts to the test data of these participants until we reach a point where the model starts overfitting and it does not perform well for this participant and the others. This point can be seen around a sampling weight ratio of one-to-fifteen in both Figure  10a  and Figure  10b , which shows a relative average enhancement for PM over UBM by 7.16% and 11.76% for RMSE and MRDE respectively. To further prove the personalization effect, it is tested against other participants that the model has not been personalized or adapted for. It can be seen from the same figures that, on average, PM works best for the personalized participant for all values of sample weight, with the largest gap around the oneto-fifteen optimum point. Further analysis on participants' level shows that at least 70% of the participants conform to these average figures and their referencing performance enhances with personalization. In conclusion, while there could be other personalization techniques that would enhance the referencing task performance further, these results show that our suggested PM outperforms a generalized one-model-fits-all approach.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Referencing Behavior Analysis",
      "text": "From Equation  2 , we defined relative distance-agnostic error as the division of the difference between the predicted angle and the ground truth angle (ùúÉ ùëíùëüùëüùëúùëü ) by half of the angular width ( ùúÉ ùë§ùëñùëëùë°‚Ñé 2 ). However, it was noticed that at the middle frame of pointing, most of the buildings have very small angular widths (ùúÉ ùë§ùëñùëëùë°‚Ñé ). Additionally, it was noticed that participants' average ground truth angle (ùúÉ ùê∫ùëá ) at the middle pointing frame was ¬±9.8 degrees (i.e., 9.8 degrees to the left or the right) and the ninety percent quantile of the ground truth angle was ¬±17.3 degrees. This means that most of the time participants did not wait for the building to get closer (a close building angle being around 35 to 45 degrees) and pointed rather quickly as soon as they identified it on the horizon. This is also confirmed by the average pointing time of 3.69 seconds reported in  [19] , which shows that the users point quite quickly when the building appears, rather than waiting, even though the building remains in their view for approximately 20 seconds.\n\nTo leverage from this observation, a slightly modified task was created removing the pointing attempts whose ground truth exceeded ¬±17.3 degrees (i.e., the 90% quantile threshold) from the dataset and analyzing only pointing at faraway buildings, since pointing at close buildings corresponds to only 10% of this dataset and might not be representative. Then the PM's average RMSE and MRDE values are enhanced from 14.53 and 1.81 respectively (for the original task) to 5.45 and 0.544 respectively (for the modified far-pointing only task). This could be evidence that users attempt to point more accurately and gaze more precisely when referencing faraway objects to identify them, while they tend to be less accurate and more careless when pointing at near objects. In a real car environment, this observation is quite useful, since the surrounding world is captured using the car exterior camera and PoI angular width and distance can be easily identified. Moreover, the data was further split based on the ¬±9.8 average pointing angle to compare the pointing behavior at very faraway buildings (below this average angle) and faraway buildings (above this average angle). The RMSE and MRDE values for both those cases were: 4.83 and 0.64 for the very far case, and 6.67 and 0.63 for the far case. This shows a correlation between the distance of the object from the user and the referencing behavior which supports the previously mentioned hypothesis that users point more accurately at more distant buildings as seen from the better RMSE value.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "Participants behave quite differently in multimodal interaction when referencing objects while driving. While previous work (such as  [19] ) showed pointing and gaze behavior and discussed the differences in users' pointing behavior quantitatively and qualitatively towards modality fusion, they did not investigate the implementation of the fusion model itself. In this paper, we focus more on leveraging this information and the implementation of a multimodal fusion model using both pointing and gaze to enhance the referencing task. We also compare such generalized models to a more user-specific personalized one that adapts to these differences in user behavior. Several machine learning algorithms have been trained and tested for the multimodal fusion approach and compared against the single-modality one. Also, different frame selection methods have been studied to assess the effect of time-series data on the performance when compared to single-frame methods.\n\nUBM results show that, on average, a multimodal fusion approach is more suitable than a single modality approach. They also show that a machine learning-based approach works better than deep learning, and time series analysis does not add much enhancement in performance compared to single-frame methods. However, this could be an artifact due to the dataset's small size, which we shall address in future work with longer experiment durations to collect more data. As for the personalization effect, non-weighted PMs show a slight enhancement over the UBM, which could be attributed to the low contribution ratio of the participants' data points. On the other hand, weighted PMs show a large enhancement in the referencing task performance for that participant due to the emphasis on their data points (i.e., individual behavior).\n\nFinally, the proposed fusion models, frame selection methods, and PM approaches enhance the referencing performance in comparison to both single-modality models and UBM. However, some aspects, like different pointing behavior based on the relative distance of the object to the user, are obscured through averaging over all data points, and this is not addressed in this study. In future work, these aspects require additional studies tailored towards this goal and further investigation to find a more concrete interpretation.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The Referencing Task. ùúÉùëùùë°represents the horizon-",
      "page": 1
    },
    {
      "caption": "Figure 2: Our driving simulator setup overview. The point-",
      "page": 2
    },
    {
      "caption": "Figure 2: ) as Gomaa et al. [19] to collect",
      "page": 2
    },
    {
      "caption": "Figure 3: System architecture showing possible fusion approaches. Early fusion is an end-to-end machine learning approach",
      "page": 3
    },
    {
      "caption": "Figure 3: shows our proposed fusion approaches. We compared us-",
      "page": 3
    },
    {
      "caption": "Figure 1: ) to which both",
      "page": 3
    },
    {
      "caption": "Figure 4: Input features with possible different frame selec-",
      "page": 4
    },
    {
      "caption": "Figure 4: summarizes these different input feature approaches",
      "page": 4
    },
    {
      "caption": "Figure 6: further illus-",
      "page": 4
    },
    {
      "caption": "Figure 5: Dataset split approach for UBM and PM.",
      "page": 5
    },
    {
      "caption": "Figure 6: An illustration showing example pointing and gaze",
      "page": 5
    },
    {
      "caption": "Figure 7: a and Fig-",
      "page": 6
    },
    {
      "caption": "Figure 7: RMSE and MRDE results comparing UBM‚Äôs Early",
      "page": 6
    },
    {
      "caption": "Figure 8: a and Figure 8b show the com-",
      "page": 6
    },
    {
      "caption": "Figure 8: RMSE and MRDE results comparing UBM‚Äôs Early",
      "page": 6
    },
    {
      "caption": "Figure 9: a and Figure 9b show the results for these",
      "page": 7
    },
    {
      "caption": "Figure 9: a) with an average of",
      "page": 7
    },
    {
      "caption": "Figure 5: , the PM is implemented by concatenating the",
      "page": 7
    },
    {
      "caption": "Figure 9: RMSE and MRDE results comparing UBM‚Äôs Early",
      "page": 7
    },
    {
      "caption": "Figure 10: a and Figure 10b show the RMSE and MRDE results for",
      "page": 7
    },
    {
      "caption": "Figure 10: a and Figure 10b, which shows a relative average",
      "page": 7
    },
    {
      "caption": "Figure 10: Fusion RMSE and MRDE results comparing PM",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "29 Training\nParticipants": "10 Test\nParticipants",
          "Plus P1 Train\nPlus P2 Train\nPlus P10 Train": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "BMW Natural Interaction unveiled at MWC",
      "year": "2019",
      "venue": "BMW Natural Interaction unveiled at MWC"
    },
    {
      "citation_id": "2",
      "title": "Mercedes-Benz presents the MBUX Hyperscreen at CES 2021",
      "year": "2021",
      "venue": "Mercedes-Benz presents the MBUX Hyperscreen at CES 2021"
    },
    {
      "citation_id": "3",
      "title": "Human-computer interaction using emotion recognition from facial expression",
      "authors": [
        "F Abdat",
        "C Maaoui",
        "A Pruski"
      ],
      "year": "2011",
      "venue": "Proceedings of the UKSim 5th European Symposium on Computer Modeling and Simulation"
    },
    {
      "citation_id": "4",
      "title": "You have a point there: object selection inside an automobile using gaze, head pose and finger pointing",
      "authors": [
        "Abdul Rafey",
        "Aftab Michael Von Der Beeck",
        "Michael Feld"
      ],
      "year": "2020",
      "venue": "Proceedings of the 22nd International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "5",
      "title": "Selection facilitation schemes for predictive touch with mid-air pointing gestures in automotive displays",
      "authors": [
        "I Bashar",
        "Chrisminder Ahmad",
        "Harpreet Hare",
        "Arber Singh",
        "Briana Shabani",
        "Lee Lindsay",
        "Patrick Skrypchuk",
        "Simon Langdon",
        "Godsill"
      ],
      "year": "2018",
      "venue": "Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications"
    },
    {
      "citation_id": "6",
      "title": "Multimodal fusion for multimedia analysis: A survey",
      "authors": [
        "M Pradeep K Atrey",
        "Abdulmotaleb Anwar Hossain",
        "Mohan Saddik",
        "Kankanhalli"
      ],
      "year": "2010",
      "venue": "Multimedia Systems"
    },
    {
      "citation_id": "7",
      "title": "Multimodal machine learning: A survey and taxonomy",
      "authors": [
        "Tadas Baltru≈°aitis",
        "Chaitanya Ahuja",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "8",
      "title": "Towards intelligent data analytics: A case study in driver cognitive load classification",
      "authors": [
        "Shaibal Barua",
        "Mobyen Uddin Ahmed",
        "Shahina Begum"
      ],
      "year": "2020",
      "venue": "Brain Sciences"
    },
    {
      "citation_id": "9",
      "title": "Local detectors and compact descriptors for visual search: A quantitative comparison",
      "authors": [
        "S Bianco",
        "D Mazzini",
        "D Pau",
        "R Schettini"
      ],
      "year": "2015",
      "venue": "Digital Signal Processing: A Review Journal"
    },
    {
      "citation_id": "10",
      "title": "Pattern Recognition and Machine Learning",
      "authors": [
        "M Christopher",
        "Bishop"
      ],
      "year": "2006",
      "venue": "Pattern Recognition and Machine Learning"
    },
    {
      "citation_id": "11",
      "title": "Ultrahapticons: \"Haptifying\" drivers' mental models to transform automotive mid-air haptic gesture infotainment interfaces",
      "authors": [
        "Eddie Brown",
        "David Large",
        "Hannah Limerick",
        "Gary Burnett"
      ],
      "year": "2020",
      "venue": "Proceedings of the 12th International Conference on Automotive User Interfaces and Interactive Vehicular Applications"
    },
    {
      "citation_id": "12",
      "title": "Cross-validation methods",
      "authors": [
        "W Michael",
        "Browne"
      ],
      "year": "2000",
      "venue": "Journal of Mathematical Psychology"
    },
    {
      "citation_id": "13",
      "title": "Smooth Gaze: A framework for recovering tasks across devices using eye tracking",
      "authors": [
        "Shiwei Cheng",
        "Jing Fan",
        "Anind Dey"
      ],
      "year": "2018",
      "venue": "Personal and Ubiquitous Computing"
    },
    {
      "citation_id": "14",
      "title": "Fran√ßois Chollet and others",
      "year": "2015",
      "venue": "Keras"
    },
    {
      "citation_id": "15",
      "title": "Support-vector networks",
      "authors": [
        "Corinna Cortes",
        "Vladimir Vapnik"
      ],
      "year": "1995",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "16",
      "title": "Designing a user-defined gesture vocabulary for an in-vehicle climate control system",
      "authors": [
        "Hessam Jahani Fariman",
        "J Hasan",
        "Manolya Alyamani",
        "Len Kavakli",
        "Hamey"
      ],
      "year": "2016",
      "venue": "Proceedings of the 28th Australian Computer-Human Interaction Conference"
    },
    {
      "citation_id": "17",
      "title": "Driver queries using wheel-constrained finger pointing and 3-D head-up display visual feedback",
      "authors": [
        "Kikuo Fujimura",
        "Lijie Xu",
        "Cuong Tran",
        "Rishabh Bhandari",
        "Victor Ng-Thow-Hing"
      ],
      "year": "2013",
      "venue": "Proceedings of the 5th International Conference on Automotive User Interfaces and Interactive Vehicular Applications"
    },
    {
      "citation_id": "18",
      "title": "Human-Robot Facial Expression Reciprocal Interaction Platform: Case Studies on Children with Autism",
      "authors": [
        "Ali Ghorbandaei Pour",
        "Alireza Taheri",
        "Minoo Alemi",
        "Ali Meghdari"
      ],
      "year": "2018",
      "venue": "International Journal of Social Robotics"
    },
    {
      "citation_id": "19",
      "title": "Studying person-specific pointing and gaze behavior for multimodal referencing of outside objects from a moving vehicle",
      "authors": [
        "Amr Gomaa",
        "Guillermo Reyes",
        "Alexandra Alles",
        "Lydia Rupp",
        "Michael Feld"
      ],
      "year": "2020",
      "venue": "Proceedings of the 22nd International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "20",
      "title": "Deep Learning",
      "authors": [
        "Ian Goodfellow",
        "Yoshua Bengio",
        "Aaron Courville"
      ],
      "year": "2016",
      "venue": "Deep Learning"
    },
    {
      "citation_id": "21",
      "title": "Mixture models: Inference and applications to clustering",
      "authors": [
        "D Hand",
        "G Mclachlan",
        "K Basford"
      ],
      "year": "1989",
      "venue": "Applied Statistics"
    },
    {
      "citation_id": "22",
      "title": "Hand gesture recognition for human computer interaction",
      "authors": [
        "Aashni Haria",
        "Archanasri Subramanian",
        "Nivedhitha Asokkumar",
        "Shristi Poddar",
        "Jyothi Nayak"
      ],
      "year": "2017",
      "venue": "Procedia Computer Science"
    },
    {
      "citation_id": "23",
      "title": "Human-computer interaction using pointing gesture based on an adaptive virtual touch screen",
      "authors": [
        "Pan Jing",
        "Guan Ye-Peng"
      ],
      "year": "2013",
      "venue": "International Journal of Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Do you see what I see: Towards a gaze-based surroundings query processing system",
      "authors": [
        "Shinjae Kang",
        "Byungjo Kim",
        "Sangrok Han",
        "Hyogon Kim"
      ],
      "year": "2015",
      "venue": "Proceedings of the 7th International Conference on Automotive User Interfaces and Interactive Vehicular Applications"
    },
    {
      "citation_id": "25",
      "title": "Real-time pointing gesture recognition for an immersive environment",
      "authors": [
        "Roland Kehl",
        "Luc Van Gool"
      ],
      "year": "2004",
      "venue": "Proceedings of the 6th International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "26",
      "title": "Identification of the driver's interest point using a head pose trajectory for situated dialog systems",
      "authors": [
        "Young-Ho Kim",
        "Teruhisa Misu"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "27",
      "title": "Looking and acting: Vision and eye movements in natural behaviour",
      "authors": [
        "Michael Land",
        "Benjamin Tatler"
      ],
      "year": "2009",
      "venue": "Looking and acting: Vision and eye movements in natural behaviour"
    },
    {
      "citation_id": "28",
      "title": "Deep learning: A critical appraisal",
      "authors": [
        "Gary Marcus"
      ],
      "year": "2018",
      "venue": "Deep learning: A critical appraisal",
      "arxiv": "arXiv:1801.00631"
    },
    {
      "citation_id": "29",
      "title": "OpenDS: A new open-source driving simulator for research. GMM-Fachbericht-AmE 2013",
      "authors": [
        "Rafael Math",
        "Angela Mahr",
        "Mohammad Moniri",
        "Christian M√ºller"
      ],
      "year": "2013",
      "venue": "OpenDS: A new open-source driving simulator for research. GMM-Fachbericht-AmE 2013"
    },
    {
      "citation_id": "30",
      "title": "Situated language understanding at 25 miles per hour",
      "authors": [
        "Teruhisa Misu",
        "Antoine Raux",
        "Rakesh Gupta",
        "Ian Lane"
      ],
      "year": "2014",
      "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue. ACL"
    },
    {
      "citation_id": "31",
      "title": "Gaze and Peripheral Vision Analysis for Human-Environment Interaction: Applications in Automotive and Mixed-Reality Scenarios",
      "authors": [
        "Mohammad Mehdi"
      ],
      "year": "2018",
      "venue": "Gaze and Peripheral Vision Analysis for Human-Environment Interaction: Applications in Automotive and Mixed-Reality Scenarios"
    },
    {
      "citation_id": "32",
      "title": "Multimodal reference resolution for mobile spatial interaction in urban environments",
      "authors": [
        "Mohammad Mehdi",
        "Christian M√ºller"
      ],
      "year": "2012",
      "venue": "Proceedings of the 4th International Conference on Automotive User Interfaces and Interactive Vehicular Applications"
    },
    {
      "citation_id": "33",
      "title": "Combining speech, gaze, and micro-gestures for the multimodal control of in-car functions",
      "authors": [
        "Robert Ne√üelrath",
        "Mohammad Mehdi Moniri",
        "Michael Feld"
      ],
      "year": "2016",
      "venue": "Proceedings of the 12th International Conference on Intelligent Environments"
    },
    {
      "citation_id": "34",
      "title": "3D-tracking of head and hands for pointing gesture recognition in a human-robot interaction scenario",
      "authors": [
        "Kai Nickel",
        "Edgar Scemann",
        "Rainer Stiefelhagen"
      ],
      "year": "2004",
      "venue": "Proceedings of the 6th International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "35",
      "title": "Scikit-learn: Machine learning in Python",
      "authors": [
        "Pedregosa",
        "Varoquaux",
        "V Gramfort",
        "B Michel",
        "O Thirion",
        "M Grisel",
        "P Blondel",
        "R Prettenhofer",
        "V Weiss",
        "J Dubourg",
        "Vanderplas",
        "D Passos",
        "M Cournapeau",
        "M Brucher",
        "Perrot",
        "Duchesnay"
      ],
      "year": "2011",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "36",
      "title": "Gaze-based interaction on multiple displays in an automotive environment",
      "authors": [
        "Tony Poitschke",
        "Florian Laquai",
        "Stilyan Stamboliev",
        "Gerhard Rigoll"
      ],
      "year": "2011",
      "venue": "Proceedings of the International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "37",
      "title": "Eye movements and attention in reading, scene perception, and visual search",
      "authors": [
        "Keith Rayner"
      ],
      "year": "2009",
      "venue": "Quarterly Journal of Experimental Psychology"
    },
    {
      "citation_id": "38",
      "title": "The design of hand gestures for human-computer interaction: Lessons from sign language interpreters",
      "authors": [
        "David Rempel",
        "Matt Camilleri",
        "David Lee"
      ],
      "year": "2014",
      "venue": "International Journal of Human Computer Studies"
    },
    {
      "citation_id": "39",
      "title": "I see your point: Integrating gaze to enhance pointing gesture accuracy while driving",
      "authors": [
        "Florian Roider",
        "Tom Gross"
      ],
      "year": "2018",
      "venue": "Proceedings of the 10th International Conference on Automotive User Interfaces and Interactive Vehicular Applications"
    },
    {
      "citation_id": "40",
      "title": "The effects of situational demands on gaze, speech and gesture input in the vehicle",
      "authors": [
        "Florian Roider",
        "Sonja R√ºmelin",
        "Bastian Pfleging",
        "Tom Gross"
      ],
      "year": "2017",
      "venue": "Proceedings of the 9th International Conference on Automotive User Interfaces and Interactive Vehicular Applications"
    },
    {
      "citation_id": "41",
      "title": "Free-hand pointing for identification and interaction with distant objects",
      "authors": [
        "Sonja R√ºmelin",
        "Chadly Marouane",
        "Andreas Butz"
      ],
      "year": "2013",
      "venue": "Proceedings of the 5th International Conference on Automotive User Interfaces and Interactive Vehicular Applications"
    },
    {
      "citation_id": "42",
      "title": "Shrinking the tube: A new support vector regression algorithm",
      "authors": [
        "Bernhard Scholkopf",
        "L Peter",
        "Alex Bartlett",
        "Robert Smola",
        "Williamson"
      ],
      "year": "1999",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Plant species classification using flower images -A comparative study of local feature representations",
      "authors": [
        "Marco Seeland",
        "Michael Rzanny",
        "Nedal Alaqraa",
        "Jana W√§ldchen",
        "Patrick M√§der"
      ],
      "year": "2017",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "44",
      "title": "Multimodal inference for driver-vehicle interaction",
      "authors": [
        "Ian Tevfik Metin Sezgin",
        "Peter Davies",
        "Robinson"
      ],
      "year": "2009",
      "venue": "Proceedings of the 11th International Conference on Multimodal Interfaces"
    },
    {
      "citation_id": "45",
      "title": "Recognizing hand gestures for human computer interaction",
      "authors": [
        "Dushyant Kumar"
      ],
      "year": "2015",
      "venue": "Proceedings of the International Conference on Communications and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "A survey on autonomous techniques for music classification based on human emotions recognition",
      "authors": [
        "Sachin Singh",
        "Niraj Pratap Singh",
        "Deepti Chaudhary"
      ],
      "year": "2020",
      "venue": "International Journal of Computing and Digital Systems"
    },
    {
      "citation_id": "47",
      "title": "Early versus late fusion in semantic video analysis",
      "authors": [
        "G Cees",
        "Marcel Snoek",
        "Arnold Worring",
        "Smeulders"
      ],
      "year": "2005",
      "venue": "Proceedings of the 13th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "48",
      "title": "Effects of sensory modality and task duration on performance, workload, and stress in sustained attention",
      "authors": [
        "James Szalma",
        "Joel Warm",
        "Gerald Matthews",
        "William Dember",
        "Ernest Weiler",
        "Ashley Meier",
        "F Eggemeier"
      ],
      "year": "2004",
      "venue": "Human Factors"
    },
    {
      "citation_id": "49",
      "title": "A neural network model for development of reaching and pointing based on the interaction of forward and inverse transformations",
      "authors": [
        "Naohiro Takemura",
        "Toshio Inui",
        "Takao Fukui"
      ],
      "year": "2018",
      "venue": "Developmental Science"
    },
    {
      "citation_id": "50",
      "title": "Detection of smooth pursuits using eye movement shape features",
      "authors": [
        "M√©lodie Vidal",
        "Andreas Bulling",
        "Hans Gellersen"
      ],
      "year": "2012",
      "venue": "Proceedings of the Symposium on Eye Tracking Research and Applications"
    },
    {
      "citation_id": "51",
      "title": "Pursuits: spontaneous interaction with displays based on smooth pursuit eye movement and moving targets",
      "authors": [
        "M√©lodie Vidal",
        "Andreas Bulling",
        "Hans Gellersen"
      ],
      "year": "2013",
      "venue": "Proceedings of the International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "52",
      "title": "Hand-free gesture recognition for vehicle infotainment system control",
      "authors": [
        "Qi Ye",
        "Lanqing Yang",
        "Guangtao Xue"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Vehicular Networking Conference"
    },
    {
      "citation_id": "53",
      "title": "Look but don't stare: Mutual gaze interaction in social robots",
      "authors": [
        "Yanxia Zhang",
        "Jonas Beskow",
        "Hedvig Kjellstr√∂m"
      ],
      "year": "2017",
      "venue": "Social Robotics"
    },
    {
      "citation_id": "54",
      "title": "Implementation and evaluation of touch and gesture interaction modalities for in-vehicle infotainment systems",
      "authors": [
        "Dan Zhao",
        "Cong Wang",
        "Yue Liu",
        "Tong Liu"
      ],
      "year": "2019",
      "venue": "Image and Graphics"
    }
  ]
}