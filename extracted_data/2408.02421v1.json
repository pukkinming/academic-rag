{
  "paper_id": "2408.02421v1",
  "title": "Fe-Adapter: Adapting Image-Based Emotion Classifiers To Videos",
  "published": "2024-08-05T12:27:28Z",
  "authors": [
    "Shreyank N Gowda",
    "Boyan Gao",
    "David A. Clifton"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Utilizing large pre-trained models for specific tasks has yielded impressive results. However, fully finetuning these increasingly large models is becoming prohibitively resource-intensive. This has led to a focus on more parameterefficient transfer learning, primarily within the same modality. But this approach has limitations, particularly in video understanding where suitable pre-trained models are less common. Addressing this, our study introduces a novel cross-modality transfer learning approach from images to videos, which we call parameter-efficient image-to-video transfer learning. We present the Facial-Emotion Adapter (FE-Adapter), designed for efficient fine-tuning in video tasks. This adapter allows pre-trained image models, which traditionally lack temporal processing capabilities, to analyze dynamic video content efficiently. Notably, it uses about 15 times fewer parameters than previous methods, while improving accuracy. Our experiments in video emotion recognition demonstrate that the FE-Adapter can match or even surpass existing fine-tuning and video emotion models in both performance and efficiency. This breakthrough highlights the potential for cross-modality approaches in enhancing the capabilities of AI models, particularly in fields like video emotion analysis where the demand for efficiency and accuracy is constantly rising.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Deep learning has risen to prominence as a transformative technology, delivering groundbreaking achievements in multiple fields. It has revolutionized tasks ranging from image recognition  [12] ,  [7] , emotion recognition  [25] ,  [1] ,  [6]  and action recognition  [2] , pushing the limits of machine capabilities. These significant improvements in precision have altered the landscape of numerous industries, providing innovative solutions to problems that have persisted for years.\n\nThe rapid evolution of self-supervised models in machine learning, particularly in the domains of image  [30] ,  [28]  and language processing, has marked a significant advancement in the field. These models have demonstrated remarkable proficiency in a wide range of downstream tasks, setting new benchmarks across various applications. However, as these models grow in size, the traditional method of fully fine-tuning them for specific tasks becomes increasingly untenable  [13]  and computationally expensive  [9] . This is predominantly due to the exorbitant computational costs and storage requirements associated with such processes.\n\nRecognizing this challenge, the focus of recent research has shifted towards more efficient methods of transfer learning. The objective is to leverage the knowledge captured in these pre-trained models while minimizing the additional resources required for adaptation to new tasks  [13] ,  [27] ,  [8] . Existing strategies predominantly concentrate on tasks within the same modality as the pre-trained model, such as image processing  [23] ,  [35] . This approach, while effective, Fig.  1 . A comparative analysis of various video-based models on the DFEW  [15]  dataset, showcasing the correlation between the number of tunable parameters (in millions) and model accuracy (%). Our proposed FE-Adapter (highlighted in blue and bold), requires significantly fewer trainable parameters whilst outperforming recent SOTA models including vision-language models. The size of each bubble represents the number of tuneable parameters of the respective model. We compare with recent SOTA models such as IAL  [19] , EST  [24] , and DFER-CLIP  [35] . We also compare against older 3D based models such as 3D-ResNet  [12]  and C3D  [31] .\n\nhas inherent limitations, especially in modalities like video understanding where equivalent pre-trained models are either scarce or non-existent.\n\nThe introduction of vision transformers (ViTs)  [7]  has resulted in significant improvements in general computer vision problems. However, ViTs often perform inferiorly compared to CNNs due to a lack of inductive bias and a tendency to focus on occlusions and noisy areas  [1] . Specific approaches to improving ViTs for facial emotion recognition includes using attentional selective fusion  [25] , squeeze and excitation blocks  [1]  or neural resizing  [14]  among other approaches. Nonetheless, the application of ViTs in videos is very scarce.\n\nIn this context, our study introduces a novel concept in cross-modality transfer learning, explicitly transitioning from facial-image emotion understanding to video emotion understanding. We present the Facial-emotion Adapter (FE-Adapter), a bespoke solution designed for efficient finetuning in video-related tasks. The FE-Adapter stands out with its compact and effective architecture. It enables pre-trained image models, which inherently lack the capability to process temporal dynamics, to interpret and analyze video content effectively. Figure  1  highlights the significant difference in tuneable parameters (size of bubble and x-axis) whilst improving upon the state-of-the-art. 979-8-3503-9494-8/24/$31.00 ©2024 IEEE One of the most compelling aspects of the FE-Adapter is its parameter efficiency. It requires approximately 8% of the parameters per task (in comparison to current stateof-the-art models), translating to about 15 times fewer updated parameters than the state-of-the-art  [35] . This feature addresses the critical challenge of resource-intensive finetuning in large models. Through our extensive experiments in the realm of video emotion recognition, the FE-Adapter has demonstrated its ability to not only match but, in some instances, surpass both the traditional comprehensive finetuning approaches and the latest models dedicated to video emotion. This achievement is particularly noteworthy as it maintains the dual benefits of parameter efficiency and adaptability, making the FE-Adapter an invaluable tool in the field of video emotion recognition, where efficiency and flexibility are paramount.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. Emotion Recognition In Image",
      "text": "Emotion recognition in images has progressed significantly with the advent of deep learning. Convolutional Neural Networks (CNNs) have been extensively utilized  [16] ,  [18]  for this task due to their ability to capture spatial hierarchies in facial features.\n\nImage classification models have been adapted for facial emotion recognition (FER), but challenges persist  [6]  in natural environments due to issues like pose variations, occlusions, and distracting backgrounds. To tackle these, attentionbased and region-based methods have emerged. Attentionbased techniques focus on the most informative parts of facial expressions, while region-based strategies segment the face into sub-regions using various cropping techniques to minimize occlusions and background interference.\n\nHowever, they often overlook the interconnectivity between these regions. Vision Transformers (ViTs)  [7]  have brought notable advancements in computer vision, but they sometimes fall short in comparison to CNNs, especially in handling occlusions and noisy areas. Methods to enhance ViTs for FER involve attentional selective fusion  [25] , squeeze and excitation blocks  [1] , and neural resizing  [14] . Yet, their application in video-based FER remains limited.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Emotion Recognition In Videos",
      "text": "Video emotion recognition (VER) analyzes emotions in videos by combining temporal and spatial data. It differs from image recognition, using 3D CNNs  [31] ,  [10]  to track spatial and temporal variations in facial expressions.\n\nVER faces challenges like pose changes, lighting, and obstructions, exacerbated by motion blur and varying frame rates. Solutions include a convolutional spatial transformer and a temporal transformer  [34] , handling occlusions and varied poses. Other approaches process videos in short clips  [23] , employing clip-based encoders and emotional intensity networks for emotion detection, or using intensityaware losses  [19]  and Multi-Instance Learning  [32]  to manage imprecise labels and capture time-based relationships. Self-supervised pre-training  [30]  on large-scale data with a Transformer encoder is also used. DFER-CLIP  [35] , a visuallanguage model based on CLIP  [28] , specifically focuses on Dynamic Facial Expression Recognition (DFER), using a Transformer and textual descriptions to improve accuracy. While CLIP has been extensively adapted for video action recognition, its application in VER is less explored.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "C. Adapting Image Models To Video",
      "text": "The growing use of large pre-trained language models for various tasks has highlighted the importance of efficient tuning in NLP. This form of parameter-efficient tuning can be broadly categorized into two main approaches.\n\nFirst is the use of task-specific adapters  [13] ,  [11] , which are light modules added between the layers of a pre-trained model. For efficiency, only these adapters are updated during task fine-tuning, leaving the bulk of the pre-trained model's parameters unchanged.\n\nThe second approach is prompt tuning  [21] ,  [17] , where learnable tokens are added either at the model's input or intermediate layers, and only these tokens are adjusted for each task whilst obtaining impressive results.\n\nWhile approaches have been proposed  [27] ,  [8]  to adapt image models to videos for tasks in video understanding, to the best of our knowledge nothing has been proposed for video emotion recognition.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Preliminaries",
      "text": "To adapt highly-trained image models for video processing, it is essential to bridge the gap between still images and videos. We explore methods to aggregate image-based features for videos, with a focus on adapting Vision Transformers (ViTs) to video models like Timesformer  [4] .\n\nIn the simplest scenario, a pre-trained image model for video understanding involves aggregating frame features over time, such as through average pooling. Consider a video clip V defined as V ∈ R T ×H×W , where T , H, and W represent the number of frames, height, and width respectively. Each frame is divided into N = H×W P 2 patches of size P × P . These patches are flattened and projected into patch tokens Z t = [z 1 , . . . , z s , . . . , z N ], with z s ∈ R d and d = 3 × P 2 , for t = 1, . . . , T . Positional embeddings are added to this series of feature vectors, along with a trainable class token. Each sequence of N +1 tokens undergoes self-attention processing, retaining only the classification token z t cls for each sequence. Temporal average pooling is then applied to these class tokensz final = 1 T t z t cls , to form a condensed representation for the entire clip. The final prediction is derived from a classifier using z final . This method, known as the Space-Only TimeSformer  [4] , averages spatial information over time.\n\nApplying ViTs to the video domain involves enhancing spatio-temporal attention with additional temporal layers  [2] ,  [4] ,  [5] . While these models achieve higher accuracy, they typically require full fine-tuning for each task. This approach is inefficient in terms of parameters as it demands maintaining separate fully fine-tuned model parameters for every task.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Adapters",
      "text": "Adapter modules, initially prominent in natural language processing (NLP), have been increasingly integrated into computer vision applications  [13] . These modules are particularly known for their parameter efficiency, which means they add a relatively small number of trainable parameters to a pre-existing model to tailor it for specific tasks. This approach allows for adapting models to new tasks or datasets without the need to retrain the entire model, thus saving computational resources and time.\n\nIn the context of computer vision, adapter modules are inserted into pre-trained neural networks. These networks, designed for general tasks, are fine-tuned to perform specific vision tasks such as object detection, image segmentation, or image classification. The core idea behind adapters is to make these networks more flexible and efficient in learning task-specific features.\n\nGiven an input feature matrix X ∈ R N ×d at the i-th layer of a neural network, the adaptation process performed by the adapter module can be formally defined as:\n\nwhere W down ∈ R d×r denotes the down-projection layer, W up ∈ R r×d the up-projection layer, and f (•) represents the activation function. The down-projection layer reduces the dimensionality of the input features X to a lowerdimensional space R r , aiming to capture the most relevant information for the task at hand. The up-projection layer then projects these features back to the original high-dimensional space R d . This process is designed to enrich the original features with task-specific information. A residual connection adds the output of the adapter module to the original input features X, ensuring that the information present in the initial features is not lost, thereby conserving the input information.\n\nThe features X referred to in this context are typically the embeddings or representations extracted from the i-th layer of the neural network. These embeddings encode the input data (e.g., images) into a form that the network can process to perform tasks like classification or detection. By applying adapter modules to these embeddings, the network can learn to adjust its representations to better suit specific tasks or datasets, enhancing performance with minimal additional trainable parameters.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Fe-Adapter",
      "text": "The Adapter module, notably the ST-Adapter  [27] , has been adapted for image models in video tasks. However, adapting facial image emotion models to videos remains unexplored. We propose using adapters as illustrated in Figure  2 .\n\nST-Adapter uses depth-wise 3D-convolution for spatiotemporal analysis. This method might not be ideal for video emotion recognition due to its limitations in capturing complex spatial and temporal interactions, as it processes each channel independently, potentially overlooking emotional cues in facial expressions and body language. Depth-wise convolutions might not integrate spatial and temporal patterns effectively, which are crucial for emotion recognition.\n\nWe propose Dynamic Dilated Convolutions (D 2 Conv3D)  [29]  for enhanced performance in video emotion recognition tasks. The dynamic dilation rates of D 2 Conv3D are well-suited for capturing intricate spatiotemporal patterns, especially in subtle facial emotions and temporal dynamics. This makes it more suitable for accurate emotion detection. The enhanced feature extraction capabilities of D 2 Conv3D make it ideal for analyzing nuanced emotional cues in videos. We define our FE-Adapter as:\n\nOur Ablation Study highlights the importance of this choice for emotion recognition. In integrating the FE-Adapter for video emotion recognition, similar to adapters in NLP placed between Transformer layers  [13] , various strategies are considered. Typically in NLP, adapters are positioned after both the Multi-Head Self-Attention (MHSA) and the Multi-Layer Perceptron (MLP). Some studies  [13] ,  [27]  suggest that a single adapter post-MLP is effective.\n\nFor the FE-Adapter, we explore similar placements. Empirical evidence suggests that positioning a single FE-Adapter before the MHSA in each transformer block can enhance performance, allowing the FE-Adapter to preprocess input features effectively before the MHSA, and optimizing emotion recognition.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experimental Analysis A. Datasets",
      "text": "For our video emotion recognition task, we utilize a range of datasets, each offering unique characteristics and Dataset DFEW  [15]  FERV39k  [33]  MAFW  [  challenges. These include DFEW  [15] , FERV39K  [33] , and MAFW  [22] . DFEW features 16,000 video clips across seven emotions, with 12,059 clips used for 5-fold cross-validation experiments. FERV39K provides 38,935 video sequences derived from four different scenarios, split into 31,088 for training and 7,847 for testing. MAFW, focusing on the video modality, comprises 10,045 video clips annotated with 11 emotions, using 9,172 clips in a 5-fold cross-validation setup. These diverse datasets allow for comprehensive training and evaluation of our emotion recognition models and also is commonly used in current state-of-the-art models.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Implementation Details",
      "text": "In our experiments, we employ PF-ViT based on the ViT-B architecture, re-implemented with identical hyperparameters for AffectNet classification. We adapt this model for video tasks, using one adapter per PF-ViT block. The training involves the AdamW optimizer with a learning rate of 5e-4 and weight decay of 1e-2, alongside a batch size of 128. We process 16 frames per video, resized to 224×224. The learning rate is decayed using cosine annealing over 100 epochs. All experiments are conducted using an NVIDIA A100 GPU.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Ablation Study 1) Choice Of Pre-Training Dataset:",
      "text": "We use PF-ViT finetuned on AffectNet  [26]  for adapting to video datasets. We also compare the use of RAF-DB  [20]  and FERPLUS  [3] . These results are shown in Table  I .\n\n2) Comparing Temporal Convolution Choices: In comparison to ST-Adapter, we use a D 2 Conv3D instead of DW-Conv3D. Based on this, we compare the performance of different choices including a simple temporal aggregation of frame-by-frame features. We see a significant improvement using D 2 Conv3D proving our hypothesis that discerns subtle facial emotions and temporal emotional dynamics. These results are in Table  II .  3) Adapter Position: We consider two scenarios in terms of where we position the adapter. We first consider the global position i.e. what blocks do we place the adapter in. We do this by breaking the ViT-B to three sub-blocks such as 1-4, 5-8 and 9-12. We see that using an adapter in every block gives us best results. These results are in Table  III .\n\nWe also consider where to place it within each block. With this we have a choice of placing it before the Multi-Head Self-Attention (MHSA) or after. We could also place it after the MLP. These results are shown in Table  IV .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Comparison To State-Of-The-Art",
      "text": "We closely examine the performance of our proposed FE-Adapter against various leading models in the domain of facial emotion recognition. Table V presents a comparative analysis of various methods in the context of facial emotion recognition using three popular benchmarks: DFEW, FERV39k, and MAFW. Each method is evaluated based on its performance in Unweighted Average Recall (UAR) and Weighted Average Recall (WAR) across these datasets. The table also includes the number of tunable parameters (in millions) for each method, indicating the complexity and potential computational demands.\n\nNotable methods listed include C3D  [31] , 3D ResNet-18  [10] , Former-DFER  [34] , CEFLNet  [23] , IAL  [19] , EST  [24] , M3DFEL  [32] , MAE-DFER  [30]  and DFER-CLIP  [35] . A key observation is the relatively low number of tunable parameters in the FE-Adapter method (6.6 million), which is significantly lower than most other methods.\n\nOverall, the table illustrates that FE-Adapter, despite its markedly lower complexity, competes well with or outperforms recent state-of-the-art models in facial emotion recognition across different benchmarks, highlighting its effectiveness and efficiency.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In conclusion, this study presents an innovative crossmodality transfer learning approach, namely parameterefficient image-to-video transfer learning, through the development of the Facial-Emotion Adapter (FE-Adapter). This novel approach addresses the challenges posed by the conventional method of fully fine-tuning large pre-trained models, which is resource-intensive in terms of training and storage. The FE-Adapter demonstrates the feasibility and effectiveness of using pre-trained image models for video understanding tasks. It does so by equipping these models with the capability to process dynamic video content, while significantly reducing the requirement for updated parameters -approximately 15 times fewer than previous methods. Our comprehensive experiments in video emotion recognition indicate that the FE-Adapter not only holds its own against but in some cases, outperforms both the traditional comprehensive fine-tuning methods and the latest models in this domain. Importantly, it achieves these results while upholding the advantages of parameter efficiency. This makes the FE-Adapter a highly valuable contribution to the field of video emotion recognition, offering a practical solution that combines efficiency with adaptability.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A comparative analysis of various video-based models on the",
      "page": 1
    },
    {
      "caption": "Figure 1: highlights the significant difference",
      "page": 1
    },
    {
      "caption": "Figure 2: Adapters ensure minimal parameter updates whilst keeping the",
      "page": 2
    },
    {
      "caption": "Figure 2: ST-Adapter uses depth-wise 3D-convolution for spatio-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "C3D [31]\n[ICCV’15]",
          "Tunable Params (M)": "78",
          "DFEW [15]\nUAR\nWAR": "42.74\n53.54",
          "FERV39k [33]\nUAR\nWAR": "22.68\n31.69",
          "MAFW [22]\nUAR\nWAR": "31.17\n42.25"
        },
        {
          "Method": "3D ResNet-18 [10]\n[ICCVW’17]",
          "Tunable Params (M)": "33",
          "DFEW [15]\nUAR\nWAR": "46.52\n58.27",
          "FERV39k [33]\nUAR\nWAR": "26.67\n37.57",
          "MAFW [22]\nUAR\nWAR": "-\n-"
        },
        {
          "Method": "Former-DFER [34]\n[MM’21]",
          "Tunable Params (M)": "18",
          "DFEW [15]\nUAR\nWAR": "53.69\n65.70",
          "FERV39k [33]\nUAR\nWAR": "37.20\n46.85",
          "MAFW [22]\nUAR\nWAR": "-\n-"
        },
        {
          "Method": "CEFLNet\n[23]\n[IS’2022]",
          "Tunable Params (M)": "13",
          "DFEW [15]\nUAR\nWAR": "51.14\n65.35",
          "FERV39k [33]\nUAR\nWAR": "-\n-",
          "MAFW [22]\nUAR\nWAR": "-\n-"
        },
        {
          "Method": "IAL [19]\n[AAAI’23]",
          "Tunable Params (M)": "19",
          "DFEW [15]\nUAR\nWAR": "55.71\n69.24",
          "FERV39k [33]\nUAR\nWAR": "35.82\n48.54",
          "MAFW [22]\nUAR\nWAR": "-\n-"
        },
        {
          "Method": "EST [24]\n[PR’23]",
          "Tunable Params (M)": "43",
          "DFEW [15]\nUAR\nWAR": "53.43\n65.85",
          "FERV39k [33]\nUAR\nWAR": "-",
          "MAFW [22]\nUAR\nWAR": "-\n-"
        },
        {
          "Method": "M3DFEL [32]\n[CVPR’23]",
          "Tunable Params (M)": "-",
          "DFEW [15]\nUAR\nWAR": "56.10\n69.25",
          "FERV39k [33]\nUAR\nWAR": "35.94\n47.67",
          "MAFW [22]\nUAR\nWAR": "-\n-"
        },
        {
          "Method": "MAE-DFER [30]\n[MM’23]",
          "Tunable Params (M)": "85",
          "DFEW [15]\nUAR\nWAR": "63.41\n74.43",
          "FERV39k [33]\nUAR\nWAR": "43.12\n52.07",
          "MAFW [22]\nUAR\nWAR": "41.62\n54.31"
        },
        {
          "Method": "DFER-CLIP [35]\n[BMVC’23]",
          "Tunable Params (M)": "90",
          "DFEW [15]\nUAR\nWAR": "59.61\n71.25",
          "FERV39k [33]\nUAR\nWAR": "41.27\n51.65",
          "MAFW [22]\nUAR\nWAR": "38.89\n52.55"
        },
        {
          "Method": "FE-Adapter (Ours)",
          "Tunable Params (M)": "6.6",
          "DFEW [15]\nUAR\nWAR": "60.89\n73.67",
          "FERV39k [33]\nUAR\nWAR": "41.44\n52.11",
          "MAFW [22]\nUAR\nWAR": "39.41\n55.02"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning vision transformer with squeeze and excitation for facial expression recognition",
      "authors": [
        "M Aouayeb",
        "W Hamidouche",
        "C Soladie",
        "K Kpalma",
        "R Seguier"
      ],
      "year": "2021",
      "venue": "Learning vision transformer with squeeze and excitation for facial expression recognition",
      "arxiv": "arXiv:2107.03107"
    },
    {
      "citation_id": "2",
      "title": "Vivit: A video vision transformer",
      "authors": [
        "A Arnab",
        "M Dehghani",
        "G Heigold",
        "C Sun",
        "M Lučić",
        "C Schmid"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF international conference on computer vision"
    },
    {
      "citation_id": "3",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM international conference on multimodal interaction"
    },
    {
      "citation_id": "4",
      "title": "Is space-time attention all you need for video understanding?",
      "authors": [
        "G Bertasius",
        "H Wang",
        "L Torresani"
      ],
      "year": "2021",
      "venue": "ICML"
    },
    {
      "citation_id": "5",
      "title": "Space-time mixing attention for video transformer",
      "authors": [
        "A Bulat",
        "J Perez Rua",
        "S Sudhakaran",
        "B Martinez",
        "G Tzimiropoulos"
      ],
      "year": "2021",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "A survey on facial emotion recognition techniques: A state-of-the-art literature review",
      "authors": [
        "F Canal",
        "T Müller",
        "J Matias",
        "G Scotton",
        "A De Sa Junior",
        "E Pozzebon",
        "A Sobieranski"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "7",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "8",
      "title": "Optimizing vivit training: Time and memory reduction for action recognition",
      "authors": [
        "S Gowda",
        "A Arnab",
        "J Huang"
      ],
      "year": "2023",
      "venue": "Optimizing vivit training: Time and memory reduction for action recognition",
      "arxiv": "arXiv:2306.04822"
    },
    {
      "citation_id": "9",
      "title": "Watt for what: Rethinking deep learning's energy-performance relationship",
      "authors": [
        "S Gowda",
        "X Hao",
        "G Li",
        "L Sevilla-Lara",
        "S Gowda"
      ],
      "year": "2023",
      "venue": "Watt for what: Rethinking deep learning's energy-performance relationship",
      "arxiv": "arXiv:2310.06522"
    },
    {
      "citation_id": "10",
      "title": "Learning spatio-temporal features with 3d residual networks for action recognition",
      "authors": [
        "K Hara",
        "H Kataoka",
        "Y Satoh"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE international conference on computer vision workshops"
    },
    {
      "citation_id": "11",
      "title": "Towards a unified view of parameter-efficient transfer learning",
      "authors": [
        "J He",
        "C Zhou",
        "X Ma",
        "T Berg-Kirkpatrick",
        "G Neubig"
      ],
      "year": "2021",
      "venue": "Towards a unified view of parameter-efficient transfer learning",
      "arxiv": "arXiv:2110.04366"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "13",
      "title": "Parameter-efficient transfer learning for nlp",
      "authors": [
        "N Houlsby",
        "A Giurgiu",
        "S Jastrzebski",
        "B Morrone",
        "Q De Laroussilhe",
        "A Gesmundo",
        "M Attariyan",
        "S Gelly"
      ],
      "year": "2019",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "14",
      "title": "Vision transformer equipped with neural resizer on facial expression recognition task",
      "authors": [
        "H Hwang",
        "S Kim",
        "W.-J Park",
        "J Seo",
        "K Ko",
        "H Yeo"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "15",
      "title": "Dfew: A large-scale database for recognizing dynamic facial expressions in the wild",
      "authors": [
        "X Jiang",
        "Y Zong",
        "W Zheng",
        "C Tang",
        "W Xia",
        "C Lu",
        "J Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "16",
      "title": "A brief review of facial emotion recognition based on visual information",
      "authors": [
        "B Ko"
      ],
      "year": "2018",
      "venue": "sensors"
    },
    {
      "citation_id": "17",
      "title": "The power of scale for parameter-efficient prompt tuning",
      "authors": [
        "B Lester",
        "R Al-Rfou",
        "N Constant"
      ],
      "year": "2021",
      "venue": "The power of scale for parameter-efficient prompt tuning",
      "arxiv": "arXiv:2104.08691"
    },
    {
      "citation_id": "18",
      "title": "Facial expression recognition via resnet-50",
      "authors": [
        "B Li",
        "D Lima"
      ],
      "year": "2021",
      "venue": "International Journal of Cognitive Computing in Engineering"
    },
    {
      "citation_id": "19",
      "title": "Intensity-aware loss for dynamic facial expression recognition in the wild",
      "authors": [
        "H Li",
        "H Niu",
        "Z Zhu",
        "F Zhao"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "21",
      "title": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "authors": [
        "X Liu",
        "K Ji",
        "Y Fu",
        "W Tam",
        "Z Du",
        "Z Yang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks",
      "arxiv": "arXiv:2110.07602"
    },
    {
      "citation_id": "22",
      "title": "Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild",
      "authors": [
        "Y Liu",
        "W Dai",
        "C Feng",
        "W Wang",
        "G Yin",
        "J Zeng",
        "S Shan"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "23",
      "title": "Clipaware expressive feature learning for video-based facial expression recognition",
      "authors": [
        "Y Liu",
        "C Feng",
        "X Yuan",
        "L Zhou",
        "W Wang",
        "J Qin",
        "Z Luo"
      ],
      "year": "2022",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "24",
      "title": "Expression snippet transformer for robust video-based facial expression recognition",
      "authors": [
        "Y Liu",
        "W Wang",
        "C Feng",
        "H Zhang",
        "Z Chen",
        "Y Zhan"
      ],
      "year": "2023",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "25",
      "title": "Facial expression recognition with visual transformers and attentional selective fusion",
      "authors": [
        "F Ma",
        "B Sun",
        "S Li"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "St-adapter: Parameterefficient image-to-video transfer learning",
      "authors": [
        "J Pan",
        "Z Lin",
        "X Zhu",
        "J Shao",
        "H Li"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "28",
      "title": "Learning transferable visual models from natural language supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "C Hallacy",
        "A Ramesh",
        "G Goh",
        "S Agarwal",
        "G Sastry",
        "A Askell",
        "P Mishkin",
        "J Clark"
      ],
      "year": "2021",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "29",
      "title": "d: Dynamic dilated convolutions for object segmentation in videos",
      "authors": [
        "C Schmidt",
        "A Athar",
        "S Mahadevan",
        "B Leibe"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision"
    },
    {
      "citation_id": "30",
      "title": "Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition",
      "authors": [
        "L Sun",
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "31",
      "title": "Learning spatiotemporal features with 3d convolutional networks",
      "authors": [
        "D Tran",
        "L Bourdev",
        "R Fergus",
        "L Torresani",
        "M Paluri"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "32",
      "title": "Rethinking the learning paradigm for dynamic facial expression recognition",
      "authors": [
        "H Wang",
        "B Li",
        "S Wu",
        "S Shen",
        "F Liu",
        "S Ding",
        "A Zhou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "Ferv39k: A large-scale multi-scene dataset for facial expression recognition in videos",
      "authors": [
        "Y Wang",
        "Y Sun",
        "Y Huang",
        "Z Liu",
        "S Gao",
        "W Zhang",
        "W Ge",
        "W Zhang"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "34",
      "title": "Former-dfer: Dynamic facial expression recognition transformer",
      "authors": [
        "Z Zhao",
        "Q Liu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 29th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Prompting visual-language models for dynamic facial expression recognition",
      "authors": [
        "Z Zhao",
        "I Patras"
      ],
      "year": "2023",
      "venue": "The British Machine Vision Conference"
    }
  ]
}