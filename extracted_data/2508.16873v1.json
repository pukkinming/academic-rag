{
  "paper_id": "2508.16873v1",
  "title": "Do Multimodal Llms See Sentiment?",
  "published": "2025-08-23T02:11:46Z",
  "authors": [
    "Neemias B. da Silva",
    "John Harrison",
    "Rodrigo Minetto",
    "Myriam R. Delgado",
    "Bogdan T. Nassu",
    "Thiago H. Silva"
  ],
  "keywords": [
    "Visual Sentiment Analysis",
    "Multimodal Large Language Models",
    "Computational Social Systems",
    "Social Media Analytics"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Understanding how visual content communicates sentiment is critical in an era where online interaction is increasingly dominated by this kind of media on social platforms. However, this remains a challenging problem, as sentiment perception is closely tied to complex, scene-level semantics. In this paper, we propose an original framework, MLLMsent, to investigate the sentiment reasoning capabilities of Multimodal Large Language Models (MLLMs) through three perspectives: (1) using those MLLMs for direct sentiment classification from images; (2) associating them with pre-trained LLMs for sentiment analysis on automatically generated image descriptions; and (3) fine-tuning the LLMs on sentiment-labeled image descriptions. Experiments on a recent and established benchmark demonstrate that our proposal, particularly the fine-tuned approach, achieves state-ofthe-art results outperforming Lexicon-, CNN-, and Transformerbased baselines by up to 30.9%, 64.8%, and 42.4%, respectively, across different levels of evaluators' agreement and sentiment polarity categories. Remarkably, in a cross-dataset test, without any training on these new data, our model still outperforms, by up to 8.26%, the best runner-up, which has been trained directly on them. These results highlight the potential of the proposed visual reasoning scheme for advancing affective computing, while also establishing new benchmarks for future research.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "V Isual sentiment analysis, or image sentiment analysis, seeks to automatically predict the emotions conveyed by visual content as perceived by human observers. The relevance of this problem has significant implications for computational social systems. It is rooted in the need to better understand collective behavior, public sentiment, and societal trends in digital environments, as social platforms increasingly rely on visual communication  [1] ,  [2] ; and images often express emotions more powerfully than text alone  [3] .\n\nAlthough recent studies have highlighted the importance of visual attributes in images such as color, texture, and shape for sentiment prediction  [4] , the inherent complexity and richness typically provided by images -where even subtle elements can alter emotional perception -make this a challenging problem. Lopes et al.  [5]  report that the integration of humanannotated textual tags -describing perceived elements such as the presence of nature, violence, or lack of maintenancecan substantially improve visual sentiment analysis, with Fscore gains of up to 35%. However, automatically extracting N. B. da Silva, J Harrison, R. Minetto, M. R. Delgado, B. T. Nassu, are with Universidade Tecnológica Federal do Paraná (UTFPR), Brazil. T. H. Silva is with the University of Toronto, Canada. E-mail: neemiasbuceli@alunos.utfpr.edu.br, {rminetto,btnassu,myriamdelg}@utfpr.edu.br, th.silva@utoronto.ca Manuscript received 2025, revised 2025.\n\nsuch subjective perceptions can be highly complex, as it demands a deep semantic understanding of the various elements and their interactions within the visual scene. The present work explores the visual reasoning capabilities of Multimodal Large Language Models (MLLMs) in the context of image sentiment analysis. While MLLMs offer the potential to capture rich semantic and affective cues that extend beyond traditional vision-only approaches  [6] ,  [7] ,  [8] , their effectiveness in accurately interpreting and classifying sentiment from visual content remains largely under-investigated. To fill this gap, we introduce a comprehensive framework, Multimodal Large Language Model for Sentiment (MLLMsent), aiming to address four central research questions: (Q1) How effective are MLLMs for direct sentiment classification from raw images? (Q2) Can we improve performance by transforming images into textual descriptions, and analyzing, through pre-trained LLMs, the sentiment of the automatically generated image descriptions? (Q3) How does fully fine-tuning LLMs on the sentiment-labeled image descriptions affect the performance? (Q4) How does our best MLLM-based method compare with traditional approaches (those built on Lexicon, CNNs, Transformers, and hand-crafted features) on target and cross-datasets?\n\nIn response to the research questions previously posed, we present the following key contributions:\n\n• We systematically evaluate three Multimodal Language Models (MLLMs) -MiniGPT-4  [9] , GPT-4o mini  [10] , and DeepSeek-VL2-Tiny  [11]  -two of which run locally and one via paid API, across multiple task settings including varying sentiment categories and agreement thresholds used to define target labels. • We investigate whether converting images into textual image descriptions via MLLMs improves sentiment classification, leveraging LLM text-based classifiers such as BART  [12] ,  [13] , ModernBERT  [14] , and LLAMA  [15] . • We assess the benefits of fine-tuning sentiment classifiers on each MLLM-generated image description, comparing their performance with the pre-trained counterparts. • We benchmark our proposals, including image-based, caption-based, and fine-tuned methods, and compare the best one against traditional CNNs, Transformers, and language-only models to understand their relative effectiveness in modeling the sentiment of visual social data within target and cross datasets. • We release the generated image descriptions for two datasets, PerceptSent and DeepSent, along with our finetuned models to ease future research and reproducibility  1  .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Textual Sentiment Analysis. The field of textual sentiment analysis has evolved significantly from early lexicon-based approaches  [16] ,  [17]  to sophisticated deep learning models. Traditional methods rely on sentiment dictionaries and rulebased systems  [18] ,  [19] , whereas modern approaches leverage neural architectures, such as BERT-based models  [20]  and graph neural networks  [21] ,  [22] . A recent work  [23]  demonstrates that transformers can capture aspect-level sentiment through double-view graph representations. The success of textual analysis has led to its application in multimodal contexts, such as video transcriptions  [24] . However, challenges remain when analyzing platforms like Instagram where visual content dominates and textual context is sparse  [25] .\n\nVisual Sentiment Analysis. Deep learning has revolutionized visual sentiment analysis, with convolutional neural networks (CNNs) playing a pivotal role. An early work  [26]  introduces probability sampling to reduce label noise, while  [27]  combines visual features with web-mined sentiment concepts. Subsequent advances incorporate attention mechanisms  [28]  and scene semantics  [29]  to improve performance. The importance of human perception in visual sentiment is highlighted in  [5]  -the work shows that incorporating evaluator judgments significantly improves model accuracy. These approaches typically rely on handcrafted visual features or CNNextracted representations, but struggle with the subjective nature of visual affect.\n\nMultimodal Approaches. Combining visual and textual modalities has emerged as a powerful paradigm for sentiment analysis  [30] . For low-resource languages,  [31]  demonstrates the effectiveness of late fusion strategies, whereas  [32]  proposes interaction networks to capture cross-modal dependencies. Recent work has introduced sophisticated architectures such as the multi-stage perception model in  [33]  and the dualperspective fusion network in  [34] , which better align visual and textual representations. These methods show that joint modeling of modalities can overcome limitations of unimodal approaches, particularly for ambiguous or context-dependent sentiment expressions. These advances in multimodal fusion models set the stage for a new paradigm, where visionlanguage understanding is embedded directly into large-scale language models.\n\nMultimodal LLMs. The emergence of Multimodal Large Language Models (MLLMs)  [8] ,  [6] ,  [7] ,  [35]  has expanded the possibilities for multimodal sentiment analysis in computational social systems. While traditional LLMs like GPT-4o and LLAMA-2  [36]  have advanced sentiment analysis  [37] , their text-only nature faces limitations in data scarcity  [38]  and modality constraints  [39] . To address these, MLLMs integrate visual encoders, enabling joint processing of images and text. Examples include DeepSeek-VL  [40] , DeepSeek-VL2  [11] , MiniGPT-4  [9] , and GPT-4o mini  [10] . Despite differences in scale and architecture, these models share the goal of enhancing LLMs with perceptual capabilities for tasks like visual sentiment analysis. Studies  [41] ,  [42]  suggest that MLLMs exhibit human-like perception, yet their accuracy in affective content classification remains an open question.\n\nThis paper builds upon these advances by systematically evaluating how MLLMs can address key challenges in visual sentiment analysis, particularly in handling subjective interpretations and bridging the semantic gap between visual content and affective responses. Our work extends current approaches by comparing different architectures and strategies for leveraging image-to-text transformations in sentiment prediction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Datasets And Problem Formulation",
      "text": "In the present study, we use two datasets: PerceptSent  [5]  and DeepSent  [26] . The former is a publicly available 2  collection of 5,000 images, most (92.7%) of which depict outdoor scenes. The images have been sourced from Instagram, Flickr, and NYC311  3  . Designed specifically for sentiment analysis, the dataset has been fully annotated by human evaluators using mainly Amazon Mechanical Turk 4  (AMT). The latter consists of 1,269 images collected from X (old Twitter), from indoor and outdoor scenes, which have also been labeled through AMT crowd-sourcing platform.\n\nTo align with prior work and facilitate reproducibility, we adopt the problem formulation for sentiment analysis proposed by Lopes  [5] . Denoted here as ⟨σ l , P C ⟩, we have C as the number of sentiment categories considered in a problem instance P C , and l as a threshold for filtering out divergent evaluators opinions, allowing us to define different types of sentiment dominance for filter σ.\n\nFormally, let s = (s 1 , ..., s C ) represent the sentiment vector for a given image, where s c denotes the number of votes assigned to sentiment category c by the evaluators. Since each image is evaluated by E evaluators, we have\n\nThe PerceptSent dataset considers five sentiment categories (C = 5) -positive, slightly positive, neutral, slightly negative, and negative -with a nearly balanced distribution. Each image is independently annotated by five evaluators (E = 5), ensuring that 5 c=1 s c = 5. Specifically, in vector s, s 1 represents the total votes for a positive sentiment, s 2 for slightly positive, s 3 for neutral, s 4 for slightly negative, and s 5 for negative. The dataset comprises 25,000 sentiment evaluations (5,000 images × 5 evaluations per image).\n\nThe DeepSent dataset considers only two sentiment categories (C = 2) -positive and negative. The distribution is slightly imbalanced, with a higher proportion of positive instances. Each image is also independently annotated by five evaluators (E = 5), with 6,345 sentiment evaluations (1,269 images × 5 evaluations per image).\n\nA dominant sentiment or category in the sentiment vector s is then defined as:\n\nwhere max(s) is the maximum value among all elements of s for a particular image, i.e., the number of votes of the most voted category in the image; and σ l is set to define a minimum number of votes for a specific target label in the pool of five evaluators addressed in the dataset. For example, for σ 3 , only images where at least three evaluators agreed on a sentiment category are included. We consider the following values for σ l : σ 3 (simple dominance) and σ 5 (absolute dominance). Note that the higher the l level in σ l , the smaller the subset resulting from filtering the entire dataset, due to a stronger consensus requirement. Furthermore, as in  [5] , we also explore variations in the number C of output target classes to evaluate the impact of different levels of separability in sentiment classification. In this work, we consider three variations (denoted as P C ), ranging from the most challenging case (C = 5) to the simplest (C = 2):\n\n• P 5 considers all five sentiment categories as originally assigned by evaluators in PerceptSent -positive, slightly positive, neutral, slightly negative, and negative, with the sentiment vector s given by s = (s 1 , ..., s 5 ). • P 3 merges in PerceptSent slightly positive with positive evaluations and slightly negative with negative, s = (s ′ 1 , s ′ 2 , s ′ 3 ), with s ′ 1 = s 1 +s 2 , s ′ 2 = s 3 , and s ′ 3 = s 4 +s 5 .\n\n• P 2 for a binary (positive versus negative) classification problem, as defined for DeepSent. As with l in σ l , the higher the number of classes C in a problem P C , the smaller the cardinality |{P C }| of the resulting subset of images. Thus, for PerceptSent,",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Methodology",
      "text": "This section describes the proposed approach, which we refer to as MLLMsent (Multimodal Large Language Model for Sentiment). As shown in Fig.  1 , different models can be used for two main tasks. In Task 1, a Multimodal Large Language Model, MLLM for short, directly classifies, following the prompt instructions, the sentiment associated with each image to be examined. In Task 2, the visual reasoning, represented by a hatched area in Fig.  1 , relies on the MLLMs to generate, for each image, a textual description, which is then analyzed by a text-only Large Language Model (LLM), for sentiment classification. This second task can be addressed using pre-trained models (Task 2 a ), or further enhanced through a complete finetuning process (Task 2 b ). In the following sections, we describe the models for image classification, image description, and text classification we considered in this work, as well as the experimental setup.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Multimodal Large Language Models (Mllms)",
      "text": "We evaluate three models for the MLLM component (highlighted in green in Fig.  1 ), which serves as the core of Task 1 and the first stage of Task 2. The compared models are the open-source MiniGPT-4  [9] , the proprietary GPT-4o mini  [10] , and the open-weights DeepSeek-VL2-Tiny  [11] .  MiniGPT-4 (Open Source): GPT (OS) for short, is an open-source vision-language model introduced by  [9] . Claiming that the technical details behind OpenAI's GPT-4 continue to remain undisclosed, the authors of GPT (OS) proposed a model built upon Vicuna  [43] , a large language model (LLM) derived from LLAMA  [44] . In terms of visual perception, the authors employ the same pre-trained vision components of BLIP-2  [45]  that consist of a ViT-G/14 from EVA-CLIP  [46]  and a Q-Former network, which can transform users' queries based on the image. GPT (OS) adds a single projection layer to align the encoded visual features with the Vicuna language model and freezes all the other vision and language components. According to the authors, although minimalist, the proposed design enables this model to perform a range of advanced tasks, including detailed image descriptions. The exact number of parameters in this model depends on some internal decisions, but the instance we employed in this work has approximately 7 billion parameters (Vicuna).\n\nGPT-4o mini (OpenAI): GPT (OAI), for short, is a proprietary, closed-source, closed-weights multimodal model developed by OpenAI, offered as a lighter, more affordable version of GPT-4o. The exact number of internal parameters or architecture details is also undisclosed. Despite its smaller size compared to other models being offered as services, GPT-4o mini has achieved competitive performance on several benchmarks. Unlike the other models, which were run locally on our GPU, this closed and proprietary model is only available as a paid cloud service, billed per input and output tokens, and accessed via the OpenAI Python API.\n\nDeepSeek-VL2-Tiny: DeepSeek for short, is part of the DeepSeek-VL2 series, and introduces advanced Mixture-of-Experts (MoE) Vision-Language Models (VLMs) designed for multimodal understanding. Specifically, the tiny version we consider features around 3 billion parameters, of which 1 billion are activated during inference, enabling efficient processing with GPU resource requirements.\n\nTo ensure effective task alignment, we have experimented with the aforementioned MLLMs with various prompt formulations, including those generated by the models themselves through prompt engineering and self-instruction. After qualitative and quantitative comparisons on a representative subset of samples, we selected the most effective prompt for each task, balancing clarity, consistency, and model response accuracy:\n\n• Task 1 (prompt for Image Classification): \"Analyze this image, and classify it as {L} sentiments, do not describe the image, and select only one class.\" Here, {L} is the set of sentiment labels (or categories) in each classification setting P C (e.g., when the set of labels in P 3 is {positive, neutral, negative}, we have C = |{L}| = 3. • Task 2 (prompt for Image Description): \"Describe this image in details.\"\n\nIn the visual reasoning scheme proposed in Task 2, each MLLM (the models within the green box in Fig.  1 ) provides a textual image description for every image. The description is then passed to a text-only LLM, which classifies the sentiment. The objective is to classify each textual description into a predefined sentiment category based on its linguistic features.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Text-Only Large Language Models (Llms)",
      "text": "In this study, we compare different LLMs to classify the sentiment of generated image descriptions. They have been chosen based on their ability to process textual data effectively and classify the setting of the descriptions based on the content's emotional and contextual nuances. The following sections detail the two approaches used for Task 2: application of the text-only LLMs with their pre-trained weights (Task 2 a ) and fine-tuning the weights to PerceptSent images (Task 2 b ).\n\nWe consider three architectures: BART-Large-MNLI  [12] ,  [13] , ModernBERT  [14] , and LLAMA-3  [15]  to evaluate how model size, long-context capability, and architectural optimizations impact sentiment classification of image descriptions.\n\nBART-LARGE-MNLI  [12] ,  [13] : BART, for short, is a sequence-to-sequence transformer model pre-trained as a denoising autoencoder, capable of capturing complex contextual relationships in the text. Its original fine-tuning, performed on the Multi-Genre Natural Language Inference (MultiNLI) dataset  [13] , endows BART with a natural language inference capability, which is leveraged here, through a second finetuning process (Task 2 b ), to improve sentiment classification.\n\nModernBERT  [14] : MBERT, for short, is an encoder-only transformer model pre-trained on an extensive corpus of 2 trillion tokens. It supports a native sequence length of 8,192 tokens, enabling it to efficiently handle long-context inputs. The architecture incorporates several modern optimizations, including rotary positional embeddings (RoPE), GeGLU activation functions, bias-free linear layers, alternating global/local attention mechanisms, unpadding techniques, and flash attention. It optimizes model depth and width to maximize computational efficiency and downstream task performance.\n\nLLAMA-3  [15] : Large Language Model Meta AI, version 3, LLAMA for short, is a decoder-only, auto-regressive Transformer architecture. It follows the architectural principles of its predecessor, LLAMA-2, but incorporates several key improvements in scalability, efficiency, and alignment.\n\nIn our study, these LLMs are used in their pre-trained form or fine-tuned to perform sentiment classification on image descriptions. Both settings involve a weight adjustment procedure (minimal in the case of pre-trained models and more significant for fine-tuned ones).\n\nPre-trained LLMs: For Task 2 a , we consider LLMs with very few modifications. BART and MBERT are extended with a single task-specific linear layer mapping the output to the number of target sentiment classes, i.e., the number C in each problem P C . All the remaining weights of those pre-trained models are kept fixed. For LLAMA, we use a prompt engineering approach called few-shot learning, in which a subset of random samples (image descriptions taken from the training set) is included directly in the prompt for classification. In our experiments, we tested subsets ranging from 5 to 15 samples. Smaller subsets proved insufficient, while the 15-sample subset emerged as the best alternative, albeit slightly more computationally expensive.\n\nFine-tuned LLMs: For Task 2 b , LLMs are fine-tuned to enable each model to adapt its internal representations (weights) to the specific language patterns and sentiment cues found in textual image descriptions of PerceptSent images. The finetuning process proposed for BART and MBERT starts from the pre-trained models (extended with the final linear mapping to the C classes), and adjusts the whole pipeline weights via supervised learning on our labeled image-description data. In this work, we perform LLAMA fine-tuning through qLORA (Quantized Low-Rank Adapters), which applies lowrank adapters to pre-trained models, significantly reducing their memory footprint while preserving high performance.\n\nWhen adjusting the weights in Task 2 a and Task 2 b , the training loop iteratively updates model parameters over several epochs and incorporates early stopping based on validation F1score improvements to prevent overfitting. The instances used as LLM inputs are textual descriptions provided by MLLMs. Such descriptions are framed as text generation tasks using sentiment-aware prompts adapted to the number of classes in each configuration P C , where C ∈ {2, 3, 5}. Each prompt instructs the model to determine the sentiment of a given description by selecting from a predefined set of labels. For example, in the three-class setting P 3 , the prompt is \"What is the sentiment of this description? Please choose an answer from {\"Positive\": 2, \"Negative\": 0, \"Neutral\": 1}\". Then they are tokenized and go through data-loaders with dynamic batching and shuffling for robust training.\n\nThis design guides the model to generate sentiment predictions consistent with the class granularity defined by P C and the agreement threshold l in filter σ l . Class imbalance in the sentiment labels is addressed by weighting the cross-entropy loss inversely proportional to class frequencies. Aiming to improve analyses' robustness, in both tasks, 2 a and 2 b , a stratified 5-fold cross-validation scheme assesses model performance.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Experimental Setup",
      "text": "The experimental framework is implemented in Python 3.10.12 using the PyTorch v2.7.1, Hugging Face Transformers v4.14.0, trl v0.18.1, Sklearn v1.7.0 and BitsandBytes v0.45.3 libraries. The experiments have been carried out on a machine equipped with an Intel(R) Xeon(R) Silver 4316 CPU @2.30GHz, 256GB of RAM, and two GPUs: an NVIDIA RTX 4000 Ada Generation (20GB VRAM) and an NVIDIA RTX A6000 (48GB VRAM). Considering all configurations (see Table  I ), training times for BART range from [1.2 -17.9] hours, for MBERT from [1.0 -12.9] hours, and for LLAMA fine-tuning, range from [1.8 -29.3] hours, with LLAMA consistently exhibiting the highest computational cost.\n\nFor GPT (OS), image classification (Task 1) and image description (first step in Task 2) are governed by two tuned hyperparameters: temperature = 0.1 and beam search = 1. The former controls the randomness of word selection and influences the variability of the output; the latter enables the model to consider multiple candidate sequences and select the most appropriate one. For GPT (OAI), both tasks rely on the built-in default generation parameters, with temperature fixed at 1.0, max token length set to 300 to constrain output size and all other settings left at their API defaults. This configuration provides a balanced trade-off between creativity and consistency, leveraging the model's internal heuristics for token selection without additional constraints or penalties. For DeepSeek, the tasks are controlled by five hyperparameters (left as default): max new tokens (512), repetition penalty (1.1), do sample (set as true), temperature (0.1), and top p (0.9). Max new tokens sets an upper bound on the length of the generated sequence, preventing overly verbose outputs; repetition penalty discourages the model from recycling the same phrases; do sample enables stochastic sampling rather than greedy decoding; temperature modulates randomness in token selection; and top p (nucleus sampling) restricts the sampling pool to the smallest set of tokens whose cumulative probability exceeds 0.9, balancing coherence and novelty.\n\nFor the LLMs, we adopt the following general configuration unless specified otherwise. We use the AdamW optimizer -a learning rate of 2×10 -3 for a Task 2 a and 2×10 - 5  for Task 2 b , both with a weight decay of 0.01. Models are trained for up to 100 epochs. We employ an early stopping mechanism with a patience of up to 25 epochs.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Results",
      "text": "In this section, we present the results obtained from the experiments performed to address the four research questions outlined earlier. Section V-A addresses (Q1) by evaluating the performance of different MLLM models on direct image-based sentiment classification (Task 1). Section V-B responds to (Q2) by investigating whether converting images into textual descriptions using MLLM and classifying them with pretrained LLMs (Task 2 a ) can outperform direct visual analysis. Section V-C explores (Q3), examining the impact of fully fine-tuning LLMs for sentiment classification using labeled MLLM-generated textual descriptions (Task 2 b ). To investigate (Q4), we compare our best-performing MLLM-based method with conventional approaches, including Lexicon-, CNN-, and Transformer-based models. This comparison spans both the PerceptSent dataset (Section V-D) and a cross-dataset generalization to DeepSent (Section V-E).\n\nTable  I  groups the main results for Task 1 and Task 2, and will be discussed on multiple occasions in the following subsections. As previously defined in Section III, the parameter l ∈ {3, 5} in the filter σ l specifies the minimum level of agreement required among evaluators, ranging from simple consensus (σ 3 ) to absolute consensus (σ 5 ), whereas P C , C ∈ {3, 5}, specifies the number of sentiment classes for each sentiment scale in the problem setup.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Direct Sentiment Classification Using Mllms",
      "text": "As shown in the third column of Table  I  (Task 1), the lowest F -scores for GPT (OAI) and DeepSeek occur under more challenging conditions, characterized by low annotator agreement, while higher scores are observed as annotator consensus increases. The same trend is observed with respect to the number of target classes; lower scores are obtained under harsher conditions, i.e., as the number of classes increases. GPT (OAI) achieves higher F-scores than DeepSeek, with relative gains 5 of 2% to 67%. The improvement is statistically significant (paired t-test, p < 0.05) in three cases; however, in the ⟨σ 5 , P 3 ⟩ configuration, the one with the lowest difference, this value is not significant (p = 0.194).\n\nIn contrast, GPT (OS) fails to produce valid classification outputs for Task 1. As illustrated in Fig.  2 , even after extensive prompt engineering -including explicit instructions such as \"Select a single sentiment class for this image from the {L} list\", where {L} refers to the list of sentiment polarities for a specific problem setup, or \"Do not describe this image; just choose one sentiment polarity from the {L} list\" -the model consistently generates explanatory or ambiguous responses that attempt to justify the set {L} of categories, rather than selecting a single label, or combining multiple sentiment cues (e.g., \"Positive, but with some neutral elements\"), making it difficult to unambiguously assign a single label and complicating automatic parsing. This behavior contrasts with the other MLLMs, which produce discrete and task-aligned outputs, as also shown in Fig.  2 .\n\nTherefore, we can answer research question (Q1) as follows: GPT (OAI) is able to directly classify sentiments from raw images and consistently outperforms DeepSeek, whereas GPT (OS) fails to produce valid or reliable outputs for this task.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Pre-Trained Llms: Classifying Mllms Image Descriptions",
      "text": "High-quality MLLM-generated scene descriptions are fundamental to the success of Task 2 a , where the proposed visual reasoning is performed in two stages: image description followed by a text sentiment classification. Fig.  3    Problem setup: P5 Dominance threshold: σ = 3 Labeled as: Slightly Negative Task 1\n\nGPT (OS): ... Based on this image alone, it is difficult to determine the sentiment conveyed. However, assuming that the image was taken in a residential area, it is likely that the trash can is full and needs to be emptied. This could be seen as a positive sentiment, as it indicates that the area is well-maintained and that the residents are responsible about their waste disposal. On the other hand, if the trash can has been there for a long time and has not been emptied, it could be seen as a negative sentiment, indicating neglect ... correctly identify the central subject: a gray trash can on the sidewalk. However, GPT (OS) fails to capture several salient visual elements, most notably the presence of a squirrel perched on the can, and inaccurately asserts the presence of \"cars driving on the street,\" while also stating that no other objects are visible, thereby overlooking parked vehicles and surrounding vegetation. GPT (OAI) provides the most detailed interpretation, identifying the squirrel (although it incorrectly refers to two animals), describing it as likely scavenging for food in the trash can, an action commonly associated with negative sentiment, and including broader contextual cues such as barren bushes, seasonal grass tones, and the calm suburban environment. DeepSeek also detects the squirrel and its position on the edge of the can. Still, its output suffers from significant redundancy, including repeated phrases such as \"a sidewalk and a and a fence fence fence,\" a recurring issue that hinders clarity and readability. In the bottom image, both GPT (OS) and DeepSeek produce short, less context-aware descriptions, failing to capture crucial semantic elements. In contrast, GPT (OAI) is the only model to correctly identify a person sitting on the floor asking for help, an observation also made by some human annotators, whose descriptions included terms such as beggar and poverty, both strongly associated with the image's negative sentiment label. Therefore, as reported in Table  I  (Task 2 a ), GPT (OAI) achieves the highest F -scores across all ⟨σ l , P C ⟩ and LLM architectures. Although DeepSeek generally scores lower in Task 2 a compared with GPT (OS) and GPT (OAI), with BART for σ 3 or MBERT in all configurations, it shows consistent improvements from Task 1 to Task 2 a .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Gpt (Oai): Neutral",
      "text": "Overall, MLLM+LLM performance benefits more from higher annotator consensus than from reduced class granularity. Whereas decreasing the number of sentiment categories from P 5 to P 3 leads to systematic improvements, increasing the consensus level (from σ 3 to σ 5 ) mostly results in even more substantial gains. The GPT (OAI)+MBERT model in Task 2 a , under P 5 for example, improves from an average Fscore 47.9 at σ 3 to 72.2 at σ 5 , a relative gain of 50.7%. A similar pattern is observed for DeepSeek and GPT (OS), whose MBERT-based pipelines improve from an average F -score of PerceptSent image GPT (OS): \"The image shows a gray trash can sitting on the sidewalk next to a curb. The can has a lid on top of it and appears to be full of trash. There are no trees or other objects in the image, just a sidewalk and a street with cars driving on it. The sky is cloudy and overcast.\" GPT (OAI): \"The image features a gray trash bin positioned on a sidewalk, with a couple of squirrels perched on the edge. The squirrels appear to be scavenging for food, with their tails standing upright. The background includes a neighborhood scene with trees, some barren bushes, and a distant view of cars parked beyond a black fence. The pavement is light-colored concrete, and the grass alongside the sidewalk appears brownish, possibly indicating a change in season. The atmosphere looks calm and residential, typical of a suburban area.\"\n\nDeepSeek: \"The image depicts a gray trash can on a sidewalk. The trash can is open, and a squirrel is standing on the edge of the can. The can is located on the sidewalk, with a sidewalk and a and a fence fence fence in the background.\"\n\nPerceptSent image GPT (OS): \"The image is a reflection of a person walking down the street on a sunny day. The person is wearing a black hoodie, blue jeans, and white sneakers. They have a backpack on their back and are holding a skateboard under their arm. The street is lined with tall buildings on both sides and there are people walking on the sidewalk. The sky is clear and blue, and there are no clouds in the sky. The time of day appears to be morning, as the sun is shining brightly and there are no shadows on the ground. The image is taken from a low angle, looking up at the person as they walk down the street.\" GPT (OAI): \"The image appears to be taken with a fisheye lens, giving it a distinct wide-angle view that curves the edges. The foreground displays a busy city sidewalk with pedestrians. On the left, there is a storefront featuring large windows, likely displaying products or advertisements. On the right side of the image, a person is seated on the ground next to a phone booth, holding a sign that is not clearly visible. They appear to be asking for help or support, which is common in urban areas. Nearby, someone is walking past, carrying a bag and dressed in casual attire. In the background, there are several buildings with varied architecture typical of a cityscape, and several vehicles, possibly taxis, are parked or driving on the street. The overall atmosphere suggests a bustling urban environment, with a mix of commerce and social issues. The fisheye effect adds a unique perspective, emphasizing the closeness of the subjects to the lens while providing a sense of busy city life.\"\n\nDeepSeek: \"This image appears to be taken through a fisheye lens, which distorts the scene. The image shows a street scene with a sidewalk and a bus stop. There is a person sitting on the sidewalk with a sign. There is a person walking away from the camera. There are buildings in the background.\" Fig.  3 : Visual Reasoning for PerceptSent images -MLLMs descriptions (refer to Fig.  2  for evaluator votes and target sentiment labels).\n\n38.2 to 61.3, and 43.4 to 65.4, respectively, under the same conditions. The same trend can be observed for LLAMA. In some cases, MLLM+LLAMA performs worse than the MLLM standalone (Task 1), but for ⟨σ 5 , P 5 ⟩, GPT (OAI)+LLAMA achieves the highest F -score in Task 2 a .\n\nBased on the previous results, the answer to research question (Q2) depends on the LLM: using MBERT to classify textual image descriptions leads to better results for all but one case; BART and LLAMA have mixed results. Moreover, adding LLMs allows classification for GPT (OS), which could not directly perform sentiment analysis on raw images.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Fine-Tuning Llms: Classifying Mllms Descriptions",
      "text": "As shown in Table  I , fully fine-tuning the LLMs (Task 2 b ) yields the highest overall F -scores, a consistent trend across all configurations. Relative gains over Task 2 a range from [11.59%, 50.87%] for BART, [5.74%, 32.20%] for MBERT, and [4.09%, 102.87%] for LLAMA, across all MLLMs, evaluators' agreement thresholds, and number of sentiment classes.\n\nThese results suggest that parameter adaptation broadly enhances sentiment reasoning from multimodal input, independent of the task's complexity, and allows us to answer the research question (Q3): fine-tuning LLMs for sentiment classification positively impacts the overall results. Fig.  4  showcases four examples from the PerceptSent dataset alongside sentiment predictions from all considered models in both Task 2 a and Task 2 b . Figs.  4(a, b ) present image samples with strong consensus (σ 5 ) among human evaluators. In Fig.  4 (a), the image has received unanimous positive sentiment annotations from all five annotators. After fine-tuning, all MLLM+LLM combinations correctly predict the positive sentiment based on generated descriptions referencing visual elements such as a body of water, a beach, and sparkling highlights. Fig.  4 (b) similarly shows high agreement, with all MLLM+LLM combinations (pre-trained and finetuned) accurately identifying the target negative sentiment. The image description results often highlight features such as an underground tunnel, a graffiti-covered entrance, and a gritty atmosphere, with GPT (OAI) explicitly mentioning a sense of decay. Interestingly, both GPT (OAI) and DeepSeek can even recognize and transcribe the \"OHNO\" graffiti above the entrance. As shown in Table  I , these two cases exemplify the best observed performance, where high evaluator agreement (σ 5 ) correlates with elevated F -scores. In contrast, under lower agreement (σ 3 ), some issues occur. Although in Fig.  4(c ) most of the pre-trained models correctly classify slightly negative sentiment, some BART predictions shift toward wrong sentiment after fine-tuning. This change may result from the influence of image descriptions generated by GPT (OAI), such as \"The atmosphere looks calm and residential, typical of a suburban area,\" which potentially moderates the models' sentiment evaluations. Nevertheless, fine-tuning improvements occur, particularly for GPT (OS)+LLAMA and DeepSeek+MBERT. In the case illustrated by Fig.  4(d) , none of the MLLM+LLM combinations succeed in predicting the correct negative sentiment label after fine-tuning (Task 2 b ). This fact appears to be driven by limitations in the scene descriptions generated by the MLLMs. Specifically, none of the models could identify salient visual features indicative of a negative context -such as the pile of trash on the left or the overall untidy condition of the street. Instead, the generated image descriptions emphasize neutral or positive aspects, including references to a city street with parked cars, a calm atmosphere, and the presence of a few pedestrians. Such framing has likely influenced the LLMs to interpret the scene as neutral. Notably, LLAMA in Task 2 a predicts the correct negative sentiment; however, with low F -scores ([30.3%, 46.6%]) for ⟨σ 3 , P 3 ⟩, this does not indicate consistent performance. After fine-tuning, the prediction shifted to neutral, confirming that the generated descriptions failed to capture the negative elements identified by human evaluators.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Mllmsent Vs Language And Image-Based Baselines",
      "text": "To focus the discussion, we refer to the GPT (OAI) + MBERT configuration -the top performer in Table  I  as MLLMsent, here and in the following sections, hereafter designated as our proposed method. As shown in Fig.  5 , we benchmark the MLLMsent performance against three baselines designed to capture sentiment through different modeling paradigms. The first is VADER  [47] , a widely used rule-and lexicon-based sentiment analysis tool originally designed for contextual relationships rather than obvious visual features.",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "E. Cross-Dataset Evaluation",
      "text": "In this section, we present a comparative evaluation of MLLMsent on a distinct dataset focused exclusively on binary classification -i.e., positive versus negative (P 2 ) -using the ⟨σ 3 , P 2 ⟩ and ⟨σ 5 , P 2 ⟩ settings. However, MLLMsent is not fine-tuned on the new dataset (DeepSent). Instead, we adopt a cross-dataset evaluation protocol to assess the model's generalization capability. To this end, MLLMsent is fine-tuned using PerceptSent samples in a P 2 formulation, where original sentiment labels are restructured such that positive, slightly positive, and neutral are grouped as positive, while negative and slightly negative form the negative class. We report classification accuracy for consistency with prior work  [26] ,  [29] , which includes traditional hand-crafted features (GCH, LCH), shallow CNNs, and deep architectures such as VGG, Inception, ResNet, and DenseNet. It is important to note that these baselines have been trained and validated by using DeepSent images in a stratified k-fold cross-validation.\n\nAs reported in Table  II , although not trained or finetuned in DeepSent, MLLMsent consistently outperforms all competing methods across both agreement levels (σ 3 and σ 5 ). Traditional hand-crafted approaches yield substantially lower performance, reaffirming the superiority of deep visual representations for sentiment analysis. Notably, MLLMsent achieves absolute gains over the best-performing deep models reported by Oliveira et al.  [29] , who have incorporated additional semantic features such as SUN scene descriptors and object-level cues from YOLO (e.g., guns, fire, homelessness) to improve sentiment prediction. In contrast, MLLMsent attains higher accuracy without relying on such engineered features, underscoring its strong cross-dataset generalization capability.\n\nFig.  6  presents selected images from the DeepSent dataset that are misclassified by the methods of Oliveira et al.  [29]  TABLE II: Accuracy on the DeepSent dataset for two agreement levels: ⟨σ 3 , P 2 ⟩ (three-agree) and ⟨σ 5 , P 2 ⟩ (five-agree). Results include hand-crafted methods (GCH, LCH), shallow and deep CNN architectures, and our proposed MLLMsent method, which was trained only on the PerceptSent dataset. 76.5 ±3.7 InceptionV3 et al.  [29]  79.6 ±1.8 ResNet50 et al.  [29]  81.5 ±1.0 DenseNet169 et al.  [29]  81.4 ±1.4 GCH et al.  [26]  66.0 ±0.0 LCH et al.  [26]  66.4 ±0.0 Campos et al.  [48]  74.9 86.5 ±3.1 DenseNet169 et al.  [29]  88.3 ±1.6 GCH et al.  [26]  68.4 ±0.0 LCH et al.  [26]  71.0 ±0.0 Campos et al.  [48]  83.0 ±0.03 MLLMsent (proposed) 95.6 ±0.9\n\nand Campos et al.  [48] , along with the corresponding outputs from our proposed method, MLLMsent. In Figs.  6(a, b ), the MLLM image descriptions correctly identify animals and contextual elements related to rural or mountainous landscapes. In Fig.  6 (a), however, the negative classification may be attributed to descriptions such as \"a large dog standing outside, looking through a metal gate\" and \"a mix of dirt and possibly some fallen leaves.\" It is worth noting that not all descriptive sentences convey exclusively positive or negative semantics, which highlights the complexity of sentiment analysis. In Fig.  6 (d), for example, our model also produces an incorrect outcome. We conjecture that the description \"thick white steam is billowing from the smokestack\" generated by the MLLM, may have influenced the negative sentiment classificationpossibly outweighing the more neutral or positive phrase \"the overall scene captures a blend of industrial heritage and natural beauty\" -despite the image likely depicting a tourism-related train.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Vi. Conclusions And Future Work",
      "text": "This work has presented MLLMsent, a framework that harnesses Multimodal Large Language Models (MLLMs) to advance visual sentiment analysis through visual reasoning. Our experiments have systematically addressed the four research questions posed in this study. Concerning Q1, we found that direct image-based sentiment classification using MLLMs (Task 1) can be effective, particularly when employing high-capacity models such as GPT (OAI), which consistently outperformed locally run open-source or openweights alternatives. However, we conjecture that with access to higher-end GPUs and consequently access to more powerful open-source MLLMs, this performance gap could be significantly reduced. About Q2, transforming images into textual descriptions and classifying them using pre-trained LLMs (Task 2 a ) yielded moderate gains for the ModernBERT LLM in most scenarios. Addressing Q3, we observed that finetuning LLMs on MLLM-generated descriptions (Task 2 b ) consistently led to relevant performance improvements across all MLLM+LLM combinations and problem configurations. Our findings emphasize that accurate sentiment reasoning requires not only rich visual grounding but also task-specific adaptation of the language model itself. Notably, our experiments showed that ModernBERT (395M parameters), when fine-tuned on MLLM-generated descriptions, achieved an impressive result (F1-score of 0.96) in high-agreement settings-surpassing larger models such as LLAMA-3 (8B parameters). This highlights how our framework enables even moderately sized language models to reach exceptional performance when combined with multimodal reasoning. Finally, in response to Q4, MLLMsent outperformed visual-only baselines (e.g., CNNs and Swin Transformers) as well as rule-based text-only sentiment classifiers (e.g., VADER). Furthermore, it showed strong generalization in cross-dataset evaluation, outperforming deep visual models trained directly on the new target dataset.\n\nIn summary, beyond seeing sentiment, our multimodal LLM (MLLMsent) establishes a new state-of-the-art while contributing to interpretability, as each prediction is accompanied by textual descriptions that help clarify the model's reasoning. This combination of performance and interpretability is particularly valuable for analyzing user-generated content, where both accuracy and transparency are crucial.\n\nAlthough we evaluated GPT-4o mini, MiniGPT-4 and DeepSeek-VL2-Tiny, our methodology is model-agnostic. The consistent performance patterns suggest newer MLLMs would yield further improvements while maintaining interpretability benefits. Notably, ModernBERT's superior performance over larger models demonstrates that careful architecture selection and tuning can outweigh pure scale advantages. Moreover, despite GPT-4o mini achieving the best overall results, we note that open-weights (DeepSeek) or open source (GPT (OS)) alternatives can still be attractive compared to paid commercial models such as those from OpenAI. In that regard, we highlight that, due to GPU and time constraints, we employed the Tiny DeepSeek variant, and that larger models could achieve performance levels closer to that of GPT-based models. Thus, we believe a deeper exploration of open-weights and open source models remains an interesting road for future research.\n\nA key innovation of MLLMsent lies in its multimodal reasoning pipeline, where MLMMs provide image descriptions encoding not only visual attributes but also affective cues, which are then processed by LLM models to infer sentiment. This decoupling of visual perception from sentiment analysis of textual image descriptions not only improves performance but also provides support for future studies aiming at explainable AI (xAI): by producing interpretable textual representations, the framework offers a traceable path from input images to predictions, a critical feature for high-stakes domains like healthcare, policy-making, and social media moderation, where transparency and accountability are paramount.\n\nWe identify, therefore, three key future directions: (1) exploring our pipeline to sensitive domains such as mental health monitoring, where sentiment analysis can provide nuanced signals to support user well-being, and in the detection of potentially harmful or distressing content in social media;\n\n(2) integrating xAI techniques to make the model's reasoning process more interpretable, e.g., highlighting which regions in the image and which words in the generated description most influenced the sentiment classification; and (3) developing interfaces that let users correct ambiguous image descriptions to improve model performance.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , different models can be used",
      "page": 3
    },
    {
      "caption": "Figure 1: , relies on the MLLMs to generate, for",
      "page": 3
    },
    {
      "caption": "Figure 1: Architecture diagram of MLLMsent, our proposed",
      "page": 3
    },
    {
      "caption": "Figure 1: ) provides a",
      "page": 4
    },
    {
      "caption": "Figure 2: , even after extensive",
      "page": 5
    },
    {
      "caption": "Figure 2: Therefore, we can answer research question (Q1) as follows:",
      "page": 5
    },
    {
      "caption": "Figure 2: Evaluator’s votes and ground-truth labels for selected",
      "page": 6
    },
    {
      "caption": "Figure 3: Visual Reasoning for PerceptSent images - MLLMs",
      "page": 7
    },
    {
      "caption": "Figure 2: for evaluator votes and target",
      "page": 7
    },
    {
      "caption": "Figure 4: showcases four examples from the PerceptSent",
      "page": 7
    },
    {
      "caption": "Figure 4: (a), the image has received unanimous",
      "page": 7
    },
    {
      "caption": "Figure 4: (b) similarly shows high agreement,",
      "page": 7
    },
    {
      "caption": "Figure 4: (c) most of the pre-trained models correctly classify",
      "page": 7
    },
    {
      "caption": "Figure 4: Qualitative comparison of sentiment predictions on Per-",
      "page": 8
    },
    {
      "caption": "Figure 5: F-score results for image sentiment polarity classification on the PerceptSent dataset under different setups. Each",
      "page": 9
    },
    {
      "caption": "Figure 6: presents selected images from the DeepSent dataset",
      "page": 9
    },
    {
      "caption": "Figure 6: (a), however, the negative classification may be attributed",
      "page": 9
    },
    {
      "caption": "Figure 6: (d), for example, our model also produces an incorrect",
      "page": 9
    },
    {
      "caption": "Figure 6: Qualitative comparison of sentiment predictions on",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Problem": "",
          "MLLM": "",
          "Image\nClassification\nTask 1": "",
          "Classification of MLLM’s Scene Textual Descriptions": "BART\nMBERT\nLLAMA\nTask 2a\nTask 2a\nTask 2a\nTask 2b\nTask 2b\nTask 2b\npre-trained\nfine-tuned\npre-trained\nfine-tuned\npre-trained\nfine-tuned"
        },
        {
          "Problem": "⟨σ3, P5⟩",
          "MLLM": "GPT (OS)\nGPT (OAI)\nDeepSeek",
          "Image\nClassification\nTask 1": "—\n44.5 ±3.1\n26.6 ±1.9",
          "Classification of MLLM’s Scene Textual Descriptions": "36.5 ±2.6\n48.0 ±2.6\n43.4 ±1.1\n51.2 ±1.6\n24.4 ±3.7\n49.5 ±2.9\n40.1 ±3.0\n56.1 ±2.5\n33.3 ±6.3\n56.9 ±2.9\n47.9 ±2.3\n58.4 ±4.0\n34.6 ±3.0\n52.2 ±2.5\n38.2 ±1.7\n50.5 ±1.4\n25.3 ±9.0\n51.0 ±2.4"
        },
        {
          "Problem": "⟨σ3, P3⟩",
          "MLLM": "GPT (OS)\nGPT (OAI)\nDeepSeek",
          "Image\nClassification\nTask 1": "—\n61.2 ±2.9\n44.0 ±0.2",
          "Classification of MLLM’s Scene Textual Descriptions": "59.4 ±1.7\n68.7 ±2.1\n62.1 ±8.9\n69.8 ±1.7\n30.3 ±11.1\n70.9 ±2.6\n64.9 ±2.4\n76.0 ±0.7\n46.6 ±14.0\n76.7 ±1.7\n66.6 ±6.1\n77.5 ±0.9\n57.2 ±1.5\n71.1 ±1.2\n60.6 ±5.2\n71.1 ±1.6\n46.6 ±7.5\n72.9 ±3.5"
        },
        {
          "Problem": "⟨σ5, P5⟩",
          "MLLM": "GPT (OS)\nGPT (OAI)\nDeepSeek",
          "Image\nClassification\nTask 1": "—\n75.8 ±4.7\n60.7 ±6.9",
          "Classification of MLLM’s Scene Textual Descriptions": "54.1 ±4.5\n80.1 ±3.3\n65.4 ±3.5\n75.7 ±4.8\n67.4 ±3.8\n68.8 ±4.0\n60.6 ±6.1\n82.4 ±5.6\n81.4 ±4.7\n84.4 ±4.2\n78.2 ±6.6\n72.2 ±4.4\n57.3 ±4.0\n83.0 ±4.6\n61.3 ±6.1\n74.8 ±5.6\n58.4 ±1.4\n69.8 ±2.3"
        },
        {
          "Problem": "⟨σ5, P3⟩",
          "MLLM": "GPT (OS)\nGPT (OAI)\nDeepSeek",
          "Image\nClassification\nTask 1": "—\n87.7 ±1.8\n85.8 ±1.6",
          "Classification of MLLM’s Scene Textual Descriptions": "78.7 ±3.6\n89.7 ±0.8\n84.1 ±1.4\n90.4 ±1.6\n82.3 ±2.0\n87.7 ±2.1\n85.4 ±1.8\n95.3 ±1.6\n85.5 ±6.1\n91.3 ±1.1\n90.6 ±1.5\n95.8 ±0.9\n73.2 ±2.1\n91.9 ±0.9\n79.2 ±2.7\n89.5 ±1.3\n77.6 ±1.0\n88.6 ±1.4"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GPT (OS)\nGPT (OAI)\nDeepSeek\nSlightly Neg.\nSlightly Neg.\nSlightly Neg.\nTask 2a:\nBART\nNegative\nNeutral\nSlightly Neg.\nTask 2b:\nSlightly Neg.\nSlightly Neg.\nNegative\nTask 2a:\nMBERT\nSlightly Neg.\nSlightly Neg.\nSlightly Neg.\nTask 2b:\nPositive\nNeutral\nSlightly Neg.\nTask 2a:\nLLAMA\nSlightly Neg.\nNeutral\nSlightly Neg.\nTask 2b:": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "GPT (OS)\nGPT (OAI)\nDeepSeek\nPositive\nNeutral\nPositive\nTask 2a:\nBART\nNeutral\nNeutral\nPositive\nTask 2b:\nNeutral\nNeutral\nNeutral\nTask 2a:\nMBERT\nNeutral\nNeutral\nNeutral\nTask 2b:\nNegative\nNegative\nNegative\nTask 2a:\nLLAMA\nNeutral\nNeutral\nNeutral\nTask 2b:": ""
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "On the use of vision-language models for visual sentiment analysis: a study on CLIP",
      "authors": [
        "C Bustos",
        "C Civit",
        "B Du",
        "A Solé-Ribalta",
        "A Lapedriza"
      ],
      "year": "2023",
      "venue": "Proc. of ACII"
    },
    {
      "citation_id": "2",
      "title": "An integrated deep learning and belief rule-based expert system for visual sentiment analysis under uncertainty",
      "authors": [
        "S Zisad",
        "E Chowdhury",
        "M Hossain",
        "R Islam",
        "K Andersson"
      ],
      "venue": "Algorithms"
    },
    {
      "citation_id": "3",
      "title": "Visual sentiment analysis using deep learning models with social media data",
      "authors": [
        "G Chandrasekaran",
        "N Antoanela",
        "G Andrei",
        "C Monica",
        "J Hemanth"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "4",
      "title": "Survey on visual sentiment analysis",
      "authors": [
        "A Ortis",
        "G Farinella",
        "S Battiato"
      ],
      "year": "2020",
      "venue": "IET Image Processing"
    },
    {
      "citation_id": "5",
      "title": "PerceptSent -exploring subjectivity in a novel dataset for visual sentiment analysis",
      "authors": [
        "C Lopes",
        "R Minetto",
        "M Delgado",
        "T Silva"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "MLLM-CompBench: A comparative reasoning benchmark for multimodal LLMs",
      "authors": [
        "J Kil",
        "Z Mai",
        "J Lee",
        "A Chowdhury",
        "Z Wang",
        "K Cheng",
        "L Wang",
        "Y Liu",
        "W.-L Chao"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "7",
      "title": "MM-LLMs: Recent advances in multimodal large language models",
      "authors": [
        "D Zhang",
        "Y Yu",
        "J Dong",
        "C Li",
        "D Su",
        "C Chu",
        "D Yu"
      ],
      "year": "2024",
      "venue": "MM-LLMs: Recent advances in multimodal large language models",
      "arxiv": "arXiv:2401.13601"
    },
    {
      "citation_id": "8",
      "title": "Evaluating MLLMs with multimodal multiimage reasoning benchmark",
      "authors": [
        "Z Cheng",
        "B Xu",
        "L Gong",
        "Z Song",
        "T Zhou",
        "S Zhong",
        "S Ren",
        "M Chen",
        "X Meng",
        "Y Zhang"
      ],
      "year": "2025",
      "venue": "Evaluating MLLMs with multimodal multiimage reasoning benchmark",
      "arxiv": "arXiv:2506.04280"
    },
    {
      "citation_id": "9",
      "title": "MiniGPT-4: Enhancing vision-language understanding with advanced large language models",
      "authors": [
        "D Zhu",
        "J Chen",
        "X Shen",
        "X Li",
        "M Elhoseiny"
      ],
      "year": "2024",
      "venue": "Proc. of ICLR"
    },
    {
      "citation_id": "10",
      "title": "GPT-4o system card",
      "authors": [
        ": Openai",
        "A Hurst",
        "A Lerer",
        "A Goucher",
        "A Perelman",
        "A Ramesh",
        "A Clark",
        "A Ostrow"
      ],
      "year": "2024",
      "venue": "GPT-4o system card",
      "arxiv": "arXiv:2410.21276"
    },
    {
      "citation_id": "11",
      "title": "DeepSeek-VL2: Mixture-of-experts vision-language models for advanced multimodal understanding",
      "authors": [
        "Z Wu",
        "X Chen",
        "Z Pan",
        "X Liu",
        "W Liu",
        "D Dai",
        "H Gao",
        "Y Ma",
        "C Wu",
        "B Wang"
      ],
      "year": "2024",
      "venue": "DeepSeek-VL2: Mixture-of-experts vision-language models for advanced multimodal understanding",
      "arxiv": "arXiv:2412.10302"
    },
    {
      "citation_id": "12",
      "title": "BART: Denoising sequence-tosequence pre-training for natural language generation, translation, and comprehension",
      "authors": [
        "M Lewis",
        "Y Liu",
        "N Goyal",
        "M Ghazvininejad",
        "A Mohamed",
        "O Levy",
        "V Stoyanov",
        "L Zettlemoyer"
      ],
      "year": "2020",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "13",
      "title": "A broad-coverage challenge corpus for sentence understanding through inference",
      "authors": [
        "A Williams",
        "N Nangia",
        "S Bowman"
      ],
      "year": "2018",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "14",
      "title": "Smarter, better, faster, longer: A modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference",
      "authors": [
        "B Warner"
      ],
      "year": "2025",
      "venue": "Proc. of ACL"
    },
    {
      "citation_id": "15",
      "title": "The Llama 3 herd of models",
      "authors": [
        "A Grattafiori",
        "A Dubey",
        "A Jauhri",
        "A Pandey",
        "A Kadian",
        "A Al-Dahle",
        "A Letman",
        "A Mathur",
        "A Schelten",
        "A Vaughan"
      ],
      "year": "2024",
      "venue": "The Llama 3 herd of models",
      "arxiv": "arXiv:2407.21783"
    },
    {
      "citation_id": "16",
      "title": "Improved lexicon-based sentiment analysis for social media analytics",
      "authors": [
        "A Jurek",
        "M Mulvenna",
        "Y Bi"
      ],
      "year": "2015",
      "venue": "Security Informatics"
    },
    {
      "citation_id": "17",
      "title": "A novel sentiment aware dictionary for multi-domain sentiment classification",
      "authors": [
        "V Jha",
        "R Savitha",
        "P Shenoy",
        "K Venugopal",
        "A Sangaiah"
      ],
      "year": "2018",
      "venue": "Computers & Electrical Engineering"
    },
    {
      "citation_id": "18",
      "title": "A context-aware embeddings supported method to extract a fuzzy sentiment polarity dictionary",
      "authors": [
        "J Bernabé-Moreno",
        "A Tejeda-Lorente",
        "J Herce-Zelaya",
        "C Porcel",
        "E Herrera-Viedma"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "19",
      "title": "Exploiting semantic relationships for unsupervised expansion of sentiment lexicons",
      "authors": [
        "F Viegas",
        "M Alvim",
        "S Canuto",
        "T Rosa",
        "M Gonc",
        "L Rocha"
      ],
      "year": "2020",
      "venue": "Information Systems"
    },
    {
      "citation_id": "20",
      "title": "Text sentiment analysis of Douban film short comments based on BERT-CNN-BiLSTM-Att model",
      "authors": [
        "A He",
        "M Abisado"
      ],
      "year": "2024",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Aspect-based sentiment analysis via affective knowledge enhanced graph convolutional networks",
      "authors": [
        "B Liang",
        "H Su",
        "L Gui",
        "E Cambria",
        "R Xu"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "22",
      "title": "Phrase dependency relational graph attention network for aspect-based sentiment analysis",
      "authors": [
        "H Wu",
        "Z Zhang",
        "S Shi",
        "Q Wu",
        "H Song"
      ],
      "year": "2022",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "23",
      "title": "TextGT: A double-view graph transformer on text for aspect-based sentiment analysis",
      "authors": [
        "S Yin",
        "G Zhong"
      ],
      "year": "2024",
      "venue": "AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "24",
      "title": "Sentiment analysis and topic recognition in video transcriptions",
      "authors": [
        "L Stappen",
        "A Baird",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Intelligent Systems"
    },
    {
      "citation_id": "25",
      "title": "Survey of visual sentiment prediction for social media analysis",
      "authors": [
        "R Ji",
        "D Cao",
        "Y Zhou",
        "F Chen"
      ],
      "year": "2016",
      "venue": "Frontiers of Computer Science"
    },
    {
      "citation_id": "26",
      "title": "Robust image sentiment analysis using progressively trained and domain transferred deep networks",
      "authors": [
        "Q You",
        "J Luo",
        "H Jin",
        "J Yang"
      ],
      "year": "2015",
      "venue": "Proc. of AAAI"
    },
    {
      "citation_id": "27",
      "title": "DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks",
      "authors": [
        "T Chen",
        "D Borth",
        "T Darrell",
        "S Chang"
      ],
      "year": "2014",
      "venue": "DeepSentiBank: Visual sentiment concept classification with deep convolutional neural networks",
      "arxiv": "arXiv:1410.8586"
    },
    {
      "citation_id": "28",
      "title": "Boosting image sentiment analysis with visual attention",
      "authors": [
        "K Song",
        "T Yao",
        "Q Ling",
        "T Mei"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "29",
      "title": "OutdoorSent: Sentiment analysis of urban outdoor images by using semantic and deep features",
      "authors": [
        "W Oliveira",
        "L Dorini",
        "R Minetto",
        "T Silva"
      ],
      "year": "2020",
      "venue": "ACM Trans. on Information Systems"
    },
    {
      "citation_id": "30",
      "title": "Progress, achievements, and challenges in multimodal sentiment analysis using deep learning: A survey",
      "authors": [
        "A Pandey",
        "D Vishwakarma"
      ],
      "year": "2024",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "31",
      "title": "Image-text multimodal sentiment analysis framework of assamese news articles using late fusion",
      "authors": [
        "R Das",
        "T Singh"
      ],
      "year": "2023",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "32",
      "title": "Multimodal sentiment analysis with image-text interaction network",
      "authors": [
        "T Zhu",
        "L Li",
        "J Yang",
        "S Zhao",
        "H Liu",
        "J Qian"
      ],
      "year": "2022",
      "venue": "IEEE transactions on multimedia"
    },
    {
      "citation_id": "33",
      "title": "A multi-stage visual perception approach for image emotion analysis",
      "authors": [
        "J Pan",
        "J Lu",
        "S Wang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. on Affective Computing"
    },
    {
      "citation_id": "34",
      "title": "Dualperspective fusion network for aspect-based multimodal sentiment analysis",
      "authors": [
        "D Wang",
        "C Tian",
        "X Liang",
        "L Zhao",
        "L He",
        "Q Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Unveiling the ignorance of MLLMs: Seeing clearly, answering incorrectly",
      "authors": [
        "Y Liu",
        "Z Liang",
        "Y Wang",
        "X Wu",
        "F Tang",
        "M He",
        "J Li",
        "Z Liu",
        "H Yang",
        "S Lim"
      ],
      "year": "2025",
      "venue": "Proc. of CVPR"
    },
    {
      "citation_id": "36",
      "title": "Exploring clean label backdoor attacks and defense in language models",
      "authors": [
        "S Zhao",
        "L Tuan",
        "J Fu",
        "J Wen",
        "W Luo"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "37",
      "title": "Sentiment analysis in the era of large language models: A reality check",
      "authors": [
        "W Zhang",
        "Y Deng",
        "B Liu",
        "S Pan",
        "L Bing"
      ],
      "year": "2024",
      "venue": "Sentiment analysis in the era of large language models: A reality check"
    },
    {
      "citation_id": "38",
      "title": "Position: Will we run out of data? Limits of LLM scaling based on human-generated data",
      "authors": [
        "P Villalobos",
        "A Ho",
        "J Sevilla",
        "T Besiroglu",
        "L Heim",
        "M Hobbhahn"
      ],
      "year": "2024",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "39",
      "title": "Why only text: empowering vision-and-language navigation with multi-modal prompts",
      "authors": [
        "H Hong",
        "S Wang",
        "Z Huang",
        "Q Wu",
        "J Liu"
      ],
      "year": "2024",
      "venue": "Proc. of IJCAI"
    },
    {
      "citation_id": "40",
      "title": "DeepSeek-VL: towards real-world visionlanguage understanding",
      "authors": [
        "H Lu",
        "W Liu",
        "B Zhang",
        "B Wang",
        "K Dong",
        "B Liu",
        "J Sun",
        "T Ren",
        "Z Li",
        "H Yang"
      ],
      "year": "2024",
      "venue": "DeepSeek-VL: towards real-world visionlanguage understanding",
      "arxiv": "arXiv:2403.05525"
    },
    {
      "citation_id": "41",
      "title": "Exploring cognitive and aesthetic causality for multimodal aspect-based sentiment analysis",
      "authors": [
        "L Xiao",
        "R Mao",
        "S Zhao",
        "Q Lin",
        "Y Jia",
        "L He",
        "E Cambria"
      ],
      "year": "2025",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Language is not all you need: Aligning perception with language models",
      "authors": [
        "S Huang",
        "L Dong",
        "W Wang",
        "Y Hao",
        "S Singhal",
        "S Ma",
        "T Lv",
        "L Cui",
        "O Mohammed",
        "B Patra"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "43",
      "title": "Judging LLM-as-a-judge with MT-bench and chatbot arena",
      "authors": [
        "L Zheng",
        "W.-L Chiang",
        "Y Sheng",
        "S Zhuang",
        "Z Wu",
        "Y Zhuang",
        "Z Lin",
        "Z Li",
        "D Li",
        "E Xing",
        "H Zhang",
        "J Gonzalez",
        "I Stoica"
      ],
      "year": "2023",
      "venue": "Proc. of NIPS"
    },
    {
      "citation_id": "44",
      "title": "LLaMA: Open and efficient foundation language models",
      "authors": [
        "H Touvron",
        "T Lavril",
        "G Izacard",
        "X Martinet",
        "M.-A Lachaux",
        "T Lacroix",
        "B Rozière",
        "N Goyal",
        "E Hambro",
        "F Azhar"
      ],
      "year": "2023",
      "venue": "LLaMA: Open and efficient foundation language models",
      "arxiv": "arXiv:2302.13971"
    },
    {
      "citation_id": "45",
      "title": "BLIP-2: Bootstrapping languageimage pre-training with frozen image encoders and large language models",
      "authors": [
        "J Li",
        "D Li",
        "S Savarese",
        "S Hoi"
      ],
      "year": "2023",
      "venue": "Proc. of ICML"
    },
    {
      "citation_id": "46",
      "title": "EVA: Exploring the limits of masked visual representation learning at scale",
      "authors": [
        "Y Fang",
        "W Wang",
        "B Xie",
        "Q Sun",
        "L Wu",
        "X Wang",
        "T Huang",
        "X Wang",
        "Y Cao"
      ],
      "year": "2023",
      "venue": "IEEE CVPR"
    },
    {
      "citation_id": "47",
      "title": "VADER: A parsimonious rule-based model for sentiment analysis of social media text",
      "authors": [
        "C Hutto",
        "E Gilbert"
      ],
      "year": "2014",
      "venue": "International AAAI conference on web and social media"
    },
    {
      "citation_id": "48",
      "title": "From pixels to sentiment: Fine-tuning CNNs for visual sentiment prediction",
      "authors": [
        "V Campos",
        "B Jou",
        "X Nieto"
      ],
      "year": "2017",
      "venue": "Image and Vision Computing"
    }
  ]
}