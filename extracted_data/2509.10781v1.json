{
  "paper_id": "2509.10781v1",
  "title": "Emoanti: Audio Anti-Deepfake With Refined Emotion-Guided Representations",
  "published": "2025-09-13T01:58:34Z",
  "authors": [
    "Xiaokang Li",
    "Yicheng Gong",
    "Dinghao Zou",
    "Xin Cao",
    "Sunbowen Lee"
  ],
  "keywords": [
    "audio anti-deepfake",
    "finetuning",
    "Wav2Vec2",
    "emotion-guided representations",
    "speech emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Audio deepfake is so sophisticated that the lack of effective detection on methods is fatal. While most detection systems primarily rely on low-level acoustic features or pretrained speech representations, they frequently neglect high-level emotional cues, which can offer complementary and potentially anti-deepfake information to enhance generalization. In this work, we propose a novel audio anti-deepfake system that utilize emotional features(EmoAnti) by exploiting a pretrained Wav2Vec2 (W2V2) model fine-tuned on emotion recognition tasks,which derive emotion-guided representations, then designing a dedicated feature extractor based on convolutional layers with residual connections to effectively capture and refine emotional characteristics from the transformer layers outputs. Experimental results show that our proposed architecture achieves state-of-the-art performance on both the ASVspoof2019LA and ASVspoof2021LA benchmarks, and demonstrating strong generalization on the ASVspoof2021DF dataset. Our proposed approach's code is available at Anonymous GitHub 1 .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Text-to-speech (TTS) and voice conversion (VC) technologies, empowered by the rapid advancement of deep learning, have achieved remarkable realism in synthesized speech generation  [1] . While this progress has expanded their application domains, it also raises significant security concerns: such synthetic speech can be exploited to launch spoofing attacks against automatic speaker verification (ASV) systems, potentially leading to financial loss and privacy breaches. This has driven increasing research interest in audio anti-deepfake as a critical countermeasure.\n\nEarly audio anti-deepfake methods primarily relied on low-level acoustic features such as fundamental frequency (f0)  [2]  and LFCC  [3] . For example, Xue et al.  [2]  combined f0 with real and imaginary spectrograms for anti-deepfake. While interpretable, these approaches often neglect high-level emotional information and exhibit limited generalization. Leveraging emotional features, Conti et al.  [4]  proposed a neural network to detect audio deepfake, whose current generation techniques are unable to accurately synthesize natural emotional behaviors in speech. To extract key features from deepfake audio, many approaches  [5, 6, 7]  use self-supervised speech models, which learn general-purpose representations from large-scale unlabeled speech, as their backbone. Although these approaches achieve promising performance, they lack the ability to interpret the general-purpose features learned by the pretrained models.\n\nTo address both the interpretability of learned representations and the generalization capability of the model, we propose a novel audio anti-deepfake framework: EmoAnti. Our approach is built upon two key components: 1) to capture high-level emotional information while enhancing the interpretability of speech embeddings, we adopt Wav2Vec2 as the backbone model and fine-tune it on a speech emotion recognition (SER) task; 2) to improve generalization, we design a convolutional residual feature extractor(CRFE) to refine emotion-guided representations from the transformer layers of Wav2Vec2 and identify subtle emotional discrepancies between bonafide and spoof speech.\n\nOur contributions are as follows:\n\n1) Based on emotional-aware embeddings, we propose a anti-deepfake method, which achieves strong generalization performance and maintains interpretability.\n\n2) To better capture and refine emotion-guided representations in speech, we design a convolutional residual feature extractor built upon Wav2Vec2.\n\n3) To validate the importance of emotional information in audio anti-deepfake, we conduct comprehensive ablation studies, which provide new insights for future research.\n\nFig.  1 : Overview of the EmoAnti framework.Dashed -line parts denote the internal detailed structures and the workflow during model operation.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Proposed Method",
      "text": "The overall architecture of the proposed model is illustrated in Fig.  1 . Given raw audio waveforms as input, the fine-tuned Wav2Vec2 model processes them through a series of Conv1d and transformer layers to extract high-level emotion-guided representations. These features are then passed to the convolutional residual feature extractor and feature fusion moudle, which further refines and fuses multi-level emotional cues to generate enhanced fusion feature vectors. Finally, the fused representations are fed into a classifier to generate the final prediction. This section provides a detailed description of each component in the framework.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Fine-Tuned Wav2Vec2 Model",
      "text": "In this study, we adopt the wav2vec2-large-robust model  [8, 9]  as the frontend encoder. It consists of three main components: 1) a 7-layer convolutional feature extractor that transforms raw waveforms into a feature sequence X = {x 1 , x 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , x N } using strided convolutions-initially with a stride of 5, followed by strides of 2-thereby reducing the temporal resolution; 2) a 12-layer transformer encoder (with 16 attention heads per layer) that captures long-range temporal dependencies; 3) a quantization module for latent speech representation learning. Let\n\ndenote the output of the l -th transformer layer (l ‚àà  [1, 12] ), and let\n\nl=1 represent the collection of outputs from all 12 transformer layers.\n\nTo adapt the pretrained model for emotion recognition and audio anti-deepfake, we fine-tune it by retaining the backbone of wav2vec2-large-robust-including the convolutional feature extractor and transformer encoder-to leverage its latent speech representations, while adding task-specific classification heads. For emotion recognition (Fig.  2 ), the classification head takes the final transformer layer's output H L as input and predicts emotion categories by leveraging paralinguistic cues. This enables the model to effectively transfer the emotion-guided representations learned during pretraining to the task of audio anti-deepfake.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Conv1D Feature Encoder",
      "text": "Transformer Layer",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Wav2Vec2",
      "text": "Emotional Classifier üòÄ üòê üò° ‚òπ Fig.  2 : Fine-tuning for four-category emotion recognition (happy, neutral, angry and sad from top to bottom, corresponding to the labels).",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Convolutional Residual Feature Extractor",
      "text": "While final-layer representations of pre-trained transformers suffice for speech recognition, prior work shows that earlier and intermediate layers yield more discriminative features for speaker and paralinguistic tasks  [10, 11] . Inspired by this, we design a convolutional residual feature extractor to aggregate and refine emotional representations from multiple trans-former layers, thereby enhancing the ability to distinguish subtle emotional details in audio deepfakes. Specifically, the module consists of four sequential convolutional residual blocks. Each block's main branch contains two Conv1d layers (kernel size = 3, padding = 1): the first projects the input dimension d in to a hidden dimension d hidden through BatchNorm1d and ReLU activation, while the second preserves d hidden with an additional BatchNorm1d layer.\n\nwhere H l is the input hidden representation from the l-th transformer layer.\n\nThe residual branch uses a 1 √ó 1 Conv1d if d in Ã∏ = d hidden , otherwise it applies identity mapping:\n\nThe block output is then obtained by element-wise addition and ReLU activation:\n\nfollowed by transposing f to [B, T, d hidden ] for storage and further use.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Fusion Module",
      "text": "To effectively aggregate the refined emotion-guided representations from the CRFE, we introduce a feature fusion module that applies a dedicated temporal attention subnetwork to each output feature f i generated by the convolutional residual blocks, enabling adaptive weighting of temporal segments across different levels of abstraction.\n\nwhere f i,t denotes the t-th temporal segment of f i and e i,t denotes the attention score, Œ± i,t is its normalized weight, and fi ‚àà R dhidden is the temporally aggregated feature.\n\nFinally, we concatenate all fj (j ‚àà [1, i])into the fused feature:\n\n(5)",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Audio Deepfake Classifier",
      "text": "This module takes the fused feature matrix F fused ‚àà R B√óDtotal as input, where B denotes the batch size and D total represents the total dimensionality of the fused features. It performs classification through a series of linear transformations, nonlinear activations, and regularization mechanisms. Mathematically, the entire process can be expressed as:\n\n3. EXPERIMENTS",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets And Evaluation Metrics",
      "text": "We fine-tune the model on IEMOCAP  [12] , following the emotion label selection in prior work  [13] : \"angry\", \"happy\" (including \"excited\"), \"sad\", and \"neutral\", resulting in 5,531 utterances. Performance is evaluated using accuracy and macro F1-score.\n\nFor anti-deepfake, we evaluate on ASVspoof2019 LA and ASVspoof2021 LA/DF  [14, 15, 16] . Training is performed on the ASVspoof2019, as ASVspoof2021 provides no new training data. The ASVspoof2021 are more challenging, with 2021 LA introducing new speakers and transmission artifacts, while 2021 DF targets compressed deepfake attacks. Evaluation metrics are the minimum normalized tandem detection cost function (min t-DCF)  [17]  and equal error rate (EER).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Training Setup",
      "text": "During fine-tuning, the Wav2Vec2 model was optimized using the AdamW optimizer (learning rate = 1 √ó 10 -5 , weight decay = 0.01), with a cosine annealing learning rate schedule and 10% warm-up. Gradient accumulation was employed to simulate larger effective batch sizes. Model selection was based on the macro F1 score on a validation set, with checkpoints evaluated periodically and the best models retained for downstream tasks. Training was performed for 8 epochs.\n\nFor audio anti-deepfake, we used the Adam optimizer (learning rate = 1 √ó 10 -4 ) and CrossEntropyLoss. Models were trained for 6 epochs with evaluation after each epoch, and the achieving the lowest validation loss was preserved. All experiments were conducted with dataset-specific random seeds: 43, 44, 45 for ASVspoof2019 LA; 43, 45, 456 for ASVspoof2021 LA; and 46, 47, 78 for ASVspoof2021 DF.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results And Analysis",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Results On Asvspoof2019 La",
      "text": "Table 1 compares our method with representative approaches that use either handcrafted low-level acoustic features or selfsupervised speech models as frontends. On the ASVspoof2019LA Table  1 : Comparison results of EER (%) and min t-DCF between our proposed method and other anti-deepfake systems on the ASVspoof2019LA evaluation set.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "System",
      "text": "EER (%) min t-DCF LFCC-GMM  [18]  8.09 0.212 CQCC-GMM  [18]  9.57 0.237 LFCC-LCNN  [19]  5.06 0.237 RawNet2  [20]  5.64 0.130 ECAPA-TDNN  [21]  4.58 0.117 W2V2 (fixed)+LCNN+BLSTM  [22]  1.47 0.105 W2V2 (finetuned)+LCNN+BLSTM  [22]  2.31 0.120\n\nEmoAnti (ours) 0.44 0.0139 evaluation set, our model achieves state-of-the-art performance with an EER of 0.44% and a min t-DCF of 0.0139. Among these methods, those based on handcrafted lowlevel features (e.g., LFCC, CQCC) exhibit limited performance, while approaches leveraging pretrained models such as Wav2Vec2 show significant improvements.\n\nOur approach further enhances representation learning by fine-tuning the pretrained model on emotion recognition and refining the resulting representations through the CRFE, enabling it to capture subtle high-level emotional variations in speech. This strong performance demonstrates the effectiveness of modeling emotional discrepancies for anti-deepfake. Our results suggest that leveraging refined emotional cues not only improves detection accuracy but also opens a promising new direction for research in audio anti-deepfake.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Ablation Study",
      "text": "Fig.  3  summarizes our ablation studies with three experiments: 1) removing emotion-aware fine-tuning; 2) removing CRFE while keeping fine-tuning; 3) removing both.\n\nOn the ASVspoof2019 LA dataset, EmoAnti performing the best at 0.44%. On ASVspoof2021 LA, EmoAnti again outperforms other models, achieving an EER of 4.62%. Notably, on the 2021 DF dataset, removing only the emotionaware fine-tuning yields the worst performance with an EER of 16.14%, whereas removing both fine-tuning and the CRFE results in the best performance at 11.46% EER.\n\nThese results demonstrate that the coupling of emotionaware fine-tuning and the CRFE is highly effective for audio deepfake detection. The observed performance trend on the 2021 DF dataset may due to the model overfit to characteristics of the Logical Access (LA) data or the CRFE tends to capture fine-grained local features at the expense of global contextual information, potentially compromising generalization to compressed and transcoded speech.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose EmoAnti for audio anti-deepfake, which is evaluated on three datasets. The main contribution of EmoAnti is constructing a Wav2Vec2 fine-tuned on emotion recognition tasks to extract emotional cues from speech, with subsequent refinement via a convolutional residual feature extractor to distinguish emotional differences between spoofed and bonafide speech. Comparative experiments demonstrate that our approach leveraging emotional details achieves excellent performance in audio anti-deepfake. Meanwhile, ablation studies further validate the significance of emotional information for anti-deepfake. There is still room for improvement: we can replace the CRFE with a model that attends to global information to enhance generalization on the 2021DF dataset.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the EmoAnti framework.Dashed - line parts denote the internal detailed structures and the workflow during",
      "page": 2
    },
    {
      "caption": "Figure 1: Given raw audio waveforms as input, the fine-tuned",
      "page": 2
    },
    {
      "caption": "Figure 2: ), the classifica-",
      "page": 2
    },
    {
      "caption": "Figure 2: Fine-tuning for four-category emotion recognition",
      "page": 2
    },
    {
      "caption": "Figure 3: Ablation Study Results (EER%).",
      "page": 4
    },
    {
      "caption": "Figure 3: summarizes our ablation studies with three experi-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "W2V2(unft)+(CRFE)\nW2V2(ft)": "W2V2(unft)\nEmoAnti",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "13.72": "12.57",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "W2V2(unft)+(CRFE)\nW2V2(ft)": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "13.72": "",
          "Column_10": "11.46",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "W2V2(unft)+(CRFE)\nW2V2(ft)": "8.19",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "13.72": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "W2V2(unft)+(CRFE)\nW2V2(ft)": "6.79 6.71",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "13.72": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "W2V2(unft)+(CRFE)\nW2V2(ft)": "",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "4.62",
          "Column_7": "",
          "Column_8": "",
          "13.72": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        },
        {
          "W2V2(unft)+(CRFE)\nW2V2(ft)": "0.55 0.92 0.97 0.44",
          "Column_2": "",
          "Column_3": "",
          "Column_4": "",
          "Column_5": "",
          "Column_6": "",
          "Column_7": "",
          "Column_8": "",
          "13.72": "",
          "Column_10": "",
          "Column_11": "",
          "Column_12": ""
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Audio deepfake detection: A survey",
      "authors": [
        "J Yi",
        "C Wang",
        "J Tao",
        "X Zhang",
        "C Zhang",
        "Y Zhao"
      ],
      "year": "2023",
      "venue": "Audio deepfake detection: A survey",
      "arxiv": "arXiv:2308.14970"
    },
    {
      "citation_id": "3",
      "title": "Audio deepfake detection based on a combination of f0 information and real plus imaginary spectrogram features",
      "authors": [
        "J Xue",
        "C Fan",
        "Z Lv",
        "J Tao",
        "J Yi",
        "C Zheng",
        "Z Wen",
        "M Yuan",
        "S Shao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 1st international workshop on deepfake detection for audio multimedia"
    },
    {
      "citation_id": "4",
      "title": "Voice spoofing countermeasure for logical access attacks detection",
      "authors": [
        "T Arif",
        "A Javed",
        "M Alhameed",
        "F Jeribi",
        "A Tahir"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "5",
      "title": "Deepfake speech detection through emotion recognition: a semantic approach",
      "authors": [
        "E Conti",
        "D Salvi",
        "C Borrelli",
        "B Hosler",
        "P Bestagini",
        "F Antonacci",
        "A Sarti",
        "M Stamm",
        "S Tubaro"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "6",
      "title": "The vicomtech audio deepfake detection system based on wav2vec2 for the 2022 add challenge",
      "authors": [
        "J Mart√≠n-Do√±as",
        "A √Ålvarez"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "7",
      "title": "Audio anti-spoofing based on audio feature fusion",
      "authors": [
        "J Zhang",
        "G Tu",
        "S Liu",
        "Z Cai"
      ],
      "year": "2023",
      "venue": "Algorithms"
    },
    {
      "citation_id": "8",
      "title": "Audio deepfake detection with selfsupervised wavlm and multi-fusion attentive classifier",
      "authors": [
        "Yinlin Guo",
        "Haofan Huang",
        "Xi Chen",
        "He Zhao",
        "Yuehai Wang"
      ],
      "year": "2024",
      "venue": "ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve"
      ],
      "year": "2021",
      "venue": "Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training",
      "arxiv": "arXiv:2104.01027"
    },
    {
      "citation_id": "10",
      "title": "Wav2Vec2 -Large -Robust model",
      "authors": [
        "A Facebook"
      ],
      "venue": "Wav2Vec2 -Large -Robust model"
    },
    {
      "citation_id": "11",
      "title": "Large-scale self-supervised speech representation learning for automatic speaker verification",
      "authors": [
        "Z Chen",
        "S Chen",
        "Y Wu",
        "Y Qian",
        "C Wang",
        "S Liu",
        "Y Qian",
        "M Zeng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "12",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "13",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "15",
      "title": "Asvspoof 2019: Future horizons in spoofed and fake audio detection",
      "authors": [
        "M Todisco",
        "X Wang",
        "V Vestman",
        "M Sahidullah",
        "H Delgado",
        "A Nautsch",
        "J Yamagishi",
        "N Evans",
        "T Kinnunen",
        "K Lee"
      ],
      "year": "2019",
      "venue": "Asvspoof 2019: Future horizons in spoofed and fake audio detection",
      "arxiv": "arXiv:1904.05441"
    },
    {
      "citation_id": "16",
      "title": "Asvspoof 2021: accelerating progress in spoofed and deepfake speech detection",
      "authors": [
        "J Yamagishi",
        "X Wang",
        "M Todisco",
        "M Sahidullah",
        "J Patino",
        "A Nautsch",
        "X Liu",
        "K Lee",
        "T Kinnunen",
        "N Evans"
      ],
      "year": "2021",
      "venue": "Asvspoof 2021: accelerating progress in spoofed and deepfake speech detection",
      "arxiv": "arXiv:2109.00537"
    },
    {
      "citation_id": "17",
      "title": "Asvspoof 2021: Towards spoofed and deepfake speech detection in the wild",
      "authors": [
        "X Liu",
        "X Wang",
        "M Sahidullah",
        "J Patino",
        "H Delgado",
        "T Kinnunen",
        "M Todisco",
        "J Yamagishi",
        "N Evans",
        "A Nautsch"
      ],
      "year": "2023",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "18",
      "title": "t-dcf: a detection cost function for the tandem assessment of spoofing countermeasures and automatic speaker verification",
      "authors": [
        "T Kinnunen",
        "K Lee",
        "H Delgado",
        "N Evans",
        "M Todisco",
        "M Sahidullah",
        "J Yamagishi",
        "D Reynolds"
      ],
      "year": "2018",
      "venue": "t-dcf: a detection cost function for the tandem assessment of spoofing countermeasures and automatic speaker verification",
      "arxiv": "arXiv:1804.09618"
    },
    {
      "citation_id": "19",
      "title": "ASVspoof 2021 Baseline CM & Evaluation Package",
      "year": "2021",
      "venue": "ASVspoof 2021 Baseline CM & Evaluation Package"
    },
    {
      "citation_id": "20",
      "title": "Known-unknown data augmentation strategies for detection of logical access, physical access and speech deepfake attacks: Asvspoof 2021",
      "authors": [
        "R Das"
      ],
      "year": "2021",
      "venue": "Proc. 2021 Edition of the Automatic Speaker Verification and Spoofing Countermeasures Challenge"
    },
    {
      "citation_id": "21",
      "title": "End-to-end anti-spoofing with rawnet2",
      "authors": [
        "H Tak",
        "J Patino",
        "M Todisco",
        "A Nautsch",
        "N Evans",
        "A Larcher"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "22",
      "title": "Ur channelrobust synthetic speech detection system for asvspoof 2021",
      "authors": [
        "X Chen",
        "Y Zhang",
        "G Zhu",
        "Z Duan"
      ],
      "year": "2021",
      "venue": "Ur channelrobust synthetic speech detection system for asvspoof 2021",
      "arxiv": "arXiv:2107.12018"
    },
    {
      "citation_id": "23",
      "title": "Investigating selfsupervised front ends for speech spoofing countermeasures",
      "authors": [
        "X Wang",
        "J Yamagishi"
      ],
      "year": "2021",
      "venue": "Investigating selfsupervised front ends for speech spoofing countermeasures",
      "arxiv": "arXiv:2111.07725"
    }
  ]
}