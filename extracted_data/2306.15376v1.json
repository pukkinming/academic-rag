{
  "paper_id": "2306.15376v1",
  "title": "Exploiting Pseudo Future Contexts For Emotion Recognition In Conversations",
  "published": "2023-06-27T10:51:02Z",
  "authors": [
    "Yinyi Wei",
    "Shuaipeng Liu",
    "Hailei Yan",
    "Wei Ye",
    "Tong Mo",
    "Guanglu Wan"
  ],
  "keywords": [
    "Emotion Recognition in Conversations",
    "Conversation Understanding",
    "Pseudo Future Contexts"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "With the extensive accumulation of conversational data on the Internet, emotion recognition in conversations (ERC) has received increasing attention. Previous efforts of this task mainly focus on leveraging contextual and speaker-specific features, or integrating heterogeneous external commonsense knowledge. Among them, some heavily rely on future contexts, which, however, are not always available in real-life scenarios. This fact inspires us to generate pseudo future contexts to improve ERC. Specifically, for an utterance, we generate its future context with pre-trained language models, potentially containing extra beneficial knowledge in a conversational form homogeneous with the historical ones. These characteristics make pseudo future contexts easily fused with historical contexts and historical speaker-specific contexts, yielding a conceptually simple framework systematically integrating multicontexts. Experimental results on four ERC datasets demonstrate our method's superiority. Further in-depth analyses reveal that pseudo future contexts can rival real ones to some extent, especially in relatively context-independent conversations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversations (ERC), aiming to identify the emotion of each utterance in a conversation, is an essential part of conversation understanding. It plays a significant role for various downstream tasks, e.g., opinion mining in social media, building intelligent assistant through empathetic machine, health care, and detecting fake news in social media  [13, 20] .\n\nA common solution for ERC is to jointly leverage the contextual and speaker information of an utterance, which can be roughly divided into sequence-based\n\n[Ross]: I mean technically it's-it's not against the rules or anything, but it is frowned upon.\n\n[Ross]: Especially by that professor we ran into last night, Judgey von Holierthanthou.\n\n[Monica]: Well Ross, you be careful now. You don't want to get a reputation as y'know Professor McNailshisstudents.\n\n[Ross]: Yeah. What-what should I do?\n\n[Joey]: Well Ross, it seems pretty clear.\n\n[Joey]: Ross, you gotta follow your heart.\n\n[Monica]: Joey that is so sweet. [Bot]: Well, I guess it depends on the server.\n\n[Bot]: You mean the one who was in the middle of a class and was trying to get a teacher to give him a grade?\n\n[Bot]: No, the professor who gave him the grade.\n\n[Bot]: Oh, I know. I'm just saying, it's not like I was doing anything wrong.\n\n[Bot]: You're not wrong, but I don't think it was a good idea.\n\n[Bot]: You should probably just leave.\n\n[Bot]: Alright, bye.\n\n[Bot]: Okay. I'll do that.\n\n[Bot]: Alright. See you around.\n\n[Bot]: You're a sweetheart! [Bot]: Awwww, thanks.\n\n[Bot]: I'm not sure what to do.\n\n[Bot]: You can't do anything.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "[The Original Conversation] [Generated Pseudo Future Utterances]",
      "text": "Fig.  1 : An conversation of MELD. Original utterances are in dark-colored blocks and generated pseudo future contexts are in light-colored blocks.\n\nmethods  [16, 11]  and graph-based methods  [6, 19] . To further enrich conversational information, another line of research proposes to enhance representations of utterances with knowledge augmentation. These methods are mainly devoted to introducing external commonsense knowledge to facilitate comprehension of utterances, and have amply proved their effectiveness  [4, 28] . Though many sophisticated designs have been proposed to exploit context information, we found some of them heavily rely on future contexts  [6] . The problem is that we may face the unavailability of future contexts. For example, imaging a scenario of detecting the emotion of an utterance just-in-time in an after-sale conversation, we will have no future contexts available. This fact motivates us to simulate unseen future states by generating pseudo future contexts. As shown in Figure  1 , for each utterance, we can use a pre-trained language model (e.g., DialoGPT  [26] ) to generate its future context, introducing consistent yet extra beneficial knowledge for emotion recognition. Compared with heterogeneous external knowledge used in previous works (e.g., commonsense knowledge), the knowledge extracted is represented in a conversational form homogeneous with historical contexts, and hence can be easily integrated.\n\nWe further design a novel context representation mechanism that can be applied indiscriminately to multi-contexts, including historical contexts, historical speaker-specific contexts, and pseudo future contexts. Specifically, for each type of contexts, we employ relative position embeddings to capture the positional relationship between an utterance and its surroundings, obtaining a refined representation of an utterance, as well as a local conversational state. We then utilize GRUs to model how the local state evolves as the conversation progresses, characterizing long-term utterance dependencies from a global view. A final representation, which fuses multi-contexts information from both local and global perspectives, is fed into a classifier to produce the emotion label of an utterance.\n\nWe conduct extensive experiments on four ERC datasets, and the empirical results verify the effectiveness and potential of our method.\n\nIn summary, our contributions are:  (1)  We propose improving ERC from a novel perspective of generating pseudo future contexts for utterances, incorporating utterance-consistent yet potentially diversified knowledge from pre-trained language models. (2) We design a simple yet effective ERC framework to integrate pseudo future contexts with historical ones and historical speaker-specific ones, yielding competitive results on four widely-used ERC datasets.  (3)  We analyze how pseudo future contexts correlate with characteristics of emotionconsistency and context-dependency among utterances in conversations, revealing that pseudo future contexts resemble real ones in some scenarios.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Works",
      "text": "Works on ERC can be roughly categorized into sequence-based methods and graph-based methods.\n\n(1) Sequence-Based Methods. Sequence-based methods treat each utterance as a discrete sequence with RNNs  [3]  or Transformers  [21] .  [16]  and  [7]  utilized RNNs to track context and speaker states.  [10]  used two levels of Transformers to capture contextual information and seize speaker information with an auxiliary task.  [22]  treated ERC as an utterance-level sequence tagging task with a conditional random field layer.  [11]  utilized BART with supervised contrastive learning and an auxiliary response generation task.\n\n(2) Graph-Based Methods. Treating an utterance as a node, contextual and speaker relations as edges, ERC can be modelled using graph neural networks.  [6]  modelled both the context-and speaker-sensitive dependencies with graph convolutional networks. To reflect the relational structure,  [8]  proposed relational position encodings in graph attention networks. Considering the temporal property,  [19]  proposed to encode with a directed acyclic graph.\n\n(3) ERC with External Knowledge. Some works propose to introduce heterogeneous commonsense knowledge for ERC.  [27]  combined word and concept embeddings with two knowledge sources. Taking advantage of COMET, a Transformer-based generative model  [1]  for generating commonsense descriptions,  [4, 28, 9]  enhanced the representations of utterances with commonsense knowledge. It is noteworthy that our method distinguishes itself by not relying on the design of intricate instructions or prompts to extract knowledge from the pre-trained language models, setting it apart from other methods that utilize generative models as knowledge bases  [23, 14] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Task Definition",
      "text": "Formally, denote U, P and Y as conversation set, speaker set and label set. For a conversation U ‚àà U, U = (u 0 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , u n-1 ), where u i is the i-th utterance. The",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Exploiting Multi-Contexts Of Utterance",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Relative Multi-Head Self-Attention & Ff",
      "text": "State Gate ùë†! .\n\n(b) Historical Speaker-Specific Context",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "ùë°! /",
      "text": "Exploiting Multi-Contexts of Utterance i speaker of u i is denoted by function P (‚Ä¢). For example, P (u i ) = p j , p j ‚àà P means that u i is uttered by p j . The goal of ERC is to assign an emotion label y i ‚àà Y to each u i , formulated as an utterance-level sequence tagging task in this work.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Exploiting Multi-Contexts Of Utterance I",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Utterance-Level Encoder Multi-Contexts Exploiting Classifier",
      "text": "As with most previous works  [4, 28] , for an utterance, we only use its historical utterances, while future utterances are not available to fit real-life scenarios.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Generation Of Pseudo Future Contexts",
      "text": "Knowing that DialoGPT  [26]  is trained on large-scale conversational data (e.g., a 27GB Reddit dataset) and can generate responses of high quality, we employ it as a knowledge base to introduce homogeneous external knowledge. Given an utterance u i , we utilize a DialoGPT G to recursively generate a pseudo future context g 0:m i by considering at most k historical utterances prior to u i , where k = max (i, k), m is the maximum number of generated utterances.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Proposed Framework",
      "text": "Figure  2  shows the overall architecture, which is named ERCMC (Emotion Recognition in Conversations with Multi-Contexts).\n\nUtterance-Level Encoder Given a conversation U = (u 0 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , u n-1 ), for u i ‚àà U , we first obtain m generated pseudo future utterances g 0:m i . Then the utterance and its pseudo future utterances are fed into an encoder M (RoBERTa  [15]  in our experiments) to obtain their representations (we represent an utterance with the embedding of the [CLS] token).\n\nwhere x i , r j i ‚àà R dm . x i and r j i are the representations of u i and g j i respectively.\n\nMulti-Contexts Exploiting To exploit multi-contexts for an utterance, we consider the three local areas for x i : the historical context C i , the historical context of the same speaker S i and the future pseudo context A i .\n\nwhere ‚Ñì + 1 is the size of the three local areas. In S i , P (u bj ) = P (u i ). In C i and S i , ‚Ñì = max (i, ‚Ñì), and in A i , m is set to the initial value of ‚Ñì.\n\nFor each local area of an utterance, we calculate three representations focusing on local-aware state, local state, and evolution of local states. We take C i as an example in the following calculation.\n\nWe first use a multi-head self-attention layer with relative position embeddings  [18]  to obtain a local-aware embedding considering C i :\n\nwhere h c j ‚àà R dm , ‚à• is the concatenation of two vectors,\n\nare relative position embeddings between x j and x k . Similarly, for S i and A i , two embeddings h s and h a are obtained.\n\nA state gate is then used to get the local state s c i :\n\nwhere\n\nSimilarly, for S i and A i , two local states s s i and s a i are obtained. For states in a global view, a GRU unit is utilized to characterize the evolution of local states:\n\nwhere t c i ‚àà R dm is the tracked global state prior to u i and t c 0 is initialized with zero. Similarly, for S i and A i , two GRU units are used to get t s i and t a i .\n\nClassifier For u i , the exploited outcomes from multi-contexts are integrated into a final representation f i .\n\nwhere\n\n, we apply a feed forward network and a softmax layer:\n\nwhere\n\nThe model is trained using negative log-likelihood loss.\n\n4 Experimental Setups",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Datasets And Evaluation Metrics",
      "text": "We evaluate our proposed framework on four ERC datasets: IEMOCAP  [2] , DailyDialog  [12] , EmoryNLP  [25] , MELD  [17] . IEMOCAP and DailyDialog are two-party datasets, while EmoryNLP and MELD are multi-party datasets. For the four datasets, we only use the textual parts. Statistics of these datasets are shown in Table  1 .\n\nFollowing  [4, 28] , we choose weighted-average F1 for IEMOCAP, EmoryNLP and MELD. Since the neutral class constitutes to 83% of the DailyDialog, microaveraged F1 excluding neutral is chosen.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baselines",
      "text": "For models without external knowledge, we compare with: (1) Sequence-based methods: DialogueRNN  [16]  uses GRUs to track context and speaker states; HiTrans  [10]  utilizes two level Transformers with an auxiliary task to leverage both contextual and speaker information; CoG-BART  [11]  uses BART with supervised contrastive learning and a response generation task. (2) Graph-based Methods: DialogueGCN [6] models dependencies about context and speaker with graph convolutional networks; RGAT  [8]  uses relational graph attention networks with relational position encodings.\n\nFor models with external knowledge, we compare with: KET  [27]  uses a Transformer to combine word and concept embeddings; COSMIC  [4]  is a modified DialogueRNN with commonsense knowledge from COMET; TODKAT leverages COMET to integrate commonsense knowledge and a topic model to detect the potential topics of a conversation; SKAIG  [9]  models structural psychological interactions between utterances with commonsense knowledge.\n\nThree variants of ERCMC are compared: (1) ERCMC without future contexts; (2) ERCMC with multi-contexts; (3) ERCMC using real future contexts.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Implementation Details",
      "text": "We use DialoGPT-medium to generate 5 pseudo future utterances for each utterance (i.e., m = 5) with k historical utterances, where k = 2 for IEMOCAP and DailyDialog, and k = 4 for EmoryNLP and MELD. For backbones, we select RoBERTa-base for IEMOCAP and RoBERTa-large for the other datasets. For each utterance, the max length is set to 128. For multi-contexts exploiting, we set the size of the three local areas to 6 (i.e., ‚Ñì = 5). The number of head n h is set to 8. d m is set to 768 for IEMOCAP and 1,024 for the other datasets. For training setup, the batch size is set to 1 conversation for IEMOCAP and DailyDialog, and 2 conversations for EmoryNLP and MELD. The gradient accumulation step is set to 16 for DailyDialog and 4 for IEMOCAP, EmoryNLP and MELD. We train the model for 20 epochs for IEMOCAP and 10 epochs for the other datasets with the AdamW optimizer whose learning rate is set to 3e-5 for IEMOCAP and 1e-5 for the other datasets. Each model is trained for max epochs and choose the checkpoint with the best validation performance. The dropout rate is set to 0.1. All the experiments are done on a single NVIDIA Tesla V100. All of our results are the average of 5 runs. We do not implement on larger generative models, such as ChatGPT, due to their significant computational resources requirements and associated costs. Our code is available at https://github.com/Ydongd/ERCMC.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results And Insights",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Overall Results",
      "text": "Overall results are shown in Table  2 , from which we have three main observations:\n\n(1) Compared with models using future contexts, ERCMC in C&S&RF setting achieves competitive performance, demonstrating the superiority of our proposed framework.\n\n(2) In stead of leveraging heterogeneous commonsense knowledge as previous works, we use homogeneous conversational knowledge, allowing ER-CMC in C&S&PF setting to overtake other models using external knowledge.\n\n(3) Additional future contexts provide more information about conversations and thus bring improvements over C&S setting when using pseudo or real future contexts in our model. Compared with using real future contexts, using pseudo future contexts performs better on EmoryNLP and MELD, and underperforms on IEMOCAP and DailyDialog. An in-depth analysis is given in Section 5.4.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Collaboration Of Multi-Contexts",
      "text": "To reveal the collaborative effect of multi-contexts on ERCMC, we show various combinations of multi-contexts in Table  3 , from which we observe that: (1) All combinations improve the performance compared with only using raw information, suggesting that the context information is critical for ERC. (2) Regardless of the number of contexts used, settings with historical speaker-specific contexts produce the best results, indicating that historical contexts uttered by the same speaker is the predominant part. (3) Using C or S alone, and a combination of both, bring more improvements than using PF and its combination. The observation implies that historical contexts and historical speaker-specific contexts are more significant for ERC, while pseudo future contexts serve as a supplementary role to provide extra knowledge.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Ablation Study",
      "text": "Effect of Compositions of the Final Representations. We form the final representations with local-aware embeddings, local states and tracked global states, which are denoted as h, s, and t, respectively. Results with removal of each part are shown in Table  4a , from which we can find that though the calculation of local states and tracked global states contain some information from localaware embeddings, performances drop dramatically when original embeddings  Effect of Position Embeddings. Unlike modeling the distances between utterances with sinusoidal position embeddings in previous sequence-based methods  [27, 10] , we utilize relative position embeddings and further compare with three position embeddings in Table  4b : without position embeddings; sinusoidal position embeddings that use sine and cosine functions of different frequencies; position embeddings learned from scratch. It is clear that the distance information exploited by relative position embeddings at utterance-level is more compatible with a sequence-based method and therefore achieves better results.\n\nEffect of Size of Local Areas. In our experiments, we set the size of local areas to 6 (i.e., using five historical utterances at most). We further investigate the size effect of local areas by employing ERCMC in C&S setting. From the results in Figure  3 , we can draw a general trend that performances show an upward tendency when starting using historical utterances and decrease when using all historical contexts due to information redundancy. We can also see that compared with IEMOCAP and DailyDialog, MELD and EmoryNLP have a smoother performance, especially at the beginning and end of the curve.",
      "page_start": 9,
      "page_end": 11
    },
    {
      "section_name": "Future Context: Pseudo Or Real",
      "text": "In Section 5.1, we have mentioned that both pseudo and real future contexts improve the performance of ERCMC, and compared with the C&S&RF setting, the C&S&PF setting performs better on EmoryNLP and MELD and worse on IEMOCAP and DailyDialog. This section provides a more in-depth analysis of how pseudo and real future contexts affect the final results. We first categorize the four datasets concerning several previous observations. Firstly, in Table  3 , C&S setting obtains relative improvements over RAW setting of 15.92% on IEMOCAP, 4.16% on DailyDialog, 2.46% on EmoryNLP and 1.80% on MELD. Secondly, in Table  4a , compared with the full C&S&PF setting, when the original local-aware embeddings are removed, the performances drops by 4.83% on IEMOCAP, 1.27% on DailyDialog, 48.96% on EmoryNLP and 21.53% on MELD. Thirdly, in Figure  3 , the curves of EmoryNLP and MELD are stabler than those of IEMOCAP and DailyDialog.\n\nFrom them, we can conclude that conversations in IEMOCAP and DailyDialog are more context-dependent, while conversations in EmoryNLP and MELD are relatively context-independent. An alternative explanation for this conclusion is that IEMOCAP and DailyDialog are two-party datasets, and therefore the conversations are more emotion-focused, whereas EmoryNLP and MELD are multi-party datasets, resulting in more diffuse conversations and inconsistent emotions.\n\nTo further explore the effect of using pseudo or real future contexts, we attempt to investigate the attributes of future contexts. Previous works  [5, 24]  have reported a common issue with ERC: different emotions in consecutive utterances may confound a model and thereby degrade performance. Inspired by the issue, we define a new concept of \"emotion-consistency\" as the degree of emotional consistency of the subsequent utterances with the first utterance within a local area. Given a local area LC = (u 0 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , u ‚Ñì ), the emotion-consistency is:\n\nwhere W T = (wt 0 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , wt ‚Ñì-1 ) is the weight in different positions, œï(u i , u 0 ) is a function to indicate whether u i and u 0 having the same emotion.\n\nFor quantitative analysis, we employ ERCMC in C setting to predict emotions and calculate the emotion-consistency scores. To align with pseudo future contexts, we simplify the test sets to ensure each utterance has at least ‚Ñì consecutive utterances (‚Ñì = 5 in our experiments). Two kinds of weight are considered, wt 1 i = 1 ‚Ñì , wt 1 i ‚àà W T 1 ; wt 2 i = exp e ‚Ñì-i ‚Ñì j=1 exp e j-1\n\n, wt 2 i ‚àà W T 2 . W T 1 relies uniformly on consecutive utterances, while W T 2 favours utterances near the first utterance.\n\nFrom Table  5a , 5b, 5c and 5d, we can observe that: (1) Emotion-consistency scores of IEMOCAP and DailyDialog are higher than those of EmoryNLP and MELD. The finding confirms our previous observations that IEMOCAP and DailyDialog are more context-dependent, whereas EmoryNLP and MELD are somewhat context-independent. (2) Performance and emotion-consistency scores are positively correlated. And combined with Table  2 , using pseudo future contexts achieves competitive results on IEMOCAP and DailyDialog, and outperforms using real future contexts on EmoryNLP and MELD. The results demonstrate that pseudo future contexts can replace real ones to some extent when the dataset is context-dependent, and serve as more extra beneficial knowledge when the dataset is relatively context-independent.",
      "page_start": 11,
      "page_end": 12
    },
    {
      "section_name": "Case Study",
      "text": "We illustrate two examples to promote understanding of our method in Figure  4 .\n\nIn the first case from IEMOCAP, the emotions of the two speakers are consistent, which greatly helps to distinguish emotions with similar meanings, such as \"Excited\" and \"Happy\", while pseudo future contexts mislead the model to some extent.",
      "page_start": 12,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An conversation of MELD. Original utterances are in dark-colored blocks",
      "page": 2
    },
    {
      "caption": "Figure 1: , for each utterance, we can use a pre-trained",
      "page": 2
    },
    {
      "caption": "Figure 2: The overall architecture of our proposed ERCMC framework. Utterances",
      "page": 4
    },
    {
      "caption": "Figure 2: shows the overall architecture, which is named ERCMC (Emotion",
      "page": 4
    },
    {
      "caption": "Figure 3: Effect of number of historical utterances with ERCMC in C&S setting.",
      "page": 10
    },
    {
      "caption": "Figure 3: , we can draw a general trend that performances show an",
      "page": 11
    },
    {
      "caption": "Figure 3: , the curves of EmoryNLP and MELD",
      "page": 11
    },
    {
      "caption": "Figure 4: In the first case from IEMOCAP, the emotions of the two speakers are con-",
      "page": 12
    },
    {
      "caption": "Figure 4: Two cases from IEMOCAP and MELD. In the boxes on the left, from top",
      "page": 13
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "Abstract. With the extensive accumulation of conversational data on"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "the Internet, emotion recognition in conversations\n(ERC) has\nreceived"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "increasing attention. Previous efforts of this task mainly focus on lever-"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "aging contextual and speaker-specific features, or\nintegrating heteroge-"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "neous external commonsense knowledge. Among them, some heavily rely"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "on future contexts, which, however, are not always available in real-life"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "scenarios. This fact inspires us to generate pseudo future contexts to im-"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "prove ERC. Specifically,\nfor an utterance, we generate its future context"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "with pre-trained language models, potentially containing extra benefi-"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "cial knowledge in a conversational\nform homogeneous with the histori-"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "cal ones. These characteristics make pseudo future contexts easily fused"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "with historical\ncontexts and historical\nspeaker-specific\ncontexts, yield-"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "ing a conceptually simple\nframework systematically integrating multi-"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "contexts. Experimental\nresults on four ERC datasets demonstrate our"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "method‚Äôs\nsuperiority. Further\nin-depth analyses\nreveal\nthat pseudo fu-"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "ture contexts can rival real ones to some extent, especially in relatively"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "context-independent conversations."
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "Keywords: Emotion Recognition in Conversations ¬∑ Conversation Un-"
        },
        {
          "wyyy@pku.edu.cn,\nliushuaipeng@meituan.com": "derstanding ¬∑ Pseudo Future Contexts."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": "Emotion recognition in conversations (ERC), aiming to identify the emotion of",
          "Introduction": ""
        },
        {
          "1": "each utterance in a conversation, is an essential part of conversation understand-",
          "Introduction": ""
        },
        {
          "1": "ing. It plays a significant role for various downstream tasks, e.g., opinion mining",
          "Introduction": ""
        },
        {
          "1": "in social media, building intelligent assistant through empathetic machine, health",
          "Introduction": ""
        },
        {
          "1": "care, and detecting fake news in social media [13,20].",
          "Introduction": ""
        },
        {
          "1": "",
          "Introduction": "A common solution for ERC is to jointly leverage the contextual and speaker"
        },
        {
          "1": "information of an utterance, which can be roughly divided into sequence-based",
          "Introduction": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "üòê"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "üò†"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "üòÄ"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "üò® üòê üòê üòÄ"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[Bot]: Okay. I'll do that.": "[Bot]: Alright. See you around."
        },
        {
          "[Bot]: Okay. I'll do that.": "[Monica]: Joey that is so sweet."
        },
        {
          "[Bot]: Okay. I'll do that.": "[Bot]: You're a sweetheart!"
        },
        {
          "[Bot]: Okay. I'll do that.": "[Bot]: Awwww, thanks."
        },
        {
          "[Bot]: Okay. I'll do that.": "Fig. 1: An conversation of MELD. Original utterances are in dark-colored blocks"
        },
        {
          "[Bot]: Okay. I'll do that.": "and generated pseudo future contexts are in light-colored blocks."
        },
        {
          "[Bot]: Okay. I'll do that.": "methods\n[16,11] and graph-based methods\n[6,19]. To further\nenrich conversa-"
        },
        {
          "[Bot]: Okay. I'll do that.": "tional\ninformation, another line of research proposes to enhance representations"
        },
        {
          "[Bot]: Okay. I'll do that.": "of utterances with knowledge augmentation. These methods are mainly devoted"
        },
        {
          "[Bot]: Okay. I'll do that.": "to introducing external commonsense knowledge to facilitate comprehension of"
        },
        {
          "[Bot]: Okay. I'll do that.": "utterances, and have amply proved their effectiveness [4,28]."
        },
        {
          "[Bot]: Okay. I'll do that.": "Though many sophisticated designs have been proposed to exploit context"
        },
        {
          "[Bot]: Okay. I'll do that.": "information, we\nfound some of\nthem heavily rely on future\ncontexts\n[6]. The"
        },
        {
          "[Bot]: Okay. I'll do that.": "problem is\nthat we may face the unavailability of\nfuture contexts. For exam-"
        },
        {
          "[Bot]: Okay. I'll do that.": "ple,\nimaging a scenario of detecting the\nemotion of an utterance\njust-in-time"
        },
        {
          "[Bot]: Okay. I'll do that.": "in an after-sale\nconversation, we will have no future\ncontexts available. This"
        },
        {
          "[Bot]: Okay. I'll do that.": "fact motivates us to simulate unseen future states by generating pseudo future"
        },
        {
          "[Bot]: Okay. I'll do that.": "contexts. As\nshown in Figure 1,\nfor each utterance, we can use a pre-trained"
        },
        {
          "[Bot]: Okay. I'll do that.": "language model (e.g., DialoGPT [26]) to generate its future context,\nintroduc-"
        },
        {
          "[Bot]: Okay. I'll do that.": "ing consistent yet extra beneficial knowledge for emotion recognition. Compared"
        },
        {
          "[Bot]: Okay. I'll do that.": "with heterogeneous external knowledge used in previous works (e.g., common-"
        },
        {
          "[Bot]: Okay. I'll do that.": "sense knowledge),\nthe knowledge\nextracted is\nrepresented in a conversational"
        },
        {
          "[Bot]: Okay. I'll do that.": "form homogeneous with historical contexts, and hence can be easily integrated."
        },
        {
          "[Bot]: Okay. I'll do that.": "We further design a novel context representation mechanism that can be ap-"
        },
        {
          "[Bot]: Okay. I'll do that.": "plied indiscriminately to multi-contexts,\nincluding historical contexts, historical"
        },
        {
          "[Bot]: Okay. I'll do that.": "speaker-specific contexts, and pseudo future contexts. Specifically,\nfor each type"
        },
        {
          "[Bot]: Okay. I'll do that.": "of contexts, we employ relative position embeddings\nto capture the positional"
        },
        {
          "[Bot]: Okay. I'll do that.": "relationship between an utterance and its surroundings, obtaining a refined rep-"
        },
        {
          "[Bot]: Okay. I'll do that.": "resentation of an utterance, as well as a local conversational state. We then uti-"
        },
        {
          "[Bot]: Okay. I'll do that.": "lize GRUs to model how the local state evolves as the conversation progresses,"
        },
        {
          "[Bot]: Okay. I'll do that.": "characterizing long-term utterance dependencies from a global view. A final rep-"
        },
        {
          "[Bot]: Okay. I'll do that.": "resentation, which fuses multi-contexts information from both local and global"
        },
        {
          "[Bot]: Okay. I'll do that.": "perspectives,\nis fed into a classifier to produce the emotion label of an utterance."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ERCMC": "We conduct extensive experiments on four ERC datasets, and the empirical",
          "3": ""
        },
        {
          "ERCMC": "",
          "3": ""
        },
        {
          "ERCMC": "In summary, our contributions are: (1) We propose improving ERC from a",
          "3": ""
        },
        {
          "ERCMC": "",
          "3": ""
        },
        {
          "ERCMC": "",
          "3": ""
        },
        {
          "ERCMC": "",
          "3": ""
        },
        {
          "ERCMC": "",
          "3": ""
        },
        {
          "ERCMC": "results on four widely-used ERC datasets.",
          "3": "(3) We"
        },
        {
          "ERCMC": "",
          "3": ""
        },
        {
          "ERCMC": "",
          "3": ""
        },
        {
          "ERCMC": "",
          "3": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yinyi Wei et al.": "Utterance-Level Encoder"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "ùë•\""
        },
        {
          "Yinyi Wei et al.": "Utterance 0\n\":$"
        },
        {
          "Yinyi Wei et al.": "ùëî\""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "‚Ä¶‚Ä¶"
        },
        {
          "Yinyi Wei et al.": "‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "ùë•!%$\nUtterance ùëñ‚àí5"
        },
        {
          "Yinyi Wei et al.": "\":$"
        },
        {
          "Yinyi Wei et al.": "ùëî!%$"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "ùë•!%)\nUtterance ùëñ‚àí4"
        },
        {
          "Yinyi Wei et al.": "\":$"
        },
        {
          "Yinyi Wei et al.": "ùëî!%)"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "ùë•!%(\nUtterance ùëñ‚àí3"
        },
        {
          "Yinyi Wei et al.": "\":$"
        },
        {
          "Yinyi Wei et al.": "ùëî!%("
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "ùë•!%‚Äô\nUtterance ùëñ‚àí2"
        },
        {
          "Yinyi Wei et al.": "\":$"
        },
        {
          "Yinyi Wei et al.": "ùëî!%‚Äô"
        },
        {
          "Yinyi Wei et al.": "DialoGPT\nRoBERTat"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "ùë•!%&"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "Utterance ùëñ‚àí1\n\":$"
        },
        {
          "Yinyi Wei et al.": "ùëî!%&"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "ùë•!\nUtterance i"
        },
        {
          "Yinyi Wei et al.": "\":$"
        },
        {
          "Yinyi Wei et al.": "ùëî!"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "ùë•!+&"
        },
        {
          "Yinyi Wei et al.": "Utterance i+1"
        },
        {
          "Yinyi Wei et al.": "\":$\nùëî!+&"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶\n‚Ä¶‚Ä¶"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "ùë•*%&\nUtterance ùëõ‚àí1"
        },
        {
          "Yinyi Wei et al.": "\":$"
        },
        {
          "Yinyi Wei et al.": "ùëî*%&"
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": ""
        },
        {
          "Yinyi Wei et al.": "time"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ERCMC\n5": "Utterance-Level Encoder Given a conversation U = (u0, ¬∑ ¬∑ ¬∑\n, un‚àí1), for ui ‚àà"
        },
        {
          "ERCMC\n5": ". Then the utterance\nU , we first obtain m generated pseudo future utterances g0:m"
        },
        {
          "ERCMC\n5": "i"
        },
        {
          "ERCMC\n5": "and its pseudo future utterances are fed into an encoder M (RoBERTa [15]\nin"
        },
        {
          "ERCMC\n5": "our experiments) to obtain their representations (we represent an utterance with"
        },
        {
          "ERCMC\n5": "the embedding of the [CLS] token)."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": "We first use a multi-head self-attention layer with relative position embed-"
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": ")W P F\n2"
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": "nhj)W O"
        },
        {
          "local state, and evolution of local states. We take Ci as": "p + rpV\npjk)"
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        },
        {
          "local state, and evolution of local states. We take Ci as": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6\nYinyi Wei et al.": "A state gate is then used to get the local state sc"
        },
        {
          "6\nYinyi Wei et al.": "i :"
        },
        {
          "6\nYinyi Wei et al.": "i‚àí1"
        },
        {
          "6\nYinyi Wei et al.": "(cid:88)"
        },
        {
          "6\nYinyi Wei et al.": "sc\n(13)\nŒ≤jhc\ni ="
        },
        {
          "6\nYinyi Wei et al.": "j=i‚àí‚Ñì"
        },
        {
          "6\nYinyi Wei et al.": "exp stj"
        },
        {
          "6\nYinyi Wei et al.": "(14)\nŒ≤j ="
        },
        {
          "6\nYinyi Wei et al.": "(cid:80)i‚àí1"
        },
        {
          "6\nYinyi Wei et al.": "k=i‚àí‚Ñì exp stk"
        },
        {
          "6\nYinyi Wei et al.": "(15)\nstj = tanh (hc\njW S(hc\ni )‚ä§)"
        },
        {
          "6\nYinyi Wei et al.": "where sc\nstates ss"
        },
        {
          "6\nYinyi Wei et al.": "two local\nfor Si and Ai,\ni ‚àà Rdm, W S ‚àà Rdm√ódm . Similarly,\ni"
        },
        {
          "6\nYinyi Wei et al.": "and sa\nare obtained."
        },
        {
          "6\nYinyi Wei et al.": "i"
        },
        {
          "6\nYinyi Wei et al.": "For states in a global view, a GRU unit is utilized to characterize the evolution"
        },
        {
          "6\nYinyi Wei et al.": "of\nlocal states:"
        },
        {
          "6\nYinyi Wei et al.": "tc\n(16)\ni = GRU(sc\ni , tc\ni‚àí1)"
        },
        {
          "6\nYinyi Wei et al.": "where tc\nis initialized with\ni ‚àà Rdm is the tracked global state prior to ui and tc"
        },
        {
          "6\nYinyi Wei et al.": "zero. Similarly,\ni and ta\ni ."
        },
        {
          "6\nYinyi Wei et al.": "the exploited outcomes\nfrom multi-contexts are integrated\nClassifier For ui,"
        },
        {
          "6\nYinyi Wei et al.": "into a final representation fi."
        },
        {
          "6\nYinyi Wei et al.": "‚à• f s\n(17)\nfi = f h\ni ‚à• f t"
        },
        {
          "6\nYinyi Wei et al.": "f h\n(18)\ni = (hc\ni ‚à• hs\ni ‚à• ha\ni )W F"
        },
        {
          "6\nYinyi Wei et al.": "f s\n(19)\ni = (sc\ni ‚à• ss\ni ‚à• sa\ni )W F"
        },
        {
          "6\nYinyi Wei et al.": "f t\n(20)\ni = (tc\ni ‚à• ts\ni ‚à• ta\ni )W F"
        },
        {
          "6\nYinyi Wei et al.": "‚àà R3dm√ódm .\n, W F\nwhere W F"
        },
        {
          "6\nYinyi Wei et al.": "t\nh , W F"
        },
        {
          "6\nYinyi Wei et al.": "To\nobtain the\nlabels Y\nrepresentations F =\n= (y0, ¬∑ ¬∑ ¬∑\n, yn‚àí1) with the"
        },
        {
          "6\nYinyi Wei et al.": "(f0, ¬∑ ¬∑ ¬∑\n, fn‚àí1), we apply a feed forward network and a softmax layer:"
        },
        {
          "6\nYinyi Wei et al.": "(21)\nyi = argmax(Pi)"
        },
        {
          "6\nYinyi Wei et al.": "(22)\nPi = softmax(FiW M + bM )"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Statistics of datasets.",
      "data": [
        {
          "ERCMC": ""
        },
        {
          "ERCMC": ""
        },
        {
          "ERCMC": ""
        },
        {
          "ERCMC": ""
        },
        {
          "ERCMC": ""
        },
        {
          "ERCMC": ""
        },
        {
          "ERCMC": ""
        },
        {
          "ERCMC": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 1: Statistics of datasets.",
      "data": [
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "MELD\n1,038\n114\n280\n9,989\n1,109 2,610\n7"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "the four datasets, we only use the textual parts. Statistics of these datasets are"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "shown in Table 1."
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "Following [4,28], we choose weighted-average F1 for IEMOCAP, EmoryNLP"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "and MELD. Since the neutral class constitutes to 83% of the DailyDialog, micro-"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "averaged F1 excluding neutral\nis chosen."
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "4.2\nBaselines"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "For models without external knowledge, we compare with: (1) Sequence-based"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "methods: DialogueRNN [16] uses GRUs to track context and speaker states;"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "HiTrans [10] utilizes two level Transformers with an auxiliary task to leverage"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "both contextual and speaker information; CoG-BART [11] uses BART with su-"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "pervised contrastive learning and a response generation task. (2) Graph-based"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "Methods: DialogueGCN [6] models dependencies about context and speaker"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "with graph convolutional networks; RGAT [8] uses\nrelational graph attention"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "networks with relational position encodings."
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "For models with external knowledge, we\ncompare with: KET [27] uses a"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "Transformer to combine word and concept embeddings; COSMIC [4]\nis a mod-"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "ified DialogueRNN with commonsense knowledge\nfrom COMET; TODKAT"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "leverages COMET to integrate commonsense knowledge and a topic model\nto"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "detect the potential topics of a conversation; SKAIG [9] models structural psy-"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "chological\ninteractions between utterances with commonsense knowledge."
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "Three variants of ERCMC are compared:\n(1) ERCMC without\nfuture con-"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "texts; (2) ERCMC with multi-contexts; (3) ERCMC using real\nfuture contexts."
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "4.3\nImplementation Details"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "We use DialoGPT-medium to generate 5 pseudo future utterances for each ut-"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "terance (i.e., m = 5) with k historical utterances, where k = 2 for IEMOCAP"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "and DailyDialog, and k = 4 for EmoryNLP and MELD. For backbones, we select"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "RoBERTa-base for IEMOCAP and RoBERTa-large for the other datasets. For"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "each utterance, the max length is set to 128. For multi-contexts exploiting, we set"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "the size of the three local areas to 6 (i.e., ‚Ñì = 5). The number of head nh is set to"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": ".\n8. dm is set to 768 for IEMOCAP and 1,024 for the other datasets. dk, dv = dm"
        },
        {
          "EmoryNLP\n659\n89\n79\n7,551\n954\n984\n7": "nh"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-",
      "data": [
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "cates using future contexts. C, S, PF, and RF denote historical contexts, histor-"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "ical speaker-specific contexts, pseudo future contexts, and real"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": ""
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "Methods"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": ""
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": ""
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "DialogueRNN"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "+ RoBERTa"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "DialogueGCN*"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "+ RoBERTa*"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "RGAT*"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "+RoBERTa*"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "HiTrans*"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "CoG-BART*"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": ""
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "KET"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "COSMIC"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "SKAIG*"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "TODKAT"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": ""
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "C & S"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "ERCMC\nC & S & PF"
        },
        {
          "Table 2: Overall results. In each part, the highest scores are in boldface. * indi-": "C & S & RF*"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: Various combinations of Multi-Contexts. RAW denotes no contex.",
      "data": [
        {
          "ERCMC": ""
        },
        {
          "ERCMC": ""
        },
        {
          "ERCMC": "64.06"
        },
        {
          "ERCMC": "64.20"
        },
        {
          "ERCMC": "64.43"
        },
        {
          "ERCMC": "64.20"
        },
        {
          "ERCMC": "64.36"
        },
        {
          "ERCMC": "64.76"
        },
        {
          "ERCMC": "65.21"
        },
        {
          "ERCMC": "65.64"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 3: Various combinations of Multi-Contexts. RAW denotes no contex.",
      "data": [
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "C & S & PF\n66.07\n59.92\n39.34\n65.64"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "achieves competitive performance, demonstrating the superiority of our proposed"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "framework. (2) In stead of\nleveraging heterogeneous commonsense knowledge as"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "previous works, we use homogeneous\nconversational knowledge, allowing ER-"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "CMC in C&S&PF setting to overtake other models using external knowledge."
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "(3) Additional\nfuture\ncontexts provide more\ninformation about\nconversations"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "and thus bring improvements over C&S setting when using pseudo or real future"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "contexts in our model. Compared with using real\nfuture contexts, using pseudo"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "future contexts performs better on EmoryNLP and MELD, and underperforms"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "on IEMOCAP and DailyDialog. An in-depth analysis is given in Section 5.4."
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "5.2\nCollaboration of Multi-Contexts"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "To reveal the collaborative effect of multi-contexts on ERCMC, we show various"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "combinations of multi-contexts in Table 3,\nfrom which we observe that: (1) All"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "combinations improve the performance compared with only using raw informa-"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "tion, suggesting that the context information is critical\nfor ERC. (2) Regardless"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "of the number of contexts used, settings with historical speaker-specific contexts"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "produce the best results,\nindicating that historical contexts uttered by the same"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "speaker is the predominant part. (3) Using C or S alone, and a combination of"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "both, bring more improvements than using PF and its combination. The obser-"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "vation implies that historical contexts and historical speaker-specific contexts are"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "more significant for ERC, while pseudo future contexts serve as a supplementary"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "role to provide extra knowledge."
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "5.3\nAblation Study"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "Effect of Compositions of the Final Representations. We form the final"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "representations with local-aware\nembeddings,\nlocal\nstates and tracked global"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "states, which are denoted as h, s, and t, respectively. Results with removal of each"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "part are shown in Table 4a,\nfrom which we can find that though the calculation"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "of\nlocal\nstates and tracked global\nstates contain some information from local-"
        },
        {
          "C & S\n65.47\n59.85\n38.71\n65.21": "aware embeddings, performances drop dramatically when original embeddings"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 4: Ablation study of ERCMC in C&S&PF setting.",
      "data": [
        {
          "38.65 38.74": "65.06 65.30",
          "39.34": "65.64",
          "EmoryNLP 38.64 38.51 38.57 39.34": "64.98 64.83 64.41 65.64"
        },
        {
          "38.65 38.74": "65",
          "39.34": "",
          "EmoryNLP 38.64 38.51 38.57 39.34": ""
        },
        {
          "38.65 38.74": "60",
          "39.34": "",
          "EmoryNLP 38.64 38.51 38.57 39.34": ""
        },
        {
          "38.65 38.74": "55",
          "39.34": "",
          "EmoryNLP 38.64 38.51 38.57 39.34": "IEMOCAP"
        },
        {
          "38.65 38.74": "F1 scores",
          "39.34": "",
          "EmoryNLP 38.64 38.51 38.57 39.34": "MELD"
        },
        {
          "38.65 38.74": "50",
          "39.34": "",
          "EmoryNLP 38.64 38.51 38.57 39.34": "EmoryNLP"
        },
        {
          "38.65 38.74": "",
          "39.34": "",
          "EmoryNLP 38.64 38.51 38.57 39.34": "DailyDialog"
        },
        {
          "38.65 38.74": "45",
          "39.34": "",
          "EmoryNLP 38.64 38.51 38.57 39.34": ""
        },
        {
          "38.65 38.74": "40",
          "39.34": "",
          "EmoryNLP 38.64 38.51 38.57 39.34": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 4: Ablation study of ERCMC in C&S&PF setting.",
      "data": [
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "are lost, especially on EmoryNLP and MELD. And performances\nfall\nslightly"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "when local states and tracked global states are removed."
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "Effect of Position Embeddings. Unlike modeling the distances between ut-"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "terances with sinusoidal position embeddings in previous sequence-based meth-"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "ods\n[27,10], we utilize relative position embeddings and further compare with"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "three position embeddings in Table 4b: without position embeddings; sinusoidal"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "position embeddings that use sine and cosine functions of different frequencies;"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "position embeddings learned from scratch. It is clear that the distance informa-"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "tion exploited by relative position embeddings at utterance-level\nis more com-"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "patible with a sequence-based method and therefore achieves better results."
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "Effect of Size of Local Areas.\nIn our experiments, we set\nthe size of\nlocal"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "areas to 6 (i.e., using five historical utterances at most). We further investigate"
        },
        {
          "Fig. 3: Effect of number of historical utterances with ERCMC in C&S setting.": "the size effect of\nlocal areas by employing ERCMC in C&S setting. From the"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Performance and emotion-consistency on four simplified test sets.",
      "data": [
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "1468 utterances.",
          "(b) Simplified test set of DailyDialog with": "3123 utterances."
        },
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "IEMOCAP",
          "(b) Simplified test set of DailyDialog with": "DailyDialog"
        },
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "Setting",
          "(b) Simplified test set of DailyDialog with": "Setting"
        },
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "Performance W T1 W T2",
          "(b) Simplified test set of DailyDialog with": "Performance W T1 W T2"
        },
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "PF\n57.81",
          "(b) Simplified test set of DailyDialog with": "PF\n51.19"
        },
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "35.85 38.18",
          "(b) Simplified test set of DailyDialog with": "44.77 60.43"
        },
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "66.30\nC & S & PF",
          "(b) Simplified test set of DailyDialog with": "53.80\nC & S & PF"
        },
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "RF\n62.81",
          "(b) Simplified test set of DailyDialog with": "RF\n53.78"
        },
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "50.10 50.47",
          "(b) Simplified test set of DailyDialog with": "76.79 78.90"
        },
        {
          "(a) Simplified test\nset of\nIEMOCAP with": "C & S & RF\n66.68",
          "(b) Simplified test set of DailyDialog with": "C & S & RF\n54.53"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 5: a, 5b, 5c and 5d, we can observe that: (1) Emotion-consistency",
      "data": [
        {
          "12\nYinyi Wei et al.": "From them, we can conclude that conversations in IEMOCAP and DailyDi-"
        },
        {
          "12\nYinyi Wei et al.": "alog are more context-dependent, while conversations in EmoryNLP and MELD"
        },
        {
          "12\nYinyi Wei et al.": "are relatively context-independent. An alternative explanation for\nthis conclu-"
        },
        {
          "12\nYinyi Wei et al.": "sion is that\nIEMOCAP and DailyDialog are two-party datasets, and therefore"
        },
        {
          "12\nYinyi Wei et al.": "the\nconversations are more\nemotion-focused, whereas EmoryNLP and MELD"
        },
        {
          "12\nYinyi Wei et al.": "are multi-party datasets,\nresulting in more diffuse conversations and inconsis-"
        },
        {
          "12\nYinyi Wei et al.": "tent emotions."
        },
        {
          "12\nYinyi Wei et al.": "To further explore the effect of using pseudo or real\nfuture contexts, we at-"
        },
        {
          "12\nYinyi Wei et al.": "tempt to investigate the attributes of future contexts. Previous works [5,24] have"
        },
        {
          "12\nYinyi Wei et al.": "reported a common issue with ERC: different emotions in consecutive utterances"
        },
        {
          "12\nYinyi Wei et al.": "may confound a model and thereby degrade performance. Inspired by the issue,"
        },
        {
          "12\nYinyi Wei et al.": "we define a new concept of ‚Äúemotion-consistency‚Äù as\nthe degree of emotional"
        },
        {
          "12\nYinyi Wei et al.": "consistency of the subsequent utterances with the first utterance within a local"
        },
        {
          "12\nYinyi Wei et al.": "area. Given a local area LC = (u0, ¬∑ ¬∑ ¬∑\n, u‚Ñì), the emotion-consistency is:"
        },
        {
          "12\nYinyi Wei et al.": "‚Ñì(cid:88) i\nEC(LC) = 100 ¬∑\n(23)\nœï(ui, u0) ¬∑ wti‚àí1"
        },
        {
          "12\nYinyi Wei et al.": "=1"
        },
        {
          "12\nYinyi Wei et al.": "where W T = (wt0, ¬∑ ¬∑ ¬∑\n, wt‚Ñì‚àí1) is the weight in different positions, œï(ui, u0) is a"
        },
        {
          "12\nYinyi Wei et al.": "function to indicate whether ui and u0 having the same emotion."
        },
        {
          "12\nYinyi Wei et al.": "For quantitative analysis, we employ ERCMC in C setting to predict emo-"
        },
        {
          "12\nYinyi Wei et al.": "tions and calculate the emotion-consistency scores. To align with pseudo future"
        },
        {
          "12\nYinyi Wei et al.": "contexts, we simplify the test sets to ensure each utterance has at least ‚Ñì consec-"
        },
        {
          "12\nYinyi Wei et al.": "utive utterances (‚Ñì = 5 in our experiments). Two kinds of weight are considered,"
        },
        {
          "12\nYinyi Wei et al.": "exp e‚Ñì‚àíi"
        },
        {
          "12\nYinyi Wei et al.": ", wt2\nwt1\ni ‚àà W T1; wt2\ni =\ni ‚àà W T2. W T1 relies uniformly on\ni = 1\n‚Ñì , wt1"
        },
        {
          "12\nYinyi Wei et al.": "(cid:80)‚Ñì"
        },
        {
          "12\nYinyi Wei et al.": "j=1 exp ej‚àí1"
        },
        {
          "12\nYinyi Wei et al.": "consecutive utterances, while W T2 favours utterances near the first utterance."
        },
        {
          "12\nYinyi Wei et al.": "From Table 5a, 5b, 5c and 5d, we can observe that: (1) Emotion-consistency"
        },
        {
          "12\nYinyi Wei et al.": "scores of IEMOCAP and DailyDialog are higher than those of EmoryNLP and"
        },
        {
          "12\nYinyi Wei et al.": "MELD. The finding confirms our previous observations\nthat\nIEMOCAP and"
        },
        {
          "12\nYinyi Wei et al.": "DailyDialog are more context-dependent, whereas EmoryNLP and MELD are"
        },
        {
          "12\nYinyi Wei et al.": "somewhat context-independent. (2) Performance and emotion-consistency scores"
        },
        {
          "12\nYinyi Wei et al.": "are positively correlated. And combined with Table 2, using pseudo future con-"
        },
        {
          "12\nYinyi Wei et al.": "texts achieves competitive results on IEMOCAP and DailyDialog, and outper-"
        },
        {
          "12\nYinyi Wei et al.": "forms using real\nfuture contexts on EmoryNLP and MELD. The results demon-"
        },
        {
          "12\nYinyi Wei et al.": "strate that pseudo future contexts can replace real ones\nto some extent when"
        },
        {
          "12\nYinyi Wei et al.": "the dataset is context-dependent, and serve as more extra beneficial knowledge"
        },
        {
          "12\nYinyi Wei et al.": "when the dataset is relatively context-independent."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Excited\n[M]: It's just you know a dream come true. How could you not"
        },
        {
          "IEMOCAP": "Excited\nbe excited about a dream come true?"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Neutral"
        },
        {
          "IEMOCAP": "[F]: You're gonna be so busy, too. You're going to have to work so hard\nExcited"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Happy"
        },
        {
          "IEMOCAP": "[M]: Oh I know, I know.\nHappy"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "[M]: But I'm so looking forward to being so busy. You know what I \nHappy"
        },
        {
          "IEMOCAP": "Excited\nmean? It's the kind of busy you've got to love, right?"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Happy\n[M]: Because you're doing something you love. That's the kind of busy you"
        },
        {
          "IEMOCAP": "Happy\nwant. I want to be able to do that for the rest of my life. Be that kind of busy."
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "[F]: All right. Wow. So exciting.\nExcited"
        },
        {
          "IEMOCAP": "Excited"
        },
        {
          "IEMOCAP": "MELD"
        },
        {
          "IEMOCAP": "Joy"
        },
        {
          "IEMOCAP": "Joy"
        },
        {
          "IEMOCAP": "[Host]: Welcome everybody, welcome to Amazing Discoveries!"
        },
        {
          "IEMOCAP": "Joy"
        },
        {
          "IEMOCAP": "Joy"
        },
        {
          "IEMOCAP": "Joy"
        },
        {
          "IEMOCAP": "Joy"
        },
        {
          "IEMOCAP": "[Phoebe]: Oh, oh! It's on again!\nJoy"
        },
        {
          "IEMOCAP": "Surprise"
        },
        {
          "IEMOCAP": "Disgust"
        },
        {
          "IEMOCAP": "Neutral"
        },
        {
          "IEMOCAP": "[Joey]: You guys, can we please not watch this all right.\nDisgust"
        },
        {
          "IEMOCAP": "Sadness"
        },
        {
          "IEMOCAP": "Anger"
        },
        {
          "IEMOCAP": "Anger"
        },
        {
          "IEMOCAP": "[All]: Shhhh!\nAnger"
        },
        {
          "IEMOCAP": "Joy"
        },
        {
          "IEMOCAP": "Neutral"
        },
        {
          "IEMOCAP": "Neutral"
        },
        {
          "IEMOCAP": "[Host]: Folks, has this ever happened to you.\nNeutral"
        },
        {
          "IEMOCAP": "Neutral"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Neutral": "[Bot]: I'm not sure, but I think I've seen it happen to me.\nNeutral"
        },
        {
          "Neutral": "[Host]: Folks, has this ever happened to you.\n[Bot]: It's a good thing you're not a doctor.\nNeutral"
        },
        {
          "Neutral": "Neutral"
        },
        {
          "Neutral": ""
        },
        {
          "Neutral": "labels, predictions from C&S, C&S&PF, and C&S&RF settings."
        },
        {
          "Neutral": "In the second case from MELD, the emotions of multiple speakers are diverse"
        },
        {
          "Neutral": ""
        },
        {
          "Neutral": "supplementary knowledge\nfrom pseudo future\ncontexts\ncould"
        },
        {
          "Neutral": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14\nYinyi Wei et al.": "2. Busso, C., Bulut, M., Lee, C.C., Kazemzadeh, A., Mower, E., Kim, S., Chang, J.N.,"
        },
        {
          "14\nYinyi Wei et al.": "Lee, S., Narayanan, S.S.:\nIemocap:\nInteractive emotional dyadic motion capture"
        },
        {
          "14\nYinyi Wei et al.": "database. Language resources and evaluation 42(4), 335‚Äì359 (2008)"
        },
        {
          "14\nYinyi Wei et al.": "3. Elman, J.L.: Finding structure in time. Cognitive science 14(2), 179‚Äì211 (1990)"
        },
        {
          "14\nYinyi Wei et al.": "4. Ghosal, D., Majumder, N., Gelbukh, A., Mihalcea, R., Poria, S.: Cosmic: Com-"
        },
        {
          "14\nYinyi Wei et al.": "monsense knowledge for emotion identification in conversations. In: Findings of the"
        },
        {
          "14\nYinyi Wei et al.": "Association for Computational Linguistics: EMNLP 2020. pp. 2470‚Äì2481 (2020)"
        },
        {
          "14\nYinyi Wei et al.": "5. Ghosal, D., Majumder, N., Mihalcea, R., Poria, S.: Exploring the role of context in"
        },
        {
          "14\nYinyi Wei et al.": "utterance-level emotion, act and intent classification in conversations: An empirical"
        },
        {
          "14\nYinyi Wei et al.": "study. In: Findings of the Association for Computational Linguistics: ACL-IJCNLP"
        },
        {
          "14\nYinyi Wei et al.": "2021. pp. 1435‚Äì1449 (2021)"
        },
        {
          "14\nYinyi Wei et al.": "6. Ghosal, D., Majumder, N., Poria, S., Chhaya, N., Gelbukh, A.: Dialoguegcn: A"
        },
        {
          "14\nYinyi Wei et al.": "graph convolutional neural network for emotion recognition in conversation.\nIn:"
        },
        {
          "14\nYinyi Wei et al.": "Proceedings of\nthe 2019 Conference on Empirical Methods\nin Natural Language"
        },
        {
          "14\nYinyi Wei et al.": "Processing and the 9th International Joint Conference on Natural Language Pro-"
        },
        {
          "14\nYinyi Wei et al.": "cessing (EMNLP-IJCNLP). pp. 154‚Äì164 (2019)"
        },
        {
          "14\nYinyi Wei et al.": "7. Hu, D., Wei, L., Huai, X.: Dialoguecrn: Contextual reasoning networks for emotion"
        },
        {
          "14\nYinyi Wei et al.": "recognition in conversations.\nIn: Proceedings of\nthe 59th Annual Meeting of\nthe"
        },
        {
          "14\nYinyi Wei et al.": "Association for Computational Linguistics and the 11th International Joint Con-"
        },
        {
          "14\nYinyi Wei et al.": "ference on Natural Language Processing (Volume 1: Long Papers). pp. 7042‚Äì7052"
        },
        {
          "14\nYinyi Wei et al.": "(2021)"
        },
        {
          "14\nYinyi Wei et al.": "8.\nIshiwatari, T., Yasuda, Y., Miyazaki, T., Goto, J.: Relation-aware graph attention"
        },
        {
          "14\nYinyi Wei et al.": "networks with relational position encodings\nfor emotion recognition in conversa-"
        },
        {
          "14\nYinyi Wei et al.": "tions.\nIn: Proceedings of\nthe 2020 Conference on Empirical Methods\nin Natural"
        },
        {
          "14\nYinyi Wei et al.": "Language Processing (EMNLP). pp. 7360‚Äì7370 (2020)"
        },
        {
          "14\nYinyi Wei et al.": "9. Li, J., Lin, Z., Fu, P., Wang, W.: Past, present, and future: Conversational emotion"
        },
        {
          "14\nYinyi Wei et al.": "recognition through structural modeling of psychological knowledge.\nIn: Findings"
        },
        {
          "14\nYinyi Wei et al.": "of\nthe Association for Computational Linguistics: EMNLP 2021. pp. 1204‚Äì1214"
        },
        {
          "14\nYinyi Wei et al.": "(2021)"
        },
        {
          "14\nYinyi Wei et al.": "10. Li, J., Ji, D., Li, F., Zhang, M., Liu, Y.: Hitrans: A transformer-based context-and"
        },
        {
          "14\nYinyi Wei et al.": "speaker-sensitive model\nfor emotion detection in conversations. In: Proceedings of"
        },
        {
          "14\nYinyi Wei et al.": "the 28th International Conference on Computational Linguistics. pp. 4190‚Äì4200"
        },
        {
          "14\nYinyi Wei et al.": "(2020)"
        },
        {
          "14\nYinyi Wei et al.": "11. Li, S., Yan, H., Qiu, X.: Contrast and generation make bart a good dialogue emo-"
        },
        {
          "14\nYinyi Wei et al.": "tion recognizer. In: Proceedings of the AAAI Conference on Artificial Intelligence."
        },
        {
          "14\nYinyi Wei et al.": "vol. 36, pp. 11002‚Äì11010 (2022)"
        },
        {
          "14\nYinyi Wei et al.": "12. Li, Y., Su, H., Shen, X., Li, W., Cao, Z., Niu, S.: Dailydialog: A manually la-"
        },
        {
          "14\nYinyi Wei et al.": "belled multi-turn dialogue dataset.\nIn: Proceedings of\nthe Eighth International"
        },
        {
          "14\nYinyi Wei et al.": "Joint Conference on Natural Language Processing (Volume 1: Long Papers). pp."
        },
        {
          "14\nYinyi Wei et al.": "986‚Äì995 (2017)"
        },
        {
          "14\nYinyi Wei et al.": "13. Lin, Z., Madotto, A., Shin, J., Xu, P., Fung, P.: Moel: Mixture of\nempathetic"
        },
        {
          "14\nYinyi Wei et al.": "listeners.\nIn: Proceedings of\nthe 2019 Conference on Empirical Methods\nin Nat-"
        },
        {
          "14\nYinyi Wei et al.": "ural Language Processing and the 9th International Joint Conference on Natural"
        },
        {
          "14\nYinyi Wei et al.": "Language Processing (EMNLP-IJCNLP). pp. 121‚Äì132 (2019)"
        },
        {
          "14\nYinyi Wei et al.": "14. Ling, T., Chen, L., Lai, Y., Liu, H.L.: Evolutionary verbalizer search for prompt-"
        },
        {
          "14\nYinyi Wei et al.": "based few shot text classification. arXiv preprint arXiv:2306.10514 (2023)"
        },
        {
          "14\nYinyi Wei et al.": "15. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,"
        },
        {
          "14\nYinyi Wei et al.": "Zettlemoyer, L., Stoyanov, V.: Roberta: A robustly optimized bert pretraining"
        },
        {
          "14\nYinyi Wei et al.": "approach. arXiv preprint arXiv:1907.11692 (2019)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ERCMC\n15": "16. Majumder, N., Poria, S., Hazarika, D., Mihalcea, R., Gelbukh, A., Cambria, E.:"
        },
        {
          "ERCMC\n15": "Dialoguernn: An attentive\nrnn for\nemotion detection in conversations.\nIn: Pro-"
        },
        {
          "ERCMC\n15": "ceedings of the AAAI conference on artificial\nintelligence. vol. 33, pp. 6818‚Äì6825"
        },
        {
          "ERCMC\n15": "(2019)"
        },
        {
          "ERCMC\n15": "17. Poria, S., Hazarika, D., Majumder, N., Naik, G., Cambria, E., Mihalcea, R.: Meld:"
        },
        {
          "ERCMC\n15": "A multimodal multi-party dataset\nfor\nemotion recognition in conversations.\nIn:"
        },
        {
          "ERCMC\n15": "Proceedings of\nthe 57th Annual Meeting of\nthe Association for Computational"
        },
        {
          "ERCMC\n15": "Linguistics. pp. 527‚Äì536 (2019)"
        },
        {
          "ERCMC\n15": "18. Shaw, P., Uszkoreit, J., Vaswani, A.: Self-attention with relative position represen-"
        },
        {
          "ERCMC\n15": "tations.\nIn: Proceedings of\nthe 2018 Conference of\nthe North American Chapter"
        },
        {
          "ERCMC\n15": "of the Association for Computational Linguistics: Human Language Technologies,"
        },
        {
          "ERCMC\n15": "Volume 2 (Short Papers). pp. 464‚Äì468 (2018)"
        },
        {
          "ERCMC\n15": "19. Shen, W., Wu, S., Yang, Y., Quan, X.: Directed acyclic graph network for conver-"
        },
        {
          "ERCMC\n15": "sational emotion recognition.\nIn: Proceedings of the 59th Annual Meeting of the"
        },
        {
          "ERCMC\n15": "Association for Computational Linguistics and the 11th International Joint Con-"
        },
        {
          "ERCMC\n15": "ference on Natural Language Processing (Volume 1: Long Papers). pp. 1551‚Äì1560"
        },
        {
          "ERCMC\n15": "(2021)"
        },
        {
          "ERCMC\n15": "20. Shu, K., Mosallanezhad, A., Liu, H.: Cross-domain fake news detection on social"
        },
        {
          "ERCMC\n15": "media: A context-aware adversarial approach. In: Frontiers in Fake Media Gener-"
        },
        {
          "ERCMC\n15": "ation and Detection, pp. 215‚Äì232. Springer (2022)"
        },
        {
          "ERCMC\n15": "21. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,"
        },
        {
          "ERCMC\n15": "L., Polosukhin, I.: Attention is all you need. Advances in neural\ninformation pro-"
        },
        {
          "ERCMC\n15": "cessing systems 30 (2017)"
        },
        {
          "ERCMC\n15": "22. Wang, Y., Zhang, J., Ma, J., Wang, S., Xiao, J.: Contextualized emotion recog-"
        },
        {
          "ERCMC\n15": "nition in conversation as\nsequence\ntagging.\nIn: Proceedings of\nthe 21th annual"
        },
        {
          "ERCMC\n15": "meeting of the special interest group on discourse and dialogue. pp. 186‚Äì195 (2020)"
        },
        {
          "ERCMC\n15": "23. Wei, Y., Mo, T., Jiang, Y., Li, W., Zhao, W.: Eliciting knowledge from pretrained"
        },
        {
          "ERCMC\n15": "language models for prototypical prompt verbalizer. In: Artificial Neural Networks"
        },
        {
          "ERCMC\n15": "and Machine Learning‚ÄìICANN 2022: 31st\nInternational Conference on Artificial"
        },
        {
          "ERCMC\n15": "Neural Networks. pp. 222‚Äì233 (2022)"
        },
        {
          "ERCMC\n15": "24. Yang, L., Shen, Y., Mao, Y., Cai, L.: Hybrid curriculum learning for\nemotion"
        },
        {
          "ERCMC\n15": "recognition in conversation. In: Proceedings of the AAAI Conference on Artificial"
        },
        {
          "ERCMC\n15": "Intelligence. vol. 36, pp. 11595‚Äì11603 (2022)"
        },
        {
          "ERCMC\n15": "25. Zahiri, S.M., Choi, J.D.: Emotion detection on tv show transcripts with sequence-"
        },
        {
          "ERCMC\n15": "based convolutional neural networks. In: Workshops at the thirty-second aaai con-"
        },
        {
          "ERCMC\n15": "ference on artificial\nintelligence (2018)"
        },
        {
          "ERCMC\n15": "26. Zhang, Y., Sun, S., Galley, M., Chen, Y.C., Brockett, C., Gao, X., Gao, J., Liu,"
        },
        {
          "ERCMC\n15": "J., Dolan, W.B.: Dialogpt: Large-scale generative pre-training for conversational"
        },
        {
          "ERCMC\n15": "response generation. In: Proceedings of the 58th Annual Meeting of the Association"
        },
        {
          "ERCMC\n15": "for Computational Linguistics: System Demonstrations. pp. 270‚Äì278 (2020)"
        },
        {
          "ERCMC\n15": "27. Zhong, P., Wang, D., Miao, C.: Knowledge-enriched transformer for emotion detec-"
        },
        {
          "ERCMC\n15": "tion in textual conversations. In: Proceedings of the 2019 Conference on Empirical"
        },
        {
          "ERCMC\n15": "Methods in Natural Language Processing and the 9th International Joint Confer-"
        },
        {
          "ERCMC\n15": "ence on Natural Language Processing (EMNLP-IJCNLP). pp. 165‚Äì176 (2019)"
        },
        {
          "ERCMC\n15": "28. Zhu, L., Pergola, G., Gui, L., Zhou, D., He, Y.: Topic-driven and knowledge-aware"
        },
        {
          "ERCMC\n15": "transformer\nfor dialogue emotion detection.\nIn: Proceedings of\nthe 59th Annual"
        },
        {
          "ERCMC\n15": "Meeting of\nthe Association for Computational Linguistics and the 11th Interna-"
        },
        {
          "ERCMC\n15": "tional Joint Conference on Natural Language Processing (Volume 1: Long Papers)."
        },
        {
          "ERCMC\n15": "pp. 1571‚Äì1582 (2021)"
        }
      ],
      "page": 15
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "It's just you know a dream come true. How could you not be excited about a dream come true? [Bot]: Thank you. I think I could have done better",
      "venue": "It's just you know a dream come true. How could you not be excited about a dream come true? [Bot]: Thank you. I think I could have done better"
    },
    {
      "citation_id": "2",
      "title": "You're gonna be so busy, too. You're going to have to work so hard [Bot]: I'm gonna have a lot of fun",
      "venue": "You're gonna be so busy, too. You're going to have to work so hard [Bot]: I'm gonna have a lot of fun"
    },
    {
      "citation_id": "3",
      "title": "You know what I mean? It's the kind of busy you've got to love",
      "venue": "You know what I mean? It's the kind of busy you've got to love"
    },
    {
      "citation_id": "4",
      "title": "Because you're doing something you love. That's the kind of busy you want. I want to be able to do that for the rest of my life. Be that kind of busy. [Bot]: I'm not sure if I can do it",
      "venue": "Because you're doing something you love. That's the kind of busy you want. I want to be able to do that for the rest of my life. Be that kind of busy. [Bot]: I'm not sure if I can do it"
    },
    {
      "citation_id": "5",
      "title": "Excited Excited Excited Excited [Host]: Welcome everybody, welcome to Amazing Discoveries! [Bot]: I'm so excited! [Bot]: Welcome to the party!! :D [Phoebe]: Oh, oh! It's on again! [Bot]: I'm so glad I'm not the only one who thought that. [Bot]: It's a great show! [Joey]: You guys, can we please not watch this all right. [Bot]: I'm sorry, I'm afraid I can't do that",
      "venue": "Thank you! I hope you have a good day"
    },
    {
      "citation_id": "6",
      "title": "Shhhh! [Bot]: I'm not watching it! [Bot]: It's not on!! It'S NOT ON! [Host]: Folks, has this ever happened to you. [Bot]: I'm not sure, but I think I've seen it happen to me",
      "authors": [
        "All"
      ],
      "venue": "Shhhh! [Bot]: I'm not watching it! [Bot]: It's not on!! It'S NOT ON! [Host]: Folks, has this ever happened to you. [Bot]: I'm not sure, but I think I've seen it happen to me"
    },
    {
      "citation_id": "7",
      "title": "Comet: Commonsense transformers for automatic knowledge graph construction",
      "authors": [
        "A Bosselut",
        "H Rashkin",
        "M Sap",
        "C Malaviya",
        "A Celikyilmaz",
        "Y Choi"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "8",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "9",
      "title": "Finding structure in time",
      "authors": [
        "J Elman"
      ],
      "year": "1990",
      "venue": "Cognitive science"
    },
    {
      "citation_id": "10",
      "title": "Cosmic: Commonsense knowledge for emotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2020"
    },
    {
      "citation_id": "11",
      "title": "Exploring the role of context in utterance-level emotion, act and intent classification in conversations: An empirical study",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021"
    },
    {
      "citation_id": "12",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Dialoguecrn: Contextual reasoning networks for emotion recognition in conversations",
      "authors": [
        "D Hu",
        "L Wei",
        "X Huai"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations",
      "authors": [
        "T Ishiwatari",
        "Y Yasuda",
        "T Miyazaki",
        "J Goto"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "15",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "W Wang"
      ],
      "year": "2021",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2021"
    },
    {
      "citation_id": "16",
      "title": "Hitrans: A transformer-based context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "Contrast and generation make bart a good dialogue emotion recognizer",
      "authors": [
        "S Li",
        "H Yan",
        "X Qiu"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "18",
      "title": "Dailydialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Moel: Mixture of empathetic listeners",
      "authors": [
        "Z Lin",
        "A Madotto",
        "J Shin",
        "P Xu",
        "P Fung"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "20",
      "title": "Evolutionary verbalizer search for promptbased few shot text classification",
      "authors": [
        "T Ling",
        "L Chen",
        "Y Lai",
        "H Liu"
      ],
      "year": "2023",
      "venue": "Evolutionary verbalizer search for promptbased few shot text classification",
      "arxiv": "arXiv:2306.10514"
    },
    {
      "citation_id": "21",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "22",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "23",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "24",
      "title": "Self-attention with relative position representations",
      "authors": [
        "P Shaw",
        "J Uszkoreit",
        "A Vaswani"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter"
    },
    {
      "citation_id": "25",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "26",
      "title": "Cross-domain fake news detection on social media: A context-aware adversarial approach",
      "authors": [
        "K Shu",
        "A Mosallanezhad",
        "H Liu"
      ],
      "year": "2022",
      "venue": "Frontiers in Fake Media Generation and Detection"
    },
    {
      "citation_id": "27",
      "title": "Attention is all you need. Advances in neural information processing systems",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need. Advances in neural information processing systems"
    },
    {
      "citation_id": "28",
      "title": "Contextualized emotion recognition in conversation as sequence tagging",
      "authors": [
        "Y Wang",
        "J Zhang",
        "J Ma",
        "S Wang",
        "J Xiao"
      ],
      "year": "2020",
      "venue": "Proceedings of the 21th annual meeting of the special interest group on discourse and dialogue"
    },
    {
      "citation_id": "29",
      "title": "Eliciting knowledge from pretrained language models for prototypical prompt verbalizer",
      "authors": [
        "Y Wei",
        "T Mo",
        "Y Jiang",
        "W Li",
        "W Zhao"
      ],
      "year": "2022",
      "venue": "Artificial Neural Networks and Machine Learning-ICANN 2022: 31st International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "30",
      "title": "Hybrid curriculum learning for emotion recognition in conversation",
      "authors": [
        "L Yang",
        "Y Shen",
        "Y Mao",
        "L Cai"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "31",
      "title": "Emotion detection on tv show transcripts with sequencebased convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "32",
      "title": "Dialogpt: Large-scale generative pre-training for conversational response generation",
      "authors": [
        "Y Zhang",
        "S Sun",
        "M Galley",
        "Y Chen",
        "C Brockett",
        "X Gao",
        "J Gao",
        "J Liu",
        "W Dolan"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations"
    },
    {
      "citation_id": "33",
      "title": "Knowledge-enriched transformer for emotion detection in textual conversations",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "34",
      "title": "Topic-driven and knowledge-aware transformer for dialogue emotion detection",
      "authors": [
        "L Zhu",
        "G Pergola",
        "L Gui",
        "D Zhou",
        "Y He"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    }
  ]
}