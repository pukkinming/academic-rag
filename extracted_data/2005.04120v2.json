{
  "paper_id": "2005.04120v2",
  "title": "K-Emocon, A Multimodal Sensor Dataset For Continuous Emotion Recognition In Naturalistic Conversations",
  "published": "2020-05-08T15:51:12Z",
  "authors": [
    "Cheul Young Park",
    "Narae Cha",
    "Soowon Kang",
    "Auk Kim",
    "Ahsan Habib Khandoker",
    "Leontios Hadjileontiadis",
    "Alice Oh",
    "Yong Jeong",
    "Uichin Lee"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Recognizing emotions during social interactions has many potential applications with the popularization of low-cost mobile sensors, but a challenge remains with the lack of naturalistic affective interaction data. Most existing emotion datasets do not support studying idiosyncratic emotions arising in the wild as they were collected in constrained environments. Therefore, studying emotions in the context of social interactions requires a novel dataset, and K-EmoCon is such a multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations. The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue. Distinct from previous datasets, it includes emotion annotations from all three available perspectives: self, debate partner, and external observers. Raters annotated emotional displays at intervals of every 5 seconds while viewing the debate footage, in terms of arousal-valence and 18 additional categorical emotions. The resulting K-EmoCon is the first publicly available emotion dataset accommodating the multiperspective assessment of emotions during social interactions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Background & Summary",
      "text": "Emotion recognition research seeks to enable computers to identify emotions. It is a foundation for creating machines capable of understanding emotions, and possibly, even expressing one. Such a set of skills to recognize, understand, and express emotions form emotional intelligence  [1, 2] . It is suggested that emotional intelligence is necessary for the navigation of oneself within a society, as it allows one to reason what is desirable and what is not, and to regulate behaviors of self and others accordingly  [3, 4] .\n\nThen why do machines need emotional skills? With advances in Machine Learning and Artificial Intelligence, the transition from human to machine is noticeable in all areas of the society, including those requiring expertise such as medical prognosis/diagnosis  [5, 6]  or automobile driving  [7] . It seems inevitable that these narrow AI systems  [8]  supersede human experts in respective domains, as it has already been demonstrated with AlphaGo's superior performance in the game of Go over human champions  [9, 10] .\n\nNot all AI will compete with humans, albeit their superhuman ability. Instead, many AI systems will work with us or for us. Emotional intelligence is critical for such human-computer interaction systems  [11] . Imagine a smart speaker that delightfully greets users when they come home. How should a speaker greet when a user had a rough day? A speaker neglectful of the user's emotional states may aggravate the user, but a speaker aware of the user's temper could remain silent to avoid the trouble. Similarly, emotional intelligence is critical for AI systems designed for complex tasks. For example, on roads where autonomous and human-driven vehicles mix, accurate recognition of emotions of human drivers' by autonomous vehicles would lead to more safety as autonomous vehicles can better judge human drivers' intentions  [12] . Now for machines to become emotionally intelligent, they must first learn to recognize emotions, and the prerequisite to learning is data. However, there lie several challenges in the acquisition of emotion data. While emotions are prevalent, their accurate measurement is difficult. Most commonly, emotions are viewed as psychological states expressed through faces, with distinct categories  [13] , but research evidence claims the contrary. Rather than distinct, facial expressions are compound  [14] , relative  [15] , and misleading  [16] . A recent review of scientific evidence also presses against the common view, suggesting that facial expressions lack reliability, specificity, and generalizability  [17] , together with past studies on contextual dependency  [18, 19, 20]  and individual variability of emotions  [21, 22] .\n\nSuch inherent elusiveness of emotion renders many existing emotion datasets inapplicable for studying emotions in the wild. The majority of emotion datasets consist of emotions induced with selected stimuli in a static environment, i.e., a laboratory  [23, 24, 25, 26, 27, 28, 29] . This method provides experimenters with full-control over data collection, allowing assessment of specific emotional behaviors  [30, 31]  and acquiring fine-grained data with advanced techniques like neuroimaging. Nevertheless, lab-generated data may generalize poorly to realistic scenarios as they frequently contain intense expressions of prototypical emotions, which are rarely observed in the real world  [32, 33] , acquired from only a subset of the population  [34] .\n\nAn alternative approach utilizes media contents  [35, 36, 37, 38]  and crowdsourcing  [39] , compensating for the shortcomings of the conventional method. The abundance of contents available online, such as TV-shows and movies, allows researchers to glean rich emotion data representative of various contexts efficiently. Crowdsourcing further supports inexpensive data annotation while serving as another data source  [40, 41] . Datasets of this type have advantages in sample size and the diversity of subjects, but generalizability remains an issue. Datasets based on media contents often contain emotional displays produced by trained actors supposing fictitious situations. To what extent such emotional portrayals resemble spontaneous emotional expressions is debatable  [42, 43, 44] . They also provide no access to physiological signals, which are known to carry information vital for the detection of less visible changes in emotional states  [45, 46, 47, 48, 49, 50] .\n\nTo amend this lack of a dataset for recognition of emotions in their natural forms, we introduce K-EmoCon, a multimodal dataset acquired from 32 subjects participating in 16 paired debates on a social issue. It consists of physiological sensor data collected with three off-the-shelf wearable devices, audiovisual footage of participants during the debate, and continuous emotion annotations. It contributes to the current literature of emotion recognition, as according to our knowledge, it is the first dataset with emotion annotations from all possible perspectives as the following: subject him/herself, debate partner, and external observers.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset Design",
      "text": "Intended usage Inspired by previous works that set out to investigate emotions during conversations  [51, 52, 53, 38] , K-EmoCon was designed in consideration of a social interaction scenario involving two people and wearable devices capable of unobtrusive tracking of physiological signals. The dataset aims to allow a multi-perspective analysis of emotions with the following objectives:\n\n1. Extend the research on how having multiple perspectives on emotional expressions may improve their automatic recognition.\n\n2. Provide a novel opportunity to investigate how emotions can be perceived differently from multiple perspectives, especially in the context of social interaction.\n\nPrevious research has shown that having multiple sources for emotion annotations can increase their recognition accuracy  [54, 55] . However, no research in our awareness employs all three available perspectives in the annotation of emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to the issue of establishing ground truth in emotion annotations. Emotions are inherently internal phenomena, and their mechanism is unavailable for external scrutiny, even for oneself who is experiencing emotions. As a result, there may not be a ground truth for emotions. Should we consider what is most agreed upon by external observers of emotions as the ground truth, or what the person who experiences emotions reports to have felt the ground truth  [56] ? Two views are likely to match if emotions are intense and pure, but as discussed, such emotions are rare. Instead, self-reported and observed emotions are likely to disagree for a variety of reasons. People often conceal their true emotions; sometimes, people are not fully mindful of their internal states; and some people may have difficulties in interpreting or articulating emotions  [57, 58] .\n\nWith K-EmoCon, we intend to enable the comprehensive examination of such cases where perceptions of emotions do not match, by bringing all three available perspectives into the annotation of emotions, in the context of a social interaction involving three parties of:\n\n1. The subject -is the source who experiences emotions firsthand and produces self annotations, particularly the \"felt sense\"  [55]  of the emotions.\n\n2. The partner -is the person who interacts with the subject, experiencing the subject's emotions secondhand; thus, he or she has a contextual knowledge of the interaction that induced the subject's emotions and produces partner annotations based on that.\n\n3. The external observers -are people who observe the subject's emotions without the exact contextual knowledge of the interaction that induced the emotions, producing external observer annotations.\n\nNotice, that while our definition of perspectives involved in emotion annotation is similar to definitions previously used by other researchers (self-reported vs. perceived  [55] /observed  [59] ), we further segment observer annotations based on whether the contextual information of the situation in which the emotion was generated is available to an observer, as we wish to consider the role of contextual knowledge in emotion perception and recognition.\n\nExisting datasets of emotions in conversations provide a limited scope on this issue as they at most contain emotion annotations from subjects and external observers  [51] , leaving out annotations from other people who engaged in the conversation (whom we call partners). Or, they either only consider a particular type of annotations that is sufficient to serve their research goal  [53]  or their designs do not allow acquiring multi-perspective annotations  [52, 38]  (e.g., a dataset is constructed upon conversations from a TV-show, only allowing the collection of external observer annotations). Refer to Table  1  to see how K-EmoCon is distinguished from existing emotion datasets.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Context Of Data Collection",
      "text": "In this regard, we chose a semi-structured, turn-taking debate on a social issue with randomly assigned partners as the setting for data collection. This setting is appropriate for collecting emotions that may naturally arise in a day, as it is similar to a social interaction that one could engage in a workplace.\n\nAlso, the setting is particularly suitable for studying the misperception of emotions. It is sufficiently formal and spontaneous as it involves randomly assigned partners. We expect such formality and spontaneity of the setting compelled participants to regulate their emotions in a socially appropriate manner, allowing us to observe less pronounced emotions from participants, which were more likely to be misperceived by their partners  [60] .\n\nData collection apparatus Our choice of mobile, wearable, and low-cost devices to collect affective physiological signals together with audiovisual recordings, while primarily aims to make findings based on our data more reproducible and expandable, was also in consideration of our goal of investigating mismatches in perceptions of emotions in the wild. Research has shown that fusing implicit and explicit affective information can result in more accurate recognition of subtle emotional expressions from professional actors  [61] . However, no work we are aware of has shown that a similar result can be achieved for subtle emotions collected from in-the-wild social interactions of individuals without professional training in acting. Therefore, our dataset provides an opportunity to examine if emotions of lower intensity, produced from non-actors during communication, can be recognized accurately.\n\nIt is also interesting to examine whether subtle emotions could signal instances where emotions are misperceived during communication if their accurate detection is possible. In the same vein, to what extent the intensity of emotions influences their decoding accuracy during a social interaction, where a broader array of contextual information is present, is also worth exploring. K-EmoCon could enable an in-depth investigation of such issues.\n\nFurther, we considered the use case of mobile and wearable technologies for facilitating emotional communication.\n\nResearchers are actively exploring the potential for using expressive biosignals collected via wearables to communicate one's emotional and psychological states with others  [62, 63, 64, 65, 66] . Our dataset can contribute to the research of Spon. Natural Interval-based continuous S, P, E Dyadic † A dataset was considered to contain induced emotions if scripted interaction was involved in the data collection, even though no artificial stimuli (such as an emotion inducing video clip) was used. ‡ Predefined emotion categories of stimuli and success rates of participants in a set of purposefully selected cognitive tasks were used as ground-truth labels.\n\nbiosignal-based assistive technologies to enable affective communication by providing insights on when are apposite moments for communicating emotions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Ethics Statement",
      "text": "The construction of the K-EmoCon dataset was approved by the Korea Advanced Institute of Science and Technology (KAIST) Institutional Review Board. KAIST IRB also reviewed and approved the consent form, which contained information on the following: the purpose of data collection, data collection procedure, types of data to be collected from participants, compensation to be provided for participation, and the protocol for the protection of privacy-sensitive data.\n\nParticipants were given the same consent forms upon arriving at the data collection site and were asked to provide written consent after fully reading the form indicating that they are willing to participate in data collection. Since K-EmoCon is to be open to public access, a separate consent was obtained for the disclosure of the data that contains personally identifiable information (PII), which is the audiovisual footage of participants during debates, including their faces and voices. Participants were also notified that their participation is voluntary, and they can terminate the data collection at any. The resulting K-EmoCon dataset includes the audiovisual recordings of 21 participants, out of 32, who agreed to disclose their personal information, excluding the 11 who did not agree.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Participant Recruitment And Preparation",
      "text": "Table  2 : Participant pairs for debates.\n\nParticipants Gender and ages\n\n32 participants were recruited between January and March of 2019. An announcement calling for participation in an experiment on \"emotion-sensing during a debate\" was posted on an online bulletin board of a KAIST student community. The post stated that participants would have a debate on the issue of accepting Yemeni refugees on Jeju Island of South Korea for 10 minutes. It also stated that the debate must be in English, and participants should be capable of speaking competently in English, but not necessarily at the level of a native speaker. Specifically, participants were required to have at least three years of experience living in an English-speaking country, or have achieved a score above criteria in any one of standardized English speaking tests as listed here: TOEIC speaking level 7, TOEFL speaking score 27, or IELTS speaking level 7.\n\nOnce participants were assigned a date and time to participate in data collection, they were provided four news articles on the topic of the Jeju Yemeni refugee crisis via email. The articles included two articles with neutral views on the issue  [67, 68] , one in favor of refugees  [69] , and one in opposition to refugees  [70] . We instructed the participants to read the articles beforehand to familiarize themselves with the debate topic.\n\nAll selected participants were students at KAIST, but their ages varied from 19 to 36 years old (mean = 23.8 years, stdev. = 3.3 years), as well as their gender and nationality. We randomly paired participants into 16 dyads based on their available times. See Table  2  for the breakdown of participants' gender, nationality, and age.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Data Collection Setup",
      "text": "All data collection sessions were conducted in two rooms with controlled temperature and illumination. Two participants sat across a table facing each other with a distance in between for a comfortable communication (see Figure  1 ). Two Samsung Galaxy S7 smartphones mounted on tripods were placed in the middle of the table facing each participant, capturing facial expressions and movements in the upper body from the 3rd-person point of view (POV) along with the speech audio, via the camera app.\n\nDuring a debate, participants wore a suite of wearable sensors, as shown in Figure  2 , which includes:\n\n1. Empatica E4 Wristband -captured photoplethysmography (PPG), 3-axis acceleration, body temperature, and electrodermal activity (EDA). Heart rate and the inter-beat interval (IBI) were derived from Blood Volume Pulse (BVP) measured by a PPG sensor.  4. LookNTell Head-Mounted Camera -with a camera attached at one end of a plastic circlet, was worn on participants' heads to capture videos from a first-person POV.   3  summarizes sampling rates and signal ranges of data collected from each device.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Collection Procedure",
      "text": "Administration All data collection sessions were conducted in four stages of 1) onboarding, 2) baseline measurement, 3) debate, and 4) emotion annotation. Two experimenters administered each session (see Table  4  for the overview of a data collection procedure). One experimenter served as a moderator during debates, notifying participants of the remaining time and intervening under any necessary circumstances, such as when a debate gets too heated, or a participant exceeds an allotted time of 2 minutes in his or her turn.\n\nOnboarding Upon their arrival, participants were each provided a consent form asking for two written consents, first for the participation in data collection that was mandatory, and second for the disclosure of privacy-sensitive data collected during the session, which participants could opt-out without any disadvantage.\n\nOnce they agreed to participate in the research, participants decided whether they would argue for or against admitting the Yemeni refugees in Jeju. Participants could either briefly discuss with each other to settle on their preferred positions or toss a coin to decide at random. The same procedure was followed for deciding who goes first in the debate.\n\nNext, participants were given up to 15 minutes to prepare their arguments. Each participant was given a pen, paper, and prints of the articles that they previously received via email. After they finished preparing, experimenters equipped participants with wearable devices. Participants wore E4 wristbands on their non-dominant hand, as arm movements may impede an accurate measurement of PPG. Experimenters assured that wristbands are tightly fastened, and electrodes are in good contact with participants' skin. Experimenters also assured the EEG headsets and head-mounted cameras are well fitted on participants' head, and manually adjusted head-mounted cameras' lens to make sure the captured views are similar to participants' subjective view. Participants wore Polar H7 sensors attached to flexible bands underneath their clothes, so the electrodes are in contact with their skin and placed the sensors above their solar plexus.\n\nBaseline measurement With all devices equipped, sensor measurements were taken from participants while they watched a short clip. This step was to establish a baseline that constitutes a neutral state for each participant. Establishing a neutral baseline is commonly used in the construction of emotion datasets to account for individual biases and reduce the effect of previous emotional states, especially when repeated measurements are taken.\n\nA procedure for a baseline measurement varies across researchers and is often dependent on the purpose of an experiment  [71] . In stimuli-based experiments, researchers take measurements as their subjects watch a stimulus intended to induce a neutral emotional state  [23, 24]  or measure resting-state activities between stimuli if they are taking multiple consecutive measurements  [25] . Similarly, for K-EmoCon, participants watched Color Bars clip, which was previously reported in the work of Gross et al. to induce a neutral emotion  [72] . Experimenters also ensured that no devices were malfunctioning during the baseline measurement.\n\nTable  4 : Steps for a data collection session.\n\nStep",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Allocated Time Description",
      "text": "Read and sign consent forms 10 min Experimenters provided consent forms to participants, and two written consents each for participation and the collection of privacy-sensitive data were obtained.\n\nChoose sides and the order 5 min Participants were assigned to either argue in favor of or against accepting refugees and decided on the first speaker.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Prepare Debate 15 Min",
      "text": "Participants were provided with supplementary materials to prepare their arguments.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Equip Sensors 10 Min",
      "text": "Experimenters explained wearable devices to participants and assisted them in wearing devices.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Measure Baseline 2 Min",
      "text": "A baseline corresponding to a neutral state was measured for each participant.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Overview Debate 5 Min",
      "text": "The moderator explained the debate rules and notified participants that they are allowed to intervene.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Debate 10 Min",
      "text": "Participants could speak for two consecutive minutes during their turns and they were notified twice at 30 and 60 seconds before the end of the debate.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Annotate Emotions 60 Min",
      "text": "Participants annotated emotions at intervals of every 5 seconds, watching footage of themselves and their partners.\n\n* One session lasted approximately two hours.\n\nDebate A debate began at the sign of the moderator and lasted approximately 10 minutes. Participants' facial expressions, movements in their upper body, and speeches were recorded throughout a debate. Participants were allowed to speak consecutively up to two minutes during their turns, with turns alternating between two participants. However, participants were also notified that they could intervene during an opponent's turn, to allow a more natural communication. The moderator notified participants 30 and 60 seconds before the end of their turns and intervened if they exceeded two minutes. A debate stopped at the ten-minute mark with some flexibility to allow the last speaker to finish his or her argument.\n\nEmotion annotation Participants took a 15-minute break upon finishing a debate. Participants then were each assigned to a PC and annotated their own emotions and their partner's emotions during the debate. Specifically, each participant watched two audiovisual recordings of him/herself and his/her partner from 3rd-person POV (including facial expressions, upper body movements, and speeches), to annotate emotions at intervals of every 5 seconds from the beginning to the end of a debate. We chose 5 seconds based on the report of Busso et al. that the average duration of the speaker turns in IEMOCAP was about 4.5s  [51] , and findings from linguistics research also support this number  [73, 74, 75] .\n\nThis annotation method we employed, a retrospective affect judgment protocol, is widely used in affective computing to collect self-reports of emotions, especially in studies where an uninterrupted engagement of subjects during an emotion induction process is essential  [76, 77, 78, 79] . Likewise, we opted for this method as participants' natural interaction was necessary for acquiring quality emotion data.\n\nNote that we did not provide 1st-person POV recordings captured from head-mounted cameras to participants, and they only had 3rd-person POV recordings to annotate felt emotions. One may have a reasonable concern regarding this choice, that participants watching their faces likely caused them to occupy a perspective similar to an observer. Hence, this might have resulted in an unnatural measurement of felt emotions. Indeed, the headcam footage could have been a more naturalistic instrument, as we intuitively take an embodied perspective to recall how we felt at a specific moment in the past.\n\nHowever, we found the extent of information captured by the headcam footage insufficient for accurate annotation of felt emotions. Experimenters manually adjusted headcam lenses, so the recordings resembled participants' subjective views, but the headcam footage was missing fine-grained information such as participants' gazes. Also, past research   [98]  Choose one on memories for emotions has shown that they are prone to biases and distortion  [80, 81, 82] . In that regard, it seemed headcam videos, which contain limited information compared to frontal face recordings, would only result in an incorrect annotation of felt emotions, especially in retrospect. Further, we noted that it is not uncommon for people to infer emotions from their faces, as they frequently do when looking in a mirror or taking a selfie.\n\nAs a result, participants were given 3rd-person recordings of themselves for the retrospective annotation of felt emotions. In total, participants annotated emotions with 20 unique categories, as shown in Table  5 . Experimenters assisted participants throughout the annotation procedure. Before participants began annotating, experimenters explained individual emotion categories to participants, so they correctly understood a meaning and a specific annotation procedure for each item. Experimenters also explicitly instructed participants to report felt emotions, not perceived emotions on their faces. Lastly, experimenters ensured that the start time and end time for two participants matched to obtain synchronized annotations. External emotion annotation Additionally, we recruited five external raters to annotate participants' emotions during debates (see Table  6 ). We applied the same criteria we used for recruiting participants in data collection for the recruitment of the raters. The raters were provided the audiovisual debate footage of participants and annotated emotions following the same procedure our participants followed. External raters performed their tasks independently, and the experimenters communicated remotely with the raters. Once a rater finished annotating, an experimenter checked completed annotations for incorrect entries and requested a rater to review annotations if there were any missing values or misplaced entries.\n\n3 Data Records",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Dataset Summary",
      "text": "The resulting K-EmoCon dataset contains multimodal data from 16 paired-debates on a social issue, which sum to 172.92 minutes of dyadic interaction. It includes physiological signals measured with three wearable devices, audiovisual recordings of debates, and continuous annotations of emotions from three distinct perspectives of the subject, the partner, and the external observers. Table  7  summarizes data collection results and dataset contents.\n\nPreprocessing For the timewise synchronization across data, we converted all timestamps from Korea Standard Time (UTC +9) to UTC +0 and clipped raw data such that only parts of data corresponding to debates and baseline measurements are included. For debate audios and the footage, subclips corresponding to debates were extracted from the raw footage. Audio tracks containing participants' speeches were copied and saved separately as WAV files. Physiological signals were clipped from the respective beginnings of data collection sessions to the respective ends of debates, as the initial 1.5 to 2 minutes immediately after a session begins corresponds to a baseline measurement for a neutral state. Parts in between baseline measurements and debates correspond to debate preparations, which may be excluded from the analysis.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Dataset Contents",
      "text": "The K-EmoCon dataset  [83]  is available upon request on Zenodo: https://doi.org/10.5281/zenodo.3814370. In the following, we describe directories and files in the dataset and their contents.\n\nmetadata.tar.gz: includes files with auxiliary information about the dataset. Included files are:\n\n1. subjects.csv -each row contains a participant ID (pid) and three timestamps in UTC +0. Three timestamps respectively mark the beginning of a data collection (initTime), the start of a debate (startTime), and the end of a debate (endTime). 2. data_availability.csv -shows files available for each participant. For each participant (row), if a data file (column) is available, the corresponding cell is marked TRUE, otherwise FALSE.\n\ndata_quality_tables.tar.gz: includes seven CSV tables with information regarding the quality of physiological signals in the dataset. With participant IDs (pid) in rows and file types (ACC, BVP, EDA, HR, IBI, and TEMP for E4 data, and Attention, BrainWave, Meditation, and Polar_HR for NeuroSky + Polar H7 data) in columns, included files are as follows:\n\n1. e4_durations.csv -contains the duration of each file in seconds, where duration = (last timestamp -first timestamp) / 1000. 2. neuro_polar_durations.csv -same as above. 3. e4_zeros.csv -contains the number of zero values in each file. ACC and BVP were excluded as zero crossings are to be expected during their measurement. 4. neuro_polar_zeros.csv -same as above. Note that zero values for NeuroSky data (Attention, BrainWave, Mediation) indicate the inability of a device at a given moment to obtain a sufficiently reliable measurement due to various reasons. 5. e4_outliers.csv -contains the number of outliers in each file. Chauvenet's criterion was used for outlier detection (refer to Code Availability section for its implementation in Python).\n\n6. e4_completeness.csv -contains the completeness of each file as a ratio in the range of [0.0, 1.0]. 1.0 indicates a file without any missing value or an outlier. The completeness ratio was calculated as completeness = (total number of values -(number of outliers + number of zeros)) / total number of values.\n\n7. neuro_polar_completeness.csv -same as above, with completeness calculated as completeness = (total number of values -number of zeros) / total number of values.\n\ndebate_audios.tar.gz: contains 16 audio recordings of debates in the WAV file format. The name of each file follows the convention of p<X>.p<Y>.wav, where <X> and <Y> stand for IDs of two participants appearing in the audio. The start and the end of each recording correspond to startTime and endTime values in the subjects.csv file, respectively.\n\ndebate_recordings.tar.gz: contains video recordings of 21 participants during debates in the MP4 file format. The name of a file p<X>_<T>.mp4 indicates that the file is the recording of participant <X> that is <T> seconds long.\n\nneurosky_polar_data.tar.gz: includes subdirectories for each participant, from P1 to P32, which may contain up to four files as the following:\n\n1. Attention.csv -contains eSense Attention ranging from 1 to 100, representing how attentive a user was at a given moment. Attention values can be interpreted as the following: 1 to 20 -\"strongly lowered\", 20 to 40 -\"reduced\", 40 to 60 -\"neutral\", 60 to 80 -\"slightly elevated\", and 80 to 100 -\"elevated\". 0 indicates that the device was unable to calculate a sufficiently reliable value, possibly due to a signal contamination with noises.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "2.",
      "text": "BrainWave.csv -records the relative power of brainwave in the following 8 bands of EEG: delta (0.5 -2.75Hz), theta (3.5 -6.75Hz), low-alpha (7.5 -9.25Hz), high-alpha (10 -11.75Hz), low-beta (13 -16.75Hz), high-beta (18 -29.75Hz), low-gamma (31 -39.75Hz), and middle-gamma ). The values are without a unit and are only meant for inferring the fluctuation in the power of a certain band or comparing the relative strengths of bands with each other.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "3.",
      "text": "Meditation.csv -contains eSense Meditation ranging from 0 to 100, measuring the relaxedness of a user.\n\nFor their interpretation, use the same ranges and the meanings as those for the attention values.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "4.",
      "text": "Polar_HR.csv -contains heart rates measured with ECG sensors during debates.\n\ne4_data.tar.gz: contains subdirectories for each participant (except P2, P3, P6, and P7), which may contain up to six files as the following:\n\n1. E4_ACC.csv -measurements from a 3-axis accelerometer sampled at 32Hz in the range [-2g, 2g] under columns x, y, and z. Multiply raw numbers by 1/64 to convert them into units of g (i.e., a raw value of 64 is equivalent to 1g).\n\n2. E4_BVP.csv -PPG measurements sampled at 64Hz.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "3.",
      "text": "E4_EDA.csv -EDA sensor readings in units of µS, sampled at 4Hz. 4. E4_HR.csv -the average heart rates calculated in 10-second windows. The values are derived from the BVP measurements, and the values are entered at the frequency of 1Hz. The first 10 seconds of data after the beginning of a recording is not included as the derivation algorithm requires the initial 10 seconds of data to produce the first value.\n\n5. E4_IBI.csv -IBI measurements in milliseconds computed from the BVP. From a second row onwards, one row is separated from the previous row with an amount equal to a distance between two peaks (i.e., t i+1 -t i = IBI i ). Note that HR in terms of BPM can be derived from IBI by taking 60/(IBI * 1000).\n\n6. E4_TEMP.csv -a body temperature measured in the Celsius scale at the frequency of 4Hz.\n\nNote that E4 data entries for P29, P30, P31, and P32 are entered with each row designated with either one of two unique device_serial values. It is necessary that users of this dataset only use rows corresponding to a single device_serial. We further recommend using rows with the following device_serial values:\n\n• P29, P31 -A013E1 for all files, except A01525 for IBI.\n\n• P30, P32 -A01A3A for all files.\n\nemotion_annotations.tar.gz: includes four subdirectories as listed below, which each contain annotations for participant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:\n\n1. self_annotations -annotations of participant emotions by participants themselves.\n\n2. partner_annotations -annotations of participant emotions by respective debate partners.\n\n3. external_annotations -annotations of participant emotions by five external raters. Files follow the naming convention of P<X>.R<Z>.csv, where <X> is a participant ID, and <Z> is a rater number.\n\n4. aggregated_external_annotations -contains external rater annotations aggregated across five raters via majority voting. Refer to Code Availability section for the Python code implementing the majority vote aggregation.\n\nThe first row in a valid file has annotations for the first five seconds, and rows coming afterward contain annotations for the next consecutive five-second intervals, non-overlapping with each other. Also, each row in a valid file contains 10 non-empty values (eight numeric values, including seconds column, and two x's). Note that annotation files for a participant may not have an equal number of rows (e.g., there may be more self-annotations than partner/external annotations for some participants). In that case, longer files should be truncated from the start such that they have the same number of rows as shorter files since the extra annotations at the beginning are possibly from participants mistakenly annotating emotions during baseline measurements.\n\n4 Technical Validation",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Emotion Annotations",
      "text": "Distribution and frequency of emotions The distributions and the frequencies of emotion annotations are as shown in Figure  3 . Overall, annotations for emotions measured on Likert scales (arousal, valence, cheerful, happy, angry, nervous, and sad) are biased towards a neutral with only a minuscule fraction of annotations for non-neutral states. Categorical emotion annotations (common and less common BROMP affective categories) are similarly biased, with a predominant portion of annotations falling under only two categories of concentration and none. This imbalance in annotations is as expected as emotion data is commonly imbalanced by its nature in the wild (i.e., people are more often neutral than angry or sad)  [84, 85, 86] .\n\nInter-rater reliability As individual-level information is missing in aggregated data, we used Krippendorff's alpha  [87] , which is a generalized statistic of agreement applicable to any number of raters, to measure the inter-rater reliability (IRR) of emotion annotations from different perspectives for each participant. Figure  4  shows heatmaps of alpha coefficients computed for seven emotions measured on ordinal scales (arousal, valence, cheerful, happy, angry, nervous, and sad).\n\nAll annotation values were interpreted as rank-ordered (ordinal scaled) for the IRR computation. Likert scales we used are not intervals or ratios with meaningful distances in-between. While participants and raters were provided numeric scales labeled with semantic meanings (see Table  5 ), the individual interpretations of scales were likely disparate.\n\nGiven that, before the computation, annotation values were scaled relative to a neutral, by estimating modes of columns as neutrals and deducting them from respective column values (i.e., if the mode of a cheerful column for a particular participant was one, then one was subtracted from all values in that cheerful column). This mode-subtraction step was necessary to prevent the underestimation of IRRs.\n\nAnnotations in our dataset for scaled emotions are highly biased as shown in Fig.  3 ). However, while arousal and valence are explicitly centered at zero (which corresponds to 3 = neutral), five emotions measured in the scale of 1 = very low to 4 = very high (cheerful, happy, angry, nervous, and sad) are systematically biased without a zero neutral. All of their values indicate that some emotion is present, and this absence of zero results in a widely varying interpretation of scale values by our participants and raters.\n\nConsider the following scenario further elaborating this issue: a subject rates that she was cheerful as much as 1 for the first third of a debate, then 2 for the rest, but her debate partner rates that she was cheerful as much as 3 for the first third then 4 for the rest. In this example, self and partner annotations both imply that the subject was less cheerful for the first third of the debate. However, an IRR of two sets of annotations is close to zero without subtracting modes. Indeed, it is possible that the partner perceived the subject as more cheerful overall, compared to the subject herself. In that case, a low IRR correctly measures the difference between emotion perceptions of the subject and partner. Nevertheless, this assumption cannot be confirmed, as there is no neutral baseline. Therefore, we applied the proposed mode-subtraction to emotion annotations such that alpha coefficients measure raters' agreement on relative changes in emotions rather than their absolute agreement with each other. This adjustment mitigates spuriously low alpha coefficient values obtained from raw annotations (refer to Code Availability section for the code implementing the mode-subtraction and plotting of heatmaps).\n\nThese fixed alpha coefficients are low in general. In particular, a noticeable pattern emerges when comparing alpha coefficients of self-partner (SP) annotations and self-external (SE) annotations. As shown in the last rows of heatmaps (Diff. [SE -SP]) in Fig.  4 , the differences between the IRRs of SE annotations and SP annotations tend to be above zero (for 20 out of 32 participants for arousal: mean = 0.143, stdev. = 0.322). This pattern possibly indicates that there exists a meaningful difference in the perception of emotions from different perspectives, while further study is required to validate its significance.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Physiological Signals",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Data Quality",
      "text": "The quality of physiological signal measurements in the dataset has been thoroughly examined. The examination results are included as a part of the dataset in the data_quality_tables.tar.gz archive file.\n\nMissing data E4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data collection. While physiological signals in the dataset are mostly error-free with most of the files complete above 95%, a portion of data is missing due to issues inherent to devices or a human error:\n\n• IBI -data from P26 is missing as the internal algorithm of E4 that derives IBI from BVP automatically discards an obtained value if its reliability is below a certain threshold.\n\n• EDA -data from P17 and P20 is missing, possibly due to poor contact between the device and a participant's skin.\n\n• NeuroSky (Attention, Meditation) -measurements from P1 and P20 are missing due to a poorly equipped device. A portion of data is missing for P19 ( 32%), P22 ( 59%) and P23 ( 36%) for the same reason. No BrainWave data was lost.\n\n• Polar HR -data from seven participants (P3, P12, P18, P20, P21, P29, and P30) are missing due to a device error during data collection. Parts of data are missing from P4 ( 38%) and P22 ( 38%) due to poor contact. 5 Usage Notes",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Potential Applications",
      "text": "In addition to the intended usage of the dataset discussed above, there are uncertainties as to how physiological markers of an individual's capacity for flexible physiological reactivity relate to experiences of positive and negative emotions.\n\nOur dataset could potentially be useful to examine the role of physiological signal based markers in assessing an individual's use of emotion regulation strategies, such as cognitive appraisal.\n\nAdditionally, various data mining and machine learning techniques could be applied to set up the models for an individual's emotional profiles based on sensor-based physiological and behavioral recordings. This could further be transferred to other use-cases, such as helping children with autism in their social communication  [88, 89] , helping people who are blind to read facial expressions and get the emotion information of their peers  [90] , helping robots interact more intelligently with people  [91, 92] , and monitoring signs of frustration and emotional saturation that affect attention while driving, to enhance driver safety  [93, 94] .",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Limitations",
      "text": "Data collection apparatus Contact-base EEG sensors are known to be susceptible to noises, for example, frowning or eyes-movement might have caused peaks in the data. Other devices may also have been subject to similar systematic errors.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Data Collection Context",
      "text": "The context of the turn-taking debate may have caused participants to regulate or even suppress their emotional expressions, as an unrestrained display of emotions is often regarded undesirable during a debate. This may have contributed to a deflated level of agreement between self-reports and partner/external perceptions of emotions, which may not be a case for more natural interactions in the wild.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Demographics",
      "text": "The participant demographics likely have introduced bias in the data. All of our participants and raters are young (their ages were between 19 to 36) and highly-educated, and the majority of them are individuals of Asian ethnicity. Therefore, our data may not generalize well to individuals of different ethnic groups or of younger or older age groups.\n\nUnaccounted variables Many variables unaccounted during data collection, such as the level of rapport between debating pairs, a participant's competence in spoken English, and a participant's familiarity with the debate topic, may also have contributed to a variance in the level of mismatch between the perceptions of emotions across different perspectives.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Code Availability",
      "text": "Python codes implementing outlier detection using Chauvenet's criterion, majority voting, mode-subtraction, and other utility functions, including the generation of heatmap plots, are available on https://github.com/Kaist-ICLab/ K-EmoCon_SupplementaryCodes. The Krippendorff Python package (https://github.com/pln-fing-udelar/ fast-krippendorff) was used for the computation of Krippendorff's alpha. The Python of version 3.6.9 was used throughout.\n\nCodes for preprocessing the raw log-level data in SQL databases to CSV files were implemented in Python with the SQLAlchemy package. However, these codes and the raw log-level data are not made available as they include privacy-sensitive information outside the agreed boundary for public sharing of the dataset, which was collected only for logistic reasons. Nevertheless, we welcome users of the dataset to contact the corresponding authors if they need any further assistance or information regarding the raw data, and it's preprocessing.",
      "page_start": 15,
      "page_end": 15
    }
  ],
  "figures": [
    {
      "caption": "Figure 2: , which includes:",
      "page": 5
    },
    {
      "caption": "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on",
      "page": 6
    },
    {
      "caption": "Figure 2: Frontal view of a participant equipped with wearable sensors.",
      "page": 6
    },
    {
      "caption": "Figure 3: Overall, annotations for emotions measured on Likert scales (arousal, valence, cheerful, happy, angry,",
      "page": 12
    },
    {
      "caption": "Figure 4: shows heatmaps of",
      "page": 12
    },
    {
      "caption": "Figure 3: ). However, while arousal and",
      "page": 12
    },
    {
      "caption": "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and",
      "page": 13
    },
    {
      "caption": "Figure 4: , the differences between the IRRs of SE annotations and SP annotations tend to be above zero",
      "page": 13
    },
    {
      "caption": "Figure 4: Heatmaps of inter-rater reliabilities measured with Krippendorff’s alpha. External annotations were aggregated",
      "page": 14
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": "Recognizing emotions during social interactions has many potential applications with the popular-"
        },
        {
          "ABSTRACT": "ization of low-cost mobile sensors, but a challenge remains with the lack of naturalistic affective"
        },
        {
          "ABSTRACT": "interaction data. Most existing emotion datasets do not support studying idiosyncratic emotions"
        },
        {
          "ABSTRACT": "arising in the wild as they were collected in constrained environments. Therefore, studying emotions"
        },
        {
          "ABSTRACT": "in the context of social interactions requires a novel dataset, and K-EmoCon is such a multimodal"
        },
        {
          "ABSTRACT": "dataset with comprehensive annotations of continuous emotions during naturalistic conversations."
        },
        {
          "ABSTRACT": "The dataset contains multimodal measurements, including audiovisual recordings, EEG, and periph-"
        },
        {
          "ABSTRACT": "eral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately"
        },
        {
          "ABSTRACT": "10-minute long paired debates on a social issue. Distinct from previous datasets, it includes emotion"
        },
        {
          "ABSTRACT": "annotations from all three available perspectives: self, debate partner, and external observers. Raters"
        },
        {
          "ABSTRACT": "annotated emotional displays at intervals of every 5 seconds while viewing the debate footage, in"
        },
        {
          "ABSTRACT": "terms of arousal-valence and 18 additional categorical emotions. The resulting K-EmoCon is the"
        },
        {
          "ABSTRACT": "ﬁrst publicly available emotion dataset accommodating the multiperspective assessment of emotions"
        },
        {
          "ABSTRACT": "during social interactions."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "during social interactions.": "1\nBackground & Summary"
        },
        {
          "during social interactions.": "Emotion recognition research seeks to enable computers to identify emotions. It is a foundation for creating machines"
        },
        {
          "during social interactions.": "capable of understanding emotions, and possibly, even expressing one. Such a set of skills to recognize, understand,"
        },
        {
          "during social interactions.": "and express emotions form emotional intelligence [1, 2]. It is suggested that emotional intelligence is necessary for"
        },
        {
          "during social interactions.": "the navigation of oneself within a society, as it allows one to reason what is desirable and what is not, and to regulate"
        },
        {
          "during social interactions.": "behaviors of self and others accordingly [3, 4]."
        },
        {
          "during social interactions.": "Then why do machines need emotional skills? With advances in Machine Learning and Artiﬁcial\nIntelligence,"
        },
        {
          "during social interactions.": "the transition from human to machine is noticeable in all areas of the society,\nincluding those requiring expertise"
        },
        {
          "during social interactions.": "such as medical prognosis/diagnosis [5, 6] or automobile driving [7].\nIt seems inevitable that\nthese narrow AI"
        },
        {
          "during social interactions.": "∗Correspondence and requests for materials should be addressed to C.P.\n(email:\n(email: cheulyop@kaist.ac.kr) or U.L."
        },
        {
          "during social interactions.": "uclee@kaist.edu)"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "superior performance in the game of Go over human champions [9, 10]."
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "Not all AI will compete with humans, albeit their superhuman ability. Instead, many AI systems will work with us or"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "for us. Emotional intelligence is critical for such human-computer interaction systems [11]. Imagine a smart speaker"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "that delightfully greets users when they come home. How should a speaker greet when a user had a rough day? A"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "speaker neglectful of the user’s emotional states may aggravate the user, but a speaker aware of the user’s temper could"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "remain silent to avoid the trouble. Similarly, emotional intelligence is critical for AI systems designed for complex"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "tasks. For example, on roads where autonomous and human-driven vehicles mix, accurate recognition of emotions of"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "human drivers’ by autonomous vehicles would lead to more safety as autonomous vehicles can better judge human"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "drivers’ intentions [12]."
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "Now for machines to become emotionally intelligent, they must ﬁrst learn to recognize emotions, and the prerequisite to"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "learning is data. However, there lie several challenges in the acquisition of emotion data. While emotions are prevalent,"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "their accurate measurement is difﬁcult. Most commonly, emotions are viewed as psychological states expressed through"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "faces, with distinct categories [13], but research evidence claims the contrary. Rather than distinct, facial expressions"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "are compound [14], relative [15], and misleading [16]. A recent review of scientiﬁc evidence also presses against the"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "common view, suggesting that facial expressions lack reliability, speciﬁcity, and generalizability [17], together with"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "past studies on contextual dependency [18, 19, 20] and individual variability of emotions [21, 22]."
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "Such inherent elusiveness of emotion renders many existing emotion datasets inapplicable for studying emotions in the"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "wild. The majority of emotion datasets consist of emotions induced with selected stimuli in a static environment, i.e.,"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "a laboratory [23, 24, 25, 26, 27, 28, 29]. This method provides experimenters with full-control over data collection,"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "allowing assessment of speciﬁc emotional behaviors [30, 31] and acquiring ﬁne-grained data with advanced techniques"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "like neuroimaging. Nevertheless, lab-generated data may generalize poorly to realistic scenarios as they frequently"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "contain intense expressions of prototypical emotions, which are rarely observed in the real world [32, 33], acquired"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "from only a subset of the population [34]."
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "An alternative approach utilizes media contents [35, 36, 37, 38] and crowdsourcing [39], compensating for\nthe"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "shortcomings of the conventional method. The abundance of contents available online, such as TV-shows and movies,"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "allows researchers to glean rich emotion data representative of various contexts efﬁciently. Crowdsourcing further"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "supports inexpensive data annotation while serving as another data source [40, 41]. Datasets of this type have advantages"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "in sample size and the diversity of subjects, but generalizability remains an issue. Datasets based on media contents"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "often contain emotional displays produced by trained actors supposing ﬁctitious situations. To what extent such"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "emotional portrayals resemble spontaneous emotional expressions is debatable [42, 43, 44]. They also provide no"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "access to physiological signals, which are known to carry information vital for the detection of less visible changes in"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "emotional states [45, 46, 47, 48, 49, 50]."
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "To amend this lack of a dataset for recognition of emotions in their natural forms, we introduce K-EmoCon, a multimodal"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "dataset acquired from 32 subjects participating in 16 paired debates on a social\nissue.\nIt consists of physiological"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "sensor data collected with three off-the-shelf wearable devices, audiovisual footage of participants during the debate,"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "and continuous emotion annotations.\nIt contributes to the current literature of emotion recognition, as according to"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "our knowledge, it is the ﬁrst dataset with emotion annotations from all possible perspectives as the following: subject"
        },
        {
          "systems [8] supersede human experts in respective domains, as it has already been demonstrated with AlphaGo’s": "him/herself, debate partner, and external observers."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "the issue of establishing ground truth in emotion annotations. Emotions are inherently internal phenomena, and their"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "mechanism is unavailable for external scrutiny, even for oneself who is experiencing emotions. As a result, there may"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "not be a ground truth for emotions. Should we consider what is most agreed upon by external observers of emotions as"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "the ground truth, or what the person who experiences emotions reports to have felt the ground truth [56]? Two views"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "are likely to match if emotions are intense and pure, but as discussed, such emotions are rare. Instead, self-reported and"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "observed emotions are likely to disagree for a variety of reasons. People often conceal their true emotions; sometimes,"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "people are not fully mindful of their internal states; and some people may have difﬁculties in interpreting or articulating"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "emotions [57, 58]."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "With K-EmoCon, we intend to enable the comprehensive examination of such cases where perceptions of emotions"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "do not match, by bringing all three available perspectives into the annotation of emotions, in the context of a social"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "interaction involving three parties of:"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "1. The subject – is the source who experiences emotions ﬁrsthand and produces self annotations, particularly the"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "“felt sense” [55] of the emotions."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "2. The partner – is the person who interacts with the subject, experiencing the subject’s emotions secondhand;"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "thus, he or she has a contextual knowledge of the interaction that induced the subject’s emotions and produces"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "partner annotations based on that."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "3. The external observers – are people who observe the subject’s emotions without the exact contextual knowledge"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "of the interaction that induced the emotions, producing external observer annotations."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "Notice, that while our deﬁnition of perspectives involved in emotion annotation is similar to deﬁnitions previously used"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "by other researchers (self-reported vs. perceived [55]/observed [59]), we further segment observer annotations based on"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "whether the contextual information of the situation in which the emotion was generated is available to an observer, as"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "we wish to consider the role of contextual knowledge in emotion perception and recognition."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "Existing datasets of emotions in conversations provide a limited scope on this issue as they at most contain emotion"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "annotations from subjects and external observers [51], leaving out annotations from other people who engaged in the"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "conversation (whom we call partners). Or, they either only consider a particular type of annotations that is sufﬁcient to"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "serve their research goal [53] or their designs do not allow acquiring multi-perspective annotations [52, 38] (e.g., a"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "dataset is constructed upon conversations from a TV-show, only allowing the collection of external observer annotations)."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "Refer to Table 1 to see how K-EmoCon is distinguished from existing emotion datasets."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "Context of data collection\nIn this regard, we chose a semi-structured,\nturn-taking debate on a social\nissue with"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "randomly assigned partners as the setting for data collection. This setting is appropriate for collecting emotions that"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "may naturally arise in a day, as it is similar to a social interaction that one could engage in a workplace."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "Also,\nthe setting is particularly suitable for studying the misperception of emotions.\nIt\nis sufﬁciently formal and"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "spontaneous as it\ninvolves randomly assigned partners. We expect such formality and spontaneity of\nthe setting"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "compelled participants\nto regulate their emotions\nin a socially appropriate manner, allowing us\nto observe less"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "pronounced emotions from participants, which were more likely to be misperceived by their partners [60]."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "Data collection apparatus\nOur choice of mobile, wearable, and low-cost devices to collect affective physiological"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "signals together with audiovisual recordings, while primarily aims to make ﬁndings based on our data more reproducible"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "and expandable, was also in consideration of our goal of investigating mismatches in perceptions of emotions in the"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "wild. Research has shown that fusing implicit and explicit affective information can result in more accurate recognition"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "of subtle emotional expressions from professional actors [61]. However, no work we are aware of has shown that a"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "similar result can be achieved for subtle emotions collected from in-the-wild social interactions of individuals without"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "professional training in acting. Therefore, our dataset provides an opportunity to examine if emotions of lower intensity,"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "produced from non-actors during communication, can be recognized accurately."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "It\nis also interesting to examine whether subtle emotions could signal\ninstances where emotions are misperceived"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "during communication if their accurate detection is possible. In the same vein, to what extent the intensity of emotions"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "inﬂuences their decoding accuracy during a social\ninteraction, where a broader array of contextual\ninformation is"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "present, is also worth exploring. K-EmoCon could enable an in-depth investigation of such issues."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "Further, we considered the use case of mobile and wearable technologies for facilitating emotional communication."
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "Researchers are actively exploring the potential for using expressive biosignals collected via wearables to communicate"
        },
        {
          "emotions (i.e., subject him/herself, interacting partner, and external observers). Having multiple perspectives relates to": "one’s emotional and psychological states with others [62, 63, 64, 65, 66]. Our dataset can contribute to the research of"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "annotations."
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "Name (year)"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "IEMOCAP"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2008) [51]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "SEMAINE"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2011) [52]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "MAHNOB-HCI"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2011) [23]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "DEAP"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2012) [24]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "DECAF"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2015) [25]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "ASCERTAIN"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2016) [26]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "MSP-IMPROV"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2016) [53]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "DREAMER"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2017) [27]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "AMIGOS"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2018) [28]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "MELD"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2019) [38]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "CASE"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2019) [29]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "CLAS"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2020) [95]"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "K-EmoCon"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": "(2020)"
        },
        {
          "Table 1: Comparison of the K-EmoCon dataset with the existing multimodal emotion recognition datasets. Posed emotions are when a": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: for the breakdown of participants’",
      "data": [
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "The construction of the K-EmoCon dataset was approved by the Korea Advanced Institute of Science and Technology"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "(KAIST) Institutional Review Board. KAIST IRB also reviewed and approved the consent form, which contained"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "from participants, compensation to be provided for participation, and the protocol for the protection of privacy-sensitive"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "Participants were given the same consent forms upon arriving at"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "written consent after fully reading the form indicating that"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "K-EmoCon is to be open to public access, a separate consent was obtained for the disclosure of the data that contains"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "personally identiﬁable information (PII), which is the audiovisual footage of participants during debates, including their"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "faces and voices. Participants were also notiﬁed that their participation is voluntary, and they can terminate the data"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "collection at any. The resulting K-EmoCon dataset includes the audiovisual recordings of 21 participants, out of 32,"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "who agreed to disclose their personal information, excluding the 11 who did not agree."
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "Participant recruitment and preparation"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "32 participants were recruited between January and March of 2019. An"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "announcement calling for participation in an experiment on “emotion-sensing"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "during a debate” was posted on an online bulletin board of a KAIST student"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "community. The post stated that participants would have a debate on the issue"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "of accepting Yemeni refugees on Jeju Island of South Korea for 10 minutes."
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "It also stated that the debate must be in English, and participants should be"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "capable of speaking competently in English, but not necessarily at the level of"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "a native speaker. Speciﬁcally, participants were required to have at least three"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "years of experience living in an English-speaking country, or have achieved"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "a score above criteria in any one of standardized English speaking tests as"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "listed here: TOEIC speaking level 7, TOEFL speaking score 27, or IELTS"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "Once participants were assigned a date and time to participate in data collec-"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "tion, they were provided four news articles on the topic of the Jeju Yemeni"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "refugee crisis via email. The articles included two articles with neutral views"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "on the issue [67, 68], one in favor of refugees [69], and one in opposition to"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "refugees [70]. We instructed the participants to read the articles beforehand"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "to familiarize themselves with the debate topic."
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "All selected participants were students at KAIST, but their ages varied from"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "gender and nationality. We randomly paired participants into 16 dyads based"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "See Table 2 for"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "All data collection sessions were conducted in two rooms with controlled temperature and illumination. Two participants"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "sat across a table facing each other with a distance in between for a comfortable communication (see Figure 1). Two"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "Samsung Galaxy S7 smartphones mounted on tripods were placed in the middle of the table facing each participant,"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "capturing facial expressions and movements in the upper body from the 3rd-person point of view (POV) along with the"
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": ""
        },
        {
          "biosignal-based assistive technologies to enable affective communication by providing insights on when are apposite": "During a debate, participants wore a suite of wearable sensors, as shown in Figure 2, which includes:"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "tripods in the middle of the table (highlighted in red) recorded participants’ facial expressions and movements in their"
        },
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "upper body, as shown on the right in the sample screenshot of footage."
        },
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "Figure 2: Frontal view of a participant equipped with wearable sensors."
        },
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "2. Polar H7 Bluetooth Heart Rate Sensor – detected heart rates using an electrocardiogram (ECG) sensor and"
        },
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "was used to complement a PPG sensor in E4, which is susceptible to motion."
        },
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "3. NeuroSky MindWave Headset – collected electroencephalogram (EEG) signals via two dry sensor electrodes,"
        },
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "one on the forehead (fp1 channel-10/20 system at the frontal lobe) and one on the left earlobe (reference)."
        },
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "4. LookNTell Head-Mounted Camera – with a camera attached at one end of a plastic circlet, was worn on"
        },
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "participants’ heads to capture videos from a ﬁrst-person POV."
        },
        {
          "Figure 1: Picture on the left shows a pair of participants sitting at a table preparing for a debate. Two smartphones on": "6"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "Collected data"
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "3-axis acceleration"
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "BVP (PPG)"
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "EDA"
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": ""
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "Heart rate (from BVP)"
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "IBI (from BVP)"
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "Body temperature"
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "Brainwave (fp1 channel EEG)"
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": ""
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "Attention & Meditation"
        },
        {
          "Table 3: Data collected with each wearable device, with respective sampling rates and signal ranges.": "HR (ECG)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "Polar H7 Heart Rate Sensor\nHR (ECG)\n2Hz\nn/a"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "All listed devices can operate in a mobile setting. Empatica E4 keeps the data on the device, and the collected data"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "is later uploaded to a computer. Polar H7 sensor and MindWave headset can communicate with a mobile phone via"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "Bluetooth Low Energy (BLE) to store data. Table 3 summarizes sampling rates and signal ranges of data collected from"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "each device."
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "2.5\nData collection procedure"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "Administration\nAll data collection sessions were conducted in four stages of 1) onboarding, 2) baseline measurement,"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "3) debate, and 4) emotion annotation. Two experimenters administered each session (see Table 4 for the overview"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "of a data collection procedure). One experimenter served as a moderator during debates, notifying participants of"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "the remaining time and intervening under any necessary circumstances, such as when a debate gets too heated, or a"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "participant exceeds an allotted time of 2 minutes in his or her turn."
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "Onboarding\nUpon their arrival, participants were each provided a consent form asking for two written consents, ﬁrst"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "for the participation in data collection that was mandatory, and second for the disclosure of privacy-sensitive data"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "collected during the session, which participants could opt-out without any disadvantage."
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "Once they agreed to participate in the research, participants decided whether they would argue for or against admitting"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "the Yemeni refugees in Jeju. Participants could either brieﬂy discuss with each other to settle on their preferred positions"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "or toss a coin to decide at random. The same procedure was followed for deciding who goes ﬁrst in the debate."
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "Next, participants were given up to 15 minutes to prepare their arguments. Each participant was given a pen, paper, and"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "prints of the articles that they previously received via email. After they ﬁnished preparing, experimenters equipped"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "participants with wearable devices. Participants wore E4 wristbands on their non-dominant hand, as arm movements"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "may impede an accurate measurement of PPG. Experimenters assured that wristbands are tightly fastened, and electrodes"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "are in good contact with participants’ skin. Experimenters also assured the EEG headsets and head-mounted cameras are"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "well ﬁtted on participants’ head, and manually adjusted head-mounted cameras’ lens to make sure the captured views"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "are similar to participants’ subjective view. Participants wore Polar H7 sensors attached to ﬂexible bands underneath"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "their clothes, so the electrodes are in contact with their skin and placed the sensors above their solar plexus."
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "Baseline measurement\nWith all devices equipped, sensor measurements were taken from participants while they"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "watched a short clip. This step was to establish a baseline that constitutes a neutral state for each participant. Establishing"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "a neutral baseline is commonly used in the construction of emotion datasets to account for individual biases and reduce"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "the effect of previous emotional states, especially when repeated measurements are taken."
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "A procedure for a baseline measurement varies across researchers and is often dependent on the purpose of an"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "experiment [71].\nIn stimuli-based experiments,\nresearchers take measurements as their subjects watch a stimulus"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "intended to induce a neutral emotional state [23, 24] or measure resting-state activities between stimuli if they are taking"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "multiple consecutive measurements [25]. Similarly, for K-EmoCon, participants watched Color Bars clip, which was"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "previously reported in the work of Gross et al.\nto induce a neutral emotion [72]. Experimenters also ensured that no"
        },
        {
          "Attention & Meditation\n1Hz\n[0, 100]": "devices were malfunctioning during the baseline measurement."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 4: Steps for a data collection session.": "Allocated time"
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": "10 min"
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": "5 min"
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": "15 min"
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": "10 min"
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": "2 min"
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": "5 min"
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": "10 min"
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": ""
        },
        {
          "Table 4: Steps for a data collection session.": "60 min"
        },
        {
          "Table 4: Steps for a data collection session.": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "*\nOne session lasted approximately two hours.": "Debate\nA debate began at\nthe sign of\nthe moderator and lasted approximately 10 minutes.\nParticipants’\nfacial"
        },
        {
          "*\nOne session lasted approximately two hours.": "expressions, movements in their upper body, and speeches were recorded throughout a debate. Participants were"
        },
        {
          "*\nOne session lasted approximately two hours.": "allowed to speak consecutively up to two minutes during their turns, with turns alternating between two participants."
        },
        {
          "*\nOne session lasted approximately two hours.": "However, participants were also notiﬁed that they could intervene during an opponent’s turn, to allow a more natural"
        },
        {
          "*\nOne session lasted approximately two hours.": "communication. The moderator notiﬁed participants 30 and 60 seconds before the end of their turns and intervened if"
        },
        {
          "*\nOne session lasted approximately two hours.": "they exceeded two minutes. A debate stopped at the ten-minute mark with some ﬂexibility to allow the last speaker to"
        },
        {
          "*\nOne session lasted approximately two hours.": "ﬁnish his or her argument."
        },
        {
          "*\nOne session lasted approximately two hours.": "Emotion annotation\nParticipants took a 15-minute break upon ﬁnishing a debate.\nParticipants then were each"
        },
        {
          "*\nOne session lasted approximately two hours.": "assigned to a PC and annotated their own emotions and their partner’s emotions during the debate. Speciﬁcally, each"
        },
        {
          "*\nOne session lasted approximately two hours.": "participant watched two audiovisual recordings of him/herself and his/her partner from 3rd-person POV (including"
        },
        {
          "*\nOne session lasted approximately two hours.": "facial expressions, upper body movements, and speeches), to annotate emotions at intervals of every 5 seconds from"
        },
        {
          "*\nOne session lasted approximately two hours.": "the beginning to the end of a debate. We chose 5 seconds based on the report of Busso et al.\nthat\nthe average"
        },
        {
          "*\nOne session lasted approximately two hours.": "duration of the speaker turns in IEMOCAP was about 4.5s [51], and ﬁndings from linguistics research also support this"
        },
        {
          "*\nOne session lasted approximately two hours.": "number [73, 74, 75]."
        },
        {
          "*\nOne session lasted approximately two hours.": "This annotation method we employed, a retrospective affect judgment protocol, is widely used in affective computing to"
        },
        {
          "*\nOne session lasted approximately two hours.": "collect self-reports of emotions, especially in studies where an uninterrupted engagement of subjects during an emotion"
        },
        {
          "*\nOne session lasted approximately two hours.": "induction process is essential [76, 77, 78, 79]. Likewise, we opted for this method as participants’ natural interaction"
        },
        {
          "*\nOne session lasted approximately two hours.": "was necessary for acquiring quality emotion data."
        },
        {
          "*\nOne session lasted approximately two hours.": "Note that we did not provide 1st-person POV recordings captured from head-mounted cameras to participants, and"
        },
        {
          "*\nOne session lasted approximately two hours.": "they only had 3rd-person POV recordings to annotate felt emotions. One may have a reasonable concern regarding this"
        },
        {
          "*\nOne session lasted approximately two hours.": "choice, that participants watching their faces likely caused them to occupy a perspective similar to an observer. Hence,"
        },
        {
          "*\nOne session lasted approximately two hours.": "this might have resulted in an unnatural measurement of felt emotions. Indeed, the headcam footage could have been a"
        },
        {
          "*\nOne session lasted approximately two hours.": "more naturalistic instrument, as we intuitively take an embodied perspective to recall how we felt at a speciﬁc moment"
        },
        {
          "*\nOne session lasted approximately two hours.": "in the past."
        },
        {
          "*\nOne session lasted approximately two hours.": "However, we found the extent of information captured by the headcam footage insufﬁcient for accurate annotation of"
        },
        {
          "*\nOne session lasted approximately two hours.": "felt emotions. Experimenters manually adjusted headcam lenses, so the recordings resembled participants’ subjective"
        },
        {
          "*\nOne session lasted approximately two hours.": "views, but the headcam footage was missing ﬁne-grained information such as participants’ gazes. Also, past research"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: Experimenters assisted",
      "data": [
        {
          "Table 5: Collected emotion annotations.": "Description"
        },
        {
          "Table 5: Collected emotion annotations.": "Two affective dimensions from Russell’s"
        },
        {
          "Table 5: Collected emotion annotations.": ""
        },
        {
          "Table 5: Collected emotion annotations.": "circumplex model of affect [96]"
        },
        {
          "Table 5: Collected emotion annotations.": "Emotion states describing a subjective"
        },
        {
          "Table 5: Collected emotion annotations.": "stress state [97]"
        },
        {
          "Table 5: Collected emotion annotations.": "Commonly used Baker Rodrigo"
        },
        {
          "Table 5: Collected emotion annotations.": "Ocumpaugh Monitoring Protocol (BROMP)"
        },
        {
          "Table 5: Collected emotion annotations.": "educationally relevant affective categories [98]"
        },
        {
          "Table 5: Collected emotion annotations.": ""
        },
        {
          "Table 5: Collected emotion annotations.": "Less commonly used BROMP"
        },
        {
          "Table 5: Collected emotion annotations.": ""
        },
        {
          "Table 5: Collected emotion annotations.": "educationally relevant affective categories [98]"
        },
        {
          "Table 5: Collected emotion annotations.": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 5: Experimenters assisted",
      "data": [
        {
          "Choose one\nDisgust / Eureka / Pride /": "educationally relevant affective categories [98]"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "Sorrow / None"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "on memories for emotions has shown that they are prone to biases and distortion [80, 81, 82]. In that regard, it seemed"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "headcam videos, which contain limited information compared to frontal face recordings, would only result\nin an"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "incorrect annotation of felt emotions, especially in retrospect. Further, we noted that it is not uncommon for people to"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "infer emotions from their faces, as they frequently do when looking in a mirror or taking a selﬁe."
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "As a result, participants were given 3rd-person recordings of themselves for the retrospective annotation of felt emotions."
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "In total, participants annotated emotions with 20 unique categories, as shown in Table 5. Experimenters assisted"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "participants throughout\nthe annotation procedure. Before participants began annotating, experimenters explained"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "individual emotion categories to participants,\nso they correctly understood a meaning and a speciﬁc annotation"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "procedure for each item. Experimenters also explicitly instructed participants to report felt emotions, not perceived"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "emotions on their faces. Lastly, experimenters ensured that the start time and end time for two participants matched to"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "obtain synchronized annotations."
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "External emotion annotation\nAdditionally, we recruited ﬁve external raters to anno-\nTable 6: Gender and age of"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "tate participants’ emotions during debates (see Table 6). We applied the same criteria we\nexternal raters."
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "used for recruiting participants in data collection for the recruitment of the raters. The"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "raters were provided the audiovisual debate footage of participants and annotated emo-\nRaters\nGender and age"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "tions following the same procedure our participants followed. External raters performed"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "R1\nM (27)"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "their tasks independently, and the experimenters communicated remotely with the raters."
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "R2\nM (25)"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "Once a rater ﬁnished annotating, an experimenter checked completed annotations for"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "R3\nF (22)"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "incorrect entries and requested a rater to review annotations if there were any missing"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "R4\nM (24)"
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "values or misplaced entries."
        },
        {
          "Choose one\nDisgust / Eureka / Pride /": "R5\nF (28)"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 7: Summary of data collection results and the dataset.": "Data collection summary"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "32 (20 males and 12 females)"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "19 to 36 (mean = 23.8 years, stdev. = 3.3 years)"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "Total 172.92 min, (mean = 10.8 min, stdev. = 1.04 min)"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "1 - 5: Arousal, Valence"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "1 - 4: Cheerful, Happy, Angry, Nervous, Sad"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": ""
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "Choose one: Common BROMP affective categories +"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "less common BROMP affective categories"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "3-axis Acc. (32Hz), BVP (64Hz), EDA (4Hz), heart rate"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "(1Hz), IBI (n/a), body temperature (4Hz), EEG (8 band,"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "32Hz), ECG (2Hz)"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "Dataset contents"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "172.92 min (from 16 debate sessions)"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "223.35 min (from 21 participants)"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "Refer to Dataset contents subsection"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "Self: 4,159"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "Partner: 4,159"
        },
        {
          "Table 7: Summary of data collection results and the dataset.": "5 external observers: 20,803"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "cates a ﬁle without any missing value or an outlier. The completeness ratio was calculated as completeness"
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "= (total number of values - (number of outliers + number of zeros)) / total number of values."
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "7. neuro_polar_completeness.csv – same as above, with completeness calculated as completeness ="
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "(total number of values - number of zeros) / total number of values."
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "debate_audios.tar.gz:\ncontains 16 audio recordings of debates in the WAV ﬁle format. The name of each ﬁle follows"
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "the convention of p<X>.p<Y>.wav, where <X> and <Y> stand for IDs of two participants appearing in the audio. The"
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "start and the end of each recording correspond to startTime and endTime values in the subjects.csv ﬁle, respectively."
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "debate_recordings.tar.gz:\ncontains video recordings of 21 participants during debates in the MP4 ﬁle format. The"
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "name of a ﬁle p<X>_<T>.mp4 indicates that the ﬁle is the recording of participant <X> that is <T> seconds long."
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "neurosky_polar_data.tar.gz:\nincludes subdirectories for each participant, from P1 to P32, which may contain up to"
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "four ﬁles as the following:"
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "1. Attention.csv – contains eSense Attention ranging from 1 to 100, representing how attentive a user was at a"
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "given moment. Attention values can be interpreted as the following: 1 to 20 – “strongly lowered”, 20 to 40 –"
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "“reduced”, 40 to 60 – “neutral”, 60 to 80 – “slightly elevated”, and 80 to 100 – “elevated”. 0 indicates that the"
        },
        {
          "6. e4_completeness.csv – contains the completeness of each ﬁle as a ratio in the range of [0.0, 1.0]. 1.0 indi-": "device was unable to calculate a sufﬁciently reliable value, possibly due to a signal contamination with noises."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": ""
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "2. partner_annotations – annotations of participant emotions by respective debate partners."
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "raters."
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "naming convention of P<X>.R<Z>.csv, where <X> is a participant ID, and <Z> is a rater number."
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "4. aggregated_external_annotations – contains external rater annotations aggregated across ﬁve raters"
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "via majority voting. Refer to Code Availability section for the Python code implementing the majority vote"
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": ""
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "The ﬁrst row in a valid ﬁle has annotations for the ﬁrst ﬁve seconds, and rows coming afterward contain annotations"
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "for the next consecutive ﬁve-second intervals, non-overlapping with each other. Also, each row in a valid ﬁle contains"
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": ""
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "a participant may not have an equal number of rows (e.g., there may be more self-annotations than partner/external"
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "longer ﬁles should be truncated from the start such that"
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": "the same number of rows as shorter ﬁles since the extra annotations at the beginning are possibly from participants"
        },
        {
          "pant emotions during debates at intervals of every 5 seconds, acquired from three distinct perspectives:": ""
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "external raters (E), with external annotations aggregated by majority voting. Annotations were summed across 32"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "subjects for each emotion and affective categories. Means and standard deviations measured respectively from three"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "perspectives are shown on the upper right corner of ﬁgures if available."
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "Therefore, we applied the proposed mode-subtraction to emotion annotations such that alpha coefﬁcients measure"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "raters’ agreement on relative changes in emotions rather than their absolute agreement with each other. This adjustment"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "mitigates spuriously low alpha coefﬁcient values obtained from raw annotations (refer to Code Availability section for"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "the code implementing the mode-subtraction and plotting of heatmaps)."
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "These ﬁxed alpha coefﬁcients are low in general.\nIn particular, a noticeable pattern emerges when comparing alpha"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "coefﬁcients of self-partner (SP) annotations and self-external (SE) annotations. As shown in the last rows of heatmaps"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "(Diff. [SE - SP]) in Fig. 4, the differences between the IRRs of SE annotations and SP annotations tend to be above zero"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "(for 20 out of 32 participants for arousal: mean = 0.143, stdev. = 0.322). This pattern possibly indicates that there exists"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "a meaningful difference in the perception of emotions from different perspectives, while further study is required to"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "validate its signiﬁcance."
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "4.2\nPhysiological signals"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "Data quality\nThe quality of physiological signal measurements in the dataset has been thoroughly examined. The"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "examination results are included as a part of the dataset in the data_quality_tables.tar.gz archive ﬁle."
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "collection. While physiological signals in the dataset are mostly error-free with most of the ﬁles complete above 95%, a"
        },
        {
          "Figure 3: Distributions and frequencies of emotion annotations from three perspectives of self (S), partner (P), and": "portion of data is missing due to issues inherent to devices or a human error:"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "collection. While physiological signals in the dataset are mostly error-free with most of the ﬁles complete above 95%, a"
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "portion of data is missing due to issues inherent to devices or a human error:"
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "•\nIBI – data from P26 is missing as the internal algorithm of E4 that derives IBI from BVP automatically discards"
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "an obtained value if its reliability is below a certain threshold."
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "• EDA – data from P17 and P20 is missing, possibly due to poor contact between the device and a participant’s"
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "skin."
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "• NeuroSky (Attention, Meditation) – measurements from P1 and P20 are missing due to a poorly equipped"
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "device. A portion of data is missing for P19 ( 32%), P22 ( 59%) and P23 ( 36%) for the same reason. No"
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "BrainWave data was lost."
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "• Polar HR – data from seven participants (P3, P12, P18, P20, P21, P29, and P30) are missing due to a device"
        },
        {
          "Missing data\nE4 data of 4 participants (P2, P3, P6, and P7) were excluded due to a device malfunction during data": "error during data collection. Parts of data are missing from P4 ( 38%) and P22 ( 38%) due to poor contact."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 4: Heatmaps of inter-rater reliabilities measured with Krippendorff’s alpha. External annotations were aggregated": "by majority voting. The ﬁrst 4 rows of each heatmap show alpha coefﬁcients across four different combinations of"
        },
        {
          "Figure 4: Heatmaps of inter-rater reliabilities measured with Krippendorff’s alpha. External annotations were aggregated": "(1) SP = self vs. partner, (2) SE = self vs. external, (3) PE = partner vs. external, and (4)"
        },
        {
          "Figure 4: Heatmaps of inter-rater reliabilities measured with Krippendorff’s alpha. External annotations were aggregated": ""
        },
        {
          "Figure 4: Heatmaps of inter-rater reliabilities measured with Krippendorff’s alpha. External annotations were aggregated": ""
        },
        {
          "Figure 4: Heatmaps of inter-rater reliabilities measured with Krippendorff’s alpha. External annotations were aggregated": "14"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "5\nUsage Notes": "5.1\nPotential applications"
        },
        {
          "5\nUsage Notes": "In addition to the intended usage of the dataset discussed above, there are uncertainties as to how physiological markers"
        },
        {
          "5\nUsage Notes": "of an individual’s capacity for ﬂexible physiological reactivity relate to experiences of positive and negative emotions."
        },
        {
          "5\nUsage Notes": "Our dataset could potentially be useful\nto examine the role of physiological signal based markers in assessing an"
        },
        {
          "5\nUsage Notes": "individual’s use of emotion regulation strategies, such as cognitive appraisal."
        },
        {
          "5\nUsage Notes": "Additionally, various data mining and machine learning techniques could be applied to set up the models for an"
        },
        {
          "5\nUsage Notes": "individual’s emotional proﬁles based on sensor-based physiological and behavioral recordings. This could further be"
        },
        {
          "5\nUsage Notes": "transferred to other use-cases, such as helping children with autism in their social communication [88, 89], helping"
        },
        {
          "5\nUsage Notes": "people who are blind to read facial expressions and get the emotion information of their peers [90], helping robots"
        },
        {
          "5\nUsage Notes": "interact more intelligently with people [91, 92], and monitoring signs of frustration and emotional saturation that affect"
        },
        {
          "5\nUsage Notes": "attention while driving, to enhance driver safety [93, 94]."
        },
        {
          "5\nUsage Notes": "5.2\nLimitations"
        },
        {
          "5\nUsage Notes": "Data collection apparatus\nContact-base EEG sensors are known to be susceptible to noises, for example, frowning"
        },
        {
          "5\nUsage Notes": "or eyes-movement might have caused peaks in the data. Other devices may also have been subject to similar systematic"
        },
        {
          "5\nUsage Notes": "errors."
        },
        {
          "5\nUsage Notes": "Data collection context\nThe context of\nthe turn-taking debate may have caused participants to regulate or even"
        },
        {
          "5\nUsage Notes": "suppress their emotional expressions, as an unrestrained display of emotions is often regarded undesirable during a"
        },
        {
          "5\nUsage Notes": "debate. This may have contributed to a deﬂated level of agreement between self-reports and partner/external perceptions"
        },
        {
          "5\nUsage Notes": "of emotions, which may not be a case for more natural interactions in the wild."
        },
        {
          "5\nUsage Notes": "Demographics\nThe participant demographics likely have introduced bias in the data. All of our participants and raters"
        },
        {
          "5\nUsage Notes": "are young (their ages were between 19 to 36) and highly-educated, and the majority of them are individuals of Asian"
        },
        {
          "5\nUsage Notes": "ethnicity. Therefore, our data may not generalize well to individuals of different ethnic groups or of younger or older"
        },
        {
          "5\nUsage Notes": "age groups."
        },
        {
          "5\nUsage Notes": "Unaccounted variables\nMany variables unaccounted during data collection, such as the level of rapport between"
        },
        {
          "5\nUsage Notes": "debating pairs, a participant’s competence in spoken English, and a participant’s familiarity with the debate topic,"
        },
        {
          "5\nUsage Notes": "may also have contributed to a variance in the level of mismatch between the perceptions of emotions across different"
        },
        {
          "5\nUsage Notes": "perspectives."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[9] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "deep neural networks and tree search. nature, 529(7587):484, 2016."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[10] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "Nature, 550(7676):354–359, 2017."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "television, and new\n[11] Byron Reeves and Clifford Ivar Nass. The media equation: How people treat computers,"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "media like real people and places. Cambridge university press, 1996."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[12] Aaron Turpen.\nMit wants\nself-driving cars\nto trafﬁc\nin human emotion.\nNew Atlas,\n2019.\nhttps:"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "//newatlas.com/automotive/mit-self-driving-cars-human-emotion/."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[13] Lisa Feldman Barrett. How emotions are made: The secret life of the brain. Houghton Mifﬂin Harcourt, 2017."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[14] Shichuan Du, Yong Tao, and Aleix M Martinez. Compound facial expressions of emotion. Proceedings of the"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "National Academy of Sciences, 111(15):E1454–E1462, 2014."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[15] Georgios N Yannakakis, Roddy Cowie, and Carlos Busso. The ordinal nature of emotions.\nIn 2017 Seventh"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "International Conference on Affective Computing and Intelligent Interaction (ACII), pages 248–255. IEEE, 2017."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[16] Mark G Frank and Elena Svetieva. Microexpressions and deception.\nIn Understanding facial expressions in"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "communication, pages 227–242. Springer, 2015."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[17] Lisa Feldman Barrett, Ralph Adolphs, Stacy Marsella, Aleix M Martinez, and Seth D Pollak.\nEmotional"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "expressions reconsidered: challenges to inferring emotion from human facial movements. Psychological Science"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "in the Public Interest, 20(1):1–68, 2019."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "James M Carroll and James A Russell. Do facial expressions signal speciﬁc emotions? judging emotion from the\n[18]"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "face in context. Journal of personality and social psychology, 70(2):205, 1996."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[19] Richard T Cauldwell. Where did the anger go? the role of context in interpreting emotion in speech.\nIn ISCA"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "Tutorial and Research Workshop (ITRW) on Speech and Emotion, 2000."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[20] Lisa Feldman Barrett, Batja Mesquita, and Maria Gendron. Context in emotion perception. Current Directions in"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "Psychological Science, 20(5):286–290, 2011."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[21] Randy J Larsen and Ed Diener. Affect intensity as an individual difference characteristic: A review. Journal of"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "Research in personality, 21(1):1–39, 1987."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[22]\nJames J Gross and Oliver P John.\nIndividual differences in two emotion regulation processes:\nimplications for"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "affect, relationships, and well-being. Journal of personality and social psychology, 85(2):348, 2003."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[23] Mohammad Soleymani, Jeroen Lichtenauer, Thierry Pun, and Maja Pantic. A multimodal database for affect"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "recognition and implicit tagging.\nIEEE transactions on affective computing, 3(1):42–55, 2011."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[24] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi,"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis; using physiological"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "signals.\nIEEE transactions on affective computing, 3(1):18–31, 2011."
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "[25] Mojtaba Khomami Abadi, Ramanathan Subramanian, Seyed Mostafa Kia, Paolo Avesani, Ioannis Patras, and Nicu"
        },
        {
          "general intelligence, pages 1–30. Springer, 2007.": "IEEE Transactions\nSebe. Decaf: Meg-based multimodal database for decoding affective physiological responses."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "tions. Basic Books, 1997."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[4] Paulo N Lopes, Marc A Brackett, John B Nezlek, Astrid Schütz,\nIna Sellin, and Peter Salovey.\nEmotional"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "intelligence and social interaction. Personality and social psychology bulletin, 30(8):1018–1034, 2004."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[5] Andre Esteva, Brett Kuprel, Roberto A Novoa, Justin Ko, Susan M Swetter, Helen M Blau, and Sebastian Thrun."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "Dermatologist-level classiﬁcation of skin cancer with deep neural networks. Nature, 542(7639):115–118, 2017."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[6] Rafail-Evangelos Mastoras, Dimitrios Iakovakis, Stelios Hadjidimitriou, Vasileios Charisis, Seada Kassie, Taouﬁk"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "Alsaadi, Ahsan Khandoker, and Leontios J Hadjileontiadis.\nTouchscreen typing pattern analysis for remote"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "detection of the depressive tendency. Scientiﬁc reports, 9(1):1–12, 2019."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "40+ corporations working on autonomous vehicles.\n2020.\n[7] CB Insights.\nhttps://www.cbinsights.com/"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "research/autonomous-driverless-vehicles-corporations-list/."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[8] Cassio Pennachin and Ben Goertzel. Contemporary approaches to artiﬁcial general\nintelligence.\nIn Artiﬁcial"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "general intelligence, pages 1–30. Springer, 2007."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[9] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of go with"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "deep neural networks and tree search. nature, 529(7587):484, 2016."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[10] David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, et al. Mastering the game of go without human knowledge."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "Nature, 550(7676):354–359, 2017."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "television, and new\n[11] Byron Reeves and Clifford Ivar Nass. The media equation: How people treat computers,"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "media like real people and places. Cambridge university press, 1996."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[12] Aaron Turpen.\nMit wants\nself-driving cars\nto trafﬁc\nin human emotion.\nNew Atlas,\n2019.\nhttps:"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "//newatlas.com/automotive/mit-self-driving-cars-human-emotion/."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[13] Lisa Feldman Barrett. How emotions are made: The secret life of the brain. Houghton Mifﬂin Harcourt, 2017."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[14] Shichuan Du, Yong Tao, and Aleix M Martinez. Compound facial expressions of emotion. Proceedings of the"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "National Academy of Sciences, 111(15):E1454–E1462, 2014."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[15] Georgios N Yannakakis, Roddy Cowie, and Carlos Busso. The ordinal nature of emotions.\nIn 2017 Seventh"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "International Conference on Affective Computing and Intelligent Interaction (ACII), pages 248–255. IEEE, 2017."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[16] Mark G Frank and Elena Svetieva. Microexpressions and deception.\nIn Understanding facial expressions in"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "communication, pages 227–242. Springer, 2015."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[17] Lisa Feldman Barrett, Ralph Adolphs, Stacy Marsella, Aleix M Martinez, and Seth D Pollak.\nEmotional"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "expressions reconsidered: challenges to inferring emotion from human facial movements. Psychological Science"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "in the Public Interest, 20(1):1–68, 2019."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "James M Carroll and James A Russell. Do facial expressions signal speciﬁc emotions? judging emotion from the\n[18]"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "face in context. Journal of personality and social psychology, 70(2):205, 1996."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[19] Richard T Cauldwell. Where did the anger go? the role of context in interpreting emotion in speech.\nIn ISCA"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "Tutorial and Research Workshop (ITRW) on Speech and Emotion, 2000."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[20] Lisa Feldman Barrett, Batja Mesquita, and Maria Gendron. Context in emotion perception. Current Directions in"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "Psychological Science, 20(5):286–290, 2011."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[21] Randy J Larsen and Ed Diener. Affect intensity as an individual difference characteristic: A review. Journal of"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "Research in personality, 21(1):1–39, 1987."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[22]\nJames J Gross and Oliver P John.\nIndividual differences in two emotion regulation processes:\nimplications for"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "affect, relationships, and well-being. Journal of personality and social psychology, 85(2):348, 2003."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[23] Mohammad Soleymani, Jeroen Lichtenauer, Thierry Pun, and Maja Pantic. A multimodal database for affect"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "recognition and implicit tagging.\nIEEE transactions on affective computing, 3(1):42–55, 2011."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[24] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi,"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis; using physiological"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "signals.\nIEEE transactions on affective computing, 3(1):18–31, 2011."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "[25] Mojtaba Khomami Abadi, Ramanathan Subramanian, Seyed Mostafa Kia, Paolo Avesani, Ioannis Patras, and Nicu"
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "IEEE Transactions\nSebe. Decaf: Meg-based multimodal database for decoding affective physiological responses."
        },
        {
          "[3] Peter Ed Salovey and David J Sluyter. Emotional development and emotional intelligence: Educational implica-": "on Affective Computing, 6(3):209–222, 2015."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "IEEE Transactions on Affective\nSebe. Ascertain: Emotion and personality recognition using commercial sensors."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "Computing, 9(2):147–160, 2016."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[27] Stamos Katsigiannis and Naeem Ramzan.\nDreamer: A database for emotion recognition through eeg and"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "ecg signals from wireless low-cost off-the-shelf devices.\nIEEE journal of biomedical and health informatics,"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "22(1):98–107, 2017."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[28]\nJuan Abdon Miranda Correa, Mojtaba Khomami Abadi, Niculae Sebe, and Ioannis Patras. Amigos: A dataset"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "for affect, personality and mood research on individuals and groups.\nIEEE Transactions on Affective Computing,"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "2018."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[29] Karan Sharma, Claudio Castellini, Egon L van den Broek, Alin Albu-Schaeffer, and Friedhelm Schwenker. A"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "dataset of continuous affect annotations and physiological signals for emotion analysis. Scientiﬁc data, 6(1):1–13,"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "2019."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[30] Wen-Jing Yan, Qi Wu, Yong-Jin Liu, Su-Jing Wang, and Xiaolan Fu. Casme database: a dataset of spontaneous"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "micro-expressions collected from neutralized faces.\nIn 2013 10th IEEE international conference and workshops"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "on automatic face and gesture recognition (FG), pages 1–7. IEEE, 2013."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[31] Philip Schmidt, Attila Reiss, Robert Duerichen, Claus Marberger, and Kristof Van Laerhoven.\nIntroducing wesad,"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "the 20th ACM International\na multimodal dataset for wearable stress and affect detection.\nIn Proceedings of"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "Conference on Multimodal Interaction, pages 400–408, 2018."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[32] David Watson. Mood and temperament. Guilford Press, 2000."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[33] Anton Batliner, Kerstin Fischer, Richard Huber, Jörg Spilker, and Elmar Nöth. How to ﬁnd trouble in communica-"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "tion. Speech communication, 40(1-2):117–143, 2003."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[34]\nJoseph Henrich, Steven J Heine, and Ara Norenzayan. The weirdest people in the world? Behavioral and brain"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "sciences, 33(2-3):61–83, 2010."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[35] Abhinav Dhall, Roland Goecke, Simon Lucey, and Tom Gedeon.\nCollecting large,\nrichly annotated facial-"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "expression databases from movies.\nIEEE multimedia, (3):34–41, 2012."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[36] Ali Mollahosseini, Behzad Hasani, and Mohammad H Mahoor. Affectnet: A database for facial expression,"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "valence, and arousal computing in the wild.\nIEEE Transactions on Affective Computing, 10(1):18–31, 2017."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[37] Daniel McDuff, May Amr, and Rana El Kaliouby. Am-fed+: An extended dataset of naturalistic facial expressions"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "collected in everyday settings.\nIEEE Transactions on Affective Computing, 10(1):7–17, 2018."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[38] Soujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "Meld: A multimodal multi-party dataset for emotion recognition in conversations.\nIn Proceedings of the 57th"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "Annual Meeting of the Association for Computational Linguistics, pages 527–536, 2019."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[39] Daniel McDuff, Rana El Kaliouby, and Rosalind W Picard. Crowdsourcing facial responses to online videos."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "IEEE Transactions on Affective Computing, 3(4):456–468, 2012."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[40] R Morris, Daniel McDuff, and R Calvo. Crowdsourcing techniques for affective computing.\nIn The Oxford"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "handbook of affective computing, pages 384–394. Oxford Univ. Press, 2014."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[41] Olga Korovina, Marcos Baez, and Fabio Casati. Reliability of crowdsourcing as a method for collecting emotions"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "labels on pictures. BMC research notes, 12(1):1–6, 2019."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[42] Michael T Motley and Carl T Camden. Facial expression of emotion: A comparison of posed expressions versus"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "spontaneous expressions in an interpersonal communication setting. Western Journal of Communication (includes"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "Communication Reports), 52(1):1–22, 1988."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[43] Rebecca Jürgens, Annika Grass, Matthis Drolet, and Julia Fischer.\nEffect of acting experience on emotion"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "Journal of nonverbal\nexpression and recognition in voice: Non-actors provide better stimuli\nthan expected."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "behavior, 39(3):195–214, 2015."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[44] Patrik N Juslin, Petri Laukka, and Tanja Bänziger. The mirror to our soul? comparisons of spontaneous and posed"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "vocal expression of emotion. Journal of nonverbal behavior, 42(1):1–40, 2018."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[45]\nJohn T Cacioppo, Gary G Berntson, Jeff T Larsen, Kirsten M Poehlmann, Tiffany A Ito, et al. The psychophysiol-"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "ogy of emotion. Handbook of emotions, 2:173–191, 2000."
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "[46] Rosalind W. Picard, Elias Vyzas, and Jennifer Healey. Toward machine emotional\nintelligence: Analysis of"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "affective physiological state.\nIEEE transactions on pattern analysis and machine intelligence, 23(10):1175–1191,"
        },
        {
          "[26] Ramanathan Subramanian, Julia Wache, Mojtaba Khomami Abadi, Radu L Vieriu, Stefan Winkler, and Nicu": "2001."
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "from physiological signals. EURASIP Journal on Advances in Signal Processing, 2004(11):929414, 2004."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[48] Pierre Rainville, Antoine Bechara, Nasir Naqvi, and Antonio R Damasio. Basic emotions are associated with"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "distinct patterns of cardiorespiratory activity.\nInternational journal of psychophysiology, 61(1):5–18, 2006."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[49] Lauri Nummenmaa, Enrico Glerean, Riitta Hari, and Jari K Hietanen. Bodily maps of emotions. Proceedings of"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "the National Academy of Sciences, 111(2):646–651, 2014."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[50] Edward F Pace-Schott, Marlissa C Amole, Tatjana Aue, Michela Balconi, Lauren M Bylsma, Hugo Critchley,"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Heath A Demaree, Bruce H Friedman, Anne Elizabeth Kotynski Gooding, Olivia Gosseries, et al. Physiological"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "feelings. Neuroscience & Biobehavioral Reviews, 2019."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[51] Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N Chang,"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Sungbok Lee, and Shrikanth S Narayanan.\nIemocap:\nInteractive emotional dyadic motion capture database."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Language resources and evaluation, 42(4):335, 2008."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[52] Gary McKeown, Michel Valstar, Roddy Cowie, Maja Pantic, and Marc Schroder. The semaine database: Annotated"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "multimodal records of emotionally colored conversations between a person and a limited agent. IEEE transactions"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "on affective computing, 3(1):5–17, 2011."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[53] Carlos Busso, Srinivas Parthasarathy, Alec Burmania, Mohammed AbdelWahab, Najmeh Sadoughi,\nand"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Emily Mower Provost. Msp-improv: An acted corpus of dyadic interactions to study emotion perception."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "IEEE Transactions on Affective Computing, 8(1):67–80, 2016."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Jennifer Healey. Recording affect in the ﬁeld:\ntowards methods and metrics for improving ground truth labels.\nIn\n[54]"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "International conference on affective computing and intelligent interaction, pages 107–116. Springer, 2011."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[55] Biqiao Zhang, Georg Essl, and Emily Mower Provost. Automatic recognition of self-reported and perceived"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "emotion: Does joint modeling help? In Proceedings of the 18th ACM International Conference on Multimodal"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Interaction, pages 217–224, 2016."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[56] Khiet P Truong, David A van Leeuwen, and Mark A Neerincx. Unobtrusive multimodal emotion detection in"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "adaptive interfaces: speech and facial expressions.\nIn International Conference on Foundations of Augmented"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Cognition, pages 354–363. Springer, 2007."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "James B Grossman, Ami Klin, Alice S Carter, and Fred R Volkmar. Verbal bias in recognition of facial emotions\n[57]"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "in children with asperger syndrome. The Journal of Child Psychology and Psychiatry and Allied Disciplines,"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "41(3):369–379, 2000."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[58] Hannah Dickson, Monica E Calkins, Christian G Kohler, Sheilagh Hodgins, and Kristin R Laurens. Misperceptions"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "of facial emotions among youth aged 9–14 years who present multiple antecedents of schizophrenia. Schizophrenia"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "bulletin, 40(2):460–468, 2014."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[59] Khiet P Truong, David A Van Leeuwen, and Franciska MG De Jong. Speech-based recognition of self-reported"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "and observed emotion in a dimensional space. Speech communication, 54(9):1049–1063, 2012."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[60] Ursula Hess, Sylvie Blairy, and Robert E Kleck. The intensity of emotional facial expressions and decoding"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "accuracy. Journal of Nonverbal Behavior, 21(4):241–257, 1997."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[61] Hiranmayi Ranganathan, Shayok Chakraborty, and Sethuraman Panchanathan. Multimodal emotion recognition"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "using deep learning architectures.\nIn 2016 IEEE Winter Conference on Applications of Computer Vision (WACV),"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "pages 1–9. IEEE, 2016."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[62] Hyeryung Christine Min and Tek-Jin Nam. Biosignal sharing for affective connectedness.\nIn CHI’14 Extended"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Abstracts on Human Factors in Computing Systems, pages 2191–2196. 2014."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[63] Mariam Hassib, Daniel Buschek, Paweł W Wozniak, and Florian Alt. Heartchat: Heart rate augmented mobile"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "the 2017 CHI Conference on Human Factors in\nchat\nto support empathy and awareness.\nIn Proceedings of"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Computing Systems, pages 2239–2251, 2017."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[64] Fannie Liu, Laura Dabbish, and Geoff Kaufman. Supporting social\ninteractions with an expressive heart rate"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "sharing application. Proceedings of\nthe ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies,"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "1(3):1–26, 2017."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[65] Fannie Liu, Mario Esparza, Maria Pavlovskaia, Geoff Kaufman, Laura Dabbish, and Andrés Monroy-Hernández."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "the ACM on\nAnimo: Sharing biosignals on a smartwatch for lightweight social connection. Proceedings of"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "Interactive, Mobile, Wearable and Ubiquitous Technologies, 3(1):1–19, 2019."
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "[66] Fannie Liu, Geoff Kaufman, and Laura Dabbish. The effect of expressive biosignals on empathy and closeness for"
        },
        {
          "[47] Christine Lætitia Lisetti and Fatma Nasoz. Using noninvasive wearable computers to recognize human emotions": "a stigmatized group member. Proceedings of the ACM on Human-Computer Interaction, 3(CSCW):1–17, 2019."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "Hill, 2018. https://thehill.com/opinion/international/395977-south-koreas-refugee-debate-"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "eclipses-a-deeper-more-fundamental-question."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[68]\nJin-kyu Kang.\nYemeni\nrefugees become a major\nissue on jeju.\nKorea JoongAng Daily, 2018.\nhttp://"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "koreajoongangdaily.joins.com/news/article/article.aspx?aid=3049562."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "South korea is going crazy over a handful of\nrefugees.\nForeign Policy, 2018.\n[69] Nathan Park.\nhttps://"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "foreignpolicy.com/2018/08/06/south-korea-is-going-crazy-over-a-handful-of-refugees/."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[70] Bo Seo. In south korea, opposition to yemeni refugees is a cry for help. CNN, 2018. https://edition.cnn.com/"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "2018/09/13/opinions/south-korea-jeju-yemenis-intl/index.html."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "Instructions matter: a\n[71] Kersten Diers, Fanny Weber, Burkhard Brocke, Alexander Strobel, and Sabine Schönfeld."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "comparison of baseline conditions for cognitive emotion regulation paradigms. Frontiers in psychology, 5:347,"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "2014."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[72]\nJames J Gross and Robert W Levenson. Emotion elicitation using ﬁlms. Cognition & emotion, 9(1):87–108, 1995."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[73] Susan Kemper and Aaron Sumner. The structure of verbal abilities in young and older adults. Psychology and"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "aging, 16(2):312, 2001."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[74]\nJiahong Yuan, Mark Liberman, and Christopher Cieri. Towards an integrated understanding of speaking rate in"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "conversation.\nIn Ninth International Conference on Spoken Language Processing, 2006."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[75] Cheryl Smith Gabig. Mean length of utterance (mlu).\nEncyclopedia of autism spectrum disorders, pages"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "1813–1814, 2013."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[76] AC Graesser, Bethany McDaniel, Patrick Chipman, Amy Witherspoon, Sidney D’Mello, and Barry Gholson."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "Detection of emotions during learning with autotutor.\nIn Proceedings of the 28th annual meetings of the cognitive"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "science society, pages 285–290. Citeseer, 2006."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[77] Shazia Afzal and Peter Robinson. Natural affect data—collection & annotation in a learning context.\nIn 2009 3rd"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "International Conference on Affective Computing and Intelligent Interaction and Workshops, pages 1–7. IEEE,"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "2009."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[78] Sidney K D’Mello, Blair Lehman, and Natalie Person. Monitoring affect states during effortful problem solving"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "activities.\nInternational Journal of Artiﬁcial Intelligence in Education, 20(4):361–389, 2010."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[79] Sidney K D’Mello. On the inﬂuence of an iterative affect annotation approach on inter-observer and self-observer"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "reliability.\nIEEE Transactions on Affective Computing, 7(2):136–149, 2015."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[80] Linda J Levine and Martin A Safer. Sources of bias in memory for emotions. Current Directions in Psychological"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "Science, 11(5):169–173, 2002."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[81] Martin A Safer, Linda J Levine, and Amy L Drapalski. Distortion in memory for emotions: The contributions of"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "personality and post-event knowledge. Personality and Social Psychology Bulletin, 28(11):1495–1507, 2002."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[82] Heather C Lench and Linda J Levine. Motivational biases in memory for emotions. Cognition and emotion,"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "24(3):401–418, 2010."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[83] Cheul Young Park, Narae Cha, Soowon Kang, Auk Kim, Ahsan Habib Khandoker, Leontios Hadjileontiadis, Alice"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "Oh, Yong Jeong, and Uichin Lee. K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "naturalistic conversations, April 2020."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[84] Ricardo A Calix, Sri Abhishikth Mallepudi, Bin Chen, and Gerald M Knapp. Emotion recognition in text for 3-d"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "facial expression rendering.\nIEEE Transactions on Multimedia, 12(6):544–551, 2010."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[85] Wenbo Wang, Lu Chen, Krishnaprasad Thirunarayan, and Amit P Sheth. Harnessing twitter\" big data\" for"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "automatic emotion identiﬁcation.\nIn 2012 International Conference on Privacy, Security, Risk and Trust and 2012"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "International Confernece on Social Computing, pages 587–592. IEEE, 2012."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[86] Ruifeng Xu, Tao Chen, Yunqing Xia, Qin Lu, Bin Liu, and Xuan Wang. Word embedding composition for data"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "imbalances in sentiment and emotion classiﬁcation. Cognitive Computation, 7(2):226–240, 2015."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[87] Klaus Krippendorff. Computing krippendorff’s alpha-reliability. 2011."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[88] Rosalind W Picard. Future affective technology for autism and emotion communication. Philosophical Transac-"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "tions of the Royal Society B: Biological Sciences, 364(1535):3575–3584, 2009."
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "[89] Peter Washington, Catalin Voss, Aaron Kline, Nick Haber, Jena Daniels, Azar Fazel, Titas De, Carl Feinstein,"
        },
        {
          "The\n[67] Soo Kim.\nSouth\nkorea’s\nrefugee\ndebate\neclipses\na\ndeeper,\nmore\nfundamental\nquestion.": "Terry Winograd, and Dennis Wall. Superpowerglass: a wearable aid for the at-home therapy of children with"
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "Van Wezel, and Yan Zhao. Conveying facial expressions to blind and visually impaired persons through a wearable"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "vibrotactile device. PloS one, 13(3), 2018."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "[91] Cynthia Breazeal. Emotion and sociable humanoid robots.\nInternational journal of human-computer studies,"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "59(1-2):119–155, 2003."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "[92] Dong-Soo Kwon, Yoon Keun Kwak, Jong C Park, Myung Jin Chung, Eun-Sook Jee, Kyung-Sook Park, Hyoung-"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "Rock Kim, Young-Min Kim, Jong-Chan Park, Eun Ho Kim, et al. Emotion interaction system for a service robot."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "In RO-MAN 2007-The 16th IEEE International Symposium on Robot and Human Interactive Communication,"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "pages 351–356. IEEE, 2007."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "[93] Clifford Nass,\nIng-Marie Jonsson, Helen Harris, Ben Reaves, Jack Endo, Scott Brave, and Leila Takayama."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "Improving automotive safety by pairing driver emotion and car voice emotion.\nIn CHI’05 extended abstracts on"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "Human factors in computing systems, pages 1973–1976, 2005."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "[94] Florian Eyben, Martin Wöllmer, Tony Poitschke, Björn Schuller, Christoph Blaschke, Berthold Färber, and Nhu"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "Nguyen-Thien. Emotion on the road—necessity, acceptance, and feasibility of affective computing in the car."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "Advances in human-computer interaction, 2010, 2010."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "[95] Valentina Markova, Todor Ganchev, and Kalin Kalinkov. Clas: A database for cognitive load, affect and stress"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "recognition.\nIn 2019 International Conference on Biomedical Innovations and Applications (BIA), pages 1–4."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "IEEE, 2019."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "[96]\nJames A Russell. A circumplex model of affect. Journal of personality and social psychology, 39(6):1161, 1980."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "[97] Kurt Plarre, Andrew Raij, Syed Monowar Hossain, Amin Ahsan Ali, Motohiro Nakajima, Mustafa Al’Absi, Emre"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "Ertin, Thomas Kamarck, Santosh Kumar, Marcia Scott, et al. Continuous inference of psychological stress from"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "sensory measurements collected in the natural environment.\nIn Proceedings of the 10th ACM/IEEE international"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "conference on information processing in sensor networks, pages 97–108. IEEE, 2011."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "[98]\nJaclyn Ocumpaugh. Baker rodrigo ocumpaugh monitoring protocol (bromp) 2.0 technical and training manual."
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "New York, NY and Manila, Philippines: Teachers College, Columbia University and Ateneo Laboratory for the"
        },
        {
          "[90] Hendrik P Buimer, Marian Bittner, Tjerk Kostelijk, Thea M Van Der Geest, Abdellatif Nemri, Richard JA": "Learning Sciences, 60, 2015."
        }
      ],
      "page": 20
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotional intelligence. Imagination, cognition and personality",
      "authors": [
        "Peter Salovey",
        "John Mayer"
      ],
      "year": "1990",
      "venue": "Emotional intelligence. Imagination, cognition and personality"
    },
    {
      "citation_id": "2",
      "title": "Emotional intelligence meets traditional standards for an intelligence",
      "authors": [
        "David John D Mayer",
        "Peter Caruso",
        "Salovey"
      ],
      "year": "1999",
      "venue": "Intelligence"
    },
    {
      "citation_id": "3",
      "title": "Emotional development and emotional intelligence: Educational implications",
      "authors": [
        "Ed Peter",
        "David Salovey",
        "Sluyter"
      ],
      "year": "1997",
      "venue": "Emotional development and emotional intelligence: Educational implications"
    },
    {
      "citation_id": "4",
      "title": "Emotional intelligence and social interaction",
      "authors": [
        "Paulo Lopes",
        "Marc Brackett",
        "John Nezlek",
        "Astrid Schütz",
        "Ina Sellin",
        "Peter Salovey"
      ],
      "year": "2004",
      "venue": "Personality and social psychology bulletin"
    },
    {
      "citation_id": "5",
      "title": "Dermatologist-level classification of skin cancer with deep neural networks",
      "authors": [
        "Andre Esteva",
        "Brett Kuprel",
        "Roberto Novoa",
        "Justin Ko",
        "Susan Swetter",
        "Helen Blau",
        "Sebastian Thrun"
      ],
      "year": "2017",
      "venue": "Nature"
    },
    {
      "citation_id": "6",
      "title": "Touchscreen typing pattern analysis for remote detection of the depressive tendency",
      "authors": [
        "Rafail-Evangelos Mastoras",
        "Dimitrios Iakovakis",
        "Stelios Hadjidimitriou",
        "Vasileios Charisis",
        "Seada Kassie",
        "Taoufik Alsaadi",
        "Ahsan Khandoker",
        "Leontios Hadjileontiadis"
      ],
      "year": "2019",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "7",
      "title": "40+ corporations working on autonomous vehicles",
      "year": "2020",
      "venue": "40+ corporations working on autonomous vehicles"
    },
    {
      "citation_id": "8",
      "title": "Contemporary approaches to artificial general intelligence",
      "authors": [
        "Cassio Pennachin",
        "Ben Goertzel"
      ],
      "year": "2007",
      "venue": "Artificial general intelligence"
    },
    {
      "citation_id": "9",
      "title": "Mastering the game of go with deep neural networks and tree search",
      "authors": [
        "David Silver",
        "Aja Huang",
        "Chris Maddison",
        "Arthur Guez",
        "Laurent Sifre",
        "George Van Den",
        "Julian Driessche",
        "Ioannis Schrittwieser",
        "Veda Antonoglou",
        "Marc Panneershelvam",
        "Lanctot"
      ],
      "year": "2016",
      "venue": "nature"
    },
    {
      "citation_id": "10",
      "title": "Mastering the game of go without human knowledge",
      "authors": [
        "David Silver",
        "Julian Schrittwieser",
        "Karen Simonyan",
        "Ioannis Antonoglou",
        "Aja Huang",
        "Arthur Guez",
        "Thomas Hubert",
        "Lucas Baker",
        "Matthew Lai",
        "Adrian Bolton"
      ],
      "year": "2017",
      "venue": "Nature"
    },
    {
      "citation_id": "11",
      "title": "The media equation: How people treat computers, television, and new media like real people and places",
      "authors": [
        "Byron Reeves",
        "Clifford Nass"
      ],
      "year": "1996",
      "venue": "The media equation: How people treat computers, television, and new media like real people and places"
    },
    {
      "citation_id": "12",
      "title": "Mit wants self-driving cars to traffic in human emotion",
      "authors": [
        "Aaron Turpen"
      ],
      "year": "2019",
      "venue": "Mit wants self-driving cars to traffic in human emotion"
    },
    {
      "citation_id": "13",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "Lisa Feldman"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "14",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "Shichuan Du",
        "Yong Tao",
        "Aleix Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "15",
      "title": "The ordinal nature of emotions",
      "authors": [
        "Roddy Georgios N Yannakakis",
        "Carlos Cowie",
        "Busso"
      ],
      "year": "2017",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "16",
      "title": "Microexpressions and deception",
      "authors": [
        "G Mark",
        "Elena Frank",
        "Svetieva"
      ],
      "year": "2015",
      "venue": "Understanding facial expressions in communication"
    },
    {
      "citation_id": "17",
      "title": "Emotional expressions reconsidered: challenges to inferring emotion from human facial movements",
      "authors": [
        "Lisa Feldman",
        "Ralph Adolphs",
        "Stacy Marsella",
        "Aleix Martinez",
        "Seth Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest"
    },
    {
      "citation_id": "18",
      "title": "Do facial expressions signal specific emotions? judging emotion from the face in context",
      "authors": [
        "M James",
        "James Carroll",
        "Russell"
      ],
      "year": "1996",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "19",
      "title": "Where did the anger go? the role of context in interpreting emotion in speech",
      "authors": [
        "Richard T Cauldwell"
      ],
      "year": "2000",
      "venue": "ISCA Tutorial and Research Workshop (ITRW) on Speech and Emotion"
    },
    {
      "citation_id": "20",
      "title": "Context in emotion perception",
      "authors": [
        "Lisa Feldman",
        "Batja Mesquita",
        "Maria Gendron"
      ],
      "year": "2011",
      "venue": "Current Directions in Psychological Science"
    },
    {
      "citation_id": "21",
      "title": "Affect intensity as an individual difference characteristic: A review",
      "authors": [
        "J Randy",
        "Ed Larsen",
        "Diener"
      ],
      "year": "1987",
      "venue": "Journal of Research in personality"
    },
    {
      "citation_id": "22",
      "title": "Individual differences in two emotion regulation processes: implications for affect, relationships, and well-being",
      "authors": [
        "J James",
        "Oliver Gross",
        "John"
      ],
      "year": "2003",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "23",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "Mohammad Soleymani",
        "Jeroen Lichtenauer",
        "Maja Thierry Pun",
        "Pantic"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "24",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "25",
      "title": "Decaf: Meg-based multimodal database for decoding affective physiological responses",
      "authors": [
        "Mojtaba Khomami",
        "Ramanathan Subramanian",
        "Seyed Mostafa Kia",
        "Paolo Avesani",
        "Ioannis Patras",
        "Nicu Sebe"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Ascertain: Emotion and personality recognition using commercial sensors",
      "authors": [
        "Ramanathan Subramanian",
        "Julia Wache",
        "Mojtaba Khomami Abadi",
        "L Radu",
        "Stefan Vieriu",
        "Nicu Winkler",
        "Sebe"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "Stamos Katsigiannis",
        "Naeem Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "28",
      "title": "Amigos: A dataset for affect, personality and mood research on individuals and groups",
      "authors": [
        "Juan Abdon",
        "Miranda Correa",
        "Mojtaba Khomami Abadi",
        "Niculae Sebe",
        "Ioannis Patras"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "A dataset of continuous affect annotations and physiological signals for emotion analysis",
      "authors": [
        "Karan Sharma",
        "Claudio Castellini",
        "L Egon",
        "Alin Van Den Broek",
        "Friedhelm Albu-Schaeffer",
        "Schwenker"
      ],
      "year": "2019",
      "venue": "Scientific data"
    },
    {
      "citation_id": "30",
      "title": "Casme database: a dataset of spontaneous micro-expressions collected from neutralized faces",
      "authors": [
        "Wen-Jing Yan",
        "Qi Wu",
        "Yong-Jin Liu",
        "Su-Jing Wang",
        "Xiaolan Fu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "31",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "Philip Schmidt",
        "Attila Reiss",
        "Robert Duerichen",
        "Claus Marberger",
        "Kristof Van Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "32",
      "title": "Mood and temperament",
      "authors": [
        "David Watson"
      ],
      "year": "2000",
      "venue": "Mood and temperament"
    },
    {
      "citation_id": "33",
      "title": "How to find trouble in communication",
      "authors": [
        "Anton Batliner",
        "Kerstin Fischer",
        "Richard Huber",
        "Jörg Spilker",
        "Elmar Nöth"
      ],
      "year": "2003",
      "venue": "Speech communication"
    },
    {
      "citation_id": "34",
      "title": "The weirdest people in the world?",
      "authors": [
        "Joseph Henrich",
        "Steven Heine",
        "Ara Norenzayan"
      ],
      "year": "2010",
      "venue": "Behavioral and brain sciences"
    },
    {
      "citation_id": "35",
      "title": "Collecting large, richly annotated facialexpression databases from movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "36",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "Am-fed+: An extended dataset of naturalistic facial expressions collected in everyday settings",
      "authors": [
        "Daniel Mcduff",
        "May Amr",
        "Rana Kaliouby"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Gautam Naik, Erik Cambria, and Rada Mihalcea. Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "Soujanya Poria",
        "Devamanyu Hazarika",
        "Navonil Majumder"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "39",
      "title": "Crowdsourcing facial responses to online videos",
      "authors": [
        "Daniel Mcduff",
        "Rana Kaliouby",
        "Rosalind Picard"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Crowdsourcing techniques for affective computing",
      "authors": [
        "Daniel Morris",
        "Mcduff",
        "Calvo"
      ],
      "year": "2014",
      "venue": "The Oxford handbook of affective computing"
    },
    {
      "citation_id": "41",
      "title": "Reliability of crowdsourcing as a method for collecting emotions labels on pictures",
      "authors": [
        "Olga Korovina",
        "Marcos Baez",
        "Fabio Casati"
      ],
      "year": "2019",
      "venue": "BMC research notes"
    },
    {
      "citation_id": "42",
      "title": "Facial expression of emotion: A comparison of posed expressions versus spontaneous expressions in an interpersonal communication setting",
      "authors": [
        "T Michael",
        "Carl Motley",
        "Camden"
      ],
      "year": "1988",
      "venue": "Western Journal of Communication (includes Communication Reports)"
    },
    {
      "citation_id": "43",
      "title": "Effect of acting experience on emotion expression and recognition in voice: Non-actors provide better stimuli than expected",
      "authors": [
        "Rebecca Jürgens",
        "Annika Grass",
        "Matthis Drolet",
        "Julia Fischer"
      ],
      "year": "2015",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "44",
      "title": "The mirror to our soul? comparisons of spontaneous and posed vocal expression of emotion",
      "authors": [
        "Petri Patrik N Juslin",
        "Tanja Laukka",
        "Bänziger"
      ],
      "year": "2018",
      "venue": "Journal of nonverbal behavior"
    },
    {
      "citation_id": "45",
      "title": "The psychophysiology of emotion",
      "authors": [
        "John T Cacioppo",
        "Jeff Gary G Berntson",
        "Kirsten Larsen",
        "Tiffany Poehlmann",
        "Ito"
      ],
      "year": "2000",
      "venue": "Handbook of emotions"
    },
    {
      "citation_id": "46",
      "title": "Toward machine emotional intelligence: Analysis of affective physiological state",
      "authors": [
        "Rosalind Picard",
        "Elias Vyzas",
        "Jennifer Healey"
      ],
      "year": "2001",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "47",
      "title": "Using noninvasive wearable computers to recognize human emotions from physiological signals",
      "authors": [
        "Christine Laetitia",
        "Fatma Nasoz"
      ],
      "year": "2004",
      "venue": "EURASIP Journal on Advances in Signal Processing"
    },
    {
      "citation_id": "48",
      "title": "Basic emotions are associated with distinct patterns of cardiorespiratory activity",
      "authors": [
        "Pierre Rainville",
        "Antoine Bechara",
        "Nasir Naqvi",
        "Antonio Damasio"
      ],
      "year": "2006",
      "venue": "International journal of psychophysiology"
    },
    {
      "citation_id": "49",
      "title": "Bodily maps of emotions",
      "authors": [
        "Lauri Nummenmaa",
        "Enrico Glerean",
        "Riitta Hari",
        "Jari Hietanen"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "50",
      "title": "Neuroscience & Biobehavioral Reviews",
      "authors": [
        "Edward F Pace-Schott",
        "Tatjana Marlissa C Amole",
        "Michela Aue",
        "Lauren Balconi",
        "Hugo Bylsma",
        "Heath Critchley",
        "Demaree",
        "Anne Bruce H Friedman",
        "Elizabeth Kotynski",
        "Olivia Gooding",
        "Gosseries"
      ],
      "year": "2019",
      "venue": "Neuroscience & Biobehavioral Reviews"
    },
    {
      "citation_id": "51",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "52",
      "title": "The semaine database: Annotated multimodal records of emotionally colored conversations between a person and a limited agent",
      "authors": [
        "Gary Mckeown",
        "Michel Valstar",
        "Roddy Cowie",
        "Maja Pantic",
        "Marc Schroder"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "53",
      "title": "Msp-improv: An acted corpus of dyadic interactions to study emotion perception",
      "authors": [
        "Carlos Busso",
        "Srinivas Parthasarathy",
        "Alec Burmania",
        "Mohammed Abdelwahab",
        "Najmeh Sadoughi",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "54",
      "title": "Recording affect in the field: towards methods and metrics for improving ground truth labels",
      "authors": [
        "Jennifer Healey"
      ],
      "year": "2011",
      "venue": "International conference on affective computing and intelligent interaction"
    },
    {
      "citation_id": "55",
      "title": "Automatic recognition of self-reported and perceived emotion: Does joint modeling help?",
      "authors": [
        "Biqiao Zhang",
        "Georg Essl",
        "Emily Provost"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "56",
      "title": "Unobtrusive multimodal emotion detection in adaptive interfaces: speech and facial expressions",
      "authors": [
        "David Khiet P Truong",
        "Mark Van Leeuwen",
        "Neerincx"
      ],
      "year": "2007",
      "venue": "International Conference on Foundations of Augmented Cognition"
    },
    {
      "citation_id": "57",
      "title": "Verbal bias in recognition of facial emotions in children with asperger syndrome",
      "authors": [
        "Ami James B Grossman",
        "Alice Klin",
        "Fred Carter",
        "Volkmar"
      ],
      "year": "2000",
      "venue": "The Journal of Child Psychology and Psychiatry and Allied Disciplines"
    },
    {
      "citation_id": "58",
      "title": "Misperceptions of facial emotions among youth aged 9-14 years who present multiple antecedents of schizophrenia",
      "authors": [
        "Hannah Dickson",
        "Monica Calkins",
        "Christian Kohler",
        "Sheilagh Hodgins",
        "Kristin Laurens"
      ],
      "year": "2014",
      "venue": "Schizophrenia bulletin"
    },
    {
      "citation_id": "59",
      "title": "Speech-based recognition of self-reported and observed emotion in a dimensional space",
      "authors": [
        "David Khiet P Truong",
        "Franciska Mg De Van Leeuwen",
        "Jong"
      ],
      "year": "2012",
      "venue": "Speech communication"
    },
    {
      "citation_id": "60",
      "title": "The intensity of emotional facial expressions and decoding accuracy",
      "authors": [
        "Ursula Hess",
        "Sylvie Blairy",
        "Robert Kleck"
      ],
      "year": "1997",
      "venue": "Journal of Nonverbal Behavior"
    },
    {
      "citation_id": "61",
      "title": "Multimodal emotion recognition using deep learning architectures",
      "authors": [
        "Shayok Hiranmayi Ranganathan",
        "Sethuraman Chakraborty",
        "Panchanathan"
      ],
      "year": "2016",
      "venue": "2016 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "62",
      "title": "Biosignal sharing for affective connectedness",
      "authors": [
        "Christine Hyeryung",
        "Tek-Jin Nam"
      ],
      "year": "2014",
      "venue": "CHI'14 Extended Abstracts on Human Factors in Computing Systems"
    },
    {
      "citation_id": "63",
      "title": "Heartchat: Heart rate augmented mobile chat to support empathy and awareness",
      "authors": [
        "Mariam Hassib",
        "Daniel Buschek",
        "Paweł Wozniak",
        "Florian Alt"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems"
    },
    {
      "citation_id": "64",
      "title": "Supporting social interactions with an expressive heart rate sharing application",
      "authors": [
        "Fannie Liu",
        "Laura Dabbish",
        "Geoff Kaufman"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "65",
      "title": "Animo: Sharing biosignals on a smartwatch for lightweight social connection",
      "authors": [
        "Fannie Liu",
        "Mario Esparza",
        "Maria Pavlovskaia",
        "Geoff Kaufman",
        "Laura Dabbish",
        "Andrés Monroy-Hernández"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
    },
    {
      "citation_id": "66",
      "title": "The effect of expressive biosignals on empathy and closeness for a stigmatized group member",
      "authors": [
        "Fannie Liu",
        "Geoff Kaufman",
        "Laura Dabbish"
      ],
      "year": "2019",
      "venue": "Proceedings of the ACM on Human-Computer Interaction"
    },
    {
      "citation_id": "67",
      "title": "South korea's refugee debate eclipses a deeper, more fundamental question. The Hill",
      "authors": [
        "Soo Kim"
      ],
      "year": "2018",
      "venue": "South korea's refugee debate eclipses a deeper, more fundamental question. The Hill"
    },
    {
      "citation_id": "68",
      "title": "Yemeni refugees become a major issue on jeju. Korea JoongAng Daily",
      "authors": [
        "Jin-Kyu Kang"
      ],
      "year": "2018",
      "venue": "Yemeni refugees become a major issue on jeju. Korea JoongAng Daily"
    },
    {
      "citation_id": "69",
      "title": "South korea is going crazy over a handful of refugees",
      "authors": [
        "Nathan Park"
      ],
      "year": "2018",
      "venue": "Foreign Policy"
    },
    {
      "citation_id": "70",
      "title": "In south korea, opposition to yemeni refugees is a cry for help",
      "authors": [
        "Bo Seo"
      ],
      "year": "2018",
      "venue": "CNN"
    },
    {
      "citation_id": "71",
      "title": "Instructions matter: a comparison of baseline conditions for cognitive emotion regulation paradigms",
      "authors": [
        "Kersten Diers",
        "Fanny Weber",
        "Burkhard Brocke",
        "Alexander Strobel",
        "Sabine Schönfeld"
      ],
      "year": "2014",
      "venue": "Frontiers in psychology"
    },
    {
      "citation_id": "72",
      "title": "Emotion elicitation using films",
      "authors": [
        "J James",
        "Robert Gross",
        "Levenson"
      ],
      "year": "1995",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "73",
      "title": "The structure of verbal abilities in young and older adults",
      "authors": [
        "Susan Kemper",
        "Aaron Sumner"
      ],
      "year": "2001",
      "venue": "Psychology and aging"
    },
    {
      "citation_id": "74",
      "title": "Towards an integrated understanding of speaking rate in conversation",
      "authors": [
        "Jiahong Yuan",
        "Mark Liberman",
        "Christopher Cieri"
      ],
      "year": "2006",
      "venue": "Ninth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "75",
      "title": "Encyclopedia of autism spectrum disorders",
      "authors": [
        "Cheryl Smith"
      ],
      "year": "2013",
      "venue": "Encyclopedia of autism spectrum disorders"
    },
    {
      "citation_id": "76",
      "title": "Detection of emotions during learning with autotutor",
      "authors": [
        "Bethany Ac Graesser",
        "Patrick Mcdaniel",
        "Amy Chipman",
        "Witherspoon",
        "D' Sidney",
        "Barry Mello",
        "Gholson"
      ],
      "year": "2006",
      "venue": "Proceedings of the 28th annual meetings of the cognitive science society"
    },
    {
      "citation_id": "77",
      "title": "Natural affect data-collection & annotation in a learning context",
      "authors": [
        "Shazia Afzal",
        "Peter Robinson"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "78",
      "title": "Monitoring affect states during effortful problem solving activities",
      "authors": [
        "K D' Sidney",
        "Blair Mello",
        "Natalie Lehman",
        "Person"
      ],
      "year": "2010",
      "venue": "International Journal of Artificial Intelligence in Education"
    },
    {
      "citation_id": "79",
      "title": "On the influence of an iterative affect annotation approach on inter-observer and self-observer reliability",
      "authors": [
        "K D' Sidney",
        "Mello"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "80",
      "title": "Sources of bias in memory for emotions",
      "authors": [
        "J Linda",
        "Martin Levine",
        "Safer"
      ],
      "year": "2002",
      "venue": "Current Directions in Psychological Science"
    },
    {
      "citation_id": "81",
      "title": "Distortion in memory for emotions: The contributions of personality and post-event knowledge",
      "authors": [
        "Martin Safer",
        "Linda Levine",
        "Amy Drapalski"
      ],
      "year": "2002",
      "venue": "Personality and Social Psychology Bulletin"
    },
    {
      "citation_id": "82",
      "title": "Motivational biases in memory for emotions",
      "authors": [
        "C Heather",
        "Linda Lench",
        "Levine"
      ],
      "year": "2010",
      "venue": "Cognition and emotion"
    },
    {
      "citation_id": "83",
      "title": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations",
      "authors": [
        "Young Cheul",
        "Narae Park",
        "Soowon Cha",
        "Auk Kang",
        "Ahsan Kim",
        "Leontios Habib Khandoker",
        "Alice Hadjileontiadis",
        "Yong Oh",
        "Uichin Jeong",
        "Lee"
      ],
      "year": "2020",
      "venue": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations"
    },
    {
      "citation_id": "84",
      "title": "Emotion recognition in text for 3-d facial expression rendering",
      "authors": [
        "Ricardo Calix",
        "Abhishikth Sri",
        "Bin Mallepudi",
        "Gerald Chen",
        "Knapp"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "85",
      "title": "Harnessing twitter\" big data\" for automatic emotion identification",
      "authors": [
        "Wenbo Wang",
        "Lu Chen",
        "Krishnaprasad Thirunarayan",
        "Amit P Sheth"
      ],
      "year": "2012",
      "venue": "2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing"
    },
    {
      "citation_id": "86",
      "title": "Word embedding composition for data imbalances in sentiment and emotion classification",
      "authors": [
        "Ruifeng Xu",
        "Tao Chen",
        "Yunqing Xia",
        "Qin Lu",
        "Bin Liu",
        "Xuan Wang"
      ],
      "year": "2015",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "87",
      "title": "Computing krippendorff's alpha-reliability",
      "authors": [
        "Klaus Krippendorff"
      ],
      "year": "2011",
      "venue": "Computing krippendorff's alpha-reliability"
    },
    {
      "citation_id": "88",
      "title": "Future affective technology for autism and emotion communication",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "1535",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "89",
      "title": "Superpowerglass: a wearable aid for the at-home therapy of children with autism",
      "authors": [
        "Peter Washington",
        "Catalin Voss",
        "Aaron Kline",
        "Nick Haber",
        "Jena Daniels",
        "Azar Fazel",
        "Titas De",
        "Carl Feinstein",
        "Terry Winograd",
        "Dennis Wall"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies"
    },
    {
      "citation_id": "90",
      "title": "Conveying facial expressions to blind and visually impaired persons through a wearable vibrotactile device",
      "authors": [
        "Marian Hendrik P Buimer",
        "Tjerk Bittner",
        "Thea Kostelijk",
        "Abdellatif Van Der Geest",
        "Nemri",
        "J Richard",
        "Yan Van Wezel",
        "Zhao"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "91",
      "title": "Emotion and sociable humanoid robots",
      "authors": [
        "Cynthia Breazeal"
      ],
      "year": "2003",
      "venue": "International journal of human-computer studies"
    },
    {
      "citation_id": "92",
      "title": "Emotion interaction system for a service robot",
      "authors": [
        "Dong-Soo Kwon",
        "Jong Yoon Keun Kwak",
        "Myung Park",
        "Jin Chung",
        "Eun-Sook Jee",
        "Kyung-Sook Park",
        "Hyoung-Rock Kim",
        "Young-Min Kim",
        "Jong-Chan Park",
        "Eun Kim"
      ],
      "year": "2007",
      "venue": "RO-MAN 2007-The 16th IEEE International Symposium on Robot and Human Interactive Communication"
    },
    {
      "citation_id": "93",
      "title": "Improving automotive safety by pairing driver emotion and car voice emotion",
      "authors": [
        "Clifford Nass",
        "Ing-Marie Jonsson",
        "Helen Harris",
        "Ben Reaves",
        "Jack Endo",
        "Scott Brave",
        "Leila Takayama"
      ],
      "year": "2005",
      "venue": "CHI'05 extended abstracts on Human factors in computing systems"
    },
    {
      "citation_id": "94",
      "title": "Emotion on the road-necessity, acceptance, and feasibility of affective computing in the car",
      "authors": [
        "Florian Eyben",
        "Martin Wöllmer",
        "Tony Poitschke",
        "Björn Schuller",
        "Christoph Blaschke",
        "Berthold Färber",
        "Nhu Nguyen-Thien"
      ],
      "year": "2010",
      "venue": "Advances in human-computer interaction"
    },
    {
      "citation_id": "95",
      "title": "Clas: A database for cognitive load, affect and stress recognition",
      "authors": [
        "Valentina Markova",
        "Todor Ganchev",
        "Kalin Kalinkov"
      ],
      "year": "2019",
      "venue": "2019 International Conference on Biomedical Innovations and Applications (BIA)"
    },
    {
      "citation_id": "96",
      "title": "A circumplex model of affect",
      "authors": [
        "Russell James"
      ],
      "year": "1980",
      "venue": "Journal of personality and social psychology"
    },
    {
      "citation_id": "97",
      "title": "Continuous inference of psychological stress from sensory measurements collected in the natural environment",
      "authors": [
        "Kurt Plarre",
        "Andrew Raij",
        "Syed Monowar Hossain",
        "Ahsan Amin",
        "Motohiro Ali",
        "Mustafa Nakajima",
        "Emre Al'absi",
        "Thomas Ertin",
        "Santosh Kamarck",
        "Marcia Kumar",
        "Scott"
      ],
      "year": "2011",
      "venue": "Proceedings of the 10th ACM/IEEE international conference on information processing in sensor networks"
    },
    {
      "citation_id": "98",
      "title": "Baker rodrigo ocumpaugh monitoring protocol (bromp) 2.0 technical and training manual",
      "authors": [
        "Jaclyn Ocumpaugh"
      ],
      "year": "2015",
      "venue": "Baker rodrigo ocumpaugh monitoring protocol (bromp) 2.0 technical and training manual"
    }
  ]
}