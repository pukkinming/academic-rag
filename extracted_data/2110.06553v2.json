{
  "paper_id": "2110.06553v2",
  "title": "Spatial-Temporal Transformers For Eeg Emotion Recognition",
  "published": "2021-10-13T08:08:57Z",
  "authors": [
    "Jiyao Liu",
    "Hao Wu",
    "Li Zhang",
    "Yanxi Zhao"
  ],
  "keywords": [
    "EEG",
    "emotion recognition",
    "transformer"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalography (EEG) is a popular and effective tool for emotion recognition. However, the propagation mechanisms of EEG in the human brain and its intrinsic correlation with emotions are still obscure to researchers. This work proposes four variant transformer frameworks (spatial attention, temporal attention, sequential spatial-temporal attention and simultaneous spatial-temporal attention) for EEG emotion recognition to explore the relationship between emotion and spatial-temporal EEG features. Specifically, spatial attention and temporal attention are to learn the topological structure information and time-varying EEG characteristics for emotion recognition respectively. Sequential spatial-temporal attention does the spatial attention within a one-second segment and temporal attention within one sample sequentially to explore the influence degree of emotional stimulation on EEG signals of diverse EEG electrodes in the same temporal segment. The simultaneous spatial-temporal attention, whose spatial and temporal attention are performed simultaneously, is used to model the relationship between different spatial features in different time segments. The experimental results demonstrate that simultaneous spatial-temporal attention leads to the best emotion recognition accuracy among the design choices, indicating modeling the correlation of spatial and temporal features of EEG signals is significant to emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "EEG emotion recognition is to detect the current emotional states of the subjects  [1] ,  [2] ,  [3] . In recent years, with the development of deep learning and the availability of EEG data, many emotion recognition methods based on neural networks have dominated the state-of-art position  [4, 5] . In general, the EEG signals collected by a spherical EEG cap have three-dimensional characteristics which are spatial, spectral and temporal respectively. Many researchers have drawn attention to how to effectively utilize time-varying spatial and temporal features from multi-channel brain signals.\n\nIn order to model the space relationships among multichannel EEG signals, a hierarchical convolutional neural net-work (CNN) is proposed by Li et al.  [5]  to capture spatial information among different channels. A deep CNN model is present by Zhang et al.  [6]  to capture the spatio-temporal robust feature representation of the raw EEG data stream for motion intention classification. A utilization of multi-layer CNN with no full connection layers is proposed by Lawhern et al.  [7]  for P300-based oddball recognition task, finger motor task and motor imagination task.\n\nConsidering the change of EEG signals over time, an Echo State Network (ESN) is present by Fourati et al.  [8] , ESN used recursive layer to map the EEG signal into the high-dimension state space. A two-layer long short term memory (LSTM), which uses EEG signal as the input, is adopted by Alhagry et al.  [9]  and obtain promising EEG emotion classification results. A deep recursive convolutional neural network (R-CNN) is present by Bashivan et al.  [10] , the proposed R-CNN gets a satisfactory result on the task mental load classification based on EEG signal.\n\nMost of the above works are on the basis of convolution or recursive operation. CNN is good at modeling local receptive field message, while pays less attention to the global information. Recurrent Neural Networks (RNN) network is relatively weak to capture the spatial information and its parallel computational efficiency is slower. To solve the above weaknesses, some works lead attention mechanism into CNN and RNN.\n\nSince different spatial-temporal features have different contributions to emotion recognition, they should be assigned to different weight in the classifier recognizing emotions. A LSTM with attention mechanism is proposed by Kim et al.  [11] , the network assigns weights to the emotional states appearing at specific moments to conduct two-level and three-level classification on the valence and arousal emotion models. A fresh multi-channel model on the basis of sparse graphic attention long short term memory (SGA-LSTM) is present by Liu et al.  [12]  to classify EEG emotion.\n\nAs is mentioned above, existing works have attained gratifying results. However,The transmission characteristic as well as spatial-temporal relevance of different EEG electrodes are more or less neglected in most of them. The changeless size kernels in convolution operation  [13]  may damage the spatial correlation of EEG signals. Though the RNN operation  [14]  takes the temporal features of EEG signals into consideration, it ignores the spatial relation among EEG electrodes. Furthermore, on account of the diverse impedance of various brain areas, there may be a slight error in time between the EEG signal displayed by the EEG collection device and the real EEG signal, that is, EEG signals may delay varies with different EEG electrodes.\n\nTo deal with the mentioned issues, we present a fresh EEG emotion transformer (EeT) framework built exclusively on self-attention blocks. The variants of self-attention block include spatial (S) attention, temporal (T) attention, sequential spatial-temporal (S-T) attention and simultaneous spatialtemporal (S+T) attention. The spatial attention is to learn the spatial structure information. The temporal attention is to learn the correlation between EEG signals and emotional stimuli as well as temporal changes. The sequential spatialtemporal attention is to do spatial attention within the same time segment and temporal attention among different time segments in one sample. The simultaneous spatial-temporal attention is to do the two attention simultaneously. Experimental setups are elaborately picked to study the effects of spatial and temporal EEG signals on emotion recognition, and whether there is some correlation between the features of different channels at different time segments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Variants Of Eeg Emotion Transformers",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Framework Of Eet",
      "text": "EEG-based emotion recognition is to classify the emotion states according to the EEG signal. As illustrated in Fig.  1 , the overview of EeT includes four modules, namely the feature preparation module, the spatial-temporal attention module, deep neural network (DNN) module and classification module. We focus on the design of self-attention module which includes spatial attention, temporal attention, sequential spatial-temporal attention and simultaneous spatial-temporal attention.\n\nwhere X represents the EEG features. f represents the mapping function i.e., convolutional neural network transformation. Y ∈ {y 1 , y 2 , ..., y n } represents the emotional tags. In our work, the cross entropy is adopted as the loss function, which can be defined as:\n\nwhere L denotes the loss function of the EEG emotion classification, C denotes the number of emotion states, y c is the ground-truth emotion tag and y c is the predictor of neural networks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Preprocessing",
      "text": "The EEG features can be denoted as\n\nand γ [31-50 Hz]) used to compute the EEG features. S equals to the number of electrodes in the EEG cap and T equals to the number of the time slots in a EEG sample.\n\npresents the feature of one EEG channel. Specifically, we map the S electrodes into a V × H matrix according to the layout of the EEG cap in order to preserve the spatial topology structure information of the EEG cap, then we take advantage of the linear interpolation  [15]  to replenish the spatial information which are not collected by EEG acquisition equipment. The re-organized feature can be represent as\n\nfor the whole EEG sample.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Positional Encoding For Spatial Eeg Electrodes",
      "text": "We divided the EEG feature of each second F t (i = 1, 2, 3..S) into G non-overlapping regions, just like the different brain regions in neuroscience. Here we regroup the V × H matrices into region sequences, the size of each divided region is P × P , so we get G = V H/P 2 regions. Each region is flatten into a vector I(x) (p,t) ∈ R 5P 2 with p = 1, 2..., G representing spatial layout of EEG electrodes and t = 1, 2, ...T denoting the index over seconds. Then we linearly map each region I(x) (p,t) into a latent vector z\n\nwhere ⊗ is matrix multiplication and e pos (p,t) ∈ R D stands for a learnable position embedding to encode the spatial-temporal position of each brain region. The resulting sequence of embedding vectors z (0) (p,t) stands for the input to the next layer of the self-attention block. Note that z i is output of the ith layer in self-attention block. p = 1, ...G and t = 1, ..., T are the spatial locations and indexes over time segments respectively.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Query-Key-Value Mechanism",
      "text": "Our Transformer consists of L encoding blocks. Instead of performing a single attention function, we use different pro-jected versions of queries, keys and values to perform the attention function in parallel, which is called multi-head attention. At each block l, the query/key/value vectors are computed for each region from the representation z (l-1) (p,t) encoded by the preceding block:\n\n(p,t) , which is the output of the previous block, need to be layer normalized before the above operations. a = 1, 2..., A denotes an index over multiple attention heads and A is the number of attention heads.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Variants Of Attention Mask Learning",
      "text": "The variants of self-attention block include spatial attention (S), temporal attention (T), sequential spatial-temporal attention (S-T) and simultaneous spatial-temporal attention (S+T). The spatial attention is to learn the spatial structure information while the temporal attention is to the relationship between EEG and time. The sequential spatial-temporal attention is the concatenation of two operations. The simultaneous spatial-temporal attention is to do the two operations simultaneously.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Spatial Attention",
      "text": "In the case of spatial attention, the self-attention weights α (a,l) (p,t) ∈ R N +1 for query brain region (p, t) are given by:\n\nwhere p denotes the index of the brain regions. σ denotes the softmax activation function. The formula is to consider that different brain regions react differently under the same emotional stimulation thus different weights are given to the features of different brain regions.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal Attention",
      "text": "For the temporal attention, the self-attention weights α (l,a) (p,t) ∈ R T +1 for query brain region (p, t) are given by:\n\nwhere t denotes the index of the time slots. In this formula, different weights are given to the of different time slots in consideration of the change of EEG signals with emotional stimulation and time.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Sequential Spatial-Temporal (S-T) Attention",
      "text": "The sequential spatial-temporal attention is to do spatial attention within the same time segment and temporal attention among different time segments. Firstly, the spatial selfattention weights are calculated as Eq. 7. Then the the temporal attention weights are learned by Eq. 8 from the output of spatial attention layer. (S-T) Attention comprehensively considers the attention of space and time, but the default is that the spatial features in the same time period are closely related, while the features of different brain in different time are weakly related.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Simultaneous Spatial-Temporal (S+T) Attention",
      "text": "The simultaneous spatial-temporal (S+T) attention is to do spatial and temporal attention simultaneously. The selfattention weights α (a,l) (p,t) ∈ R N T +1 for query brain region (p, t) are given by:\n\nDifferent from S-T Attention, which regards space and time separately, the S+T attention considers that the spatial information in the same time point and different time points are both strongly correlated.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Multi-Head Attention Recalibration",
      "text": "The encoding z (l) (p,t) at block l is obtained by the first computing the weighted sum of value vectors using self-attention coefficients from each attention head:  (10)  As is mentioned above, a denotes an index over multiple attention heads and l is the index of the blocks. Then, the concatenation of these vectors from all heads is projected and passed through an MLP, using residual connections after each operation:\n\nwhere\n\nThe z l (p,t) goes through the MLP layer to get the output of lth layer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Datasets",
      "text": "We validate our model on SEED  [16, 17] , SEED-IV  [18]  and Deap  [19]  databases.\n\nDeap dataset is a open source dataset including diverse physiological signals with emotion evaluations provided by the research team of Queen Mary University in London. It records the EEG, ECG, EMG and other bioelectrical signals of 32 subjects induced by watching 40 one-minute music videos of different emotional tendencies. The subjects evaluated the videos' emotion categories on scale of one to nine in dimension of arousal, valence, liking, dominance and familiarity. Valence reports the degree of subjects' joy, the greater the valence value, the higher the joy degree. Arousal reports the subjects' emotional intensity, the higher the arousal value, the more intense and perceptible the emotion. The rating value from small to large indicates the emotion metric is from negative to positive or from weak to strong. The 40 stimulus videos include 20 high valence/arousal stimuli and 20 low valence/arousal stimuli.\n\nSEED contains three different categories of emotion, namely positive, negative, and neutral. Fifteen participants' EEG data of the dataset were collected while they were watching the stimulus videos. The videos are carefully selected and can elicit a single desired target emotion. With an interval of about one week, each subject participated in three experiments, and each session contained 15 film clips. The participants are asked to give feedback immediately after each experiment. The EEG signals of 62 channels are recorded at a sampling frequency of 1000 Hz and down-sampled with 200 Hz. The Differential entropy  [17]  DE features are precomputed over different frequency bands for each sample in each channel.\n\nSEED-IV contains four different categories of emotions, including happy, sad, fear, and neutral emotion. The experiment consists of 15 participants. Three experiments are designed for each participant on different days, and each session contains 24 video clips and six clips for each type of emotion in each session. After each experiment, the subjects are asked to give feedback, while 62 EEG signals of the subjects are recorded. The EEG signals are sliced into 4-second nonoverlapping segments and down-sampled with 128 Hz. The DE feature is also pre-computed over five frequency bands in each channel.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "We train our model on NVIDIA RTX 2080 GPU. Cross entropy loss is used as the loss function. The optimizer is Adam. The initial learning rate is set to 1e-3 with multi-step decay to 1e-7. The number of the attention blocks is set to 4 and the length of each sample is set to 10s. We conduct experiments on each subject. For each experiment, we randomly shuffle the samples and use 5-fold cross validation. The ratio of the training set to test set is 9:6.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Compared Models",
      "text": "We compare the proposed EeT with the following competitive models.\n\nSVM  [20]  is a least squares support vector machine classifier. DBN  [21]  is deep Belief Networks investigate the critical frequency bands and channels. DGCNN  [22]  is Dynamical Graph Convolutional Neural Networks model the multichannel EEG features. BiDANN  [23]  is bi-hemispheres domain adversarial neural network maps the EEG feature of both hemispheres into discriminative feature spaces separately. BiHDM  [24]  is bi-hemispheric discrepancy model learns the asymmetric differences between two hemispheres for EEG emotion recognition. 3D-CNN with PST-Attention  [25]  is a self-attention module combined with 3D-CNN to learn critical information among different dimensions of EEG feature. LSTM  [9]  is a time series model for identifying continuous dimension emotion. 3D-CNN  [26]  is 3D-CNN model to recognize arousal and valence. BT  [27]  is deep convolution neural network for continuous dimension emotion recognition.   1  presents the average accuracy (acc) and F1 value (F1) of the compared models for EEG based emotion recognition on the DEAP datasets. Compared with 3D-CNN  [26] , the acc of the proposed simultaneous spatial-temporal (S+T) attention EeT framework have 4.85%/5.42% improvement. EeT with S+T attention also gets superior performance compared with other competitive models. Table  2  presents the average accuracy (Mean) and standard deviation (Std) of the compared models for EEG based emotion recognition on SEED and SEED-IV datasets. Compared with 3D-CNN with PST-Attention, the means of the proposed joint spatial+temporal (S+T) attention EeT framework have 0.52%/0.54% improvements on SEED and SEED-IV. The Stds of Eet with S+T attention achieve 0.59%/0.59% reductions on SEED and SEED-IV respectively compared with those of 3D-CNN with PST-Attention. Moreover, EeT with S+T attention gets superior performance compared with other competitive models.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experimental Results And Analysis",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "The Visualization Features Extracted From Eet Without Attention",
      "text": "The Visualization Features Extracted From Eet with S+T Attention We use t-SNE  [28]  to visualize the high-level bottleneck features from the well trained Eet. As shown in Fig.  2 , different colors represent different emotional labels, the distances of different classes in the high-level feature space of Eet with S+T Attention (the right part) are more dispersed than that of EeT without attention (the left part), which demonstrates that the high-level features learned with simultaneous spatialtemporal attention is more discriminative. Table  3  presents the results of different variants of the proposed transformer framework, from which we can see that Joint Spatial-Temporal Attention gets the best results, achieving 0.63%/2.96% improvements compared with the second best variant, (S-T) Attention on SEED and SEED-IV respectively, indicating comprehensively considering the temporal and spatial characteristics of EEG may boost the emotion recognition results most notably. As for single dimensional attention, the results are a bit of lower than those of combined variants'. Spatial Attention is 0.4% /0.94% higher than that of Temporal Attention, implying the spatial dimension may have more emotion-related message than the temporal dimension.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we propose a new EEG emotion recognition framework based on self-attention, which is built exclusively on self-attention. Our approach considers the relationship between emotion and brain regions, time series change as well as the intrinsic spatiotemporal characteristics of EEG signals.\n\nThe results of our methods show that the attention mechanism can boost the performance of emotion recognition evidently. Furthermore, the simultaneous spatio-temporal attention gets the best results among the four designed structures, the result is also better than most state of the art methods, indicating that considering the spatio-temporal feature jointly and simultaneously is more in line with the transmission law of EEG signals in the human brain.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of EEG Emotion Recognition Transformer (EeT)",
      "page": 2
    },
    {
      "caption": "Figure 2: The Visualization of High Level Features Extracted From Eet",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "ABSTRACT"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "Electroencephalography (EEG) is a popular and effective tool"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "for emotion recognition. However,\nthe propagation mecha-"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "nisms of EEG in the human brain and its intrinsic correlation"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "with emotions are still obscure to researchers. This work pro-"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "poses four variant\ntransformer frameworks (spatial attention,"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "temporal attention, sequential spatial-temporal attention and"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "simultaneous\nspatial-temporal\nattention)\nfor EEG emotion"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "recognition to explore the relationship between emotion and"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "spatial-temporal EEG features. Speciﬁcally, spatial attention"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "and temporal attention are to learn the topological structure"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "information and time-varying EEG characteristics for emo-"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "tion\nrecognition\nrespectively.\nSequential\nspatial-temporal"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "attention does the spatial attention within a one-second seg-"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "ment and temporal attention within one sample sequentially"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "to explore the inﬂuence degree of emotional stimulation on"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "EEG signals of diverse EEG electrodes in the same tempo-"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "ral\nsegment.\nThe\nsimultaneous\nspatial-temporal\nattention,"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "whose spatial and temporal attention are performed simulta-"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "neously,\nis used to model\nthe relationship between different"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "spatial features in different time segments. The experimental"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "results demonstrate that simultaneous spatial-temporal atten-"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "tion leads\nto the best emotion recognition accuracy among"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "the design choices,\nindicating modeling the\ncorrelation of"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "spatial and temporal features of EEG signals is signiﬁcant\nto"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "emotion recognition."
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "Index Terms— EEG, emotion recognition, transformer"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "1.\nINTRODUCTION"
        },
        {
          "School of Computer Science,": ""
        },
        {
          "School of Computer Science,": "EEG emotion recognition is to detect\nthe current emotional"
        },
        {
          "School of Computer Science,": "states of\nthe subjects [1],\n[2],\n[3].\nIn recent years, with the"
        },
        {
          "School of Computer Science,": "development of deep learning and the availability of EEG"
        },
        {
          "School of Computer Science,": "data, many emotion recognition methods based on neural net-"
        },
        {
          "School of Computer Science,": "works have dominated the state-of-art position [4, 5]. In gen-"
        },
        {
          "School of Computer Science,": "eral,\nthe EEG signals collected by a spherical EEG cap have"
        },
        {
          "School of Computer Science,": "three-dimensional characteristics which are spatial,\nspectral"
        },
        {
          "School of Computer Science,": "and temporal respectively. Many researchers have drawn at-"
        },
        {
          "School of Computer Science,": "tention to how to effectively utilize time-varying spatial and"
        },
        {
          "School of Computer Science,": "temporal features from multi-channel brain signals."
        },
        {
          "School of Computer Science,": "In order\nto model\nthe space relationships among multi-"
        },
        {
          "School of Computer Science,": "channel EEG signals, a hierarchical convolutional neural net-"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "consideration, it ignores the spatial relation among EEG elec-": "trodes. Furthermore, on account of the diverse impedance of",
          "where L denotes the loss function of the EEG emotion clas-": "siﬁcation, C denotes the number of emotion states, yc is the"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "various brain areas,\nthere may be a slight error\nin time be-",
          "where L denotes the loss function of the EEG emotion clas-": "(cid:48) c\nground-truth emotion tag and y\nis the predictor of neural net-"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "tween the EEG signal displayed by the EEG collection device",
          "where L denotes the loss function of the EEG emotion clas-": "works."
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "and the real EEG signal, that is, EEG signals may delay varies",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "with different EEG electrodes.",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "2.2. Preprocessing"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "To deal with the mentioned issues, we present a fresh",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "EEG emotion transformer (EeT) framework built exclusively",
          "where L denotes the loss function of the EEG emotion clas-": "The EEG features can be denoted as X = (F1, F2, ..., FT ) ∈"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "on self-attention blocks. The variants of self-attention block",
          "where L denotes the loss function of the EEG emotion clas-": "RC×S×T , where C is 5, equals to the number of\nfrequency"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "include spatial (S) attention,\ntemporal (T) attention, sequen-",
          "where L denotes the loss function of the EEG emotion clas-": "θ\nβ\nbands\n(δ[1-4Hz],\n[4-8Hz],\nα [8-14Hz],\n[14-31Hz],"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "tial spatial-temporal (S-T) attention and simultaneous spatial-",
          "where L denotes the loss function of the EEG emotion clas-": "S\nand γ [31-50 Hz]) used to compute the EEG features."
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "temporal\n(S+T) attention.\nThe spatial attention is\nto learn",
          "where L denotes the loss function of the EEG emotion clas-": "equals\nto\nthe\nnumber\nof\nelectrodes\nin\nthe EEG cap\nand"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "the spatial structure information.\nThe temporal attention is",
          "where L denotes the loss function of the EEG emotion clas-": "T\nequals\nto the number of\nthe\ntime\nslots\nin a EEG sam-"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "to learn the correlation between EEG signals and emotional",
          "where L denotes the loss function of the EEG emotion clas-": "ple.\nFt = (B1, B2, ..., BS) ∈ RC×S(t ∈ {1, 2, ..., T })"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "stimuli as well as temporal changes. The sequential spatial-",
          "where L denotes the loss function of the EEG emotion clas-": "is\nthe one-second EEG feature.\nBs = (b1, b2, ..., bC) ∈"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "temporal attention is to do spatial attention within the same",
          "where L denotes the loss function of the EEG emotion clas-": "RC(s ∈ {1, 2, ..., S}) presents the feature of one EEG chan-"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "time segment and temporal attention among different\ntime",
          "where L denotes the loss function of the EEG emotion clas-": "nel.\nSpeciﬁcally, we map the S electrodes\ninto a V × H"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "segments in one sample. The simultaneous spatial-temporal",
          "where L denotes the loss function of the EEG emotion clas-": "matrix according to the layout of\nthe EEG cap in order\nto"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "attention is to do the two attention simultaneously.\nExperi-",
          "where L denotes the loss function of the EEG emotion clas-": "preserve\nthe\nspatial\ntopology structure\ninformation of\nthe"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "mental setups are elaborately picked to study the effects of",
          "where L denotes the loss function of the EEG emotion clas-": "EEG cap,\nthen we\ntake\nadvantage of\nthe\nlinear\ninterpola-"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "spatial and temporal EEG signals on emotion recognition, and",
          "where L denotes the loss function of the EEG emotion clas-": "tion [15]\nto replenish the spatial\ninformation which are not"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "whether there is some correlation between the features of dif-",
          "where L denotes the loss function of the EEG emotion clas-": "collected by EEG acquisition equipment.\nThe re-organized"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "ferent channels at different time segments.",
          "where L denotes the loss function of the EEG emotion clas-": "(cid:48) t\nfeature\ncan be\nrepresent\nas F\n= (B1, B2, ..., BV ×H ) ∈"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "RC×V ×H (t ∈ 1, 2, ..., T ) for a one-second EEG slice and"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "(cid:48)"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "X\nfor\nthe whole EEG\n= (F1, F2, ..., FT ) ∈ RC×V ×H×T"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "2. VARIANTS OF EEG EMOTION TRANSFORMERS",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "sample."
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "2.1. Framework of EeT",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "2.3. Positional Encoding for Spatial EEG Electrodes"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "EEG-based emotion recognition is\nto classify the emotion",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "states according to the EEG signal. As illustrated in Fig. 1,",
          "where L denotes the loss function of the EEG emotion clas-": "We divided the EEG feature of each second Ft(i = 1, 2, 3..S)"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "the\noverview of EeT includes\nfour modules,\nnamely\nthe",
          "where L denotes the loss function of the EEG emotion clas-": "into G non-overlapping regions,\njust\nlike the different brain"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "feature\npreparation module,\nthe\nspatial-temporal\nattention",
          "where L denotes the loss function of the EEG emotion clas-": "regions in neuroscience. Here we regroup the V × H ma-"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "module, deep neural network (DNN) module\nand classiﬁ-",
          "where L denotes the loss function of the EEG emotion clas-": "trices into region sequences,\nthe size of each divided region"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "cation module.\nWe\nfocus on the design of\nself-attention",
          "where L denotes the loss function of the EEG emotion clas-": "is P × P , so we get G = V H/P 2 regions. Each region is"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "module which\nincludes\nspatial\nattention,\ntemporal\natten-",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "with p = 1, 2..., G rep-\nﬂatten into a vector I(x)(p,t) ∈ R5P 2"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "tion,\nsequential spatial-temporal attention and simultaneous",
          "where L denotes the loss function of the EEG emotion clas-": "resenting spatial\nlayout of EEG electrodes and t = 1, 2, ...T"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "spatial-temporal attention.",
          "where L denotes the loss function of the EEG emotion clas-": "denoting the index over seconds. Then we linearly map each"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "into a latent vector z(0)"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "4 ✖ Blocks",
          "where L denotes the loss function of the EEG emotion clas-": "region I(x)(p,t)\n(p,t) ∈ RD by means of"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "EEG\nFeature\nSpatial-\nTemporal-\nClassification \n...",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "DNN\nSignal\nPreparation \nAttention\nAttention\nLayer",
          "where L denotes the loss function of the EEG emotion clas-": "learnable matrix M ∈ RD×5P 2\n:"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "z(0)"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "Fig. 1: Overview of EEG Emotion Recognition Transformer (EeT)",
          "where L denotes the loss function of the EEG emotion clas-": "(3)\n(p,t) = M ⊗ I(x)(p,t) + eposition"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "where ⊗ is matrix multiplication and epos"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "Emotion recognition based on EEG is to learn a function",
          "where L denotes the loss function of the EEG emotion clas-": "(p,t) ∈ RD stands for a"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "f which maps the raw signals to emotion tags:",
          "where L denotes the loss function of the EEG emotion clas-": "learnable position embedding to encode the spatial-temporal"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "position of each brain region. The resulting sequence of em-"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "(cid:48)",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "Y = f (X\n)\n(1)",
          "where L denotes the loss function of the EEG emotion clas-": "bedding vectors z(0)"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "(p,t) stands for the input to the next layer of"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "(cid:48)",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "where X\nrepresents the EEG features. f represents the map-",
          "where L denotes the loss function of the EEG emotion clas-": "the self-attention block. Note that zi\nis output of the ith layer"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "ping function i.e., convolutional neural network transforma-",
          "where L denotes the loss function of the EEG emotion clas-": "in self-attention block.\np = 1, ...G and t = 1, ..., T are the"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "tags.\nIn\ntion. Y ∈ {y1, y2, ..., yn} represents the emotional",
          "where L denotes the loss function of the EEG emotion clas-": "spatial locations and indexes over time segments respectively."
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "our work,\nthe cross entropy is adopted as the loss function,",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "which can be deﬁned as:",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "2.4. Query-Key-Value Mechanism"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "Our Transformer consists of L encoding blocks.\nInstead of"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "C(cid:88) c\n(cid:48) c\nL = −\n)\n(2)\nyclog(y",
          "where L denotes the loss function of the EEG emotion clas-": ""
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "",
          "where L denotes the loss function of the EEG emotion clas-": "performing a single attention function, we use different pro-"
        },
        {
          "consideration, it ignores the spatial relation among EEG elec-": "=1",
          "where L denotes the loss function of the EEG emotion clas-": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "jected versions of queries, keys and values to perform the at-": "tention function in parallel, which is called multi-head atten-",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "The sequential spatial-temporal attention is to do spatial at-"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "tion. At each block l,\nthe query/key/value vectors are com-",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "tention within the same time segment and temporal atten-"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "encoded\nputed for each region from the representation z(l−1)",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "(p,t)",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "tion among different\ntime segments. Firstly,\nthe spatial self-"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "by the preceding block:",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "attention weights are calculated as Eq. 7. Then the the tem-"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "q(l,a)\nz(l−1)",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "poral attention weights are learned by Eq. 8 from the output"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "∈ RDh\n(4)\n(p,t) = W (l,a)\n(p,t)",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "of spatial attention layer.\n(S-T) Attention comprehensively"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "k(l,a)\nz(l−1)",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "∈ RDh\n(5)\n(p,t) = W (l,a)\n(p,t)",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "considers the attention of space and time, but\nthe default\nis"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "that\nthe spatial\nfeatures in the same time period are closely"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "v(l,a)\nz(l−1)",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "∈ RDh\n(6)\n(p,t) = W (l,a)\n(p,t)",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "related, while the features of different brain in different\ntime"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "are weakly related."
        },
        {
          "jected versions of queries, keys and values to perform the at-": "z(l−1)\n, which is the output of the previous block, need to be",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "(p,t)",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "layer normalized before the above operations. a = 1, 2..., A",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "2.5.4.\nSimultaneous Spatial-Temporal (S+T) Attention"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "denotes an index over multiple attention heads and A is the",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "number of attention heads.",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "The simultaneous\nspatial-temporal\n(S+T) attention is\nto do"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "spatial\nand\ntemporal\nattention\nsimultaneously.\nThe\nself-"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "2.5. Variants of Attention Mask Learning",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "attention weights α(a,l)"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "for query brain region\n(p,t) ∈ RN T +1"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "(p, t) are given by:"
        },
        {
          "jected versions of queries, keys and values to perform the at-": "The variants of self-attention block include spatial attention",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": ""
        },
        {
          "jected versions of queries, keys and values to perform the at-": "(S), temporal attention (T), sequential spatial-temporal atten-",
          "2.5.3.\nSequential Spatial-Temporal (S-T) Attention": "T"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 2: presents the average accuracy (Mean) and stan-",
      "data": [
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": "are recorded. The EEG signals are sliced into 4-second non-"
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": "overlapping segments and down-sampled with 128 Hz. The"
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": "DE feature is also pre-computed over ﬁve frequency bands in"
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": "each channel."
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": "3.2. Experimental Setup"
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": "We train our model on NVIDIA RTX 2080 GPU. Cross en-"
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": "tropy loss is used as the loss function. The optimizer is Adam."
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": ""
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": "The initial learning rate is set to 1e-3 with multi-step decay to"
        },
        {
          "asked to give feedback, while 62 EEG signals of the subjects": "1e-7. The number of the attention blocks is set"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 2: presents the average accuracy (Mean) and stan-",
      "data": [
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "training set to test set is 9:6."
        },
        {
          "3. EXPERIMENTS": "3.1. Datasets",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "3.3. Compared Models"
        },
        {
          "3. EXPERIMENTS": "We validate our model on SEED [16, 17], SEED-IV [18] and",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "Deap [19] databases.",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "We compare the proposed EeT with the following competitive"
        },
        {
          "3. EXPERIMENTS": "Deap dataset\nis a open source dataset\nincluding diverse",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "models."
        },
        {
          "3. EXPERIMENTS": "physiological signals with emotion evaluations provided by",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "SVM [20] is a least squares support vector machine classi-"
        },
        {
          "3. EXPERIMENTS": "the research team of Queen Mary University in London.\nIt",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "ﬁer. DBN [21] is deep Belief Networks investigate the critical"
        },
        {
          "3. EXPERIMENTS": "records the EEG, ECG, EMG and other bioelectrical signals",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "frequency bands and channels. DGCNN [22]\nis Dynamical"
        },
        {
          "3. EXPERIMENTS": "of 32 subjects\ninduced by watching 40 one-minute music",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "Graph Convolutional Neural Networks model the multichan-"
        },
        {
          "3. EXPERIMENTS": "videos of different emotional tendencies. The subjects evalu-",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "nel EEG features. BiDANN [23]\nis bi-hemispheres domain"
        },
        {
          "3. EXPERIMENTS": "ated the videos’ emotion categories on scale of one to nine in",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "adversarial neural network maps\nthe EEG feature of both"
        },
        {
          "3. EXPERIMENTS": "dimension of arousal, valence,\nliking, dominance and famil-",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "hemispheres\ninto\ndiscriminative\nfeature\nspaces\nseparately."
        },
        {
          "3. EXPERIMENTS": "iarity. Valence reports the degree of subjects’ joy, the greater",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "BiHDM [24] is bi-hemispheric discrepancy model\nlearns the"
        },
        {
          "3. EXPERIMENTS": "the valence value,\nthe higher the joy degree. Arousal reports",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "asymmetric differences between two hemispheres\nfor EEG"
        },
        {
          "3. EXPERIMENTS": "the subjects’ emotional intensity, the higher the arousal value,",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "emotion recognition.\n3D-CNN with PST-Attention [25] is a"
        },
        {
          "3. EXPERIMENTS": "the more intense and perceptible the emotion.\nThe rating",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "self-attention module combined with 3D-CNN to learn criti-"
        },
        {
          "3. EXPERIMENTS": "value from small to large indicates the emotion metric is from",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "cal\ninformation among different dimensions of EEG feature."
        },
        {
          "3. EXPERIMENTS": "negative to positive or from weak to strong. The 40 stimulus",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "LSTM [9]\nis a time series model\nfor\nidentifying continuous"
        },
        {
          "3. EXPERIMENTS": "videos\ninclude 20 high valence/arousal\nstimuli and 20 low",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "dimension emotion.\n3D-CNN\n[26]\nis 3D-CNN model\nto"
        },
        {
          "3. EXPERIMENTS": "valence/arousal stimuli.",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "recognize arousal and valence. BT [27] is deep convolution"
        },
        {
          "3. EXPERIMENTS": "SEED contains\nthree\ndifferent\ncategories\nof\nemotion,",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "neural network for continuous dimension emotion recogni-"
        },
        {
          "3. EXPERIMENTS": "namely positive, negative, and neutral.\nFifteen participants’",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "tion."
        },
        {
          "3. EXPERIMENTS": "EEG data\nof\nthe\ndataset were\ncollected while\nthey were",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "watching the stimulus videos.\nThe videos are carefully se-",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "3.4. Experimental Results and Analysis"
        },
        {
          "3. EXPERIMENTS": "lected and can elicit a single desired target emotion. With an",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "interval of about one week, each subject participated in three",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "Table 1: Experimental Results on DEAP Dataset"
        },
        {
          "3. EXPERIMENTS": "experiments, and each session contained 15 ﬁlm clips. The",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "participants are asked to give feedback immediately after each",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "Arousal\nValence"
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "Models"
        },
        {
          "3. EXPERIMENTS": "experiment. The EEG signals of 62 channels are recorded at",
          "the samples and use 5-fold cross validation. The ratio of the": "acc(%)\nF1\nacc(%)\nF1"
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "LSTM[9]\n85.65\n-\n85.45\n-"
        },
        {
          "3. EXPERIMENTS": "a sampling frequency of 1000 Hz and down-sampled with",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "3D-CNN [26]\n88.49\n-\n87.44\n-"
        },
        {
          "3. EXPERIMENTS": "200 Hz. The Differential entropy [17] DE features are pre-",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "BT[27]\n86.18\n-\n86.31\n-"
        },
        {
          "3. EXPERIMENTS": "computed over different frequency bands for each sample in",
          "the samples and use 5-fold cross validation. The ratio of the": "93.34\n92.86\nEeT∼(S+T Attention)\n0.9326\n0.9196"
        },
        {
          "3. EXPERIMENTS": "each channel.",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "SEED-IV contains four different categories of emotions,",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "Table 1 presents the average accuracy (acc) and F1 value"
        },
        {
          "3. EXPERIMENTS": "including happy, sad,\nfear, and neutral emotion. The exper-",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "(F1) of the compared models for EEG based emotion recog-"
        },
        {
          "3. EXPERIMENTS": "iment consists of 15 participants. Three experiments are de-",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "nition on the DEAP datasets. Compared with 3D-CNN [26],"
        },
        {
          "3. EXPERIMENTS": "signed for each participant on different days, and each session",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "the acc of the proposed simultaneous spatial-temporal\n(S+T)"
        },
        {
          "3. EXPERIMENTS": "contains 24 video clips and six clips for each type of emo-",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "attention EeT framework have 4.85%/5.42% improvement."
        },
        {
          "3. EXPERIMENTS": "tion in each session. After each experiment,\nthe subjects are",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "EeT with S+T attention also gets superior performance com-"
        },
        {
          "3. EXPERIMENTS": "asked to give feedback, while 62 EEG signals of the subjects",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "pared with other competitive models."
        },
        {
          "3. EXPERIMENTS": "are recorded. The EEG signals are sliced into 4-second non-",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "overlapping segments and down-sampled with 128 Hz. The",
          "the samples and use 5-fold cross validation. The ratio of the": "Table 2: Experimental Results on SEED and SEED-IV Dataset"
        },
        {
          "3. EXPERIMENTS": "DE feature is also pre-computed over ﬁve frequency bands in",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "SEED\nSEED-IV"
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "Models"
        },
        {
          "3. EXPERIMENTS": "each channel.",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "Mean (%)\nStd (%) Mean (%)\nStd (%)"
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "SVM [20]\n83.99\n9.72\n56.61\n20.05"
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "7.38\nDBN [21]\n86.08\n8.34\n66.77"
        },
        {
          "3. EXPERIMENTS": "3.2. Experimental Setup",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "DGCNN [22]\n90.40\n8.49\n69.88\n16.29"
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "BiDANN [23]\n92.38\n7.04\n70.29\n12.63"
        },
        {
          "3. EXPERIMENTS": "We train our model on NVIDIA RTX 2080 GPU. Cross en-",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "BiHDM [24]\n93.12\n6.06\n74.35\n14.09"
        },
        {
          "3. EXPERIMENTS": "tropy loss is used as the loss function. The optimizer is Adam.",
          "the samples and use 5-fold cross validation. The ratio of the": "3D-CNN with PST-Attention[25]\n95.76\n4.98\n82.73\n8.96"
        },
        {
          "3. EXPERIMENTS": "",
          "the samples and use 5-fold cross validation. The ratio of the": "EeT (S+T Attention)\n96.28\n4.39\n83.27\n8.37"
        },
        {
          "3. EXPERIMENTS": "The initial learning rate is set to 1e-3 with multi-step decay to",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "1e-7. The number of the attention blocks is set\nto 4 and the",
          "the samples and use 5-fold cross validation. The ratio of the": ""
        },
        {
          "3. EXPERIMENTS": "length of each sample is set\nto 10s. We conduct experiments",
          "the samples and use 5-fold cross validation. The ratio of the": "Table 2 presents the average accuracy (Mean) and stan-"
        },
        {
          "3. EXPERIMENTS": "on each subject.\nFor each experiment, we randomly shufﬂe",
          "the samples and use 5-fold cross validation. The ratio of the": "dard deviation (Std) of the compared models for EEG based"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: presents the results of different variants of the",
      "data": [
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "pared with 3D-CNN with PST-Attention,\nthe means of\nthe",
          "4. CONCLUSION": ""
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "proposed joint spatial+temporal\n(S+T) attention EeT frame-",
          "4. CONCLUSION": "In this paper, we propose a new EEG emotion recognition"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "work have 0.52%/0.54% improvements on SEED and SEED-",
          "4. CONCLUSION": "framework based on self-attention, which is built exclusively"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "IV. The Stds of Eet with S+T attention achieve 0.59%/0.59%",
          "4. CONCLUSION": "on self-attention. Our approach considers the relationship be-"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "reductions on SEED and SEED-IV respectively compared",
          "4. CONCLUSION": "tween emotion and brain regions,\ntime series change as well"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "with those of 3D-CNN with PST-Attention. Moreover, EeT",
          "4. CONCLUSION": "as the intrinsic spatiotemporal characteristics of EEG signals."
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "with S+T attention gets superior performance compared with",
          "4. CONCLUSION": "The results of our methods show that the attention mechanism"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "other competitive models.",
          "4. CONCLUSION": "can boost\nthe performance of emotion recognition evidently."
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "",
          "4. CONCLUSION": "Furthermore, the simultaneous spatio-temporal attention gets"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "",
          "4. CONCLUSION": "the best results among the four designed structures, the result"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "",
          "4. CONCLUSION": "is also better than most state of the art methods, indicating that"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "",
          "4. CONCLUSION": "considering the spatio-temporal feature jointly and simultane-"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "",
          "4. CONCLUSION": "ously is more in line with the transmission law of EEG signals"
        },
        {
          "emotion recognition on SEED and SEED-IV datasets. Com-": "",
          "4. CONCLUSION": "in the human brain."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 3: presents the results of different variants of the",
      "data": [
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "[1] Danny Oude Bos et al.,\n“Eeg-based emotion recogni-"
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "tion,” The inﬂuence of visual and auditory stimuli, vol."
        },
        {
          "5. REFERENCES": "56, no. 3, pp. 1–17, 2006."
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "[2] Roddy Cowie, Ellen Douglas-Cowie, Nicolas Tsapat-"
        },
        {
          "5. REFERENCES": "soulis, George Votsis, Stefanos Kollias, Winfried Fel-"
        },
        {
          "5. REFERENCES": "lenz,\nand\nJohn G Taylor,\n“Emotion\nrecognition\nin"
        },
        {
          "5. REFERENCES": "IEEE Signal processing\nhuman-computer interaction,”"
        },
        {
          "5. REFERENCES": "magazine, vol. 18, no. 1, pp. 32–80, 2001."
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "[3] Hatice Gunes, Bj¨orn Schuller, Maja Pantic, and Roddy"
        },
        {
          "5. REFERENCES": "Cowie, “Emotion representation, analysis and synthesis"
        },
        {
          "5. REFERENCES": "in continuous space: A survey,”\nin 2011 IEEE Interna-"
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "tional Conference on Automatic Face & Gesture Recog-"
        },
        {
          "5. REFERENCES": "nition (FG). IEEE, 2011, pp. 827–834."
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "[4] Abeer Al-Nafjan, Manar Hosny, Areej Al-Wabil,\nand"
        },
        {
          "5. REFERENCES": "Yousef Al-Ohali,\n“Classiﬁcation of human emotions"
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "from electroencephalogram (eeg) signal using deep neu-"
        },
        {
          "5. REFERENCES": "ral network,”\nInt. J. Adv. Comput. Sci. Appl, vol. 8, no."
        },
        {
          "5. REFERENCES": "9, pp. 419–425, 2017."
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "[5]\nJinpeng Li, Zhaoxiang Zhang, and Huiguang He,\n“Hi-"
        },
        {
          "5. REFERENCES": "erarchical convolutional neural networks for eeg-based"
        },
        {
          "5. REFERENCES": "emotion recognition,” Cognitive Computation, vol. 10,"
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "no. 2, pp. 368–380, 2018."
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "[6] Dalin Zhang,\nLina Yao, Xiang Zhang,\nSen Wang,"
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "Weitong Chen, and Robert Boots,\n“Eeg-based inten-"
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "tion recognition from spatio-temporal\nrepresentations"
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "via cascade and parallel convolutional recurrent neural"
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "networks,” arXiv preprint arXiv:1708.06578, 2017."
        },
        {
          "5. REFERENCES": ""
        },
        {
          "5. REFERENCES": "[7] Vernon J Lawhern, Amelia J Solon, Nicholas R Way-"
        },
        {
          "5. REFERENCES": "towich, Stephen M Gordon, Chou P Hung, and Brent J"
        },
        {
          "5. REFERENCES": "Lance,\n“Eegnet:\na compact convolutional neural net-"
        },
        {
          "5. REFERENCES": "work for eeg-based brain–computer interfaces,” Journal"
        },
        {
          "5. REFERENCES": "of neural engineering, vol. 15, no. 5, pp. 056013, 2018."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "Javier Sanchez-Medina,\nand Adel M Alimi,\n“Opti-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "leymani,\nJong-Seok Lee, Ashkan Yazdani,\nTouradj"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "mized echo state network with intrinsic plasticity for",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Ebrahimi, Thierry Pun, Anton Nijholt, and Ioannis Pa-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "eeg-based emotion recognition,”\nin International Con-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "tras,\n“Deap: A database for emotion analysis; using"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "ference\non Neural\nInformation Processing. Springer,",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "IEEE transactions on affective\nphysiological signals,”"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "2017, pp. 718–727.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "computing, vol. 3, no. 1, pp. 18–31, 2011."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[9] Salma Alhagry, Aly Aly Fahmy, and Reda A El-Khoribi,",
          "[19] Sander Koelstra,": "[20]",
          "Christian Muhl, Mohammad\nSo-": "Johan AK Suykens and Joos Vandewalle, “Least squares"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "“Emotion recognition based on eeg using lstm recurrent",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "support vector machine classiﬁers,” Neural processing"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "neural network,” Emotion, vol. 8, no. 10, pp. 355–358,",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "letters, vol. 9, no. 3, pp. 293–300, 1999."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "2017.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "[21] Wei-Long Zheng,",
          "Christian Muhl, Mohammad\nSo-": "Jia-Yi Zhu, Yong Peng,\nand Bao-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[10] Pouya Bashivan,\nIrina Rish, Mohammed Yeasin,\nand",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Liang Lu, “Eeg-based emotion classiﬁcation using deep"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "Noel Codella, “Learning representations from eeg with",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "belief networks,”\nin 2014 IEEE International Confer-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "arXiv\ndeep recurrent-convolutional neural networks,”",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "ence on Multimedia and Expo (ICME). IEEE, 2014, pp."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "preprint arXiv:1511.06448, 2015.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "1–6."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[11] Youmin Kim and Ahyoung Choi,\n“Eeg-based emo-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "[22] Tengfei Song, Wenming Zheng, Peng Song, and Zhen"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "tion classiﬁcation using long short-term memory net-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Cui,\n“Eeg emotion recognition using dynamical graph"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "work with attention mechanism,”\nSensors, vol. 20, no.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "IEEE Transactions on\nconvolutional neural networks,”"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "23, pp. 6727, 2020.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Affective Computing, vol. 11, no. 3, pp. 532–541, 2018."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[12] Suyuan Liu, Wenming Zheng, Tengfei Song, and Yuan",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "[23] Yang Li, Wenming Zheng, Zhen Cui, Tong Zhang, and"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "Zong,\n“Sparse graphic attention lstm for eeg emotion",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Yuan Zong,\n“A novel neural network model based on"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "recognition,” in International Conference on Neural In-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "cerebral hemispheric asymmetry for eeg emotion recog-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "formation Processing. Springer, 2019, pp. 690–697.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "nition.,” in IJCAI, 2018, pp. 1561–1567."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[13] Alex Krizhevsky,\nIlya Sutskever, and Geoffrey E Hin-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "[24] Yang Li, Lei Wang, Wenming Zheng, Yuan Zong, Lei"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "ton,\n“Imagenet classiﬁcation with deep convolutional",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Qi, Zhen Cui, Tong Zhang,\nand Tengfei Song,\n“A"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "information pro-\nneural networks,” Advances in neural",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "novel bi-hemispheric discrepancy model\nfor eeg emo-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "cessing systems, vol. 25, pp. 1097–1105, 2012.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "IEEE Transactions on Cognitive and\ntion recognition,”"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[14] David E Rumelhart, Geoffrey E Hinton, and Ronald J",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Developmental Systems, vol. 13, no. 2, pp. 354–367,"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "Williams,\n“Learning\nrepresentations\nby\nback-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "2020."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "propagating errors,” nature, vol. 323, no. 6088, pp. 533–",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "[25]",
          "Christian Muhl, Mohammad\nSo-": "Jiyao Liu, Yanxi Zhao, Hao Wu, and Dongmei Jiang,"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "536, 1986.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "“Positional-spectral-temporal attention in 3d convolu-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[15] Heidi A Schlitt,\nL Heller,\nR Aaron,\nE Best,\nand",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "tional neural networks\nfor\neeg emotion recognition,”"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "DM Ranken, “Evaluation of boundary element methods",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Proc. APSIPA, 2021."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "for\nthe eeg forward problem:\neffect of\nlinear\ninterpo-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "lation,”\nIEEE transactions on biomedical engineering,",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "[26] Elham S Salama, Reda A El-Khoribi, Mahmoud E"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "vol. 42, no. 1, pp. 52–58, 1995.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Shoman, and Mohamed A Wahby Shalaby, “Eeg-based"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "emotion recognition using 3d convolutional neural net-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[16] Wei-Long Zheng and Bao-Liang Lu, “Investigating crit-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "works,” Int. J. Adv. Comput. Sci. Appl, vol. 9, no. 8, pp."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "ical\nfrequency bands and channels for eeg-based emo-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "329–337, 2018."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "IEEE\ntion recognition with deep neural networks,”",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "Transactions on Autonomous Mental Development, vol.",
          "[19] Sander Koelstra,": "[27]",
          "Christian Muhl, Mohammad\nSo-": "JX Chen, PW Zhang, ZJ Mao, YF Huang, DM Jiang,"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "7, no. 3, pp. 162–175, 2015.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "and YN Zhang,\n“Accurate eeg-based emotion recogni-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "tion on combined features using deep convolutional neu-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[17] Ruo-Nan Duan, Jia-Yi Zhu, and Bao-Liang Lu, “Differ-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "ral networks,”\nIEEE Access, vol. 7, pp. 44317–44328,"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "ential entropy feature for eeg-based emotion classiﬁca-",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "2019."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "tion,” in 2013 6th International IEEE/EMBS Conference",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "on Neural Engineering (NER). IEEE, 2013, pp. 81–84.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "[28] Laurens Van der Maaten and Geoffrey Hinton,\n“Visu-"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "Journal of machine learning\nalizing data using t-sne.,”"
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "[18] Wei-Long Zheng, Wei Liu, Yifei Lu, Bao-Liang Lu,",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": "research, vol. 9, no. 11, 2008."
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "and Andrzej Cichocki,\n“Emotionmeter: A multimodal",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "IEEE\nframework for\nrecognizing human emotions,”",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "transactions on cybernetics, vol. 49, no. 3, pp. 1110–",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        },
        {
          "[8] Rahma\nFourati,\nBoudour Ammar,\nChaouki Aouiti,": "1122, 2018.",
          "[19] Sander Koelstra,": "",
          "Christian Muhl, Mohammad\nSo-": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Eeg-based emotion recognition",
      "authors": [
        "Danny Oude"
      ],
      "year": "2006",
      "venue": "The influence of visual and auditory stimuli"
    },
    {
      "citation_id": "3",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "Roddy Cowie",
        "Ellen Douglas-Cowie",
        "Nicolas Tsapatsoulis",
        "George Votsis",
        "Stefanos Kollias",
        "Winfried Fellenz",
        "John Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal processing magazine"
    },
    {
      "citation_id": "4",
      "title": "Emotion representation, analysis and synthesis in continuous space: A survey",
      "authors": [
        "Hatice Gunes",
        "Björn Schuller",
        "Maja Pantic",
        "Roddy Cowie"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG)"
    },
    {
      "citation_id": "5",
      "title": "Classification of human emotions from electroencephalogram (eeg) signal using deep neural network",
      "authors": [
        "Abeer Al-Nafjan",
        "Manar Hosny",
        "Areej Al-Wabil",
        "Yousef Al-Ohali"
      ],
      "year": "2017",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "6",
      "title": "Hierarchical convolutional neural networks for eeg-based emotion recognition",
      "authors": [
        "Jinpeng Li",
        "Zhaoxiang Zhang",
        "Huiguang He"
      ],
      "year": "2018",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "7",
      "title": "Eeg-based intention recognition from spatio-temporal representations via cascade and parallel convolutional recurrent neural networks",
      "authors": [
        "Dalin Zhang",
        "Lina Yao",
        "Xiang Zhang",
        "Sen Wang",
        "Weitong Chen",
        "Robert Boots"
      ],
      "year": "2017",
      "venue": "Eeg-based intention recognition from spatio-temporal representations via cascade and parallel convolutional recurrent neural networks",
      "arxiv": "arXiv:1708.06578"
    },
    {
      "citation_id": "8",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "Amelia Vernon J Lawhern",
        "Nicholas Solon",
        "Waytowich",
        "Stephen M Gordon",
        "P Chou",
        "Brent Hung",
        "Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "9",
      "title": "Optimized echo state network with intrinsic plasticity for eeg-based emotion recognition",
      "authors": [
        "Rahma Fourati",
        "Boudour Ammar",
        "Chaouki Aouiti",
        "Javier Sanchez-Medina",
        "Adel Alimi"
      ],
      "year": "2017",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition based on eeg using lstm recurrent neural network",
      "authors": [
        "Salma Alhagry",
        "Aly Aly Fahmy",
        "Reda El-Khoribi"
      ],
      "year": "2017",
      "venue": "Emotion"
    },
    {
      "citation_id": "11",
      "title": "Learning representations from eeg with deep recurrent-convolutional neural networks",
      "authors": [
        "Pouya Bashivan",
        "Irina Rish",
        "Mohammed Yeasin",
        "Noel Codella"
      ],
      "year": "2015",
      "venue": "Learning representations from eeg with deep recurrent-convolutional neural networks",
      "arxiv": "arXiv:1511.06448"
    },
    {
      "citation_id": "12",
      "title": "Eeg-based emotion classification using long short-term memory network with attention mechanism",
      "authors": [
        "Youmin Kim",
        "Ahyoung Choi"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "13",
      "title": "Sparse graphic attention lstm for eeg emotion recognition",
      "authors": [
        "Suyuan Liu",
        "Wenming Zheng",
        "Tengfei Song",
        "Yuan Zong"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "14",
      "title": "Imagenet classification with deep convolutional neural networks",
      "authors": [
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Geoffrey Hinton"
      ],
      "year": "2012",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "15",
      "title": "Learning representations by backpropagating errors",
      "authors": [
        "Geoffrey David E Rumelhart",
        "Ronald Hinton",
        "Williams"
      ],
      "year": "1986",
      "venue": "nature"
    },
    {
      "citation_id": "16",
      "title": "Evaluation of boundary element methods for the eeg forward problem: effect of linear interpolation",
      "authors": [
        "Heidi Schlitt",
        "L Heller",
        "R Aaron",
        "E Best",
        "Ranken"
      ],
      "year": "1995",
      "venue": "IEEE transactions on biomedical engineering"
    },
    {
      "citation_id": "17",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "Wei-Long Zheng",
        "Bao-Liang Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "18",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "Jia-Yi Ruo-Nan Duan",
        "Bao-Liang Zhu",
        "Lu"
      ],
      "year": "2013",
      "venue": "2013 6th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "19",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "Wei-Long Zheng",
        "Wei Liu",
        "Yifei Lu",
        "Bao-Liang Lu",
        "Andrzej Cichocki"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "20",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "Sander Koelstra",
        "Christian Muhl",
        "Mohammad Soleymani",
        "Jong-Seok Lee",
        "Ashkan Yazdani",
        "Touradj Ebrahimi",
        "Anton Thierry Pun",
        "Ioannis Nijholt",
        "Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "21",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "A Johan",
        "Joos Suykens",
        "Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural processing letters"
    },
    {
      "citation_id": "22",
      "title": "Eeg-based emotion classification using deep belief networks",
      "authors": [
        "Wei-Long Zheng",
        "Jia-Yi Zhu",
        "Yong Peng",
        "Bao-Liang Lu"
      ],
      "year": "2014",
      "venue": "2014 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "23",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "Tengfei Song",
        "Wenming Zheng",
        "Peng Song",
        "Zhen Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "24",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Yang Li",
        "Wenming Zheng",
        "Zhen Cui",
        "Tong Zhang",
        "Yuan Zong"
      ],
      "year": "2018",
      "venue": "IJCAI"
    },
    {
      "citation_id": "25",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Yang Li",
        "Lei Wang",
        "Wenming Zheng",
        "Yuan Zong",
        "Lei Qi",
        "Zhen Cui",
        "Tong Zhang",
        "Tengfei Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "26",
      "title": "Positional-spectral-temporal attention in 3d convolutional neural networks for eeg emotion recognition",
      "authors": [
        "Jiyao Liu",
        "Yanxi Zhao",
        "Hao Wu",
        "Dongmei Jiang"
      ],
      "year": "2021",
      "venue": "Proc. APSIPA"
    },
    {
      "citation_id": "27",
      "title": "Eeg-based emotion recognition using 3d convolutional neural networks",
      "authors": [
        "Reda Elham S Salama",
        "Mahmoud El-Khoribi",
        "Mohamed A Wahby Shoman",
        "Shalaby"
      ],
      "year": "2018",
      "venue": "Int. J. Adv. Comput. Sci. Appl"
    },
    {
      "citation_id": "28",
      "title": "Accurate eeg-based emotion recognition on combined features using deep convolutional neural networks",
      "authors": [
        "Jx Chen",
        "Zhang",
        "Y Mao",
        "D Huang",
        "Jiang",
        "Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "29",
      "title": "Visualizing data using t-sne",
      "authors": [
        "Laurens Van Der Maaten",
        "Geoffrey Hinton"
      ],
      "year": "2008",
      "venue": "Journal of machine learning research"
    }
  ]
}