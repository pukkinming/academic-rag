{
  "paper_id": "2009.09585v1",
  "title": "A Novel Transferability Attention Neural Network Model For Eeg Emotion Recognition",
  "published": "2020-09-21T02:42:30Z",
  "authors": [
    "Yang Li",
    "Boxun Fu",
    "Fu Li",
    "Guangming Shi",
    "Wenming Zheng"
  ],
  "keywords": [
    "EEG emotion recognition",
    "transferable attention",
    "brain region"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The existed methods for electroencephalograph (EEG) emotion recognition always train the models based on all the EEG samples indistinguishably. However, some of the source (training) samples may lead to a negative influence because they are significant dissimilar with the target (test) samples. So it is necessary to give more attention to the EEG samples with strong transferability rather than forcefully training a classification model by all the samples. Furthermore, for an EEG sample, from the aspect of neuroscience, not all the brain regions of an EEG sample contains emotional information that can transferred to the test data effectively. Even some brain region data will make strong negative effect for learning the emotional classification model. Considering these two issues, in this paper, we propose a transferable attention neural network (TANN) for EEG emotion recognition, which learns the emotional discriminative information by highlighting the transferable EEG brain regions data and samples adaptively through local and global attention mechanism. This can be implemented by measuring the outputs of multiple brain-region-level discriminators and one single sample-level discriminator. We conduct the extensive experiments on three public EEG emotional datasets. The results validate that the proposed model achieves the state-of-the-art performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Emotion plays an important role in human daily life. It influences our rational decision-making, perception and cognition, and is essential in interpersonal communication  [1] . Thus, it is necessary to make machines to understand human emotions in the field of human-computer interaction (HCI). To this end, the technology of emotion recognition provides a possible way for computers to capture human emotions, which is the first step to improve and humanize the interaction between humans and machines.\n\nGenerally, emotion recognition measures the emotional states by analyzing the data of bodily reactions under emotional conditions  [2] . These reactions, including speech, facial expression and gesture, can adequately express our emotions under most circumstances. Nevertheless, these methods are subjective and cannot guarantee the authenticity of emotion  [3] . Except for the above external methods, the internal physiological variables tend to be much close to the real emotions. Human brain, as the source of all the reactions, Yang Li, Boxun Fu, Fu Li and Guangming Shi are with the Key Laboratory of Intelligent Perception and Image Understanding of Ministry of Education, the School of Artificial Intelligence, Xidian University, Xian, 710071, China.( * Corresponding author: Fu Li (E-mail: fuli@mail.xidian.edu.cn).)\n\nWenming Zheng is with the Key Laboratory of Child Development and Learning Science (Ministry of Education), School of Biological Sciences and Medical Engineering, Southeast University, Nanjing, Jiangsu, 210096, China. can reflect the mental activities including the emotion states. According to the studies of neurophysiology and psychology, EEG has the ability to record the brain neural activities, and can be used to decode the effective information of human emotional states  [4] ,  [5] . Consequently, EEG emotion recognition has received substantial attention from human-computer interaction and pattern recognition research communities in recent years  [6] ,  [7] ,  [8] .\n\nMost EEG emotion recognition methods focus on two major tasks, i.e., EEG feature extraction and classification. The first task aims at seeking the discriminative emotion-related information from the raw EEG signals. EEG emotional signals usually consist of many neural processes and hence present a highly heterogeneous and nonstationary behavior  [2] . Hence, how to extract the specific emotion information that contribute to the emotion recognition becomes a very important task. In  [9] , Jenke et al. summarized and evaluated all the existing EEG features extracted from time domain, frequency domain and time-frequency domain on their self-recorded EEG emotional dataset. The target of classification is modeling the correlation between the EEG emotional feature and the class labels, which leads to the interpretation of raw EEG emotional signals. Classification performance provides insight about how well a trained model can estimate the emotional state. Many advanced classification algorithms have been proposed over the years. For example, Zheng et al.  [10]  proposed a group sparse canonical correlation analysis method for simultaneous EEG channel selection and emotion recognition. Li et al.  [8]  fused the information propagation patterns and activation difference in the brain to improve emotional recognition. In  [11] , Alarcao and Fonseca summarized, reviewed and compared these works comprehensively.\n\nRecently, many domain adaptation methods have been proposed to deal with EEG emotion recognition, especially in the subject-independent task, where the source and target data come from different subjects. These methods have significantly advanced the EEG emotion recognition task. For example, Zheng and Lu  [12]  evaluated four different domain adaptation approaches including Transfer component analysis (TCA)  [13] , Kernel Principle Analysis (KPCA)  [14] , Transductive Support Vector Machine (T-SVM)  [15]  and Transductive Parameter Transfer (TPT)  [16]  on SEED dataset, and find that the accuracy can be improved by 20% compared with the generic classifier. Lan et al.  [17]  made a comparative study on several state-of-the-art domain adaptation techniques on two EEG emotional datasets and the experiment results show that using domain adaptation technique can improve the accuracy significantly by 7.25% and 13.40% compared with the baseline accuracy where no domain adaptation technique is used. In all the domain adaptation methods, the most well-established one is the domain adversarial neural network (DANN)  [18] , which constructs a two players mini-max game by using a domain discriminator that works adversarially with the feature extractor to generate the domain-invariable data representations. Li et al. adopted this setting and proposed a bi-hemisphere domain adversarial neural network (BiDANN) for EEG emotion recognition and achieved the state-of-the-art performance  [19] .\n\nNevertheless, we argue that the there are two issues need to be better addressed in EEG emotion recognition tasks. The first one is how to identify the positive EEG samples that consist of more emotion-related information. EEG emotional signals usually consist of many neural processes and are much vulnerable to negative effect of irrelevant knowledge, which incurs that some training EEG samples are significantly dissimilar with the test ones. Exploring how to highlight the positive EEG emotional samples and weaken the effect of negative samples will contribute more to emotion recognition. The second issue is how to weight the variability of different brain regions for EEG emotion recognition. Some studies of neuroscience have shown that different brain regions have different contributions for emotion expression  [20] . In an EEG emotional sample, it is obvious that not all the brain regions contain the knowledge of emotion that can be transferred to the test samples. Making a strategy to distinguish the transferable and nontransferable brain regions is helpful to improve EEG emotion recognition.\n\nTo this end, in this paper, we propose a transferable attention neural network (TANN) to deal with the above tranferability learning problem for EEG emotion recognition. This transferability of data can be measured by calculating from the outputs of domain discriminators. Specifically, for the domain adversarial neural network  [18] , the output of domain discriminator is the probability of input data belongs to source or target domain. When the probability approaches 0, it represents the input data belongs to source domain, while approaching 1 indicates that it belongs to the target domain. Therefore, TANN takes advantages of the domain discriminator to measure the transferability from the training data to test data. Concretely, the framework of TANN includes the following three major modules:\n\n• Feature extractor. The goal of feature extractor is to extract the high-level discriminative deep feature from raw EEG data for classification. EEG data is made of several electrodes that are set under the coordinates on the scalp, which are predefined referring to the locations of different brain regions. In the feature learning procedure, we should well retain this intrinsic structural information that will be helpful for classification. To achieve this, TANN employs two directional recurrent neural networks (RNN) that traverse all the electrodes from horizontal and vertical directions, which will construct a complete relationship and generate discriminative deep features for all the EEG electrodes.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Preliminary",
      "text": "In this section, we briefly overview the preliminary of transferable attention and then address how we can apply it to EEG emotion recognition.\n\nMost attention based methods focus on how to highlight or weaken different parts in an image according to their contribution for classification but neglect the evaluation for each training sample  [21] . It is known that not all the training samples are similar with the test. It will be a negative influence in the learning process if we feed the model with all the training samples forcefully. Transferable attention (TA) is designed to deal with this problem  [22] . When a training sample is much easier to be transferred to the test, it will be rewarded with more attention due to the high similarity with the test data, which is called transferable attention. Inspired by adversarial learning methods, this attention can be realized by calculating the outputs of the discriminator, which can reflect the similarity between training and test data.\n\nSince in EEG emotion recognition tasks, not all the training EEG data are useful in the process of learning a model, exploring the transferability of EEG data will be meaningful and can further improve EEG emotion recognition.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Iii. The Proposed Model For Eeg Emotion",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Recognition",
      "text": "To specify the proposed method clearly, we illustrate the framework of the proposed TANN model in Fig.  1 . TANN aims to distinguish which training samples are easy or hard to be transferred to test samples. Through penalizing these training samples, it can further improve EEG emotion recognition. Besides, considering not all the brain regions have the equal transferability, as well as measuring the similarity across EEG samples, TANN also focuses on the brain regions with high transferability. To achieve this goal, we adopt local and global attentions to the EEG emotion sample and its inside brain regions' data, respectively. These attention weights can be obtained from the outputs of multiple local and one global domain discriminators. Concretely, TANN consists of three major modules, i.e., feature extractor, attention layers, and classifier. In the following, we illustrate these parts detailedly.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "A. Feature Extractor",
      "text": "The process of feature extraction is depicted in Fig.  2 , and the goal is to represent the EEG emotional data in a more discriminative feature space so as to improve the EEG classification performance. The EEG deep features are extracted by two directional RNN modules that traverse the spatial regions under two predefined stacks, which are determined with respect to horizontal and vertical directions. These two directional RNNs are complementary to construct a complete relationship of electrodes locations that avoid losing the intrinsic structural information of EEG data. By doing this, we can obtain the high-level features for each EEG electrode that facilitate to construct the brain regions' features. Concretely, for an EEG sample\n\nwhere d and n are the dimension and number of EEG electrode, the above process can be formulated as\n\nwhere s • i is the hidden unit of the RNN module as well as the data representation for the electrode x i , and\n\n} are the learnable transformation matrices of RNN module; σ(•) denotes the nonlinear operation such as Sigmoid function; and N (x • i ) denotes the set of predecessors of node x • i . Due to that TANN consists of horizontal and vertical directional RNNs to represent EEG electrode, we can obtain the data representations that not only contain the information of the electrodes itself but also the nearby relationship. Specifically, it can be expressed as S h = {s h i } that contains the information from left and right electrodes, and S v = {s v i } that includes the information from up and down electrodes. To integrate these spatial information into a overall representation, we arrange the order of the columns of S h and S v , and use two transformation matrices P and Q to obtain the deep features H = {h k } for all the electrodes, in which\n\nHere h i is the deep representation of electrode x i that kepdf the location structural relation, d f is the dimension.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Attention Layers",
      "text": "For EEG emotion samples, there is a large distribution gap between training and test data. Some training samples are very dissimilar with the test ones. Therefore, to avoid training a model with all the source samples indiscriminately, TANN measures the transferability of all the training samples and then strengthen or weaken them in the learning process of the model. Besides, as we know, for emotion recognition, not all the brain regions of an EEG sample contains emotional information that can transferred to the test data effectively. Some brain regions are more transferable than the others. Due to this, TANN not only employs a global attention layer to weight the sample-level transferability but also a local attention layer as a complement to focus on the brain-regionlevel transferability. Specifically, the transferability is quantified by the entropy of the outputs of domain discriminator. The domain discriminator can generate the probability of confusion between source (training) and target (test) data. When the probability approaches 0.5, it indicates that the input has good ability to confuse the domain discriminator, which nicely meet our need to highlight the data with positive transferability. In the following, we will demonstrate how to achieve the local and global attentions by transferability learning.\n\n1) Local transferable attention on brain-region-level: After obtaining the data representation h i of each electrode of X, TANN employs local attention to highlight the brain regions with high transferability. Here we first group the electrodes into several clusters according to the associated brain region locations, which can be formulated as brain region 1:\n\nwhere N is the number of brain regions, n c denotes the number of electrodes in the c-th brain region,\n\nIn this case, the reordered deep feature can be expressed as\n\nBased on the above process, we can obtain the deep features of all the brain regions from source and target EEG samples, which can be denoted as are the probabilities that the input belongs to the source and target data, respectively. Then we can quantity the transferability of this brain region through the entropy function in information theory  [22] , which is defined as\n\nThen the higher transferability of a brain region has, the more attention value is. However, for an EEG signal, the emotion information is the most difficult component to be transferred. Due to this, we reverse the attention values for the brain regions to make the model pay attention on the difficult transferred brain regions. Thus the attention value for brain region N i is defined as\n\nBesides, to mitigate the negative effect of wrong attentions, we adopt the residual attention mechanism to make the model more robust. Thus, after local attention layer, the data representations for EEG sample X can be formulated as\n\nHere the loss function of the local discriminators for all the brain regions can be formulated as\n\nwhere\n\nlogp(1|X\n\ndenote the loss of the local discriminator for brain region N i ; p(0|X\n\n) and p(1|X\n\n) are the probabilities of the input data belongs to source and target domains respectively; θ l N i d is the parameter of the local attention network; X S N i t and X\n\nT N i t represent the N i brain region data of the t-th and t -th source and target sample, respectively; M 1 and M 2 are the number of the source and target data.\n\n2) Global transferable attention on sample-level: Although the above local attention for all the brain regions can make a fine-grained transfer learning between the source and target domain data, there is a possible that the local domain discriminator find fewer brain regions to transfer. Meanwhile, due to the distribution difference, there are some negative samples in the source data that are very dissimilar with the target data. It will weak the efficiency If we force training the model with these negative samples equaling with the other positive samples. Hence, after weighting the transferability of brain regions with local attention, we adopt the global transferable attention on the sample-level to transfer the knowledge from source to target domain.\n\nConcretely, after local attention module, the input feature can be expressed as\n\nwhere S is a learnable transformation matrix. Then it is sent to a global discriminator\n\nto highlight the EEG samples with higher transferability, where θ g d is the parameter of the global attention network. Concretely, let d = {d s , d t } denote the output probability of the global discriminator, where d s and d t are the probabilities that the input belongs to the source and target data respectively. The global attention value w can be calculated as\n\nHere we also adopt the residual mechanism to avoid the wrong attention. In this case, we obtain that the more transferability is, the larger attention value w is.\n\nInspired by Long et al.  [23] , the entropy minimization principle can refine the classifier adaptation, which can increase the confidence of the classifier prediction. Thus, we utilize the global domain discriminator to generate the global attention values acting on the label entropy to enhance the certainty of the source samples that are more similar with the target samples. Then w is embedded into the label entropy loss to achieve the function for global attention. Hence the loss function of the label entropy, which is called attentive entropy loss, can be written as  (15)  where X k is the k-th sample in {X S , X T }; w is the global attention value for EEG sample X k ; and C is the number of emotion classes.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Classifier",
      "text": "To enhance the discriminative ability of the model, we add the classifier to TANN model. Concretely, based on the final feature vector H in Eq. (  11 ), we first arrange the matrix H into a vector h, and then use the simple linear transform approach to predict the class label, which can be formulated as\n\nwhere G and b c are the transformation matrices. Finally, the output vector O is fed into the softmax layer for emotion classification, which can be written as\n\nwhere p(c|X t ) denotes the predicted probability that the input sample X t belongs to the c-th class. As a result, the label l of sample X t is predicted as\n\nHence, the loss function of the classifier can be expressed as\n\nwhere θ c denotes the parameter of the classifier.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. The Optimization",
      "text": "In summary, the overall loss function includes four parts, i.e., local and global discriminator losses, classifier loss and the attentive entropy loss. Concretely, the loss function of the proposed TANN method can be formulated as\n\nwhere α and β are the hyper-parameters, L l N i d and L g d represent the losses of local and global attention discriminators. Then we iteratively optimize the classifier, attentive entropy, local and global attention discriminators. Concretely, the parameters can be found through minimizing and maximizing\n\nThe above maximization problem, i.e., Eq. (  23 ) and (  24 ), can be transferred to a minimization problem through adopting a gradient reversal layer (GRL)  [18]  before the discriminator, which will act as an identity transform in the forwardpropagation but reverse the gradient sign while performing the back-propagation operation. Then we can use the stochastic gradient decent (SGD) algorithm to solve the parameter optimization process easily. Specifically, the parameters can be updated by the rules below",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Iv. Experiments A. Datasets And Settings",
      "text": "To evaluate the proposed TANN method adequately, we conduct the experiments on three public EEG emotion datasets, namely,\n\n(1) SEED  [7]  dataset is a standard benchmark for EEG emotion recognition. It contains three types of emotions, i.e., happy, neutral and sad, from 15 subjects' EEG emotional signals.\n\n(2) SEED-IV 1    [24]  dataset includes four types of emotions from 15 subjects. Compared with SEED, it contains an extra emotion fear. (3) MPED 1  [25]  dataset includes seven refined emotion types, i.e., joy, funny, neutral, sad, fear, disgust and anger from 30 subjects. On these datasets, we design two kinds of EEG emotion recognition experiments including the subject-dependent and subject-independent ones. Table  I  summarizes the number of training and test samples, and the experimental protocols used in the experiments. The concrete protocols are described as follows:\n\n• The subject-dependent experiment -In this experiment, the training and test data come from the same subject but different trials. We adopt the same protocols as  [7] ,  [24]  and  [26] . Namely, for SEED, we use the former nine trials of EEG data per session of each subject as source (training) domain data while using the remaining six trials per session as target (test) domain data; for SEED-IV, we use the first sixteen trials per session of each subject as the training data, and the last eight trials containing all emotions (each emotion with two trials) as the test data; for MPED, we use twenty-one trials of EEG data as training data and the rest seven trials consisting of seven emotions as test data for each subject. The mean accuracy (ACC) and standard deviation (STD) are used as the evaluation criteria for all the subjects in the dataset. • The subject-independent experiment -In this experiment, the training and test data come from different subjects, which is a harder task than the above subjectdependent one but more conductive to practical applications. We adopt the leave-one-subject-out (LOSO) crossvalidation strategy  [12]  to evaluate the proposed TANN model. LOSO strategy uses the EEG signals of one subject as test data and the rest subjects' EEG signals as training data. This procedure is repeated such that the EEG signals of each subject will be used as test data once. Again, the mean accuracy (ACC) and standard deviation (STD) are used as the evaluation criteria. Besides, we use the released handcraft features, namely, the differential entropy (DE) in SEED and SEED-IV, and the Short-Time Fourier Transform (STFT) in MPED, as the input to feed our model. Thus the sizes d×n of the input sample X t are 5 × 62, 5 × 62 and 1 × 62 for these three datasets, respectively. Moreover, in the experiment, we respectively set the dimension d f and d f of the feature extractor to 32; the number of brain region N to 16 2 ; the dimension n of the input for the global attention layer to 6; the hyper-parameters α and β are both set to 0.1 throughout the experiment. Specifically, we implemented TANN using TensorFlow 3 on one Nvidia 1080Ti GPU. The learning rate, momentum and weight decay rate are set as 0.003, 0.9 and 0.95, respectively. The network is trained using SGD with batch size of 200.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Experiment Results",
      "text": "To validate the classification superiority of TANN, we also conduct the same experiments using various existed methods. Recall that the distribution gap in the subject-independent task is much larger than that in the subject-dependent one. In this case, domain adaptation methods shall be properly employed in order to achieve promising performance. Therefore, in the experiment on subject-independent task, we include many domain adaptation methods in the comparison. By doing so, we can effectively validate the state-of-the-art performance of our method. The comparable methods are listed as follows:\n\n• Two baseline methods: linear support vector machine (SVM)  [28] , and random forest (RF)  [29] ;\n\n2 Concretely, the brain regions include Pre-Frontal (AF3, FP1, FPZ, FP2, AF4), Frontal (F3, F1, FZ, F2, F4), Left Frontal (F7, F5), Right Frontal (F8, F6), Left Temporal (FT7, FC5, T7, C5, TP7, CP5), Right Temporal (FT8, FC6, T8, C6, TP8, CP6), Frontal Central (FC3, FC1, FCZ, FC2, FC4), Central (C3, C1, CZ, C2, C4), Central Parietal (CP3, CP1, CPZ, CP2, CP4), Left Parietal (P7, P5), Right Parietal (P8, P6), Parietal (P3, P1, PZ, P2, P4), Left Parietal Occipital (PO7, PO5, CB1), Right Parietal Occipital (PO8, PO6, CB2), Parietal Occipital (PO3, POZ, PO4), Occipital (O1, OZ, O2) lobes.\n\n3 https://www.tensorflow.org/ • Three subspace learning methods: canonical correlation analysis (CCA)  [30] , group sparse canonical correlation analysis (GSCCA)  [31] , and graph regularization sparse linear regression (GRSLR)  [32] ; • Six transfer subspace learning methods: Kullback-Leibler importance estimation procedure (KLIEP)  [33] , unconstrained least-squares importance fitting (ULSIF)  [34] , selective transfer machine (STM)  [35] , transfer component analysis (TCA)  [13] , subspace alignment (SA)  [36] , and geodesic flow kernel (GFK)  [37] ; • Seven recent deep learning methods: deep believe network (DBN)  [7] , graph convolutional neural network (GCNN)  [38] , dynamical graph convolutional neural network (DGCNN)  [25] , domain adversarial neural networks (DANN)  [18] , bi-hemisphere domain adversarial neural network (BiDANN)  [39] , EmotionMeter  [24] , and attention-long short-term memory (A-LSTM)  [26] .\n\nAll the methods are representative ones in the previous studies of emotion recognition. We directly quote (or reproduce) their results from the literature to ensure a convincing comparison with the proposed method.\n\nThe results are summarized in Table  II  and III. Note that the subspace based methods, such as TCA, SA and GFK, are problematic to handle a large amount of EEG data due to the computer memory limitation and computational issue. Therefore, to compare with them, we have to randomly select 3000 EEG feature samples from the training data set to train these methods. Besides, the comparable methods adopting domain adaptation technique train the model with labeled training data and unlabeled test data as TANN does. From Table  II  and III, we have three observations:\n\n(1) The proposed TANN model outperforms all the comparable methods on all the three datasets. Especially on SEED-IV dataset, the mean improvement is about 3.4% and 2.5% over the state-of-the-art methods A-LSTM and BiDANN. It verifies the learned transferable data representation are useful for EEG emotion recognition.\n\n(2) The proposed TANN is superior to the recent domain adaptation methods. TANN has an improvement of 1.0%, 3.7% and 2.1% for subject-dependent task in Table  II , and 1.2%, 2.4% and 2.5% for subjectindependent task in Table  III  than the BiDANN method, which also adopts domain adversarial learning strategy to train the model. This reveals that the local and global attention structures are helpful to learn the discriminative information for emotion recognition. (3) Even under the same classification models, the performance of the subject-independent tasks are quite lower than the subject-dependent ones. It is clear to see the gaps on three datasets are about 13%, 5% and 12%, respectively. This reveals that the individual difference is a negative influence on EEG emotion recognition, and should be mitigated in the subject-independent task.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Discussion",
      "text": "1) The confusion of different emotions based on TANN model: To better understand the confusion of TANN in recognizing different emotions, we depict the confusion matrices of subject-dependent and subject-independent EEG emotion recognition experiments in Fig.  3  and 4 , respectively, from which we have the following observations:\n\n(1) In Fig.  3 , for SEED, the classification accuracies for three emotions are about 90%, and the happy and neutral emotions are easier to be recognized than the sad emotion; for SEED-IV, which consists of four emotions, we can see the negative emotions, i.e., sad and fear, are confused by the classifier with higher possibility; and for MPED, the confusion is more complex because it has more emotions than the other two datasets. It is obvious to see that the funny emotion is the easiest to be recognized and has 16% more than the neutral emotion on the second place. Except this, we can find that the funny and joy are easier to be confused maybe because both of them are positive emotions.\n\n(2) From the results of subject-independent EEG emotion recognition experiment in Fig.  4 , we can observe that, for SEED, which has three types of emotions, the happy emotion is much easier to be recognized than neutral and sad; for SEED-IV, the neutral and sad emotions are much easier to be recognized; for MPED, which is a hard seven classification problem, the accuraries of funny, neutral and anger emotions overpass that of the other emotions, and this reveals that we should focus on the joy, sad, fear and disgust emotion data in the task of classifying seven emotions.  2) The transferability of different brain regions: To investigate the transferability of different brain regions for EEG emotion recognition, we visualize all the brain regions by mapping the local attention values w in Eq. (  7 ) into the corresponding electrodes. The obtained results are shown in Fig.  5 , from which we have two observations:\n\n(1) The left and right temporal lobes make more important contribution for emotion recognition in all the three datasets, which coincides with the previous EEG emotion studies  [6] ,  [7] . This also reveals that, as well as the proposed model can adaptively give attention to different brain regions, it is still effective to capture the most important ones. (2) The activation areas are slightly different across datasets.\n\nFor example, there is a broader activation to the temporal lobes for SEED-IV compared with SEED. And for MPED, which consists of more types of emotions, the occipital lobe, as well as the temporal lobe, contributes more for emotion expression. 3) Ablation study: To see the importance of each module of TANN for EEG emotion recognition, we conduct an ablation study by removing the local and global attention layers both and separately. These reduced models are depicted in Fig.  6 , which includes The above results verify the effectiveness of the three important modules in TANN.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we propose a transferable attention neural network (TANN) to deal with EEG emotion recognition problem, which is motivated by the finding that not all the training samples have the equal contribution for emotion recognition, which also happens for the importance of different brain regions in this sample. TANN has the ability to learn the positive and negative information from the sample-level and brain-region-level, which can improve EEG emotion recognition. The proposed framework is easy to implement and the extensive experiments on three public EEG emotion datasets demonstrated that the proposed TANN method achieves the state-of-the-art performance. Besides, based on TANN, we also investigate the transferability of different brain regions in EEG emotion recognition and find that the temporal lobe and occipital lobe contribute more for emotion expression. In the future work, we will further investigate more operations for learning the transferability information to explore the potential efficacy of transferable attention for EEG emotion recognition.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The framework of TANN. TANN consists of two major modules, i.e., local and global attentions, that can make the",
      "page": 3
    },
    {
      "caption": "Figure 2: The process of feature extraction. We ﬁrst extract the",
      "page": 3
    },
    {
      "caption": "Figure 3: and 4, respectively, from",
      "page": 7
    },
    {
      "caption": "Figure 3: , for SEED, the classiﬁcation accuracies for",
      "page": 7
    },
    {
      "caption": "Figure 4: , we can observe that,",
      "page": 7
    },
    {
      "caption": "Figure 3: The confusion matrices based on the subject-dependent",
      "page": 7
    },
    {
      "caption": "Figure 4: The confusion matrices based on the subject-",
      "page": 8
    },
    {
      "caption": "Figure 5: , from which we have two observations:",
      "page": 8
    },
    {
      "caption": "Figure 5: The transferability of different EEG brain regions.",
      "page": 8
    },
    {
      "caption": "Figure 6: The frameworks of the reduced models of TANN: (a)",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "SEED",
          "Training": "2010",
          "Test": "1384",
          "Protocol": "[Zheng and Lu][7]"
        },
        {
          "Dataset": "Session 1\nSEED-IV\nSession 2\nSession 3",
          "Training": "561\n550\n576",
          "Test": "290\n282\n246",
          "Protocol": "[Zheng et al.][24]"
        },
        {
          "Dataset": "MPED",
          "Training": "2520",
          "Test": "840",
          "Protocol": "[Song et al.][27]"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "SEED",
          "Training": "47516",
          "Test": "3394",
          "Protocol": "[Zheng et al.][12]"
        },
        {
          "Dataset": "Session 1\nSEED-IV\nSession 2\nSession 3",
          "Training": "11914\n11648\n11508",
          "Test": "851\n832\n822",
          "Protocol": "LOSO∗"
        },
        {
          "Dataset": "MPED",
          "Training": "97440",
          "Test": "3360",
          "Protocol": "LOSO∗"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "ACC / STD (%)": "SEED"
        },
        {
          "Method": "KLIEP [33]",
          "ACC / STD (%)": "45.71/17.76"
        },
        {
          "Method": "ULSIF [34]",
          "ACC / STD (%)": "51.18/13.57"
        },
        {
          "Method": "STM [35]",
          "ACC / STD (%)": "51.23/14.82"
        },
        {
          "Method": "SVM [28]",
          "ACC / STD (%)": "56.73/16.29"
        },
        {
          "Method": "TCA [13]",
          "ACC / STD (%)": "63.64/14.88"
        },
        {
          "Method": "SA [36]",
          "ACC / STD (%)": "69.00/10.89"
        },
        {
          "Method": "GFK [37]",
          "ACC / STD (%)": "71.31/14.09"
        },
        {
          "Method": "A-LSTM [27]",
          "ACC / STD (%)": "72.18/10.85∗"
        },
        {
          "Method": "DANN [18]",
          "ACC / STD (%)": "75.08/11.18"
        },
        {
          "Method": "DGCNN [25]",
          "ACC / STD (%)": "79.95/09.02"
        },
        {
          "Method": "DAN [40]",
          "ACC / STD (%)": "83.81/08.56"
        },
        {
          "Method": "BiDANN [39]",
          "ACC / STD (%)": "83.28/09.60"
        },
        {
          "Method": "TANN",
          "ACC / STD (%)": "84.41/08.75"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "ACC / STD (%)": "SEED"
        },
        {
          "Method": "SVM [28]",
          "ACC / STD (%)": "83.99/09.72"
        },
        {
          "Method": "RF [29]",
          "ACC / STD (%)": "78.46/11.77"
        },
        {
          "Method": "CCA [30]",
          "ACC / STD (%)": "77.63/13.21"
        },
        {
          "Method": "GSCCA [31]",
          "ACC / STD (%)": "82.96/09.95"
        },
        {
          "Method": "DBN [7]",
          "ACC / STD (%)": "86.08/08.34"
        },
        {
          "Method": "GRSLR [32]",
          "ACC / STD (%)": "87.39/08.64"
        },
        {
          "Method": "GCNN [38]",
          "ACC / STD (%)": "87.40/09.20"
        },
        {
          "Method": "DGCNN [25]",
          "ACC / STD (%)": "90.40/08.49"
        },
        {
          "Method": "DANN [18]",
          "ACC / STD (%)": "91.36/08.30"
        },
        {
          "Method": "BiDANN [39]",
          "ACC / STD (%)": "92.38/07.04"
        },
        {
          "Method": "EmotionMeter\n[24]",
          "ACC / STD (%)": "−"
        },
        {
          "Method": "A-LSTM [27]",
          "ACC / STD (%)": "88.61/10.16∗"
        },
        {
          "Method": "TANN",
          "ACC / STD (%)": "93.34/06.64"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Method": "",
          "ACC / STD (%)": "SEED"
        },
        {
          "Method": "TANN-R1",
          "ACC / STD (%)": "87.06/09.45"
        },
        {
          "Method": "TANN-R2",
          "ACC / STD (%)": "89.73/07.53"
        },
        {
          "Method": "TANN-R3",
          "ACC / STD (%)": "91.03/07.63"
        },
        {
          "Method": "TANN",
          "ACC / STD (%)": "93.34/06.64"
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "A review on nonlinear methods using electroencephalographic recordings for emotion recognition",
      "authors": [
        "B García-Martínez",
        "A Martinez-Rodrigo",
        "R Alcaraz",
        "A Fernández-Caballero"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "3",
      "title": "Accurate eeg-based emotion recognition on combined features using deep convolutional neural networks",
      "authors": [
        "J Chen",
        "P Zhang",
        "Z Mao",
        "Y Huang",
        "D Jiang",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "4",
      "title": "Music and emotion: electrophysiological correlates of the processing of pleasant and unpleasant music",
      "authors": [
        "D Sammler",
        "M Grigutsch",
        "T Fritz",
        "S Koelsch"
      ],
      "year": "2007",
      "venue": "Psychophysiology"
    },
    {
      "citation_id": "5",
      "title": "Investigating models of affect: relationships among eeg alpha asymmetry, depression, and anxiety",
      "authors": [
        "D Mathersul",
        "L Williams",
        "P Hopkinson",
        "A Kemp"
      ],
      "year": "2008",
      "venue": "Emotion"
    },
    {
      "citation_id": "6",
      "title": "Eeg-based emotion recognition in music listening",
      "authors": [
        "Y.-P Lin",
        "C.-H Wang",
        "T.-P Jung",
        "T.-L Wu",
        "S.-K Jeng",
        "J.-R Duann",
        "J.-H Chen"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "7",
      "title": "Investigating critical frequency bands and channels for eeg-based emotion recognition with deep neural networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Autonomous Mental Development"
    },
    {
      "citation_id": "8",
      "title": "Eeg based emotion recognition by combining functional connectivity network and local activations",
      "authors": [
        "P Li",
        "H Liu",
        "Y Si",
        "C Li",
        "F Li",
        "X Zhu",
        "X Huang",
        "Y Zeng",
        "D Yao",
        "Y Zhang"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "9",
      "title": "Feature extraction and selection for emotion recognition from eeg",
      "authors": [
        "R Jenke",
        "A Peer",
        "M Buss"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "W Zheng"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "11",
      "title": "Emotions recognition using eeg signals: a survey",
      "authors": [
        "S Alarcao",
        "M Fonseca"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "12",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "13",
      "title": "Domain adaptation via transfer component analysis",
      "authors": [
        "S Pan",
        "I Tsang",
        "J Kwok",
        "Q Yang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Neural Networks"
    },
    {
      "citation_id": "14",
      "title": "Nonlinear component analysis as a kernel eigenvalue problem",
      "authors": [
        "B Schölkopf",
        "A Smola",
        "K.-R Müller"
      ],
      "year": "1998",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "15",
      "title": "Large scale transductive svms",
      "authors": [
        "R Collobert",
        "F Sinz",
        "J Weston",
        "L Bottou"
      ],
      "year": "2006",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "16",
      "title": "We are not all equal: Personalizing models for facial expression analysis with transductive parameter transfer",
      "authors": [
        "E Sangineto",
        "G Zen",
        "E Ricci",
        "N Sebe"
      ],
      "year": "2014",
      "venue": "Proceedings of the 22nd ACM international conference on Multimedia (MM)"
    },
    {
      "citation_id": "17",
      "title": "Domain adaptation techniques for eeg-based emotion recognition: a comparative study on two public datasets",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "R Scherer",
        "G Müller-Putz"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "18",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "19",
      "title": "A bihemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Decoding the nature of emotion in the brain",
      "authors": [
        "P Kragel",
        "K Labar"
      ],
      "year": "2016",
      "venue": "Trends in Cognitive Sciences"
    },
    {
      "citation_id": "21",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "L Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention is all you need"
    },
    {
      "citation_id": "22",
      "title": "Transferable attention for domain adaptation",
      "authors": [
        "X Wang",
        "L Li",
        "W Ye",
        "M Long",
        "J Wang"
      ],
      "year": "2019",
      "venue": "Transferable attention for domain adaptation"
    },
    {
      "citation_id": "23",
      "title": "Unsupervised domain adaptation with residual transfer networks",
      "authors": [
        "M Long",
        "H Zhu",
        "J Wang",
        "M Jordan"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "24",
      "title": "Emotionmeter: A multimodal framework for recognizing human emotions",
      "authors": [
        "W.-L Zheng",
        "W Liu",
        "Y Lu",
        "B.-L Lu",
        "A Cichocki"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "25",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "authors": [
        "T Song",
        "W Zheng",
        "C Lu",
        "Y Zong",
        "X Zhang",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "27",
      "title": "Mped: A multi-modal physiological emotion database for discrete emotion recognition",
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "28",
      "title": "Least squares support vector machine classifiers",
      "authors": [
        "J Suykens",
        "J Vandewalle"
      ],
      "year": "1999",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "29",
      "title": "Random forests",
      "authors": [
        "L Breiman"
      ],
      "year": "2001",
      "venue": "Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Canonical correlation analysis",
      "authors": [
        "B Thompson"
      ],
      "year": "2005",
      "venue": "Encyclopedia of Statistics in Behavioral Science"
    },
    {
      "citation_id": "31",
      "title": "Multichannel eeg-based emotion recognition via group sparse canonical correlation analysis",
      "authors": [
        "W Zheng"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "32",
      "title": "Eeg emotion recognition based on graph regularized sparse linear regression",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "S Ge"
      ],
      "year": "2018",
      "venue": "Neural Processing Letters"
    },
    {
      "citation_id": "33",
      "title": "Direct importance estimation with model selection and its application to covariate shift adaptation",
      "authors": [
        "M Sugiyama",
        "S Nakajima",
        "H Kashima",
        "P Buenau",
        "M Kawanabe"
      ],
      "year": "2008",
      "venue": "Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "34",
      "title": "A least-squares approach to direct importance estimation",
      "authors": [
        "T Kanamori",
        "S Hido",
        "M Sugiyama"
      ],
      "year": "2009",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "35",
      "title": "Selective transfer machine for personalized facial expression analysis",
      "authors": [
        "W.-S Chu",
        "F De La Torre",
        "J Cohn"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "36",
      "title": "Unsupervised visual domain adaptation using subspace alignment",
      "authors": [
        "B Fernando",
        "A Habrard",
        "M Sebban",
        "T Tuytelaars"
      ],
      "year": "2013",
      "venue": "IEEE International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "37",
      "title": "Geodesic flow kernel for unsupervised domain adaptation",
      "authors": [
        "B Gong",
        "Y Shi",
        "F Sha",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "38",
      "title": "Convolutional neural networks on graphs with fast localized spectral filtering",
      "authors": [
        "M Defferrard",
        "X Bresson",
        "P Vandergheynst"
      ],
      "year": "2016",
      "venue": "Advances in Neural Information Processing Systems (NIPS)"
    },
    {
      "citation_id": "39",
      "title": "A novel neural network model based on cerebral hemispheric asymmetry for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Z Cui",
        "T Zhang",
        "Y Zong"
      ],
      "year": "2018",
      "venue": "International Joint Conference on Artificial Intelligence (IJCAI)"
    },
    {
      "citation_id": "40",
      "title": "Cross-subject emotion recognition using deep adaptation networks",
      "authors": [
        "H Li",
        "Y.-M Jin",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2018",
      "venue": "International Conference on Neural Information Processing"
    }
  ]
}