{
  "paper_id": "2205.05572v2",
  "title": "Face Detection On Mobile: Five Implementations And Analysis",
  "published": "2022-05-11T15:39:21Z",
  "authors": [
    "Kostiantyn Khabarlak"
  ],
  "keywords": [
    "Face Detection",
    "Inference Time",
    "Mobile Device",
    "Smartphone",
    "Neural Network",
    "Edge Computing"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In many practical cases face detection on smartphones or other highly portable devices is a necessity. Applications include mobile face access control systems, driver status tracking, emotion recognition, etc. Mobile devices have limited processing power and should have long-enough battery life even with face detection application running. Thus, striking the right balance between algorithm quality and complexity is crucial. In this work we adapt 5 algorithms to mobile. These algorithms are based on handcrafted or neural-network-based features and include: Viola-Jones (Haar cascade), LBP, HOG, MTCNN, BlazeFace. We analyze inference time of these algorithms on different devices with different input image resolutions. We provide guidance, which algorithms are the best fit for mobile face access control systems and potentially other mobile applications. Interestingly, we note that cascaded algorithms perform faster on scenes without faces, while BlazeFace is slower on empty scenes. Exploiting this behavior might be useful in practice.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Low face detection inference time is crucial for mobile applications, such as driver sleepiness tracking, emotion recognition or mobile face access controls systems  [1] ,  [2] . However, many of the recent research papers focus on algorithm quality, disregarding inference time. This makes many of the novel high-quality algorithms inapplicable on mobile.\n\nIn this work we provide some guidance on the algorithms best applicable to smartphones and other low-power portable devices. We have designed a special Android Citation: K.  Khabarlak   testing application, where we implement and analyze performance of 5 algorithms on mobile devices. The algorithms include well-known algorithms based on handcrafted features, such as Viola-Jones (Haar cascade), LBP, HOG, and more recent neural-network-based approaches, like MTCNN, BlazeFace. The analysis includes tests on low-and high-end devices, at different input image resolutions and on scenes with different number of faces. Interestingly, we find that inference time of algorithms typically depends on number of faces on the image. Based on the analysis conducted, recommendations on which algorithms should be preferred are provided.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Algorithms Under Consideration",
      "text": "We start with older, more established face detection methods. These algorithms are based on handcrafted features and a classifier (commonly, tree-based) that decides, which patch of an image contains a face. Only the classifier and not feature extraction logic is learned during training process. The first method is Haar cascade classifier, also known as Viola-Jones face detector  [3] . At the time of its inception it proposed one of the best quality to inference time ratios. To perform face detection, the image is split into overlapping sub-windows. Haar features combined with AdaBoost classifier are used to perform classification on each of the windows. Each window is then assigned label \"definitely not a face\" or \"possibly a face\". This allows to quickly reject regions that do not contain faces. Regions that possibly contain a face are then fed into the next detection stage. Thus, forming a cascade. Most face regions are rejected at early stages, which makes the algorithm fast. In addition, the authors have proposed a faster way to compute Haar filters for the whole picture by constructing the so-called integral image.\n\nOther well-known face detection algorithms include Local Binary Pattern (LBP) method. The algorithm follows the idea of Viola and Jones, but uses LBP features that are faster to compute. Both Haar cascade and LBP algorithms are available in OpenCV  [4]  computer vision library. Another face detection method is based on Histogram of Ori-ented Gradients (HOG). First introduced in  [5]  for human detection and later adapted to face detection. HOG features are combined with SVM classifier. This algorithm is implemented in Dlib library  [6] .\n\nIn contrast to the above-mentioned methods, recent neural-network-based algorithms do not split the image into-subregions. The whole image is processed at once, which is more efficient and enables prediction of bounding boxes of non-standard sizes. The problem is then framed as combined regression and classification. To get coordinates of face bounding boxes regression is used, while classification predicts probability that certain bounding box contains a face. We have selected MTCNN  [7]  and BlazeFace  [8]  algorithms for this section. We have adapted PyTorch 3.9  [9]  implementations of these algorithms to a smartphone.\n\nMulti-task Cascaded Convolutional Neural Networks (MTCNN)  [7]  is the first of the selected algorithms, that is based on neural networks. According to authors, better face detection accuracy is achieved when solving several tasks at the same time: 1) face bounding box regression; 2) probability prediction that each bounding box contains a face; 3) face landmark detection (location of eyes, nose tip, mouth corners). MTCNN has several networks in a cascade: 1) P-Net is a fast network that processes the image at multiple resolutions and predicts initial face bounding boxes; 2) R-Net refines the predictions; 3) O-Net further refines the predictions and outputs final bounding boxes. To filter out overlapping bounding boxes Non-Maximum Suppression algorithm is used. Note, that face landmark detection performed in this algorithm may be used in many applications  [1] .\n\nBlazeFace  [8]  is a novel face detection algorithm. The only algorithm in this work, which primarily targets smartphones. The authors base on Single-Shot Detector (SSD)  [10]  with MobileNetV2  [11]  backbone. Despite the fact, that MobileNetV2 is a mobile-friendly neural network, it can still be improved. Bottleneck block is the key building block of the MobileNetV2 architecture. The authors propose to enlarge block's receptive field by using convolutions with larger kernel sizes. At the same time they reduce the overall number of Bottleneck blocks. This results in smaller inference time at comparable quality. The next step was to simplify SSD architecture to become a better fit for face detection. SSD was originally designed for detection of arbitrary objects (including wide and tall objects). In the meantime, face bounding boxes are typically square. The authors propose to remove SSD features irrelevant for face detection. Finally, NMS algorithm (also used in MTCNN) has been improved. It has been noticed that when applied to videos, NMS results in visible face bounding box jitter between adjacent frames. The jitter might reduce applicability of face detection algorithm on videos. To resolve the problem a small regression module has been proposed instead of NMS to stabilize bounding box prediction. Similarly to MTCNN, BlazeFace also predicts face landmarks for each face detected (eyes, ears, mouth center, nose tip). Despite the improvements proposed, the authors haven't compared the work with other algorithms in the field. Also, neural network training has been performed on a closed dataset. In our work, we will compare the algorithm both in terms of inference speed and quality on several scenes. In contrast to the above-described algorithms, BlazeFace doesn't work with images of arbitrary resolution. BlazeFace has 2 modifications: for photos taken on front-facing camera with resolution of 128 × 128 and on rear-facing camera at 256 × 256. The authors expect photos taken on frontal camera to be quite large, and on rear camera to be smaller.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Experiment Setup",
      "text": "To conduct the experiments we have developed a mobile face detection application for Android OS. Face and landmark detection is shown in Fig.  1a . The application has several configuration options available (Fig.  1b ). First is algorithm input image resolution. There are typical values, such as 128 × 128, 256 × 256, 480 × 360, 640 × 480, and extremely low: 32 × 32, 64 × 64. The latter are used to test, whether certain algorithm is capable of face detection on images with low level of detail. Next, it is possible to configure face detection model. Here we can enable face landmark detection if the algorithm supports it. Finally, diagnostics configuration is available. An example of diagnostics information is available in Fig.  1a  on top, where information about last frame detection time, mean and standard deviation for inference times over a certain calculation window are presented.\n\nWe have selected 2 different smartphones with Android OS and Qualcomm Snapdragon processors. Smartphones with these processors are widely available on the market. Thus, our results will be easily reproducible. The first processor in Snapdragon 800, that is a flagship processor from 2013. Given year-over-year processor performance improvements, in 2022 its performance is similar to lowend processors. The second processor is Snapdragon 845, which is a flagship processor from 2018. In 2022 it can still be considered as mid to high-end CPU.\n\nNext, each of the algorithms presented will be evaluated based on 3 criteria: 1) inference time on a mobile device given certain configuration and CPU; 2) whether the algorithm can detect face in one of the test scenarios; 3) visual traits of the algorithms, such as bounding box jitter on video.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "The goal of the first experiment conducted is to estimate mean and standard deviation of algorithm inference time on  both processors. In Table  1  experiment results are shown for images of size 256×256. Statistics are calculated over a timeframe of 30 seconds, with equal time devoted for scenes without faces, with single large face, and 2 small faces. The algorithms execute 2.13 -5.64 times slower on older Snapdragon 800 processor. Interestingly, neural-network-based algorithms have seen smaller relative slowdown, than algorithms with handcrafted features. Because-of this, Haar cascades algorithm has become slower to execute on Snapdragon 800 than both MTCNN and BlazeFace, while it was faster on Snapdragon 845.\n\nIt has been noted previously, that most algorithms in this paper can work with input images of different resolutions. In Figs.  2a  and 2b  we show inference time with respect to input resolution on Snapdragon 800 and 845 processors correspondingly. Dashed horizontal line depicts realtime inference baseline (at 25 frames per second or 40 milliseconds). Note, that BlazeFace has models for images taken on frontfacing camera (at resolution of 128×128) and on rear-facing (at resolution of 256 × 256). As these models have somewhat different architectures, the results between these resolutions cannot be interpolated. Thus, BlazeFace results are shown with crosses.\n\nBased on Fig.  2   As it has been shown, not all algorithms can work fastenough with images of large size. However, up to this point we didn't specify if there is any difference in face detection quality between different resolutions. As algorithms pre-  sented in this survey have been trained on different datasets, we will conduct visual quality testing targeting mobile face control system  [2] . During the experiment, we will score each algorithm from 0 to 3. 1 point is given for each of the following criteria: 1) given a picture without any faces, the algorithm does not have false face detections; 2) given a single large face on a picture, the algorithms finds it; 3) given two small faces, the algorithm finds both of them. Testing results are shown in Fig.  3 . Cases with score of 1, 2, 3 are shown in red, yellow and green colors correspondingly. Note, that in our testing each algorithm has got at least 1 point.\n\nVisual issues observed. Haar Cascades: on images of size 480 × 360 or above, the algorithm finds non-existing faces. On a video, such face detection sporadically appears and disappears. LBP: on images of size 256 × 256 and below the algorithm fails to find small faces. At 64 × 64 or below no faces are detected. HOG: is quite good at finding faces on images at resolution of 256 × 256 or above. MTCNN: is able to find single large face on photos even at resolution of 32 × 32, which makes this algorithm the most adaptive among the selected algorithms. BlazeFace: has 2 fixed resolutions and works perfectly on both. It should be noted, that thanks to the special regression module, face bounding box predictions are much more stable on video in comparison to other algorithms.\n\nBased on the data from Table  1  it has been noticed, that standard deviation of the algorithms is substantially different. It is interesting to understand the roots of such be- havior. In Fig.  4  we show box plots for inference times. Statistics were calculated over a timeframe of 15 seconds. The measurements are split into testing scenes with 0, 1, and 2 faces. Most algorithms clearly show inference time growth with the number of faces present in the image. For MTCNN, Haar, LBP algorithms this can be explained by their cascaded nature, where the fewer image regions con- tain face-like objects, the earlier such regions are rejected. Interestingly, BlazeFace (rear) shows maximum inference time when no faces are present. BlazeFace (front) shows no clear trend. Such behavior is less efficient for cases, when most frames contain no faces.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "Five face detection algorithms have been adapted to smartphones in this work. Based on the conducted analysis, it is possible to conclude that the best algorithms in terms of mobile applicability are:\n\n• BlazeFace, whose face bounding box predictions are stable (have almost no jitter) in videos. However, the algorithm accepts input images of only 2 fixed resolutions, which is a drawback. Also, high inference time on images without faces is untypical;\n\n• MTCNN, which thanks to the cascaded architecture is able to conserve computational resources on frames without faces. It is the algorithm that can detect faces on the widest range of input resolutions;\n\n• if inference time is the most important, face detector based on Histogram of Oriented Gradients (HOG) can be used. This algorithm has higher speed at a cost of some quality.\n\nAlso, in the work it has been shown that cascaded architectures adaptively change inference time based on input image contents (i.e. whether there are any patches similar to a face). We hope that the analysis and practical recommendations presented in this work will widen applicability of the described face detection methods in mobile applications.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Funding",
      "text": "The work is supported by the state budget scientific research project of Dnipro University of Technology \"Development of New",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: a. The application has",
      "page": 2
    },
    {
      "caption": "Figure 1: b). First is al-",
      "page": 2
    },
    {
      "caption": "Figure 1: a on top, where",
      "page": 2
    },
    {
      "caption": "Figure 1: Android face detection application interface. (a) camera preview with inference time diagnostics (last value, mean and standard",
      "page": 3
    },
    {
      "caption": "Figure 2: it is possible to conclude that Haar",
      "page": 3
    },
    {
      "caption": "Figure 2: Face detection inference time comparison with respect to input image resolution. Most algorithms accept input images of",
      "page": 4
    },
    {
      "caption": "Figure 3: Cases with score of 1, 2, 3",
      "page": 4
    },
    {
      "caption": "Figure 3: Visual algorithm evaluation on test scenarios. Algo-",
      "page": 4
    },
    {
      "caption": "Figure 4: we show box plots for inference times.",
      "page": 4
    },
    {
      "caption": "Figure 4: Inference time box plots for face detection algorithms. Measurements taken over 15s video on scenes with 0, 1, or 2 faces. Image",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table 1: experiment results are shown",
      "page": 3
    },
    {
      "caption": "Table 1: Algorithm inference time comparison on mobile devices.",
      "page": 3
    },
    {
      "caption": "Table 1: it has been noticed, that",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Fast facial landmark detection and applications: A survey",
      "authors": [
        "K Khabarlak",
        "L Koriashkina"
      ],
      "year": "2022",
      "venue": "Journal of Computer Science and Technology",
      "doi": "10.24215/16666038.22.e02"
    },
    {
      "citation_id": "2",
      "title": "Mobile access control system based on RFID tags and facial information",
      "authors": [
        "K Khabarlak",
        "L Koriashkina"
      ],
      "year": "2020",
      "venue": "Bulletin of National Technical University \"KhPI\". Series: System Analysis, Control and Information Technologies",
      "doi": "10.20998/2079-0023.2020.02.12"
    },
    {
      "citation_id": "3",
      "title": "Rapid object detection using a boosted cascade of simple features",
      "authors": [
        "P Viola",
        "M Jones"
      ],
      "year": "2001",
      "venue": "2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2001), with CD-ROM",
      "doi": "10.1109/CVPR.2001.990517"
    },
    {
      "citation_id": "4",
      "title": "The OpenCV Library",
      "authors": [
        "G Bradski"
      ],
      "year": "2000",
      "venue": "Dr. Dobb's Journal of Software Tools"
    },
    {
      "citation_id": "5",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2005",
      "venue": "2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005)",
      "doi": "10.1109/CVPR.2005.177"
    },
    {
      "citation_id": "6",
      "title": "Dlib-ml: A Machine Learning Toolkit",
      "authors": [
        "D King"
      ],
      "year": "2009",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "7",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE Signal Process. Lett",
      "doi": "10.1109/LSP.2016.2603342"
    },
    {
      "citation_id": "8",
      "title": "Blazeface: Submillisecond neural face detection on mobile gpus",
      "authors": [
        "V Bazarevsky",
        "Y Kartynnik",
        "A Vakunov",
        "K Raveendran",
        "M Grundmann"
      ],
      "year": "2019",
      "venue": "CoRR",
      "arxiv": "arXiv:1907.05047"
    },
    {
      "citation_id": "9",
      "title": "Pytorch: An imperative style, high-performance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga",
        "A Desmaison",
        "A Kopf",
        "E Yang",
        "Z Devito",
        "M Raison",
        "A Tejani",
        "S Chilamkurthy",
        "B Steiner",
        "L Fang",
        "J Bai",
        "S Chintala"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "10",
      "title": "SSD: single shot multibox detector",
      "authors": [
        "W Liu",
        "D Anguelov",
        "D Erhan",
        "C Szegedy",
        "S Reed",
        "C Fu",
        "A Berg"
      ],
      "year": "2016",
      "venue": "Computer Vision -ECCV 2016 -14th European Conference",
      "doi": "10.1007/978-3-319-46448-0\\_2"
    },
    {
      "citation_id": "11",
      "title": "Mobilenetv2: Inverted residuals and linear bottlenecks",
      "authors": [
        "M Sandler",
        "A Howard",
        "M Zhu",
        "A Zhmoginov",
        "L Chen"
      ],
      "year": "2018",
      "venue": "2018 IEEE Conference on Computer Vision and Pattern Recognition",
      "doi": "10.1109/CVPR.2018.00474"
    }
  ]
}