{
  "paper_id": "2403.04041v1",
  "title": "Cascaded Self-Supervised Learning For Subject-Independent Eeg-Based Emotion Recognition",
  "published": "2024-03-06T20:41:53Z",
  "authors": [
    "Hanqi Wang",
    "Tao Chen",
    "Liang Song"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "EEG-based Emotion recognition holds significant promise for applications in human-computer interaction, medicine, and neuroscience. While deep learning has shown potential in this field, current approaches usually rely on large-scale high-quality labeled datasets, limiting the performance of deep learning. Self-supervised learning offers a solution by automatically generating labels, but its inter-subject generalizability remains under-explored. For this reason, our interest lies in offering a self-supervised learning paradigm with better inter-subject generalizability. Inspired by recent efforts in combining low-level and high-level tasks in deep learning, we propose a cascaded self-supervised architecture for EEG emotion recognition. Then, we introduce a low-level task, time-tofrequency reconstruction (TFR). This task leverages the inherent timefrequency relationship in EEG signals. Our architecture integrates it with the high-level contrastive learning modules, performing self-supervised learning for EEG-based emotion recognition. Experiment on DEAP and DREAMER datasets demonstrates superior performance of our method over similar works. The outcome results also highlight the indispensability of the TFR task and the robustness of our method to label scarcity, validating the effectiveness of the proposed method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition plays a pivotal role in human-computer interaction and finds widespread applications in fields such as medicine and neuroscience, ren-dering research in this domain of significant importance. Compared with other techniques, electroencephalography (EEG) has garnered attention as an emotion measurement in this area due to its objectivity and high temporal resolution. Recently, deep learning has been introduced to recognize emotion from EEG signals, achieving notable success  [1] [2] [3] . However, prevailing methods in deep learning-based EEG emotion recognition are based on supervised learning that needs the label information to guide the training of model  [4] . Thus, the largescale labeled training data is generally indispensable for the performance of the supervised model. Meanwhile, the manual annotation of EEG signal labels is time-consuming and laborious, presenting a challenge in collecting large-scale labeled emotional EEG samples  [4] [5] [6] . In addition, considering that the labeling quality usually relies on expertise and self-report, the collected labels are susceptible to noise and subjective bias that mislead the learned representation of supervised model  [4] [5] [6] . These facts constrain the performance of the deep learning model in the EEG-based emotion recognition area.\n\nTo alleviate the problem, researchers introduce self-supervised learning to extract representation from EEG signals  [5] [6] [7] [8] . In general, self-supervised learning can automatically generate labels for unlabeled data by constructing a pretext task. This training paradigm mitigates the dependence of the deep learning model on large-scale labeled training data  [4] . And, the learned comprehensive representation of the underlying structure of the data can serve for following downstream tasks  [4] . Some recent works have shown the effectiveness of self-supervised learning for EEG emotion recognition  [5] [6] [7] . However, these advances remain insufficient to cope with the challenges in the field of EEG emotion recognition. The current EEG-based self-supervised learning research pays little attention to improving the generalization of the model on unseen subjects, limiting its practicability in real-life applications. Due to the high inter-subject variability, the EEG signal presents a challenge for deep learning models to generalize across subjects. There is a large discrepancy in the data distribution of EEG signals collected from different subjects. Thus, the subject-dependent model trained on some subjects usually performs inadequately on other unseen subjects, leading to limited performance on subject-independent emotion recognition task. Although some researchers propose to adopt contrastive learning to improve the generalizability of self-supervised learning model  [7] , a single task learning is insufficient to capture comprehensive subject-invariant information for emotion recognition. Currently, this challenge for self-supervised learning remains under-explore in the field of EEG emotion recognition. Motivated by this, our interest lies in exploring how to improve the generalization ability of the self-supervised EEG emotion recognition model.\n\nRecently, there has been an effort in the deep learning field to combine a low-level task with a high-level task. Usually, the low-level task refers to a task that focuses on the coarse pattern in raw data, while the high-level task involves complex semantic information understanding and reasoning. In some works  [9] [10] [11] [12] [13] , researchers establish a low-to-high cascaded pipeline that jointly optimizes multiple tasks at different levels. In such a pipeline, the cascaded architecture can enable the representation learning to perform coarse-to-fine Figure  1 : The illustration of cascaded low-to-high architecture and single task learning. Instead of a single task being responsible for the entire learning process, low-to-high architecture evolves from low-level to high-level tasks, pruning the learned representations.\n\npruning at different stages, evolving from low-level to high-level as shown in Fig 2 . Empirical findings from these works have demonstrated that connecting a low-level task to a high-level task can improve the generalization ability of trained model  [9] [10] [11] . This observation suggests a possible approach to develop a more generalizable method. We hope the low-level task can help capture the subject-invariant simple pattern in the raw EEG data, aligning the distribution of coarse representation of various subjects as shown in Fig ? ?. Based on that, the following high-level task continues to refine the extracted coarse representation for the final representation with complex semantic information, improving the generalized capacity. However, the identification of a suitable low-level task remains unexplored in this area.\n\nThe time-to-frequency transform relationship provides a clue regarding how to define a low-level task for self-supervised EEG representation learning. A raw EEG signal in the time domain includes all the information to obtain the corresponding sample in the frequency domain. Thus, it is feasible to reconstruct the sample in the frequency domain using the representation learned from the time domain. To perform this reconstruction task, the model is actually forced to learn a Fourier-based transformation. Compared with the other self-supervised tasks, this task aims to learn a relatively coarse pattern involving less semantic information, i.e., a fixed transform relationship. In addition, this relationship holds invariant regardless of the subject from whom the EEG signal was collected. This fact provides a subject-invariant property of emotional EEG responses. By leveraging this property, we can align data from different subjects and map them into a subject-invariant distribution, thereby preserving more subject-invariant information for subject-independent emotion recognition. This inspiration motivates us to formalize such a relationship into a low-level task suitable for integration within the cascaded architecture.\n\nTo address these issues, this work proposes a cascaded self-supervised architecture to learn representation from emotional EEG signals. The purpose of our work is to explore the potential of the low-level task to facilitate the performance of the high-level self-supervised task. To this end, we formulate the relationship between time and frequency domain into a low-level reconstruction task, named time-to-frequency reconstruction (TFR). The representation learned in the time domain is used to reconstruct the input sample in the frequency domain. In addition, contrastive learning modules are adopted in our work as a high-level self-supervised learning task. Specifically, our proposal is based on a two-stream architecture. The proposed TFR module is connected to a contrastive learning module in the time-domain stream. Then, our work performs the second contrastive learning task in the frequency-domain stream to enhance the representation learning ability further. All three modules are combined in a joint loss to optimize this architecture. In the experiment section, we perform our method on two public benchmark datasets, DEAP and DREAMER. The comparison with existing methods demonstrates that our method outperforms similar works and shows a competitive performance when compared with the others. Besides that, our experiment also illustrates the indispensability of such a low-level task for performance by canceling or replacing the TFR module, further validating the effectiveness of our proposal. Moreover, the capacity of our method with limited labeled data is evaluated to assess the resistance of our method to label scarcity.\n\nOur primary contribution can be summarized as follows:\n\n• We propose to cascade the low-level and high-level self-supervised task in a pipeline. Our proposed method follows a low-to-high representation learning procedure, aiming to preserve more generalizable representations. In this manner, the proposed method is expected to exhibit enhanced inter-subject generalizability on previously unseen data.\n\n• To define an appropriate low-level task, we propose a time-to-frequency reconstruction task for the proposed low-to-high pipeline. The representation learned in time domain is used to reconstruct its frequency-domain features, learning the low-level representation.\n\n• We implemented extensive experiments on two public benchmark datasets. Our results show that our method outperforms the existing similar works and show a competitive performance over the others. Besides that, our experiment further demonstrates the effectiveness of the proposed modules.",
      "page_start": 1,
      "page_end": 5
    },
    {
      "section_name": "Related Works 2.1 Subject-Independent Eeg Emotion Recognition",
      "text": "Considering the practicability, many researchers are interested in subject-independent approaches capable of recognizing emotions well across different subjects. However, EEG signal presents a high inter-subject variability. This characteristic of EEG signals makes it difficult for models to perform well when shifting from training data to testing data. Currently, there are two approaches to cope with this issue. One way is through domain adaptation (DA), which aims to reduce the differences between the training and test data. For example, Zheng et al.  [14]  applied classical DA methods to the SEED dataset. In addition, domainadversarial neural network  [15]  (DANNs) is also introduced to this area, using a domain classifier to learn domain-indiscriminate representation. This method further led to ideas like bi-hemispheres domain-adversarial neural network  [16]  (Bi-DANN) and regularized graph neural network  [17]  (RGNN). Bi-DANN uses two hemisphere domain classifiers and a global domain classifier to get subjectindependent emotion representations. RGNN changes the usual way of training to be more effective. These methods make the inter-subject model work better, but they rely on access to the test data. However, the test data is usually inaccessible in practice, limiting the application of EEG-based emotion recognition algorithms. Another way is domain generalization (DG), a promising method for subject-independent EEG emotion recognition. This method allows models to perform well across subjects without access to test data. DG aims to extract subject-invariant representation applicable to various subjects. Ma et al.  [18]  extended the domain-adversarial neural networks (DANNs) into a (DG) method. They introduced a domain residual network (DResNet) that learns domain-shared and domain-specific weights. However, most of the existing subject-independent works still follow the supervised training strategy, limiting the practicability in real-world applications. Recently, a novel contrastive learning method, CLISA, as presented in  [7] , has been introduced for subject-independent EEG-based emotion recognition. CLISA couples two samples from distinct subjects, each corresponding to the same emotional stimuli, as anchor and positive samples. This method introduces a robust contrastive learning framework tailored for EEG-based emotion recognition, reaching a better performance than the work in  [18] . More importantly, this work draws further attention to subject-independent approaches in a self-supervised manner.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Self-Supervised Representation Learning For Eeg Emotion Recognition",
      "text": "Self-supervised representation learning has exhibited notable achievements in many research fields, such as natural language processing, computer vision, etc. Recently, although the application of self-supervised learning in EEG emotion recognition still needs to be explored  [4] , we still have seen a few efforts adopting this training strategy in some works. For example, in  [19] , authors apply multiple signal transformations to the original signals and use the signal transformation recognition as the pretext task. And, the work presented in  [5]  proposes a self-supervised contrastive learning framework, using a genetics-inspired data augmentation method named meiosis. Moreover, the self-supervised reconstruction task is also adopted to learn the representation through a masked autoencoder architecture in  [20] . However, these works are subject-dependent algorithms. Considering the practicability of subject-independent models, some works aim to explore the subject-independent self-supervised learning in this area. The authors in  [7]  propose a contrastive learning method named CLISA, which maximize the similarity of inter-subject EEG responses to the same emotional stimuli in the representation space. While their inspiring work demonstrates effectiveness in capturing inter-subject correlations, there remains room for further improvement in performance. In  [21] , a novel LSTM with attention mechanism is proposed to extract subject-invariant features of EEG data, based on self-supervised reconstruction pretext task. Although this work demonstrates impressive performance on public datasets, it also shows reliance on the welldesign and complex network architecture. In the broader context of time-series research, TF-C, as introduced in  [22] , also recognizes the potential of timefrequency properties in facilitating self-supervised learning. However, our approach diverges from TF-C. TF-C utilizes the time-frequency property to make a novel definition for the different views of the data in contrastive learning. It assumes that the time-based and the frequency-based representations should show consistency in the latent space under the guidance of the proposed novel contrastive loss. However, we treat the time-frequency property as a clue to define the low-level self-supervised task. The newly formulated low-level task is expected to facilitate the finding of subject-invariant features by evolving the learning procedure from low-level to high-level. Consequently, our work should not be perceived as redundant or overlapping with TF-C. We compare the performance with TF-C in the experiment section to underscore this distinction. i and xf i for x t i and x f i . Subsequently, time-to-frequency reconstruction and temporal contrastive learning are performed in a cascaded pipeline. Moreover, the frequency contrastive learning is implemented for a more comprehensive representation learning. Finally, we sum the reconstruction loss L recon and two contrastive learning losses L t con and L f con as the final loss L.\n\n3 Method",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Overview",
      "text": "In this section, the cascaded self-supervised learning architecture will be described in detail as shown in Fig 3 . First, a low-level self-supervised task is performed at the beginning. The input of EEG data is projected into the frequency domain via the fast Fourier Transform. Next, the EEG data is sent to a depth-wise convolution layer to learn EEG representation in the time domain, and a linear layer is added to reconstruct the EEG data in the frequency domain from the learned representation. Then, two contrastive learning modules, i.e., temporal and frequency contrastive learning, are embedded to implement the high-level contrastive learning task. In particular, temporal contrastive learning adopts the learned representation in the TFR task as input. Instead, the frequency contrastive learning adopts the raw EEG frequency spectrum as input. Finally, all three tasks are combined to optimize the model in a joint training process.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Time-To-Frequency Reconstruction",
      "text": "As we mentioned above, a time-to-frequency reconstruction task is formulated as a low-level self-supervised task. To this end, we propose an encoder-decoder structure to reconstruct the input sample in the frequency domain, using the representation extracted from the input sample in the time domain. This objective is supposed to force the encoder to learn the time-to-frequency transform relationship, preserving the subject-invariant information. Let\n\n, denote a batch of raw EEG data with size N in the time domain. Moreover, we use {x f |x f i ∈ R C×T } N 1 to represent the generated frequency spectrum. Our model learn the representation r t i in time domain can be seen in the Eq 1\n\nwhere the x f recon,i denotes the reconstructed input in the frequency domain, Enc t f denotes the encoder, and the Enc t f denotes the decoder for TFR.\n\nIn order to encourage the consistency between the reconstruction x f recon and the original frequency-domain input x f , we adopt the mean square error (MSE) to measure the similarity between x f recon and x f as shown in Eq 2.\n\nwhere L recon denotes the reconstruction loss.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Temporal And Frequency Contrastive Learning",
      "text": "To further enhance the effectiveness of extracted representation, we include contrastive learning tasks following the TFR. The strategy of contrastive learning is to align the representations between the different views of data. Thus, the first step is to create the augmentation for the original EEG data. Here, we adopt the augmentation methods in  [22]  for both time and frequency domains, generating the augmentation sets\n\nIn both domains, we randomly select an augmenting method for each sample from the bank. The time-domain augmentation bank includes jittering, scaling, time shift, and neighborhood segmentation. And the frequency-domain augmentation bank includes removing and adding frequency components. Then, two encoders are set to learn the representation in time and frequency domains, respectively.\n\nwhere rt i = Enc t f (x t i ), Enc t denotes the time-domain encoder, and Enc f denotes the frequency-domain encoder.\n\nSubsequently, the learned representations are projected into a latent space for alignment. For this reason, two projectors, G t and G f , are added into the pipeline following the Enc t and Enc f .\n\nwhere z t i represents the projection of h t i in the latent space, with analogous definitions for zt i , z f i , and zf i . Then, we construct the contrastive losses to guide the optimization. Initially, we merge the two sets {z t i } N 1 and\n\nwith size of 2N . Similarly, we also merge the\n\nand z t 2k in the new set, k ∈ {1, . . . , N }, are regarded as positive for each other because they are different views of the z t k , and so are the cases of {z f i } 2N 1 . Let I = {1, . . . , 2N } denotes the indexes set. The p t (i) denotes the index of the positive sample of z t i , and p f (i) denotes the index of the positive sample of z f i . Subsequently, the contrastive learning losses can be calculated as described in Eq 5.\n\nwhere A(i) = {a|a ∈ I, a ̸ = i}, and τ is a scalar temperature parameter. We use L t con to denote the loss in time domain and L f con to denote the loss in frequency domain.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Optimization And Prediction",
      "text": "The final representation learning loss comprises three components: temporal contrastive loss, frequency contrastive loss, and time-to-frequency reconstruction loss as described in 6.\n\nwhere λ is a hyper-parameter that controls the balance of contrastive and reconstruction losses.\n\nIn the prediction phase, the trained model is refined through a fine-tuning process on unseen test data. Augmentations, decoders, and projectors are excluded, retaining only the three trained encoders to extract representations h t i and h f i , followed by the flattening operation. These representations are then concatenated to achieve information fusion, resulting in the final representation h cat i . Subsequently, a classifier is trained to classify the learned final representations into various emotion status. The whole process is visualized in Fig  4 .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Detailed Structure",
      "text": "The structure of the encoders, decoder, non-linear projectors, and the classifier are presented in details in this subsection. Considering the property of Fourierbased transform, we adopted a depth-wise 1-D convolution layer as the encoder structure to learn representation from x t i for TFR. And then, a linear layer is adopted as a decoder to reconstruct x f i .\n\nwhere K 1 denotes the kernel, S 1 denotes the stride, and P 1 denotes the paddding.\n\nIn this work, we aim to present a compact encoder structure suited to our task for contrastive learning. For simplicity, the two encoders in both domains, Enc t and Enc f , adopt the identical structure. For fewer trainable parameters, our work adopts separable one-dimensional 2-D convolutions to learn the pattern on different dimensions separately  [7, 23] . First, we extend the input size into 1 × C × T . Next, we adopt a convolution layer with kernel size (1, K 2 ) to learn the temporal or frequency pattern. Moreover, Existing research demonstrates that the responses to emotion between the right and left hemispheres of the brain show an asymmetric pattern  [24] . Considering that, we follow the  [25]  to adopt global and hemisphere kernels, learning the spatial pattern. In detail, we adopt two convolution layers with kernel size (C/2, 1) and (C, 1). Finally, the outputs of the two spatial convolution layers are concatenated along the spatial dimension and fused by a one-dimensional 2-D convolution layer with kernel size (K 3 × 1). The learning process of our encoder structure can be described in the Eq 8. h = Enc(x);\n\nThe Conv2D tf denotes the convolution layer that learns temporal or frequency pattern, the Conv2D spa glb denotes the convolution layer that learns the global spatial pattern, the Conv2D spa hem denotes the convolution layer that learns the hemisphere spatial pattern, and Conv2D f use denotes the convolution layer that fuse the outputs of Conv2D spa glb and Conv2D spa hem . Besides, S 1 and S 2 represent the strides, P 3 represents the padding, and F 2 represents the filter size. Finally, AvgP ool represents an average pooling operation with kernel (1, K 4 ) and stride S 3 , and LReLU represents the LeakyReLU activation layer.\n\nAs for the projector, we adopt a non-linear projector structure for both contrastive learning modules. It comprise of two fully-connected layers with a batch normalization layer and a ReLUs layer inserted in the middle. The hidden dimension of the two fully-connected layers are 256 and 128. And we build a three-layer multilayer perceptron (MLP) and used it as a classifier.The MLP consists of two hidden layers, each with 30 units. Rectified linear units (ReLUs) are inserted between every two layers.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Experiment",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Dataset And Preprocessing",
      "text": "Our experiments are conducted on two widely used benchmark datasets: DEAP and DREAMER. DEAP  [26]  is a multi-modal dataset focused on human affective computing, consisting of 32 subjects, each participating in 40 trials. Subjects watch one-minute music videos, self-reporting their emotional states in arousal and valence dimensions. For our experiment, we adopt the 32-channel EEG signals included in it that are recorded at 512 Hz during the trial. Then, the EEG signals is down-sampled to 128 Hz. This experiment approach this as a binary classification task, transforming the 9-level labels into low and high classes. Additionally, following  [25] , we segment the trial into 4-second segments.\n\nDREAMER  [27]  features recordings of 14-channel EEG signals captured during affect-inducing audio-visual stimuli. With 23 subjects watching 18 movie clips, the duration ranging from 63 to 393 seconds. Participants are asked to rate arousal and valence using self-assessment manikins (SAM) scores from 1 to 5. Similar to DEAP, we categorize the 5-level labels into low and high classes. We employ sliding windows to segment EEG recordings, breaking each trial into 9-second segments with a 1-second slide.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Implementation Details",
      "text": "We empirically select hyperparameters for the implementation, as detailed in Table  1 . In the representation learning procedure, we set τ to 0.07. All subsequent experiments are conducted using an NVIDIA RTX 2080Ti GPU. The representation learning model employs Adam as the optimizer, with a learning rate of 0.0001 for the DEAP dataset and 0.00008 for the DREAMER dataset. The batch size is configured as 128. For the classifier, we also use the Adam optimizer to optimize its cross-entropy loss with a learning rate of 0.00001.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Performance Evaluation Protocol",
      "text": "We implement the proposed method on the DEAP and DREAMER datasets.\n\nTo assess the inter-subject generalization of our approach, we employ the leaveone-subject-out protocol for evaluation. This protocol reserve one subject for evaluation, utilizing the remainder of subjects for training. This process is repeated across all subjects in the dataset. As the result, the mean accuracy and standard deviation is calculated to measure the subject-independent performance.\n\nSignificantly, for enhanced practicality, we aspire for our model to exhibit commendable performance even in the absence of access to test data. For this reason, both the representation learning modules and the classifier are exclusively trained on the training data. It is noteworthy that our proposed model refrains from fine-tuning on the test data, positioning it as a subject-independent work comparable with domain generalization methods.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Comparison With The Existing Methods",
      "text": "Here, our method is compared with three groups of methods: supervised subjectindependent, self-supervised subject-independent, and intra-subject. To begin with, the supervised subject-independent methods comprise DA and DG methods, with which we will compare our method. In the first place, the classical DA baseline methods, i.e., TSVM, TPT, TCA, and KPCA, are compared to ours. Besides that, we also compare our work with some recent DA approaches in this area. The approaches outlined in  [28, 29]  employ the widely recognized adversarial training strategy to minimize dissimilarities between diverse subjects. Additionally, in  [30] , the authors introduce a Symmetric and Positive Definite (SPD) matrix network (daSPDnet) aimed at capturing shared emotional representations with short calibration. These endeavors represent recent advancements in DA for our specific task. Notably, these methods necessitate access to test data for optimal performance. In contrast, our methodology aligns with DG principles, obviating the need for test data access. Simultaneously, we conduct comparative evaluations with alternative DG methods  [31, 32] .\n\nIn  [32] , the approach focuses on extracting invariant features through Variational Mode Decomposition (VMD). Moreover,  [31]  proposes capturing discriminative features from multiple perspectives, demonstrating commendable performance across various subjects. However, these supervised works need the label information to guide the representation learning. In  [7, 34] , the authors explore self-supervised learning for subject-independent EEG emotion recognition. Besides,  [22]  proposes a selfsupervised contrastive learning using time-frequency consistency for time-series pre-training. Here, we transfer it to our task for comparison. Specifically, we reimplement the CLISA and TF-C using the reported setting in  [7, 22] . For a fair comparison, we performed a segment-level classification rather than a trial-level classification in  [7] . Besides that, we cancel the fine-tuning process, training the TF-C architecture on the train data only.\n\nRecently, there have been many effective methods designed for within-subject EEG emotion recognition. These approaches have shown their ability to capture discriminative information for identifying human emotions in EEG signals.\n\nConsequently, there is a theoretical potential for these methods to perform intersubject task. To explore this, we compare our method with some representative methods, e.g., DeepConvNet  [33] , ShallowConvNet  [33] , and TSception  [25] . All the within-subject methods are evaluated following the leave-one-subjectout protocol. The comparison helps highlight how well our method generalizes to different individuals.\n\nThe experiment is conducted on two widely used benchmark datasets, namely DEAP and DREAMER. The outcomes for the DEAP dataset are presented in Table  2 . Our proposed method exhibits markedly better performance compared to existing self-supervised methods, highlighting its efficacy in the selfsupervised learning domain. In terms of a comparison with supervised subjectindependent models, our method outperforms all the listed methods without requiring access to test data. While not universally surpassing all the presented DA methods, our approach remains competitive and generally outperforms a significant portion of them. These experimental findings substantiate the robustness of our method in capturing subject-invariant representations as a selfsupervised method. Supervised TSVM  [14]  55.67±12.07 60.76±9.77 TPT  [14]  61.89±13.18 59.22±15.01 TCA  [14]  54.37±8.56 55.85±6.45 KPCA  [14]  60.03±11.24 53.74±8.47 AD-TCN  [29]  63.69±6.57 66.56±10.04 Wang et al.  [30]  76.57±14.04 67.99±6.34 BiSMSM*  [31]  61.87±/ 62.97±/ TSception*  [25]  62.60±8. Furthermore, the experimental outcomes on the DREAMER dataset are detailed in Table  3 . Notably, our approach maintains its superior performance over all self-supervised methods, mirroring the observations on the DEAP dataset.\n\nCompared to supervised methods, our model remains competitive and outperforms most of them. While our model is slightly outperformed by the method in  [30]  in the arousal dimension on mean accuracy, this discrepancy is justifiable due to the lack of access to label information and test data. This outcome underscores the efficacy of our approach in enhancing generalizability within a self-supervised framework. In summary, our method demonstrates superior performance compared to similar approaches and remains competitive against alternative methods.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Effectiveness The Proposed Time-To-Frequency Reconstruction",
      "text": "To further investigate the effectiveness of the proposed TFR, we perform experiments on DREAMER using two variants of the proposed method. In the first variant, we replace the TFR with a conventional time-to-time reconstruction task. Specifically, the decoder is modified to reconstruct the original features in the time domain. The L recon in Eq 2 is changed to\n\n, where x t recon,i denotes the output of the decoder. With this variant, we aim to verify the effectiveness of the proposed low-level reconstruction task compared to the other reconstruction task. In the second variant, we aim to testify the capacity of proposed model without any reconstruction task. For this reason, we remove the decoder while retaining the encoder in the TFR module. In this way, the variant cancels the low-to-high representation learning procedure but maintains a similar learning capacity to perform the subsequent contrastive learning module. The results can be seen in Table  4 . As we can see, the TFR module plays an indispensable role in enhancing the performance of the proposed model, outperforming all modified variants in both dimensions. Furthermore, when considering the second variant as a baseline, it is obvious that the efficacy of the conventional time-to-time reconstruction, a high-level self-supervised task, is limited in comparison to the proposed TFR. This strengthen the indispensability of a low-level task to boost performance. In conclusion, the experimental results support the efficacy of the proposed TFR.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Performance With Limited Labeled Data",
      "text": "Given the scarcity of labeled data, it is imperative to assess the performance of our method with limited training data. Consequently, we systematically reduce the labeled data to percentages of 20%, 40%, 60%, and 80%. In order to maintain data balance, we assign the same pre-defined proportions of reserved labeled data for each subject in each trial. Subsequently, the classifier is trained using the reserved labeled data, while the representation learning architecture is trained using all the data. And we still adopt leave-one-subject out protocol to evaluate the performance. The experiment is conducted on the DREAMER benchmark dataset, and the results are presented in Fig  5 .",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Visualization",
      "text": "To provide a more intuitive demonstration of the effect of our model, we employ the T-SNE (t-distributed stochastic neighbor embedding) technique to visualize the data in the DREAMER dataset in a two-dimensional space. Initially, to show the capacity of our model to learn subject-independent discriminative emotional information, we randomly sampled one-tenth of the raw data from each subject for analysis. As shown in in the distribution of learned representations, different emotional states could be separated more effectively. This observation supports the effect of our model on learning subject-independent emotional information.\n\nBesides that, we further demonstrate the capacity of our model to reduce the inter-subject discrepancy. In",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Ablation Study",
      "text": "To further assess the efficacy of the components within our proposed method, we conduct an ablation study on the arousal dimension of the DREAMER dataset. This subsection outlines three variants of our method, aiming to discern the impact of each component on the overall performance of the model. The experiments adhere to the leave-one-subject-out protocol. The three variants are delineated as follows.\n\n•Single time-domain stream. In this variant, we remove the encoder in the frequency-domain stream and the time-to-frequency reconstruction. And, the input of classifier h cat i is replaced with h t i . Our goal is to examine the necessity of the complementary information in the frequency-domain stream and the proposed TFR module.\n\n•Base model. To further prove the effectiveness of our method, we are interested in the learning capacity of the encoder and classifier without the lowand high-level tasks. In this variant, we combine the encoder and the classifier into an end-to-end deep learning model. Without the two-stream architecture, we only use the raw EEG sample as the input of the new base model. Because the projector is designed to learn the mapping to a suitable latent space, it is not necessary anymore when we remove the contrastive learning modules. Hence, we also discard the projector in this variant. The representation learned from the raw EEG sample is directly flattened and input into the classifier. The combination of the encoder and the classifier is regarded as a whole structure optimized by the cross-entropy loss.\n\n•Time-to-frequency reconstruction. In earlier subsection, we discuss the efficacy of the TFR module. However, the performance of the model only consists solely of a TFR module has not yet been evaluated. In order to obtain a more reliable demonstration, we are also interested in exploring the repre- sentation learning capacity of the low-level TFR module without the help of high-level task. For this reason, we cancel all the components in the learning framework, only reserving the TFR module to extract the representation.\n\nThe results can be seen in the Fig 9 . All the variants suffer a decline in performance in different degrees. It proves that the proposed modules have the effect of improving subject-independent performance. Notably, the Timeto-frequency reconstruction variant reaches the worst mean accuracy, 54.86%. This observation suggests that a low-level task is insufficient to solely capture the key information for emotion recognition, needing the refinement of the following high-level tasks.",
      "page_start": 18,
      "page_end": 19
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we investigate the effectiveness of cascaded low-to-high architecture in enhancing the generalizability of self-supervised learning for emotion EEG recognition. To perform such an architecture, we define a novel time-tofrequency reconstruction task as a low-level self-supervised task. Moreover, we incorporate contrastive learning into the proposed architecture as a high-level task. Our extensive experiments demonstrate that the proposed self-supervised learning method can reach an advanced performance compared with the existing methods. Besides that, our further experiments highlight the indispensability of such a low-level task of our model by evaluating the model's performance when the TFR module is replaced or canceled. In summary, our work substantiates its proposals in enhancing the generalizability of the self-supervised EEG-based emotion recognition model.\n\nIn addition, we also proposes to suggest avenues for future exploration. There remains room to advance the low-to-high paradigm in this area. Future research can delve into the alternative types of low-level self-supervised tasks. Moreover, the integration of low-level and high-level tasks is a promising avenue for future advance. The relationship between the two tasks in depth is still under-explored, which might expose the reasons underlying their collective impact on the final learned representation. This motivation can be further extended to investigate the design of interaction mechanisms between different tasks.",
      "page_start": 19,
      "page_end": 20
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The illustration of cascaded low-to-high architecture and single task",
      "page": 3
    },
    {
      "caption": "Figure 2: Empirical findings from these works have demonstrated that connecting",
      "page": 3
    },
    {
      "caption": "Figure 2: The demonstration of the effects of low-level tasks. The low-level task",
      "page": 4
    },
    {
      "caption": "Figure 3: The overview of the representation learning framework. The samples",
      "page": 7
    },
    {
      "caption": "Figure 3: First, a low-level self-supervised task is",
      "page": 7
    },
    {
      "caption": "Figure 4: The overview of prediction procedure. The ⊕denotes the concate-",
      "page": 10
    },
    {
      "caption": "Figure 5: Figure 5: The mean accuracy of the proposed method with limited labeled data",
      "page": 16
    },
    {
      "caption": "Figure 6: , both the learned representations",
      "page": 16
    },
    {
      "caption": "Figure 6: The visualization of the learned representation and the original EEG",
      "page": 17
    },
    {
      "caption": "Figure 7: , we randomly select three subjects and",
      "page": 17
    },
    {
      "caption": "Figure 7: , the data points",
      "page": 17
    },
    {
      "caption": "Figure 8: Although the generated reconstruction results have some errors",
      "page": 17
    },
    {
      "caption": "Figure 7: The visualization of the learned representation and the original EEG",
      "page": 18
    },
    {
      "caption": "Figure 8: The visualization of the original EEG data in the frequency domain",
      "page": 19
    },
    {
      "caption": "Figure 9: All the variants suffer a decline in",
      "page": 19
    },
    {
      "caption": "Figure 9: The mean accuracy results of the variants in the ablation study.",
      "page": 20
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The Mean Accuracy and Standard Deviation of Existing Emotion",
      "data": [
        {
          "Method": "TSVM [14]\nTPT [14]\nTCA [14]\nKPCA [14]\nRODAN [28]\nAD-TCN [29]\nWang et al.\n[30]\nBiSMSM* [31]\nVMD* [32]\nTSception* [25]\nDeepConvNet* [33]\nShallowConvNet* [33]",
          "Arousal": "56.59±11.98\n54.76 ±12.48\n51.81±15.03\n58.15 ±14.96\n56.60±3.48\n63.25± 4.62\n69.79±11.93\n61.87±/\n61.25±/\n63.67±10.30\n63.39±9.74\n61.37±10.93",
          "Valence": "61.77±8.93\n57.43±14.54\n56.23 ±14.33\n54.35 ±10.22\n56.78±3.3\n64.33±7.06\n66.47±8.75\n62.97±/\n62.50±/\n60.26±6.51\n60.22±6.13\n59.85±6.07"
        },
        {
          "Method": "CLISA* [7]\nEEGFuseNet* [34]\nTF-C* [22]\nOurs*",
          "Arousal": "64.50±10.1\n58.55±/\n63.00±12.85\n69.67±8.85",
          "Valence": "61.46±6.7\n56.44±/\n59.23±7.97\n65.50±5.47"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 2: Our proposed method exhibits markedly better performance com-",
      "data": [
        {
          "Method": "TSVM [14]\nTPT [14]\nTCA [14]\nKPCA [14]\nAD-TCN [29]\nWang et al.\n[30]\nBiSMSM* [31]\nTSception* [25]\nDeepConvNet* [33]\nShallowConvNet* [33]",
          "Arousal": "55.67±12.07\n61.89±13.18\n54.37±8.56\n60.03±11.24\n63.69±6.57\n76.57±14.04\n61.87±/\n62.60±8.16\n65.84±7.35\n64.58±6.50",
          "Valence": "60.76±9.77\n59.22±15.01\n55.85±6.45\n53.74±8.47\n66.56±10.04\n67.99±6.34\n62.97±/\n64.19±8.48\n65.88±6.81\n63.61±7.45"
        },
        {
          "Method": "CLISA* [7]\nTF-C* [22]\nOurs*",
          "Arousal": "62.14±10.03\n60.95±12.99\n71.04±6.06",
          "Valence": "63.04±8.83\n62.65±10.56\n69.63±7.07"
        }
      ],
      "page": 14
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Investigating eeg-based functional connectivity patterns for multimodal emotion recognition",
      "authors": [
        "X Wu",
        "W.-L Zheng",
        "Z Li",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "2",
      "title": "Data augmentation for enhancing eeg-based emotion recognition with deep generative models",
      "authors": [
        "Y Luo",
        "L.-Z Zhu",
        "Z.-Y Wan",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "3",
      "title": "Semi-supervised eeg emotion recognition model based on enhanced graph fusion and gcn",
      "authors": [
        "G Li",
        "N Chen",
        "J Jin"
      ],
      "year": "2022",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "4",
      "title": "Self-supervised learning for electroencephalography",
      "authors": [
        "M Rafiei",
        "L Gauthier",
        "H Adeli",
        "D Takabi"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "5",
      "title": "Self-supervised group meiosis contrastive learning for eeg-based emotion recognition",
      "authors": [
        "H Kan",
        "J Yu",
        "J Huang",
        "Z Liu",
        "H Wang",
        "H Zhou"
      ],
      "year": "2023",
      "venue": "Applied Intelligence"
    },
    {
      "citation_id": "6",
      "title": "Selfsupervised eeg emotion recognition models based on cnn",
      "authors": [
        "X Wang",
        "Y Ma",
        "J Cammon",
        "F Fang",
        "Y Gao",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "7",
      "title": "Contrastive learning of subject-invariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Uncovering the structure of clinical eeg signals with self-supervised learning",
      "authors": [
        "H Banville",
        "O Chehab",
        "A Hyvärinen",
        "D.-A Engemann",
        "A Gramfort"
      ],
      "year": "2021",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "9",
      "title": "An interactively reinforced paradigm for joint infrared-visible image fusion and saliency object detection",
      "authors": [
        "D Wang",
        "J Liu",
        "R Liu",
        "X Fan"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "10",
      "title": "Image fusion in the loop of high-level vision tasks: A semantic-aware real-time infrared and visible image fusion network",
      "authors": [
        "L Tang",
        "J Yuan",
        "J Ma"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "11",
      "title": "Connecting image denoising and high-level vision tasks via deep learning",
      "authors": [
        "D Liu",
        "B Wen",
        "J Jiao",
        "X Liu",
        "Z Wang",
        "T Huang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "12",
      "title": "Targetaware dual adversarial learning and a multi-scenario multi-modality benchmark to fuse infrared and visible for object detection",
      "authors": [
        "J Liu",
        "X Fan",
        "Z Huang",
        "G Wu",
        "R Liu",
        "W Zhong",
        "Z Luo"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "Superfusion: A versatile image registration and fusion network with semantic awareness",
      "authors": [
        "L Tang",
        "Y Deng",
        "Y Ma",
        "J Huang",
        "J Ma"
      ],
      "year": "2022",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "14",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the twenty-fifth international joint conference on artificial intelligence"
    },
    {
      "citation_id": "15",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "Y Ganin",
        "E Ustinova",
        "H Ajakan",
        "P Germain",
        "H Larochelle",
        "F Laviolette",
        "M Marchand",
        "V Lempitsky"
      ],
      "year": "2016",
      "venue": "The journal of machine learning research"
    },
    {
      "citation_id": "16",
      "title": "A bi-hemisphere domain adversarial neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "Y Zong",
        "Z Cui",
        "T Zhang",
        "X Zhou"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "17",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "P Zhong",
        "D Wang",
        "C Miao"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Reducing the subject variability of eeg signals with adversarial domain generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Neural Information Processing: 26th International Conference"
    },
    {
      "citation_id": "19",
      "title": "Selfsupervised eeg emotion recognition models based on cnn",
      "authors": [
        "X Wang",
        "Y Ma",
        "J Cammon",
        "F Fang",
        "Y Gao",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "20",
      "title": "A multi-view spectral-spatialtemporal masked autoencoder for decoding emotions with self-supervised learning",
      "authors": [
        "R Li",
        "Y Wang",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "21",
      "title": "Subject independent emotion recognition using eeg signals employing attention driven neural networks",
      "authors": [
        "A Arjun",
        "M Rajpoot",
        "Panicker"
      ],
      "year": "2022",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "22",
      "title": "Self-supervised contrastive pre-training for time series via time-frequency consistency",
      "authors": [
        "X Zhang",
        "Z Zhao",
        "T Tsiligkaridis",
        "M Zitnik"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "23",
      "title": "Eegnet: a compact convolutional neural network for eegbased brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "24",
      "title": "Frontal eeg alpha asymmetry and emotion: From neural underpinnings and methodological considerations to psychopathology and social cognition",
      "authors": [
        "J Allen",
        "P Keune",
        "M Schönenberg",
        "R Nusslock"
      ],
      "year": "2018",
      "venue": "Frontal eeg alpha asymmetry and emotion: From neural underpinnings and methodological considerations to psychopathology and social cognition"
    },
    {
      "citation_id": "25",
      "title": "Tsception: Capturing temporal dynamics and spatial asymmetry from eeg for emotion recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "26",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "27",
      "title": "Dreamer: A database for emotion recognition through eeg and ecg signals from wireless low-cost off-the-shelf devices",
      "authors": [
        "S Katsigiannis",
        "N Ramzan"
      ],
      "year": "2017",
      "venue": "IEEE journal of biomedical and health informatics"
    },
    {
      "citation_id": "28",
      "title": "Eeg-based emotion recognition using spatial-temporal representation via bi-gru",
      "authors": [
        "W.-C Lew",
        "D Wang",
        "K Shylouskaya",
        "Z Zhang",
        "J.-H Lim",
        "K Ang",
        "A.-H Tan"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "29",
      "title": "An adversarial discriminative temporal convolutional network for eeg-based cross-domain emotion recognition",
      "authors": [
        "Z He",
        "Y Zhong",
        "J Pan"
      ],
      "year": "2022",
      "venue": "Computers in biology and medicine"
    },
    {
      "citation_id": "30",
      "title": "A prototype-based spd matrix network for domain adaptation eeg emotion recognition",
      "authors": [
        "Y Wang",
        "S Qiu",
        "X Ma",
        "H He"
      ],
      "year": "2021",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "Bismsm: A hybrid mlpbased model of global self-attention processes for eeg-based emotion recognition",
      "authors": [
        "W Li",
        "Y Tian",
        "B Hou",
        "J Dong",
        "S Shao"
      ],
      "year": "2022",
      "venue": "Artificial Neural Networks and Machine Learning-ICANN 2022: 31st International Conference on Artificial Neural Networks"
    },
    {
      "citation_id": "32",
      "title": "Subject independent emotion recognition from eeg using vmd and deep learning",
      "authors": [
        "P Pandey",
        "K Seeja"
      ],
      "year": "2022",
      "venue": "Journal of King Saud University -Computer and Information Sciences"
    },
    {
      "citation_id": "33",
      "title": "Deep learning with convolutional neural networks for eeg decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human brain mapping"
    },
    {
      "citation_id": "34",
      "title": "Eegfusenet: Hybrid unsupervised deep feature characterization and fusion for high-dimensional eeg with an application to emotion recognition",
      "authors": [
        "Z Liang",
        "R Zhou",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Zhang",
        "S Ishii"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    }
  ]
}