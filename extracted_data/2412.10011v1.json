{
  "paper_id": "2412.10011v1",
  "title": "Enhanced Speech Emotion Recognition With Efficient Channel Attention Guided Deep Cnn-Bilstm Framework",
  "published": "2024-12-13T09:55:03Z",
  "authors": [
    "Niloy Kumar Kundu",
    "Sarah Kobir",
    "Md. Rayhan Ahmed",
    "Tahmina Aktar",
    "Niloya Roy"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Efficient Channel Attention",
    "Local and Global Feature Aggregation",
    "Human-Computer Interaction",
    "Deep Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech emotion recognition (SER) is crucial for enhancing affective computing and enriching the domain of human-computer interaction. However, the main challenge in SER lies in selecting relevant feature representations from speech signals with lower computational costs. In this paper, we propose a lightweight SER architecture that integrates attention-based local feature blocks (ALFBs) to capture high-level relevant feature vectors from speech signals. We also incorporate a global feature block (GFB) technique to capture sequential, global information and long-term dependencies in speech signals. By aggregating attention-based local and global contextual feature vectors, our model effectively captures the internal correlation between salient features that reflect complex human emotional cues. To evaluate our approach, we extracted four types of spectral features from speech audio samples: melfrequency cepstral coefficients, mel-spectrogram, root mean square value, and zero-crossing rate. Through a 5-fold cross-validation strategy, we tested the proposed method on five multi-lingual standard benchmark datasets: TESS, RAVDESS, BanglaSER, SUBESCO, and Emo-DB, and obtained a mean accuracy of 99.65%, 94.88%, 98.12%, 97.94%, and 97.19% respectively. The results indicate that our model achieves state-of-the-art (SOTA) performance",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The intricate process of human communication includes the exchange of knowledge between individuals using a variety of modalities, including speech, gestures, and facial expressions. Individuals regularly interact with information and emotions through speech  [1] . Speaking causes sound waves to be released into the environment, which the listener's ears pick up on. The listener's brain then decodes those sound waves to produce important details, such as the meaning of those words and their underlying emotions. Speech emotion recognition (SER) is an approach used to recognize and comprehend the emotional states of individuals by analyzing their speech, audio, and video. It works by extracting features from speech signals and mapping them to specific emotional states like happiness, sadness, anger, or fear  [2] . SER provides various practical applications, such as improving interactions between humans and computers, otherwise known as human-computer interaction (HCI). Furthermore, in domains like psychology and healthcare, SER can be utilized to understand patients' emotional states and provide relevant interventions  [3] . Despite it's potential benefits, SER remains a challenging task. Due to the intricate nature of human emotions and the diverse characteristics of vocal signals, achieving accurate recognition results poses a significant challenge.\n\nFeature extraction is a crucial part of building an accurate and robust SER model. The most essential and extensively used features in SER are acoustic features. In several studies, SER has been developed by incorporating a wide range of acoustic features, including prosodic features such as pitch, intonation, energy, and loudness, spectral features like centroid, flux, mel-frequency cepstral coefficients (MFCCs), and mel frequency magnitude coefficient, as well as voice-quality-related features such as jitter, shimmer, and harmonic-to-noise ratio  [4, 5, 6] . Additionally, features like the energy operator have also been incorporated into this process  [4] . These features are used to acquire both the local and global contextual dependencies for the model to train upon in building SER systems. Both prosodic features and spectral features in speech contain emotional information, for that reason, they can be utilized to recognize speech emotions. Since prosodic and spectral features fluctuate over time, speech audio is segmented into frames of suitable sizes in SER systems. Low-level descriptor (LLD) features such as MFCC, Zero crossing rate (ZCR), centroid, pitch, energy, Chromagram, root-mean-square (RMS), root-mean-square energy (RMSE), spectral contrast, and roll-off are widely used in the SER task  [7] . In our study, we have extracted four different types of features from the speech audio and they are MFCC, mel-spectrogram, ZCR, and RMS value features. Selecting optimized features and an accurate classifier is a crucial aspect of SER because it can significantly impact the system's overall performance and preciousness. Over time, a wide variety of classifiers have been integrated for SER, and research in this field is ongoing. Some well-known classifiers in previous studies include Support Vector Machines (SVM)  [8] , Hidden Markov Model (HMM)  [9] , Gaussian Mixture Model (GMM)  [10] , K-nearest neighbors (KNN)  [11] , decision trees  [12] , and ensemble approaches  [7] . However, recent trends in SER research have shifted towards using deep learning-based frameworks such as Deep Neural Networks (DNN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN) used in SER tasks  [13, 7, 14] . In addition, techniques like transfer learning, multitask learning, auto-encoders, attention mechanisms, adversarial training, etc, are also being utilized by researchers  [15, 16] .\n\nRecent advancements in signal processing and deep learning-based methods have enriched the speech-processing domain. Deep learning is used for salient feature extraction which performs well for many image classification, SER, computer vision, and natural language processing-related tasks. Deep learning is essential in data science which has a close relationship to statistics and predictions  [17] . Researchers have discovered that deep learning-based methods can express speech features in higher dimensions, which can be highly challenging for traditional methods. Since each model is different from the other ones, there is bound to be some complementary information regarding the represented features. To fully understand and utilize these complementary aspects, numerous studies have been conducted in the field of deep learning-based speech feature combinations  [14] . Studies showed that 1D CNN can extract the local features efficiently from sequential data  [18] . The capability of 1D CNNs to extract these features from sequential data makes them highly useful across a wide range of applications. Time-series data are successfully processed using the 1D CNN model and audio classification tasks have shown tremendous promise  [19] . To capture the longterm contextual correlations, temporal information, and understand the cues of emotions from speech, researchers have used Recurrent Neural Networks (RNN)  [20] , Gated Recurrent Unit (GRU)  [21, 7] , Stacked Ensemble, Long Short Term Memory (LSTM)  [22] , Bidirectional Long Short-Term Memory (BiLSTM)  [23]  along with 1D CNN and Fully Connected Networks  [24] . The CNN-based network can selectively focus on the most informative channels integrated with the channel-based attention method introduced by Wang et al.  [25]  which is an extremely lightweight efficient channel attention network (ECA-NET) module that calculates the relevance of each channel in the 1D CNN feature vectors. In recent years, ensemble learning performed well in many sectors along with SER. Ensemble learning improves accuracy and robustness by combining the predictions of multiple models, decreasing the risk of over-fitting, and increasing the diversity of the learned representations using the 1D CNN model  [26] . The main objective of incorporating multiple models in a single framework is to capture both spatial and temporal cues from the speech audios which can provide improved and robust SER performance.\n\nInspired by the notable achievements of deep learning-based models in the SER domain and the ability of attention mechanisms to re-calibrate fea-tures based on their relevance, we propose an architecture that integrates attention-based local feature blocks (ALFBs) which incorporates 1D CNN, ECA-Net, batch normalization, and 1D max-pooling for extracting high-level hidden local features. Parallel to the ALFBs the architecture also integrates global feature blocks (GFBs), which use the BiLSTM network and batch normalization for extracting global features with contextual and temporal dependencies efficiently. This allows the proposed framework to acquire contextual dependencies and acquire necessary long-term patterns for the precise identification of emotions within the speech. To evaluate our approach, we employed five widely-used benchmark datasets of multiple languages from the public domain: Toronto Emotional Speech Set (TESS)  [27] , Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)  [28] , Bangla speech emotion recognition dataset (BanglaSER)  [29] , SUST Bangla Emotional Speech Corpus (SUBESCO)  [30] , and Berlin Database of Emotional Speech (Emo-DB)  [31] . Furthermore, the domain of audio-based SER has seen limited exploration in the context of the Bangla language.\n\nThe limited number of samples present in individual classes in each of these datasets poses a significant challenge for the effective training of deep learning-based models. Additionally, some of these datasets have problems of class imbalance. To address these issues, we augmented the data, which improved our proposed model's generalization and overall performance. Figure  1 , represents a general and comprehensive high-level overview of the workflow of the proposed SER system. We extracted MFCC, mel-spectrogram, ZCR, and RMS value features from the audio samples as they are known to be beneficial for this task. The proposed model is trained using the mean values of these extracted features to enhance the recognition performance in detecting various human emotions, including \"happiness\", \"sadness\", \"surprise\", \"fearful\", \"anger\", \"boredom\" and \"neutral\" from audio signals. Our model demonstrates great state-of-the-art (SOTA) results for the SER task compared to most existing literature. This work's significant contributions are as follows:\n\n• Our proposed model utilizes a dual channel architecture where in the first channel, we proposed four blocks of efficient channel attentionbased local feature block (ALFB), and parallelly in the second channel, we incorporate two blocks of global feature block (GFB).\n\n• The ALFBs extract the hidden high-level local spatial features from the features extracted from speech audios by effectively combining 1D CNN and ECA-Net. It helps the model selectively focus on salient features and reduce the influence of less relevant features in the network.\n\n• The GFBs extract the contextual global features using BiLSTM which can further produce better feature representations to acquire more accurate emotional cues.\n\n• We have carried out a comprehensive set of experiments using five widely used, publicly accessible benchmark datasets incorporating English, German, and less-resourced Bengali language for SER: TESS, RAVDESS, BanglaSER, SUBESCO, and Emo-DB.\n\n• The performance of the proposed model is compared with the previous SOTA models. After conducting 5-fold cross-validation, our model achieves the SOTA mean accuracy of 99.65% for TESS, 94.88% for RAVDESS, 98.12% for BanglaSER, 97.94% for SUBESCO, and 97.19% for Emo-DB datasets. These are improved results compared to the previous SOTA methods on each dataset and provide great generalization ability.\n\nThe subsequent sections of this paper are organized as follows: Section II provides an overview of related works conducted in recent years, while Section III delves into the methodology and detailed discussion of the proposed framework. Section IV covers the datasets used and the features extracted in this study. The experimental analysis is presented in Section V, followed by a discussion of the obtained results and a comparison with other studies in Section VI. Lastly, Section VII concludes the paper with a summary of the findings.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Related Works",
      "text": "This section presents recent findings and studies related to SER tasks. Deep learning is highly useful for data scientists who deal with collecting, analyzing, and understanding huge amounts of data. Deep learning-based models have performed tremendously well in SER systems. Kwon et al.  [32]  proposed a model based on a one-dimensional dilated CNN with residual blocks using a skip connection to recognize the local features from each segment of the speech signal. The proposed model was evaluated by the IEMOCAP and Emo-DB datasets with a prediction accuracy of 73% and 90% respectively. Alluhaidan et al.  [33]  combined both MFCCs and time-domain features (MFCCT) to enhance the SER performance. The proposed hybrid features were given to a CNN to build the SER model. The model achieves 97%, 93% and 92% accuracy on the Emo-DB, SAVEE, and RAVDESS datasets, respectively. Patnaik  [34]  proposed a deep sequential model that classifies emotions based on complex mel frequency cepstral coefficients (c-MFCCs). The c-MFCCs capture both the magnitude and phase information of the speech signal which provides more comprehensive information. A 7-layered sequential deep CNN model is used to learn the temporal dependencies. Ahmed et al.  [7]  proposed a model that used MFCC, Chromagram and Pitch, LMS, ZCR, and RMS features to extract information from speech signals. They provided a novel approach for SER by combining 1D-CNN, LSTM, and GRU neural networks in an ensemble model. For the RAVDESS dataset, the suggested model achieved a weighted average accuracy of 95.62%, 99.46% on the TESS, 95.42% on the Emo-DB, 93.22% on SAVEE and 90.47% on CREMA-D datasets, respectively. Zhong  [35]  presents an SER system based on SVM and CNN with MFCC feature extraction. The author obtained two accuracies that are low. The penalty coefficient and Gamma value have been changed to improve the accuracy of SVM. For CNN, the dropout layer has been added to the CNN structure and changed the L2 normalizer and number of epochs to increase the accuracy. The author used the CASIA Chinese emotional speech database to train the model. Sadia et al.  [2]  proposed a model which used deep CNNs and a BiLSTM with a time-distributed flatten layer for the SER focusing on the Bangla language. The proposed model has attained a SOTA perceptual efficiency achieving weighted accuracies of 86.9%, and 82.7% for the SUBESCO and RAVDESS datasets, respectively. Aggarwal et al.  [36]  provided two different feature extraction methods to address successful SER. The first set of features has been obtained using principal component analysis (PCA) along with a DNN with dense and dropout layers. In the second approach, they extracted mel-spectrogram images from the audio files. After that, the images are sent as input to the pre-trained Visual Geometry Group-16 (VGG-16)  [37]  model. They tested their model using two datasets. The accuracy of the proposed-I model was 73.95% and 99.99% on the RAVDESS and TESS datasets, respectively. On the other hand, the accuracy of the proposed-II model was 81.94% and 97.15% on the RAVDESS and TESS datasets, respectively. Ala Saleh Alluhaidan et al.  [33]  proposed a hybrid feature called MFCCT which is a combination of MFCC and time-domain features. The derived features were used as input for an 1D CNN model. On the Emo-DB, SAVEE, and RAVDESS datasets, using hybrid MFCCT feature and 1D CNN achieved an accuracy of 97%, 93%, and 92%, respectively. Nowadays, attention mechanisms are widely known to have a lot of potentials to improve the performance of deep CNNs. Some attention architectures that have been widely used by researchers include Channel Attention, Squeeze-and-Excitation Networks (SEnet)  [38] , Convolutional Block Attention module (CBAM)  [39] , and ECA-Net  [25] , etc. Zou et al.  [40]  proposed a model using three different encoders CNN, BiLSTM and the transformerbased wav2vec2. These encoders help to extract the acoustic information. CNN is used for extracting the spectrogram. Similarly, for MFCC, they used BiLSTM, and for extracting the raw audio signals wav2vec2 is also used. In the wav2vec2 embedding, they applied a co-attention module. Based on the MFCC and spectrogram features, the attention module assigns weights to each frame. After that, they combine all three extracted features. The experimental results achieved an weighted accuracy of 71.64% and an unweighted accuracy of 72.70% on the IEMOCAP dataset. Zhao et al.  [41]  proposed a hybrid architecture that uses parallel convolutional layers integrated with SEnet to extract relationships from 3D spectrograms. In the classification block, they introduced a self-attention Residual Dilated Network (SADRN) with Connectionist Temporal Classification (CTC) loss for discrete SER. The suggested technique achieves a weighted accuracy of 73.1%, unweighted accuracy of 66.3% on IEMOCAP, and an unweighted accuracy of 41.1% on the FAU-AEC dataset, respectively. Mustaqeem et al.  [42]  proposed a self-attention module that receives a transitional feature map. It produces a channel and spatial attention map. They incorporate dilated CNN in spatial attention for extracting spatial information. Moreover, a multi-layer perceptron in channel attention is also added to extract global features. The accuracy of the proposed model is 78.01%, 80.00%, and 93.00% over IEMOCAP, RAVDESS, and EMO-DB datasets, respectively. Sun et al.  [43]  developed a multimodal cross and self-attention network. The crossattention module captures inter-modal interactions between acoustic frames and textual words in pairs. Meanwhile, the self-attention module utilizes the self-attention mechanism to propagate information within each modality. The model achieved a weighted accuracy of 61.2% and an unweighted accuracy of 56% over the IEMOCAP dataset. For the MELD dataset, the model achieved an F1-score of 59.2%.\n\nEnsemble learning is used for improving predicting performance by combining multiple models. Researchers use ensemble learning to increase ML models' accuracy, robustness, and generalization performance. Zhang et al.  [44]  proposed an ensemble model that combines a random forest classifier with the weighted binary cuckoo search method to select the optimal feature subset. Falahzadeh et al.  [45]  proposed a new transform of speech signals into a chromatogram which is derived from the reconstructed phase space of speech. This chromatogram is basically a color image. The extracted images are passed through the DCNN with a gray wolf optimization module for optimizing the hyperparameters of the model. Chalapathi et al.  [46]  proposed a model that utilizes the adaptive boosting ensemble method and the fuzzy cmeans approach. This model extracts high-dimensional acoustic features for emotion recognition from speech audio signals using the benchmark dataset RAVDESS. Zvarevashe et al.  [47]  proposed a stacked ensemble algorithm to recognize a cross-lingual acoustic emotional valence by using logistic regression, random decision forest, gradient boosting, and AdaBoost algorithms. Five speech emotion corpora SAVEE, EMO-DB, RAVDESS, CREMA-D, and EMOVO that serve as the study's materials are combined in the proposed models.\n\nIn contrast to the studies discussed above, the primary objective of this paper is to exploit the distinctive attributes of diverse neural network architectures. This is done to effectively capture both low-level and highlevel representations of speech features within an optimized attention-guided framework, which emphasizes the preservation of salient feature vectors. An extensive comparative evaluation is provided in Tables  6 to 10 . This comparison is performed between the notable works discussed in the literature review section and our proposed work. In the comparison, we highlight utilized datasets, the methodology, extracted features, and achieved results.",
      "page_start": 6,
      "page_end": 9
    },
    {
      "section_name": "Methodology",
      "text": "As depicted in Figure  2 , our initial step involved utilizing the SER dataset. In Stage 1, we extracted MFCC, mel-spectrogram, RMS, and ZCR values, which were then represented as a feature vector. This feature vector was combined and the dimension is (3210, 150). Subsequently, we split the feature vector into training, validation, and testing sets with a ratio of 60:25:15 respectively. For model training, we fed both the training and validation data. Moving on to Stage 2, our proposed model employed four ALFBs and two GFBs to extract local and global features. Then we concatenated both ALFB and GFB. The classification results were obtained by applying the softmax function after performing the execution of the dense block. Finally, we evaluated our model using the test data. Additionally, we conducted 5fold cross-validation, which will be discussed in the upcoming sections. The evaluation metrics are presented in Stage 3. This section provides a detailed overview of the proposed architecture as depicted in Figure  3 .",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Proposed Dual Channel Ser Model",
      "text": "We have designed dual-channel attention-guided 1D CNN-BiLSTM networksbased deep neural networks. One channel is used for extracting high-level hidden local features. The other channel is utilized for contextual and sequential global feature extraction. In addition, ECA-Net employs a parallel convolutional structure that enables it to capture both local and global contextual information in the input signal, which can be important for recognizing complex emotional expressions.  In figure  4 , X is the Input feature map, where C, H, and W are Channel dimensions, height, and width, respectively. To describe the outcome of a given transformation, F tr : X → U, X ∈ R H ′ ×W ′ ×C ′ , U ∈ R H×W ×C and X represent the resulting feature map. F sq represents the squeeze operation that reduces the spatial dimensions of each feature map to 1x1, resulting in a channel-wise representation. The calculation formula is given below:\n\nF ex denotes the excitation operation. The goal here is to learn a set of weights for each channel of the input feature map X to capture the importance of each channel. These weights are learned using a small neural network consisting of one or more fully connected layers. The calculation formula is:\n\nIn Eq 4, W1 and W2 represent the two fully connected layers. F s cale denotes the scaling operation which is also known as reweighting. This is done by multiplying each channel of the original feature maps. For this reason, it gives more importance to the channels that are important and reduces the significance of less important channels. The result is a feature map that is focused on the most relevant information, which can be beneficial for improving the performance of deep neural networks.\n\nHowever, SE-Net also has some disadvantages. Sometimes, it loses important information during the process of squeezing and capturing the dependencies of all channels. To overcome this issue, ECA-Net is proposed. It allows the network to selectively focus on the most informative features. For that reason, it can easily suppress the noise and irrelevant information.  In Figure  5 , X is the Input feature map, where C, H, and W are Channel dimensions (number of filters), height, and width, respectively. A global average pooling (GAP) operation is applied to the input feature map X which compresses H × W × C data into 1 × 1 × C without dimensionality reduction and the features of each channel are aggregated. In ECA-Net, first, we adaptively select the kernel size K = 1 by a function of channel dimension C. After that, a one-dimensional convolution of size K is performed. Finally, a sigmoid function has been performed to learn channel attention. The convolution kernel size (K), plays an important role in identifying the coverage of interaction within the data. This parameter is related to the channel dimension C, which is typically chosen to be a power of 2. The mapping relationship between these two parameters can be calculated as follows:\n\nThe computational formula of the convolution kernel size K is given below.\n\nHere, |t| odd indicates the nearest odd number of t, γ is set to 2, and b to 1.\n\nAfter that, the output feature map from ECA-Net is reshaped so that it can pass through the third part of our model which is batch normalization (BN). It increased the training stability and speed of the network. As a result, it is passed through the final part 1D max-pooling layer with a pool size of two, a stride of one, and using the padding as 'same'. It is used to produce the same output length similar to the input sequence length while keeping the most important features. The following three ALFBs are employed, with kernel sizes of 5, 5, and 3 for the 1D convolution layers. The filters on these blocks are 256 except for the last block which is 32. Padding and stride configurations are the same as the previous blocks. In addition, ECA-Net is also performed after each 1D convolution layer. Batch normalization is also passed through these layers and a 1D max-pooling with the following three ALFBs with a pool size of 2, 2, 5, and a stride of two is performed. After that, we passed this input into a flattened layer.",
      "page_start": 10,
      "page_end": 14
    },
    {
      "section_name": "Global Feature Blocks",
      "text": "The global feature blocks (GFBs) are responsible for global contextual feature extraction and consist of BiLSTM. It can capture how speech features change over time by processing speech frames or signals in both forward and backward directions. They take into account data from previous and forthcoming frames, allowing the model to comprehend the larger context and identify long-term patterns crucial for precise emotion recognition in speech. For the SER task, it is important because emotional cues in speech often occur over a longer period of time than just a few milliseconds. For that reason, it is influenced by the context of preceding and succeeding speech segments. The LSTM unit has four fundamental components, including one input gate i t with corresponding weight matrices W x i , W c i , W h i and bias b i , one forget gate f t with corresponding weight matrices W x f , W c f , W h f and bias b f , one output gate o t with corresponding weight matrices W xo , W co , W ho and bias b o , and the cell state c t . Each of these gates calculates its output based on the current input x i , the previous hidden state h (t-1) , and the previous cell state c (t-1) . Simultaneously, the same input sequence is passed through the backward LSTM layer. It processes the sequence in reverse order. Additionally, it also maintains hidden states and memory cell states. The computational formula is given below:\n\n)\n\nThe final output of the BiLSTM is calculated by concatenating the forward and backward hidden states at each time step. Eq. 11 represents the final output.\n\nIn our proposed model, BiLSTM has been performed using the input from the input layer with units of 512 and making the return sequence true. Then it is passed through a BN layer to improve the training stability and efficiency of the network. Furthermore, we have used dropout with a dropout rate of 0.1 to prevent over-fitting and improve the generalization performance of our model. The next GFB used the same configuration as the previous GFB except for the dropout rate. In this block, we incorporate the dropout rate is 0.2 in the dropout layer. After that, the output of the dropout layer is passed through the 1D global average pooling layer and merged with the output of the other channel.\n\nFinally, we concatenated both the final ALFB and GFB together to enhance the model's performance with better feature representation consisting of hidden local emotional cues as well as long-term contextual relations and dependencies. After merging both the local and global features, it is passed through a dense layer with a unit of 32, followed by a batch normalization layer. The output layer utilizes the softmax activation function to differentiate between the emotions present in the speech audio.",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "Dataset And Feature Extraction",
      "text": "",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Dataset",
      "text": "For this study, we have used five different datasets: TESS, RAVDESS, BanglaSER, SUBESCO, and Emo-DB which cover English, Bengali, and German languages.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Tess:",
      "text": "The TESS  [27]  dataset is a collection of high-precision, highcadence time-series photometry data gathered by the TESS mission. This dataset contains 200 target words that are spoken by two actresses, one aged 26 and the other aged 64. The dataset comprises 2800 audio recordings and is nicely balanced and represents seven emotions: \"happy\", \"neutral\", \"angry\", \"disgust\", \"fear\", \"surprise\", and \"sad\".",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Ravdess:",
      "text": "The RAVDESS  [28]  has been an extensively used dataset in the field of SER. It contains audio and video recordings of 12 male and 12 female performers. This dataset contains 8 different emotional expressions such as \"sad\", \"fearful\", \"happy\", \"calm\", \"angry\", \"surprised\", \"disgust\", and \"neutral\". In this research, our focus was on utilizing speech audio samples. There are 1440 audio samples overall. Each audio sample has a sample rate of 48 kHz and 60 trials per performer. It is a balanced dataset except for the \"neutral\" class. It contains fewer records than the other classes.\n\nBanglaSER: The BanglaSER  [29]  is an important resource for research on SER in Bengali, a language spoken by over 200 million people. The ban-glaSER dataset comprises speech-audio data from 34 participating speakers. It includes an equal distribution of 17 male and 17 female nonprofessional actors from diverse age groups ranging between 19 and 47 years. The dataset has a sampling rate of 44.1 kHz with 306 recordings for each of \"happy\", \"sad\", \"angry\", and \"surprised\" emotions, but there were 243 recordings available for the \"neutral\" emotion.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Subesco:",
      "text": "The SUBESCO  [30]  dataset has been used in ML and deep learning studies on emotion recognition in speech. This dataset is a genderbalanced dataset. The dataset includes voice data from 20 professional speakers, with an equal distribution of 10 male and 10 female speakers. Each of them expressed seventh different emotions: \"sadness\", \"happiness\", \"surprise\", \"anger\", \"disgust\", \"fear\", and \"neutral\". In the context of the Bengali language, this dataset stands as the largest with a total duration of 7 hours, and encompasses 7000 utterances.\n\nEmo-DB: The Emo-DB  [31]  dataset is a collection of emotional audio clips in the German language, consisting of 535 recordings from 10 German speakers (5 male and 5 female) expressing seven distinct emotions: \"anger\", \"boredom\", \"disgust\", \"sadness\", \"happiness\", \"fear\", and \"neutral\". Each recording is accompanied by subtitles indicating the speaker's gender, the spoken sentence, and the expressed emotional state.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Data Augmentation",
      "text": "In the field of SER, data augmentation serves as a valuable technique for expanding the quantity and diversity of training data, which can enhance the effectiveness and generalization of ML-based models. By performing various transformations on the existing data, data augmentation involves producing new training samples. Due to the relatively low number of speech utterance records in each class, this study employs five types of audio data augmentation techniques., Additive White Gaussian Noise (AWGN) injection  [48] , pitch shifting, time-stretching, adding noise on the pitch shifting data, and adding pitch shifting on the time-stretching data. We added AWGN to the samples by using NumPy's uniform method with a rate of 0.015. Pitch shift was performed using the librosa library in Python by employing a factor of 0.7. Speed or Duration has been added without changing the pitch by stretching the time using the method called time stretch -python's librosa library with a factor of 0.8. We also added noise to the pitch-shifted audio files. In addition, we also shifted the pitch of the audio after stretching the time using the same procedure. Notably, these augmentation techniques were implemented without compromising the performance of the SER system. After performing data augmentations, the overall samples of TESS, RAVDESS, BanglaSER, SUBESCO, and Emo-DB datasets increased to 16799, 8637, 8796, 41991, and 3210, respectively.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Extracted Features",
      "text": "To enhance the performance of our model, we have extracted various spectral features as the initial feature set which include MFCC, mel-spectrogram, ZCR, and RMS values from the audio samples. MFCC is one of the widely used features for the SER task  [7, 34] . We focused on MFCCs because they are often used in speech processing and are good candidates for speech feature extraction since they capture the spectrum features of audio signals. They are particularly useful for conveying phonetic and prosodic information because they closely resemble the way the human auditory system reacts to sound. They can aid in capturing variations in speech that express emotion, like shifts in pitch, intonation, and timbre, in the process of recognizing emotions  [49] . The extraction process relies on the inherent characteristics of the human auditory system, which serves as a natural reference for speech recognition. To obtain MFCC, the speech frame undergoes an initial application of a hamming window. Subsequently, the discrete spectrum is computed using a discrete fourier transform (DFT)  [50] . MFCC can be calculated through Eq. 12, where N is the sample length, h(n) is the hamming window and K is the length of the DFT.\n\nThe human auditory system perceives frequencies on a logarithmic scale, and the mel-spectrogram is a logarithmic representation of signal frequencies that share this characteristic. Mel-spectrogram visualizes the power of frequency bands over time, showing the relative importance of different frequency bands, similar to how our ears perceive sound  [51] . We use the mel-spectrogram to depict the spectrum features of speech. With MFCCs, it is a helpful function that is commonly used. Mel-spectrograms can highlight important frequency components and acoustic patterns in audio that point to emotional content. Mel-spectrogram helps to identify important information related to the spectral content and the frequency distribution of speech  [52] . The correlation between the mel spectrum frequency (f mel ) and the signal frequency (f Hz ) can be characterized as follows:\n\nZCR records the frequency at which the audio stream crosses the threshold of zero amplitude. We looked at it because of the information it can provide about how rapidly the audio signal changes and how it relates to speech tempo and prosody. ZCR shifts during emotive speech may be a reflection of rhythmic changes that might convey subtle feelings  [53] . In addition, ZCR divides the number of samples in a specific region of an audio frame. After that, it provides the number of zero crossings in that region. Mathematically, the ZCR can be calculated using Eq. 14, where x represents the length of N. For the positive sample amplitude, the α function provides 1 and 0 for the negative sample amplitude over a time frame (t).\n\nwhere, α = 1,\n\nRMS is used to measure the loudness of the sound. RMS is widely used by researchers for the SER task that uses the magnitude of the audio signal  [7, 54] . It provides information about the intensity of the speech signals. It measures the average power of an audio signal. It is practical for spotting variations in loudness and intensity, which may be a sign of various emotional states  [55] . RMS can be calculated by considering the square root of the sum of the mean squares of the amplitudes of the sound samples. Eq. 15 represents the formula of RMS.\n\nIn this study, we extracted a total of 20 MFCC features, 128 Mel-spectrogram features, as well as one RMS and one ZCR feature. These features are combined to create a feature vector with a dimension of 3210 × 150.",
      "page_start": 18,
      "page_end": 20
    },
    {
      "section_name": "Experimental Analysis",
      "text": "In this section, we discuss about the Experimental Setup, and Hyperparameter Tuning for the model's optimization.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Experimental Setup",
      "text": "This section focuses on the setup of the proposed model. The specific details of our model are provided in Table  1 . In order to train the model, we needed to augment the dataset as there were not enough training examples. We used a 60:25:15 ratio to partition the augmented dataset, with 60% of the data used for training the model, 25% for validation, and the remaining 15% for testing the model. We set the batch size to 32 and the learning rate at 0.0001 which is selected by the hyperparameter tuning. The model was trained for 100 epochs in each dataset. For training, the system used an NVIDIA Tesla P100 GPU, 16 GB of RAM, and the TensorFlow backend. The proposed model has a total of 9.14 million trainable parameters. To train the model, we incorporated a loss function by adding categorical cross-entropy. The loss function can be defined as follows:\n\nThe loss function Loss CE , as described in Eq. 16, can be defined in terms of the number of classes N, the natural logarithm, and the true probability distribution y, along with the predicted probability distribution ŷ for each class c. The model adjusts its weights and biases to minimize the categorical cross-entropy loss function.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Hyperparameter Tuning",
      "text": "For fine-tuning our proposed model, we have chosen Keras Tuner  [56]  as the hyperparameter tuner. This library is specifically developed for optimizing hyperparameters in Keras. It provides a simple and flexible API for automatically searching the hyperparameters of the deep learning model to enhance its performance. Table  2  represents the hyperparameters used in the ALFBs.\n\nWe have also tuned two GFBs using the hyperparameters mentioned in Table  3 . After merging the ALFBs with the GFBs, we passed the sequence through an output block. Table  4  represents the hyperparameters used in the output block.\n\nAfter performing hyperparameter tuning using the Keras Tuner on the utilized datasets, we obtained the best model parameters which improved the overall performance of our model. The results of hyperparameter tuning showed that the ALFBs performed well with 1D convolution layers that had filter sizes of 256, 256, 256, and 32 and kernel sizes of 5, 5, 5, and 3, respectively. The pool size for the 1D max-pooling layers was set to 2, except for the last layer which had a pool size of 5. Based on the results of the hyperparameter tuning, in the GFBs, we found that both the BiLSTM layers performed well with 512 units. For the output blocks, a unit size of 32 was selected for the first dense layer. We evaluated multiple optimizers, including Adam and RMSprop, and found that Adam performed well for our model. The learning rate of Adam was set to 1e-4 from 1e-2, 1e-3, and 1e-4. We incorporate the resultant output parameter into our model to perform the SER task efficiently.",
      "page_start": 22,
      "page_end": 23
    },
    {
      "section_name": "Result Analysis",
      "text": "In this section, we analyze the achieved results of the proposed model and provide an in-depth comparative analysis with other SOTA existing methods.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Evaluation Metrics",
      "text": "In order to measure the performance of our model we have chosen precision, recall, F1 score, and accuracy.\n\nPrecision: Precision is a performance measure that evaluates the effectiveness of a classification model  [57] . It quantifies the ratio of true positive predictions to the total number of positive predictions made by the model.\n\nIn the form of an equation, precision can be written as:\n\nWhere TP stands for true positives (cases that were correctly labeled as positive) and FP stands for false positives (incorrectly classified positive instances).\n\nRecall: The concept of recall  [58]  can be represented mathematically as follows:\n\nWhere TP stands for true positives (cases that were correctly labeled as positive), and false negatives, are indicated by FN (incorrectly classified negative instances).\n\nF1-score: The F1-score  [59]  is a single metric that provides a balanced measure of the effectiveness of a classification model by combining both precisions and recalls into a single value. The F1 score can be stated as an equation, which looks like this:\n\nWhere precision and recall are the respective values obtained from the confusion matrix of the classification model.\n\nK-fold cross-validation: K-fold cross-validation  [60]  is commonly used to evaluate ML models. It divides the data into k equal-sized segments, trains the model on k-1, and evaluates the remaining subsets. Each subset is tested once. Validation accuracy is used to monitor the performance of the model during training and to modify its hyperparameters. For this study, we have performed a 5-fold cross-validation to evaluate the model's overall performance. Mean accuracy can be expressed as an equation:\n\nThe mean accuracy = 1\n\nWhere, K is the number of folds, and E i denotes the evaluation metric (such as accuracy, F1-score, etc.) computed for the i th fold. The average of the evaluation metrics across all the folds gives an estimate of the model's performance.",
      "page_start": 23,
      "page_end": 24
    },
    {
      "section_name": "Performance Analysis",
      "text": "We have utilized five benchmark datasets such as TESS, RAVDESS, Ban-glaSER, SUBESCO, and Emo-DB. Table  5  represents the five folds crossvalidation for each data set and reports validation accuracy, precision, recall, F1-score, and mean accuracy for each dataset. We have taken the weighted average precision, recall, and F1-score as the evaluation metrics for our model. The mean validation accuracy for the TESS dataset is 99.65%, the mean precision is 99.65%, the mean recall is 99.65% and the mean F1score is 99.65%. The mean validation accuracy for the RAVDESS dataset is 94.88%, the mean precision is 95.09%, the mean recall is 94.87%, and the mean F1-score is 94.89%. The mean validation accuracy for the BanglaSER dataset is 98.12%, mean precision is 98.21%, mean recall is 98.18%, and mean F1-score is 98.18%. The mean validation accuracy for the SUBESCO dataset is 97.94%, the mean precision is 97.70%, the mean recall is 97.69%, and the mean F1-score is 97.69%. The Emo-DB dataset's mean validation accuracy is 97.19%, it's mean precision is 97.4%, mean recall is 97.18% and it's mean F1-score is 97.18%. The mean accuracy of the proposed model in all the datasets is presented in Figure  7 .\n\nThe confusion matrix represents the true positive and true negative values along the diagonal. In addition, the off-diagonal entries represent instances of misclassification or confusion between different classes. Figure  8 , represents the confusion matrix of the five benchmark datasets of our proposed model. Our model is comparatively better at classifying different emotion classes. The features of the TESS and Emo-DB datasets are classified well by the proposed model. In most of the datasets, Happy and Sad are perfectly classified by our proposed model. In the SUBESCO dataset, Disgust shows the highest misclassification rate which is confused with Angry and Surprise. The rest of the datasets also show a good classification result in the confusion matrix. Figure  9     graph. Training accuracy measures the accuracy of the model on the training set during the training process, while validation accuracy measures the accuracy of the model on the validation set, which is a subset of the data that was held out from the training process. The alignment of these metrics is a good indicator that our model is not over-fitting or under-fitting the data, and is capable of generalizing well to new and unseen data. Based on this observation, we have run model for 100 epochs to ensure optimal performance. The Receiver Operating Characteristic (ROC) curve is an important metric used to evaluate the performance of a development model. It can be used for binary or multi-class classification. In binary classification, it is a graphical plot that shows the trade-off between true positive rate and false positive rate, whereas in multi-class classification it shows the diagnostic ability of a classifier for every class. In Figure  10 , the ROC curve of validation sets from five datasets is illustrated, which was utilized to evaluate the proposed approach. For the TESS dataset, all seven classes are showcasing superior recognition abilities with an Area Under Curve (AUC) score of 1. Similarly, the RAVDESS dataset exhibits significant performance in classifying eight distinct classes. Additionally, great classification performance was showcased by five different emotions in the BanglaSER dataset. Moreover, the SUBESCO dataset, composed of seven different emotions, showcases great distinction ability. Finally, the Emo-DB dataset with seven different types      of emotions is displaying impressive classification performance. Tables 6 to 10 present the results of a performance evaluation conducted on five different datasets. Our main objective is to compare the effectiveness of our proposed model against previous SOTA approaches. To ensure a fair comparison, we have prioritized the classification performance of the model over computational efficiency, taking into account factors such as the methodology employed, the extracted features, and the accuracy of the approaches. Results presented in Table  6 , demonstrate that our proposed method outperforms SOTA architectures on the TESS dataset, achieving an impressive classification accuracy of 99.65%. Our model surpasses all SOTA architectures in this dataset. In Table  7 , when considering the RAVDESS dataset, the 1D-CNN-LSTM-GRU model  [7]  achieves the highest accuracy among the other models, reaching 95.62%. However, our model performs significantly better compared to the existing models in this dataset. Turning to the BanglaSER dataset in Table  8 , our proposed model achieves significantly improved results compared to all SOTA architectures. For a fair comparison, we did not include several papers  [69]  [70]  [71]  that classified six or    four separate categories whereas we used five. In the SUBESCO dataset in Table  9 , our proposed model achieved an accuracy of 97.94%, showcasing a substantial improvement of 11.04% and 22.94% compared to the DCNN and BiLSTM  [2]  as well as the CNN  [65]  architectures, respectively. Lastly, our proposed model achieves an accuracy of 97.19% on the Emo-DB dataset, as shown in Table  10 .",
      "page_start": 25,
      "page_end": 27
    },
    {
      "section_name": "Comparison With State-Of-The-Art (Sota) Approaches",
      "text": "",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Ablation Study",
      "text": "We have conducted an extensive ablation study in the Emo-DB and RAVDESS datasets to examine the impact of incorporated components such as ECA-Net in the ALFBs, as well as the number of incorporated GFBs. Table  11 , represents the experimental results from the ablation study. Here, the proposed model shows the mean validation accuracy with the mean precision, recall, and F1 score. The rest of the experiments in the ablation study was run two times. Each result shows the mean value from the experiments. The study demonstrates that, if we remove the ECA-Net and GFBs from the proposed architecture, the performance of the proposed model tends to reduce. In the Emo-DB dataset, the proposed architecture without ECA-Net and GFBs achieved an accuracy of 95.04%. Removal of the ECA-Net module from ALFBs reduced the mean accuracy by 0.84%. This observation emphasizes the significance of the ECA-Net module in enhancing the model's performance. Furthermore, when the proposed model was tested without one GFB, the accuracy of the model is 97.52%. Removal of GFBs altogether from the model's architecture reduced the recognition performance by 0.44%. In the RAVDESS dataset, we observed a similar pattern in all the evaluation metrics, as shown in Table  11 . Removing the ECA-Net as well as GFBs from the proposed architecture reduced the accuracy by 2.91%. The absence of GFBs and ECA-Net contributed to the loss of accuracy of 0.99% and 1.84%, respectively in the RAVDESS dataset. Comparing the performance of the model without ECA-Net and GFBs as well as the model without ECA-Net shows that both of these integrated modules contribute highly to the overall performance.",
      "page_start": 33,
      "page_end": 34
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, a dual-channel attention-guided SER architecture is proposed that tries to address the challenge of efficient feature representation in the presence of limited data availability. The proposed model's integration of ECA-Net, 1D CNN, and BiLSTM network performs better in representing the local salient features as well as global contextual features of emotional speech utterances. Integration of ECA-Net allows the network on capturing salient and interrelated features between different channels. On the other hand, BiLSTM is integrated to capture global features from speech data. Data augmentation techniques were employed to mitigate the issue of data scarcity and enhance classification performance. Through comprehensive evaluations of five standard benchmark datasets from multiple languages, our proposed model demonstrated SOTA results compared to other studies. Future prospects involve devising a multi-modal architecture that incorporates audio, video, and textual data while maintaining a lower computation cost.",
      "page_start": 34,
      "page_end": 34
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: A graphical illustration of the proposed SER model’s generic workflow.",
      "page": 4
    },
    {
      "caption": "Figure 1: , represents a general and comprehensive high-level overview of the work-",
      "page": 5
    },
    {
      "caption": "Figure 2: , our initial step involved utilizing the SER dataset.",
      "page": 9
    },
    {
      "caption": "Figure 2: Overview of the proposed approach for SER.",
      "page": 10
    },
    {
      "caption": "Figure 3: 3.1. Proposed Dual Channel SER Model",
      "page": 10
    },
    {
      "caption": "Figure 3: Visual representation of the proposed dual-channel architecture.",
      "page": 11
    },
    {
      "caption": "Figure 4: shows the SE-Net",
      "page": 12
    },
    {
      "caption": "Figure 4: Visual representation of the Squeeze and Excitation Network block [38].",
      "page": 12
    },
    {
      "caption": "Figure 4: , X is the Input feature map, where C, H, and W are Channel",
      "page": 12
    },
    {
      "caption": "Figure 5: represents the diagram of the ECA-Net module.",
      "page": 13
    },
    {
      "caption": "Figure 5: Visual representation of the Efficient Channel Attention Network block [25].",
      "page": 13
    },
    {
      "caption": "Figure 5: , X is the Input feature map, where C, H, and W are Channel",
      "page": 14
    },
    {
      "caption": "Figure 6: Visual representation of the Bidirectional Long Short Term Memory (BiLSTM).",
      "page": 15
    },
    {
      "caption": "Figure 7: The confusion matrix represents the true positive and true negative values",
      "page": 25
    },
    {
      "caption": "Figure 9: illustrates, the training and validation accuracy achieved",
      "page": 25
    },
    {
      "caption": "Figure 7: Graphical representation of the mean accuracy, computed via 5-fold cross-",
      "page": 27
    },
    {
      "caption": "Figure 10: , the ROC curve of validation sets",
      "page": 27
    },
    {
      "caption": "Figure 8: Evaluation of the proposed model’s performance on the utilized datasets. Here,",
      "page": 28
    },
    {
      "caption": "Figure 9: Represent the training and validation loss in our proposed dual-channel multi-",
      "page": 29
    },
    {
      "caption": "Figure 10: Performance evaluation of the proposed models for (a) TESS, (b) RAVDESS,",
      "page": 30
    }
  ],
  "tables": [
    {
      "caption": "Table 2: represents the hyperparameters used in",
      "data": [
        {
          "ALFB": "",
          "Convolution layer": "No of filters\nKernel size",
          "1D Max pooling\n(Pool Size)": ""
        },
        {
          "ALFB": "1st",
          "Convolution layer": "256, 512",
          "1D Max pooling\n(Pool Size)": "2,3,5"
        },
        {
          "ALFB": "2nd",
          "Convolution layer": "128, 256, 512",
          "1D Max pooling\n(Pool Size)": "2,3,5"
        },
        {
          "ALFB": "3rd",
          "Convolution layer": "64, 128, 256",
          "1D Max pooling\n(Pool Size)": "2,3,5"
        },
        {
          "ALFB": "4th",
          "Convolution layer": "32, 64, 128",
          "1D Max pooling\n(Pool Size)": "2,3,5"
        }
      ],
      "page": 22
    },
    {
      "caption": "Table 3: Global Feature Blocks (GFBs)",
      "data": [
        {
          "GFB": "1st",
          "BiLSTM (Units)": "128, 256, 512",
          "Dropout": "0.1, 0.2"
        },
        {
          "GFB": "2nd",
          "BiLSTM (Units)": "128, 256, 512",
          "Dropout": "0.1, 0.2"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 3: Global Feature Blocks (GFBs)",
      "data": [
        {
          "Dense Layer": "1st",
          "Units": "8, 16, 32"
        },
        {
          "Dense Layer": "2nd",
          "Units": "Output Labels"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table 5: 5-Fold cross-validation analysis using the TESS, RAVDESS, BanglaSER,",
      "data": [
        {
          "Dataset Name": "TESS",
          "K-Fold": "1",
          "Accuracy (%)": "99.84",
          "Precision (%)": "99.84",
          "Recall (%)": "99.84",
          "F1-Score (%) Mean Accuracy (%)": "99.65"
        },
        {
          "Dataset Name": "",
          "K-Fold": "2",
          "Accuracy (%)": "99.68",
          "Precision (%)": "99.68",
          "Recall (%)": "99.68",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "3",
          "Accuracy (%)": "99.68",
          "Precision (%)": "99.68",
          "Recall (%)": "99.68",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "4",
          "Accuracy (%)": "99.52",
          "Precision (%)": "99.53",
          "Recall (%)": "99.52",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "5",
          "Accuracy (%)": "99.52",
          "Precision (%)": "99.53",
          "Recall (%)": "99.52",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "RAVESS",
          "K-Fold": "1",
          "Accuracy (%)": "94.44",
          "Precision (%)": "94.80",
          "Recall (%)": "94.44",
          "F1-Score (%) Mean Accuracy (%)": "94.88"
        },
        {
          "Dataset Name": "",
          "K-Fold": "2",
          "Accuracy (%)": "96.30",
          "Precision (%)": "96.54",
          "Recall (%)": "96.30",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "3",
          "Accuracy (%)": "95.06",
          "Precision (%)": "95.13",
          "Recall (%)": "95.06",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "4",
          "Accuracy (%)": "94.75",
          "Precision (%)": "94.97",
          "Recall (%)": "94.75",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "5",
          "Accuracy (%)": "93.83",
          "Precision (%)": "94.01",
          "Recall (%)": "93.82",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "BanglaSER",
          "K-Fold": "1",
          "Accuracy (%)": "97.58",
          "Precision (%)": "97.58",
          "Recall (%)": "97.58",
          "F1-Score (%) Mean Accuracy (%)": "98.12"
        },
        {
          "Dataset Name": "",
          "K-Fold": "2",
          "Accuracy (%)": "97.88",
          "Precision (%)": "97.92",
          "Recall (%)": "97.88",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "3",
          "Accuracy (%)": "98.18",
          "Precision (%)": "98.24",
          "Recall (%)": "98.18",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "4",
          "Accuracy (%)": "98.79",
          "Precision (%)": "98.80",
          "Recall (%)": "98.79",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "5",
          "Accuracy (%)": "98.48",
          "Precision (%)": "98.50",
          "Recall (%)": "98.48",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "SUBESCO",
          "K-Fold": "1",
          "Accuracy (%)": "97.51",
          "Precision (%)": "97.55",
          "Recall (%)": "97.51",
          "F1-Score (%) Mean Accuracy (%)": "97.94"
        },
        {
          "Dataset Name": "",
          "K-Fold": "2",
          "Accuracy (%)": "96.97",
          "Precision (%)": "96.99",
          "Recall (%)": "96.98",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "3",
          "Accuracy (%)": "98.75",
          "Precision (%)": "98.51",
          "Recall (%)": "98.48",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "4",
          "Accuracy (%)": "97.42",
          "Precision (%)": "97.4",
          "Recall (%)": "97.42",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "5",
          "Accuracy (%)": "98.04",
          "Precision (%)": "98.06",
          "Recall (%)": "98.04",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "Emo-DB",
          "K-Fold": "1",
          "Accuracy (%)": "97.52",
          "Precision (%)": "97.70",
          "Recall (%)": "97.52",
          "F1-Score (%) Mean Accuracy (%)": "97.19"
        },
        {
          "Dataset Name": "",
          "K-Fold": "2",
          "Accuracy (%)": "99.17",
          "Precision (%)": "99.22",
          "Recall (%)": "99.17",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "3",
          "Accuracy (%)": "98.35",
          "Precision (%)": "98.51",
          "Recall (%)": "98.34",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "4",
          "Accuracy (%)": "92.56",
          "Precision (%)": "93.06",
          "Recall (%)": "92.56",
          "F1-Score (%) Mean Accuracy (%)": ""
        },
        {
          "Dataset Name": "",
          "K-Fold": "5",
          "Accuracy (%)": "98.35",
          "Precision (%)": "98.51",
          "Recall (%)": "98.35",
          "F1-Score (%) Mean Accuracy (%)": ""
        }
      ],
      "page": 26
    },
    {
      "caption": "Table 6: Performance analysis of the proposed work and compare it with the latest liter-",
      "data": [
        {
          "Reference": "Dolka et al.\n[49]",
          "Methodology": "ANN",
          "Features": "MFCC",
          "Accuracy": "99.52%"
        },
        {
          "Reference": "Jothimani et al.\n[61]",
          "Methodology": "CNN+LSTM",
          "Features": "MFCC, RMS, ZCR",
          "Accuracy": "99.6 %"
        },
        {
          "Reference": "Uddin et al.\n[62]",
          "Methodology": "CNN+LSTM",
          "Features": "MFCC, Energy, Fundamental\nfrequency",
          "Accuracy": "97.5%"
        },
        {
          "Reference": "Barkur et al.\n[63]",
          "Methodology": "DNN",
          "Features": "Attention Wavenet",
          "Accuracy": "99%"
        },
        {
          "Reference": "Ahmed et al.\n[7]",
          "Methodology": "1D-CNN-LSTM-GRU",
          "Features": "ZCR, RMS, Chromagram,",
          "Accuracy": "99.46%"
        },
        {
          "Reference": "",
          "Methodology": "",
          "Features": "Log-mel Spectrogram and MFCC",
          "Accuracy": ""
        },
        {
          "Reference": "This work",
          "Methodology": "1D CNN+ECA-Net+BiLSTM",
          "Features": "RMS, ZCR, MFCC,Mel-spectrogram",
          "Accuracy": "99.65%"
        }
      ],
      "page": 31
    },
    {
      "caption": "Table 7: Performance analysis of the proposed work and compare it with the latest liter-",
      "data": [
        {
          "Reference": "Sultana et al.\n[2]",
          "Methodology": "Deep CNN and BiLSTM",
          "Features": "MFCCs, pitch, energy",
          "Accuracy": "82.7%"
        },
        {
          "Reference": "",
          "Methodology": "",
          "Features": "and spectral centroid",
          "Accuracy": ""
        },
        {
          "Reference": "Dair et al.\n[64]",
          "Methodology": "1D CNN",
          "Features": "MFCC, CENS, ZCR",
          "Accuracy": "70.00%"
        },
        {
          "Reference": "Dolka et al.\n[49]",
          "Methodology": "ANN",
          "Features": "MFCC",
          "Accuracy": "88.72%"
        },
        {
          "Reference": "Jothimani et al.\n[61]",
          "Methodology": "CNN+LSTM",
          "Features": "MFCC, RMS, ZCR",
          "Accuracy": "92.6 %"
        },
        {
          "Reference": "Kishor Bhangale et al.\n[55]",
          "Methodology": "1D DCNN",
          "Features": "MFCC, LPCC, spectral kurtosis",
          "Accuracy": "94.18%"
        },
        {
          "Reference": "",
          "Methodology": "",
          "Features": "spectrum centroid, spectral roll-off,",
          "Accuracy": ""
        },
        {
          "Reference": "",
          "Methodology": "",
          "Features": "wavelet packet transform, ZCR",
          "Accuracy": ""
        },
        {
          "Reference": "Alluhaidan et al.\n[33]",
          "Methodology": "CNN",
          "Features": "MFCC, MFCCT",
          "Accuracy": "92%"
        },
        {
          "Reference": "Ahmed et al.\n[7]",
          "Methodology": "1D-CNN-LSTM-GRU",
          "Features": "ZCR, RMS, Chromagram",
          "Accuracy": "95.62%"
        },
        {
          "Reference": "",
          "Methodology": "",
          "Features": "Log-mel Spectrogram and MFCC",
          "Accuracy": ""
        },
        {
          "Reference": "This work",
          "Methodology": "1D CNN+ECA-Net+BiLSTM",
          "Features": "RMS, ZCR, MFCC",
          "Accuracy": "94.88%"
        },
        {
          "Reference": "",
          "Methodology": "",
          "Features": "Mel-spectrogram",
          "Accuracy": ""
        }
      ],
      "page": 32
    },
    {
      "caption": "Table 7: Performance analysis of the proposed work and compare it with the latest liter-",
      "data": [
        {
          "Reference": "Chakraborty et al.\n[65]",
          "Methodology": "CNN",
          "Features": "Phase-based Cepstral",
          "Accuracy": "79%"
        },
        {
          "Reference": "This work",
          "Methodology": "1D CNN+ECA-Net+BiLSTM",
          "Features": "RMS, ZCR, MFCC, Mel-spectrogram",
          "Accuracy": "98.12%"
        }
      ],
      "page": 32
    },
    {
      "caption": "Table 7: Performance analysis of the proposed work and compare it with the latest liter-",
      "data": [
        {
          "Reference": "Sultana et al.\n[2]",
          "Methodology": "Deep CNN and BiLSTM",
          "Features": "MFCCs, pitch, energy, and spectral centroid",
          "Accuracy": "86.9%"
        },
        {
          "Reference": "Chakraborty et al.\n[65]",
          "Methodology": "CNN",
          "Features": "Phase-based Cepstral",
          "Accuracy": "75%"
        },
        {
          "Reference": "This work",
          "Methodology": "1D CNN+ECA-Net+BiLSTM",
          "Features": "RMS, ZCR, MFCC, Mel-spectrogram",
          "Accuracy": "97.94%"
        }
      ],
      "page": 32
    },
    {
      "caption": "Table 10: Performance analysis of the proposed work and compare it with the latest",
      "data": [
        {
          "Reference": "Thakur et al.\n[66]",
          "Methodology": "SVM",
          "Features": "Discrete Wavelet Spectral, Prosodic",
          "Accuracy": "90.02%"
        },
        {
          "Reference": "Bhattacharya et al.\n[67]",
          "Methodology": "CNN",
          "Features": "MFCC, chroma, Tonnetz, Contrast",
          "Accuracy": "96.22%"
        },
        {
          "Reference": "Alluhaidan et al.\n[33]",
          "Methodology": "CNN",
          "Features": "MFCC, MFCCT",
          "Accuracy": "97%"
        },
        {
          "Reference": "Al-onazi et al.\n[68]",
          "Methodology": "Transformer",
          "Features": "Chromagram, Spectral Contrast",
          "Accuracy": "93.40%"
        },
        {
          "Reference": "",
          "Methodology": "",
          "Features": "MFCC, Mel spectrogram, Tonnetz",
          "Accuracy": ""
        },
        {
          "Reference": "Ahmed et al.\n[7]",
          "Methodology": "1D-CNN-LSTM-GRU",
          "Features": "ZCR, RMS, Chromagram,",
          "Accuracy": "95.42%"
        },
        {
          "Reference": "",
          "Methodology": "",
          "Features": "Log-mel Spectrogram and MFCC",
          "Accuracy": ""
        },
        {
          "Reference": "This work",
          "Methodology": "1D CNN+ECA-Net+BiLSTM",
          "Features": "RMS, ZCR, MFCC, Mel-spectrogram",
          "Accuracy": "97.19%"
        }
      ],
      "page": 33
    },
    {
      "caption": "Table 11: Experimental results from the ablation study.",
      "data": [
        {
          "Dataset": "Emo-DB",
          "Model": "Proposed Architecture w/o ECA-Net & GFBs",
          "Accuracy (%)": "95.04",
          "Precision (%)": "95.33",
          "Recall (%)": "95.04",
          "F1-score (%)": "94.99"
        },
        {
          "Dataset": "",
          "Model": "Proposed Architecture w/o GFBs",
          "Accuracy (%)": "96.75",
          "Precision (%)": "96.80",
          "Recall (%)": "96.75",
          "F1-score (%)": "96.74"
        },
        {
          "Dataset": "",
          "Model": "Proposed Architecture w/o ECA-Net",
          "Accuracy (%)": "96.35",
          "Precision (%)": "96.36",
          "Recall (%)": "96.35",
          "F1-score (%)": "96.35"
        },
        {
          "Dataset": "",
          "Model": "Proposed Architecture w/o One GFB",
          "Accuracy (%)": "97.01",
          "Precision (%)": "97.01",
          "Recall (%)": "97.01",
          "F1-score (%)": "97.01"
        },
        {
          "Dataset": "",
          "Model": "Proposed Architecture",
          "Accuracy (%)": "97.19",
          "Precision (%)": "97.40",
          "Recall (%)": "97.18",
          "F1-score (%)": "97.18"
        },
        {
          "Dataset": "RAVDESS",
          "Model": "Proposed Architecture w/o ECA-Net & GFBs",
          "Accuracy (%)": "91.97",
          "Precision (%)": "92.33",
          "Recall (%)": "91.97",
          "F1-score (%)": "91.99"
        },
        {
          "Dataset": "",
          "Model": "Proposed Architecture w/o GFBs",
          "Accuracy (%)": "93.89",
          "Precision (%)": "93.90",
          "Recall (%)": "93.88",
          "F1-score (%)": "93.88"
        },
        {
          "Dataset": "",
          "Model": "Proposed Architecture w/o ECA-Net",
          "Accuracy (%)": "93.04",
          "Precision (%)": "93.88",
          "Recall (%)": "93.04",
          "F1-score (%)": "93.04"
        },
        {
          "Dataset": "",
          "Model": "Proposed Architecture w/o One GFB",
          "Accuracy (%)": "94.45",
          "Precision (%)": "94.44",
          "Recall (%)": "94.45",
          "F1-score (%)": "94.45"
        },
        {
          "Dataset": "",
          "Model": "Proposed Architecture",
          "Accuracy (%)": "94.88",
          "Precision (%)": "95.09",
          "Recall (%)": "94.87",
          "F1-score (%)": "94.89"
        }
      ],
      "page": 34
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases, Pattern recognition",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Survey on speech emotion recognition: Features, classification schemes, and databases, Pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Bangla speech emotion recognition and cross-lingual study using deep cnn and blstm networks",
      "authors": [
        "S Sultana",
        "M Iqbal",
        "M Selim",
        "M Rashid",
        "M Rahman"
      ],
      "year": "2021",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "3",
      "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
      "authors": [
        "D France",
        "R Shiavi",
        "S Silverman",
        "M Silverman",
        "M Wilkes"
      ],
      "year": "2000",
      "venue": "IEEE transactions on Biomedical Engineering"
    },
    {
      "citation_id": "4",
      "title": "Stress and emotion classification using jitter and shimmer features",
      "authors": [
        "X Li",
        "J Tao",
        "M Johnson",
        "J Soltis",
        "A Savage",
        "K Leong",
        "J Newman"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP'07"
    },
    {
      "citation_id": "5",
      "title": "Towards a standard set of acoustic features for the processing of emotion in speech",
      "authors": [
        "F Eyben",
        "A Batliner",
        "B Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of Meetings on Acoustics 159ASA"
    },
    {
      "citation_id": "6",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "7",
      "title": "An ensemble 1d-cnnlstm-gru model with data augmentation for speech emotion recognition",
      "authors": [
        "M Ahmed",
        "S Islam",
        "A Islam",
        "S Shatabda"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "8",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "R Shah"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "9",
      "title": "Revisiting hidden markov models for speech emotion recognition",
      "authors": [
        "S Mao",
        "D Tao",
        "G Zhang",
        "P Ching",
        "T Lee"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
      "doi": "10.1109/ICASSP.2019.8683172"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition using hybrid gaussian mixture model and deep neural network",
      "authors": [
        "I Shahin",
        "A Nassif",
        "S Hamsa"
      ],
      "year": "2019",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2019.2901352"
    },
    {
      "citation_id": "11",
      "title": "Speech emotion recognition using k-nearest neighbor classifiers",
      "authors": [
        "M Venkata Subbarao",
        "S Terlapu",
        "N Geethika",
        "K Harika"
      ],
      "year": "2021",
      "venue": "Recent Advances in Artificial Intelligence and Data Engineering: Select Proceedings of AIDE 2020"
    },
    {
      "citation_id": "12",
      "title": "Speech emotion recognition based on dnn-decision tree svm model",
      "authors": [
        "L Sun",
        "B Zou",
        "S Fu",
        "J Chen",
        "F Wang"
      ],
      "year": "2019",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "13",
      "title": "End-to-end speech emotion recognition using deep neural networks",
      "authors": [
        "P Tzirakis",
        "J Zhang",
        "B Schuller"
      ],
      "year": "2018",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2018.8462677"
    },
    {
      "citation_id": "14",
      "title": "Multi-type features separating fusion learning for speech emotion recognition",
      "authors": [
        "X Xu",
        "D Li",
        "Y Zhou",
        "Z Wang"
      ],
      "year": "2022",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "15",
      "title": "Learning multiscale features for speech emotion recognition with connection attention mechanism",
      "authors": [
        "Z Chen",
        "J Li",
        "H Liu",
        "X Wang",
        "H Wang",
        "Q Zheng"
      ],
      "year": "2023",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "16",
      "title": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning, Interspeech",
      "authors": [
        "R Lotfian",
        "C Busso"
      ],
      "year": "2018",
      "venue": "Predicting categorical emotions by jointly learning primary and secondary emotions through multitask learning, Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Deep learning",
      "authors": [
        "Y Lecun",
        "Y Bengio",
        "G Hinton"
      ],
      "year": "2015",
      "venue": "nature"
    },
    {
      "citation_id": "18",
      "title": "Convolutional neural network for sentence classification, Master's thesis",
      "authors": [
        "Y Chen"
      ],
      "year": "2015",
      "venue": "Convolutional neural network for sentence classification, Master's thesis"
    },
    {
      "citation_id": "19",
      "title": "Experimenting with 1d cnn architectures for generic audio classification",
      "authors": [
        "L Vrysis",
        "I Thoidis",
        "C Dimoulas",
        "G Papanikolaou"
      ],
      "year": "2020",
      "venue": "Audio Engineering Society Convention"
    },
    {
      "citation_id": "20",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "A Graves",
        "S Fernández",
        "F Gomez",
        "J Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd international conference on Machine learning"
    },
    {
      "citation_id": "21",
      "title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "authors": [
        "K Cho",
        "B Van Merriënboer",
        "C Gulcehre",
        "D Bahdanau",
        "F Bougares",
        "H Schwenk",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "Learning phrase representations using rnn encoder-decoder for statistical machine translation",
      "arxiv": "arXiv:1406.1078"
    },
    {
      "citation_id": "22",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    },
    {
      "citation_id": "23",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "M Schuster",
        "K Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE transactions on Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Stock price forecast based on cnn-bilstm-eca model, Scientific Programming 2021",
      "authors": [
        "Y Chen",
        "R Fang",
        "T Liang",
        "Z Sha",
        "S Li",
        "Y Yi",
        "W Zhou",
        "H Song"
      ],
      "year": "2021",
      "venue": "Stock price forecast based on cnn-bilstm-eca model, Scientific Programming 2021"
    },
    {
      "citation_id": "25",
      "title": "Eca-net: Efficient channel attention for deep convolutional neural networks",
      "authors": [
        "Q Wang",
        "B Wu",
        "P Zhu",
        "P Li",
        "W Zuo",
        "Q Hu"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "26",
      "title": "Ensemble learning, The handbook of brain theory and neural networks",
      "authors": [
        "T Dietterich"
      ],
      "year": "2002",
      "venue": "Ensemble learning, The handbook of brain theory and neural networks"
    },
    {
      "citation_id": "27",
      "title": "Aging affects identification of vocal emotions in semantically neutral sentences",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2015",
      "venue": "Journal of Speech, Language, and Hearing Research"
    },
    {
      "citation_id": "28",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "29",
      "title": "Islam, Banglaser: A speech emotion recognition dataset for the bangla language",
      "authors": [
        "R Das",
        "N Islam",
        "M Ahmed",
        "S Islam",
        "S Shatabda"
      ],
      "year": "2022",
      "venue": "Data in Brief"
    },
    {
      "citation_id": "30",
      "title": "Sust bangla emotional speech corpus (subesco): An audio-only emotional speech corpus for bangla",
      "authors": [
        "S Sultana",
        "M Rahman",
        "M Selim"
      ],
      "year": "2021",
      "venue": "Plos one"
    },
    {
      "citation_id": "31",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "A database of german emotional speech"
    },
    {
      "citation_id": "32",
      "title": "Mlt-dnet: Speech emotion recognition using 1d dilated cnn based on multi-learning trick approach",
      "authors": [
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "33",
      "title": "Speech emotion recognition through hybrid features and convolutional neural network",
      "authors": [
        "A Alluhaidan",
        "O Saidani",
        "R Jahangir",
        "M Nauman",
        "O Neffati"
      ],
      "year": "2023",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition by using complex mfcc and deep sequential model",
      "authors": [
        "S Patnaik"
      ],
      "year": "2023",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion recognition based on svm and cnn using mfcc feature extraction",
      "authors": [
        "Z Zhong"
      ],
      "year": "2023",
      "venue": "International Conference on Statistics, Data Science, and Computational Intelligence (CSDSCI 2022)"
    },
    {
      "citation_id": "36",
      "title": "Two-way feature extraction for speech emotion recognition using deep learning",
      "authors": [
        "A Aggarwal",
        "A Srivastava",
        "A Agarwal",
        "N Chahal",
        "D Singh",
        "A Alnuaim",
        "A Alhadlaq",
        "H.-N Lee"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "37",
      "title": "Very deep convolutional networks for largescale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for largescale image recognition",
      "arxiv": "arXiv:1409.1556"
    },
    {
      "citation_id": "38",
      "title": "Squeeze-and-excitation networks",
      "authors": [
        "J Hu",
        "L Shen",
        "G Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "39",
      "title": "Cbam: Convolutional block attention module",
      "authors": [
        "S Woo",
        "J Park",
        "J.-Y Lee",
        "I Kweon"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "40",
      "title": "Speech emotion recognition with co-attention based multi-level acoustic information",
      "authors": [
        "H Zou",
        "Y Si",
        "C Chen",
        "D Rajan",
        "E Chng"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Combining a parallel 2d cnn with a self-attention dilated residual network for ctc-based discrete speech emotion recognition",
      "authors": [
        "Z Zhao",
        "Q Li",
        "Z Zhang",
        "N Cummins",
        "H Wang",
        "J Tao",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition based on deep networks: A review",
      "authors": [
        "M Mustaqeem",
        "S Kwon"
      ],
      "year": "2021",
      "venue": "Proceedings of the Korea Information Processing Society Conference"
    },
    {
      "citation_id": "43",
      "title": "Multimodal cross-and self-attention network for speech emotion recognition",
      "authors": [
        "L Sun",
        "B Liu",
        "J Tao",
        "Z Lian"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "44",
      "title": "Speech feature selection and emotion recognition based on weighted binary cuckoo search",
      "authors": [
        "Z Zhang"
      ],
      "year": "2021",
      "venue": "Alexandria Engineering Journal"
    },
    {
      "citation_id": "45",
      "title": "Deep convolutional neural network and gray wolf optimization algorithm for speech emotion recognition",
      "authors": [
        "M Falahzadeh",
        "F Farokhi",
        "A Harimi",
        "R Sabbaghi-Nadooshan"
      ],
      "year": "2023",
      "venue": "Circuits, Systems, and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Ensemble learning by high-dimensional acoustic features for emotion recognition from speech audio signal, Security and Communication Networks",
      "authors": [
        "M Chalapathi",
        "M Kumar",
        "N Sharma",
        "S Shitharth"
      ],
      "year": "2022",
      "venue": "Ensemble learning by high-dimensional acoustic features for emotion recognition from speech audio signal, Security and Communication Networks"
    },
    {
      "citation_id": "47",
      "title": "Recognition of cross-language acoustic emotional valence using stacked ensemble learning",
      "authors": [
        "K Zvarevashe",
        "O Olugbara"
      ],
      "year": "2020",
      "venue": "Algorithms"
    },
    {
      "citation_id": "48",
      "title": "Communication in the presence of noise",
      "authors": [
        "C Shannon"
      ],
      "year": "1949",
      "venue": "Proceedings of the IRE",
      "doi": "10.1109/JRPROC.1949.232969"
    },
    {
      "citation_id": "49",
      "title": "Speech emotion recognition using ann on mfcc features",
      "authors": [
        "H Dolka",
        "A Vm",
        "S Juliet"
      ],
      "year": "2021",
      "venue": "2021 3rd international conference on signal processing and communication (ICPSC)"
    },
    {
      "citation_id": "50",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z.-T Liu",
        "Q Xie",
        "M Wu",
        "W.-H Cao",
        "Y Mei",
        "J.-W Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "51",
      "title": "Audio recognition using mel spectrograms and convolution neural networks",
      "authors": [
        "M Leitner",
        "J Thornton"
      ],
      "year": "2019",
      "venue": "Audio recognition using mel spectrograms and convolution neural networks",
      "arxiv": "arXiv:1905.00078"
    },
    {
      "citation_id": "52",
      "title": "Speech recognition using efficientnet",
      "authors": [
        "Q Lu",
        "Y Li",
        "Z Qin",
        "X Liu",
        "Y Xie"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 5th International Conference on Multimedia Systems and Signal Processing"
    },
    {
      "citation_id": "53",
      "title": "Feature extraction algorithms to improve the speech emotion recognition rate",
      "authors": [
        "A Koduru",
        "H Valiveti",
        "A Budati"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "54",
      "title": "A novel approach for classification of speech emotions based on deep and acoustic features",
      "authors": [
        "M Er"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "55",
      "title": "Speech emotion recognition based on multiple acoustic features and deep convolutional neural network",
      "authors": [
        "K Bhangale",
        "M Kothandaraman"
      ],
      "year": "2023",
      "venue": "Electronics"
    },
    {
      "citation_id": "56",
      "title": "Kerastuner",
      "authors": [
        "T O'malley",
        "E Bursztein",
        "J Long",
        "F Chollet",
        "H Jin",
        "L Invernizzi"
      ],
      "year": "2019",
      "venue": "Kerastuner"
    },
    {
      "citation_id": "57",
      "title": "The values of precision",
      "authors": [
        "M Wise"
      ],
      "year": "1997",
      "venue": "The values of precision"
    },
    {
      "citation_id": "58",
      "title": "Extending the irrelevant speech effect beyond serial recall",
      "authors": [
        "D Lecompte"
      ],
      "year": "1994",
      "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition"
    },
    {
      "citation_id": "59",
      "title": "Thresholding classifiers to maximize f1 score",
      "authors": [
        "Z Lipton",
        "C Elkan",
        "B Narayanaswamy"
      ],
      "year": "2014",
      "venue": "Thresholding classifiers to maximize f1 score",
      "arxiv": "arXiv:1402.1892"
    },
    {
      "citation_id": "60",
      "title": "The'k'in k-fold cross validation",
      "authors": [
        "D Anguita",
        "L Ghelardoni",
        "A Ghio",
        "L Oneto",
        "S Ridella"
      ],
      "year": "2012",
      "venue": "The'k'in k-fold cross validation"
    },
    {
      "citation_id": "61",
      "title": "Mff-saug: Multi feature fusion with spectrogram augmentation of speech emotion recognition using convolution neural network",
      "authors": [
        "S Jothimani",
        "K Premalatha"
      ],
      "year": "2022",
      "venue": "Chaos, Solitons & Fractals"
    },
    {
      "citation_id": "62",
      "title": "The efficacy of deep learning-based mixed model for speech emotion recognition",
      "authors": [
        "M Uddin",
        "M Chowdury",
        "M Khandaker",
        "N Tamam",
        "A Sulieman"
      ],
      "year": "2023",
      "venue": "Computers, Materials & Continua"
    },
    {
      "citation_id": "63",
      "title": "Ensemblewave: An ensembled approach for automatic speech emotion recognition",
      "authors": [
        "R Barkur",
        "D Suresh",
        "M Tn",
        "A Narasimhadhan"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Electronics, Computing and Communication Technologies (CONECCT)"
    },
    {
      "citation_id": "64",
      "title": "Linguistic and gender variation in speech emotion recognition using spectral features",
      "authors": [
        "Z Dair",
        "R Donovan",
        "R O'reilly"
      ],
      "year": "2021",
      "venue": "Linguistic and gender variation in speech emotion recognition using spectral features",
      "arxiv": "arXiv:2112.09596"
    },
    {
      "citation_id": "65",
      "title": "Phase-based cepstral features for automatic speech emotion recognition of low resource indian languages",
      "authors": [
        "C Chakraborty",
        "T Dash",
        "G Panda",
        "S Solanki"
      ],
      "year": "2022",
      "venue": "Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "66",
      "title": "Language-independent hyperparameter optimization based speech emotion recognition system",
      "authors": [
        "A Thakur",
        "S Dhull"
      ],
      "year": "2022",
      "venue": "International Journal of Information Technology"
    },
    {
      "citation_id": "67",
      "title": "Emotion detection from multilingual audio using deep analysis",
      "authors": [
        "S Bhattacharya",
        "S Borah",
        "B Mishra",
        "A Mondal"
      ],
      "year": "2022",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "68",
      "title": "Transformer-based multilingual speech emotion recognition using data augmentation and feature fusion",
      "authors": [
        "B Al-Onazi",
        "M Nauman",
        "R Jahangir",
        "M Malik",
        "E Alkhammash",
        "A Elshewey"
      ],
      "year": "2022",
      "venue": "Applied Sciences"
    },
    {
      "citation_id": "69",
      "title": "Emotion recognition from bengali speech using rnn modulation-based categorization",
      "authors": [
        "H Hasan",
        "M Islam"
      ],
      "year": "2020",
      "venue": "2020 third international conference on smart systems and inventive technology (ICSSIT)"
    },
    {
      "citation_id": "70",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng",
        "T Yan",
        "F Yuan",
        "H Wei"
      ],
      "year": "2019",
      "venue": "IEEE access"
    },
    {
      "citation_id": "71",
      "title": "A novel end-toend speech emotion recognition network with stacked transformer layers",
      "authors": [
        "X Wang",
        "M Wang",
        "W Qi",
        "W Su",
        "X Wang",
        "H Zhou"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    }
  ]
}