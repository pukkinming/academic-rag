{
  "paper_id": "2204.02385v2",
  "title": "Learning Speech Emotion Representations In The Quaternion Domain",
  "published": "2022-04-05T17:45:09Z",
  "authors": [
    "Eric Guizzo",
    "Tillman Weyde",
    "Simone Scardapane",
    "Danilo Comminiello"
  ],
  "keywords": [
    "Speech Emotion Recognition",
    "Quaternion Neural Networks",
    "Quaternion Algebra",
    "Transferable Embeddings"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The modeling of human emotion expression in speech signals is an important, yet challenging task. The high resource demand of speech emotion recognition models, combined with the general scarcity of emotion-labelled data are obstacles to the development and application of effective solutions in this field. In this paper, we present an approach to jointly circumvent these difficulties. Our method, named RH-emo, is a novel semisupervised architecture aimed at extracting quaternion embeddings from real-valued monoaural spectrograms, enabling the use of quaternion-valued networks for speech emotion recognition tasks. RH-emo is a hybrid real/quaternion autoencoder network that consists of a real-valued encoder in parallel to a real-valued emotion classifier and a quaternion-valued decoder. On the one hand, the classifier permits to optimization of each latent axis of the embeddings for the classification of a specific emotionrelated characteristic: valence, arousal, dominance, and overall emotion. On the other hand, quaternion reconstruction enables the latent dimension to develop intra-channel correlations that are required for an effective representation as a quaternion entity. We test our approach on speech emotion recognition tasks using four popular datasets: IEMOCAP, RAVDESS, EmoDB, and TESS, comparing the performance of three well-established real-valued CNN architectures (AlexNet, ResNet-50, VGG) and their quaternion-valued equivalent fed with the embeddings created with RH-emo. We obtain a consistent improvement in the test accuracy for all datasets, while drastically reducing the resources' demand of models. Moreover, we performed additional experiments and ablation studies that confirm the effectiveness of our approach. The RH-emo repository is available at: https://github.com/ispamm/rhemo.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Background",
      "text": "In the literature, two main approaches to labeling expressed human emotions exist. On the one hand, discrete models provide a set of fixed emotion categories, such as happy, sad, angry, fearful, surprised, disgusted, neutral. On the other hand, continuous models map emotions into a multidimensional space. The most common model is a 2D valence-arousal space, where valence describes the degree of emotional pleasantness and arousal (or activation) of the intensity of the emotion. Dominance can be added as a third dimension describing the amount of control of a person expressing an emotion. This encodes a so-called valence-arousal-dominance space  [6] -  [8] . Discrete emotions can be mapped in this continuous space although the exact mapping is not standardized and different studies can use slightly different mappings.\n\nA traditional approach to SER is based on two consecutive stages: hard-coded extraction of affect-salient features followed by a learning-based classification or regression. Various combinations of features and classifier types have been proposed. The most commonly used features are: base pitch, formant features, energy/spectral features, and prosody. A wide variety of classifiers has been proposed: artificial neural networks  [9] -  [11] , Bayesian networks,  [12] , Hidden Markov Models  [13] ,  [14] , support vector machines  [15] ,  [16] , and Gaussian mixture models  [17] . Nevertheless, in state-ofthe-art methods, there is no default choice of features and classifier type  [18] . With the advent of deep learning, end-toend learning mostly replaced hard-coded feature extraction and selection, with models automatically extracting features from low-level representations of the input data (usually Fourierbased transforms, wavelet transforms, or raw audio data). This enables a model to fine-tune the feature extraction for a specific task and, consequently, often obtain a higher accuracy compared to engineered feature extraction. A range of deep learning architectures have been adopted for SER. The most commonly used are convolutional neural networks  [19] -  [21] , recurrent neural networks  [22] ,  [23]  and combinations of the two  [24] -  [26]  Various studies directly compare the performance of approaches using end-to-end learning and hardcoded feature extraction, showing that the former generally provides a higher classification accuracy on the same data  [27] -  [30] . Nevertheless, as a drawback, deep learning models generally require a higher computational cost and longer 1 https://github.com/ispamm/rhemo. 2 Pretrained models: rhemo/weights. training times than traditional machine learning techniques and the end-to-end learning usually requires a large number of labelled data  [31] ,  [32] .\n\nA well-established solution to overcome the data scarcity in SER is transfer learning by weight initialization: network weights are initialized with values from a network that was pretrained with a different task, possibly on a different (usually large) dataset. Many variants of this method have been shown to improve the performance of SER models in limited-data scenarios and even when the task is rather distant from speech emotion  [33] -  [35] . Also, various data augmentation strategies have been successfully adopted for the same purpose, e.g.  [36] ,  [37] . On the other hand, the application of dimensionality reduction transformations to the model's input data is an established strategy for reducing resource demands while limiting the loss of useful information carried by the input data. Among others, autoencoders, PCA-based approaches, and transformer networks have been used in the field of SER  [34] ,  [38] ,  [39] , obtaining improvement both in the model's efficiency and classification accuracy.\n\nA recent and increasingly popular strategy to improve the efficiency and the performance of deep learning models is the use of quaternion information processing  [40] -  [46] . Performing operations in the quaternion domain permits bootstrap intra-channel correlations in multidimensional signals  [47] ,  [48] , i.e., among the color channels of RGB-encoded images. Moreover, due to the fewer degrees of freedom of the Hamilton product compared to the regular dot product, quaternion networks have a significantly lower number of parameters compared to the real counterparts  [40] . Quaternionvalued neural networks have also been successfully adopted in the audio domain  [49] ,  [50]  and specifically for speech recognition  [45]  and speech emotion recognition  [46] . Nevertheless, an intrinsic limitation of quaternion information processing is that it requires three or four-dimensional data as input, where intra-channel correlations exist  [41] -  [44] . This is necessary to enable the benefits derived from the use of the Hamilton product instead of the regular dot product, as further discussed in Section III. In the audio domain, first-order Ambisonics  [51]  signals are naturally suited for a quaternion representation, being four-dimensional and presenting strong correlations among the spatial channels, and the application of quaternion networks to problems related to this audio format has already been extensively investigated  [50] ,  [52] -  [54] . Nevertheless, in the vast majority of cases, audio-related machine-learning tasks deal with monaural signals, which are usually treated as vectors of scalars (time-domain signals), matrices of scalars (magnitude spectrograms), or 3D tensors (complex spectrograms). Hence they can not be naturally represented as a quaternion entity and additional processing is required to produce a suitable quaternion representation of these signals.\n\nA number of different approaches have been proposed to overcome the necessity of having three or four-dimensional input data with intra-channel correlations. Among others,  [45]  use Mel spectrograms, cepstral coefficients, and first and second-order derivatives as the four axes of the encoded quaternion. In contrast,  [46]  convert Mel spectrograms to color-scaled images and use the RGB channels as axes of the encoded quaternion, following a computer vision-oriented approach. Parcollet et al.  [55]  presented two learning-based approaches to map real-valued vectors into the quaternion domain, by producing through a network four-channel representations of the input data that present meaningful intrachannel correlations. On the one hand, the Real to H-space encoder  [55] , applied to speech recognition tasks, consists of a simple real-valued dense layer applied at the beginning of a quaternion classifier network, which is trained jointly with the classifier. On the other hand, the Real to H-space Autoencoder, tested in the natural language processing field (conversation theme identification)  [55]  operates in an unsupervised way. Such a method contains a real-valued encoder and a quaternion-valued decoder, where the latter is expected to enable both the network's embeddings and output to present meaningful intra-channel correlations that can be exploited by a quaternion-valued classifier network.\n\nIn this paper, we introduce RH-emo, a hybrid realquaternion autoencoder-classifier architecture that is trained in a semi-supervised fashion in order to optimize each axis of the embedding dimension to different emotional characteristics: the first channel is optimized for discrete emotion recognition and the 3 other channels are individually optimized for the classification of valence, arousal, and dominance (as shown in Figure  1 ). RH-emo is intended to be used as a feature extractor that permits using QNNs for SER tasks with real-valued signals without additional preprocessing. This approach has two advantages: it improves the performance of SER models even in situations where data is scarce and it drastically reduces the number of network parameters, consequently reducing the resource demand. We extend the approach of the quaternion autoencoder in  [55]  by specializing the learned quaternion representation for our specific task (SER), where the different axes are optimized for the detection of different emotional characteristics that are coherent with the most used criteria of emotion classification. Moreover, we implement it with a more complex architecture (deep convolutional autoencoder) and we apply it to a different domain: emotion recognition from speech audio.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iii. Quaternion Convolutional Neural Networks",
      "text": "Operations between quaternion numbers are defined in the quaternions algebra H. A quaternion Q is a four-dimensional extension of a complex number, defined as q = ùëû 0 + ùëû 1 ƒ± + ùëû 2 »∑ + ùëû 3 Œ∫ = ùëû 0 + ùëû, where, ùëû 0 , ùëû 1 , ùëû 2 are real numbers, and ƒ±, »∑ and Œ∫ are the quaternion unit basis. In this representation ùëû 0 is the real part and ùëû 1 ƒ± + ùëû 2 »∑ + ùëû 3 Œ∫ is the imaginary part, where ƒ±2 = »∑2 = Œ∫2 = -1 and ƒ± »∑ =»∑ƒ±. From the latter assumption follows that the quaternion vector multiplication is not commutative. A quaternion can also be represented as a matrix of real numbers:\n\nAnalogously to real and complex numbers, a set of operations can be defined in the quaternion space:\n\nThe quaternion convolutional neural network (QCNN) is an extension of the real-valued convolutional neural network to the quaternion domain. For each input vector of a quaternion layer, the dimensions are split into four parts to compose a quaternion representation. In a quaternion-valued fullyconnected layerthe parameters matrices are treated as a single quaternion entity with four components, even though they are manipulated as matrices of real numbers  [56] . In a quaternion layer, the dot product operations used in real layers are replaced with the Hamilton product (eq. (  2 )) between the input vector and a quaternion-represented weight matrix. This allows the processing of all input channels together as a single entity maintaining original intra-channels dependencies because the weights submatrices are shared among the input channels. Consequently, quaternion layers permit to spare the 75% of free parameters compared to their real-valued equivalents because, as shown in eq. (  2 ), the same components are re-used to build the output matrix.\n\nIn a QCNN, the convolution of a quaternion filter matrix with a quaternion vector is performed as the Hamilton product between the real-valued matrices representation of the input vector and filters. A quaternion convolution between a quaternion input vector x = ùë• 0 + ùë• 1 ƒ± + ùë• 2 »∑ + ùë• 3 Œ∫ and a quaternion filter ùëä = ùëä 0 + ùëä 1 ƒ± + ùëä 2 »∑ + ùëä 3 Œ∫ can be defined as:\n\nThe optimization of quaternion-valued networks is identical to the one of a real network and can be achieved through regular backpropagation. This is possible because of the use of split activation and loss functions, as introduced in  [55] ,  [57] . These functions map a quaternion-like entity back to the real domain, consequently enabling the use of standard loss functions for the network training.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iv. The Proposed Rh-Emo Model A. Approach",
      "text": "The main aim of RH-emo is to map real-valued spectrograms to the quaternion domain, building compact emotionrelated quaternion embeddings where each axis is optimized for a different emotional characteristic. In the embedded dimension, the real axis of the quaternion is optimized for the discrete classification of 4 emotions: neutrality, anger, happiness, sadness and the 3 complex axes are optimized for the prediction of emotion in a valence, arousal and dominance 3D space. This representation exploits the natural predisposition of quaternion algebra to process data where a 4 or 3-channels representation is meaningful. Nevertheless, in most machine learning applications of quaternion algebra, the input data is naturally organized with a meaningful shape, as happens for instance with RGB/RGBA images (where the color/alpha channels are treated as different quaternion axes) and first-order Ambisonics audio signals (where the 4 spatial channels are considered as the quaternion axes). In our case, instead, such quaternion representation is created through a semi-supervised learning procedure, where the different axes are forced to contain information related to different complementary emotion characteristics. Therefore, in a certain sense, the axes of this embedded dimension can be thought of as different \"emotional points of view\" of an audio signal.\n\nRH-emo is intended to be used as a pretrained feature extractor to enable the use of quaternion-valued neural networks for SER tasks applied to monoaural audio signals. On the one hand, the emotion-related disentanglement among channels helps to enhance the performance of SER models, especially under conditions of data scarcity. Whereas, on the other hand, the reduced dimensionality together with the enabled possibility to classify the data with quaternion-valued networks permits to spare of a large number of network parameters, consequently lowering the resource demand and speeding up the training.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Rh-Emo Architecture",
      "text": "RH-emo is a hybrid real/quaternion autoencoder network. Its structure is similar to R2Hae  [55] , nevertheless, RH-emo is based on a convolutional design and it embraces multiple classification branches, as opposed to R2Hae. We used a public PyTorch implementation of convolution layers and operators 3 . As Figure  1  shows, our RH-emo is composed of three components: an encoder ùê∏ (ùëã) acting on the (real-valued) input spectrogram, producing an embedded vector. The output of the encoder is then fed separately to a (quaternion-valued) decoder ùê∑ (ùëç) to reconstruct the original spectrogram and to a classification head ùê∂ (ùëç) for performing emotion recognition. The classifier outputs four separate predictions ùë¶ ùê∑ , ùë¶ ùë£ , ùë¶ ùëé , and ùë¶ ùëë which are, respectively, a discrete and a continuous (in the valence, arousal, dominance space) categorization of the emotional content of the spectrogram. The specific architecture for each of these blocks, as well as the loss function we optimize and the training strategy we adopt, is described more in detail in the following paragraphs.\n\n1) Encoder: The input data, a magnitudes-only real-valued spectrogram in our case, is forward propagated through a realvalued autoencoder made up of 3 convolution blocks. Each block contains a 2D convolution layer (ReLU activations, 3x3 kernels, single-pixel stride, increasing channels number: 1, 2, 4), followed by max-pooling layers of dimension [2x2],\n\n[2x1], [2x1]. Moreover, only between the first and the second block, a batch normalization layer is present. The encoder produces an embedded vector that presents a dimensionality reduced by a factor of 0.25 compared to the input. In our experiments, we use input spectrograms with a shape of 1x512x128 (channels, time-steps, frequency-bins) and the embedded dimension created by the encoder has a shape of 4x64x64. The embedded vector is then forward propagated in parallel into four distinct real-valued classifiers and also into a quaternion-valued decoder. It is therefore important that the embedded vector contains a number of elements that is multiple of four, in order to be properly treated as a quaternion by the decoder section of the network.\n\n2) Classifiers: Each classifier consists of a sequence of 3 real-valued fully connected layers, where the first 2 contain 4096 neurons and are followed by a dropout layer. In the first classifier, the output layer contains 4 output neurons (the number of emotional classes to be classified) and softmax activation. Instead, the other 3 classifiers are identical and have one single output neuron with sigmoid activation, as they are individually aimed at a binary classification task: the prediction of \"high\" or \"low\" valence, arousal, and dominance, respectively.\n\n3) Decoder: The decoder mirrors the encoder's structure but uses quaternion-valued 2D transposed convolutions with a stride that mirrors the pooling dimensions of the encoder, instead of the sequence of 2D real-valued convolutions and 2x2 max-pooling and a quaternion-valued batch normalization layer instead of its real-valued counterpart. The output of the decoder is therefore a matrix with the same dimensions as the input, but with 4 channels instead of a single one.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "C. Loss Function",
      "text": "The loss function we minimize during the training of RHemo is a weighted sum of the binary crossentropy reconstruction loss between the input spectrogram and the decoder's output, the categorical crossentropy classification loss of the emotion labels predicted by the supervised classifier in the middle of the network (discrete, valence and arousal).\n\nThe objective function we minimize is, therefore:\n\nwhere ùêµùê∂ùê∏ is the binary crossentropy loss, ùê∂ùê∏ is the categorical crossentropy loss, ùõΩ and ùõº are scalar weight factors, ùëã is the input spectrogram, ùëå ùëü is the decoder's output remapped to the real domain through the split activation function (as discussed below), ùëù and ùë° are respectively the discrete emotion prediction and truth label, ùë£ ùëù /ùë£ ùë° , ùëé ùëù /ùëé ùë° and ùëë ùëù /ùëë ùë° are respectively the valence, arousal and dominance prediction, and truth labels.\n\nFor the reconstruction loss computation, it is necessary to map the quaternion-valued decoder output back to the real domain, in order to have the same shape as the input vector. For this purpose we use a stratagem similar to the \"split activation\" described in  [55] ,  [57] : we perform an elementwise mean across the channel dimension of the quaternion output, bringing back the 4-channels vector to a single-channel shape. During the training, this forces the model to not weigh the intra-channel correlations among the quaternion axes in the reconstruction term of the loss (the leftmost term of eq. (  4 )). Our expectation is that this leaves room for the emotion recognition term of the loss (the rightmost term of eq. (  4 )) for tuning these correlations, making them related to the emotional information.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Training Strategy",
      "text": "For the RH-emo training, we use the Interactive Emotional Dyadic Motion Capture Database (IEMOCAP) dataset  [58] , which includes: 5 speakers, 7529 utterances, 9:32 hours of audio, 10 emotion labels and it is in the English language. We selected this specific dataset for the following reasons: it is one of the most popular SER datasets, it contains a large number of datapoints, it is not limited to a restricted set of sentences, emotions are expressed by actors with a natural feeling rather than being over-emphasized  [58]  and it is labelled both in the discrete and continuous (valence, arousal, dominance) emotional domains.\n\nWe apply 4 preprocessing stages to the raw data: we first extract 4-second non-overlapped fragments (or zero-pad if a datapoint is shorter that this duration). Then, we compute the short-time Fourier transform (STFT) using 16 ms sliding windows with 50% overlap, applying a Hamming window and discarding the phase information. After this point, we normalize the whole dataset between 0 and 1 and, in the end, we zero-pad the spectrograms to match a shape of 512 (timesteps) x 128 (frequency-bins).\n\nTo permit proper convergence, we perform the training in 2 consecutive stages: we first train the network until convergence with the ùõΩ weight set to 0. This removes the rightmost term from eq. (  4 ), consequently eliminating the emotion classification part of the loss. Doing so, we train the network in a completely unsupervised way only to perform a quaternion projection of the real input spectrogram, without taking into account any emotion-related information. After this stage, we re-train the network adding also the classification term in the loss in order to specialize the learned representations to the emotion recognition task, but also maintaining the embedded vector in a quaternion-compatible shape that is meaningful for the decoder part of the network. For this stage, we performed a grid search to find the best combination of the emotion classification weights ùõΩ and ùõº and we ended up using ùõΩ = 0.01 and ùõº = 100. This means that overall we weigh more the reconstruction error in the loss function (thanks to the low ùõΩ), and we weigh more the dimensional emotion classification compared to the discrete classification (thanks to the high ùõº).\n\nWhile for the first, completely unsupervised, training stage we use all data available with IEMOCAP, in the second supervised stage we use only a subset of the dataset, including only the datapoints related to 4 emotions (angry, happy, neutral, sad) and we merge the classes happy and excited as one single emotion class happy. This is a standard procedure with IEMOCAP, as the other labels contained in the dataset are highly imbalanced. For both training stages, we use subsets of approximately 70% of the data for training, 20% for validation, and 10% for the test set. We use a learning rate of 0.001 in the first stage and of 0.000001 in the second one, a batch size of 20 and the Adam optimizer  [59] . We use dropout at 50% in the classification branches for the second training stage. We apply early stopping by testing at the validation loss improvement with patience of 100 epochs in the first stage and 30 epochs for the second one.\n\nAfter these 2 training stages, we obtain a test reconstruction loss (the isolated leftmost term of eq. (  4 )) of 0.00413 and competitive test classification accuracy: 60.7% for the discrete classification and respectively 65.4%, 75.3% and 70.2% for the valence, arousal, and dominance dimensions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "V. Evaluation",
      "text": "In order to test the capabilities and properties of RH-emo, we compare the classification accuracy for SER tasks obtained with real-valued CNN networks and equivalent quaternionvalued versions of them (QCNNs). For the quaternion versions we keep the same architecture of the real CNNs, but we use quaternion-valued convolution and quaternion-valued fully connected layers instead of the canonical real-valued ones, with the exception of the final layer of the networks, which are real-valued also in the QCNNs. For the real networks, we use the magnitudes-only spectra as input, while for the quaternion networks we use the embeddings generated with RH-emo pretrained on IEMOCAP. Moreover, we compare and combine our approach with a standard transfer learning method performed on the same dataset (IEMOCAP): pretraining with weight initialization. Therefore we have two distinct types of pretraining: the pretraining of the RH-emo network, which we use to compute the emotional embeddings, and the pretraining of the CNNs that we use to perform the actual SER task. Both pretrainings are performed on the IEMOCAP dataset. To avoid confusion, from here on we will refer to the first as RH-emo pretraining and to the latter as CNN's pretraining.\n\nFigure  2  depicts all cases we include in our experimental setup. The color coding of Figure  2  shows the 3 consecutive stages of our experiments: first, we pretrain RH-emo (yellow), then we pretrain the CNNs (orange) on IEMOCAP and finally we train or retrain the CNNs on other datasets. We have two types of baseline: the first one, shown in the upper row of Figure  2 , is a standard real-valued CNN with randomlyinitialized weights. As a further baseline, as depicted in the second row of Figure  2 , we test a standard transfer learning approach applied to the real-valued CNNs: we pretrain on IEMOCAP (the same dataset used to train RH-emo) and we then initialize all weights of the SER CNNs but the ones of the final classification layer. The last two rows of Figure  2 , instead, show our approach, where we use RH-emo as a feature extractor to feed quaternion-valued CNNs. In the third row, only RH-emo pretraining happens, while in the last row both RH-emo and CNNs pretraining are performed. In the latter case, we first pretrain RH-emo, then we pretrain the CNN on IEMOCAP, and finally, we re-train the same CNN on different datasets.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "We evaluate RH-emo with 3 benchmark SER datasets: 1) RAVDESS, the Ryerson Audio Visual Database of Emotional Speech and Song  [60] . 24 speakers, English language, 2542 utterances, 2:47 hours of audio, 8 emotion labels. 2) EmoDB, a Database of German Emotional Speech  [61] .\n\n10 speakers, German language, 535 utterances, 25 min of audio, 7 emotion labels. 3) TESS, the Toronto Emotional Speech Set  [62] . 2 speakers, English language, 2800 utterances, 1:36 hours of audio, 7 emotion labels. The preprocessing pipeline for these datasets is identical to the one we applied to IEMOCAP, as described in Section IV, except for the final normalization step. For the quaternionvalued networks we normalize data between 0 and 1 (as required by RH-emo), and for the real-valued networks we normalize to 0 mean and unity standard deviation to permit proper convergence.\n\nWe apply this approach to 3 popular CNN architectures with increasing capacity: VGG16  [63] , AlexNet  [64]  and ResNet-50  [65] , based on the Torchvision implementations 4 . These implementations present an adaptive average pooling layer between the convolution-based feature extractor and the fullyconnected classifier. This permits to obtain an identical output shape from the feature extractor for any input dimension. We removed this layer from only VGG16, in order to test the behavior of our approach also in this situation. Doing this, in fact, the feature extractor presents a reduced output dimensionality when the networks are fed with the quaternion 4 https://pytorch.org/vision/stable/ modules/torchvision.html embeddings (75% smaller than using the real spectrograms), enabling to spare of a major number of network parameters.\n\nFor all experiments we used a learning rate of 0.00001, ADAM optimizer, and a batch size of 20 samples, we apply early stopping with the patience of 20 epochs on the validation   The main aim of this research is to provide a valid comparison between the proposed approach (quaternion-valued CNNs fed with RH-Emo embeddings) and standard equivalent realvalued architectures, isolating as much as possible the pure difference between them. We configured our experimental setup in order to show the performance difference between real and corresponding quaternion CNNs fed with the emotional quaternion embeddings. Therefore, we paid attention to performing each experiment in as-close-as-possible conditions, rather than optimizing each architecture for each different dataset, in order to highlight the properties of our approach. State-of-the-art results for SER tasks usually involve more complex solutions, as, among others, data augmentation  [66] -  [69] , attention  [66] ,  [69] -  [71] , adversarial attacks  [72] , multimodal processing  [70] ,  [73] , speaker-aware processing  [74] ,   [75] , transformer designs  [70] ,  [75] . Moreover, the state-ofthe-art approach can be radically different for each dataset, and therefore using the best method for each dataset would not permit having the same configuration for all possible aspects in both RH-Emo experiments and the baselines. This would add much more complexity to the setup, consequently making it less straightforward to isolate and understand the properties of our approach.\n\nBecause of these reasons and the fact that many existing studies are based on different methods to compute the scores, different data splits and may use multiple data domains, our results can not be directly compared to the current stateof-the-art accuracy for these datasets, which, to the best of our knowledge are 75.60% for IEMOCAP  [71] , 87.5% for RAVDESS  [73] , 88.47% for EmoDb  [66]  and 99.6% for TESS  [67] .",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Experimental Results",
      "text": "Table  I  shows the pretraining results we obtained on IEMO-CAP, while Tables II, III, and IV provide the results on RAVDESS, EmoDB, and TESS, respectively. Table V, shows the average and best test accuracy improvement provided by our approach, among all CNN architectures for each dataset. Here, average improvement refers to the difference between the average test accuracy among all real-valued and all quaternion-valued outcomes, whereas the best improvement is the difference between the best real-valued and the best quaternion-valued accuracy we obtained. For the core results (Tables II, III, and IV) we include also the test set results in terms of Unweighted Average Recall (UAR). This gives further insight into the model's generalization performance with a metric that does not take into account possible imbalance of the datasets' labels.\n\nThe results clearly show that our approach enhances the model's performance while improving its efficiency. For all datasets, the quaternion CNNs fed with RH-emo embeddings provide the best test accuracy overall, with an accuracy improvement of 6.01 percentage points (pp) for RAVDESS, 2.34 pp for EmoDB, and 0.97 for TESS in the case we do not apply CNN pretraining. The only case where our approach does not improve the test accuracy is with the EmoDB dataset, applying CNN pretraining, where we have a performance drop of 9 pp. In the other cases where we applied CNN pretraining, our approach provides a strong average improvement of 12.88 and 13.63 pp, respectively for RAVDESS and TESS. Moreover, the test set results in terms of UAR metric confirm the overall trend of the accuracy metric. Nevertheless, in one single case (VGG-16 network on RAVDESS) there is a narrow inconsistency between the two metrics. Here the pretrained QCNN shows the best test accuracy, while the best UAR score is given by the non-pretrained QCNN.\n\nThe results computed on IEMOCAP (Table  I  and first row of Table  V ) depict a limit case, where knowledge is not transferred to different data because the same dataset is used for the RH-emo pretraining and for SER. Therefore here we did not apply any CNN pretraining. in this special case is evident that models benefit from the use of quaternion-valued SER CNNs fed with emotional embeddings, with an average improvement of 9.74 pp among all CNN designs we tested.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Vi. Ablation Studies",
      "text": "In order to further explore the properties of our approach and to support its foundations, we performed additional experiments and ablation studies. For these studies we applied the same experimental setup presented in Section V, altering only specific details, as described below.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A. Removing Rh-Emo Components",
      "text": "In this study, we alter the RH-emo structure and test the emotion recognition accuracy using the embeddings generated from the modified RH-emo networks. We compared the full RH-emo, as described in Section IV, to the following altered versions:\n\n‚Ä¢ Real: identical to the regular network, but the decoder part is real-valued and no split activation is applied to the reconstructed output in the loss function.\n\n‚Ä¢ Reconstruction only: we removed the supervised classification branch, resulting in a completely unsupervised real-quaternion hybrid autoencoder. ‚Ä¢ Emotion only: we removed the unsupervised reconstruction branch from the network, obtaining a completely supervised and real-valued emotion classification CNN. In this configuration, there are still 4 target outputs, each with a dedicated classifier (discrete emotion, valence, arousal, dominance). ‚Ä¢ Discrete emotion only: we removed the valence, arousal, and dominance classifiers, keeping only the discrete emotion classification branch. The rest of the network is unaltered. ‚Ä¢ valence-arousal-dominance only: we removed the discrete emotion recognition branch, keeping only the branches for valence, arousal, and dominance. The rest of the network is unaltered. Figure  3  exposes the results of this ablation study. In the figure, we show the mean test accuracy improvement obtained for all corpora with the quaternion-valued VGG16, AlexNet, and ResNet-50 over the real-valued baselines. Each row shows the results obtained feeding the quaternion-valued networks with the embeddings created with the above-described variants of RH-emo. These results consistently confirm the foundation of our approach. The performance of all variants is inferior to the full RH-emo. In addition, we recall that the quaternion-valued CNNs fed with the emotional embeddings use a considerably lower amount of parameters. The results point out that the unsupervised branch of RH-emo is fundamental to obtain useful embeddings, in fact, the emotion-only version, where the decoder part of RH-emo is removed, provides the most severe drop in performance compared to all variants and also the baseline. As we expected, the quaternion-valued decoder of the actual RH-emo outperforms the completely real-valued version (by 2.8pp). This supports our hypothesis that a quaternionvalue decoder is able to create embeddings that present more suitable intra-channel correlations for the quaternion-valued CNNs. Moreover, also here, the quaternion approach leads to faster (pre)training and less memory demand due to the lower amount of parameters. The completely unsupervised variant (recognition-only) is conceptually similar to R2Hae  [55] , but it relies on a convolutional design and it is applied to a different domain. This ablation study shows that the addition of a classification branch to R2Hae provides an improvement in performance (by 0.3 pp in our case) and therefore the semi-supervision can be considered a valuable extension to R2Hae. This ablation study also shows that the classification of emotion in the valence-arousal-dominance space is more influential in the creation of stronger embeddings. In fact, the RH-emo variant without discrete classification provides superior accuracy compared to the discrete-only version (by 0.5 pp) This is further supported by the fact that, as a result of an extensive grid search, we apply a stronger weight to the valence-arousal-dominance term of the loss function (the ùõº term in eq. (  4 )).",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "B. Removing Rh-Emo Pretraining And Backpropagation",
      "text": "We performed an additional ablation study where we alter how the RH-emo weights are initialized and backpropagated during the SER training. Figure  4  depicts the results of this study, showing the average difference in test accuracy per-dataset among all CNN designs. On the one hand, we initialized the weights of RH-emo with random values while we regularly backpropagate the gradients of the RH-emo's encoder layers (blue rows). By doing this, we completely ignore the RH-emo pretraining and we force the QCNN network to perform end-to-end training, directly learning how to map the real-valued input spectrograms into quaternioncompatible representations to feed the QCNNs. This approach is conceptually similar to (R2He)  [55] . the other hand, we regularly initialize the weights of RH-emo with the pretrained RH-emo network, but we don't backpropagate the RH-emo layers (orange rows). The results of this experiment strongly support the foundation of our approach. The removal of RHemo pretraining causes a consistent and substantial decrease in the QCNNs test performance, of 29.4, 3.25, and 6.97 pp for RAVDESS, EmoDB, and TESS, respectively. This confirms the importance of the prior training of the RH-emo encoder, as exposed in Section IV, for the development of adequate quaternion emotional embeddings. On the contrary, the lack of backpropagation of the RH-emo layers does not provide a consistent performance drop. While the performance decreases for EmoDB (25 pp ) and for TESS (0.22 pp), a narrow accuracy boost is evident for RAVDESS (0.91 pp). Moreover, the performance difference is averagely inferior compared to the no-pretraining case.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "C. Reducing Training Data",
      "text": "As a further study, we re-trained all CNNs and QCNNs, progressively decreasing the amount of training and validation data. The size of the test set, instead, is kept unaltered, in order to have a consistent performance measure that can be compared with the other results presented in this paper. Figure  5  shows the outcomes of this experiment. Each line shows the trend of the average test accuracy among all CNN architectures, at different reduction rates of the data. Specifically, we trained on 100%, 75%, 50%, 25%, 10%, 5% and 1% of the available data. The yellow and red lines are the baselines, respectively with and without CNN pretraining on IEMOCAP. Instead, the green and blue lines show the trend for the QCNNs + RH-emo, respectively with and without CNN pretraining.  The results of this ablation study clearly point out that our method can provide consistent performance improvement even in conditions with less data. In all cases but one (5% of training data) our pretrained approach surpasses both realvalued baselines. This is a convenient property for SER tasks, considering the general scarcity of emotion-labelled speech audio data. VII. DISCUSSION A. Resource savings RH-emo permits to spare of a considerable amount of parameters. Compared to the real counterparts, the quaternion VGG16 uses the ‚àº6% of the parameters, while the quaternion AlexNet and ResNet-50 use the ‚àº25%. The difference between the VGG16 and the others is due to the lack of adaptive average pooling (as described above). Therefore, on the one hand, the use of quaternion-valued layers instead of realvalued ones permits to drop in the number of parameters by a factor of 0.25, while, on the other hand, the smaller feature dimensionality obtained with the embeddings further cuts down the number of parameters by a factor of 0.25. This in turn permits the reduction of the model's memory requirements and training time. In our implementation, the embedding computation happens during the training for every batch and, therefore, both the main network and the RH-emo feature extractor are loaded into the memory. This simulates a plausible application scenario of RH-emo, where the embeddings need to be computed in real-time. Although it is possible to pre-compute the embeddings as part of the preprocessing pipeline, further reducing the memory demand and computation time. As regards the memory demand, in our setup the quaternion networks require on average 84.2% of memory, compared to their real-valued equivalents. For the VGG16 (where we don't apply average pooling) the memory demand is approximately 70%, for AlexNet the 89%, and for ResNet-50 the 93%. Regarding the training time, the epoch duration of our quaternion networks compared to the real networks is approximately 15.9% for VGG16, 88.1% for AlexNet, and 162.6% for ResNet-50. These outcomes show that the maximum efficiency in terms of both memory demand and computation time is obtained for VGG16, where we take advantage of the reduced dimensionality of the embeddings. On the other hand, the accuracy improvement for ResNet-50 comes at the cost of an increased computation time with respect to the real networks, but still reducing the model's memory demand.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Reconstruction Properties",
      "text": "Figure  6  shows an example of the decoder's output of the pretrained RH-emo model. The Input subplot is the input magnitudes-only spectrogram and the Output: mean is the element-wise mean of the quaternion separate axes and, therefore, the actual matrix that is compared to the input in the loss function. The sub-plots labelled as Output: real, ƒ±, »∑, Œ∫ depict the separate quaternion axes, which are generated from the emotional embeddings: real from the discrete emotion classification matrix, and ƒ±, »∑, Œ∫ from the valence, arousal, and dominance channels, respectively.\n\nBy comparing the Input and the Output: mean plots, it is evident that the reconstruction is not perfect. While the time-wise articulation of the speech seems to be accurately reproduced, the model is not able to reconstruct in detail the most feeble harmonics of the signal. Although it is interesting the way the different quaternion axes are differentiated. In the real axis, the model seems to perform an operation similar to Decoder output matrices Fig.  6 . Example of RH-emo quaternion reconstruction of a speech spectrogram. Input is the magnitudes-only input spectrogram, Output: real, ƒ±, »∑, Œ∫ are the four output matrices of RH-emo, respectively reconstructed from the discrete emotion, valence, arousal and dominance axes of the embeddings, Output: mean is the pixel-wise average of Output: real, ƒ±, »∑, Œ∫ and is the matrix that is compared to the input in the loss function.\n\namplitude compression (obtainable, for instance, by computing the square root of the matrix), bringing up the signal's quietest portions around the speech region. Instead, in the 3 complex axes (ƒ±, »∑, Œ∫) different aspects of the signal are highlighted, focusing on different harmonics and/or temporal areas. Our intuition is that these representations may represent different \"emotional points of view\" of the input speech signal.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C. Limitations",
      "text": "Besides the numerous advantages that our approach provides, there are also some intrinsic limitations. The main constraint of our approach is that a pretrained RH-emo network can be used for only a fixed time scale. In this paper, we considered a temporal window of 4 seconds, which is well suited for most SER tasks and datasets. If a different time scale is needed, then a specific RH-emo has to be trained on purpose. Another limitation is that training with an end-to-end fashion is not possible, as a pre-trained RH-emo is needed and the omission of the RH-emo pretraining stage leads to a drastic decrease in the model's performance, as shown in Section VI-B.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Applications And Future Work",
      "text": "The advantages provided by the combination of RH-emo and quaternion-valued networks suggest several application scenarios. Due to the substantial saving of trainable parameters, memory, and training time, our approach is particularly suited for situations where limited resources are available and performance can not be sacrificed. Another useful property of RH-emo is that while the embeddings carry the necessary information to perform SER tasks (as proven by our experimental results), they also provide speaker anonymity, as it is not possible to reconstruct the input spectrogram without the RH-emo pretrained weights. This could be exploited in situations where sensible speech data must be used for SER tasks.\n\nThe positive results we obtained justify further investigation of this approach. An immediate research objective is to test RH-emo with different datasets, and architectures (including recurrent networks), with multiple time scales and to different tasks. In particular, we intend to apply the same principle of RH-emo (based on a semi-supervised autoencoder where each embedded channel is optimized for the classification of a different characteristic of an entity) for different tasks, where a quadral representation of input data can not be directly inferred from data, as for speech emotion. An example of this is music genre recognition tasks, where the embedded dimensions of the autoencoder are optimized for tempo, harmonic key, spoken words, and instrument type recognition.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Viii. Conclusions",
      "text": "In this paper we presented RH-emo, a semi-supervised approach to obtain quaternion emotional embeddings from real speech spectrograms. This method enables to perform speech emotion recognition tasks with quaternion-valued convolutional neural networks, using real-valued magnitudes spectrograms as input. We use RH-emo pretrained on IEMOCAP to extract quaternion embeddings from speech spectrograms, where the individual axes are optimized for the classification of different emotional characteristics: valence, arousal, dominance, and overall discrete emotion.\n\nWe compare the performance on SER tasks of real-valued CNNs fed with regular spectrograms and quaternion-valued CNNs fed with RH-emo embeddings. We evaluate our approach on a variety of cases, using 4 popular SER datasets (IEMOCAP, RAVDESS, EmoDB, TESS) and with 3 widelyused CNN designs of increasing capacity (ResNet-50, AlexNet and VGG16). Our approach provides a consistent improvement in the test accuracy for all datasets while using a considerably lower amount of resources. We obtained an average improvement of 6.01 pp for RAVDESS, 2.34 pp for EmoDB, and 0.97 pp for TESS and we spared up to 94% of the trainable parameters, up to the 30% of GPU memory and up to 84.1% of training time. Moreover, we performed additional experiments and ablations studies that confirm the properties and foundations of our approach. The results show that the combination of RH-emo and QCNNs is a suitable strategy to circumvent the high resource demand of SER models and that our approach provides consistent performance improvement also in scenarios where the available training data is scarce.\n\nThe positive results justify further investigation of this approach. An immediate research objective is to test RHemo with different datasets, architectures (including recurrent networks), with multiple signal dimensions, and different tasks.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). RH-emo is intended to be used as a feature extractor",
      "page": 3
    },
    {
      "caption": "Figure 1: shows, our RH-emo is composed of",
      "page": 4
    },
    {
      "caption": "Figure 1: RH-emo Block Diagram. An input magnitudes-only spectrogram is",
      "page": 4
    },
    {
      "caption": "Figure 2: depicts all cases we include in our experimental",
      "page": 6
    },
    {
      "caption": "Figure 2: shows the 3 consecutive",
      "page": 6
    },
    {
      "caption": "Figure 2: , is a standard real-valued CNN with randomly-",
      "page": 6
    },
    {
      "caption": "Figure 2: , we test a standard transfer learning",
      "page": 6
    },
    {
      "caption": "Figure 2: Block diagram of our experimental setup. The yellow-to-blue color",
      "page": 6
    },
    {
      "caption": "Figure 3: Ablation study results. The x axis shows the average drop in",
      "page": 8
    },
    {
      "caption": "Figure 3: exposes the results of this ablation study. In the Ô¨Åg-",
      "page": 8
    },
    {
      "caption": "Figure 4: depicts the results of",
      "page": 9
    },
    {
      "caption": "Figure 5: shows the outcomes of this experiment. Each line",
      "page": 9
    },
    {
      "caption": "Figure 4: Ablation study results. The x axis shows the average difference in test",
      "page": 9
    },
    {
      "caption": "Figure 5: Ablation study results. The y axis shows the test accuracy drop of each",
      "page": 9
    },
    {
      "caption": "Figure 6: shows an example of the decoder‚Äôs output of",
      "page": 10
    },
    {
      "caption": "Figure 6: Example of RH-emo quaternion reconstruction of a speech spectro-",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arch.": "RH-emo",
          "Method": "/",
          "Params": "1.3 √ó 108",
          "Train acc.": "80.34",
          "Test acc.": "60.7"
        },
        {
          "Arch.": "VGG16",
          "Method": "Real\nRH-emo+Quat",
          "Params": "1.6 √ó 108\n1 √ó 107",
          "Train acc.": "74.88\n72.25",
          "Test acc.": "62.87\n71.10"
        },
        {
          "Arch.": "AlexNet",
          "Method": "Real\nRH-emo+Quat",
          "Params": "5.7 √ó 107\n1 √ó 107",
          "Train acc.": "71.02\n71.81",
          "Test acc.": "63.33\n70.31"
        },
        {
          "Arch.": "ResNet",
          "Method": "Real\nRH-emo+Quat",
          "Params": "2.3 √ó 107\n4.9 √ó 106",
          "Train acc.": "61.05\n73.03",
          "Test acc.": "57.20\n71.20"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arch.": "VGG16",
          "Method": "Real\nRH-emo+Quat\nReal-Pre\nRH-emo+Quat-Pre",
          "Params": "1.6 √ó 108\n1 √ó 107\n1.6 √ó 108\n1 √ó 107",
          "Train acc. Test acc. Test UAR": "97.62\n97.62\n99.52\n97.85"
        },
        {
          "Arch.": "AlexNet",
          "Method": "Real\nRH-emo+Quat\nReal-Pre\nRH-emo+Quat-Pre 1.4 √ó 107",
          "Params": "5.7 √ó 107\n1 √ó 107\n5.7 √ó 107",
          "Train acc. Test acc. Test UAR": "98.01\n98.56\n98.01\n98.81"
        },
        {
          "Arch.": "ResNet",
          "Method": "Real\nRH-emo+Quat\nReal-Pre\nRH-emo+Quat-Pre 4.9 √ó 106",
          "Params": "2.3 √ó 107\n4.9 √ó 106\n2.3 √ó 107",
          "Train acc. Test acc. Test UAR": "97.38\n99.76\n57.53\n99.28"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Dataset": "",
          "Average improvement": "No pret.\nPret.\nOverall",
          "Best\nimprovement": ""
        },
        {
          "Dataset": "IEMOCAP",
          "Average improvement": "9.74\n/\n/",
          "Best\nimprovement": "7.87"
        },
        {
          "Dataset": "RAVDESS",
          "Average improvement": "6.01\n12.88\n9.45",
          "Best\nimprovement": "4.09"
        },
        {
          "Dataset": "EmoDB",
          "Average improvement": "2.34\n-9.00\n-3.34",
          "Best\nimprovement": "1.00"
        },
        {
          "Dataset": "TESS",
          "Average improvement": "0.97\n13.63\n7.30",
          "Best\nimprovement": "0.24"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arch.": "VGG16",
          "Method": "Real\nRH-emo+Quat\nReal-Pre\nRH-emo+Quat-Pre",
          "Params": "1.6 √ó 108\n1 √ó 107\n1.6 √ó 108\n1 √ó 107",
          "Train acc. Test acc. Test UAR": "41.06\n49.85\n45.30\n53.79"
        },
        {
          "Arch.": "AlexNet",
          "Method": "Real\nRH-emo+Quat\nReal-Pre\nRH-emo+Quat-Pre 1.4 √ó 107",
          "Params": "5.7 √ó 107\n1 √ó 107\n5.7 √ó 107",
          "Train acc. Test acc. Test UAR": "46.36\n43.94\n51.06\n47.58"
        },
        {
          "Arch.": "ResNet",
          "Method": "Real\nRH-emo+Quat\nReal-Pre\nRH-emo+Quat-Pre 4.9 √ó 106",
          "Params": "2.3 √ó 107\n4.9 √ó 106\n2.3 √ó 107",
          "Train acc. Test acc. Test UAR": "43.48\n55.15\n18.79\n52.42"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Arch.": "VGG16",
          "Method": "Real\nRH-emo+Quat\nReal-Pre\nRH-emo+Quat-Pre",
          "Params": "1.6 √ó 108\n1 √ó 107\n1.6 √ó 108\n1 √ó 107",
          "Train acc. Test acc. Test UAR": "70.00\n50.00\n52.00\n47.00"
        },
        {
          "Arch.": "AlexNet",
          "Method": "Real\nRH-emo+Quat\nReal-Pre\nRH-emo+Quat-Pre 1.4 √ó 107",
          "Params": "5.7 √ó 107\n1 √ó 107\n5.7 √ó 107",
          "Train acc. Test acc. Test UAR": "47.00\n49.00\n67.00\n71.00"
        },
        {
          "Arch.": "ResNet",
          "Method": "Real\nRH-emo+Quat\nReal-Pre\nRH-emo+Quat-Pre 4.9 √ó 106",
          "Params": "2.3 √ó 107\n4.9 √ó 106\n2.3 √ó 107",
          "Train acc. Test acc. Test UAR": "48.00\n73.00\n72.00\n46.00"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Comparison of speaker dependent and speaker independent emotion recognition",
      "authors": [
        "J Rybka",
        "A Janicki"
      ],
      "year": "2013",
      "venue": "Int. J. of Applied Math. and Comput. Science"
    },
    {
      "citation_id": "2",
      "title": "Context-independent multilingual emotion recognition from speech signals",
      "authors": [
        "V Hozjan",
        "Z Kaƒçiƒç"
      ],
      "year": "2003",
      "venue": "Int. J. of Speech Techn"
    },
    {
      "citation_id": "3",
      "title": "Feeling backwards? How temporal order in speech affects the time course of vocal emotion recognition",
      "authors": [
        "S Rigoulot",
        "E Wassiliwizky",
        "M Pell"
      ],
      "year": "2013",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "4",
      "title": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition",
      "authors": [
        "S Khorram",
        "Z Aldeneh",
        "D Dimitriadis",
        "M Mcinnis",
        "E Provost"
      ],
      "year": "2017",
      "venue": "Capturing long-term temporal dependencies with convolutional networks for continuous emotion recognition"
    },
    {
      "citation_id": "5",
      "title": "Unsupervised representation learning with future observation prediction for speech emotion recognition",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Unsupervised representation learning with future observation prediction for speech emotion recognition"
    },
    {
      "citation_id": "6",
      "title": "Affect representation and recognition in 3d continuous valence-arousal-dominance space",
      "authors": [
        "G Verma",
        "U Tiwary"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "7",
      "title": "Multiclass emotion recognition within the valence-arousal-dominance space using eeg",
      "authors": [
        "M Gaertner",
        "D Sauter",
        "H Baumgartl",
        "T Rieg",
        "R Buettner"
      ],
      "year": "2021",
      "venue": "AMCIS"
    },
    {
      "citation_id": "8",
      "title": "Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "authors": [
        "S Buechel",
        "U Hahn"
      ],
      "year": "2022",
      "venue": "Emobank: Studying the impact of annotation perspective and representation format on dimensional emotion analysis",
      "arxiv": "arXiv:2205.01996"
    },
    {
      "citation_id": "9",
      "title": "A neural network approach for human emotion recognition in speech",
      "authors": [
        "M Bhatti",
        "Y Wang",
        "L Guan"
      ],
      "year": "2004",
      "venue": "IEEE Int. Symp. on Circuits and Syst. (ISCAS)"
    },
    {
      "citation_id": "10",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Process. Mag"
    },
    {
      "citation_id": "11",
      "title": "Emotion recognition in speech using neural networks",
      "authors": [
        "J Nicholson",
        "K Takahashi",
        "R Nakatsu"
      ],
      "year": "2000",
      "venue": "Neural Comput. & Applicat"
    },
    {
      "citation_id": "12",
      "title": "Fast and accurate sequential floating forward feature selection with the bayes classifier applied to speech emotion recognition",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2008",
      "venue": "Signal Process"
    },
    {
      "citation_id": "13",
      "title": "Multi-level speech emotion recognition based on HMM and ANN",
      "authors": [
        "X Mao",
        "L Chen",
        "L Fu"
      ],
      "year": "2009",
      "venue": "WRI World Congress on Comput"
    },
    {
      "citation_id": "14",
      "title": "Speech emotion recognition using hidden Markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech Communic"
    },
    {
      "citation_id": "15",
      "title": "Speech emotion recognition based on rough set and SVM",
      "authors": [
        "J Zhou",
        "G Wang",
        "Y Yang",
        "P Chen"
      ],
      "year": "2006",
      "venue": "IEEE Int. Conf. on Cognitive Informat"
    },
    {
      "citation_id": "16",
      "title": "GMM supervector based SVM with spectral features for speech emotion recognition",
      "authors": [
        "H Hu",
        "M.-X Xu",
        "W Wu"
      ],
      "year": "2007",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "17",
      "title": "Emotion recognition in spontaneous speech using GMMs",
      "authors": [
        "D Neiberg",
        "K Elenius",
        "K Laskowski"
      ],
      "year": "2006",
      "venue": "Int. Conf. on Spoken Language Process"
    },
    {
      "citation_id": "18",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Ayadi",
        "M Kamel",
        "F Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recogn"
    },
    {
      "citation_id": "19",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah",
        "J Ahmad",
        "N Rahim",
        "S Baik"
      ],
      "year": "2017",
      "venue": "Int. Conf. on Platform Techn. and Service (PlatCon)"
    },
    {
      "citation_id": "20",
      "title": "End-to-end speech emotion recognition with gender information",
      "authors": [
        "T.-W Sun"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "21",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomed. Signal Process. and Control"
    },
    {
      "citation_id": "22",
      "title": "High-level feature representation using recurrent neural network for speech emotion recognition",
      "authors": [
        "J Lee",
        "I Tashev"
      ],
      "year": "2015",
      "venue": "High-level feature representation using recurrent neural network for speech emotion recognition"
    },
    {
      "citation_id": "23",
      "title": "Emotion recognition from speech with recurrent neural networks",
      "authors": [
        "V Chernykh",
        "P Prikhodko"
      ],
      "year": "2017",
      "venue": "Emotion recognition from speech with recurrent neural networks",
      "arxiv": "arXiv:1701.08071"
    },
    {
      "citation_id": "24",
      "title": "Adieu features? End-to-end speech emotion recognition using a deep convolutional recurrent network",
      "authors": [
        "G Trigeorgis",
        "F Ringeval",
        "R Brueckner",
        "E Marchi",
        "M Nicolaou",
        "B Schuller",
        "S Zafeiriou"
      ],
      "year": "2016",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process"
    },
    {
      "citation_id": "25",
      "title": "Improving convolutional recurrent neural networks for speech emotion recognition",
      "authors": [
        "P Meyer",
        "Z Xu",
        "T Fingscheidt"
      ],
      "year": "2021",
      "venue": "IEEE Spoken Language Techn. Workshop (SLT)"
    },
    {
      "citation_id": "26",
      "title": "Speech emotion recognition using convolutional recurrent neural networks and spectrograms",
      "authors": [
        "M Qamhan",
        "A Meftah",
        "S.-A Selouani",
        "Y Alotaibi",
        "M Zakariah",
        "Y Seddiq"
      ],
      "year": "2020",
      "venue": "Speech emotion recognition using convolutional recurrent neural networks and spectrograms"
    },
    {
      "citation_id": "27",
      "title": "Deep learning for robust feature generation in audiovisual emotion recognition",
      "authors": [
        "Y Kim",
        "H Lee",
        "E Provost"
      ],
      "year": "2013",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process"
    },
    {
      "citation_id": "28",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE Trans. Multimedia"
    },
    {
      "citation_id": "29",
      "title": "Speech emotion recognition using CNN",
      "authors": [
        "Z Huang",
        "M Dong",
        "Q Mao",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "ACM Int. Conf. on Multimedia"
    },
    {
      "citation_id": "30",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Speech emotion recognition using deep neural network and extreme learning machine"
    },
    {
      "citation_id": "31",
      "title": "Generating synthetic audio data for attention-based speech recognition systems",
      "authors": [
        "N Rossenbach",
        "A Zeyer",
        "R Schl√ºter",
        "H Ney"
      ],
      "year": "2020",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "32",
      "title": "You do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation",
      "authors": [
        "A Laptev",
        "R Korostik",
        "A Svischev",
        "A Andrusenko",
        "I Medennikov",
        "S Rybin"
      ],
      "year": "2020",
      "venue": "Int. Congress on Image and Signal Process., BioMed. Engin. and Informatics"
    },
    {
      "citation_id": "33",
      "title": "On the use of self-supervised pre-trained acoustic and linguistic features for continuous speech emotion recognition",
      "authors": [
        "M Macary",
        "M Tahon",
        "Y Est√®ve",
        "A Rousseau"
      ],
      "year": "2021",
      "venue": "IEEE Spoken Language Techn. Workshop (SLT)"
    },
    {
      "citation_id": "34",
      "title": "Emotion recognition from speech using Wav2vec 2.0 embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Emotion recognition from speech using Wav2vec 2.0 embeddings"
    },
    {
      "citation_id": "35",
      "title": "Anti-transfer learning for task invariance in convolutional neural networks for speech processing",
      "authors": [
        "E Guizzo",
        "T Weyde",
        "G Tarroni"
      ],
      "year": "2021",
      "venue": "Neural Netw"
    },
    {
      "citation_id": "36",
      "title": "Multi-window data augmentation approach for speech emotion recognition",
      "authors": [
        "S Padi",
        "D Manocha",
        "R Sriram"
      ],
      "year": "2020",
      "venue": "Multi-window data augmentation approach for speech emotion recognition",
      "arxiv": "arXiv:2010.09895"
    },
    {
      "citation_id": "37",
      "title": "Speech emotion recognition using data augmentation method by cycle-generative adversarial networks",
      "authors": [
        "A Shilandari",
        "H Marvi",
        "H Khosravi"
      ],
      "year": "2022",
      "venue": "Signal, Image and Video Process"
    },
    {
      "citation_id": "38",
      "title": "Dimensionality reduction for emotional speech recognition",
      "authors": [
        "P Fewzee",
        "F Karray"
      ],
      "year": "2012",
      "venue": "Int. Conf. on Privacy, Security, Risk and Trust and Int. Conf. on Social Comput"
    },
    {
      "citation_id": "39",
      "title": "Impact of autoencoder based compact representation on emotion detection from audio",
      "authors": [
        "N Patel",
        "S Patel",
        "S Mankad"
      ],
      "year": "2021",
      "venue": "J. of Ambient Intelligence and Humanized Comput"
    },
    {
      "citation_id": "40",
      "title": "Lightweight and efficient neural natural language processing with quaternion networks",
      "authors": [
        "Y Tay",
        "A Zhang",
        "L Tuan",
        "J Rao",
        "S Zhang",
        "S Wang",
        "J Fu",
        "S Hui"
      ],
      "year": "2019",
      "venue": "Proc. of the 57th Ann. Meeting of the Assoc. for Computat. Linguistics"
    },
    {
      "citation_id": "41",
      "title": "A quaternion-valued variational autoencoder",
      "authors": [
        "E Grassucci",
        "D Comminiello",
        "A Uncini"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "42",
      "title": "Quaternion generative adversarial networks",
      "authors": [
        "E Grassucci",
        "E Cicero",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "Generative Adversarial Learning: Architectures and Applications"
    },
    {
      "citation_id": "43",
      "title": "PHNNs: Lightweight neural networks via parameterized hypercomplex convolutions",
      "authors": [
        "E Grassucci",
        "A Zhang",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "44",
      "title": "Introducing quaternion multi-valued neural networks with numerical examples",
      "authors": [
        "A Greenblatt",
        "S Agaian"
      ],
      "year": "2018",
      "venue": "Information Sciences"
    },
    {
      "citation_id": "45",
      "title": "Quaternion convolutional neural networks for end-toend automatic speech recognition",
      "authors": [
        "T Parcollet",
        "Y Zhang",
        "M Morchid",
        "C Trabelsi",
        "G Linar√®s",
        "R Mori",
        "Y Bengio"
      ],
      "year": "2018",
      "venue": "Quaternion convolutional neural networks for end-toend automatic speech recognition"
    },
    {
      "citation_id": "46",
      "title": "Speech emotion recognition using quaternion convolutional neural networks",
      "authors": [
        "A Muppidi",
        "M Radfar"
      ],
      "year": "2021",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "47",
      "title": "Hypercomplex signals-a novel extension of the analytic signal to the multidimensional case",
      "authors": [
        "T Bulow",
        "G Sommer"
      ],
      "year": "2001",
      "venue": "IEEE Trans. Signal Process"
    },
    {
      "citation_id": "48",
      "title": "A quaternion gradient operator and its applications",
      "authors": [
        "D Mandic",
        "C Jahanchahi",
        "C Took"
      ],
      "year": "2010",
      "venue": "IEEE Signal Process. Lett"
    },
    {
      "citation_id": "49",
      "title": "Frequencydomain adaptive filtering: From real to hypercomplex signal processing",
      "authors": [
        "D Comminiello",
        "M Scarpiniti",
        "R Parisi",
        "A Uncini"
      ],
      "year": "2019",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "50",
      "title": "Quaternion convolutional neural networks for detection and localization of 3D sound events",
      "authors": [
        "D Comminiello",
        "M Lella",
        "S Scardapane",
        "A Uncini"
      ],
      "year": "2019",
      "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Process. (ICASSP)"
    },
    {
      "citation_id": "51",
      "title": "Ambisonics-an overview",
      "authors": [
        "R Furness"
      ],
      "year": "1990",
      "venue": "AES 8th Int. Conf. The Sound of Audio"
    },
    {
      "citation_id": "52",
      "title": "Quaternion neural networks for multi-channel distant speech recognition",
      "authors": [
        "X Qiu",
        "T Parcollet",
        "M Ravanelli",
        "N Lane",
        "M Morchid"
      ],
      "year": "2020",
      "venue": "Quaternion neural networks for multi-channel distant speech recognition"
    },
    {
      "citation_id": "53",
      "title": "Efficient sound event localization and detection in the quaternion domain",
      "authors": [
        "C Brignone",
        "G Mancini",
        "E Grassucci",
        "A Uncini",
        "D Comminiello"
      ],
      "year": "2022",
      "venue": "IEEE Trans. Circuits and Systems"
    },
    {
      "citation_id": "54",
      "title": "Dual quaternion ambisonics array for six-degree-of-freedom acoustic representation",
      "authors": [
        "E Grassucci",
        "G Mancini",
        "C Brignone",
        "A Uncini",
        "D Comminiello"
      ],
      "year": "2023",
      "venue": "Pattern Recognition Lett"
    },
    {
      "citation_id": "55",
      "title": "Real to H-space autoencoders for theme identification in telephone conversations",
      "authors": [
        "T Parcollet",
        "M Morchid",
        "X Bost",
        "G Linar√®s",
        "R Mori"
      ],
      "year": "2019",
      "venue": "IEEE/ACM Trans. Audio, Speech, Language Process"
    },
    {
      "citation_id": "56",
      "title": "Deep quaternion networks",
      "authors": [
        "C Gaudet",
        "A Maida"
      ],
      "year": "2018",
      "venue": "IEEE Int. Joint Conf. on Neural Netw. (IJCNN)"
    },
    {
      "citation_id": "57",
      "title": "Quaternion-valued nonlinear adaptive filtering",
      "authors": [
        "B Ujang",
        "C Took",
        "D Mandic"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Neural Netw"
    },
    {
      "citation_id": "58",
      "title": "IEMOCAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "59",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "60",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS One"
    },
    {
      "citation_id": "61",
      "title": "A database of German emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Europ. Conf. on Speech Commun. and Techn"
    },
    {
      "citation_id": "62",
      "title": "Recognition of emotional speech for younger and older talkers: Behavioural findings from the toronto emotional speech set",
      "authors": [
        "K Dupuis",
        "M Pichora-Fuller"
      ],
      "year": "2011",
      "venue": "Canadian Acoust"
    },
    {
      "citation_id": "63",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2015",
      "venue": "Int. Conf. on Learning Representations (ICLR)"
    },
    {
      "citation_id": "64",
      "title": "ImageNet classification with deep convolutional neural networks",
      "authors": [
        "A Krizhevsky",
        "I Sutskever",
        "G Hinton"
      ],
      "year": "2012",
      "venue": "Advances in Neural Inform. Process. Syst. (NIPS)"
    },
    {
      "citation_id": "65",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "IEEE Conf. on Comp. Vision and Pattern Recog"
    },
    {
      "citation_id": "66",
      "title": "Hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition",
      "authors": [
        "N Pham",
        "D Dang",
        "S Nguyen"
      ],
      "year": "2021",
      "venue": "Hybrid data augmentation and deep attention-based dilated convolutional-recurrent neural networks for speech emotion recognition",
      "arxiv": "arXiv:2109.09026"
    },
    {
      "citation_id": "67",
      "title": "Mff-saug: Multi feature fusion with spectrogram augmentation of speech emotion recognition using convolution neural network",
      "authors": [
        "S Jothimani",
        "K Premalatha"
      ],
      "year": "2022",
      "venue": "Chaos, Solitons & Fractals"
    },
    {
      "citation_id": "68",
      "title": "Speech emotion recognition with data augmentation and layer-wise learning rate adjustment",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Speech emotion recognition with data augmentation and layer-wise learning rate adjustment",
      "arxiv": "arXiv:1802.05630"
    },
    {
      "citation_id": "69",
      "title": "Speech emotion recognition with multiscale area attention and data augmentation",
      "authors": [
        "M Xu",
        "F Zhang",
        "X Cui",
        "W Zhang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "70",
      "title": "Multimodal approach of speech emotion recognition using multi-level multi-head fusion attention-based recurrent neural network",
      "authors": [
        "N.-H Ho",
        "H.-J Yang",
        "S.-H Kim",
        "G Lee"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "71",
      "title": "Speechbased emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing",
      "authors": [
        "S Kakouros",
        "T Stafylakis",
        "L Mosner",
        "L Burget"
      ],
      "year": "2022",
      "venue": "Speechbased emotion recognition with self-supervised models using attentive channel-wise correlations and label smoothing",
      "arxiv": "arXiv:2211.01756"
    },
    {
      "citation_id": "72",
      "title": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "arxiv": "arXiv:1811.11402"
    },
    {
      "citation_id": "73",
      "title": "Cross-modal learning for audio-visual emotion recognition in acted speech",
      "authors": [
        "Y Bouali",
        "O Ahmed",
        "S Mazouzi"
      ],
      "year": "2022",
      "venue": "2022 6th International Conference on Advanced Technologies for Signal and Image Processing"
    },
    {
      "citation_id": "74",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "arxiv": "arXiv:2108.12009"
    },
    {
      "citation_id": "75",
      "title": "Hitrans: A transformerbased context-and speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    }
  ]
}