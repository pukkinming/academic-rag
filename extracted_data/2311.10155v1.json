{
  "paper_id": "2311.10155v1",
  "title": "Exploring Emotions In Eeg: Deep Learning Approach With Feature Fusion",
  "published": "2023-11-16T19:19:56Z",
  "authors": [
    "Danastan Tasaouf Mridula",
    "Abu Ahmed Ferdaus",
    "Tanmoy Sarkar Pias"
  ],
  "keywords": [
    "Emotion Recognition",
    "EEG Signal",
    "EYE Movement signal",
    "1D-CNN",
    "Deep Learning",
    "Subject-Dependant Emotion Recognition",
    "SEED-V Emotion Dataset"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion is an intricate physiological response that plays a crucial role in how we respond and cooperate with others in our daily affairs. Numerous experiments have been evolved to recognize emotion, however still require exploration to intensify the performance. To enhance the performance of effective emotion recognition, this study proposes a subjectdependent robust end-to-end emotion recognition system based on a 1D convolutional neural network (1D-CNN). We evaluate the SJTU 1 Emotion EEG Dataset SEED-V with five emotions (happy, sad, neural, fear, and disgust). To begin with, we utilize the Fast Fourier Transform (FFT) to decompose the raw EEG signals into six frequency bands and extract the power spectrum feature from the frequency bands. After that, we combine the extracted power spectrum feature with eye movement and differential entropy (DE) features. Finally, for classification, we apply the combined data to our proposed system. Consequently, it attains 99.80% accuracy which surpasses each prior state-of-the-art system.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The concept of emotion detection seems an alluring research topic for humankind. Emotion is associated with both our physical health and mental health  [1] . For instance, sometimes we feel intense chest pain when we are emotionally sad. Now, researchers conduct emotion recognition experiments in terms of different aspects including computer science  [2] , neuroscience  [3] , psychology, medical diagnostics, and cognitive science. The study of emotion recognition not only recognizes mental state but also assists in recovering from anxiety  [4] , depression, neurological disorders, and driving fatigue  [5] . The emotion recognition experiment has two categories from the viewpoint of data sources: non-physiological signals-based and physiological signals-based. Considering non-physiological signals, for instance, facial expression  [6] , voice intonation, and gestures: the emotion recognition system is not effective as people can conceal emotions. Besides, the people who are physically disabled and can not speak, it is pretty tough to realize their emotions through their gestures and activities. Consequently, investigators utilize physiological signals for emotion classification. C. -J. Yang  [7]  aimed to generate an emotion recognition system based on the 1 Shanghai Jiao Tong University(SJTU) electrocardiogram (ECG) and photoplethysmography (PPG) signals for three emotional states including positive, negative, and neutral. Mahrukh  [8]  proposed an automatic labelsgenerating approach from movie subtitles and used brain fMRI images for positive, negative, and neural sentiment analysis. Among several physiological signals like electrocardiogram (ECG), functional magnetic resonance imaging (fMRI), and electroencephalogram (EEG): EEG reveals promising performance in real-time emotion recognition. Recently, with the improvement of brain-computer interaction  [9]  and the use of EEG signals, it seems human emotion detection systems are reliable. However, a single modality is not fully appropriate for emotion recognition as the different modality elicits different prospect and beneficial information. In  [10] , Zhang proposed a Deep Emotional Arousal Network (DEAN) for multimodal sentiment analysis and emotion recognition. Though immense emotion recognition research has been done using electroencephalogram(EEG) signals or multi-modalities fusion techniques but still requires more scrutinize. Our main contributions to this study are as follows:\n\n• We combine the power spectrum with eye movement and differential entropy (DE) features.\n\n• We propose a robust subject-dependent end-to-end emotion recognition system applying 1D-CNN. Also, we evaluate our method on the SEED-V dataset which achieves 99.80% accuracy.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Literature Review",
      "text": "Researchers have delved into a significant amount of experiments based on EEG signals using various publicly available datasets, including DEAP  [11] , SEED  [12] , LUMED-2  [13] , DREAMER  [14] , and MAHNOB-HCI  [15]  and feature extraction methods up to now. These days, machine learningbased emotion recognition approaches demonstrate better performance. Ahmed  [16]  applied different ML techniques, including SVM (support vector machine), and KNN (k-nearest Neighbour) for emotion recognition. K. S. Kamble  [17]  compared five ensembles learning-based ML approaches with five conventional ML approaches to classify emotions. However, ML algorithms elicit poor results in extracting deep features and nonlinear patterns  [18]  whilst deep learning techniques are convenient for extracting complex features  [19] . In  [20] , R. Hassan proposed a hybrid model with CNN and LSTM to detect real-time emotion recognition. In comparison with other deep learning models, a convolutional neural network (CNN) is a fruitful neural network model with the ability to extract high-layer semantic features. These days, CNN is widely used in signal processing  [21] , image classification  [22]  etc. The research community acclaims that CNN manifests good performance in emotion analysis. In  [23] , Liu proposed a subjectindependent algorithm, a dynamic empirical convolutional neural network (DECNN) for emotion recognition. In another study,  [24]  Akter used two different convolutional neural network (CNN) models for two levels of valence, and arousal utilizing the DEAP emotion recognition dataset. Nevertheless, numerous studies have been done with SEED and SEED-IV  [25] ,  [26]  for neural pattern analysis and emotion recognition. A. Samavat  [19]  claimed the gamma and beta band was beneficial for EEG-based emotion recognition and N. Priyadarshini  [27]  initiated a multimodal fusion network to find out better discriminative information using Gated Recurrent Unit (GRU) and Long-Short Term Memory (LSTM). Y. Jiang  [25]  brought forward a novel attention mechanism-based multiscale feature fusion network (AM-MSFFN) that could extract high-level features at different scales. Furthermore, L. Feng  [26]  was captivated to analyze the topological information of each brain segment applying ST-GCLSTM. Among three SEED emotion dataset versions including SEED, SEED-IV, and SEED-V, this study has investigated SEED-V. To the best of our knowledge, a handful of research has been done on the SEED-V emotion dataset. Table  I  illustrates some prior research regarding the SEED-V emotion dataset.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "In this study, we propose a subject-dependent end-to-end emotion detection system. A complete workflow of the proposed system is demonstrated in Fig.  1 . At first, we explore the SJTU SEED-V emotion dataset. After that, we decompose raw EEG signals and extract the power spectrum feature from sub-bands. Afterward, we combine all features and fit into a 1D-CNN model. Finally, the model recognizes emotion from it.\n\nA. The SEED-V Dataset BCMI laboratory provides the SJTU Emotion EEG Dataset (SEED-V). This dataset is freely available to the academic community as a modified version of the SEED dataset. This dataset comprises EEG signals and eye movement signals of 16 participants for five emotions (happy, fear, sad, disgust, and neutral). Besides, their team provides an extracted differential entropy(DE) feature. An overview of the SEED-V dataset is shown in Fig.  2 .\n\nThe investigator recorded EEG signals through a 62-channel ESI NeuroScan System 3 and eye movement signals with SMI    II  illustrates the characteristics of the SEED-V dataset. Participants provided their ratings of sad, happy, disgust, neural, and fear according to their point of view.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Preprocessing",
      "text": "EEG signals make noise instinctively due to artifacts, for instance, eye blink, saccade, muscle movement, and head movement. Such raw data can interrupt while analysis. However, from the raw EEG signals, the SEED-V dataset team has extracted the differential entropy feature. Consequently, the pre-processed differential entropy (DE) feature is unchained from any noise and convenient for analysis.  with FFT. After that, we extract the power spectrum feature using the bin power function from the PyEEG module of Python from the bands with 66 channels. The power spectrum denotes how much power is contained in the frequency components of the signal. 2 Equation 1 depicts the power spectrum as follows:\n\nwhere R xx (τ ) denotes autocorrelation function of the signal. The power of the signal for a given frequency band is [f1,f2], where 0 < f 1 < f 2.\n\nAs S xx (-f ) = S xx (f ), from a positive and negative frequency band, a similar amount of power can be attributed, which is depicted as equation  2 2) Differential Entropy feature extraction: SEED-V team has provided a pre-processed noise-free differential entropy(DE) feature. First of all, with a 200Hz sampling rate, the raw EEG data are downsampled. Afterwards, via a bandpass filter between 1Hz and 75Hz, the EEG data are processed. 62 channels along with 5 sub-bands are used to extract the DE feature.\n\n3) Eye Movement feature extraction: Several features from various parameters, including pupil diameter, fixation, dispersion, saccade, blink, and event statistics are extracted as eye movement features through SMI ETG eye-tracking glasses. Finally, as eye movement features, a total number of 33 features are selected.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "D. Data Preparation",
      "text": "In this study, we combine eye movement, DE, and power spectrum features. All features data array and label array shape and contents are listed in Table  III . After applying FFT, we obtained 2,352,720 rows from 720 raw EEG files and 396 columns for the power spectrum feature. The columns denote the result of 66 channels and 6 sub-bands. The rows denote Prior to combining data, we are required to do padding in the eye movement and DE feature. For clarification, we demonstrate an example of the process of combined data. To begin with, we pass 1 1 1 20180804.cnt.npy (1st EEG raw file) into our declared FFT function whose shape is (66, 72000). After FFT,data shape becomes (1437,396) and saves in 1 1 1 FFT.npy file. The corresponding eye movement and DE file's (1 1 1 EYE.npy), (1 1 1 DE.npy) shape is  (18, 33)  and  (18, 310) . So, we do padding using repeat and insert function from the python numpy library to (1 1 1 EYE.npy) and (1 1 1 DE.npy) for increasing row numbers like 1 1 1 FFT.npy file. After padding, (1 1 1 EYE.npy),(1 1 1 DE.npy) files shape are (1437,33) and (1437,310) respectively. Finally, we stack the 3 files including 1 1 1 FFT.npy, (1 1 1 EYE.npy), (1 1 1 DE.npy) and shape becomes (1437,739). We follow the same procedure for each file and finally stack all the files.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Model Architecture",
      "text": "In the Literature review section, we have elaborately discussed various deep learning algorithms for signal recognition that are used by many researchers. Among those, ANN  [33] ,  [34] , CNN  [35] -  [37] , hybrid CNN-LSTM, LSTM, BDAE, and DCCA are frequently applied to the SEED-V dataset. However, in this study, we utilize a 1D-CNN model with two convolution layers. 1D-CNN has vast advantages including the ability to recognize complex features, parameter sharing, and the capability of extracting local patterns from sequential data. Our proposed 1D-CNN architecture is demonstrated in Fig.  4 . This proposed method comprises two convolution layers, two max-pooling layers, and one output layer after the dense layer.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experiment And Result",
      "text": "The data pre-processing, feature extraction, and experiment with a single modality including eye movement, DE, and power spectrum features have been done in Google Colab with the Keras library. Later, we do our experiment on a Linux Virtual Machine with combined data. We use both Keras and tensorflow version 2.6. We utilize FFT as a feature extraction technique. We divide 70 percent data as training and the rest 30 percent of data as testing. The array size of the train and test is indicated in Table  IV . We normalize Train and Test data using the StandardScaler() function from the Python sklearn library. Apart from this, the optimizer and the loss function of the proposed method were Adam, categorical crossentropy respectively. Besides, batch size is declared at 1000 as a hyperparameter. We classify five emotional states by applying the proposed model. And our proposed method obtains 99.80% test accuracy within 2 milliseconds. Apart from this, we evaluate the SEED-V dataset with individual modality. Table  V  illustrates the comparison between single-modality and multimodality. With the single eye movement feature, we obtain 61% testing accuracy whilst the power spectrum and DE features exhibit 73%, and 26% testing accuracy respectively. We obtain effective performance when we combine all the features. On top of that, to the best of our exploration, our method has prevailed the best performance among existing systems on the SEED-V dataset which is given in Table  VI . In addition, to evaluate the proposed model more accurately we generate a confusion matrix report and examine with precision, recall, and f1-score that are shown in Table VII and Fig.  5 . respectively.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "V. Conclusion",
      "text": "In this paper, we propose a subject-dependent end-to-end emotion recognition system using the 1D-CNN model which    enhances the classification performance. We decompose raw EEG signals into frequency bands utilizing FFT and extract the power spectrum feature which provides high-resolution information. We combine three features including the power spectrum, eye movement, and differential entropy. Afterward, we apply these features in 1D-CNN with two convolution layers. The classification result accounts for 99.80% which outperforms the performance of existing models utilizing the SEED-V dataset. However, a limitation of this research is to sole use of the SEED-V dataset whilst there are significant Hence, along with the SEED-V dataset, we will explore other datasets to enhance the generalization reliability. Furthermore, in terms of splitting the dataset into Train-Test, we will expand our study in the future to propose a subject-independent endto-end beneficial emotion recognition system.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Complete workflow of the proposed system on the SEED-V dataset.",
      "page": 2
    },
    {
      "caption": "Figure 1: At first, we explore",
      "page": 2
    },
    {
      "caption": "Figure 2: The investigator recorded EEG signals through a 62-channel",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of SEED-V Dataset.",
      "page": 3
    },
    {
      "caption": "Figure 3: 62-channel ESI NeuroScan System",
      "page": 3
    },
    {
      "caption": "Figure 4: This proposed method comprises two convolution layers, two",
      "page": 4
    },
    {
      "caption": "Figure 5: respectively.",
      "page": 4
    },
    {
      "caption": "Figure 4: Our proposed 1D-CNN model architecture",
      "page": 5
    },
    {
      "caption": "Figure 5: Confusion matrix",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Year": "2021",
          "Research": "Wei Liu [14]",
          "Features": "EEG, Eye Movement",
          "Related ML/DL algorithms": "Bimodal deep autoencoder\n(BDAE)"
        },
        {
          "Year": "2020",
          "Research": "Xun Wu [28]",
          "Features": "EEG,Eye Movement",
          "Related ML/DL algorithms": "Deep Canonical Correlation Analysis"
        },
        {
          "Year": "2020",
          "Research": "Yu-Ting Lan1 [29]",
          "Features": "EEG, EIG, and Eye Movement",
          "Related ML/DL algorithms": "Deep Generalized Canonical Correlation Analysis"
        },
        {
          "Year": "2019",
          "Research": "Luo [30]",
          "Features": "Differential Entropy, Eye Movement",
          "Related ML/DL algorithms": "Conditional Boundary Equilibrium GAN (cBEGAN)"
        },
        {
          "Year": "2019",
          "Research": "Jiang-Jian Guo [31]",
          "Features": "EEG, Eye Movement, Eye Image",
          "Related ML/DL algorithms": "Hybrid CNN-LSTM"
        },
        {
          "Year": "2019",
          "Research": "Tian-Hao Li\n[32]",
          "Features": "EEG, Eye Movement",
          "Related ML/DL algorithms": "SVM"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Feature": "Power Spectrum",
          "Shape": "Data:(2352720, 396)\nLabel:(2352720, 5)",
          "Contents": "Data:\n(rows = windows , cols = 66 channels x 6 bands)\nLabel:\n(rows = windows, cols = 5 emotions)"
        },
        {
          "Feature": "Differential\nEntropy",
          "Shape": "Data:\n(29168, 310)\nLabel:\n(29168, 5)",
          "Contents": "Data:\n(rows = 3 sessions data x 16 participants, cols = 62 channels x 5 bands)\nLabel:\n(rows = 3 sessions data x 16 participants, cols = 5 emotions)"
        },
        {
          "Feature": "Eye Movement",
          "Shape": "Data:\n(29168, 33)\nLabel:\n(29168, 5)",
          "Contents": "Data:(rows = 3 sessions data x 16 participants, cols = 33 features)\nLabel:\n(rows = 3 sessions data x 16 participants, cols = 5 emotions)"
        },
        {
          "Feature": "Combined Data",
          "Shape": "Data:\n(2352720, 739)\nLabel:(2352720, 5)",
          "Contents": "Data: rows=windows, cols = (66 channels x 6 bands)+(62 channels x 5 bands)+(33 eye features)\nLabel:\n(rows = windows, cols= 5 emotions)"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "A simple neural network with 3 hidden layers",
          "Features": "Differential Entropy",
          "Testing Accuracy": "26%"
        },
        {
          "Model": "A simple neural network with 3 hidden layers",
          "Features": "Eye Movement",
          "Testing Accuracy": "61%"
        },
        {
          "Model": "Proposed model",
          "Features": "Power Spectrum",
          "Testing Accuracy": "73%"
        },
        {
          "Model": "Proposed model",
          "Features": "Combined features (Power spectrum, Eye movement, Differ-\nential Entropy)",
          "Testing Accuracy": "99.80%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Year": "2019",
          "Model": "cBEGAN [30]",
          "Accuracy": "68.32%"
        },
        {
          "Year": "2019",
          "Model": "CNN and LSTM [31]",
          "Accuracy": "79.63%"
        },
        {
          "Year": "2020",
          "Model": "DGCCA [29]",
          "Accuracy": "82.11%"
        },
        {
          "Year": "2020",
          "Model": "SVM [28]",
          "Accuracy": "84.51%"
        },
        {
          "Year": "2021",
          "Model": "DCCA [14]",
          "Accuracy": "85.3%"
        },
        {
          "Year": "2023",
          "Model": "Proposed model",
          "Accuracy": "99.80%"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Class-\nLabel": "Disgust\n- 0",
          "Precision": "1.00",
          "Recall": "1.00",
          "F1-\nscore": "1.00",
          "Data\nPoints": "118,542"
        },
        {
          "Class-\nLabel": "Fear\n- 1",
          "Precision": "1.00",
          "Recall": "1.00",
          "F1-\nscore": "1.00",
          "Data\nPoints": "144,408"
        },
        {
          "Class-\nLabel": "Sad - 2",
          "Precision": "1.00",
          "Recall": "1.00",
          "F1-\nscore": "1.00",
          "Data\nPoints": "183,908"
        },
        {
          "Class-\nLabel": "Neutral\n- 3",
          "Precision": "1.00",
          "Recall": "1.00",
          "F1-\nscore": "1.00",
          "Data\nPoints": "141,745"
        },
        {
          "Class-\nLabel": "Happy - 4",
          "Precision": "1.00",
          "Recall": "1.00",
          "F1-\nscore": "1.00",
          "Data\nPoints": "117,213"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Joint EEG Feature Transfer and Semisupervised Cross-Subject Emotion Recognition",
      "authors": [
        "Y Peng",
        "H Liu",
        "W Kong",
        "F Nie",
        "B. -L Lu",
        "A Cichocki"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Industrial Informatics",
      "doi": "10.1109/TII.2022.3217120"
    },
    {
      "citation_id": "2",
      "title": "Spatial-Temporal Feature Fusion Neural Network for EEG-Based Emotion Recognition",
      "authors": [
        "Z Wang",
        "Y Wang",
        "J Zhang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement",
      "doi": "10.1109/TIM.2022.3165280"
    },
    {
      "citation_id": "3",
      "title": "The role of facial movements in emotion recognition",
      "authors": [
        "Eva Krumhuber"
      ],
      "year": "2023",
      "venue": "Nature Reviews Psychology"
    },
    {
      "citation_id": "4",
      "title": "Feature extraction to identify depression and anxiety based on EEG",
      "authors": [
        "Laura Minkowski",
        "Kristiina Valter Mai",
        "Dharmendra Gurve"
      ],
      "year": "2021",
      "venue": "2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society"
    },
    {
      "citation_id": "5",
      "title": "Classification of EEG signals from driving fatigue by image-based deep recurrent neural networks",
      "authors": [
        "Fang",
        "Zhaoguo"
      ],
      "year": "2022",
      "venue": "2022 IEEE International Conference on Mechatronics and Automation (ICMA)"
    },
    {
      "citation_id": "6",
      "title": "Emognition dataset: emotion recognition with self-reports, facial expressions, and physiology using wearables",
      "authors": [
        "Saganowski",
        "Stanisław"
      ],
      "year": "2022",
      "venue": "Scientific data"
    },
    {
      "citation_id": "7",
      "title": "A convolution neural network based emotion recognition system using multimodal physiological signals",
      "authors": [
        "Cheng-Jie Yang"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Consumer Electronics-Taiwan"
    },
    {
      "citation_id": "8",
      "title": "Sentiments analysis of fMRI using automatically generated stimuli labels under naturalistic paradigm",
      "authors": [
        "Rimsha Mahrukh",
        "Sadia Shakil",
        "Aamir Saeed"
      ],
      "year": "2023",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "9",
      "title": "Open multi-session and multi-task EEG cognitive Dataset for passive brain-computer Interface Applications",
      "authors": [
        "Marcel Hinss"
      ],
      "year": "2023",
      "venue": "Scientific Data"
    },
    {
      "citation_id": "10",
      "title": "Deep emotional arousal network for multimodal sentiment analysis and emotion recognition",
      "authors": [
        "Feng Zhang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "11",
      "title": "Evaluating the Effectiveness of Classification Algorithms for EEG Sentiment Analysis",
      "authors": [
        "Akter",
        "Sumya"
      ],
      "year": "2023",
      "venue": "Sentiment Analysis and Deep Learning: Proceedings of ICSADL 2022"
    },
    {
      "citation_id": "12",
      "title": "Self-Supervised EEG Emotion Recognition Models Based on CNN",
      "authors": [
        "Xingyi Wang"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "13",
      "title": "Human Emotion Recognition Models Using Machine Learning Techniques",
      "authors": [
        "Aftab Alam",
        "Shabana Urooj",
        "Abdul Quaiyum"
      ],
      "year": "2023",
      "venue": "2023 International Conference on Recent Advances in Electrical"
    },
    {
      "citation_id": "14",
      "title": "Comparing recognition performance and robustness of multimodal deep learning models for multimodal emotion recognition",
      "authors": [
        "Wei Liu"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "15",
      "title": "Multimodal emotion recognition based on facial expressions, speech, and EEG",
      "authors": [
        "Jiahui Pan"
      ],
      "year": "2023",
      "venue": "IEEE Open Journal of Engineering in Medicine and Biology"
    },
    {
      "citation_id": "16",
      "title": "Emotion Analysis on EEG Signal Using Machine Learning and Neural Network",
      "authors": [
        "S Ahmed",
        "Eshaan Tanzim"
      ],
      "year": "2023",
      "venue": "Emotion Analysis on EEG Signal Using Machine Learning and Neural Network",
      "arxiv": "arXiv:2307.05375"
    },
    {
      "citation_id": "17",
      "title": "Ensemble machine learning-based affective computing for emotion recognition using dualdecomposed EEG signals",
      "authors": [
        "Kranti Kamble",
        "Joydeep Sengupta"
      ],
      "year": "2021",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition using multi-modal data and machine learning techniques: A tutorial and review",
      "authors": [
        "Jianhua Zhang"
      ],
      "year": "2020",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "19",
      "title": "Deep learning model with adaptive regularization for EEG-based emotion recognition using temporal and frequency features",
      "authors": [
        "Alireza Samavat"
      ],
      "year": "2022",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "20",
      "title": "Human attention recognition with machine learning from brain-EEG signals",
      "authors": [
        "Reshad Hassan"
      ],
      "year": "2020",
      "venue": "Biomedical Engineering, Healthcare and Sustainability"
    },
    {
      "citation_id": "21",
      "title": "Fine-Grained Emotion Recognition from EEG Signal Using Fast Fourier Transformation and CNN",
      "authors": [
        "M Hasan",
        "S Rokhshana-Nishat-Anzum",
        "T Yasmin",
        "Pias"
      ],
      "year": "2021",
      "venue": "2021 Joint 10th International Conference on Informatics, Electronics & Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision & Pattern Recognition (icIVPR)",
      "doi": "10.1109/ICIEVi-cIVPR52578.2021.9564204"
    },
    {
      "citation_id": "22",
      "title": "Deep Convolutional Neural Network Based Automatic COVID-19 Detection from Chest X-ray Images",
      "authors": [
        "Barman",
        "Shawon"
      ],
      "year": "2022",
      "venue": "2022 4th International Conference on Electrical"
    },
    {
      "citation_id": "23",
      "title": "Subject-independent emotion recognition of EEG signals based on dynamic empirical convolutional neural network",
      "authors": [
        "Shuaiqi Liu"
      ],
      "year": "2020",
      "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics"
    },
    {
      "citation_id": "24",
      "title": "M1M2: deep-learning-based real-time emotion recognition from neural activity",
      "authors": [
        "Akter",
        "Sumya"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "25",
      "title": "Emotion Recognition via Multi-Scale Feature Fusion Network and Attention Mechanism",
      "authors": [
        "Yiye Jiang"
      ],
      "year": "2023",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "26",
      "title": "EEG-based emotion recognition using spatial-temporal graph convolutional LSTM with attention mechanism",
      "authors": [
        "Lin Feng"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "27",
      "title": "Emotion Recognition based on fusion of multimodal physiological signals using LSTM and GRU",
      "authors": [
        "N Priyadarshini",
        "J Aravinth"
      ],
      "year": "2023",
      "venue": "2023 Third International Conference on Secure Cyber Computing and Communication (ICSCCC)"
    },
    {
      "citation_id": "28",
      "title": "Investigating EEG-based functional connectivity patterns for multimodal emotion recognition",
      "authors": [
        "Xun Wu"
      ],
      "year": "2022",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "29",
      "title": "Multimodal emotion recognition using deep generalized canonical correlation analysis with an attention mechanism",
      "authors": [
        "Yu Lan",
        "Wei Ting",
        "Bao-Liang Liu",
        "Lu"
      ],
      "year": "2020",
      "venue": "2020 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "30",
      "title": "A GAN-based data augmentation method for multimodal emotion recognition",
      "authors": [
        "Yun Luo",
        "Li-Zhen Zhu",
        "Bao-Liang Lu"
      ],
      "year": "2019",
      "venue": "Advances in Neural Networks-ISNN 2019: 16th International Symposium on Neural Networks, ISNN 2019"
    },
    {
      "citation_id": "31",
      "title": "Multimodal emotion recognition from eye image, eye movement and EEG using deep neural networks",
      "authors": [
        "Guo",
        "Jiang-Jian"
      ],
      "year": "2019",
      "venue": "st annual international conference of the IEEE engineering in medicine and biology society"
    },
    {
      "citation_id": "32",
      "title": "Classification of five emotions from EEG and eye movement signals: Discrimination ability and stability over time",
      "authors": [
        "Tian- Li",
        "Hao"
      ],
      "year": "2019",
      "venue": "Neural Engineering"
    },
    {
      "citation_id": "33",
      "title": "Vehicle Recognition Via Sensor Data From Smart Devices",
      "authors": [
        "T Pias",
        "D Eisenberg",
        "M Islam"
      ],
      "year": "2019",
      "venue": "2019 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE)"
    },
    {
      "citation_id": "34",
      "title": "Gender Recognition by Monitoring Walking Patterns via Smartwatch Sensors",
      "authors": [
        "T Pias",
        "R Kabir",
        "D Eisenberg",
        "N Ahmed",
        "M Islam"
      ],
      "year": "2019",
      "venue": "2019 IEEE Eurasia Conference on IOT, Communication and Engineering (ECICE)"
    },
    {
      "citation_id": "35",
      "title": "Accuracy improvement of vehicle recognition by using smart device sensors",
      "authors": [
        "T Pias",
        "D Eisenberg",
        "J Fernandez"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "36",
      "title": "ECG Arrhythmia Classification Using 1D CNN Leveraging the Resampling Technique and Gaussian Mixture Model",
      "authors": [
        "M Apu",
        "F Akter",
        "M Lubna",
        "T Helaly",
        "T Pias"
      ],
      "year": "2021",
      "venue": "2021 Joint 10th International Conference on Informatics, Electronics Vision (ICIEV) and 2021 5th International Conference on Imaging, Vision and Pattern Recognition (icIVPR)"
    },
    {
      "citation_id": "37",
      "title": "Emotion recognition from brain wave using multitask machine learning leveraging residual connections",
      "authors": [
        "R Prodhan",
        "S Akter",
        "M Mujib",
        "M Adnan",
        "T Pias"
      ],
      "year": "2023",
      "venue": "Emotion recognition from brain wave using multitask machine learning leveraging residual connections"
    }
  ]
}