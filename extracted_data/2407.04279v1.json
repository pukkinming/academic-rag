{
  "paper_id": "2407.04279v1",
  "title": "Bioserc: Integrating Biography Speakers Supported By Llms For Erc Tasks",
  "published": "2024-07-05T06:25:34Z",
  "authors": [
    "Jieying Xue",
    "Minh Phuong Nguyen",
    "Blake Matheny",
    "Le Minh Nguyen"
  ],
  "keywords": [
    "speaker modeling",
    "biography of speaker in conversation",
    "emotion recognition in conversation",
    "large language models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "In the Emotion Recognition in Conversation task, recent investigations have utilized attention mechanisms exploring relationships among utterances from intra-and inter-speakers for modeling emotional interaction between them. However, attributes such as speaker personality traits remain unexplored and present challenges in terms of their applicability to other tasks or compatibility with diverse model architectures. Therefore, this work introduces a novel framework named BiosERC, which investigates speaker characteristics in a conversation. By employing Large Language Models (LLMs), we extract the \"biographical information\" of the speaker within a conversation as supplementary knowledge injected into the model to classify emotional labels for each utterance. Our proposed method achieved state-of-the-art (SOTA) results on three famous benchmark datasets: IEMOCAP, MELD, and EmoryNLP, demonstrating the effectiveness and generalization of our model and showcasing its potential for adaptation to various conversation analysis tasks. Our source code is available at https://github.com/yingjie 7/BiosERC.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition in conversation (ERC) is a pivotal research topic that has garnered growing attention due to its extensive range of applications  [1, 2] . In ERC tasks, the input text frequently consists of transcribed spoken dialogues from a speech recognition system, featuring colloquial or truncated statements that lack standardized grammar, thereby complicating emotional recognition in the dialogue. Unlike the traditional non-conversation sentiment analysis task, ERC emphasizes some of the many factors that influence ERC tasks, including contextual and speaker-specific information  [1] .\n\nTherefore, recent approaches have inclined toward encoding acoustic features  [3, 4]  or contextual information  [5, 6, 7]  to enrich utterance vector representation. On the other hand, numerous previous works have typically utilized GRU  [8, 9, 10] , GNN  [11, 2] , or self-attention network  [1, 12, 13]  to encode richer speaker-specific information, including intra-and inter-speaker features. However, this latent information is predominantly learned from relationships among utterances. It poses challenges for validating its effectiveness and applying it to alternative tasks, and is problematic for other model architectures. Addition-I can't believe that Ross is gone. It is just so sad.\n\n[sadness] I didn't know Ross and you were so close.\n\n[surprise] I thought so many times about calling him ...  [  (Conversation)\n\nSPEAKER_A seem to be a very emotional person, as evidenced by their sadness and regret over the loss of someone they barely ... SPEAKER_B seems to be a supportive and empathetic listener ... SPEAKER_C seems to be a bit of a romantic, and express excitement at the idea of Kori Weston having a crush on him... speaker biography Emotion Recognition in Conversation I'm sure that would mean a lot to him.\n\n[neutral] #4\n\nWe weren't but we had one class together. He was such a great guy ...\n\n[sadness] #3\n\nFig.  1 . Overview of our BiosERC framework ally, speaker characteristics as a crucial and foundational feature in ERC tasks has not been comprehensively explored. We posit that within a dialogue, an individual's character can significantly influence their manner of emotional expression and habitual vocabulary selection, leading to varying emotions for the same statement even when articulated by different speakers. Comprehension of interlocutors' personality traits can thus facilitate accurately discerning their emotional inclinations within the discourse.\n\nTo tackle the aforementioned challenge, we propose BiosERC, a novel method designed to discover speakers' personality information to enhance ERC systems. In contrast to previous methodologies relying on GRU  [9, 10]  or speaker-based masked attention mechanisms  [1, 12, 13]  to capture emotional expression features of different speakers, BiosERC stands out by precisely extracting individual personalities of speakers within dialogues (Figure  1 ). This uniqueness empowers the model to intricately comprehend character traits and encapsulate events of emotional transitions occurring within the characters. Moreover, our mechanism for extracting speaker characteristics is explicit and more amenable to verification and adaptation for application to various conversation analysis tasks.\n\nSpecifically, BiosERC utilizes LLMs with a prompting technique  [14, 15]  to extract descriptions of interlocutor features as supplementary knowledge, which are then injected into the emotion recognition process within conversations. As shown in Figure  1 , this conversation involves three distinct speakers, each presenting unique perspectives and exhibiting markedly different emotional states. The speaker description facilitates the model's thorough understanding of each speaker's role within a conversation. Particularly, SPEAKER A is experiencing sadness and regret (as mentioned in the speaker description), resulting in expressions predominantly filled with sadness. SPEAKER B appears to be a supportive and empathetic listener, with limited involvement in the conversation, and reacts through SPEAKER A's utterances. Meanwhile, SPEAKER C responds with excitement upon hearing their conversation. Intuitively, the integration of biographical data plays an important role in enriching the emotional background of each speaker in conversations, and holds the potential for more precise and comprehensive emotional recognition, especially in complex dialogues.\n\nWe carry out experiments on three benchmark datasets, including IEMO-CAP, MELD, and EmoryNLP. The experimental results demonstrate that our method achieves SOTA performance, which indicates the effectiveness of our proposed model. Furthermore, our proposed mechanism, which uses a prompting technique for LLMs to extract the speakers' biographical information, shows the potential to adapt to various conversation-level tasks such as opinion analysis, recommendation, and others.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Emotion Recognition in Conversation. In contrast to the conventional non-conversation sentiment analysis task, ERC demands a greater reliance on contextual and speaker-specific information for its support. For the purpose of modeling the conversational context, numerous studies employ Recurrent Neural Networks (RNNs)  [16, 17]  or Graph Convolution Network (GCN)  [1, 18]  to explore the hidden relationships between utterances. Moreover, the incorporation of contextual information and external knowledge into utterance vector representations has been notably achieved in recent works  [12, 6, 16, 19]  through the utilization of self-attention mechanisms and pre-trained Language Models (LM)  [20, 21] . In the recent success of LLMs on various NLP tasks, InstructERC  [22]  is proposed to utilize the instruction prompting technique and fine-tune the LLM model for ERC tasks. MKFM framework  [23]  proposed the utilization of diverse supplementary knowledge information (e.g., emotional cause, topics) by ChatGPT service to inject into a graph-based model. In comparison, our work focuses on modeling speaker characteristics, a fundamental information which can be extracted by open-source LLMs (e.g, LLama-2). In addition, we also prove our proposed mechanism worked effectively when fine-tuning on both popular architectures: BERT and transformer-based decoder-only LLM. Speaker-based ERC. Because of the significant impact of speakers on ERC, researchers have placed emphasis on speaker modeling. DialogueRNN  [8]  and COSMIC  [16]  leverage Gated Recurrent Units (GRU) for the modeling of speakerspecific semantic context. Some researches  [11, 2]  treat conversations as graphs while incorporating prior speaker information as distinct relationships between utterances, or considers speakers as nodes within the graph. HiTrans  [5]  exploits an auxiliary task to classify whether two utterances belong to the same speaker to make the model speaker-sensitive. S+PAGE  [24]  employs a two-stream conver-sation Transformer architecture to extract both self and inter-speaker contextual features. However, the majority of prior research has predominantly concentrated on modeling individual speaker utterances or interactions among different speakers, with particular attention given to the intra-and inter-speaker aspects for the extraction of speaker-based information  [6, 13] . Regrettably, limited emphasis has been placed on exploring speaker characteristics, which constitute critical and foundational elements of conversational information. Therefore, we propose a novel method named BiosERC, which employs external tools to extract speaker characteristics and inject them into the process of emotion recognition within conversations.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Methodology",
      "text": "This section introduces our baseline model architecture for the ERC task, which utilizes intra-and inter-speaker information following current SOTA methods  [24, 6, 10] , and our proposed method BiosERC, which incorporates the biography of the speakers into an ERC model. Formally, we define a conversation as: C = {u i } 0≤i<|C| , where each individual utterance u i is articulated by speaker p(u i ) ∈ S, with S = {s j } 0≤j<|S| representing the set of speakers in the conversation. Here, p denotes a mapping function that associates utterances with their respective speakers.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Intra-Inter Erc (Baseline)",
      "text": "Based on recent SOTA methods in the ERC task  [24, 10, 13] , we implement our baseline model consisting of three principal components: utterance vector representation, context modeling, and an emotion classification layer. Utterance Vector Representation. To enrich meaning representations, we follow an approach that mixes the surrounding utterances within a fixed-window size  [25, 9, 19, 13] . Particularly, to encode a sentence u i , the text input is combined by surrounding the utterance according to the following template: \"[cls], u i-w , .., </s>, u i , </s>, ... u i+w \", where w is the local contextual window size hyperparameter. The utterance vector is computed by aggregating the respective word vectors following  [13] :\n\nwhere h words of ui denotes the word vectors selected from h words at the positions of the utterance u i ; h utt is all utterance vectors in a conversation; and W * refers to learnable weights.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Context Modeling.",
      "text": "Utterance vectors are integrated contextual information of whole conversation by attention mechanism:\n\nwhere H is the number of heads in the MultiHead attention layer; q t , k t , v t are utterance vectors in various semantic space (dimension size d t ). In detail, following  [6, 13] , we construct the relation matrices (M ) for modeling relationship among utterances, where\n\nFor the baseline model, we implement three different relationships: global context (all utterance pairs are connected), intra-speaker (only utterance pairs of the same speaker are connected), and inter-speaker (only utterance pairs of the different speaker are connected). Consequently, we acquire three new hidden states (from Equation  6 ) h contxt , h intra , h inter feed-forward to the Classification component.\n\nClassification. This component aims to integrate all the hidden features of utterances to classify the emotion label.\n\nThen, the emotion vector (e o i ) is used to compute the loss via Cross-Entropy function and is trained based on the gold emotional label of the i-th utterance.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Bios Erc",
      "text": "In this section, we describe the process of generating the speaker's biography and present our BiosERC framework, leveraging two popular pre-trained LM as backbones: a BERT-based model  [21]  (e.g., RoBERTa) and a transformer-based decoder-only LLM model  [15]  (e.g., Llama-2). Notably, we also introduce an effective mechanism using the biography of speakers incorporating fine-tuning a LLM-based  [26]  with the prompting technique.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Biography Of Speaker",
      "text": "In this part, we introduce a mechanism using the prompting technique for the LLMs to generate the description (d j ) for the respective speaker (u j ). Given a conversation C, the output of this step is the biography (description) of all speakers in a conversation B = {d j } 0≤j<|S| .\n\nLLMs refers to large language models such as Llama2  [15] , which can generalize a speaker's biography based on their conversation. The prompting function is a template containing two conversation instances (C) and speaker identification (s j ) to exploit the knowledge of the LLMs (Table  1 ). To avoid long plain text descriptions, we instruct the LLMs to limit the output by adding a \"note\" concerning the length of the prompting template. Consequently, we obtain additional data about the persona of the speakers in each conversation (B), which is utilized for speaker modeling in the subsequent step.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Bert-Based Bioserc Architecture",
      "text": "Firstly, we encode the speaker's description using a pre-trained language model to acquire hidden vector representation (h desc j\n\n).\n\nwhere j is the speaker index in the set of speakers in a conversation, 0 ≤ j < |S|.\n\nOur proposed method, BiosERC, extends the baseline model and redefines the speaker's hidden vector representation (h speaker i in Equation  7 ) (Figure  2 ). This architecture is designed with a straightforward target that injects the personality information of each speaker into their corresponding utterances by a multi-layer perceptron network. Then, the speaker information in Equation 7 is replaced by:\n\nwhere p(u i ) denotes the corresponding speaker of utterance u i . Through this mechanism, all the utterances from the same speaker are shared in the unified speaker vector representation, while the weights are updated in the training process. Finally, the utterance vector is fused with the speaker vector which supports emotional classification. BiosERC -biography injected by attention mechanism. We consider a variant of our BiosERC model, which is engineered to dynamically incorporate the speaker's information into each utterance via the attention mechanism. integrated to enrich the utterance vector representation.\n\nWe first compute a fusion vector (h f usion ) between the utterance and respective speaker description vectors. Then we collect all the speaker description vectors (h desc ) and use the attention mechanism to model the relationship between the utterance and all speakers in a conversation. Finally, the speaker features are embedded in this vector, h speaker i , and are replaced using Equation 7 in the baseline system.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Llm-Based Bioserc + Instruction Fine-Tuning (Ft Llm)",
      "text": "Since the robust natural language understanding capabilities of LLMs  [15] , we provide the speaker description as part of the text prompting input for the model (highlighted in blue in Table  2 ) instead of modifying model architecture. We follow the instruction fine-tuning approach  [27] , with causal language modeling objective to train an LLM to generate emotional label text (highlighted in red in Table  2 ):\n\nwhere x, z is a sequence of tokens and the token's index in prompting input (Table  2 ), respectively. Additionally, we utilize LoRA  [28] , a lightweight training technique, to reduce the number of trainable parameters. The instruction finetuned LLM learned the distribution of emotional labels given prompting input (x). During the inferring phase, the emotional label (e i ) which is omitted from prompting input, is left to be generated by the fine-tuned LLM.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Experimental Setting",
      "text": "Datasets We conducted evaluations on three ERC benchmark datasets in text-only version: IEMOCAP  [29] , involving daily conversations between pairs with ten different speakers; MELD  [30] , derived from TV shows and featuring multiparty conversations; EmoryNLP  [31] , another multiparty daily dialogue dataset sourced from TV shows. The statistical information of these datasets is shown in Table  3 . In accordance with prior works  [16, 12] , we employed the Weighted-F1 score as the evaluation metric to maintain compatibility. Implementation Details Since the recent successful applications and advancing capabilities of pre-trained LLMs  [15, 14] , we leverage LLama-2 model to procure personality descriptions for each participant in the conversation. Specifically, we verify the effectiveness of speaker description information on two aforementioned pre-trained LMs: the BERT-based model with roberta-large and the transformer-based decoder-only LLM model with Llama-2-13b. The best model is determined based on the development set of each dataset and employed to evaluate the test set. For fine-tuning BERT-based BiosERC (section 3.4), the hyper-parameters were selected as follows: the learning rate is selected from {1e -5 ; 5e -6 }; the dropout value is 0.2, and number epochs is 30; and the local context window size (w) is chosen in {2, 4}; we report the average scores obtained across 10 independent runs. For fine-tuning LLM-based BiosERC (section 3.5), learning rate is selected from {2e -4 ; 3e -4 }, number epochs is 3; we report the average scores obtained across 5 independent runs because of the computation cost. All the source code of this project is published at MASKED LINK.\n\n5 Results and Analysis",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Main Results",
      "text": "Our approach demonstrated competitive performance compared to recent SOTA methods on three famous benchmark datasets (Table  4 ) on both two architectures BERT-based and transformer-based decoder-only LLM model. In com- parison with the previous speaker-based methods (SGED + DAG-ERC  [10] , S+PAGE  [24]  and DialogueEIN  [6] ) experimental results demonstrated the effectiveness of our proposed approach and further affirm that speaker modeling by speaker descriptions are superior to the information offered by intra-and inter-speaker contexts. In addition, our BiosERC model achieved significant differences with the baseline system on both the EmoryNLP and MELD datasets, which is shown clearly in Figure  3 . Because the MELD and EmoryNLP are multiparty conversation datasets (the average number of interlocutors are 2.72 and 3.34, respectively), the emotions are influenced more by different speaker personalities in a conversation than the IEMOCAP dataset.\n\nIn the previous method of fine-tuning an LLM, InstructERC  [22]  considers speaker identifier as an auxiliary task, requiring two-stage training, which is more time-consuming than ours in the training process. Besides, our proposed method uses speaker descriptions generated by LLM in natural language, which can be easier incorporated with humans using our system for customization (e.g., customer support staff can directly provide or modify characteristics generated by LLM of their customers). Similar to BERT-based BiosERC, among three datasets, the LLM-based BiosERC shows the strengthens of multi-party datasets (more than two speakers in each conversation), EmoryNLP and MELD. By finetuning an LLM, Llama-2-13b, the performance of our BiosERC increased by 1-4% weighted F1 scores compared to BERT-based models and achieved new SOTA performance on EmoryNLP and MELD datasets. Besides, since utilizing a lightweight training technique, LoRA  [28] , the number of training parameters in LLM-based BiosERC was smaller than BERT-based BiosERC (only fine-tuned on two last layers) which proved the potential of LLM-based BiosERC in the real application.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Ablation Study",
      "text": "We conducted an ablation study to evaluate the effectiveness of integrating speaker biographies into the broader system encompassing various aspects. BiosERC architecture. As shown in Table  5 , it is apparent that our BERTbased BiosERC (row 3), which incorporates the speaker's descriptions, exhibits significant advantages in F1 score, outperforming the baseline system that relies on intra/inter-speaker relationships. Besides, by using the attention mechanism to encode the speaker's biography (row 2), BiosERC achieved high performance and it also clearly outperformed the baseline model. Moreover, in the setting of BiosERC +fine-tuning LLM (row 8), when removing the speaker description (the blue part in Table  2 ) from the input prompting (row 6), the performance significantly decreased by 1.05 F1 score. By fine-tuning the different LLM models, Llama-2-13b and Llama-2-7b, the performance is slightly decreased with 0.52 F1 score (rows 7, 8). These results proved the importance of the speaker's biography information and the efficacy of our proposed approach for speaker modeling. Speaker biographies. We explored various currently popular LLMs for generating speaker biographies, including LLama-2-chat-70b, Llama-2-chat-7b  [15] , and vicuna-33b-v1.3  [34] . Among these, LLama-2-chat-70b yielded the best outcomes. Based on observation, we found that the Vicuna model failed to provide speaker descriptions in some \"extra difficult\" cases, such as when the conversation length is too short (e.g., less than three utterances) or when the specific speaker has extremely short utterances (e.g., Hmm). These solid improvements worked on the diverse biographies generated by various LLMs underscore the versatility and effectiveness of extracting \"speaker biographies\", demonstrating that the LLMs framework can be highly beneficial for biography generation and assisting with ERC tasks.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Conversation Length",
      "text": "We analyzed the MELD development set to evaluate the impact of conversation length on the performance as depicted in Figure  4 . Overall, our method outperforms intra-and inter-speaker methods across conversations of varying lengths.\n\nIt is worth noting that the performance of short dialogues (conversation length less than 15) improves significantly more than that of long dialogues. These results also proved the importance of \"speaker characteristic\" in short conversations lacking contextual information. When contextual information is limited, speaker characteristics are based on the speaker's lexical choices, which contain explicit or implicit meaning in the sentence. An LLM can extract the speaker's characteristics by recognizing the explicit or implicit meaning conveyed in these statements. In addition, MELD is a multiparty dataset, which contains many conversations involving more than three speakers. Based on our observations on the improvement examples, \"speaker characteristic\" is especially important in short conversations which lack much contextual information.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Case Study",
      "text": "Our model enhances emotion recognition accuracy, even in short conversations with limited contextual information. As show in Table  6 , conversation 1041 is a short dialogue consisting of only five sentences. Our model perceives two speaker descriptions, these descriptions facilitate a more accurate identification of SPEAKER 0's discourse, leaning towards positivity rather than anger. In addition, our architecture shows an improved capacity in predicting emotions in shorter sentences through the utilization of speaker description, particularly in cases where traditional models struggle due to the minimal information contained in expressions such as \"Yeah\", or \"Okay\". Additionally, our approach adeptly copes with scenarios where the error rate is high at the beginning of the conversation as show in Table  6  (u 0 , u 1 , u 2 in conversation 1061). Because contextual and speaker information is lacking at the outset of the conversation, the baseline model consistently produces incorrect initial sentences. However, with the assistance of our \"speaker description\", it delivers a better performance from the beginning of the dialogue. Experimental results prove the effectiveness and versatility of our approach across diverse conversations, including those with complex contextual information.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Limitation",
      "text": "In this work, we introduce a novel method for modeling speaker characteristics based on biographical information of interlocutors in a conversation, generated by a large language model (LLM). In terms of computation time, our BiosERC method requires additional computing resources for the inference of the LLM compared to methods like Intra-inter ERC, which utilize hidden speaker identity information. Additionally, for the scope of this paper, we have not addressed issues related to human privacy data. In realistic applications, access to conversation history should be granted and clarified by the data owner. However, we believe that, with appropriate agreements to protect users' privacy data, it is possible to obtain this permission. d0 SPEAKER 0 seems to be a very inquisitive and curious person. ... SPEAKER 0 appears to be quite blunt and direct in his communication style, not mincing words or sugarcoating his thoughts. d1 SPEAKER 1 seems to be a humorous and light-hearted person. ... SPEAKER 1 shares that they have only been with one person in their whole life, and this is met with surprise and disbelief by the other ... d2 SPEAKER 2 seems to be a humorous and light-hearted person... SPEAKER 2 is someone who enjoys having fun and is not afraid to poke fun at themselves or others...",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "In conclusion, we proposed a novel mechanism incorporating speakers' characteristics into the ERC task, which has not been fully developed in prior research. We improve the performance of the ERC task by investigating the influence of the personality of interlocutors on emotions, considering this external knowledge as a unique feature. Our experiments on three benchmark datasets consistently yielded SOTA or competitive results, thereby substantiating the effectiveness of our proposed method. Furthermore, our model is straightforward yet highly adaptable, thus enabling its applicability to a wide range of conversation analysis tasks.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our BiosERC framework",
      "page": 2
    },
    {
      "caption": "Figure 1: ). This uniqueness empowers the",
      "page": 2
    },
    {
      "caption": "Figure 1: , this conversation involves three distinct speakers, each pre-",
      "page": 2
    },
    {
      "caption": "Figure 2: Overview of our BiosERC model architecture.",
      "page": 7
    },
    {
      "caption": "Figure 3: Because the MELD and EmoryNLP are",
      "page": 9
    },
    {
      "caption": "Figure 3: Performance comparison between our BERT-based BiosERC and the baseline",
      "page": 10
    },
    {
      "caption": "Figure 4: Overall, our method outper-",
      "page": 11
    },
    {
      "caption": "Figure 4: Performance comparison respect to length of conversation (number of utter-",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Japan Advanced Institute of Science and Technology": "{xuejieying,phuongnm,matheny.blake,nguyenml}@jaist.ac.jp"
        },
        {
          "Japan Advanced Institute of Science and Technology": "Abstract.\nIn the Emotion Recognition in Conversation task, recent in-"
        },
        {
          "Japan Advanced Institute of Science and Technology": "vestigations have utilized attention mechanisms exploring relationships"
        },
        {
          "Japan Advanced Institute of Science and Technology": "among utterances from intra- and inter-speakers for modeling emotional"
        },
        {
          "Japan Advanced Institute of Science and Technology": "interaction between them. However, attributes such as speaker person-"
        },
        {
          "Japan Advanced Institute of Science and Technology": "ality traits\nremain unexplored and present challenges\nin terms of\ntheir"
        },
        {
          "Japan Advanced Institute of Science and Technology": "applicability to other tasks or compatibility with diverse model architec-"
        },
        {
          "Japan Advanced Institute of Science and Technology": "tures. Therefore, this work introduces a novel framework named BiosERC,"
        },
        {
          "Japan Advanced Institute of Science and Technology": "which investigates speaker characteristics in a conversation. By employ-"
        },
        {
          "Japan Advanced Institute of Science and Technology": "ing Large Language Models (LLMs), we extract the “biographical\ninfor-"
        },
        {
          "Japan Advanced Institute of Science and Technology": "mation” of the speaker within a conversation as supplementary knowl-"
        },
        {
          "Japan Advanced Institute of Science and Technology": "edge injected into the model to classify emotional\nlabels for each utter-"
        },
        {
          "Japan Advanced Institute of Science and Technology": "ance. Our proposed method achieved state-of-the-art (SOTA) results on"
        },
        {
          "Japan Advanced Institute of Science and Technology": "three famous benchmark datasets: IEMOCAP, MELD, and EmoryNLP,"
        },
        {
          "Japan Advanced Institute of Science and Technology": "demonstrating\nthe\neffectiveness\nand generalization of\nour model\nand"
        },
        {
          "Japan Advanced Institute of Science and Technology": "showcasing its potential\nfor adaptation to various\nconversation analy-"
        },
        {
          "Japan Advanced Institute of Science and Technology": "sis tasks. Our source code is available at https://github.com/yingjie"
        },
        {
          "Japan Advanced Institute of Science and Technology": "7/BiosERC."
        },
        {
          "Japan Advanced Institute of Science and Technology": "Keywords:\nspeaker modeling · biography of speaker\nin conversation ·"
        },
        {
          "Japan Advanced Institute of Science and Technology": "emotion recognition in conversation ·\nlarge language models"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotion recognition in conversation ·\nlarge language models": "1\nIntroduction"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "Emotion recognition in conversation (ERC) is a pivotal research topic that has"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "garnered growing attention due to its extensive range of applications\n[1,2].\nIn"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "ERC tasks,\nthe input\ntext\nfrequently consists of\ntranscribed spoken dialogues"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "from a speech recognition system,\nfeaturing colloquial or truncated statements"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "that lack standardized grammar, thereby complicating emotional recognition in"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "the dialogue. Unlike the traditional non-conversation sentiment analysis\ntask,"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "ERC emphasizes some of the many factors that influence ERC tasks,\nincluding"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "contextual and speaker-specific information [1]."
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "Therefore,\nrecent approaches have\ninclined toward encoding acoustic\nfea-"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "tures\n[3,4] or\ncontextual\ninformation [5,6,7]\nto enrich utterance vector\nrepre-"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "sentation. On the other hand, numerous previous works have typically utilized"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "GRU [8,9,10], GNN [11,2], or\nself-attention network [1,12,13]\nto encode richer"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "speaker-specific information,\nincluding intra- and inter-speaker\nfeatures. How-"
        },
        {
          "emotion recognition in conversation ·\nlarge language models": "ever, this latent information is predominantly learned from relationships among"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "empathetic": ""
        },
        {
          "empathetic": "listener ..."
        },
        {
          "empathetic": ""
        },
        {
          "empathetic": ""
        },
        {
          "empathetic": "I didn't know Ross and you were so"
        },
        {
          "empathetic": "close."
        },
        {
          "empathetic": ""
        },
        {
          "empathetic": "[surprise]"
        },
        {
          "empathetic": "I'm sure that would mean a lot to him."
        },
        {
          "empathetic": "[neutral]"
        },
        {
          "empathetic": ""
        },
        {
          "empathetic": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "#1\n#3\n#6": "(Conversation)"
        },
        {
          "#1\n#3\n#6": "Speaker Biography"
        },
        {
          "#1\n#3\n#6": "time\ngenerated by LLMs"
        },
        {
          "#1\n#3\n#6": "#2\n#4\n#5\n#7\n#8"
        },
        {
          "#1\n#3\n#6": "speaker biography"
        },
        {
          "#1\n#3\n#6": "SPEAKER_C seems"
        },
        {
          "#1\n#3\n#6": "SPEAKER_B\n           SPEAKER_A seem to be"
        },
        {
          "#1\n#3\n#6": "to be a bit of a"
        },
        {
          "#1\n#3\n#6": "seems to be\n           a very emotional person,"
        },
        {
          "#1\n#3\n#6": "romantic, and"
        },
        {
          "#1\n#3\n#6": "a supportive and\n  as evidenced by their sadness"
        },
        {
          "#1\n#3\n#6": "express excitement at the"
        },
        {
          "#1\n#3\n#6": "empathetic\nand regret over the loss of"
        },
        {
          "#1\n#3\n#6": "idea of Kori Weston having a"
        },
        {
          "#1\n#3\n#6": "listener ...\nsomeone they barely ..."
        },
        {
          "#1\n#3\n#6": "crush on him..."
        },
        {
          "#1\n#3\n#6": "Emotion Recognition in Conversation"
        },
        {
          "#1\n#3\n#6": "I can't believe that Ross is"
        },
        {
          "#1\n#3\n#6": "I didn't know Ross and you were so"
        },
        {
          "#1\n#3\n#6": "gone. It is just so sad."
        },
        {
          "#1\n#3\n#6": "close."
        },
        {
          "#1\n#3\n#6": "[sadness]\n#1"
        },
        {
          "#1\n#3\n#6": "#2\n[surprise]"
        },
        {
          "#1\n#3\n#6": "We weren't but we had one class"
        },
        {
          "#1\n#3\n#6": "I'm sure that would mean a lot to him. \ntogether. He was such a great guy ..."
        },
        {
          "#1\n#3\n#6": "#3\n[neutral]\n#4\n[sadness]"
        },
        {
          "#1\n#3\n#6": "I thought so many times about calling him ..."
        },
        {
          "#1\n#3\n#6": "But you didn't!"
        },
        {
          "#1\n#3\n#6": "#6\n[sadness]"
        },
        {
          "#1\n#3\n#6": "[joy]\n#7"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "speaker’s role within a conversation. Particularly, SPEAKER A is experiencing"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "sadness and regret (as mentioned in the speaker description), resulting in expres-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "sions predominantly filled with sadness. SPEAKER B appears to be a supportive"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "and empathetic listener, with limited involvement\nin the conversation, and re-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "acts through SPEAKER A’s utterances. Meanwhile, SPEAKER C responds with"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "excitement upon hearing their conversation.\nIntuitively, the integration of bio-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "graphical data plays an important\nrole in enriching the emotional background"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "of each speaker\nin conversations, and holds the potential\nfor more precise and"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "comprehensive emotional recognition, especially in complex dialogues."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "We carry out experiments on three benchmark datasets,\nincluding IEMO-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "CAP, MELD, and EmoryNLP. The experimental results demonstrate that our"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "method achieves SOTA performance, which indicates the effectiveness of our pro-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "posed model. Furthermore, our proposed mechanism, which uses a prompting"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "technique for LLMs to extract the speakers’ biographical\ninformation, shows the"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "potential to adapt to various conversation-level tasks such as opinion analysis,"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n3": "recommendation, and others."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "sation Transformer architecture to extract both self and inter-speaker contextual"
        },
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "features. However, the majority of prior research has predominantly concentrated"
        },
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "on modeling individual speaker utterances or interactions among different speak-"
        },
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "ers, with particular attention given to the intra- and inter-speaker aspects for the"
        },
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "extraction of speaker-based information [6,13]. Regrettably, limited emphasis has"
        },
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "been placed on exploring speaker characteristics, which constitute critical and"
        },
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "foundational\nelements of\nconversational\ninformation. Therefore, we propose a"
        },
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "novel method named BiosERC, which employs external tools to extract speaker"
        },
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "characteristics and inject\nthem into the process of emotion recognition within"
        },
        {
          "4\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "conversations."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "Context Modeling.\nUtterance vectors are integrated contextual information"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "of whole conversation by attention mechanism:"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "q · k⊺"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "√\nAttn(q, k, v, M ) = softmax(\n+ M ) · v\n(3)"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "dt"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": ", huttW v\n(4)\nqt, kt, vt = huttW q\nt , huttW k\nt"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "(5)\nheadt = Attn(qt, kt, vt, M )"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "(6)\nhMultiHead = concat([headt|0<t≤H ])W o"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "where H is the number of heads in the MultiHead attention layer; qt, kt, vt are ut-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "terance vectors in various semantic space (dimension size dt). In detail, following"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "[6,13], we construct the relation matrices (M ) for modeling relationship among"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "utterances, where Mik = 0 if ui and uk\nshould have interaction, Mik = −∞ if"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "otherwise. For\nthe baseline model, we implement\nthree different\nrelationships:"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "global context\n(all utterance pairs are connected),\nintra-speaker\n(only utterance"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "pairs of\nthe\nsame\nspeaker are\nconnected), and inter-speaker\n(only utterance"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "pairs of\nthe different\nspeaker are\nconnected). Consequently, we acquire\nthree"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "new hidden states (from Equation 6) hcontxt, hintra, hinter\nfeed-forward to the"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "Classification component."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "Classification.\nThis component aims to integrate all the hidden features of"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "utterances to classify the emotion label."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "hspeaker\n= hintra\nW a + hinter\nW r\n(7)\ni\ni"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "i"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "eo\nW g + hspeaker\n)\n(8)\ni = softmax(hutt\ni W u + hcontxt"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "i"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "(eo"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "i )"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n5": "function and is trained based on the gold emotional\nlabel of the i-th utterance."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: ). To avoid long plain",
      "data": [
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "LLMs\nrefers to large language models such as Llama2 [15], which can general-"
        },
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "ize a speaker’s biography based on their conversation. The prompting\nfunction"
        },
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "is a template containing two conversation instances (C) and speaker identifica-"
        },
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "tion (sj) to exploit the knowledge of the LLMs (Table 1). To avoid long plain"
        },
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "text descriptions, we instruct the LLMs to limit the output by adding a “note”"
        },
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "concerning the length of the prompting template. Consequently, we obtain ad-"
        },
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "ditional data about the persona of the speakers in each conversation (B), which"
        },
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "is utilized for speaker modeling in the subsequent step."
        },
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Table 1. Prompting template to extract the description of characteristics of the speaker"
        },
        {
          "6\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "from a conversation with LLMs."
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 2: ) instead of modifying model architecture. We",
      "data": [
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": "utterance and all"
        },
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": ""
        },
        {
          "integrated to enrich the utterance vector representation.": "baseline system."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 2: Prompting input template using speaker description and content of conver-",
      "data": [
        {
          "8\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "technique, to reduce the number of trainable parameters. The instruction fine-"
        },
        {
          "8\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "tuned LLM learned the distribution of emotional\nlabels given prompting input"
        },
        {
          "8\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "(x). During the inferring phase, the emotional\nlabel (ei) which is omitted from"
        },
        {
          "8\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "prompting input,\nis left to be generated by the fine-tuned LLM."
        },
        {
          "8\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Table 2. Prompting input template using speaker description and content of conver-"
        },
        {
          "8\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "sation for fine-tuning LLMs."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 2: Prompting input template using speaker description and content of conver-",
      "data": [
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "sation for fine-tuning LLMs."
        },
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "system"
        },
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "### You are an expert at analyzing the emotion of utterances among speakers in a conversation."
        },
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "### Given the characteristic of this speaker, {speaker\nname sj}: {speaker"
        },
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "### Given the following conversation as a context {conversation C}"
        },
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "user"
        },
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "Based on the above conversation and characteristics of the speakers, which emotional"
        },
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "in the utterance {utterance ui} ?"
        },
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "assistant"
        },
        {
          "Table 2. Prompting input template using speaker description and content of conver-": "{emotional\nlabel\nin\nof ui\ntext: ei}"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 4: ) on both two architec-",
      "data": [
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks": "to evaluate the test set. For fine-tuning BERT-based BiosERC (section 3.4), the",
          "9": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks": "hyper-parameters were\nselected as\nfollows:\nthe\nlearning rate\nis",
          "9": "selected from"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks": "{1e−5; 5e−6}; the dropout value is 0.2, and number epochs is 30; and the local",
          "9": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks": "context window size (w) is chosen in {2, 4}; we report the average scores obtained",
          "9": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks": "across 10 independent runs. For fine-tuning LLM-based BiosERC (section 3.5),",
          "9": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks": "learning rate is selected from {2e−4; 3e−4}, number epochs is 3; we report",
          "9": "the"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks": "average scores obtained across 5 independent runs because of the computation",
          "9": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks": "cost. All the source code of this project is published at MASKED LINK.",
          "9": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 4: ) on both two architec-",
      "data": [
        {
          "on the test\nsets. Column #T.Params.": "",
          "refers": "The notations ‡, † indicate the significant difference (t-test) with the baseline in levels",
          "to the number of": "",
          "trainable parameters.": ""
        },
        {
          "on the test\nsets. Column #T.Params.": "p < 0.01 and p < 0.05, separately.",
          "refers": "",
          "to the number of": "",
          "trainable parameters.": ""
        },
        {
          "on the test\nsets. Column #T.Params.": "Methods",
          "refers": "",
          "to the number of": "#T.Params. IEMOCAP EmoryNLP MELD",
          "trainable parameters.": ""
        },
        {
          "on the test\nsets. Column #T.Params.": "HiTrans [32]",
          "refers": "",
          "to the number of": "64.50",
          "trainable parameters.": "61.94"
        },
        {
          "on the test\nsets. Column #T.Params.": "DAG [12]",
          "refers": "",
          "to the number of": "68.03",
          "trainable parameters.": "63.65"
        },
        {
          "on the test\nsets. Column #T.Params.": "DialogXL [33]",
          "refers": "",
          "to the number of": "65.94",
          "trainable parameters.": "62.14"
        },
        {
          "on the test\nsets. Column #T.Params.": "DialogueEIN [6]",
          "refers": "",
          "to the number of": "68.93",
          "trainable parameters.": "65.37"
        },
        {
          "on the test\nsets. Column #T.Params.": "SGED + DAG-ERC [10]",
          "refers": "",
          "to the number of": "68.53",
          "trainable parameters.": "65.46"
        },
        {
          "on the test\nsets. Column #T.Params.": "S+PAGE [24]",
          "refers": "",
          "to the number of": "68.93",
          "trainable parameters.": "64.67"
        },
        {
          "on the test\nsets. Column #T.Params.": "InstructERC [22] +(ft LLM)",
          "refers": "",
          "to the number of": "71.39",
          "trainable parameters.": "69.15"
        },
        {
          "on the test\nsets. Column #T.Params.": "Intra/inter ERC (baseline) [13]",
          "refers": "189 × 106",
          "to the number of": "67.65",
          "trainable parameters.": "64.58"
        },
        {
          "on the test\nsets. Column #T.Params.": "BiosERCBERT-based",
          "refers": "186 × 106",
          "to the number of": "67.79",
          "trainable parameters.": "65.51‡"
        },
        {
          "on the test\nsets. Column #T.Params.": "BiosERC +ft LLMLlama-2-7b",
          "refers": "80 × 106",
          "to the number of": "69.02",
          "trainable parameters.": "68.72"
        },
        {
          "on the test\nsets. Column #T.Params.": "BiosERC +ft LLMLlama-2-13b",
          "refers": "125 × 106",
          "to the number of": "71.19",
          "trainable parameters.": "69.83"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table 2: ) from the input prompting (row 6), the performance",
      "data": [
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "In the previous method of fine-tuning an LLM,\nInstructERC [22] considers"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "speaker\nidentifier as an auxiliary task,\nrequiring two-stage\ntraining, which is"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "more time-consuming than ours in the training process. Besides, our proposed"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "method uses speaker descriptions generated by LLM in natural\nlanguage, which"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "can be easier incorporated with humans using our system for customization (e.g.,"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "customer support staff can directly provide or modify characteristics generated"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "by LLM of\ntheir\ncustomers). Similar\nto BERT-based BiosERC, among three"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "datasets, the LLM-based BiosERC shows the strengthens of multi-party datasets"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "(more than two speakers in each conversation), EmoryNLP and MELD. By fine-"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "the performance of our BiosERC increased by\ntuning an LLM, Llama-2-13b,"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "1-4% weighted F1 scores compared to BERT-based models and achieved new"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "SOTA performance on EmoryNLP and MELD datasets. Besides, since utilizing"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "a lightweight training technique, LoRA [28], the number of training parameters"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "in LLM-based BiosERC was smaller than BERT-based BiosERC (only fine-tuned"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "on two last\nlayers) which proved the potential of LLM-based BiosERC in the"
        },
        {
          "10\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "real application."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 5: Performance comparison among variants of BiosERC on the MELD devel-",
      "data": [
        {
          "Table 5.": "opment set.",
          "Performance comparison among variants of BiosERC on the MELD devel-": ""
        },
        {
          "Table 5.": "Methods",
          "Performance comparison among variants of BiosERC on the MELD devel-": ""
        },
        {
          "Table 5.": "1. Intra/inter ERC (baseline)",
          "Performance comparison among variants of BiosERC on the MELD devel-": "-"
        },
        {
          "Table 5.": "2. BiosERC injecting bio. by attention",
          "Performance comparison among variants of BiosERC on the MELD devel-": "Llama-2-chat-70b"
        },
        {
          "Table 5.": "3. BiosERC",
          "Performance comparison among variants of BiosERC on the MELD devel-": "Llama-2-chat-70b"
        },
        {
          "Table 5.": "4. BiosERC",
          "Performance comparison among variants of BiosERC on the MELD devel-": "Llama-2-chat-7b"
        },
        {
          "Table 5.": "5. BiosERC",
          "Performance comparison among variants of BiosERC on the MELD devel-": "vicuna-33b-v1.3"
        },
        {
          "Table 5.": "6. BiosERC +ft LLMLlama-2-13b w/o speaker bio. -",
          "Performance comparison among variants of BiosERC on the MELD devel-": ""
        },
        {
          "Table 5.": "7. BiosERC +ft LLMLlama-2-7b",
          "Performance comparison among variants of BiosERC on the MELD devel-": "Llama-2-chat-70b"
        },
        {
          "Table 5.": "8. BiosERC +ft LLMLlama-2-13b",
          "Performance comparison among variants of BiosERC on the MELD devel-": "Llama-2-chat-70b"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 6: , conversation 1041",
      "data": [
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "5.4\nCase study"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Our model enhances emotion recognition accuracy, even in short conversations"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "with limited contextual\ninformation. As\nshow in Table\n6,\nconversation 1041"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "is a short dialogue consisting of only five sentences. Our model perceives\ntwo"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "speaker descriptions, these descriptions facilitate a more accurate identification"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "leaning towards positivity rather than anger. In ad-\nof SPEAKER 0’s discourse,"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "dition, our architecture shows an improved capacity in predicting emotions\nin"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "shorter sentences through the utilization of speaker description, particularly in"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "cases where\ntraditional models\nstruggle due\nto the minimal\ninformation con-"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "tained in expressions such as “Yeah”, or “Okay”."
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Additionally, our approach adeptly copes with scenarios where the error rate"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "is high at\nthe beginning of\nthe\nconversation as\nshow in Table 6 (u0, u1, u2"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "in conversation 1061). Because contextual and speaker information is lacking at"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "the outset of the conversation, the baseline model consistently produces incorrect"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "initial\nsentences. However, with the assistance of our “speaker description”,\nit"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "delivers a better performance from the beginning of the dialogue. Experimental"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "results prove\nthe\neffectiveness and versatility of our approach across diverse"
        },
        {
          "12\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "conversations,\nincluding those with complex contextual\ninformation."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Really?!\nu3\nsurprise surprise": "joy\njoy\nu4\nYeah!",
          "surprise": "anger"
        },
        {
          "Really?!\nu3\nsurprise surprise": "Conversation 1061",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "Idx\nLabel",
          "surprise": "BiosERC Baseline"
        },
        {
          "Really?!\nu3\nsurprise surprise": "Speaker 0\nSpeaker 1\nSpeaker 2",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "d0",
          "surprise": "SPEAKER 0 seems to be a very inquisitive and curious person. ... SPEAKER 0 appears to be quite"
        },
        {
          "Really?!\nu3\nsurprise surprise": "blunt and direct in his communication style, not mincing words or sugarcoating his thoughts.",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "SPEAKER 1 seems to be a humorous and light-hearted person.\nd1",
          "surprise": "... SPEAKER 1 shares that they"
        },
        {
          "Really?!\nu3\nsurprise surprise": "",
          "surprise": "have only been with one person in their whole life, and this is met with surprise and disbelief by the"
        },
        {
          "Really?!\nu3\nsurprise surprise": "other ...",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "SPEAKER 2 seems\nto be a humorous and light-hearted person... SPEAKER 2 is\nd2",
          "surprise": "someone who"
        },
        {
          "Really?!\nu3\nsurprise surprise": "enjoys having fun and is not afraid to poke fun at themselves or others...",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "u0 Well, what?\nneutral\nneutral",
          "surprise": "surprise"
        },
        {
          "Really?!\nu3\nsurprise surprise": "neutral\nneutral\nu1 What?",
          "surprise": "surprise"
        },
        {
          "Really?!\nu3\nsurprise surprise": "neutral\nneutral\nu2 What is it?",
          "surprise": "sadness"
        },
        {
          "Really?!\nu3\nsurprise surprise": "That she left you?\nu3\nsurprise surprise",
          "surprise": "sadness"
        },
        {
          "Really?!\nu3\nsurprise surprise": "u4\nneutral\nsadness\nThat she likes women?",
          "surprise": "sadness"
        },
        {
          "Really?!\nu3\nsurprise surprise": "That\nshe left you for another woman that",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "u5\nneutral\nsurprise",
          "surprise": "sadness"
        },
        {
          "Really?!\nu3\nsurprise surprise": "likes women?",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "Little louder, okay,\nI\nthink there’s a man on the",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "anger\nneutral\nu6",
          "surprise": "anger"
        },
        {
          "Really?!\nu3\nsurprise surprise": "twelfth floor in a coma that didn’t quite hear you.",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "...",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "u11\nsurprise surprise\nWith Carol? Oh.",
          "surprise": "neutral"
        },
        {
          "Really?!\nu3\nsurprise surprise": "So in your whole life, you’ve only been with",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "u12\nsurprise neutral",
          "surprise": "neutral"
        },
        {
          "Really?!\nu3\nsurprise surprise": "one oh.",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "Whoah, boy, hockey was a big mistake! There was",
          "surprise": ""
        },
        {
          "Really?!\nu3\nsurprise surprise": "u13\nsurprise surprise",
          "surprise": "joy"
        },
        {
          "Really?!\nu3\nsurprise surprise": "a whole bunch of stuff we could’ve done tonight!",
          "surprise": ""
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "and green labels refer to the incorrect and correct prediction of the models, respectively."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Conversation 1041"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Idx\nLabel\nBiosERC Baseline"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Speaker 0\nSpeaker 1"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "SPEAKER 0 in the conversation comes across as someone who is confident, friendly\nd0"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": ".. to create a relaxed atmosphere ..."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "SPEAKER 1 in the conversation comes across as a friendly .. have a strong sense of\nd1"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "loyalty and trust in their relationships..."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Hey Estelle,listen\nu0\nneutral\nneutral\nneutral"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Well! Well! Well!\nJoey Tribbiani!\nSo"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "u1\nsurprise surprise\njoy"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "you came back huh?"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "What are you talking about? I never"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "u2\nsurprise surprise\nanger"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "left you! You’ve always been my agent!"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Really?!\nu3\nsurprise surprise\nsurprise"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "joy\njoy\nanger\nu4\nYeah!"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Conversation 1061"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Idx\nLabel\nBiosERC Baseline"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Speaker 0\nSpeaker 1\nSpeaker 2"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "SPEAKER 0 seems to be a very inquisitive and curious person. ... SPEAKER 0 appears to be quite\nd0"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "blunt and direct in his communication style, not mincing words or sugarcoating his thoughts."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "SPEAKER 1 seems to be a humorous and light-hearted person.\n... SPEAKER 1 shares that they\nd1"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "have only been with one person in their whole life, and this is met with surprise and disbelief by the"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "other ..."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "SPEAKER 2 seems\nto be a humorous and light-hearted person... SPEAKER 2 is\nsomeone who\nd2"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "enjoys having fun and is not afraid to poke fun at themselves or others..."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "u0 Well, what?\nneutral\nneutral\nsurprise"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "neutral\nneutral\nsurprise\nu1 What?"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "neutral\nneutral\nsadness\nu2 What is it?"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "That she left you?\nu3\nsurprise surprise\nsadness"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "u4\nneutral\nsadness\nsadness\nThat she likes women?"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "That\nshe left you for another woman that"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "u5\nneutral\nsurprise\nsadness"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "likes women?"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Little louder, okay,\nI\nthink there’s a man on the"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "anger\nneutral\nanger\nu6"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "twelfth floor in a coma that didn’t quite hear you."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "..."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "u11\nsurprise surprise\nneutral\nWith Carol? Oh."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "So in your whole life, you’ve only been with"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "u12\nsurprise neutral\nneutral"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "one oh."
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "Whoah, boy, hockey was a big mistake! There was"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "u13\nsurprise surprise\njoy"
        },
        {
          "Table 6. Case study of improvement examples collected in the MELD dataset. The red": "a whole bunch of stuff we could’ve done tonight!"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "7\nConclusion"
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "In conclusion, we proposed a novel mechanism incorporating speakers’ charac-"
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "teristics into the ERC task, which has not been fully developed in prior research."
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "We improve the performance of the ERC task by investigating the influence of"
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "the personality of interlocutors on emotions, considering this external knowledge"
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "as a unique feature. Our experiments on three benchmark datasets consistently"
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "yielded SOTA or\ncompetitive\nresults,\nthereby substantiating the\neffectiveness"
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "of our proposed method. Furthermore, our model\nis straightforward yet highly"
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "adaptable, thus enabling its applicability to a wide range of conversation analysis"
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "tasks."
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Acknowledgement."
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "This work is supported partly by AOARD grant FA23862214039."
        },
        {
          "14\nJieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "References"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "1. D. Ghosal, N. Majumder, S. Poria, N. Chhaya, and A. Gelbukh, “DialogueGCN:"
        },
        {
          "References": "A graph convolutional neural network for emotion recognition in conversation,”"
        },
        {
          "References": "in Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Language"
        },
        {
          "References": "Processing and the 9th International Joint Conference on Natural Language Pro-"
        },
        {
          "References": "cessing (EMNLP-IJCNLP), K.\nInui, J. Jiang, V. Ng, and X. Wan, Eds.\nHong"
        },
        {
          "References": "Kong, China: Association for Computational Linguistics, Nov. 2019, pp. 154–164."
        },
        {
          "References": "2. D. Zhang, L. Wu, C. Sun, S. Li, Q. Zhu, and G. Zhou, “Modeling both context-"
        },
        {
          "References": "and speaker-sensitive dependence for emotion detection in multi-speaker conversa-"
        },
        {
          "References": "tions.” in IJCAI, 2019, pp. 5415–5421."
        },
        {
          "References": "3. G. Hu, T.-E. Lin, Y. Zhao, G. Lu, Y. Wu, and Y. Li, “UniMSE: Towards unified"
        },
        {
          "References": "multimodal sentiment analysis and emotion recognition,” in Proceedings of the 2022"
        },
        {
          "References": "Conference on Empirical Methods\nin Natural Language Processing, Y. Goldberg,"
        },
        {
          "References": "Z. Kozareva, and Y. Zhang, Eds.\nAbu Dhabi, United Arab Emirates: Association"
        },
        {
          "References": "for Computational Linguistics, Dec. 2022, pp. 7837–7851."
        },
        {
          "References": "4. X. Shi, X. Li, and T. Toda, “Emotion Awareness\nin Multi-utterance Turn for"
        },
        {
          "References": "Improving Emotion Prediction in Multi-Speaker Conversation,” in Proc. INTER-"
        },
        {
          "References": "SPEECH 2023, 2023, pp. 765–769."
        },
        {
          "References": "5.\nJ. Li, D. Ji, F. Li, M. Zhang, and Y. Liu, “Hitrans: A transformer-based context-"
        },
        {
          "References": "and speaker-sensitive model for emotion detection in conversations,” in Proceedings"
        },
        {
          "References": "of the 28th International Conference on Computational Linguistics, 2020, pp. 4190–"
        },
        {
          "References": "4200."
        },
        {
          "References": "6. Y. Liu, J. Zhao, J. Hu, R. Li, and Q. Jin, “DialogueEIN: Emotion interaction"
        },
        {
          "References": "network for dialogue affective analysis,” in Proceedings of\nthe 29th International"
        },
        {
          "References": "Conference on Computational Linguistics.\nGyeongju, Republic of Korea:\nInter-"
        },
        {
          "References": "national Committee on Computational Linguistics, Oct. 2022, pp. 684–693."
        },
        {
          "References": "7.\nJ. Li, Z. Lin, P. Fu, and W. Wang, “Past, present, and future: Conversational"
        },
        {
          "References": "emotion recognition through structural modeling of psychological knowledge,” in"
        },
        {
          "References": "Findings of\nthe association for computational\nlinguistics: EMNLP 2021, 2021, pp."
        },
        {
          "References": "1204–1214."
        },
        {
          "References": "8. N. Majumder, S. Poria, D. Hazarika, R. Mihalcea, A. Gelbukh, and E. Cambria,"
        },
        {
          "References": "“Dialoguernn: An attentive rnn for emotion detection in conversations,” in Pro-"
        },
        {
          "References": "ceedings of the AAAI conference on artificial\nintelligence, vol. 33, no. 01, 2019, pp."
        },
        {
          "References": "6818–6825."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "J. Lee and W. Lee, “CoMPM: Context modeling with speaker’s pre-trained mem-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "ory tracking for emotion recognition in conversation,” in Proceedings of\nthe 2022"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Conference of\nthe North American Chapter of\nthe Association for Computational"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Linguistics: Human Language Technologies.\nSeattle, United States: Association"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "for Computational Linguistics, Jul. 2022, pp. 5669–5679."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "10. Y. Bao, Q. Ma, L. Wei, W. Zhou, and S. Hu, “Speaker-guided encoder-decoder"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "framework for emotion recognition in conversation,” in Proceedings of\nthe Thirty-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "First\nInternational Joint Conference on Artificial\nIntelligence,\nIJCAI-22, L. D."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Raedt, Ed.\nInternational Joint Conferences on Artificial Intelligence Organization,"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "7 2022, pp. 4051–4057, main Track."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "11. S. Poria, N. Majumder, D. Hazarika, D. Ghosal, R. Bhardwaj, S. Y. B. Jian,"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "P. Hong, R. Ghosh, A. Roy, N. Chhaya et al., “Recognizing emotion cause\nin"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "conversations,” Cognitive Computation, vol. 13, pp. 1317–1332, 2021."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "12. W. Shen, S. Wu, Y. Yang, and X. Quan, “Directed acyclic graph network for"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "conversational emotion recognition,” in Proceedings of\nthe 59th Annual Meeting"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "of\nthe Association for Computational Linguistics and the 11th International Joint"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Conference on Natural Language Processing (Volume 1: Long Papers).\nOnline:"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Association for Computational Linguistics, Aug. 2021, pp. 1551–1560."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "13. X. Jieying, N. Phuong, M. Blake, and N. Le Minh, “Accumulating word represen-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "tations in multi-level context integration for erc task,” in 2023 15th International"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Conference on Knowledge and Systems Engineering (KSE), 2023, pp. 1–6."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "et al., “Chain-of-thought prompting elicits\nreasoning in large language models,”"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Advances\nin Neural\nInformation Processing Systems, vol. 35, pp. 24 824–24 837,"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "2022."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "S. Batra, P. Bhargava, S. Bhosale et al., “Llama 2: Open foundation and fine-tuned"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "chat models,” arXiv preprint arXiv:2307.09288, 2023."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "monSense knowledge for eMotion identification in conversations,” in Findings of"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "the Association for Computational Linguistics: EMNLP 2020. Online: Association"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "for Computational Linguistics, Nov. 2020, pp. 2470–2481."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "multi-turn dialogue dataset,” in Proceedings of the Eighth International Joint Con-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "ference on Natural Language Processing (Volume 1: Long Papers). Taipei, Taiwan:"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Asian Federation of Natural Language Processing, Nov. 2017, pp. 986–995."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "of turns in dialogue,” in Proceedings of the 2021 Conference on Empirical Methods"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "in Natural Language Processing.\nOnline and Punta Cana, Dominican Republic:"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Association for Computational Linguistics, Nov. 2021, pp. 443–455."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": ""
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "model\nfor conversational emotion recognition,” Proceedings of\nthe AAAI Confer-"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "ence on Artificial Intelligence, vol. 37, no. 11, pp. 13 121–13 129, Jun. 2023."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-training of deep"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "bidirectional transformers for language understanding,” in Proceedings of the 2019"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Conference of\nthe North American Chapter of\nthe Association for Computational"
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp."
        },
        {
          "Integrating Biography Speakers Supported by LLMs for ERC Tasks\n15": "4171–4186."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "21. Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettle-"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "moyer, and V. Stoyanov, “Roberta: A robustly optimized bert pretraining ap-"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "proach,” arXiv preprint arXiv:1907.11692, 2019."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "22. S. Lei, G. Dong, X. Wang, K. Wang, and S. Wang, “Instructerc: Reforming emo-"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "tion recognition in conversation with a retrieval multi-task llms framework,” arXiv"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "preprint arXiv:2309.11911, 2023."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "23. G. Tu, B. Liang, B. Qin, K.-F. Wong, and R. Xu, “An empirical study on multiple"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "knowledge from ChatGPT for emotion recognition in conversations,” in Findings of"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "the Association for Computational Linguistics: EMNLP 2023, H. Bouamor, J. Pino,"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "and K. Bali, Eds.\nSingapore: Association for Computational Linguistics, Dec."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "2023, pp. 12 160–12 173."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "24. C. Liang, J. Xu, Y. Lin, C. Yang, and Y. Wang, “S+PAGE: A speaker and position-"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "aware graph neural network model\nfor\nemotion recognition in conversation,” in"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Proceedings of\nthe 2nd Conference of\nthe Asia-Pacific Chapter of\nthe Association"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "for Computational Linguistics and the 12th International Joint Conference on Nat-"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "ural Language Processing (Volume 1: Long Papers).\nOnline only: Association for"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Computational Linguistics, Nov. 2022, pp. 148–157."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "25. T. Kim and P. Vossen, “Emoberta: Speaker-aware emotion recognition in conver-"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "sation with roberta,” CoRR, vol. abs/2108.12009, 2021."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "26. A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "models are unsupervised multitask learners,” 2019."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "27. H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, E. Li, X. Wang,"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "M. Dehghani, S. Brahma, A. Webson, S. S. Gu, Z. Dai, M. Suzgun, X. Chen,"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "A. Chowdhery, S. Narang, G. Mishra, A. Yu, V. Zhao, Y. Huang, A. Dai, H. Yu,"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "S. Petrov, E. H. Chi, J. Dean, J. Devlin, A. Roberts, D. Zhou, Q. V. Le, and J. Wei,"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "“Scaling instruction-finetuned language models,” 2022."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "28. E. J. Hu, yelong shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "W. Chen, “LoRA: Low-rank adaptation of large language models,” in International"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Conference on Learning Representations, 2022."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "29. C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang,"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "S. Lee, and S. S. Narayanan, “Iemocap: Interactive emotional dyadic motion cap-"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "ture database,” Language resources and evaluation, vol. 42, pp. 335–359, 2008."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "30. S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea, “Meld:"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "A multimodal multi-party dataset for emotion recognition in conversations,” arXiv"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "preprint arXiv:1810.02508, 2018."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "31. S. M. Zahiri and J. D. Choi, “Emotion detection on tv show transcripts with"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "sequence-based convolutional neural networks,” in Workshops at\nthe thirty-second"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "aaai conference on artificial\nintelligence, 2018."
        },
        {
          "16": "32.",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "J. Li, D. Ji, F. Li, M. Zhang, and Y. Liu, “HiTrans: A transformer-based context-"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "and speaker-sensitive model for emotion detection in conversations,” in Proceedings"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "of\nthe 28th International Conference on Computational Linguistics.\nBarcelona,"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Spain (Online): International Committee on Computational Linguistics, Dec. 2020,"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "pp. 4190–4200."
        },
        {
          "16": "33. W. Shen, J. Chen, X. Quan, and Z. Xie, “Dialogxl: All-in-one xlnet for multi-party",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": ""
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "conversation\nemotion\nrecognition,”\nProceedings\nof\nthe AAAI Conference\non"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Artificial\nIntelligence,\nvol.\n35,\nno.\n15,\npp.\n13 789–13 797,\n5\n2021.\n[Online]."
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "Available: https://ojs.aaai.org/index.php/AAAI/article/view/17625"
        },
        {
          "16": "34. L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li,",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": ""
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "D. Li, E. P. Xing, H. Zhang, J. Gonzalez, and I. Stoica, “Judging llm-as-a-judge"
        },
        {
          "16": "",
          "Jieying Xue, Minh-Phuong Nguyen, Blake Matheny, and Le-Minh Nguyen": "with mt-bench and chatbot arena,” ArXiv, vol. abs/2306.05685, 2023."
        }
      ],
      "page": 16
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "DialogueGCN: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "S Poria",
        "N Chhaya",
        "A Gelbukh"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "2",
      "title": "Modeling both contextand speaker-sensitive dependence for emotion detection in multi-speaker conversations",
      "authors": [
        "D Zhang",
        "L Wu",
        "C Sun",
        "S Li",
        "Q Zhu",
        "G Zhou"
      ],
      "year": "2019",
      "venue": "IJCAI"
    },
    {
      "citation_id": "3",
      "title": "UniMSE: Towards unified multimodal sentiment analysis and emotion recognition",
      "authors": [
        "G Hu",
        "T.-E Lin",
        "Y Zhao",
        "G Lu",
        "Y Wu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "4",
      "title": "Emotion Awareness in Multi-utterance Turn for Improving Emotion Prediction in Multi-Speaker Conversation",
      "authors": [
        "X Shi",
        "X Li",
        "T Toda"
      ],
      "year": "2023",
      "venue": "Proc. INTER-SPEECH 2023"
    },
    {
      "citation_id": "5",
      "title": "Hitrans: A transformer-based contextand speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "6",
      "title": "DialogueEIN: Emotion interaction network for dialogue affective analysis",
      "authors": [
        "Y Liu",
        "J Zhao",
        "J Hu",
        "R Li",
        "Q Jin"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics. Gyeongju, Republic of Korea: International Committee on Computational Linguistics"
    },
    {
      "citation_id": "7",
      "title": "Past, present, and future: Conversational emotion recognition through structural modeling of psychological knowledge",
      "authors": [
        "J Li",
        "Z Lin",
        "P Fu",
        "W Wang"
      ],
      "year": "2021",
      "venue": "Findings of the association for computational linguistics: EMNLP 2021"
    },
    {
      "citation_id": "8",
      "title": "Dialoguernn: An attentive rnn for emotion detection in conversations",
      "authors": [
        "N Majumder",
        "S Poria",
        "D Hazarika",
        "R Mihalcea",
        "A Gelbukh",
        "E Cambria"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "9",
      "title": "CoMPM: Context modeling with speaker's pre-trained memory tracking for emotion recognition in conversation",
      "authors": [
        "J Lee",
        "W Lee"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2022 Conference of the North American Chapter"
    },
    {
      "citation_id": "10",
      "title": "Speaker-guided encoder-decoder framework for emotion recognition in conversation",
      "authors": [
        "Y Bao",
        "Q Ma",
        "L Wei",
        "W Zhou",
        "S Hu"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22"
    },
    {
      "citation_id": "11",
      "title": "Recognizing emotion cause in conversations",
      "authors": [
        "S Poria",
        "N Majumder",
        "D Hazarika",
        "D Ghosal",
        "R Bhardwaj",
        "S Jian",
        "P Hong",
        "R Ghosh",
        "A Roy",
        "N Chhaya"
      ],
      "year": "2021",
      "venue": "Cognitive Computation"
    },
    {
      "citation_id": "12",
      "title": "Directed acyclic graph network for conversational emotion recognition",
      "authors": [
        "W Shen",
        "S Wu",
        "Y Yang",
        "X Quan"
      ],
      "year": "2021",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "13",
      "title": "Accumulating word representations in multi-level context integration for erc task",
      "authors": [
        "X Jieying",
        "N Phuong",
        "M Blake",
        "N Minh"
      ],
      "year": "2023",
      "venue": "2023 15th International Conference on Knowledge and Systems Engineering (KSE)"
    },
    {
      "citation_id": "14",
      "title": "Chain-of-thought prompting elicits reasoning in large language models",
      "authors": [
        "J Wei",
        "X Wang",
        "D Schuurmans",
        "M Bosma",
        "F Xia",
        "E Chi",
        "Q Le",
        "D Zhou"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models",
      "arxiv": "arXiv:2307.09288"
    },
    {
      "citation_id": "16",
      "title": "COSMIC: COm-monSense knowledge for eMotion identification in conversations",
      "authors": [
        "D Ghosal",
        "N Majumder",
        "A Gelbukh",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2020",
      "venue": "COSMIC: COm-monSense knowledge for eMotion identification in conversations"
    },
    {
      "citation_id": "17",
      "title": "DailyDialog: A manually labelled multi-turn dialogue dataset",
      "authors": [
        "Y Li",
        "H Su",
        "X Shen",
        "W Li",
        "Z Cao",
        "S Niu"
      ],
      "year": "2017",
      "venue": "Proceedings of the Eighth International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "18",
      "title": "Graph based network with contextualized representations of turns in dialogue",
      "authors": [
        "B Lee",
        "Y Choi"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "19",
      "title": "Skier: A symbolic knowledge integrated model for conversational emotion recognition",
      "authors": [
        "W Li",
        "L Zhu",
        "R Mao",
        "E Cambria"
      ],
      "year": "2023",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies"
    },
    {
      "citation_id": "21",
      "title": "Roberta: A robustly optimized bert pretraining approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "Roberta: A robustly optimized bert pretraining approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "22",
      "title": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "authors": [
        "S Lei",
        "G Dong",
        "X Wang",
        "K Wang",
        "S Wang"
      ],
      "year": "2023",
      "venue": "Instructerc: Reforming emotion recognition in conversation with a retrieval multi-task llms framework",
      "arxiv": "arXiv:2309.11911"
    },
    {
      "citation_id": "23",
      "title": "An empirical study on multiple knowledge from ChatGPT for emotion recognition in conversations",
      "authors": [
        "G Tu",
        "B Liang",
        "B Qin",
        "K.-F Wong",
        "R Xu"
      ],
      "year": "2023",
      "venue": "Findings of the Association for Computational Linguistics: EMNLP 2023"
    },
    {
      "citation_id": "24",
      "title": "S+PAGE: A speaker and positionaware graph neural network model for emotion recognition in conversation",
      "authors": [
        "C Liang",
        "J Xu",
        "Y Lin",
        "C Yang",
        "Y Wang"
      ],
      "year": "2022",
      "venue": "Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing"
    },
    {
      "citation_id": "25",
      "title": "Emoberta: Speaker-aware emotion recognition in conversation with roberta",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "CoRR"
    },
    {
      "citation_id": "26",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "Language models are unsupervised multitask learners"
    },
    {
      "citation_id": "27",
      "title": "Scaling instruction-finetuned language models",
      "authors": [
        "H Chung",
        "L Hou",
        "S Longpre",
        "B Zoph",
        "Y Tay",
        "W Fedus",
        "E Li",
        "X Wang",
        "M Dehghani",
        "S Brahma",
        "A Webson",
        "S Gu",
        "Z Dai",
        "M Suzgun",
        "X Chen",
        "A Chowdhery",
        "S Narang",
        "G Mishra",
        "A Yu",
        "V Zhao",
        "Y Huang",
        "A Dai",
        "H Yu",
        "S Petrov",
        "E Chi",
        "J Dean",
        "J Devlin",
        "A Roberts",
        "D Zhou",
        "Q Le",
        "J Wei"
      ],
      "year": "2022",
      "venue": "Scaling instruction-finetuned language models"
    },
    {
      "citation_id": "28",
      "title": "LoRA: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "29",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "30",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "31",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second aaai conference on artificial intelligence"
    },
    {
      "citation_id": "32",
      "title": "HiTrans: A transformer-based contextand speaker-sensitive model for emotion detection in conversations",
      "authors": [
        "J Li",
        "D Ji",
        "F Li",
        "M Zhang",
        "Y Liu"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "33",
      "title": "Dialogxl: All-in-one xlnet for multi-party conversation emotion recognition",
      "authors": [
        "W Shen",
        "J Chen",
        "X Quan",
        "Z Xie"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "34",
      "title": "Judging llm-as-a-judge with mt-bench and chatbot arena",
      "authors": [
        "L Zheng",
        "W.-L Chiang",
        "Y Sheng",
        "S Zhuang",
        "Z Wu",
        "Y Zhuang",
        "Z Lin",
        "Z Li",
        "D Li",
        "E Xing",
        "H Zhang",
        "J Gonzalez",
        "I Stoica"
      ],
      "year": "2023",
      "venue": "ArXiv"
    }
  ]
}