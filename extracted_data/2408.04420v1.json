{
  "paper_id": "2408.04420v1",
  "title": "Recognizing Emotion Regulation Strategies From Human Behavior With Large Language Models",
  "published": "2024-08-08T12:47:10Z",
  "authors": [
    "Philipp Müller",
    "Alexander Heimerl",
    "Sayed Muddashir Hossain",
    "Lea Siegel",
    "Jan Alexandersson",
    "Patrick Gebhard",
    "Elisabeth André",
    "Tanja Schneeberger"
  ],
  "keywords": [
    "emotion regulation",
    "large language models",
    "emotion recognition",
    "bayesian networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Human emotions are often not expressed directly, but regulated according to internal processes and social display rules. For affective computing systems, an understanding of how users regulate their emotions can be highly useful, for example to provide feedback in job interview training, or in psychotherapeutic scenarios. However, at present no method to automatically classify different emotion regulation strategies in a cross-user scenario exists. At the same time, recent studies showed that instruction-tuned Large Language Models (LLMs) can reach impressive performance across a variety of affect recognition tasks such as categorical emotion recognition or sentiment analysis. While these results are promising, it remains unclear to what extent the representational power of LLMs can be utilized in the more subtle task of classifying users' internal emotion regulation strategy. To close this gap, we make use of the recently introduced DEEP corpus for modeling the social display of the emotion shame, where each point in time is annotated with one of seven different emotion regulation classes. We fine-tune Llama2-7B as well as the recently introduced Gemma model using Lowrank Optimization on prompts generated from different sources of information on the DEEP corpus. These include verbal and nonverbal behavior, person factors, as well as the results of an in-depth interview after the interaction. Our results show, that a fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation strategy with high accuracy (0.84) without needing access to data from post-interaction interviews. This represents a significant improvement over previous approaches based on Bayesian Networks and highlights the importance of modeling verbal behavior in emotion regulation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "One key finding of emotion research is that there is no oneto-one mapping of displayed emotional expressions to internally experienced emotions  [1] . Emotions do not necessarily become visible  [2] , nor consciously experienced  [3] -  [5] . One reason for this is emotion regulation, which encompasses various conscious or unconscious strategies that individuals use to influence their emotional experience  [6] . Especially unpleasant emotions such as shame are regulated to protect the self  [3] -  [5] . For many affective computing systems, knowledge of users' emotion regulation strategies has the potential to be highly useful. Such systems include social skill training systems  [7] -  [9]  or therapeutic assistance systems  [10] ,  [11] . The recently introduced DEEP approach was the first attempt to create a computational model of emotion regulation, focusing on the emotion shame elicited in job interviews  [12] . While the authors presented a Bayesian Network (BN) model to classify emotion regulation strategies, their approach had two key limitations prohibiting application in realistic scenarios. First, they require results of an extensive analysis of in-depth postinteraction interviews as input. Second, they did not evaluate their model in a subject-independent scenario.\n\nRecent studies indicate that generative large language models (LLMs) are able to, in a certain sense, understand human emotion in social situations. In zero-shot scenarios, GPT3.5 and GPT4 were successfully applied across a variety of emotion-related tasks, including sentiment analysis, emotion and emotion cause recognition, toxicity detection, and opinion extraction, albeit they are often still outperformed by approaches directly trained on the respective tasks  [13] ,  [14] . In contrast to zero-shot scenarios, instruction-tuning is an effective means to utilize the representational power of generative LLMs and at the same time adapt to a specific target task  [15] -  [19] . Using Low-rank Adaptation (LoRA)  [20] , this process is computationally efficient, and was already utilized for tasks related to affect and social behavior  [21] -  [23] . In particular, DialogueLLM  [21]  reached state-of-the-art results for emotion recognition on the MELD  [24] , IEMOCAP  [25] , and EmoryNLP  [26]  datasets. While these results are encouraging, it is unclear to what extent instruction-tuned LLMs can be used to classify emotion regulation strategies. In contrast to expressions of emotion, these strategies reflect inner processes that may not have distinct observable cues and are believed to be heavily related to nonverbal aspects of behavior  [12] .\n\nIn our work, we investigate to what extent instruction-tuned LLMs are capable of classifying the strategies employed by humans to regulate shame. To this end, we make use of the recently introduced DEEP corpus comprising recordings of human behavior in shame inducing situations and self-reported information about individual experience  [12] . Inspired by Dia-logueLLM  [21] , we encode participants' multimodal behavior into prompts that are used for instruction-tuning Llama2-7B  [27] ,  [28]  and Gemma  [29]  models with LoRA  [20] . We present the first cross-user evaluations on the DEEP corpus and show that our LLM-based approach can reach an accuracy of 0.84 in emotion regulation classification without access to any information from informative but impractical post-interaction interviews. As such, our results represent an important step towards affective computing systems that can recognize human emotion regulation strategies in realistic scenarios.\n\nOur specific contributions are three-fold. 1) We utilize LLMs instruction-tuned on prompts incorporating multimodal behavior to classify peoples' strategies to regulate the emotion shame. 2) In the first cross-user evaluations on the recently introduced DEEP corpus  [12] , our approach outperforms the previous state of the art based on expertconstructed Bayesian Networks when information from post-interaction interviews is not available. 3) We conduct extensive ablation experiments, highlighting the impact of different modalities on performance.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Work",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Model Of Emotions And Emotion Regulation",
      "text": "There is a variety of emotion models both in psychology  [30]  and affective computing  [31] . In our work, we follow a model of emotions that differentiates between internal and external components inspired by cognitive psychoanalysis  [32] . Internal components of emotions are not directly observable as they represent individual experience occurring in humans' inner worlds. Due to intrapersonal emotion regulation processes, the internal components may or may not be experienced consciously  [33] . The intrapersonal emotion regulation, refers to how internal emotional components are managed  [4] . It originates from psychoanalytical defense mechanism concepts and differs from the cognitive coping mechanism, which refers to a conscious-focused emotion regulation  [34] . People regulate emotions to avoid or decrease experiential and/or behavioral aspects of negative emotions such as anger, sadness, and shame. Also positive emotions may be regulated -for example, if the social situation requires it. The result of intrapersonal emotion regulation is the experienced component of emotions and can be seen as the emotional information that is \"bearable\" within the related situation  [35] . External components of emotions represent communicated information that regulates relationships with others and how they are experienced and represented internally. What is communicated externally is i.a. influenced by social display rules  [36] . Due to both, intrapersonal emotion regulation and social display rules (interpersonal emotion regulation), the connection between internal and external components is not immediate and they do not necessarily match  [1] ,  [32] .\n\nFor modeling human emotions computationally, computer scientists focused on cognitive appraisal theories for emotions  [37] . Some models take emotion regulation into account. One example is MARSSI  [38] , which models appraisal rules, emotion regulation rules, and social signal interpretation, and allows to define multiple possible and plausible relations between these components. Furthermore, MARSSI differentiates between internal and external components of emotions.\n\nRecently,  [12]  presented the DEEP method, a cognitionbased method that focuses on modeling the internal component of emotions. It incorporates an approach to query individual internal emotional experiences and to represent such information computationally. It combines social signals, with context information and information from a post-interaction interview (\"verbalized introspection\"). These different components were modeled with a Bayesian Network constructed from theoretical domain knowledge. They also presented first prediction results for the emotion regulation strategy employed by users. However, their approach is limited in two key aspects which makes it impractical in many application scenarios. First, it requires knowledge from the post-interaction interview, and second, it was not evaluated in a cross-subject scenario. In contrast, we present instruction-tuned LLMs that are able to predict emotion regulation strategies with high accuracy in a cross-subject setting and without having access to information from the verbalized introspection collected post-interaction.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Llms And Emotion Understanding",
      "text": "Large language models (LLMs) have been applied to a variety of tasks related to human affect expression  [39] ,  [40] . One of the most popular of these tasks is sentiment analysis, which commonly involves classifying text into expressing a positive, negative, or neutral sentiment. Transformer-based LLMs such as BERT, RoBERTa, or XLNet have been a key component of state-of-the-art sentiment analysis approaches in recent years  [41] -  [43] . With the success of generative LLMs such as GPT-3.5, GPT4, or Llama, researchers have investigated their utility for sentiment analysis, mainly in zero-shot and few-shot scenarios  [40] ,  [44] . A slightly more complex task compared to sentiment analysis is categorical or dimensional emotion recognition. Language models such as BERT or ROBERTa have been widely applied on these tasks  [13] ,  [45] . GPT3.5 was shown to reach good performance on emotionand emotion cause recognition, but is still outperformed by models fine-tuned for the specific task  [14] . GPT4 improved upon GPT3.5 and is able to outperform an approach based on RoBERTa on tasks such as toxicity detection and opinion extraction, but it still lacks behind on tasks with strong implicit components such as subjectivity of personality estimation  [13] . Emotion recognition in GPT-like models operating in zero-shot scenarios can be highly biased with respect to ground truth definition, prompt construction, or label word selection  [40] .\n\nRecently, instruction tuning of large language models has become a popular technique to adapt generative LLMs to new tasks  [19] . By utilizing Low-rank Adaptation (LoRA)  [20] , fine-tuning models such as Llama2-7B became feasible on a single GPU. This approach was also utilized for tasks related to affect and social behavior  [21] -  [23] . In  [22] , authors used LoRA to create an instruction-tuned variant of Llama2-7B on various social behavior analysis tasks including, among others, sentiment and emotion classification. In their experiments, instruction tuning leads to large performance gains relative to the standard Llama2 model. In  [23] , authors showed that instruction-tuned Llama2 models can clearly outperform all zero or few-shot approaches, including those based on GPT4 across a variety of affect recognition tasks. The utilized emotion datasets of these approaches are entirely textual however, i.e. they do not incorporate nonverbal behavior present in a face-to-face interaction. Despite the importance of nonverbal behavior for the expression of emotions, only few works have made attempts to include nonverbal behavior into the prompts given to LLMs  [21] ,  [46] . In  [46] , authors extracted textual descriptions from clusters of nonverbal behavioral features and used this information in addition to verbal input for sentiment analysis. DialogueLLM  [21]  classified emotions in conversation by constructing prompts describing the conversational and visual context, including nonverbal behavior of the interactants. They fine-tuned Llama2-7B on several emotion recognition datasets, and outperform the previous state of the art on MELD  [24] , IEMOCAP  [25] , and EmoryNLP  [26] . As such, instruction-tuning of LLMs seems to be a promising way to model interactions between verbal-and nonverbal behavior for emotion understanding tasks. To the best of our knowledge, we for the first time apply instruction tuning on prompts generated from multimodal inputs to recognize emotion regulation strategies.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Corpus",
      "text": "For our work, we utilize the recently introduced DEEP corpus, which we received upon request from the authors  [12] . The corpus consists of shame-inducing situations during job interviews. It includes data from 20 expert-annotated videos of ten participants, each in two shame-eliciting situations, comprising 11535 video frames. Shame was elicited in mock job interviews framed as job interview trainings. During these, participants were confronted with a virtual job interviewer (avatar). To elicit shame in participants, the following validated, controlled and pre-evaluated situations  [47]  were:\n\n1) After greeting the interviewee, the job interviewer says: \"Before we start, a quick question. Where did you get that outfit? Somehow it doesn't really suit you.\" Following  [5] , this statement reflects the association personal attractiveness to the self. 2) After the interviewee has presented their experience, the interviewer reacts as follows: \"All the other applicants have already said what you said. You haven't exactly stood out.\" Following  [5] , this statement reflects the association Sense of self. After the interaction with the avatar, participants went through an interview reflecting about their experience, called the \"verbalized introspection\". The DEEP corpus consists of data from different sources of information about each specific shame-eliciting situation. Annotations were done by three trained raters (all with a degree in psychology, one of them an experienced psychotherapist) based on the behavior of the participant in the shame-eliciting interview, the transcribed verbalized introspection, the context and the theoretical knowledge about shame and shame regulation. We utilize the annotations from  [12]  as ground truth as well as input features.\n\nAs ground truth, we use the seven emotion emotion regulation strategy classes (see Table  I  for an overview). In the DEEP corpus, primary and secondary emotion regulation strategy annotations exist, as -similar to emotions  [48]  -several emotion regulation processes can be active at the same time.\n\nFor the purpose of this paper, we chose to focus on the primary emotion regulation strategy exclusively. The input features consist of annotations extracted from nonverbal behavior, verbalized introspection, personal context, and situational context (Table  II ). For the purpose of this paper, we transcribed participants' verbal answers in the shame eliciting situations and added these to the situational context features. For further information on the corpus and the different annotations, we refer to the Supplemental Material of  [12] .",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iv. Approach",
      "text": "We preset our approach based on instruction-tuned Large Language Models (LLMs), as well as our baseline implementation of the Bayesian Networks proposed in  [12] .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Multi-Modal Llm Approach",
      "text": "Our approach uses instruction tuning to fine tune LLMs on prompts created from different sources of information, including verbal and nonverbal behavior as well as contextual information.\n\n1) Prompt generation: We construct one prompt from every frame in the corpus. Similar to  [21] , we generate textual descriptions from different sources of information. An example prompt, broken down into components, is shown in Figure  1 .\n\nIn particular, we provide situational context by describing the particular shame induction situation, and providing a transcript of the verbal exchange up until the current frame. We also clearly define the utterance for which the model is supposed to classify the shame regulation strategy, i.e. the utterance corresponding to the current frame. The nonverbal behavior annotated on the Deep corpus at the current frame is directly translated to textual descriptions. E.g. annotation \"TILT\" for interviewee head behavior is annotated, that would translate to \"The interviewee tilts their head to the side\". Finally, we add a textualization of the personal context variables. The results of verbalized introspection are not part of our default approach, however as we add them in certain experiments, they are included for reference in Figure  1 . For the verbal prompt components, we translated the German transcripts on the DEEP corpus to English, using the mbart-large-50-manyto-many-mmt model  [49] . This model's multilingual capabilities enabled it to surpass the performance of several one-toone translation models. Our experiments confirmed this; we initially tested the smaller opus-mt-en-de model  [50] , but its translations were notably inferior to those produced by the mbart-based model after careful review.\n\n2) Context information: In addition to the prompt generated from each frame, we provide constant context information to the model, explaining the task, situation, and ground truth definitions (i.e. extended definitions of the shame regulation strategies shown in Table  I ). We include this context information in the supplementary material.\n\n3) Utilized LLMs: We utilized a variant of the Llama LLM  [27]  specifically, the Llama-2-7b-chat-hf model  [28] . We opted for this chat-oriented model as its fine-tuning on conversational data enhances its ability to understand the nuances of human dialogue. Preliminary experiments comparing the base Llama model and the chat variant supported this decision, with the latter yielding superior results. Additionally, we incorporated the recent Gemma model  [29]  from Google DeepMind, which reached state-of-the-art performance across various NLP tasks.\n\n4) Training Details: For training, the inputs were the Prompt and the Context. The relevant output was the emotion regulation strategy. To fine-tune our models, we applied the Low-Rank Adaptation of Large Language Models (LoRA) technique  [20] . For LoRA, we follow previous work  [51]  and set r = 8, α = 16 and dropout of 0.1. Further hyperparameters are documented in the supplementary material. We trained both models for 5 epochs. For Llama2-7B we were able to use 16 batches per device, for Gemma only 4. Further training details and code are available online  1  . In total, we made use of three Nvidia A100 GPUs with 40GB VRam each: two for fine-tuning and one for test-time inference. Training lasted for about 2 weeks to generate all results in this paper.\n\n5) Testing: For testing, the model was put first into inference mode, with zero temperature. Then, the Prompt and Context were were fed to the model for each of the instances. We extracted the predicted emotion regulation strategy from the model's response. We checked for anomalies in the response of the LLMs via string matching between the generated and the set of desired output classes, but both LLMs always predicted exactly one emotion regulation strategy label for each sample.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Bayesian Network Model",
      "text": "As a baseline comparison, we make use of the DEEP-BN approach proposed in  [12] . This method conceptualizes a Bayesian network (BN) model representing the Internal Emotion Component, Emotion Regulation and related concepts (see Figure  2 ). Bayesian Networks are graphical models and, compared to other Machine Learning frameworks relatively easy to comprehend and therefore are ideal for modeling theory-based implications and their explanation.\n\nIn general, there are two types of nodes in the DEEP-BN. Blue nodes in the Figure represent information that is updated based on observations in the BN, red nodes represent information that is inferred by the BN. When it comes to understanding the internal emotions it is essential to model the interplay between the Internal Emotion Component, the process of Emotion Regulation, the Experienced Emotion Component, and related Social Signals. We are concerned with a moment in time in the first shame induction situation. The agent tries to induce shame by attacking the interviewee's personal attractiveness: \"Before we start, one short question: Where did you get this outfit? Somehow it doesn't really suit you.\"\n\nThe conversation history up to the current point is:  [Avatar]  Where did you get this outfit from?\n\n[Avatar] Somehow it doesn't really suit you.\n\n[Interviewee] Don't you like it so much? [Interviewee] I thought I felt very comfortable in it, and I find that when you feel comfortable, you always sell yourself a bit better and in the application situation I thought that makes the most sense.\n\nThe current utterance is:\n\n[Interviewee] I thought I felt very comfortable in it, and I find that when you feel comfortable, you always sell yourself a bit better and in the application situation I thought that makes the most sense.\n\nThe interviewee shows the following nonverbal behavior at the current moment: The interviewee looks straight at the interviewer. The interviewee holds their head straight. The interviewee tilts their head to the side. The interviewee shows a non-Duchenne smile, i.e. a smile that concentrates only on the mouth. The interviewee is speaking. The upper body is moved forwards\n\nThe following information was gathered from the qualitative interview after the interaction: The interviewee experiences the following internal emotion at the current moment in time: shame/shyness. The interviewee was aware of feeling ashamed during the current moment in the job interview. During the qualitative interview, the interviewee became aware that they were having the emotion shame during the current moment in the job interview. The interviewee has the intention to maintain the relationship with the avatar.\n\nThe following additional personal information was collected from the interviewer: The mindedness score of the interviewee is 4,77. The interviewee is female.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "The Internal Emotion Component Represents Possible Emotion Classes That -Depending On The Emotion Regulationmay Or May Not Result In A Consciously Experienced Emotion Component. It Is Possible That Individuals Do Not Apply Strong Emotion Regulation Which Results In A Match Between The Internal Emotion Component And The Experienced Emotion",
      "text": "Component. However, it may also be that the Emotion Regulation is strong and unconscious resulting in a completely different Experienced Emotion Component compared to the Internal Emotion Component (e.g., Experiencing anger when unconsciously applying the Emotion Regulation strategy Attack Other but not shame) (see Sec. II-A). The Social Signals represent the observable result of the underlying Experienced Emotion Component and applied Emotion Regulation. Fig.  2 . The DEEP-BN schema constructed based on the DEEP method information. The ground truth of emotion regulation is also part of the verbalized introspection. But since it represents the ground truth (and not a potential input to the model), it is colored pink. The emotion regulation strategies are based on  [5] , while internal emotion component and the experienced emotion are based on the Differential Emotions Scale  [52]  and PANAS-X  [53] .\n\nThe Internal Emotion Component, the Emotion Regulation and the Experienced Emotion Component are influenced by the Personal Context, for example, demographic aspects (e.g., gender), or personality aspects (e.g., mindedness), as well as the Situational Context (i.e. the shame-inducing situation).\n\nThe BN we built based on the DEEP method acts as a benchmark to investigate the capabilities of LLMs to recognize emotion regulation strategies. Even though the main focus of the DEEP method is to provide a deeper understanding of the Internal Emotion Component, the architecture of a BN allows us to easily change the inference target from predicting internal emotions to predicting emotion regulation strategies given a specific emotion.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Evaluation",
      "text": "We trained and evaluated the approaches discussed in section IV on the task of classifying the user's emotion regulation strategy for each frame during the shame induction situations on the DEEP corpus  [12] . To assess the generalizability of the models we employed a LOSO (leave-one-subject-out) evaluation. We evaluate our models in two general settings:\n\n(1) with verbalized introspection, i.e. including the information gathered from the post-interaction interview, and (2) without verbalized introspection. While we expect the first setting to reach higher performance, the second setting respects the demands of application scenarios, where it is usually impractical to perform an additional interview with the user.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Overall Results",
      "text": "Table III presents the models' accuracy and F1 score for each class for our two evaluation scenarios, i.e. with verbalized introspection, and without verbalized introspection. When considering all available information including the verbalized introspection the three models achieved excellent accuracy and F1 scores. However, the BN slightly outperformed the two LLMs in terms of overall accuracy and F1 score, with 0.96 and 0.96 respectively. The BN achieved the highest accuracy and F1 scores for all classes except the Rest class, here the Gemma model was able to surpass the BN with an accuracy of 0.98 and F1 score of 0.95. However, when excluding the information about the verbalized introspection the predictive performance of the BN heavily decreased. The BN was only able to achieve an overall accuracy of 0.23 and F1 score of 0.25. In contrast to that, the LLMs were still able to largely maintain their performance. The Llama2-7B model outperformed the Gemma model for both metrics with an accuracy of 0.84 and a f1-score of 0.84 in comparison to an accuracy of 0.71 and F1 score of 0.72. In addition to comparing the predictive performance of the three models when including or excluding the information about the verbalized introspection we also investigated the influence of the other modalities on the recognition scores. When inspecting the per-class F1 scores we observe a slight trend towards lower performances for less frequent classes across all models. In the case of Llama2-7B without verbalized introspection, F1 scores for Withdrawal (655 frames), Attack self (515 frames), and Attack other (629 frames) are between 0.71 and 0.76, whereas for the remaining classes (each > 1500 frames) they range from 0.84 to 0.88.\n\nIn preliminary experiments, we investigated the feasibility of a zero-shot approach without instruction tuning based on Llama2-7B. We made two observations. First, we were not able to instruct the model to output a classification decision instead of a text generation, making this approach impractical for full-scale quantitative evaluations. Second, on a small test set of five samples from each ground truth class, we observed that the model's outputs are highly biased: in 30 out of 35 cases the model predicted Stabilize self.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Ablation Results",
      "text": "Table IV displays the overall accuracy and weighted F1 score for the three classifiers considering different modalities. When considering verbalized introspection and all other available modalities we already reported that the BN performed the best with the highest scores overall. However, when removing information about nonverbal behavior or even only considering the verbalized introspection the scores of the BN drastically decrease. The removal of nonverbal behavior has very little influence on the accuracy and F1 score of both LLMs. But removing situational context (which includes the transcript) leads to a noticeable decrease in prediction performance for the Llama2-7B and Gemma models. In fact, the accuracy and F1 score similarly decrease as when excluding the transcript only (but keeping the information about the shame inducing situation), indicating that the key information the LLMs utilize is users' verbal behavior. For the BN, the information about the situational context is less important to correctly predict the emotion regulation strategies. Without access to verbalized introspection, the BN only reaches F1 scores between 0.23 to 0.28, while the LLMs can better maintain their performance. As in the condition with available verbalized introspection, removal of situational context or transcript impacts the LLMs most.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Vi. Discussion",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. On Performance",
      "text": "While the Bayesian Network based approach achieved the highest performance when all modalities including verbalized introspection were available, the LLMs where much more robust when modalities were removed. Especially the fact that LLMs proved to be relatively robust to the removal of verbalized introspection information makes them a decidedly better choice in application scenarios where post-interaction interviews are impractical, or online prediction is desired.\n\nIt is crucial to acknowledge that the Bayesian Network (BN) does not include the raw transcript of the job interview, but a distilled representation of the data sourced from the job interview and verbalized introspection. The distillation can be beneficial, especially if it encapsulates the most pertinent information required to identify affective states. However, there's a risk that during this process, potentially relevant details may be excluded. Thus, depending on the quality of abstraction, the removal of modalities may have a less or more detrimental impact on the performance of the BN. For example, the internal emotion component appears to contain the most relevant information by representing the extracted emotion classes. In our case, leaving out the information associated with that component has a detrimental impact on the performance of the BN which cannot be compensated by the information associated with the situational context. This underscores the critical nature of the abstraction approach, particularly in how omitted information impacts the comparative efficacy of the BN and LMM in emotion recognition tasks.\n\nLLMs bear the advantage that they are able to access semantic information from the transcripts of the job interviews. This information enables them to leverage additional nuanced information crucial for affect recognition while the BN has only access to this information in terms of abstract representations gathered from the verbalized introspection. While the BN benefits from incorporating a theory-driven emotion model, the advantage of such a model is contingent upon its access to relevant information resulting from verbal introspection.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Limitations And Future Work",
      "text": "While our results represent an encouraging step towards emotion regulation recognition in realistic scenarios, several limitations remain. Due to the need for verbalized introspection and the complexity of the annotations, the DEEP corpus is limited in size and variability. The ten participants were all having the same cultural background, similar age and were pre-selected having good skills to reflect on their internal experiences. Therefore, the full range of emotion regulation strategies and associated nonverbal behavior may not be captured, which may limit the generalizability of our findings. The reduction of effort by using an LLM to predict emotion regulation strategies, where verbalized introspection seems to be less crucial, seems promising. It would allow for more economical data collection and annotation for future work investigating emotion regulation strategies, however the accuracy of the LLM predictions need to be rigorously evaluated in any new scenario.\n\nThis paper focuses on emotion regulation in validated shame-eliciting situations, limiting the extension of the work to situations where other emotion classes are elicited. Shame is an ideal starting point for this kind of research, both because of the existing extensive theoretical background describing shame regulation strategies  [5] , as well as due to the availability of the DEEP corpus. However, emotions are not only regulated in shame eliciting situations, as most (if not all) emotions are intrapersonally regulated  [33] . Therefore, future work should extend the application of this proposed hybrid approach to other emotion classes, to gain an overall deeper understanding of individual emotional experiences.\n\nFinally, while our proposed approach allows to automatically infer emotion regulation strategies from behavioral descriptions, the descriptions provided with the DEEP dataset were manually annotated. Future work should replace such manual steps with automatic methods. While this might not be easy to do for features extracted from the verbalized introspection, automatic methods to detect facial behavior  [54] , body language  [55] , and to recognize speech  [56]  are available. When using automatic approaches, the set of nonverbal behaviors can also easily be extended, e.g. by detecting backchannels  [57] , or analyzing prosody  [58] .",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, we presented the first evaluation of instructiontuned large language models (LLMs) on the task of recognizing the strategy employed to regulate the emotion shame. We utilized the recently introduced DEEP corpus of shameinducing situations during job interviews, which is annotated with multi-modal behaviors and verbalized introspection gathered after the shame-inducing interactions. Our results indicate that while theory-driven Bayesian Networks perform best when all information is available, LLMs can cope much better with missing information from the verbalized introspection, likely due to their capability to effectively make use of users' verbal behavior. As such, our insights are an important building block towards affective computing systems able to recognize emotion regulation strategies in realistic scenarios.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "The paper employs data from the recently introduced DEEP corpus, which we received upon request from the authors. The DEEP corpus includes recordings of human behaviors in job interviews and subsequent verbal introspection. The collection and analysis of such data involves processing personal and potentially sensitive data. Furthermore, it exposes participants to shameful situations which may lead to negative emotional states. It is crucial to obtain informed consent from the participants, ensure that the employed stimuli don't negatively affect their mental health, implement robust data protection measures and only collect data necessary for the intended affect recognition purposes. Approval for collecting and processing these data was obtained from the ethical review board of the DEEP corpus' authors. The current analysis of multimodal interview data and subsequent verbal introspection is covered by the ethics' approval for the DEEP corpus.\n\nOur model contributes to endeavors aimed at deciphering internal states, particularly benefiting Affective Computing systems reliant on discerning user emotions, such as social training systems or therapeutical assistants. However, the potential for misapplication raises pertinent privacy concerns.\n\nIn our investigation, we solicited insights from participants concerning their internal experience in shame-eliciting situations. Although participants provided consent for research purposes, in practical scenarios, individuals may withhold consent due to apprehensions surrounding the exposure of their internal experiences. Such reluctance could engender adverse ramifications for social interactions and interpersonal relationships.\n\nPrior to engaging with systems employing models for interpreting observable expressions and internal states, it is imperative that users are adequately informed and provide consent regarding functionality, data collection, processing, and attendant risks. The utilization of such systems without the informed consent of individuals subject to observation may result in deleterious outcomes. Unsanctioned application of such technologies may inadvertently gather deeply personal information about individuals' internal experiences, subsequently exposing them to potential harm to their social standing, privacy, and overall well-being.",
      "page_start": 7,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In particular, we provide situational context by describing the",
      "page": 4
    },
    {
      "caption": "Figure 1: For the verbal",
      "page": 4
    },
    {
      "caption": "Figure 2: ). Bayesian Networks are graphical models and,",
      "page": 4
    },
    {
      "caption": "Figure 1: Example prompt consisting of situational context, nonverbal behavior,",
      "page": 5
    },
    {
      "caption": "Figure 2: The DEEP-BN schema constructed based on the DEEP method",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "Abstract—Human emotions are often not\nexpressed directly,",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "highly useful. Such systems include social skill\ntraining sys-"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "but regulated according to internal processes and social display",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "tems [7]–[9] or therapeutic assistance systems [10], [11]. The"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "rules.\nFor\naffective\ncomputing\nsystems,\nan\nunderstanding\nof",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "recently introduced DEEP\napproach was\nthe first\nattempt\nto"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "how users\nregulate\ntheir\nemotions\ncan\nbe\nhighly\nuseful,\nfor",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "create a computational model of emotion regulation, focusing"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "example\nto provide\nfeedback in job interview training,\nor\nin",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "psychotherapeutic scenarios. However, at present no method to",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "on the emotion shame elicited in job interviews [12]. While the"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "automatically classify different emotion regulation strategies in a",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "authors presented a Bayesian Network (BN) model\nto classify"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "cross-user scenario exists. At the same time, recent studies showed",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "emotion\nregulation\nstrategies,\ntheir\napproach\nhad\ntwo\nkey"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "that instruction-tuned Large Language Models (LLMs) can reach",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "limitations prohibiting application in realistic scenarios. First,"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "impressive performance\nacross\na\nvariety\nof\naffect\nrecognition",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "they require results of an extensive analysis of\nin-depth post-"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "tasks\nsuch\nas\ncategorical\nemotion\nrecognition\nor\nsentiment",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "analysis. While these results are promising,\nit remains unclear to",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "interaction interviews as input. Second,\nthey did not evaluate"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "what extent\nthe representational power of LLMs can be utilized",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "their model\nin a subject-independent scenario."
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "in the more\nsubtle\ntask of\nclassifying users’\ninternal\nemotion",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "Recent studies indicate that generative large language mod-"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "regulation strategy. To close this gap, we make use of the recently",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "els\n(LLMs)\nare\nable\nto,\nin a\ncertain sense, understand hu-"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "introduced DEEP corpus\nfor modeling the social display of\nthe",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "man\nemotion\nin\nsocial\nsituations.\nIn\nzero-shot\nscenarios,"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "emotion shame, where each point in time is annotated with one of",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "seven different emotion regulation classes. We fine-tune Llama2-",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "GPT3.5 and GPT4 were successfully applied across a variety"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "7B as well as the recently introduced Gemma model using Low-",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "of emotion-related tasks,\nincluding sentiment analysis, emo-"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "rank Optimization on prompts generated from different sources",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "tion and emotion cause\nrecognition,\ntoxicity detection,\nand"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "of\ninformation on the DEEP corpus. These\ninclude verbal and",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "opinion\nextraction,\nalbeit\nthey\nare\noften\nstill\noutperformed"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "nonverbal behavior, person factors, as well as\nthe results of an",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "by approaches directly trained on the\nrespective\ntasks\n[13],"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "in-depth interview after the interaction. Our results show, that a",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "fine-tuned Llama2-7B LLM is able to classify the utilized emotion",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "[14].\nIn contrast\nto zero-shot\nscenarios,\ninstruction-tuning is"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "regulation strategy with high accuracy\n(0.84) without needing",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "an\neffective means\nto\nutilize\nthe\nrepresentational\npower\nof"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "access\nto data from post-interaction interviews. This represents",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "generative LLMs and at the same time adapt to a specific target"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "a\nsignificant\nimprovement\nover previous\napproaches based on",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "task [15]–[19]. Using Low-rank Adaptation (LoRA) [20],\nthis"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "Bayesian Networks and highlights\nthe\nimportance of modeling",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "process is computationally efficient, and was already utilized"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "verbal behavior in emotion regulation.",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "Index Terms—emotion regulation, large language models, emo-",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "for\ntasks\nrelated to affect\nand social behavior\n[21]–[23].\nIn"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "tion recognition, bayesian networks",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "particular, DialogueLLM [21]\nreached state-of-the-art\nresults"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "for emotion recognition on the MELD [24],\nIEMOCAP [25],"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "I.\nINTRODUCTION",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": ""
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "and EmoryNLP [26] datasets. While these results are encour-"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "One key finding of emotion research is that\nthere is no one-",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "aging,\nit\nis unclear to what extent\ninstruction-tuned LLMs can"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "to-one mapping of displayed emotional expressions\nto inter-",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "be used to classify emotion regulation strategies. In contrast to"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "nally experienced emotions\n[1]. Emotions do not necessarily",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "expressions of emotion,\nthese strategies reflect\ninner processes"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "become visible [2], nor consciously experienced [3]–[5]. One",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "that may not have distinct observable cues and are believed to"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "reason for this is emotion regulation, which encompasses vari-",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "be heavily related to nonverbal aspects of behavior\n[12]."
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "ous conscious or unconscious strategies that\nindividuals use to",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "In our work, we investigate to what extent\ninstruction-tuned"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "influence their emotional experience [6]. Especially unpleasant",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "LLMs are capable of classifying the strategies employed by"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "emotions such as shame are regulated to protect\nthe self\n[3]–",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "humans\nto regulate shame. To this end, we make use of\nthe"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "[5]. For many\naffective\ncomputing\nsystems,\nknowledge\nof",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "recently\nintroduced DEEP\ncorpus\ncomprising\nrecordings\nof"
        },
        {
          "patrick.gebhard@dfki.de\njan.alexandersson@dfki.de": "users’\nemotion regulation strategies has\nthe potential\nto be",
          "tanja.schneeberger@dfki.de\nelisabeth.andre@uni-a.de": "human behavior in shame inducing situations and self-reported"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "towards affective computing systems that can recognize human": "emotion regulation strategies in realistic scenarios.",
          "internal emotional experiences and to represent such informa-": "tion computationally.\nIt combines social signals, with context"
        },
        {
          "towards affective computing systems that can recognize human": "Our specific contributions are three-fold.",
          "internal emotional experiences and to represent such informa-": "information and information from a post-interaction interview"
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "(“verbalized introspection”). These different components were"
        },
        {
          "towards affective computing systems that can recognize human": "1) We utilize LLMs instruction-tuned on prompts incorpo-",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "modeled with a Bayesian Network constructed from theoret-"
        },
        {
          "towards affective computing systems that can recognize human": "rating multimodal behavior\nto classify peoples’",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "ical domain knowledge. They also presented first prediction"
        },
        {
          "towards affective computing systems that can recognize human": "gies to regulate the emotion shame.",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "results for the emotion regulation strategy employed by users."
        },
        {
          "towards affective computing systems that can recognize human": "2)\nIn\nthe\nfirst\ncross-user\nevaluations\non\nthe",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "However,\ntheir approach is limited in two key aspects which"
        },
        {
          "towards affective computing systems that can recognize human": "introduced DEEP\ncorpus\n[12],\nour\napproach",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "makes\nit\nimpractical\nin many application scenarios. First,\nit"
        },
        {
          "towards affective computing systems that can recognize human": "forms\nthe\nprevious\nstate\nof\nthe\nart\nbased\non",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "requires knowledge\nfrom the post-interaction interview,\nand"
        },
        {
          "towards affective computing systems that can recognize human": "constructed Bayesian Networks when information from",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "second,\nit was not\nevaluated in a\ncross-subject\nscenario.\nIn"
        },
        {
          "towards affective computing systems that can recognize human": "post-interaction interviews is not available.",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "contrast, we present\ninstruction-tuned LLMs\nthat are able to"
        },
        {
          "towards affective computing systems that can recognize human": "3) We conduct extensive ablation experiments, highlighting",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "predict emotion regulation strategies with high accuracy in a"
        },
        {
          "towards affective computing systems that can recognize human": "the impact of different modalities on performance.",
          "internal emotional experiences and to represent such informa-": ""
        },
        {
          "towards affective computing systems that can recognize human": "",
          "internal emotional experiences and to represent such informa-": "cross-subject setting and without having access to information"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "information about individual experience [12]. Inspired by Dia-": "logueLLM [21], we encode participants’ multimodal behavior",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "One example is MARSSI\n[38], which models appraisal\nrules,"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "into\nprompts\nthat\nare\nused\nfor\ninstruction-tuning Llama2-",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "emotion regulation rules, and social signal\ninterpretation, and"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "7B [27],\n[28] and Gemma [29] models with LoRA [20]. We",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "allows to define multiple possible and plausible relations be-"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "present the first cross-user evaluations on the DEEP corpus and",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "tween these components. Furthermore, MARSSI differentiates"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "show that our LLM-based approach can reach an accuracy of",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "between internal and external components of emotions."
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "0.84 in emotion regulation classification without access to any",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "Recently,\n[12]\npresented\nthe DEEP method,\na\ncognition-"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "information from informative but\nimpractical post-interaction",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "based method that focuses on modeling the internal component"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "interviews. As\nsuch, our\nresults\nrepresent\nan important\nstep",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "of emotions.\nIt\nincorporates an approach to query individual"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "towards affective computing systems that can recognize human",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "internal emotional experiences and to represent such informa-"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "emotion regulation strategies in realistic scenarios.",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "tion computationally.\nIt combines social signals, with context"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "Our specific contributions are three-fold.",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "information and information from a post-interaction interview"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "(“verbalized introspection”). These different components were"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "1) We utilize LLMs instruction-tuned on prompts incorpo-",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "modeled with a Bayesian Network constructed from theoret-"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "rating multimodal behavior\nto classify peoples’\nstrate-",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "ical domain knowledge. They also presented first prediction"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "gies to regulate the emotion shame.",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "results for the emotion regulation strategy employed by users."
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "2)\nIn\nthe\nfirst\ncross-user\nevaluations\non\nthe\nrecently",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "However,\ntheir approach is limited in two key aspects which"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "introduced DEEP\ncorpus\n[12],\nour\napproach\noutper-",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "makes\nit\nimpractical\nin many application scenarios. First,\nit"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "forms\nthe\nprevious\nstate\nof\nthe\nart\nbased\non\nexpert-",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "requires knowledge\nfrom the post-interaction interview,\nand"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "constructed Bayesian Networks when information from",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "second,\nit was not\nevaluated in a\ncross-subject\nscenario.\nIn"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "post-interaction interviews is not available.",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "contrast, we present\ninstruction-tuned LLMs\nthat are able to"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "3) We conduct extensive ablation experiments, highlighting",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "predict emotion regulation strategies with high accuracy in a"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "the impact of different modalities on performance.",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": ""
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "cross-subject setting and without having access to information"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "II. RELATED WORK",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "from the verbalized introspection collected post-interaction."
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "A. Model of Emotions and Emotion Regulation",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "B. LLMs and Emotion Understanding"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "There is a variety of emotion models both in psychology",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "Large language models (LLMs) have been applied to a vari-"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "[30]\nand affective\ncomputing [31].\nIn our work, we\nfollow",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "ety of tasks related to human affect expression [39], [40]. One"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "a model of emotions\nthat differentiates between internal and",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "of the most popular of these tasks is sentiment analysis, which"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "external\ncomponents\ninspired\nby\ncognitive\npsychoanalysis",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "commonly involves classifying text\ninto expressing a positive,"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "Internal\ncomponents\nof\nemotions\n[32].\nare\nnot\ndirectly\nob-",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "negative, or neutral sentiment. Transformer-based LLMs such"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "servable as\nthey represent\nindividual experience occurring in",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "as BERT, RoBERTa, or XLNet have been a key component"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "humans’ inner worlds. Due to intrapersonal emotion regulation",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "of\nstate-of-the-art\nsentiment\nanalysis\napproaches\nin\nrecent"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "processes,\nthe internal components may or may not be experi-",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "years\n[41]–[43]. With the success of generative LLMs\nsuch"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "enced consciously [33]. The intrapersonal emotion regulation,",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "as GPT-3.5, GPT4, or Llama,\nresearchers have\ninvestigated"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "refers\nto\nhow internal\nemotional\ncomponents\nare managed",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "their utility for\nsentiment\nanalysis, mainly in zero-shot\nand"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "[4].\nIt originates\nfrom psychoanalytical defense mechanism",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "few-shot\nscenarios\n[40],\n[44]. A slightly more complex task"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "concepts\nand differs\nfrom the\ncognitive\ncoping mechanism,",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "compared to sentiment analysis is categorical or dimensional"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "which refers to a conscious-focused emotion regulation [34].",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "emotion\nrecognition.\nLanguage models\nsuch\nas BERT\nor"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "People\nregulate\nemotions\nto\navoid\nor\ndecrease\nexperiential",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "ROBERTa have been widely applied on these tasks [13], [45]."
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "and/or behavioral aspects of negative emotions such as anger,",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "GPT3.5 was\nshown to reach good performance on emotion-"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "sadness, and shame. Also positive emotions may be regulated",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "and emotion cause\nrecognition, but\nis\nstill outperformed by"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "– for example,\nif\nthe social situation requires it. The result of",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "models fine-tuned for\nthe specific task [14]. GPT4 improved"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "intrapersonal emotion regulation is the experienced component",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "upon GPT3.5 and is\nable\nto outperform an approach based"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "of\nemotions\nand can be\nseen as\nthe\nemotional\ninformation",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "on RoBERTa on tasks such as toxicity detection and opinion"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "that\nis “bearable” within the related situation [35]. External",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "extraction, but it still lacks behind on tasks with strong implicit"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "components of emotions represent communicated information",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "components such as subjectivity of personality estimation [13]."
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "that\nregulates\nrelationships with\nothers\nand\nhow they\nare",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "Emotion recognition in GPT-like models operating in zero-shot"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "experienced and represented internally. What is communicated",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "scenarios can be highly biased with respect\nto ground truth"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "externally is i.a. influenced by social display rules [36]. Due to",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "definition, prompt construction, or\nlabel word selection [40]."
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "both,\nintrapersonal emotion regulation and social display rules",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "Recently,\ninstruction tuning of\nlarge language models has"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "(interpersonal\nemotion\nregulation),\nthe\nconnection\nbetween",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "become a popular technique to adapt generative LLMs to new"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "internal and external components\nis not\nimmediate and they",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "tasks\n[19]. By utilizing Low-rank Adaptation (LoRA)\n[20],"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "do not necessarily match [1],\n[32].",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "fine-tuning models such as Llama2-7B became feasible on a"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "For modeling human emotions computationally, computer",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "single GPU. This approach was also utilized for\ntasks related"
        },
        {
          "information about individual experience [12]. Inspired by Dia-": "scientists focused on cognitive appraisal\ntheories for emotions",
          "[37].\nSome models\ntake\nemotion\nregulation\ninto\naccount.": "to affect and social behavior\n[21]–[23].\nIn [22], authors used"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "strategy classes"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "primary"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "exist,"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "emotion regulation processes can be active at"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "For the purpose of this paper, we chose to focus on the primary"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "regulation"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "consist of annotations extracted from nonverbal behavior, ver-"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "balized introspection, personal context, and situational context"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "II).\nFor"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "participants’ verbal answers"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "and added these to the situational context features. For further"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "information on the corpus and the different annotations, we"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": "to the Supplemental Material of"
        },
        {
          "As ground truth, we use the seven emotion emotion regulation": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "INCLUDING THEIR\nGROUND TRUTH CLASSES ON THE DEEP CORPUS [12],"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "various social behavior analysis tasks including, among others,",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "DEFINITION, AS WELL AS POSSIBLE EXPERIENCED COMPONENTS AND"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "sentiment\nand\nemotion\nclassification.\nIn\ntheir\nexperiments,",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "NONVERBAL BEHAVIOUR."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "instruction tuning leads\nto large performance gains\nrelative",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "to the standard Llama2 model.\nIn [23], authors\nshowed that",
          "TABLE I": "WITHDRAWAL (655 frames) Cut off the current situation so there is"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "no more external\ninfluence or stimuli. Wish to hide,\nleave or escape."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "instruction-tuned Llama2 models\ncan clearly outperform all",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "Experienced emotional components: distress,\nfear"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "zero or\nfew-shot approaches,\nincluding those based on GPT4",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "Nonverbal Behavior:\nfreezing,\nlip biting, gaze/head aversion, silence"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "across a variety of affect\nrecognition tasks. The utilized emo-",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "ATTACK SELF (515 frames) Do to yourself what others may do to"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "tion datasets of these approaches are entirely textual however,",
          "TABLE I": "you, establishing impression to control\nthe situation."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "i.e.\nthey do not\nincorporate nonverbal behavior present\nin a",
          "TABLE I": "Experienced emotional components: disgust"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "Nonverbal Behavior:\nfacial expression of disgust"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "face-to-face interaction. Despite the importance of nonverbal",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "ATTACK OTHER (629 frames) Transfer\nthe diminishment of\nself-"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "behavior for the expression of emotions, only few works have",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "esteem to the person (object) who caused it by diminishing the other"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "made attempts to include nonverbal behavior into the prompts",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "person."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "given to LLMs\n[21],\n[46].\nIn [46], authors extracted textual",
          "TABLE I": "Experienced emotional components: anger"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "Nonverbal Behavior:\nlearn forward, gestures of power, facial expression"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "descriptions\nfrom clusters\nof\nnonverbal\nbehavioral\nfeatures",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "of anger"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "and\nused\nthis\ninformation\nin\naddition\nto\nverbal\ninput\nfor",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "AVOIDANCE (1650\nframes) Acting\naccording\nthe\nprinciple\n“fool"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "sentiment analysis. DialogueLLM [21] classified emotions in",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "others,\nfool myself”."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "conversation by constructing prompts describing the conversa-",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "Experienced emotional components:\njoy"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "tional and visual context,\nincluding nonverbal behavior of the",
          "TABLE I": "Nonverbal Behavior: gaze/head aversion,\nlean backwards, facial expres-"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "sion of\njoy/surprise, smile"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "interactants. They fine-tuned Llama2-7B on several emotion",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "recognition datasets, and outperform the previous state of\nthe",
          "TABLE I": "DEPRECIATION (1911 frames) Deevaluation of\ninteraction partner"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "due to different\n(or even contrary) values and ideals."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "art on MELD [24], IEMOCAP [25], and EmoryNLP [26]. As",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "Experienced emotional components: disgust, contempt"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "such,\ninstruction-tuning of LLMs\nseems\nto be\na promising",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "Nonverbal Behavior: raised eyebrows, smile, facial expression of disgust"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "way\nto model\ninteractions\nbetween\nverbal-\nand\nnonverbal",
          "TABLE I": "and contempt"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "behavior\nfor\nemotion\nunderstanding\ntasks. To\nthe\nbest\nof",
          "TABLE I": "STABILIZE SELF (3593 frames) Attempt\nto react\nin a way that\nis"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "our knowledge, we for\nthe first\ntime apply instruction tuning",
          "TABLE I": "compliant with the (ideal) self by accepting disagreement between job"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "interviewer and person."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "on prompts generated from multimodal\ninputs\nto recognize",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "Experienced emotional components: pride"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "emotion regulation strategies.",
          "TABLE I": "Nonverbal Behavior: no display of uncertainty, direct gaze"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "REST (2582 frames) No identified emotion regulation strategy."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "III. CORPUS",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "For\nour work, we\nutilize\nthe\nrecently\nintroduced DEEP",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "corpus, which we received upon request from the authors [12].",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "them an experienced psychotherapist) based on the behavior"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "The corpus consists of\nshame-inducing situations during job",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "of\nthe participant\nin the\nshame-eliciting interview,\nthe\ntran-"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "interviews.\nIt\nincludes data from 20 expert-annotated videos",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "scribed verbalized introspection, the context and the theoretical"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "of\nten\nparticipants,\neach\nin\ntwo\nshame-eliciting\nsituations,",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "knowledge about shame and shame regulation. We utilize the"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "comprising 11535 video frames. Shame was elicited in mock",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "annotations from [12] as ground truth as well as input features."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "job interviews framed as job interview trainings. During these,",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "As ground truth, we use the seven emotion emotion regulation"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "participants were\nconfronted with\na\nvirtual\njob\ninterviewer",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "strategy classes\n(see Table I\nfor an overview).\nIn the DEEP"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "(avatar). To elicit\nshame\nin participants,\nthe\nfollowing vali-",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "corpus,\nprimary\nand\nsecondary\nemotion\nregulation\nstrategy"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "dated, controlled and pre-evaluated situations [47] were:",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "annotations\nexist,\nas\n–\nsimilar\nto\nemotions\n[48]\n–\nseveral"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "1) After greeting the interviewee,\nthe job interviewer says:",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "emotion regulation processes can be active at\nthe same time."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "“Before we\nstart,\na\nquick\nquestion. Where\ndid\nyou",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "For the purpose of this paper, we chose to focus on the primary"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "get\nthat\noutfit? Somehow it\ndoesn’t\nreally\nsuit\nyou.”",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "emotion\nregulation\nstrategy\nexclusively. The\ninput\nfeatures"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "Following\n[5],\nthis\nstatement\nreflects\nthe\nassociation",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "consist of annotations extracted from nonverbal behavior, ver-"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "personal attractiveness to the self.",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "balized introspection, personal context, and situational context"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "2) After the interviewee has presented their experience,\nthe",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "(Table\nII).\nFor\nthe\npurpose\nof\nthis\npaper, we\ntranscribed"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "interviewer\nreacts as\nfollows: “All\nthe other applicants",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "participants’ verbal answers\nin the shame eliciting situations"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "have\nalready said what you said. You haven’t\nexactly",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "and added these to the situational context features. For further"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "stood\nout.” Following\n[5],\nthis\nstatement\nreflects\nthe",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "information on the corpus and the different annotations, we"
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "association Sense of self.",
          "TABLE I": ""
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "",
          "TABLE I": "refer\nto the Supplemental Material of\n[12]."
        },
        {
          "LoRA to create an instruction-tuned variant of Llama2-7B on": "After\nthe\ninteraction with\nthe\navatar,\nparticipants went",
          "TABLE I": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE II": "ANNOTATED INPUT FEATURES ON THE DEEP [12] CORPUS."
        },
        {
          "TABLE II": "Observation of external components of emotions that are encoded in social signals in the specific situation."
        },
        {
          "TABLE II": "Speech, Utterance, Facial Expression, Gaze, Eyes, Smile, Smile Control, Head, Head Tilt, Upper body, Shame display"
        },
        {
          "TABLE II": "Self-reports that\nreflect a person’s subjective experience gathered in semi-structured interviews after\nthe specific situation"
        },
        {
          "TABLE II": "with the aid of video material of\nthe experienced situation."
        },
        {
          "TABLE II": "Relationship management, Shame awareness, Experienced emotion,\nInternal emotion component, Display rule"
        },
        {
          "TABLE II": "Personal context variables."
        },
        {
          "TABLE II": "Gender, Mindedness score"
        },
        {
          "TABLE II": "Situational context variables."
        },
        {
          "TABLE II": "Situation (first vs. second shame induction), Conversation transcript"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Situational Context\nSituational context variables.": "Situation (first vs. second shame induction), Conversation transcript"
        },
        {
          "Situational Context\nSituational context variables.": "A. Multi-modal LLM Approach"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "Our\napproach uses\ninstruction tuning to fine\ntune LLMs"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "on\nprompts\ncreated\nfrom different\nsources\nof\ninformation,"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "including verbal and nonverbal behavior as well as contextual"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "information."
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "1) Prompt generation: We construct one prompt from every"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "frame in the corpus. Similar\nto [21], we generate textual de-"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "scriptions from different sources of\ninformation. An example"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "prompt, broken down into components,\nis shown in Figure 1."
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "In particular, we provide situational context by describing the"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "particular shame induction situation, and providing a transcript"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "of\nthe verbal\nexchange up until\nthe\ncurrent\nframe. We\nalso"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "clearly define the utterance for which the model\nis supposed"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "to\nclassify\nthe\nshame\nregulation\nstrategy,\ni.e.\nthe\nutterance"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "corresponding to the current\nframe. The nonverbal behavior"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "annotated on the Deep corpus at\nthe current\nframe is directly"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "translated to textual descriptions. E.g. annotation “TILT” for"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "interviewee head behavior\nis annotated,\nthat would translate"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "to “The interviewee tilts\ntheir head to the side”. Finally, we"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "add\na\ntextualization\nof\nthe\npersonal\ncontext\nvariables. The"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "results of verbalized introspection are not part of our default"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "approach, however\nas we\nadd them in certain experiments,"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "they are\nincluded for\nreference\nin Figure 1. For\nthe verbal"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "prompt components, we translated the German transcripts on"
        },
        {
          "Situational Context\nSituational context variables.": "the DEEP corpus to English, using the mbart-large-50-many-"
        },
        {
          "Situational Context\nSituational context variables.": "to-many-mmt model\n[49]. This model’s multilingual capabil-"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "ities enabled it\nto surpass the performance of several one-to-"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "one translation models. Our experiments confirmed this; we"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "initially tested the smaller opus-mt-en-de model\n[50], but\nits"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "translations were notably inferior\nto those produced by the"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "mbart-based model after careful\nreview."
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "2) Context information:\nIn addition to the prompt generated"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "from each frame, we provide constant context\ninformation to"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "the model,\nexplaining\nthe\ntask,\nsituation,\nand\nground\ntruth"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "definitions\n(i.e. extended definitions of\nthe shame regulation"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "strategies shown in Table I). We include this context\ninforma-"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "tion in the supplementary material."
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "3) Utilized LLMs: We utilized a variant of the Llama LLM"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "[27] specifically, the Llama-2-7b-chat-hf model [28]. We opted"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "for this chat-oriented model as its fine-tuning on conversational"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "data enhances its ability to understand the nuances of human"
        },
        {
          "Situational Context\nSituational context variables.": ""
        },
        {
          "Situational Context\nSituational context variables.": "dialogue. Preliminary experiments comparing the base Llama"
        },
        {
          "Situational Context\nSituational context variables.": "model and the chat variant\nsupported this decision, with the"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Situational Context:": "We are concerned with a moment in time in the first shame induction"
        },
        {
          "Situational Context:": "situation. The agent tries to induce shame by attacking the"
        },
        {
          "Situational Context:": "interviewee’s personal attractiveness: “Before we start, one short"
        },
        {
          "Situational Context:": "question: Where did you get this outfit? Somehow it doesn’t really suit"
        },
        {
          "Situational Context:": "you.”"
        },
        {
          "Situational Context:": "The conversation history up to the current point is:"
        },
        {
          "Situational Context:": "[Avatar] Where did you get this outfit from?"
        },
        {
          "Situational Context:": "[Avatar] Somehow it doesn't really suit you."
        },
        {
          "Situational Context:": "[Interviewee] Don't you like it so much?"
        },
        {
          "Situational Context:": "[Interviewee] I thought I felt very comfortable in it, and I find that when"
        },
        {
          "Situational Context:": "you feel comfortable, you always sell yourself a bit better and in the"
        },
        {
          "Situational Context:": "application situation I thought that makes the most sense."
        },
        {
          "Situational Context:": "The current utterance is:"
        },
        {
          "Situational Context:": "[Interviewee] I thought I felt very comfortable in it, and I find that when"
        },
        {
          "Situational Context:": "you feel comfortable, you always sell yourself a bit better and in the"
        },
        {
          "Situational Context:": "application situation I thought that makes the most sense."
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "Nonverbal Behavior:"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "The interviewee shows the following nonverbal behavior at the current"
        },
        {
          "Situational Context:": "moment: The interviewee looks straight at the interviewer. The"
        },
        {
          "Situational Context:": "interviewee holds their head straight. The interviewee tilts their head"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "to the side. The interviewee shows a non-Duchenne smile, i.e. a smile"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "that concentrates only on the mouth. The interviewee is speaking. The"
        },
        {
          "Situational Context:": "upper body is moved forwards"
        },
        {
          "Situational Context:": "Verbalized Introspection:"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "The following information was gathered from the qualitative interview"
        },
        {
          "Situational Context:": "after the interaction: The interviewee experiences the following"
        },
        {
          "Situational Context:": "internal emotion at the current moment in time: shame/shyness. The"
        },
        {
          "Situational Context:": "interviewee was aware of feeling ashamed during the current moment"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "in the job interview. During the qualitative interview, the interviewee"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "became aware that they were having the emotion shame during the"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "current moment in the job interview. The interviewee has the intention"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "to maintain the relationship with the avatar."
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "Personal Context:"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "The following additional personal information was collected from the"
        },
        {
          "Situational Context:": "interviewer: The mindedness score of the interviewee is 4,77. The"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "interviewee is female."
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "Fig. 1. Example prompt consisting of situational context, nonverbal behavior,"
        },
        {
          "Situational Context:": "verbalized introspection and personal context. The situational context\nincor-"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "porates a transcript\n(below the dotted line)."
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "The Internal Emotion Component\nrepresents possible emo-"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "tion classes\nthat – depending on the Emotion Regulation –"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "may or may not\nresult\nin a consciously Experienced Emotion"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "Component. It\nis possible that\nindividuals do not apply strong"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "Emotion Regulation which\nresults\nin\na match\nbetween\nthe"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "Internal Emotion Component\nand the Experienced Emotion"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "Component. However,\nit may also be that\nthe Emotion Reg-"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "ulation is\nstrong and unconscious\nresulting in a\ncompletely"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "different Experienced Emotion Component\ncompared to the"
        },
        {
          "Situational Context:": ""
        },
        {
          "Situational Context:": "Internal Emotion Component\n(e.g., Experiencing anger when"
        },
        {
          "Situational Context:": "unconsciously applying the Emotion Regulation strategy At-"
        },
        {
          "Situational Context:": "tack Other but not shame) (see Sec. II-A). The Social Signals"
        },
        {
          "Situational Context:": "represent\nthe observable result of\nthe underlying Experienced"
        },
        {
          "Situational Context:": "Emotion Component and applied Emotion Regulation."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE III": ""
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "Avoidance"
        },
        {
          "TABLE III": "ACC"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "0.99"
        },
        {
          "TABLE III": "0.98"
        },
        {
          "TABLE III": "0.98"
        },
        {
          "TABLE III": ""
        },
        {
          "TABLE III": "0.79"
        },
        {
          "TABLE III": "0.92"
        },
        {
          "TABLE III": "0.96"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Bayesian Net": "F1",
          "Llama2-7B": "ACC",
          "Gemma": "F1"
        },
        {
          "Bayesian Net": "",
          "Llama2-7B": "",
          "Gemma": ""
        },
        {
          "Bayesian Net": "0.96",
          "Llama2-7B": "0.89",
          "Gemma": "0.93"
        },
        {
          "Bayesian Net": "0.68",
          "Llama2-7B": "0.88",
          "Gemma": "0.93"
        },
        {
          "Bayesian Net": "0.85",
          "Llama2-7B": "0.49",
          "Gemma": "0.63"
        },
        {
          "Bayesian Net": "—",
          "Llama2-7B": "0.45",
          "Gemma": "0.64"
        },
        {
          "Bayesian Net": "0.01",
          "Llama2-7B": "0.87",
          "Gemma": "0.87"
        },
        {
          "Bayesian Net": "0.16",
          "Llama2-7B": "0.54",
          "Gemma": "0.56"
        },
        {
          "Bayesian Net": "",
          "Llama2-7B": "",
          "Gemma": ""
        },
        {
          "Bayesian Net": "0.25",
          "Llama2-7B": "0.84",
          "Gemma": "0.72"
        },
        {
          "Bayesian Net": "0.27",
          "Llama2-7B": "0.44",
          "Gemma": "0.47"
        },
        {
          "Bayesian Net": "0.23",
          "Llama2-7B": "0.38",
          "Gemma": "0.38"
        },
        {
          "Bayesian Net": "—",
          "Llama2-7B": "0.40",
          "Gemma": "0.37"
        },
        {
          "Bayesian Net": "0.28",
          "Llama2-7B": "0.42",
          "Gemma": "0.46"
        },
        {
          "Bayesian Net": "0.25",
          "Llama2-7B": "0.47",
          "Gemma": "0.46"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "No nonverbal behavior\n0.25": "Only nonverbal behavior\n0.25",
          "0.28\n0.42\n0.44\n0.44\n0.46": "0.25\n0.47\n0.50\n0.44\n0.46"
        },
        {
          "No nonverbal behavior\n0.25": "introspection the three models achieved excellent accuracy and",
          "0.28\n0.42\n0.44\n0.44\n0.46": "frames)\nthey range from 0.84 to 0.88."
        },
        {
          "No nonverbal behavior\n0.25": "F1 scores. However,\nthe BN slightly outperformed the\ntwo",
          "0.28\n0.42\n0.44\n0.44\n0.46": "In preliminary experiments, we investigated the feasibility"
        },
        {
          "No nonverbal behavior\n0.25": "LLMs in terms of overall accuracy and F1 score, with 0.96 and",
          "0.28\n0.42\n0.44\n0.44\n0.46": "of a zero-shot approach without\ninstruction tuning based on"
        },
        {
          "No nonverbal behavior\n0.25": "0.96 respectively. The BN achieved the highest accuracy and",
          "0.28\n0.42\n0.44\n0.44\n0.46": "Llama2-7B. We made\ntwo observations. First, we were not"
        },
        {
          "No nonverbal behavior\n0.25": "F1 scores for all classes except the Rest class, here the Gemma",
          "0.28\n0.42\n0.44\n0.44\n0.46": "able to instruct\nthe model\nto output a classification decision"
        },
        {
          "No nonverbal behavior\n0.25": "model was able to surpass the BN with an accuracy of 0.98 and",
          "0.28\n0.42\n0.44\n0.44\n0.46": "instead of a text generation, making this approach impractical"
        },
        {
          "No nonverbal behavior\n0.25": "F1 score of 0.95. However, when excluding the information",
          "0.28\n0.42\n0.44\n0.44\n0.46": "for\nfull-scale quantitative evaluations. Second, on a small\ntest"
        },
        {
          "No nonverbal behavior\n0.25": "about\nthe verbalized introspection the predictive performance",
          "0.28\n0.42\n0.44\n0.44\n0.46": "set of five samples from each ground truth class, we observed"
        },
        {
          "No nonverbal behavior\n0.25": "of the BN heavily decreased. The BN was only able to achieve",
          "0.28\n0.42\n0.44\n0.44\n0.46": "that\nthe model’s outputs are highly biased:\nin 30 out of 35"
        },
        {
          "No nonverbal behavior\n0.25": "an overall accuracy of 0.23 and F1 score of 0.25.\nIn contrast",
          "0.28\n0.42\n0.44\n0.44\n0.46": "cases the model predicted Stabilize self."
        },
        {
          "No nonverbal behavior\n0.25": "to that,\nthe LLMs were\nstill\nable\nto largely maintain their",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "B. Ablation Results"
        },
        {
          "No nonverbal behavior\n0.25": "performance. The Llama2-7B model outperformed the Gemma",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "Table\nIV displays\nthe\noverall\naccuracy\nand weighted F1"
        },
        {
          "No nonverbal behavior\n0.25": "model for both metrics with an accuracy of 0.84 and a f1-score",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "score for the three classifiers considering different modalities."
        },
        {
          "No nonverbal behavior\n0.25": "of 0.84 in comparison to an accuracy of 0.71 and F1 score of",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "When considering verbalized introspection and all other avail-"
        },
        {
          "No nonverbal behavior\n0.25": "0.72.\nIn addition to comparing the predictive performance of",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "able modalities we already reported that the BN performed the"
        },
        {
          "No nonverbal behavior\n0.25": "the three models when including or excluding the information",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "best with the highest scores overall. However, when removing"
        },
        {
          "No nonverbal behavior\n0.25": "about\nthe\nverbalized\nintrospection we\nalso\ninvestigated\nthe",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "information about nonverbal behavior or even only considering"
        },
        {
          "No nonverbal behavior\n0.25": "influence of\nthe other modalities on the\nrecognition scores.",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "the verbalized introspection the recognition scores of\nthe BN"
        },
        {
          "No nonverbal behavior\n0.25": "When inspecting the per-class F1 scores we observe a slight",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "drastically decrease. The removal of nonverbal behavior has"
        },
        {
          "No nonverbal behavior\n0.25": "trend\ntowards\nlower\nperformances\nfor\nless\nfrequent\nclasses",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "very little\ninfluence on the\naccuracy and F1 score of both"
        },
        {
          "No nonverbal behavior\n0.25": "across all models. In the case of Llama2-7B without verbalized",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "LLMs. But\nremoving situational context\n(which includes\nthe"
        },
        {
          "No nonverbal behavior\n0.25": "introspection, F1 scores for Withdrawal\n(655 frames), Attack",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "transcript)\nleads to a noticeable decrease in prediction perfor-"
        },
        {
          "No nonverbal behavior\n0.25": "self\n(515 frames), and Attack other\n(629 frames) are between",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "mance\nfor\nthe Llama2-7B and Gemma models.\nIn fact,\nthe"
        },
        {
          "No nonverbal behavior\n0.25": "0.71 and 0.76, whereas for the remaining classes (each > 1500",
          "0.28\n0.42\n0.44\n0.44\n0.46": ""
        },
        {
          "No nonverbal behavior\n0.25": "",
          "0.28\n0.42\n0.44\n0.44\n0.46": "accuracy and F1 score similarly decrease as when excluding"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "shame inducing situation),\nindicating that\nthe key information",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "and were pre-selected having good skills\nto reflect on their"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "the LLMs utilize is users’ verbal behavior. For\nthe BN,\nthe",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "internal\nexperiences. Therefore,\nthe\nfull\nrange\nof\nemotion"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "information about\nthe situational context\nis less important\nto",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "regulation strategies\nand associated nonverbal behavior may"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "correctly\npredict\nthe\nemotion\nregulation\nstrategies. Without",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "not be captured, which may limit\nthe generalizability of our"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "access\nto verbalized introspection,\nthe BN only reaches F1",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "findings. The reduction of effort by using an LLM to predict"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "scores\nbetween\n0.23\nto\n0.28, while\nthe LLMs\ncan\nbetter",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "emotion regulation strategies, where verbalized introspection"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "maintain their performance. As in the condition with available",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "seems\nto\nbe\nless\ncrucial,\nseems\npromising.\nIt would\nallow"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "verbalized\nintrospection,\nremoval\nof\nsituational\ncontext\nor",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "for more\neconomical data\ncollection and annotation for\nfu-"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "transcript\nimpacts the LLMs most.",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "ture work investigating emotion regulation strategies, however"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "the\naccuracy of\nthe LLM predictions need to be\nrigorously"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "VI. DISCUSSION",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": ""
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "evaluated in any new scenario."
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "A. On Performance",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "This\npaper\nfocuses\non\nemotion\nregulation\nin\nvalidated"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "While the Bayesian Network based approach achieved the",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "shame-eliciting situations,\nlimiting the extension of\nthe work"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "highest performance when all modalities including verbalized",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "to situations where other emotion classes are elicited. Shame is"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "introspection were\navailable,\nthe LLMs where much more",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "an ideal starting point for this kind of research, both because of"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "robust when modalities were\nremoved. Especially\nthe\nfact",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "the existing extensive theoretical background describing shame"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "that LLMs proved to be\nrelatively robust\nto the\nremoval of",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "regulation strategies [5], as well as due to the availability of"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "verbalized introspection information makes them a decidedly",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "the DEEP corpus. However, emotions are not only regulated"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "better choice in application scenarios where post-interaction",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "in shame eliciting situations, as most (if not all) emotions are"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "interviews are impractical, or online prediction is desired.",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "intrapersonally regulated [33]. Therefore,\nfuture work should"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "It is crucial to acknowledge that the Bayesian Network (BN)",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "extend the\napplication of\nthis proposed hybrid approach to"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "does not\ninclude the raw transcript of\nthe job interview, but",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "other emotion classes,\nto gain an overall deeper understanding"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "a\ndistilled\nrepresentation\nof\nthe\ndata\nsourced\nfrom the\njob",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "of\nindividual emotional experiences."
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "interview and\nverbalized\nintrospection. The\ndistillation\ncan",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "Finally, while our proposed approach allows\nto automati-"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "be beneficial, especially if\nit encapsulates\nthe most pertinent",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "cally infer emotion regulation strategies\nfrom behavioral de-"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "information\nrequired\nto\nidentify\naffective\nstates. However,",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "scriptions,\nthe descriptions provided with the DEEP\ndataset"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "there’s\na\nrisk\nthat\nduring\nthis\nprocess,\npotentially\nrelevant",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "were manually annotated. Future work should replace\nsuch"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "details may be excluded. Thus, depending on the quality of",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "manual\nsteps with automatic methods. While this might not"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "abstraction,\nthe\nremoval\nof modalities may\nhave\na\nless\nor",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "be\neasy\nto\ndo\nfor\nfeatures\nextracted\nfrom the\nverbalized"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "more detrimental\nimpact on the performance of\nthe BN. For",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "introspection, automatic methods to detect facial behavior [54],"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "example,\nthe internal emotion component appears\nto contain",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "body language [55], and to recognize speech [56] are avail-"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "the most\nrelevant\ninformation by representing the\nextracted",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "able. When using automatic approaches,\nthe set of nonverbal"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "emotion\nclasses.\nIn\nour\ncase,\nleaving\nout\nthe\ninformation",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "behaviors\ncan\nalso\neasily\nbe\nextended,\ne.g.\nby\ndetecting"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "associated with that component has a detrimental\nimpact on",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "backchannels [57], or analyzing prosody [58]."
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "the performance of\nthe BN which cannot be compensated by",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": ""
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "VII. CONCLUSION"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "the information associated with the situational context. This",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": ""
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "underscores the critical nature of the abstraction approach, par-",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "In this paper, we presented the first evaluation of instruction-"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "ticularly in how omitted information impacts the comparative",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "tuned large\nlanguage models\n(LLMs) on the\ntask of\nrecog-"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "efficacy of\nthe BN and LMM in emotion recognition tasks.",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "nizing the strategy employed to regulate the emotion shame."
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "LLMs\nbear\nthe\nadvantage\nthat\nthey\nare\nable\nto\naccess",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "We utilized the recently introduced DEEP\ncorpus of\nshame-"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "semantic information from the transcripts of the job interviews.",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "inducing situations during job interviews, which is annotated"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "This information enables them to leverage additional nuanced",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "with multi-modal behaviors and verbalized introspection gath-"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "information crucial\nfor\naffect\nrecognition while\nthe BN has",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "ered after the shame-inducing interactions. Our results indicate"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "only access to this information in terms of abstract representa-",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "that while\ntheory-driven Bayesian Networks\nperform best"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "tions gathered from the verbalized introspection. While the BN",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "when all\ninformation is available, LLMs can cope much better"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "benefits\nfrom incorporating a\ntheory-driven emotion model,",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "with missing information from the verbalized introspection,"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "the advantage of\nsuch a model\nis contingent upon its access",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "likely\ndue\nto\ntheir\ncapability\nto\neffectively make\nuse\nof"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "to relevant\ninformation resulting from verbal\nintrospection.",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "users’ verbal behavior. As such, our insights are an important"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "building block towards\naffective\ncomputing systems\nable\nto"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "B. Limitations and Future Work",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": ""
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "recognize emotion regulation strategies in realistic scenarios."
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "While\nour\nresults\nrepresent\nan\nencouraging\nstep\ntowards",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": ""
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "ETHICAL IMPACT STATEMENT"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "emotion regulation recognition in realistic scenarios,\nseveral",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": ""
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "limitations\nremain. Due\nto\nthe\nneed\nfor\nverbalized\nintro-",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "The paper employs data from the recently introduced DEEP"
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "spection\nand\nthe\ncomplexity\nof\nthe\nannotations,\nthe DEEP",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "corpus, which we\nreceived\nupon\nrequest\nfrom the\nauthors."
        },
        {
          "the\ntranscript\nonly\n(but\nkeeping\nthe\ninformation\nabout\nthe": "corpus\nis\nlimited in size and variability. The ten participants",
          "were\nall\nhaving\nthe\nsame\ncultural\nbackground,\nsimilar\nage": "The DEEP corpus\nincludes\nrecordings\nof\nhuman behaviors"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "emotion, vol. 13, no. 5, pp. 551–573, 1999."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "collection and analysis of such data involves processing per-",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "[7] M. E. Hoque, M. Courgeon, J.-C. Martin, B. Mutlu, and R. W. Picard,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "sonal and potentially sensitive data. Furthermore,\nit exposes",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Proceedings\nof\nthe\n“Mach: My\nautomated\nconversation\ncoach,”\nin"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "participants to shameful situations which may lead to negative",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "2013 ACM International Joint Conference on Pervasive and Ubiquitous"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Computing, pp. 697–706, 2013."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "emotional\nstates.\nIt\nis\ncrucial\nto\nobtain\ninformed\nconsent",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "[8]",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "P.\nGebhard,\nT.\nSchneeberger,\nE.\nAndr´e,\nT.\nBaur,\nI.\nDamian,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "from the participants, ensure that\nthe employed stimuli don’t",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "G. Mehlmann, C. K¨onig, and M. Langer, “Serious games\nfor\ntraining"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "negatively affect\ntheir mental health,\nimplement\nrobust data",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "social skills in job interviews,” IEEE Transactions on Games, 2018."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "[9]",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "T. Schneeberger, N. Sauerwein, M. S. Anglet, and P. Gebhard, “Stress"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "protection measures\nand only collect data necessary for\nthe",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "management training using biofeedback guided by social agents,” in 26th"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "intended affect\nrecognition purposes. Approval\nfor collecting",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Conference on Intelligent User Interfaces, pp. 564–574, 2021."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "and processing these data was obtained from the ethical review",
          "[6]": "[10]",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "P. Gebhard, T. Schneeberger, M. Dietz, E. Andr´e, and N. u. H. Bajwa,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "“Designing a mobile\nsocial\nand vocational\nreintegration assistant\nfor"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "board of\nthe DEEP corpus’ authors. The current analysis of",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "the 19th ACM Inter-\nburn-out outpatient\ntreatment,” in Proceedings of"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "multimodal interview data and subsequent verbal introspection",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "national Conference on Intelligent Virtual Agents, pp. 13–15, 2019."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "is covered by the ethics’ approval\nfor\nthe DEEP corpus.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "[11] D. DeVault, R. Artstein, G. Benn, T. Dey, E. Fast, A. Gainer, K. Georgila,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "J. Gratch, A. Hartholt, M. Lhommet, et al., “SimSensei Kiosk: A virtual"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "Our model contributes\nto endeavors aimed at deciphering",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "human interviewer\nfor healthcare decision support,” in Proceedings of"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "internal\nstates,\nparticularly\nbenefiting Affective Computing",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "the 2014 International Conference on Autonomous Agents and Multi-"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "systems\nreliant on discerning user\nemotions,\nsuch as\nsocial",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Agent Systems, pp. 1061–1068, 2014."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "[12]",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "T. Schneeberger, M. Hladk´y, A.-K. Thurner,\nJ. Volkert, A. Heimerl,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "training systems or\ntherapeutical assistants. However,\nthe po-",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "T. Baur,\nE. Andr´e,\nand\nP. Gebhard,\n“The\ndeep method:\nTowards"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "tential\nfor misapplication raises pertinent privacy concerns.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "computational modeling of\nthe\nsocial\nemotion shame driven by the-"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "In our\ninvestigation, we solicited insights from participants",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "ory,\nintrospection, and social\nsignals,” IEEE Transactions on Affective"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Computing, pp. 1–16, 2023."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "concerning their\ninternal\nexperience\nin shame-eliciting situ-",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "[13] M. M. Amin, R. Mao, E. Cambria,",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "and B. W.\nSchuller,\n“A wide"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "ations. Although\nparticipants\nprovided\nconsent\nfor\nresearch",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "arXiv\npreprint\nevaluation\nof\nchatgpt\non\naffective\ncomputing\ntasks,”"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "purposes,\nin\npractical\nscenarios,\nindividuals may withhold",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "arXiv:2308.13911, 2023."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "[14] W. Zhao, Y. Zhao, X. Lu, S. Wang, Y. Tong,",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "and B. Qin,\n“Is\nchat-"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "consent\ndue\nto\napprehensions\nsurrounding\nthe\nexposure\nof",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "arXiv\npreprint\ngpt\nequipped with\nemotional\ndialogue\ncapabilities?,”"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "their\ninternal\nexperiences.\nSuch\nreluctance\ncould\nengender",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "arXiv:2304.09582, 2023."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "adverse ramifications for social\ninteractions and interpersonal",
          "[6]": "[15]",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "J. Li, K. Pan, Z. Ge, M. Gao, H. Zhang, W. Ji, W. Zhang, T.-S. Chua,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "S. Tang, and Y. Zhuang, “Fine-tuning multimodal LLMs to follow zero-"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "relationships.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "shot demonstrative instructions,” in The Twelfth International Confer-"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "Prior\nto\nengaging with\nsystems\nemploying models\nfor",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "ence on Learning Representations, 2024."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "interpreting observable\nexpressions\nand internal\nstates,\nit\nis",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "[16] R. Zhang, Y.-S. Wang, and Y. Yang, “Generation-driven contrastive self-"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "training for zero-shot\ntext classification with instruction-following llm,”"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "imperative\nthat\nusers\nare\nadequately\ninformed\nand\nprovide",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "the 18th Conference of\nthe European Chapter of\nthe\nin Proceedings of"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "consent\nregarding\nfunctionality,\ndata\ncollection,\nprocessing,",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Association for Computational Linguistics\n(Volume 1: Long Papers),"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "and attendant\nrisks. The utilization of\nsuch systems without",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "pp. 659–673, 2024."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "[17] A.",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Borzunov,\nM.\nRyabinin,\nA.\nChumachenko,\nD.\nBaranchuk,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "the\ninformed\nconsent\nof\nindividuals\nsubject\nto\nobservation",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "T. Dettmers, Y. Belkada, P. Samygin,\nand C. A. Raffel,\n“Distributed"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "may\nresult\nin\ndeleterious\noutcomes. Unsanctioned\napplica-",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "inference and fine-tuning of\nlarge language models over\nthe internet,”"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "tion\nof\nsuch\ntechnologies may\ninadvertently\ngather\ndeeply",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Advances in Neural\nInformation Processing Systems, vol. 36, 2024."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "[18] Y. Liu, H. He, T. Han, X. Zhang, M. Liu, J. Tian, Y. Zhang, J. Wang,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "personal\ninformation about\nindividuals’\ninternal experiences,",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "et\nX. Gao,\nT.\nZhong,\nal.,\n“Understanding\nllms: A comprehensive"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "subsequently exposing them to potential harm to their social",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "overview from training to inference,” arXiv preprint arXiv:2401.02038,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "standing, privacy, and overall well-being.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "2024."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "[19]",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "S. Zhang, L. Dong, X. Li, S. Zhang, X. Sun, S. Wang,\nJ. Li, R. Hu,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "ACKNOWLEDGMENT",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "T. Zhang, F. Wu, et al., “Instruction tuning for\nlarge language models:"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "A survey,” arXiv preprint arXiv:2308.10792, 2023."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "P. M¨uller,\nS. Hossain, L.\nSiegel,\nand\nJ. Alexandersson",
          "[6]": "[20]",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "E.\nJ. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "and W. Chen, “Lora: Low-rank adaptation of\nlarge language models,”"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "were partially funded by the European Union Horizon Europe",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "2021."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "programme, grant number 101078950. E. Andr´e, P. Gebhard,",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "[21] Y. Zhang, M. Wang, P. Tiwari, Q. Li, B. Wang, and J. Qin, “Dialoguellm:"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "and T. Schneeberger were supported by the German Federal",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Context and emotion knowledge-tuned llama models for emotion recog-"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "nition in conversations,” arXiv preprint arXiv:2310.11374, 2023."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "Ministry for Education and Research (BMBF) as a segment",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "[22] G. Dey, A. V. Ganesan, Y. K. Lal, M. Shah, S. Sinha, M. Matero,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "of\nthe UBIDENZ project, under grant numbers 13GW0568D",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "S. Giorgi, V. Kulkarni,\nand H. A.\nSchwartz,\n“Socialite-llama: An"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "and 13GW0568F.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "arXiv\npreprint\ninstruction-tuned model\nfor\nsocial\nscientific\ntasks,”"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "arXiv:2402.01980, 2024."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "[23]",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Z. Liu, K. Yang, T. Zhang, Q. Xie, Z. Yu, and S. Ananiadou, “Emollms:"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "REFERENCES",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": ""
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "A series of emotional\nlarge language models and annotation tools\nfor"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "life of\n[1]\nL. F. Barrett, How emotions are made: The secret\nthe brain. Pan",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "arXiv\npreprint\ncomprehensive\naffective\nanalysis,”\narXiv:2401.08508,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "Macmillan, 2017.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "2024."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "[2] D. Keltner, “Signs of appeasement: Evidence for\nthe distinct displays",
          "[6]": "[24]",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihal-"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "of embarrassment, amusement, and shame.,” Journal of Personality and",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "cea, “MELD: A multimodal multi-party dataset for emotion recognition"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "Social Psychology, vol. 68, no. 3, p. 441, 1995.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "the 57th Annual Meeting of\nthe\nin conversations,”\nin Proceedings of"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "[3] M. Lewis, “Self-conscious emotions: Embarrassment, pride, shame, and",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Association\nfor Computational Linguistics\n(A. Korhonen, D. Traum,"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "guilt.,” in Handbook of Emotions (M. Lewis, J. M. Haviland-Jones, and",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "and L. M`arquez, eds.),\n(Florence,\nItaly), pp. 527–536, Association for"
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "L. Feldman Barrett, eds.), vol. 3, p. 742–756, Guilford, 2008.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Computational Linguistics, July 2019."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "[4]\nJ. J. Gross, Handbook of emotion regulation. Guilford, 2013.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "[25] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N."
        },
        {
          "in\njob\ninterviews\nand\nsubsequent\nverbal\nintrospection. The": "[5] D. L. Nathanson, Shame and pride. Norton, 1994.",
          "[6]": "",
          "J.\nJ. Gross,\n“Emotion regulation: Past, present,\nfuture,” Cognition &": "Chang, S. Lee, and S. S. Narayanan, “Iemocap:\nInteractive emotional"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "dyadic motion capture database,” Language resources and evaluation,": "vol. 42, pp. 335–359, 2008.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "“Can social agents elicit\nshame as humans do?,” in 8th International"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[26]\nS. M. Zahiri and J. D. Choi, “Emotion detection on tv show transcripts",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "Conference on Affective Computing and Intelligent\nInteraction (ACII),"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "with sequence-based convolutional neural networks,” in Workshops at",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "pp. 164–170,\nIEEE, 2019."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "the thirty-second AAAI Conference on Artificial\nIntelligence, 2018.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[48]\nP. L. Harris,\n“What\nchildren know about\nthe\nsituations\nthat provoke"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[27] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "emotion,” in The socialization of emotions, pp. 161–185, Springer, 1985."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "N. Bashlykov, S. Batra, P. Bhargava, S. Bhosale, D. Bikel, L. Blecher,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[49] Y. Tang, C. Tran, X. Li, P.-J. Chen, N. Goyal, V. Chaudhary, J. Gu, and"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu,\nJ. Fernandes,\nJ. Fu,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "A. Fan, “Multilingual translation with extensible multilingual pretraining"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "W.\nFu, B.\nFuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "and finetuning,” 2020."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "S. Hosseini, R. Hou, H.\nInan, M. Kardas, V. Kerkez, M. Khabsa,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[50]\nJ. Tiedemann and S. Thottingal, “OPUS-MT — Building open transla-"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril, J. Lee,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "the 22nd Annual Con-\ntion services for\nthe World,” in Proceedings of"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "D. Liskovich, Y. Lu, Y. Mao, X. Martinet, T. Mihaylov, P. Mishra,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "ferenec of\nthe European Association for Machine Translation (EAMT),"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "I. Molybog, Y. Nie, A. Poulton,\nJ. Reizenstein, R. Rungta, K. Saladi,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "(Lisbon, Portugal), 2020."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan, B. Tang,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[51]\nT. Dettmers, A. Pagnoni, A. Holtzman,\nand L. Zettlemoyer,\n“Qlora:"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan,\nI. Zarov, Y. Zhang,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "Efficient finetuning of quantized llms,” Advances in Neural Information"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "A. Fan, M. Kambadur, S. Narang, A. Rodriguez, R. Stojnic, S. Edunov,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "Processing Systems, vol. 36, 2024."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "and T. Scialom, “Llama 2: Open foundation and fine-tuned chat models,”",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[52] C. E.\nIzard, F. E. Dougherty, B. M. Bloxom, and N. E. Kotsch, The"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "2023.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "Differential Emotions\nScale:\na Method\nof Measuring\nthe\nSubjective"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[28] A. Patil, D. Wu, R. Ong, S.\nJain, and L. Huang, “meta-llama/Llama-",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "Experience of Discrete Emotions.\nNashville, Tenn.: Vanderbilt Univ."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "2-7b-chat-hf: Llama 2.7B fine-tuned on Conversational data for Chat-",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "Press, 1974."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "bot\ntask.” https://huggingface.co/meta-llama/Llama-2-7b-chat-hf, 2024.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[53] D. Watson and L. A. Clark, “The panas-x: Manual\nfor\nthe positive and"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "Hugging Face Model Hub.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "negative affect schedule-expanded form,” 1994."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[54]\nT. Baltrusaitis, A. Zadeh, Y. C. Lim,\nand L.-P. Morency,\n“Openface"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[29] G. D. Gemma Team, “Gemma: Open models based on gemini research",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "2.0: Facial behavior analysis toolkit,” in 2018 13th IEEE International"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "and technology,” tech.\nrep., Google DeepMind, 2024.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "Conference\non Automatic Face & Gesture Recognition\n(FG 2018),"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[30] K. R. Scherer et al., “Psychological models of emotion,” The Neuropsy-",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "pp. 59–66, 2018."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "chology of Emotion, vol. 137, no. 3, pp. 137–162, 2000.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "´\n[55] M. Balazia, P. M¨uller,\nA. L. T´anczos, A. v. Liechtenstein, and F. Bre-"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[31]\nS. PS and G. Mahalakshmi, “Emotion models: a review,” International",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "mond, “Bodily behaviors\nin social\ninteraction: Novel annotations and"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "Journal of Control Theory and Applications, vol. 10, no. 8, pp. 651–657,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "the 30th ACM Interna-\nstate-of-the-art\nevaluation,”\nin Proceedings of"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "2017.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "tional Conference on Multimedia, pp. 70–79, 2022."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[32] U. Moser and I. Von Zeppelin, “Die Entwicklung des Affektsystems,”",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[56] A. Radford,\nJ. W. Kim, T. Xu, G. Brockman, C. McLeavey,\nand"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "Psyche, vol. 50, no. 1, pp. 32–84, 1996.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "I. Sutskever,\n“Robust\nspeech recognition via\nlarge-scale weak super-"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[33]\nS. S. Tomkins, “Affect theory,” Approaches to Emotion, vol. 163, p. 195,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "vision,” in International Conference on Machine Learning, pp. 28492–"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "1984.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "28518, PMLR, 2023."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[34]\nP. Cramer, “Defense mechanisms in psychology today: Further processes",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[57] A. Amer, C. Bhuvaneshwara, G. K. Addluri, M. M. Shaik, V. Bonde, and"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "for adaptation.,” American Psychologist, vol. 55, no. 6, p. 637, 2000.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "P. M¨uller, “Backchannel detection and agreement estimation from video"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[35] D. L. Nathanson, Shame and pride: Affect, sex, and the birth of\nthe self.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "with transformer networks,” in 2023 International Joint Conference on"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "Norton, 1992.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "Neural Networks (IJCNN), pp. 1–8,\nIEEE, 2023."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[36]\nP. Ekman and W. V. Friesen,\n“The\nrepertoire of nonverbal behavior:",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "[58]\nF. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andr´e, C. Busso,"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "Categories, origins, usage, and coding,” Semiotica, vol. 1, pp. 49–98,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan, et al., “The geneva"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "1969.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "minimalistic\nacoustic\nparameter\nset\n(gemaps)\nfor\nvoice\nresearch\nand"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[37] A. Moors, P. C. Ellsworth, K. R. Scherer, and N. H. Frijda, “Appraisal",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "affective computing,” IEEE transactions on affective computing, vol. 7,"
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "Theories of Emotion: State of the Art and Future Development,” Emotion",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": "no. 2, pp. 190–202, 2015."
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "Review, vol. 5, pp. 119–124, April 2013.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[38]\nP. Gebhard, T. Schneeberger, T. Baur, and E. Andr´e, “MARSSI: Model",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "of appraisal,\nregulation, and social\nsignal\ninterpretation,” in Int. Con-",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "ference on Autonomous Agents and MultiAgent Systems, pp. 497–506,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "2018.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[39] X. Wang, X. Li, Z. Yin, Y. Wu,\nand J. Liu,\n“Emotional\nintelligence",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "of\nlarge language models,” Journal of Pacific Rim Psychology, vol. 17,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "p. 18344909231213958, 2023.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[40] R. Mao, Q. Liu, K. He, W. Li, and E. Cambria, “The biases of pre-trained",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "language models: An empirical study on prompt-based sentiment analy-",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "sis and emotion detection,” IEEE Transactions on Affective Computing,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "2022.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[41] Q. Xie, Z. Dai, E. Hovy, T. Luong,\nand Q. Le,\n“Unsupervised data",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "information\naugmentation for consistency training,” Advances in neural",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "processing systems, vol. 33, pp. 6256–6268, 2020.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[42]\nZ. Yang, Z. Dai, Y. Yang,\nJ. Carbonell, R. R.\nSalakhutdinov,\nand",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "Q. V. Le, “Xlnet: Generalized autoregressive pretraining for\nlanguage",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "in\nneural\ninformation\nprocessing\nunderstanding,” Advances\nsystems,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "vol. 32, 2019.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[43]\nS. Wang, H. Fang, M. Khabsa, H. Mao,\nand H. Ma,\n“Entailment\nas",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "few-shot\nlearner,” arXiv preprint arXiv:2104.14690, 2021.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[44] C. Qin, A. Zhang, Z. Zhang,\nJ. Chen, M. Yasunaga,\nand D. Yang,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "“Is chatgpt a general-purpose natural\nlanguage processing task solver?,”",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "arXiv preprint arXiv:2302.06476, 2023.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[45]\nS. Park,\nJ. Kim, S. Ye,\nJ.\nJeon, H. Y. Park,\nand A. Oh,\n“Dimen-",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "arXiv\npreprint\nsional\nemotion\ndetection\nfrom categorical\nemotion,”",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "arXiv:1911.02499, 2019.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "[46] M. K. Hasan, M. S.\nIslam, S. Lee, W. Rahman,\nI. Naim, M.\nI. Khan,",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "and E. Hoque,\n“Textmi: Textualize multimodal\ninformation for\ninte-",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "grating non-verbal cues in pre-trained language models,” arXiv preprint",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        },
        {
          "dyadic motion capture database,” Language resources and evaluation,": "arXiv:2303.15430, 2023.",
          "[47]\nT. Schneeberger, M. Scholtes, B. Hilpert, M. Langer, and P. Gebhard,": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "How emotions are made: The secret life of the brain",
      "authors": [
        "L Barrett"
      ],
      "year": "2017",
      "venue": "How emotions are made: The secret life of the brain"
    },
    {
      "citation_id": "2",
      "title": "Signs of appeasement: Evidence for the distinct displays of embarrassment, amusement, and shame",
      "authors": [
        "D Keltner"
      ],
      "year": "1995",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "3",
      "title": "Self-conscious emotions: Embarrassment, pride, shame, and guilt",
      "authors": [
        "M Lewis"
      ],
      "year": "2008",
      "venue": "Handbook of Emotions"
    },
    {
      "citation_id": "4",
      "title": "Handbook of emotion regulation",
      "authors": [
        "J Gross"
      ],
      "year": "2013",
      "venue": "Handbook of emotion regulation"
    },
    {
      "citation_id": "5",
      "title": "Shame and pride",
      "authors": [
        "D Nathanson"
      ],
      "year": "1994",
      "venue": "Shame and pride"
    },
    {
      "citation_id": "6",
      "title": "Emotion regulation: Past, present, future",
      "authors": [
        "J Gross"
      ],
      "year": "1999",
      "venue": "Cognition & emotion"
    },
    {
      "citation_id": "7",
      "title": "Mach: My automated conversation coach",
      "authors": [
        "M Hoque",
        "M Courgeon",
        "J.-C Martin",
        "B Mutlu",
        "R Picard"
      ],
      "year": "2013",
      "venue": "Proceedings of the 2013 ACM International Joint Conference on Pervasive and Ubiquitous Computing"
    },
    {
      "citation_id": "8",
      "title": "Serious games for training social skills in job interviews",
      "authors": [
        "P Gebhard",
        "T Schneeberger",
        "E André",
        "T Baur",
        "I Damian",
        "G Mehlmann",
        "C König",
        "M Langer"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Games"
    },
    {
      "citation_id": "9",
      "title": "Stress management training using biofeedback guided by social agents",
      "authors": [
        "T Schneeberger",
        "N Sauerwein",
        "M Anglet",
        "P Gebhard"
      ],
      "year": "2021",
      "venue": "26th Conference on Intelligent User Interfaces"
    },
    {
      "citation_id": "10",
      "title": "Designing a mobile social and vocational reintegration assistant for burn-out outpatient treatment",
      "authors": [
        "P Gebhard",
        "T Schneeberger",
        "M Dietz",
        "E André",
        "N Bajwa"
      ],
      "year": "2019",
      "venue": "Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents"
    },
    {
      "citation_id": "11",
      "title": "SimSensei Kiosk: A virtual human interviewer for healthcare decision support",
      "authors": [
        "D Devault",
        "R Artstein",
        "G Benn",
        "T Dey",
        "E Fast",
        "A Gainer",
        "K Georgila",
        "J Gratch",
        "A Hartholt",
        "M Lhommet"
      ],
      "year": "2014",
      "venue": "Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems"
    },
    {
      "citation_id": "12",
      "title": "The deep method: Towards computational modeling of the social emotion shame driven by theory, introspection, and social signals",
      "authors": [
        "T Schneeberger",
        "M Hladký",
        "A.-K Thurner",
        "J Volkert",
        "A Heimerl",
        "T Baur",
        "E André",
        "P Gebhard"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "A wide evaluation of chatgpt on affective computing tasks",
      "authors": [
        "M Amin",
        "R Mao",
        "E Cambria",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "A wide evaluation of chatgpt on affective computing tasks",
      "arxiv": "arXiv:2308.13911"
    },
    {
      "citation_id": "14",
      "title": "Is chatgpt equipped with emotional dialogue capabilities?",
      "authors": [
        "W Zhao",
        "Y Zhao",
        "X Lu",
        "S Wang",
        "Y Tong",
        "B Qin"
      ],
      "year": "2023",
      "venue": "Is chatgpt equipped with emotional dialogue capabilities?",
      "arxiv": "arXiv:2304.09582"
    },
    {
      "citation_id": "15",
      "title": "Fine-tuning multimodal LLMs to follow zeroshot demonstrative instructions",
      "authors": [
        "J Li",
        "K Pan",
        "Z Ge",
        "M Gao",
        "H Zhang",
        "W Ji",
        "W Zhang",
        "T.-S Chua",
        "S Tang",
        "Y Zhuang"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "16",
      "title": "Generation-driven contrastive selftraining for zero-shot text classification with instruction-following llm",
      "authors": [
        "R Zhang",
        "Y.-S Wang",
        "Y Yang"
      ],
      "year": "2024",
      "venue": "Proceedings of the 18th Conference of the European Chapter"
    },
    {
      "citation_id": "17",
      "title": "Distributed inference and fine-tuning of large language models over the internet",
      "authors": [
        "A Borzunov",
        "M Ryabinin",
        "A Chumachenko",
        "D Baranchuk",
        "T Dettmers",
        "Y Belkada",
        "P Samygin",
        "C Raffel"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "18",
      "title": "Understanding llms: A comprehensive overview from training to inference",
      "authors": [
        "Y Liu",
        "H He",
        "T Han",
        "X Zhang",
        "M Liu",
        "J Tian",
        "Y Zhang",
        "J Wang",
        "X Gao",
        "T Zhong"
      ],
      "year": "2024",
      "venue": "Understanding llms: A comprehensive overview from training to inference",
      "arxiv": "arXiv:2401.02038"
    },
    {
      "citation_id": "19",
      "title": "Instruction tuning for large language models: A survey",
      "authors": [
        "S Zhang",
        "L Dong",
        "X Li",
        "S Zhang",
        "X Sun",
        "S Wang",
        "J Li",
        "R Hu",
        "T Zhang",
        "F Wu"
      ],
      "year": "2023",
      "venue": "Instruction tuning for large language models: A survey",
      "arxiv": "arXiv:2308.10792"
    },
    {
      "citation_id": "20",
      "title": "Lora: Low-rank adaptation of large language models",
      "authors": [
        "E Hu",
        "Y Shen",
        "P Wallis",
        "Z Allen-Zhu",
        "Y Li",
        "S Wang",
        "L Wang",
        "W Chen"
      ],
      "year": "2021",
      "venue": "Lora: Low-rank adaptation of large language models"
    },
    {
      "citation_id": "21",
      "title": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "authors": [
        "Y Zhang",
        "M Wang",
        "P Tiwari",
        "Q Li",
        "B Wang",
        "J Qin"
      ],
      "year": "2023",
      "venue": "Dialoguellm: Context and emotion knowledge-tuned llama models for emotion recognition in conversations",
      "arxiv": "arXiv:2310.11374"
    },
    {
      "citation_id": "22",
      "title": "Socialite-llama: An instruction-tuned model for social scientific tasks",
      "authors": [
        "G Dey",
        "A Ganesan",
        "Y Lal",
        "M Shah",
        "S Sinha",
        "M Matero",
        "S Giorgi",
        "V Kulkarni",
        "H Schwartz"
      ],
      "year": "2024",
      "venue": "Socialite-llama: An instruction-tuned model for social scientific tasks",
      "arxiv": "arXiv:2402.01980"
    },
    {
      "citation_id": "23",
      "title": "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
      "authors": [
        "Z Liu",
        "K Yang",
        "T Zhang",
        "Q Xie",
        "Z Yu",
        "S Ananiadou"
      ],
      "year": "2024",
      "venue": "Emollms: A series of emotional large language models and annotation tools for comprehensive affective analysis",
      "arxiv": "arXiv:2401.08508"
    },
    {
      "citation_id": "24",
      "title": "MELD: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "26",
      "title": "Emotion detection on tv show transcripts with sequence-based convolutional neural networks",
      "authors": [
        "S Zahiri",
        "J Choi"
      ],
      "year": "2018",
      "venue": "Workshops at the thirty-second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "27",
      "title": "Llama 2: Open foundation and fine-tuned chat models",
      "authors": [
        "H Touvron",
        "L Martin",
        "K Stone",
        "P Albert",
        "A Almahairi",
        "Y Babaei",
        "N Bashlykov",
        "S Batra",
        "P Bhargava",
        "S Bhosale",
        "D Bikel",
        "L Blecher",
        "C Ferrer",
        "M Chen",
        "G Cucurull",
        "D Esiobu",
        "J Fernandes",
        "J Fu",
        "W Fu",
        "B Fuller",
        "C Gao",
        "V Goswami",
        "N Goyal",
        "A Hartshorn",
        "S Hosseini",
        "R Hou",
        "H Inan",
        "M Kardas",
        "V Kerkez",
        "M Khabsa",
        "I Kloumann",
        "A Korenev",
        "P Koura",
        "M.-A Lachaux",
        "T Lavril",
        "J Lee",
        "D Liskovich",
        "Y Lu",
        "Y Mao",
        "X Martinet",
        "T Mihaylov",
        "P Mishra",
        "I Molybog",
        "Y Nie",
        "A Poulton",
        "J Reizenstein",
        "R Rungta",
        "K Saladi",
        "A Schelten",
        "R Silva",
        "E Smith",
        "R Subramanian",
        "X Tan",
        "B Tang",
        "R Taylor",
        "A Williams",
        "J Kuan",
        "P Xu",
        "Z Yan",
        "I Zarov",
        "Y Zhang",
        "A Fan",
        "M Kambadur",
        "S Narang",
        "A Rodriguez",
        "R Stojnic",
        "S Edunov",
        "T Scialom"
      ],
      "year": "2023",
      "venue": "Llama 2: Open foundation and fine-tuned chat models"
    },
    {
      "citation_id": "28",
      "title": "meta-llama/Llama-2-7b-chat-hf: Llama 2.7B fine-tuned on Conversational data for Chatbot task",
      "authors": [
        "A Patil",
        "D Wu",
        "R Ong",
        "S Jain",
        "L Huang"
      ],
      "year": "2024",
      "venue": "Hugging Face Model Hub"
    },
    {
      "citation_id": "29",
      "title": "Gemma: Open models based on gemini research and technology",
      "authors": [
        "G Gemma Team"
      ],
      "year": "2024",
      "venue": "Gemma: Open models based on gemini research and technology"
    },
    {
      "citation_id": "30",
      "title": "Psychological models of emotion",
      "authors": [
        "K Scherer"
      ],
      "year": "2000",
      "venue": "The Neuropsychology of Emotion"
    },
    {
      "citation_id": "31",
      "title": "Emotion models: a review",
      "authors": [
        "S Ps",
        "G Mahalakshmi"
      ],
      "year": "2017",
      "venue": "International Journal of Control Theory and Applications"
    },
    {
      "citation_id": "32",
      "title": "Die Entwicklung des Affektsystems",
      "authors": [
        "U Moser",
        "I Von Zeppelin"
      ],
      "year": "1996",
      "venue": "Psyche"
    },
    {
      "citation_id": "33",
      "title": "Affect theory",
      "authors": [
        "S Tomkins"
      ],
      "year": "1984",
      "venue": "Approaches to Emotion"
    },
    {
      "citation_id": "34",
      "title": "Defense mechanisms in psychology today: Further processes for adaptation",
      "authors": [
        "P Cramer"
      ],
      "year": "2000",
      "venue": "American Psychologist"
    },
    {
      "citation_id": "35",
      "title": "Shame and pride: Affect, sex, and the birth of the self",
      "authors": [
        "D Nathanson"
      ],
      "year": "1992",
      "venue": "Shame and pride: Affect, sex, and the birth of the self"
    },
    {
      "citation_id": "36",
      "title": "The repertoire of nonverbal behavior: Categories, origins, usage, and coding",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1969",
      "venue": "Semiotica"
    },
    {
      "citation_id": "37",
      "title": "Appraisal Theories of Emotion: State of the Art and Future Development",
      "authors": [
        "A Moors",
        "P Ellsworth",
        "K Scherer",
        "N Frijda"
      ],
      "year": "2013",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "38",
      "title": "MARSSI: Model of appraisal, regulation, and social signal interpretation",
      "authors": [
        "P Gebhard",
        "T Schneeberger",
        "T Baur",
        "E André"
      ],
      "year": "2018",
      "venue": "Int. Conference on Autonomous Agents and MultiAgent Systems"
    },
    {
      "citation_id": "39",
      "title": "Emotional intelligence of large language models",
      "authors": [
        "X Wang",
        "X Li",
        "Z Yin",
        "Y Wu",
        "J Liu"
      ],
      "year": "2023",
      "venue": "Journal of Pacific Rim Psychology"
    },
    {
      "citation_id": "40",
      "title": "The biases of pre-trained language models: An empirical study on prompt-based sentiment analysis and emotion detection",
      "authors": [
        "R Mao",
        "Q Liu",
        "K He",
        "W Li",
        "E Cambria"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "Unsupervised data augmentation for consistency training",
      "authors": [
        "Q Xie",
        "Z Dai",
        "E Hovy",
        "T Luong",
        "Q Le"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "42",
      "title": "Xlnet: Generalized autoregressive pretraining for language understanding",
      "authors": [
        "Z Yang",
        "Z Dai",
        "Y Yang",
        "J Carbonell",
        "R Salakhutdinov",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "43",
      "title": "Entailment as few-shot learner",
      "authors": [
        "S Wang",
        "H Fang",
        "M Khabsa",
        "H Mao",
        "H Ma"
      ],
      "year": "2021",
      "venue": "Entailment as few-shot learner",
      "arxiv": "arXiv:2104.14690"
    },
    {
      "citation_id": "44",
      "title": "Is chatgpt a general-purpose natural language processing task solver?",
      "authors": [
        "C Qin",
        "A Zhang",
        "Z Zhang",
        "J Chen",
        "M Yasunaga",
        "D Yang"
      ],
      "year": "2023",
      "venue": "Is chatgpt a general-purpose natural language processing task solver?",
      "arxiv": "arXiv:2302.06476"
    },
    {
      "citation_id": "45",
      "title": "Dimensional emotion detection from categorical emotion",
      "authors": [
        "S Park",
        "J Kim",
        "S Ye",
        "J Jeon",
        "H Park",
        "A Oh"
      ],
      "year": "2019",
      "venue": "Dimensional emotion detection from categorical emotion",
      "arxiv": "arXiv:1911.02499"
    },
    {
      "citation_id": "46",
      "title": "Textmi: Textualize multimodal information for integrating non-verbal cues in pre-trained language models",
      "authors": [
        "M Hasan",
        "M Islam",
        "S Lee",
        "W Rahman",
        "I Naim",
        "M Khan",
        "E Hoque"
      ],
      "year": "2023",
      "venue": "Textmi: Textualize multimodal information for integrating non-verbal cues in pre-trained language models",
      "arxiv": "arXiv:2303.15430"
    },
    {
      "citation_id": "47",
      "title": "Can social agents elicit shame as humans do?",
      "authors": [
        "T Schneeberger",
        "M Scholtes",
        "B Hilpert",
        "M Langer",
        "P Gebhard"
      ],
      "year": "2019",
      "venue": "8th International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "48",
      "title": "What children know about the situations that provoke emotion",
      "authors": [
        "P Harris"
      ],
      "year": "1985",
      "venue": "The socialization of emotions"
    },
    {
      "citation_id": "49",
      "title": "Multilingual translation with extensible multilingual pretraining and finetuning",
      "authors": [
        "Y Tang",
        "C Tran",
        "X Li",
        "P.-J Chen",
        "N Goyal",
        "V Chaudhary",
        "J Gu",
        "A Fan"
      ],
      "year": "2020",
      "venue": "Multilingual translation with extensible multilingual pretraining and finetuning"
    },
    {
      "citation_id": "50",
      "title": "OPUS-MT -Building open translation services for the World",
      "authors": [
        "J Tiedemann"
      ],
      "year": "2020",
      "venue": "Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)"
    },
    {
      "citation_id": "51",
      "title": "Qlora: Efficient finetuning of quantized llms",
      "authors": [
        "T Dettmers",
        "A Pagnoni",
        "A Holtzman",
        "L Zettlemoyer"
      ],
      "year": "2024",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "52",
      "title": "The Differential Emotions Scale: a Method of Measuring the Subjective Experience of Discrete Emotions",
      "authors": [
        "C Izard",
        "F Dougherty",
        "B Bloxom",
        "N Kotsch"
      ],
      "year": "1974",
      "venue": "The Differential Emotions Scale: a Method of Measuring the Subjective Experience of Discrete Emotions"
    },
    {
      "citation_id": "53",
      "title": "The panas-x: Manual for the positive and negative affect schedule-expanded form",
      "authors": [
        "D Watson",
        "L Clark"
      ],
      "year": "1994",
      "venue": "The panas-x: Manual for the positive and negative affect schedule-expanded form"
    },
    {
      "citation_id": "54",
      "title": "Openface 2.0: Facial behavior analysis toolkit",
      "authors": [
        "T Baltrusaitis",
        "A Zadeh",
        "Y Lim",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "55",
      "title": "Bodily behaviors in social interaction: Novel annotations and state-of-the-art evaluation",
      "authors": [
        "M Balazia",
        "P Müller",
        "Á Tánczos",
        "A Liechtenstein",
        "F Bremond"
      ],
      "year": "2022",
      "venue": "Proceedings of the 30th ACM International Conference on Multimedia"
    },
    {
      "citation_id": "56",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "A Radford",
        "J Kim",
        "T Xu",
        "G Brockman",
        "C Mcleavey",
        "I Sutskever"
      ],
      "year": "2023",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "57",
      "title": "Backchannel detection and agreement estimation from video with transformer networks",
      "authors": [
        "A Amer",
        "C Bhuvaneshwara",
        "G Addluri",
        "M Shaik",
        "V Bonde",
        "P Müller"
      ],
      "year": "2023",
      "venue": "2023 International Joint Conference on Neural Networks (IJCNN)"
    },
    {
      "citation_id": "58",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E André",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan"
      ],
      "year": "2015",
      "venue": "IEEE transactions on affective computing"
    }
  ]
}