{
  "paper_id": "2406.14747v1",
  "title": "An Adapter-Based Unified Model For Multiple Spoken Language Processing Tasks",
  "published": "2024-06-20T21:39:04Z",
  "authors": [
    "Varsha Suresh",
    "Salah A√Øt-Mokhtar",
    "Caroline Brun",
    "Ioan Calapodescu"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Naver Labs Europe Self-supervised learning models have revolutionized the field of speech processing. However, the process of fine-tuning these models on downstream tasks requires substantial computational resources, particularly when dealing with multiple speech-processing tasks. In this paper, we explore the potential of adapter-based fine-tuning in developing a unified model capable of effectively handling multiple spoken language processing tasks. The tasks we investigate are Automatic Speech Recognition, Phoneme Recognition, Intent Classification, Slot Filling, and Spoken Emotion Recognition. We validate our approach through a series of experiments on the SUPERB benchmark, and our results indicate that adapter-based fine-tuning enables a single encoder-decoder model to perform multiple speech processing tasks with an average improvement of 18.4 % across the five target tasks while staying efficient in terms of parameter updates.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The fine-tuning of self-supervised learning (SSL) models, such as wav2vec 2.0  [4] , has improved the performance of Spoken Language Processing (SLP) tasks. However, as the quality of representations generated by these models improves, there is a corresponding increase in their size, necessitating additional storage and computational resources. This issue becomes particularly pronounced when dealing with multiple speechprocessing tasks, with each target task requiring separate model fine-tuning, further increasing the need for resources.\n\nModular architectures, such as adapters, have been widely used in NLP to tackle both parameter efficiency and multi-tasking  [12] . While adapter-based finetuning has been utilized in speech-related tasks, such as speech translation  [1, 11, 14]  and domain adaptation  [25] , its efficiency in developing a unified model capable of handling multiple Spoken Language Processing (SLP) tasks remains relatively unexplored. Existing attempts to model multiple SLP tasks with a single model utilises task-specific decoders  [29] . However, this approach becomes less scalable as the number of tasks increases.\n\nIn this work, we aim to develop a scalable and parameter-efficient unified encoder-decoder model to effectively handle multiple spoken language processing (SLP) tasks. For this, we use adapters  [12] , which allows new tasks to be added without the need to re-train the entire model and which also mitigates the need for dedicated decoders  [29] . Moreover, since adapters facilitate Multi-Task Learning (MTL), we investigate two approaches: Stacking  [21]  and Fusion  [31] , in addition to single-task adapters. To evaluate our approach, we choose five speech-processing tasks from the SUPERB benchmark  [29] : Automatic Speech Recognition (ASR), Phoneme Recognition (PR), Intent Classification (IC), Slot Filing (SF), and Spoken Emotion Recognition (ER). The detailed model description is provided in Figure  1 . From our experiments, we observed that adapter-based fine-tuning outperformed the SUPERB benchmark with an average improvement of 18.4 % achieved across 5 target tasks. We summarise our contributions below:\n\n‚Ä¢ We investigate the feasibility and efficiency of using adapters to build a unified encoder-decoder model that can tackle multiple spoken language processing tasks in a simple and scalable manner. ‚Ä¢ We explore multi-task learning within our unified framework with two methods: stacking and fusion, which combine adapters to enhance the performance of positively correlated tasks.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "In the field of NLP, researchers have used a single model to handle multiple tasks and adapt them to different domains  [18] . In the speech domain, most approaches that deal with multiple tasks fall under multi-task models. They either focus on improving a primary task by using auxiliary tasks, like performing ASR to enhance Emotion Recognition  [6, 9, 16] , or simultaneously perform multiple tasks -slot-filling and intent classification  [15] , ASR and speech translation  [23, 24]  etc. However, these approaches are not easily scalable for new tasks and are mostly applied for tasks that are known to be positively correlated. Some studies have aimed to create a unified model for multiple speech-processing tasks by training different modules and composing them to perform each task. These architectures comprise encoders and decoders trained to capture features from different modalities, such as text and speech  [2] , or speech characteristics like prosody  [7]  etc. In contrast, our approach focuses on constructing task-specific modular architectures. Furthermore, adding a task is straightforward as these task-specific modules are trained independently.\n\nOur work is inspired by SUPERB  [29] , where multiple tasks are modeled using pre-trained frozen encoders (such as wav2vec 2.0) and task-specific decoders, the task performance relying on the type of decoder used for the task  [30] . However, this approach does not scale well as the number of tasks increases. Instead, in our work, we aim to develop a single encoder-decoder model and show that adapters on the decoder side help us adapt to different types of tasks (both classification and generative) without the need for dedicated decoders.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Pre-Trained Encoder-Decoder Model",
      "text": "We use wav2vec 2.0-large 1  as encoder and a 6-layer transformer decoder which is randomly initialized. We fine-tuned this encoder-decoder model on the Lib-riSpeech 100-hr dataset  [20]  for the ASR task using hybrid CTC/attention objective  [27]  as the base model for our unified model. We use SentencePiece (BPE) vocabulary of size 5000. This base model achieved a word error rate (WER) of 3.54 on the test-clean split of LibriSpeech which is comparable to the 3.1 WER reported in the SUPERB benchmark.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Adapter-Based Task Modules",
      "text": "To enable the above-mentioned pre-trained encoderdecoder model to perform multiple SLP tasks, we insert task-specific adapter modules into the transformer  [26]  layers of both the encoder and the decoder. These modules are depicted in Figure  1 . The remainder of the model is frozen.\n\nWe focus on three types of architecture: i) single adapter ii) adapter stacking, and iii) adapter fusion as shown on the right side of Figure  1 . In the standard setting, a single adapter is trained for each task  [12] . However, some SLP tasks are known to benefit from MTL, such as performing ASR and emotion recognition  [9, 16]  and Intent classification and Slot-filling  [28] .\n\nAs adapters naturally support MTL  [12]  in addition to using a single adapter per task, we use the adapter stacking  [21]  and adapter fusion settings (same as the fast fusion setting in  [31] ) to perform positively correlated tasks together.\n\nTo facilitate a unified model, we encompass both classification (e.g., emotion recognition) and generative (e.g., slot filling) SLP tasks into this single encoder-decoder model. To achieve this, we model the classification task as a generative task i.e., the classification labels are generated. To accommodate multiple tasks using a single decoder output, some task-specific tokens are allocated in the vocabulary-slot value for SF task, emotion labels for ER task, etc. These tokens are selected from the least frequently used tokens in the vocabulary.\n\nFor MTL, we combine the ground truth of the tasks   1  and 9 tasks which includes the additional ASR tasks in Table  2 .\n\ninvolved using a task separator token. For example, to perform ASR along with ER, we format the ground truth as <transcript> <task separator> <emotion label>.\n\nFor training the encoder-decoder model, we use a combination of losses depending on the adapter architecture and the task. The overall objective L can be written as,\n\nwhere ùêø ùëõùëôùëô denotes the Negative Log-Likelihood loss at the decoder end. The output tokens during multi-task training comprise tokens from ùëÅ tasks which are weighted using the hyperparameter ùúÜ ùë°ùëéùë†ùëò . ùêø ùëêùë°ùëê denotes CTC loss applied at the encoder end, similar to hybrid CTC/attention objective  [27] . Hyperparameter ùúÜ ùëêùë°ùëê is used to weigh between the NLL and CTC loss. Finally, ùêø ùëêùëí denotes Cross Entropy Loss applied at the encoder end for classification tasks, and hyperparameter 1 ùëêùëí is 1 when it's a classification task.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "We train adapters to perform five different SLP tasks (corresponding datasets are denoted in the brackets), 1) ASR (LibriSpeech  [20] ), 2) PR (LibriSpeech), 3) ER (IEMOCAP  [5] ), 4) IC (Fluent Speech Commands  [17] ), and 5) SF (SNIPS  [8] ). We chose datasets used by the SUPERB benchmark 2    [29]  for the corresponding tasks.\n\nIn addition, we also train ASR adapters specifically for each domain (IEMOCAP, SNIPS and FSC) which helps in MTL (e.g, ASR + ER) and IC adapter for SNIPS which helps when performed with SF.\n\nFor evaluation, we follow the same setting as SUPERB. The adapter dimension was set to 128. For ùúÜ settings, in single-label classification tasks such as ER, ùúÜ ùëêùë°ùëê is set to 0. For the rest, ùúÜ ùëêùë°ùëê is set to 0.3 and in experiments where CTC loss was used, we combined the attention-based and CTC scores for joint decoding, assigning a weight of 0.4 to the CTC scores (following the SpeechBrain recipe). For MTL, in the stacked adapter setting  [21]  only the additional adapter is trained, while the rest of the model, including the bottom adapter(s) remains frozen. Here, ùúÜ ùë°ùëéùë†ùëò assigns a higher weight to the tokens corresponding to the new task. In our experiments, this value was set to 0.9 for the new task and 0.1 for the tasks of the already-trained bottom adapters. In fusion, the adapters are already trained with respective tasks, so we experimented with two settings: first, ùúÜ ùë°ùëéùë†ùëò is set to 1, and second, we set it to equal weights for all tasks and chose the best. We modified SpeechBrain 3 recipes for our implementation.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results And Discussion",
      "text": "Table  1  presents our results, showing that our approach achieves better performance compared to the wav2vec2 SUPERB  [29]  benchmark (+5.4) and actually also with the WavLM model (+1.4) also from SUPERB. This performance improvement can be attributed to our design choice of utilizing adapters that allows combining different tasks for improved multi-task learning. For example, for SF performance on SNIPS, the adapters where ASR, SF and IC are learned simultaneously allows an improvement of 8.3 F1 and 15.5 CER (see Table  2 ). Detailed results regarding the performance of different adapter combinations are discussed in the Ablation Study. In contrast to having a frozen encoder and task-specific decoders, we incorporate task-specific adapters on a single encoder-decoder model to perform multiple SLP tasks which leads to both efficient utilization of encoder representations, and memory efficiency (see Table  3 ).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study: Comparison Between Different Types Of Adapter-Based Task Modules",
      "text": "Research has shown that certain tasks, like ASR and ER  [9, 16] , can benefit from simultaneous learning, enhancing each other's performance. As adapters naturally enable MTL  [12] , in addition to single-adapter task modules, we investigate two adapter-based MTL approaches: Stacking and Fusion. We hypothesize that performing MTL with adapters produces less increase in computational overhead compared to the performance improvement. 4   Table 2 compares the performance amongst three differ- ent adapter settings and also with existing works that perform MTL. In the IEMOCAP dataset, the Adapter Stacking setting achieves the highest performance in Emotion Recognition. On the SNIPS dataset, the Adapter Fusion setting performs the best in SF and IC. Our performance is comparable to studies that use gold-text directly, such as  [22]  with an SF-F1 score of 95.9 and IC-Acc of 98.8% on SNIPS. For FSC, there is minimal performance variation in the literature, since models already achieve above 99% accuracy. The WER is also comparable with existing works -Ours: 0.6, and [10]: 0.5. This performance improvement of adapter-based MTL architectures aligns with previous research indicating that MTL enhances task performance  [22] . Furthermore, fine-tuning our ASR adapters for each dataset performs better than approaches that use generic ASR models, as previously demonstrated by  [16] .\n\nIn addition to the improvements in performance, our unified model shows multi-task capabilities in a parameter-efficient and scalable manner. Table  3  illustrates the comparison between our approach and the SUPERB benchmark in terms of the number of trainable parameters needed for accommodating six tasks (as in Table  1 ) and nine tasks (including the additional ASR tasks from Table  2 ). Notably, our approach requires fewer parameters, and more importantly, even as the number of tasks increases, the increase in the parameter count remains significantly lower with the ratio dropping to 53.6%.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "Our work shows that adapter-based task modules effectively enable a unified encoder-decoder model for handling multiple speech-processing tasks. Our experiments show that we are able to achieve performance improvements compared to the SUPERB benchmark, while being more efficient in terms of parameters by eliminating the need for dedicated task-specific decoders. This work highlights the potential to develop simple and scalable model architectures that are capable of performing multiple SLP tasks within a unified model. In the future, our goals include evaluating our approach for different choices of SSL models such as HuBERT and WavLM and exploring different adapter architectures. Additionally, we also aim to broaden the scope of our approach to add the remaining tasks in the SUPERB benchmark such as Speaker Identification, Speaker Diarization, and other speech-processing tasks/datasets.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: From our experiments, we observed that adapter-based",
      "page": 1
    },
    {
      "caption": "Figure 1: Left: Overall model architecture with a unified encoder-decoder model with adapter-based task modules",
      "page": 2
    },
    {
      "caption": "Figure 1: The remainder of the",
      "page": 2
    },
    {
      "caption": "Figure 1: In the standard",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Performance comparison in various speech processing tasks from the SUPERB benchmark. WavLM",
      "data": [
        {
          "LibriSpeech\nASR\nPR\n(WER ‚Üì)\n(PER ‚Üì)": "3.4\n3.1",
          "IEMOCAP\nER\n(Acc % ‚Üë)": "70.6",
          "SNIPS\nIC\nSF\n(Acc % ‚Üë)\n(F1 ‚Üë,CER ‚Üì)": "-\n92.2, 18.4",
          "FSC\nIC\n(Acc % ‚Üë)": "99.0",
          "Avg": "89.5"
        },
        {
          "LibriSpeech\nASR\nPR\n(WER ‚Üì)\n(PER ‚Üì)": "3.1\n4.7",
          "IEMOCAP\nER\n(Acc % ‚Üë)": "65.6",
          "SNIPS\nIC\nSF\n(Acc % ‚Üë)\n(F1 ‚Üë,CER ‚Üì)": "-\n87.1, 27.3",
          "FSC\nIC\n(Acc % ‚Üë)": "95.2",
          "Avg": "85.5"
        },
        {
          "LibriSpeech\nASR\nPR\n(WER ‚Üì)\n(PER ‚Üì)": "2.4\n3.5",
          "IEMOCAP\nER\n(Acc % ‚Üë)": "68.2",
          "SNIPS\nIC\nSF\n(Acc % ‚Üë)\n(F1 ‚Üë,CER ‚Üì)": "99.1\n95.4, 11.8",
          "FSC\nIC\n(Acc % ‚Üë)": "99.5",
          "Avg": "90.9"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: Performance comparison in various speech processing tasks from the SUPERB benchmark. WavLM",
      "data": [
        {
          "IEMOCAP\nASR\nER\n(WER ‚Üì)\n(Acc % ‚Üë)": "-\n67.6\n32.7\n63.4‚àó\n-\n-\n-\n-",
          "SNIPS\nASR\nIC\nSF\n(WER ‚Üì)\n(Acc % ‚Üë)\n(F1 ‚Üë,CER ‚Üì)": "-\n91.7\n-\n-\n-\n-\n-\n-\n-\n11.8\n98.6\n-",
          "FSC\nASR\nIC\n(WER ‚Üì)\n(Acc % ‚Üë)": "99.6\n-\n-\n-\n-\n98.2\n-\n-"
        },
        {
          "IEMOCAP\nASR\nER\n(WER ‚Üì)\n(Acc % ‚Üë)": "22.3\n65.6\n68.2\n24.2\n22.1\n65.4",
          "SNIPS\nASR\nIC\nSF\n(WER ‚Üì)\n(Acc % ‚Üë)\n(F1 ‚Üë,CER ‚Üì)": "8.5\n98.4\n94.7, 12.9\n7.7\n98.7\n94.4, 13.5\n7.3\n99.1\n95.4, 11.8",
          "FSC\nASR\nIC\n(WER ‚Üì)\n(Acc % ‚Üë)": "0.6\n99.4\n0.6\n99.5\n0.6\n99.3"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Findings of the iwslt 2022 evaluation campaign",
      "authors": [
        "Anastasopoulos Antonios",
        "Barrault Loc",
        "Luisa Bentivogli",
        "Zanon Marcely",
        "Bojar Boito",
        "Roldano Ond≈ôej",
        "Currey Cattoni",
        "Dinu Anna",
        "Duh Georgiana",
        "Elbayad Kevin",
        "Maha"
      ],
      "year": "2022",
      "venue": "Proc. of the 19th Int. Conf. on Spoken Language Translation"
    },
    {
      "citation_id": "2",
      "title": "Unified-modal encoderdecoder pre-training for spoken language processing",
      "authors": [
        "Junyi Ao",
        "Rui Wang",
        "Long Zhou",
        "Chengyi Wang",
        "Shuo Ren",
        "Yu Wu",
        "Shujie Liu",
        "Tom Ko",
        "Qing Li",
        "Yu Zhang"
      ],
      "year": "2022",
      "venue": "Proc. of the 60th Annual Meeting of the ACL"
    },
    {
      "citation_id": "3",
      "title": "Espnet-slu: Advancing spoken language understanding through espnet",
      "authors": [
        "Siddhant Arora",
        "Siddharth Dalmia",
        "Pavel Denisov",
        "Xuankai Chang",
        "Yushi Ueda",
        "Yifan Peng",
        "Yuekai Zhang",
        "Sujay Kumar",
        "Karthik Ganesan",
        "Brian Yan"
      ],
      "year": "2022",
      "venue": "Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "4",
      "title": "Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou"
      ],
      "year": "2020",
      "venue": "Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "Speech emotion recognition with multi-task learning",
      "authors": [
        "Xingyu Cai",
        "Jiahong Yuan",
        "Renjie Zheng",
        "Liang Huang",
        "Kenneth Church"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "7",
      "title": "Speechnet: A universal modularized model for speech processing tasks",
      "authors": [
        "Yi-Chen Chen",
        "Po-Han Chi",
        "Shu-Wen Yang",
        "Kai-Wei Chang",
        "Jheng-Hao Lin",
        "Sung-Feng Huang",
        "Da-Rong Liu",
        "Chi-Liang Liu",
        "Cheng-Kuang Lee",
        "Hung-Yi Lee"
      ],
      "year": "2021",
      "venue": "Speechnet: A universal modularized model for speech processing tasks",
      "arxiv": "arXiv:2105.03070"
    },
    {
      "citation_id": "8",
      "title": "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces",
      "authors": [
        "Alice Coucke",
        "Alaa Saade",
        "Adrien Ball",
        "Th√©odore Bluche",
        "Alexandre Caulier",
        "David Leroy",
        "Cl√©ment Doumouro",
        "Thibault Gisselbrecht",
        "Francesco Caltagirone",
        "Thibaut Lavril",
        "Ma√´l Primet",
        "Joseph Dureau"
      ],
      "year": "2018",
      "venue": "Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces"
    },
    {
      "citation_id": "9",
      "title": "Endto-end speech emotion recognition combined with acoustic-to-word asr model",
      "authors": [
        "Han Feng",
        "Sei Ueno",
        "Tatsuya Kawahara"
      ],
      "venue": "Endto-end speech emotion recognition combined with acoustic-to-word asr model"
    },
    {
      "citation_id": "10",
      "title": "Multi-task rnn-t with semantic decoder for streamable spoken language understanding",
      "authors": [
        "Xuandi Fu",
        "Feng-Ju Chang",
        "Martin Radfar",
        "Kai Wei",
        "Jing Liu",
        "Grant Strimel",
        "Kanthashree Sathyendra"
      ],
      "year": "2022",
      "venue": "Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "11",
      "title": "Marcely Zanon Boito, and Ioan Calapodescu. Naver labs europe's multilingual speech translation systems for the iwslt 2023 lowresource track",
      "authors": [
        "Edward Gow-Smith",
        "Alexandre Berard"
      ],
      "year": "2023",
      "venue": "Marcely Zanon Boito, and Ioan Calapodescu. Naver labs europe's multilingual speech translation systems for the iwslt 2023 lowresource track",
      "arxiv": "arXiv:2306.07763"
    },
    {
      "citation_id": "12",
      "title": "Parameter-efficient transfer learning for nlp",
      "authors": [
        "Neil Houlsby",
        "Andrei Giurgiu",
        "Stanislaw Jastrzebski",
        "Bruna Morrone",
        "Quentin De Laroussilhe",
        "Andrea Gesmundo",
        "Mona Attariyan",
        "Sylvain Gelly"
      ],
      "year": "2004",
      "venue": "Int. Conf. on Machine Learning"
    },
    {
      "citation_id": "13",
      "title": "Towards semi-supervised semantics understanding from speech",
      "authors": [
        "Cheng-I Lai",
        "Jin Cao",
        "Sravan Bodapati",
        "Shang-Wen Li"
      ],
      "year": "2020",
      "venue": "Towards semi-supervised semantics understanding from speech",
      "arxiv": "arXiv:2011.06195"
    },
    {
      "citation_id": "14",
      "title": "Lightweight adapter tuning for multilingual speech translation",
      "authors": [
        "Hang Le",
        "Juan Pino",
        "Changhan Wang",
        "Jiatao Gu",
        "Didier Schwab",
        "Laurent Besacier"
      ],
      "year": "2021",
      "venue": "Proc. of the 59th Annual Meeting of the ACL and the 11th Int. Joint Conf. on Natural Language Processing"
    },
    {
      "citation_id": "15",
      "title": "A joint multi-task learning framework for spoken language understanding",
      "authors": [
        "Changliang Li",
        "Cunliang Kong",
        "Yan Zhao"
      ],
      "year": "2018",
      "venue": "Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "16",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2004",
      "venue": "Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Speech model pre-training for end-to-end spoken language understanding",
      "authors": [
        "Loren Lugosch",
        "Mirco Ravanelli",
        "Patrick Ignoto",
        "Vikrant Singh Tomar",
        "Yoshua Bengio"
      ],
      "year": "2019",
      "venue": "Speech model pre-training for end-to-end spoken language understanding"
    },
    {
      "citation_id": "18",
      "title": "The natural language decathlon: Multitask learning as question answering",
      "authors": [
        "Bryan Mccann",
        "Nitish Shirish Keskar",
        "Caiming Xiong",
        "Richard Socher"
      ],
      "year": "2018",
      "venue": "The natural language decathlon: Multitask learning as question answering",
      "arxiv": "arXiv:1806.08730"
    },
    {
      "citation_id": "19",
      "title": "Multitask learning for low resource spoken language understanding",
      "authors": [
        "Quentin Meeus",
        "Marie-Francine Moens"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "20",
      "title": "Librispeech: an asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "Int. Conf. on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "21",
      "title": "Adapterhub: A framework for adapting transformers",
      "authors": [
        "Jonas Pfeiffer",
        "Andreas R√ºckl√©",
        "Clifton Poth",
        "Aishwarya Kamath",
        "Ivan Vuliƒá",
        "Sebastian Ruder",
        "Kyunghyun Cho",
        "Iryna Gurevych"
      ],
      "year": "2004",
      "venue": "Adapterhub: A framework for adapting transformers",
      "arxiv": "arXiv:2007.07779"
    },
    {
      "citation_id": "22",
      "title": "A cointeractive transformer for joint slot filling and intent detection",
      "authors": [
        "Libo Qin",
        "Tailu Liu",
        "Wanxiang Che",
        "Bingbing Kang",
        "Sendong Zhao",
        "Ting Liu"
      ],
      "year": "2021",
      "venue": "Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "23",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision",
      "arxiv": "arXiv:2212.04356"
    },
    {
      "citation_id": "24",
      "title": "A general multi-task learning framework to leverage text data for speech to text tasks",
      "authors": [
        "Yun Tang",
        "Juan Pino",
        "Changhan Wang",
        "Xutai Ma",
        "Dmitriy Genzel"
      ],
      "year": "2021",
      "venue": "Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "25",
      "title": "Efficient adapter transfer of self-supervised speech models for automatic speech recognition",
      "authors": [
        "Bethan Thomas",
        "Samuel Kessler",
        "Salah Karout"
      ],
      "year": "2022",
      "venue": "Int. Conf. on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "26",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "≈Åukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "27",
      "title": "Hybrid ctc/attention architecture for end-to-end speech recognition",
      "authors": [
        "Shinji Watanabe",
        "Takaaki Hori",
        "Suyoun Kim",
        "John Hershey",
        "Tomoki Hayashi"
      ],
      "year": "2017",
      "venue": "IEEE Journal of Selected Topics in Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "A survey of joint intent detection and slot filling models in natural language understanding",
      "authors": [
        "Henry Weld",
        "Xiaoqi Huang",
        "Siqu Long",
        "Josiah Poon",
        "Caren Soyeon",
        "Han"
      ],
      "venue": "ACM Computing Surveys"
    },
    {
      "citation_id": "29",
      "title": "Speech processing universal performance benchmark",
      "authors": [
        "Shu-Wen Yang",
        "Po-Han Chi",
        "Yung-Sung Chuang",
        "Cheng-I Jeff Lai",
        "Kushal Lakhotia",
        "Andy Yist Y Lin",
        "Jiatong Liu",
        "Xuankai Shi",
        "Guan-Ting Chang",
        "Lin"
      ],
      "year": "2004",
      "venue": "Speech processing universal performance benchmark",
      "arxiv": "arXiv:2105.01051"
    },
    {
      "citation_id": "30",
      "title": "Speech selfsupervised representation benchmarking",
      "authors": [
        "Salah Zaiem",
        "Youcef Kemiche",
        "Titouan Parcollet",
        "Slim Essid",
        "Mirco Ravanelli"
      ],
      "year": "2023",
      "venue": "Are we doing it right? arXiv preprint",
      "arxiv": "arXiv:2306.00452"
    },
    {
      "citation_id": "31",
      "title": "Multimodal robustness for neural machine translation",
      "authors": [
        "Yuting Zhao",
        "Ioan Calapodescu"
      ],
      "year": "2022",
      "venue": "Proc. of the 2022 Conf. on Empirical Methods in Natural Language Processing"
    }
  ]
}