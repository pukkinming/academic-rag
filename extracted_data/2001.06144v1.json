{
  "paper_id": "2001.06144v1",
  "title": "Learning To Augment Expressions For Few-Shot Fine-Grained Facial Expression Recognition",
  "published": "2020-01-17T03:26:32Z",
  "authors": [
    "Wenxuan Wang",
    "Yanwei Fu",
    "Qiang Sun",
    "Tao Chen",
    "Chenjie Cao",
    "Ziqi Zheng",
    "Guoqiang Xu",
    "Han Qiu",
    "Yu-Gang Jiang",
    "Xiangyang Xue"
  ],
  "keywords": [
    "Facial expression recognition",
    "few-shot learning",
    "generative adversarial networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Affective computing and cognitive theory are widely used in modern human-computer interaction scenarios. Human faces, as the most prominent and easily accessible features, have attracted great attention from researchers. Since humans have rich emotions and developed musculature, there exist a lot of subtle and fine-grained expressions in real-world applications. However, it is extremely time-consuming to collect and annotate a large number of facial images, of which some subtle expressions may even require psychologists to correctly categorize them. To the best of our knowledge, the existing expression datasets are only limited to several basic facial expressions, which are not sufficient to support our ambitions in developing successful human-computer interaction systems. To this end, a novel Fine-grained Facial Expression Database -F 2 ED is contributed in this paper; and such a dataset costs one year to be collected by us and annotated with the help of psychological annotators. Totally, it includes more than 200k images with 54 facial expressions from 119 persons. So far as we know, this is the first large dataset to label faces with subtle emotion changes for the recognition of facial expressions. Considering the phenomenon of uneven data distribution and lack of samples is common in real-world scenarios, we further evaluate several tasks of few-shot expression learning by virtue of our F 2 ED, which are to recognize the facial expressions given only few training instances. These tasks mimic human performance to learn robust and general representation from few examples. To address such few-shot tasks, we propose a unified task-driven framework -Compositional Generative Adversarial Network (Comp-GAN) learning to synthesize facial images and thus augmenting the instances of few-shot expression classes. Essentially, Comp-GAN consists of two generators: one for editing faces with desired expression and the other for changing the face posture; so it can generate many realistic and high-quality facial images according to specified posture and expression information while keeping the identity features. Extensive experiments are conducted on F 2 ED and existing facial expression datasets, i.e., JAFFE and FER2013, to validate the efficacy of our F 2 ED in pre-training facial expression recognition network and the effectiveness of our proposed approach Comp-GAN to improve the performance of few-shot recognition tasks.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Affective Computing is one important research topic for human-computer interaction  [1] . With the development of deep models deployed on mobile devices, affective computing enables various applications in psychology, medicine, security and education  [2] ,  [3] . In general, human eyes can easily recognize the facial expression; but it is still a challenge for artificial intelligence algorithms to effectively recognize the versatile facial emotional expressions.\n\nIt is well known that facial expression is the best visual representation of a person's emotional status. According to  [11] , it is found that in years of observation and research the facial expression of emotion is a common characteris- tic of human beings and contains meaningful information in communication. Humans can always reliably generate, understand and recognize facial emotional expressions. Indeed, human emotional expressions are designed to deliver useful and reliable information between different persons, so that people can decode each other's psychological states from these designed emotion expressions. Facial expression recognition is widely used in multiple applications such  as psychology, medicine, security, and education  [12] . In psychology, it can be used for depression recognition for analyzing psychological distress. On the other hand, detecting the concentration or frustration of students is also helpful in improving the educational approach. Due to the above reasons, facial expression recognition has become the recent frontier in affective computing and computer vision. Although facial expression plays an important role in affective computing, there is no uniform facial expression labeling system due to its subjective nature. According to Ekman's theory  [13] , which is the most widely used labeling system in FER, the emotion set is composed of six basic emotion types: anger, disgust, fear, happy, sad and surprise. Plutchik's wheel  [14]  expands the emotion set to contain more diverse and subtle/fine-grained expressions, which are very valuable to real-world applications. For example, fatigue expression is important to monitor the status of drivers, which is critical for traffic safety. Due to the simplicity of Ekman's theory, most academic datasets only contain six basic emotions with an additional neutral emotion, such as CK+  [4] , JAFFE  [5] , FER2013  [7]  and FER-Wild  [8] , as shown in Tab. 1. Thus it is necessary to create a dataset of more fine-grained emotions to fill the gap between academic research and industrial applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Fine-Grained Expression Dataset",
      "text": "Although, it is urgent to introduce fine-grained facial expressions into the study, contributing such a large scale facial expression dataset is non-trivial. Typically, the collection procedure should be carefully designed to ensure that humans correctly convey the desired facial expressions. Significant effort and contributions from both psychologists and subjects have been made in our expression collection, including explanations of emotion, scripts for emotion induction, communication with psychologists, etc. Furthermore, a careful review mechanism  [15]  from the expert-level judgments of psychologists is also designed to guarantee the quality of collected facial expressions.\n\nIn this work, we contribute the first large fine-grained facial expression dataset F 2 ED (Fine-grained Facial Expression Database) with 54 expression emotions, such as calm, embarrassed, pride, tension and so on, which includes abundant emotions with subtle changes, as shown in Fig.  1 . These 54 expressions are classified by referring to the recent psychological work  [16]  with discernibility and rationality. Three psychologists and several doctoral students participate in the whole collection and annotation process. Further, we also consider the influence of facial pose changes on the expression recognition, and introduce the pose as another attribute for each expression. Four orientations (postures) including front, half left, half right and bird view are labeled, and each has a balanced number of examples to avoid distribution bias.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Few-Shot Fine-Grained Expression Recognition",
      "text": "In the field of vision-based human-computer interaction, facial expression recognition (FER) is always a hot research topic. Recently, the renaissance of deep neural networks has significantly improved the performance of FER tasks. The results on those well-known public facial recognition datasets show that the deep neural networks based FER methods which can learn both the low-level and highlevel features from facial images  [17] ,  [18] ,  [19]  have outperformed the traditional methods based on hand-crafted features  [20] , [21],  [22] ,  [23] .\n\nDespite the encouraging advancements in these FER works, several key challenges still remain in extending FER system to real-world applications: (1) lack of sufficient and diverse high-quality training data.  (2)  vulnerable to the variations of facial posture and person identity.\n\nLacking sufficient data is a severe problem for FER, since deep neural network needs a large scale labeled dataset to prevent the over-fitting problem. Most research works frame the FER task as a typical supervised learning problem, and assume there are plenty of training data for each emotion. However, the annotation task for facial expression generally requires devoted contributions from the experts, and the labeling procedure is much more difficult and time-consuming than labeling image class  [24] . It is thus a severe problem in training deep FER models. To bypass the mentioned problem, it is vital to get proper feature representations for classification under the limited number of training samples. Typically, new expressions with only few training examples may be encountered in the real world. Such few-shot expression learning aims to mimic human performance in understanding facial expressions from few training instances. For the first time, this work extends the few-shot object classification to few-shot expression learning, following the typical few-shot learning setting  [25] . As illustrated in Fig.  2",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Learning To Augment Faces",
      "text": "The problem of few-shot learning is common in practical applications, and the lack of training data can lead to a significant decrease in FER accuracy. To alleviate the impediment of the expression-unrelated variations such as poses, illuminations and identities, one approach is to use various normalization mechanisms such as illumination normalization, pose normalization [23],  [26] . However, it is too cumbersome to develop different normalization mechanisms for each expression-unrelated variation, and the mutual effects of those normalization mechanisms may weaken the ability of deep FER models. The data augmentation technique may mitigate this problem in a more simple and smooth way. It can synthesize more diverse training data to make the learned model more robust to the unrelated noise. Several GAN-based methods are applied in synthesizing faces with different expressions  [27] , poses  [28]  and identities  [29] , respectively. However, they do not properly preserve the identity or expression information in generating the target images, and the generated images are distorted.\n\nIn this work, we propose a novel unified Compositional Generative Adversarial Network (Comp-GAN) to synthesize realistic facial images with arbitrary poses and expressions while keeping the identity information. Our Comp-GAN model consists of two generators: one for generating images of desired expressions, and the other for editing the poses of faces. The two generators have different focuses and are complementary to each other. The structure of our Comp-GAN is composed of two branches, which have the same ultimate generating goal, thus forming a closed-loop to the network structure. Each branch has four generating steps, changing one attribute of the face at each step, and the goal of editing facial expression and posture is achieved through the successive multi-step generating process. The difference between the two branches mainly lies in the different orders of generating attributes, e.g., one branch changes the posture first, while the other branch edits the expression first. The two branches constrain each other and improve the quality of the synthesized images. We also apply a reconstruction learning process to re-generate the input image and encourage the generators for preserving the key information such as facial identity. Aiming at enforcing the generative models to learn expression-excluding details, we employ several task-driven loss functions to synthesize more realistic and natural images, which can effectively solve the problem of insufficient training data. With more diverse training images, the FER model is more robust to various expression-unrelated changes.\n\nContribution. The contributions are as follows, (1) For the first time, we introduce a new fine-grained facial expression dataset F 2 ED with three attributes (face identity, pose, and expression) containing 54 different emotion types and more than 200k examples. The 54 expressions have greatly enriched the emotion categories, and provide more practical application scenarios. (2) Considering the lack of diverse and sufficient training data in the real-word scenarios, we design several few-shot expression learning tasks for FER to further investigate how the poses, expressions, and subject identities affect the FER model performance.  (3)  We propose a novel end-to-end Compositional Generative Adversarial Network (Comp-GAN) to synthesize natural and realistic images to improve the FER performance under the few-shot setting. We also introduce a closed-loop learning process and several task-driven loss functions in Comp-GAN, which can encourage the model to generate images with the desired expressions, specified poses and meanwhile keep the expression-excluding details, such as identity information. (4) We conduct extensive experiments on F 2 ED, as well as JAFFE  [5]  and FER2013  [7]  to evaluate our new dataset and framework. The experimental results show that our dataset F 2 ED is large enough to be used for pre-training a deep network to improve the recognition accuracy, and the images generated by Comp-GAN can be used to alleviate the problem of insufficient training data in few-shot expression setting, resulting in a more powerful model.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Affective Computing And Cognitive Theory",
      "text": "Affective computing and cognitive theory are the intersection of psychology, physiology, cognition, and computer technology  [2] . They can be widely applied to driver pressure or fatigue monitoring, emotion robot  [30] , humancomputer interaction (HCI)  [1]  and special medical service.\n\nAt present, extensive researches have been conducted on achieving affective understanding and cognition between persons and computers  [31] . As the base signal of affective computing and cognitive theory, facial expressions are the easiest visual features to be observed and detected. Especially, the research of expression recognition is important for the research of HCI and emotional robot  [32] . Our work is primarily based on the analysis and understanding of facial expressions to help affective understanding and cognition.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Evolutionary Psychology In Emotional Expression",
      "text": "Ekman et al. find  [11]  that the expression of emotion is common to human beings in years of observation and research. No matter where it is tested, humans can always reliably generate, understand and recognize related emotional expressions. Indeed, human emotional expressions are designed to provide information, and they need to be delivered reliably, so that humans have coevolved automatic facial expressions that decode these public expressions into insights of other people's psychological states. Even though, people sometimes lie, but inferences about emotional states from facial expressions don't evolve unless they create a stronger advantage for the inferrer, suggesting that these inferences are often valid and credible.\n\nIn recent years, psychologists and computational science specialists have proposed expression recognition models based on cognition, probability and deep learning network  [17] ,  [33] ,  [34] . Most of these works are based on Ekman's six basic pan-cultural emotions  [13] , however, human emotional world is rich and colorful, with facial muscles and nerves well developed, so more and more works begin to broaden the emotion and expression categories that can be recognized  [35] ,  [36] . To further study the complex and subtle expressions of humans, inspired by  [16]  which expands the emotion set, we collect a new dataset F 2 ED with 54 subtle emotional expressions, thus to a great extent to provide accurate and rich psychological and visual intersection of expression information.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Facial Expression Recognition In Computer Vision",
      "text": "Original affective computing mainly focused on facial expression recognition (FER), which has gained great progress in recent years. Facial expressions can be recognized by two measurements: message and sign judgment  [37] . In message judgment, facial expressions are categorized by the emotion conveyed by the face such as angry, fear and happy. In signal judgment, facial expressions are studied by physical signals such as raised brows or depressed lips. There are mainly two kinds of FER methods according to the input type: static image FER and dynamic sequence FER  [38] . The static image FER only uses the visual information in a single image to predict the facial expression, whereas the dynamic sequence FER also leverages the temporal information between the frames in videos to predict the facial expression  [38] . In this paper, we focus on how to use a static image to predict the emotion type such as happy and sad.\n\nThe most common static image-based FER method is composed of three main steps: pre-processing, feature extraction and facial expression classification. In the first step, there are two subtasks: face detection and face alignment. For face detection, the faces are detected from the image and labeled with bounding boxes. For face alignment, crucial landmarks are used to align the face by warping affine method. In the second step, feature extraction converts the image from pixel-level information to high-level representation, such as appearance features (e.g. Garbor wavelet  [39]  LBP  [40] , HOG  [41]  and SIFT  [42] ), geometric features and deep learning features. In the third step, an additional classifier can be adopted in the facial expression classification, such as MLP, SVM, and KNN.\n\nInspired by the success of deep neural networks on the vision tasks such as image classification and object detection, extensive efforts  [17] ,  [18] ,  [19] ,  [43] ,  [44]  have been made to employ the deep neural networks in the FER tasks. To name some promising works, Khorrami et al.  [17]  develop a zero-bias CNN for FER task, and find that those maximally activated neurons in convolutional layers strongly correspond to the Facial Action Units (FAUs)  [45]  by visualization. In  [18]  a deep neural network with the inception layer is proposed, and results show that the performance have achieved or outperformed the state-of-theart on MultiPIE, CK+, FER2013, and other common datasets. Attentional CNN  [19]  combines a spatial transformer with CNN to focus on the most salient regions of faces in FER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Gan-Based Recognition Approach",
      "text": "Generative Adversarial Networks (GANs)  [46]  based models have also been utilized in the FER task. Particularly, GAN is a minimax game between a generator and a discriminator. Conditional Generative Adversarial Nets (cGAN)  [47]  is proposed to generate the images conditioned on the class label. Isola et al.  [48]  introduce a Pix2Pix model that combines cGAN with U-Net to generate a new image conditioned on the input image, low-level information is shared between input and output through U-Net. Zhu et al.  [49]  propose a CycleGAN model that employs a pair of GANs between two image domains to form a cycle, cycle consistent loss is computed in both the forward cycle and backward cycle. Qian et al.  [26]  propose a generative adversarial network (GAN) designed specifically for pose normalization in re-id. Larsen et al. advocate a VAE/GAN  [50]  that combines GAN with auto-encoder, and high-level features can be learned and used for editing in the latent space of auto-encoder.\n\nIn order to weaken the impedance of expressionunrelated factors, various GAN based FER methods are proposed. Yan et al.  [51]  propose a de-expression model to generate neutral expression images from source images by cGAN  [47] , then the residual information in intermediate layers is used for facial expression. Lai et al. in  [28]  propose a GAN to generate a frontal face from a non-frontal face while preserving the emotion. Yang et al.  [27]  utilize a cGAN to produce six prototypic expression images for any source image, and the expression of the source image is recognized by the minimum distance between the source image and the generated six images in a subspace. Chen et al.  [29]  leverage a variational generative adversarial network (VGAN) to encode the source image into an identity-invariant latent space, and generate a new image with desired identity code while keeping the expression unchanged. However, these GAN based methods can only synthesize new images with one attribute different.\n\nSeveral existing works [23],  [52]  attempt to edit the multiple facial attributes in a unified model. He et al.  [52]   propose an Attribute GAN(AttGAN) model which can edit any attribute among a collection of attributes for face images by employing adversarial loss, reconstruction loss and attribute classification constraints.  Zhang et al. [23]  propose a joint pose and expression GAN to generate new face images with different expressions under arbitrary poses. However, these methods only employ a content similarity loss on the cycle branch where the output shares the same attributes (e.g., expression, pose) as the source image. Such design may be partially due to the lack of target ground truth images in the training set. Thus, the target branch that generates face images with different attributes is not constrained by the content similarity loss, which may weaken the ability to preserve the other facial information from the source image.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Few-Shot Learning",
      "text": "Few-shot learning  [53]  aims to learn a new concept from a limited number of labeled training data. There are three methods commonly used in few-shot learning: metalearning based methods  [54] ,  [55] , metric-based methods  [56]  and augment-based methods  [23] . Meta-learning  [54]  can transfer the knowledge from previous different domains to boost the performance on the new task. The pre-defined component in the training procedure can be taken as prior knowledge, and trained by the meta-learner. For example, the initial model parameters are taken as prior knowledge in MAML  [54] , and the parameter updating rules are taken as prior knowledge in Ravi's work  [55] . Inspired by FER tasks [23], our approach uses GAN to synthesize more training data rather than linear transformation of pairwise images.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Fine-Grained Facial Expression Database",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "How To Differentiate Fine-Grained Expressions?",
      "text": "To further investigate the subtle expressions of the human faces, we can classify expressions based on facial features (rather than conceptual psychological states). We use this concept to construct our dataset for two reasons.\n\nFirst, the basis of expression in sensory functions means that certain types of expressions are not arbitrary or random, and some expressions look that way because they have interfaces that match their environment  [57] . Thus, some indistinguishable mental states  [58]  that are conceptually similar (e.g., fear is similar to disgust), present subtle expression differences (e.g., fear is opposite to disgust).\n\nSecond, we are studying subtle variations in facial expressions, which have a wide range of real-world applications, physical attributes (rather than conceptual attributes) are crucial because they constitute essential signals to be sent to the recipient for understanding.\n\nIn this work, we expand the expression set to 54 types of expressions. Particularly, in term of the theory of Lee  [16] , which demonstrates the eye region can reliably convey diagnostic information about discrete emotion states, e.g., the eye features associated with happiness are consistent with a group of positive and stable mental states, while the eye features associated with sadness align with a cluster of negative and steady emotion states. To this end, we can easily differentiate the expression set of 54 types of expressions, which include more complex mental states based on seven eye features, i.e., temporal wrinkles, wrinkles below eyes, nasal wrinkles, brow slope, brow curve, brow distance, and eye apertures.\n\nThe 54 emotions can be clustered into 4 groups by the k-means clustering algorithm as shown in Fig.  1 , and the similar mental-state map in  [16]  shows that the eyenarrowing features of disgust are consistent with a range of mental states that express social discrimination, such as hate, suspicion, aggression, and contempt, which further prove the distinguishable nature of the 54 expressions.\n\nWe also visualize the feature distributions of data using randomly sampled 6 kinds of indistinguishable mental expressions (i.e., desire, joy, love, admiration, anticipation, and optimism) from the same person via t-SNE in Fig.  3 , which demonstrates that our expressions are totally distinguishable. For the same person with the same pose, the images with different expressions have higher similarity, as the bottom two faces circled by orange and yellow dash lines separately in Fig.  3 , which also reflects the difficulty of our dataset and the fine-grained expression recognition task, and this is the main reason why we invite professional psychologists to participate in the labeling work.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Collection And Processing",
      "text": "Data Collection. To make our dataset more practical, we invite three psychologists and several doctoral students to conduct relevant research, determine the categories of facial expressions, improve the process of guiding participants and confirm the labeling methods. The whole video data collection takes six months. Totally, we aim at capturing 54 different types of expressions  [16] , e.g., acceptance, angry, bravery, calm, disgust, envy, fear, neutral and so on. We invite more than 200 different candidates who are unfamiliar with our research topics. Each candidate is captured by four cameras placed at four different orientations to collect videos every moment. The four orientations are front, half left, half right and bird view. The half left and half right cameras have a horizontal angle of 45 degrees with the front of the person, respectively. The bird view camera has a vertical angle of 30 degrees with the front of the person. Each camera takes 25 frames per second. The whole video capturing process is designed as a normal conversation between the candidate and at least two psychological experts. The conversation will follow a script which is calibrated by psychologists, starting with the explanation of a particular expression definition by psychologists, followed by a description of the relevant scene including emotion, and finally letting the participants state similar personal experiences to induce/motivate them to successfully express the particular type of expression. For each candidate, we only select 5 seconds' video segment for each type of emotion, as noted and confirmed by psychologists. To reduce the subjective interference of participants, every subject has to cool down before a new emotion recording during the data collection.\n\nData Processing. With gathered expression videos, we further generate the final image dataset by human review, key image generation, and face alignment. Specifically, the human review step is very important to guarantee the general quality of recorded expressions. Three psychologists and five doctoral students are invited to help us review the captured emotion videos. Particularly, each captured video is given a score of 1-3 by these experts based on the video quality. We only select the videos that have an average score of beyond 1.5. Thus totally about 119 identities' videos are kept finally. Then key frames are extracted from each video. Face detection and alignment are conducted by the toolboxes of Dlib and MTCNN  [44]  over each frame. Critically, the face bounding boxes are cropped from the original images and resized to a resolution of 256 × 256 pixels. Finally, we get the dataset F 2 ED of totally 219, 719 images with 119 identities, 4 different views and 54 kinds of fine-grained facial expressions.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Statistics Of F 2 Ed",
      "text": "Our dataset is labeled with identity, pose, and expression.\n\nIdentity. The 119 persons are mainly university students including 37 male and 82 female aging from 18 to 24. Each person expresses emotions under the guidance and supervision of psychologists, and the video is taken when the emotion is observed and confirmed by experts. HL HR F BV Expression. F 2 ED contains 54 fine-grained facial expressions, which is helpful to understand the human emotion status for affective computing and cognitive theory. The number of images over each expression is shown in Fig.  6 , which indicates that F 2 ED has a relatively balanced distribution across various expressions.\n\nComparison with Previous Datasets. Table . 1 shows the comparison between our F 2 ED with existing facial expression database. As shown in the table, our dataset contains 54 fine-grained expression types, while other datasets only contain 7 or 8 expression types in the controlled environment, 23 in the wild. For the person number, CK+  [4] , KDEF  [6]  and F 2 ED are nearly the same. The current public facial expression datasets are usually collected in two ways: in the wild or in the controlled environment. The FER2013  [7] , FER-Wild  [8] , EmotionNet  [9] , AffectNet  [10]  are collected in the wild, so the number of poses and subjects can not be determined. The rest datasets are collected in a controlled environment, where the number of poses for CK+ and JAFFE  [5]  is 1, KDEF is 5 and F 2 ED is 4. Our F 2 ED is the only one that contains the bird view pose images which are very useful in real-world applications. For image number, F 2 ED contains 219,719 images, which is 44 times larger than the second-largest dataset in the controlled environment. We show that our F 2 ED is orders of magnitude larger than these existing datasets in terms of expression class numbers, the number of total images and the diversity of data.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Methodology",
      "text": "Problem Definition. Typically, facial expression recognition task (FER) aims to learn a classifier that can predict the existence of expression to input images. Assume we have a training dataset D s = I j,(i,p,e) N j=1 , where j means the jth image, i is the identity label, p indicates the posture label, and e represents the expression label. We use I (i,p,e) to denote the face image of the person i with posture p and expression e. Given an unseen test face image I (i,p,e) , our goal is to learn a robust mapping function e = Ψ I (i,p,e) using all available training information to predict the expression category e . To be noticed, each image is only labeled with one expression type.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Few-Shot Fine-Grained Facial Expression Learning",
      "text": "Generally due to the lack of sufficient and diverse facial expression training data with dramatically changed posture, learning a robust facial expression recognition model can be very challenging. With regard to this problem, we introduce the more practical few-shot learning case specializing to recognize the samples appearing only a few or unseen during the training stage, by learning the generalized features from a limited number of labeled data.\n\nFollowing the recent works  [54] ,  [59] ,  [60] ,  [61] , we establish a group of few/zero-shot settings in FER task as in Fig.  2 : we firstly define a base category set C base and a novel category set C novel , in which C base ∩ C novel = φ. Correspondingly, we have a base dataset D base = {(I (i,p,e) ), (i, p, e) ⊂ C base }, and a novel dataset D novel = (I (i,p,e) ), (i, p, e) ⊂ C novel . Our goal is to learn a generalized classification model that is able to infer the novel class data trained on the D base and D novel with few or no samples per C novel . Particularly, we propose the following tasks in the context of various standard problems, (T1) ER-SS (FER under the Standard Setting): according to the general recognition task, the supervised setting is introduced into our work. We set C novel = φ and D novel = φ, and directly learn the supervised classifier on the randomly sampled 80% of all images, and test on the rest images. During the random sampling process, we ensure the expression, identity, and pose with a balanced distribution. (T2) ER-FID (FER under the Few-shot IDentity setting): In the real-world applications, it is impossible to have training data with various expressions and postures from everyone. So studying few-shot fine-grained expression recognition learning in terms of training identity is important to the realworld applications. We randomly choose 20% identities as C novel and the rest as C base , and randomly sample 1, 3 and 5 images per identity of C novel into D novel respectively; 80% of total number of images of C base as D base . We stochastically choose 20% of all images of each identity from the rest images as the test data. In the above random sampling process, the balance of expression and posture distribution should be ensured simultaneously.  simultaneously ensure the expression and identity balanced distribution. Facial expressions vary a lot, so it is hard to collect a dataset with even distribution of expressions, which puts forward higher requirements for the application of expression recognition in practice. Therefore, it is inevitable to study the fine-grained expression recognition task with uneven expression distribution.\n\nOverview. To tackle the challenges introduced in Sec. 1, we propose the Compositional Generative Adversarial Network (Comp-GAN) to generate desired expression and specified pose images according to the input references while keeping the expression-excluding details such as face identity. The generated images can be adopted to train a robust expression recognition model. The unified facial expression recognition architecture has two components: Comp-GAN as in Fig.  7 , and expression classifier network Comp-GAN-Cls based on LightCNN-29v2  [62] .",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Structures Of Comp-Gan",
      "text": "To solve the problem of changing postures and complex expressions in the facial expression recognition task, we propose a Compositional Generative Adversarial Network (Comp-GAN) to generate new realistic face images, which dynamically edits the facial expression and pose according to the reference image, while keeping the identity information. As shown in Fig.  7 , a stacked GAN with supervision information is presented to guide the generative learning process. The generator in Comp-GAN is stacked by expression and pose components, i.e., G (Exp) and G (Pose). The former one serves as the editor of desired expressions, while the latter one can synthesize faces by varying the facial posture.\n\nFormally, we denote i t , i r , p s , p t , e s and e t as the target identity, reference identity, source posture, target pose, source expression, and target expression, respectively. The generator G (Pose) aims at transferring the source face posture p s to the target pose p t , and generator G (Exp) tries to change the source facial expression e s to the target expression e t , while keeping their identity information i t . Thus our Comp-GAN can generate the target face I (it,pt,et) , and an approximation to reconstruct the image I (it,ps,es) to remain the pose-invariant and expression-invariant information. Specifically, the whole model has two branches and four steps, and the workflow of our Comp-GAN is illustrated in Alg. 1. Note that, we utilize the subindex k ∈ {a, b} to indicate the intermediate results produced by the k-th branch of Comp-GAN, and to better understand, we simplify the I (it,pt,es) as (i t , p t , e s ) to represent the face with target identity, target pose and source expression.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Algorithm 1 Algorithm Of Comp-Gan.",
      "text": "Input: (i t , p s , e s ) indicates the image with target identity, ((i r , p t , e s ), (i r , p s , e t ), (i r , p t , e t ), (i r , p s , e s )) are the reference images with target posture or expression information.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Output:",
      "text": "((i t , p t , e s ) a , (i t , p t , e t ) b , (i t , p s , e t ) a , (i t , p s , e s ) b ) are the generated images with edited posture by G (Pose); ((i t , p s , e t ) b , (i t , p t , e t ) a , (i t , p t , e s ) b , (i t , p s , e s ) a ) mean the synthesized images with changed expression by G (Exp).",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "1.",
      "text": "F P : ((i t , p s , e s ), (i r , p t , e s )) → ((i t , p t , e s ) a ), F E : ((i t , p s , e s ), (i r , p s , e t )) → ((i t , p s , e t ) b )",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "2.",
      "text": "F E : ((i t , p t , e s ) a , (i r , p t , e t )) → ((i t , p t , e t ) a ), F P : ((i t , p s , e t ) b , (i r , p t , e t )) → ((i t , p t , e t ) b )",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "3.",
      "text": "F P : ((i t , p t , e t ) a , (i r , p s , e t )) → ((i t , p s , e t ) a ), F E : ((i t , p t , e t ) b , (i r , p t , e s )) → ((i t , p t , e s ) b )",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "4.",
      "text": "F E : ((i t , p s , e t ) a , (i r , p s , e s )) → ((i t , p s , e s ) a ), F P : ((i t , p t , e s ) b , (i r , p s , e s )) → ((i t , p s , e s ) b )\n\nIn particular, F P , i.e., G (Pose), learns to change the pose while keeping the expression and identity information as the reference data, and F E , i.e., G (Exp), learns to generate desired expression image while maintaining the expressionexcluding information. After the first two steps, we get the specified pose data (i t , p t , e s ) a and desired expression image (i t , p s , e t ) b , as well as the (i t , p t , e t ) a and (i t , p t , e t ) b whose posture and expression are changed simultaneously. We further utilize a reconstruction constraint to re-generate the original faces using the same G (Pose) and G (Exp) generators in the next two steps. It is worth noting that although our generation model has four stages, it is only repeatedly built with two generators.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Loss Function In Comp-Gan",
      "text": "To synthesize realistic facial images, Comp-GAN consists of the following losses: expression-prediction loss, IDpreserving loss, posture-prediction loss, construction loss, reconstruction loss, closed-loop loss, and adversarial loss. On D base , we train a classifier F cls based on LightCNN-29v2  [62] , which can predict the expression, pose and identity label simultaneously, to constrain the generative process. The classifier is further fine-tuned on the training instances of the novel category set C novel for different few-shot learning tasks (T1) -(T6).\n\nExpression-prediction Loss. We apply the classifier to ensure the generated images with target expression, and employ the cross-entropy loss Φ for model training to make the learned features discriminative,\n\nID-preserving loss. The cross-entropy loss Φ also is employed for identification to ensure the generated image keeping the target identity information:\n\nwhere F i cls (I) indicates the feature map of image I of the i-th layer in F cls . Finally, we define the reconstruction loss,\n\nwhere γ 1 and γ 2 are the trade-off parameters for the L1 and the perceptual loss, respectively.\n\nReconstruction loss. To capture more pose-invariant and expression-invariant features for generating more natural images, we introduce a reconstruction learning process to re-generate original faces under the last two steps in Alg. 1. We add reconstruction loss as follows:\n\nwhere L recon also contains L1 loss and perceptual loss as:\n\nClosed-Loop loss. The two branches in Comp-GAN are formed as a closed-loop, to balance the learning process and constrain the generation between the branches. The closedloop loss is proposed to ensure the properties of faces with the same identity, pose and expression between the two branches as similar as possible, and improve the ability of the model to find potential identical features. So we define the closed-loop loss as, Comp-GAN loss. Our two generators -G (Pose) and G (Exp) are followed by a discriminator D that tries to detect the synthesized faces to help improve the quality of the generated image. The adversarial learning between the generator and discriminator is introduced to make the generated images visually realistic. So we define the adversarial loss L adv as:\n\nwhere I input means the input image, and I target is the corresponding ground-truth image.\n\nGenerator G (Pose) targets at changing the facial posture and keeps the expression and identity information, and Generator G (Exp) is aiming to edit the expression of faces while maintaining the pose and identity details, so the G (Pose) loss function and the G (Exp) loss function are defined as:\n\nwhere λ exp , λ pose , λ id , λ con , λ recon , λ adv , and λ loop are the weights for the corresponding terms of L G(P ose) , respectively; and µ exp , µ pose , µ id , µ con , µ recon , µ adv and µ loop are the weights for the corresponding terms of L G(Exp) , individually.\n\nThe loss functions we proposed above are critical to our generation model. The expression-prediction loss, IDpreserving loss, and posture-prediction loss are intuitive, which are used to constrain the correct attribute labels of the synthesized faces and motivate the generation process to accurately edit the pose and expression while maintaining its identity information. The proposed construction loss and reconstruction loss both consist of L1 loss and perceptual loss to constrain the generated images to be as similar to the ground-truth. The special closed-loop loss is based on our special network structure, which has two branched and there are intersections among the generation aims, to encourage the generative model to capture more potential identical features. The last adversarial loss is a commonly used method in adversarial generative networks to play the minimax game.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Results Of Supervised Expression Recognition",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Dataset And Setting",
      "text": "Extensive experiments are conducted on JAFFE  [5]  and FER2013  [7]  to evaluate our proposed dataset F 2 ED under the supervised expression learning task. Settings. Following the setting of  [19] , we conduct the experiments on FER2013 by using the entire 28,709 training images and 3,589 validation images to train and validate our model, which is further tested on the rest 3,589 test images. As for JAFFE, we follow the split setting of the deepemotion paper  [19]  to use 120 images for training, 23 images for validation, and keep 70 images for the test (7 emotions per face ID). In our F 2 ED, we randomly choose 175,000 and 44,719 images for train and test, respectively. The FER classification accuracy is reported as the evaluation metric to compare different competitors. The results are averaged and reported over multiple rounds.\n\nCompetitors. On FER2013, we compare against several competitors, including Bag of Words  [64] , VGG+SVM  [65] , Go-Deep  [18] , DNNRL  [66] , Attention CNN  [19] , and BOVW + local SVM  [67] . These investigated classifiers are based on hand-crafted features, or specially designed for FER. As for JAFFE, we compare with several methods that are tailored for the tasks of FER, including Fisherface  [68] , Salient Facial Patch  [69] , CNN+SVM  [70]  and Attention CNN  [19] .\n\nImplementation Details. We train the baseline classification network Comp-GAN-Cls based on LightCNN-29v2  [62]  using the SGD optimizer with a momentum of 0.9 and decreasing the learning rate by 0.457 after every 10 steps. The maximum epoch number is set to 50. The learning rate and batch size vary depending on the dataset size, so we set the learning rate/batch size as 0.01/128, 2e -3/64 and 5e -4/32, on F 2 ED, FER2013, and JAFFE, respectively.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Results On Supervised Learning",
      "text": "Our dataset F 2 ED can boost facial expression recognition accuracy when it is used to pre-train the network for better initialization. To show the efficacy of F 2 ED, the classification backbone of Comp-GAN named as Comp-GAN-Cls based on LightCNN-29v2  [62]  is pre-trained on our dataset which achieves 69.13% mean accuracy under the supervised setting, and then fine-tuned on the training set of FER2013 and JAFFE. Tab. 2 and Tab. 3 show that the Comp-GAN-Cls pre-trained on F 2 ED can improve the expression recognition performance by 14.5% and 13.3% on FER2013 and JAFFE, respectively, compared to the one without pre-training. The confusion matrix in Fig.  8  shows that pre-training increases the scores on all expression types of FER2013, and the confusion matrix in Fig.  9  shows that the pre-trained Comp-GAN-Cls only makes 3 wrong predictions and surpasses the one without pre-training on all expression types of JAFFE. These demonstrate that the F 2 ED dataset with large expression and posture variations can pre-train a deep network with good initialization parameters. Note that our classification network is not specially designed for FER task, since it is built upon the LightCNN, one typical face recognition architecture. Our model can achieve the best performance among competitors. Compared to the previous methods, the results show that our model can achieve the accuracy of 76.8%, which is superior to the others on FER2013, as compared in Tab. 2. As listed in Tab. 3, our model also achieves the accuracy of 96.2%, outperforming all the other competitors. Remarkably, our model surpasses the Attention CNN by 3.4% in the same data split setting. The accuracy of CNN+SVM is slightly lower than our model by 0.9%, even though their model is trained and tested on the entire dataset. This shows the efficacy of our dataset in pre-training the facial expression network.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Results Of Few-Shot Expression Recognition",
      "text": "",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "Dataset And Setting",
      "text": "Fine-Grained Facial Expression Database (F 2 ED). We evaluate our Comp-GAN on F 2 ED as shown in Fig.  10 , which has 219,719 images with 119 identities and 54 kinds of fine-grained facial emotions, e.g., acceptance, angry, bravery, calm, disgust, envy, fear, neutral and so on. Each person is",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Model",
      "text": "Acc. Bag of Words  [64]  67.4% VGG+SVM  [65]  66.3% Go-Deep  [18]  66.4% DNNRL  [66]  70.6% Attention CNN  [19]  70.0% BOVW + local SVM  [67]  74.9% Comp-GAN-Cls w.o Pre-trained 62.3% Comp-GAN-Cls 76.8% Table  2  Accuracy on FER2013 test set in supervised setting. Comp-GAN-Cls refers to the classification backbone of our Comp-GAN.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Model",
      "text": "Acc. Fisherface  [68]  89.2% Salient Facial Patch  [69]  92.6% CNN+SVM  [70]  95.3% Attention CNN  [19]  92.8% Comp-GAN-Cls w.o Pre-trained 82.9% Comp-GAN-Cls 96.2% Table  3  Accuracy on JAFFE test set in supervised setting. Comp-GAN-Cls refers to the classification backbone of our Comp-GAN.\n\ncaptured by four different views of cameras, e.g., half left, front, half right, and bird-view. Each participant expresses his/her emotions under the guidance of a psychologist, and the images are taken when the expression is observed. As shown in Fig.  5  and Fig.  6 , F 2 ED has a relatively balanced distribution across various expressions as well as postures, which is beneficial for us to train a generative model that can change subtle facial expressions and postures synchronously. To ensure the proportional distribution balance of the test data, we only take 20% images from each category into the testing stage on the few-shot setting, and randomly select 1, 3, or and G (Exp) is a classic encoder-decoder network  [71] , which progressively down-samples the input image into compact hidden space and then progressively up-samples the hidden information to reconstruct the image of the same resolution as inputs. Our discriminator follows the design in AttGAN  [52] . Furthermore, our basic classifier is based on LightCNN-29v2  [62] . For all the experiments, we use the stochastic gradient descent algorithm to train and dropout is used for fully connected layers with the ratio 0.5. The input images are resized to 144x144 firstly, and then randomly cropped to 128x128. We pre-train our backbone LightCNN-29v2 on CelebA dataset  [72] , set the initial learning rate of 0.01 and train for 30 epochs. We train the Comp-GAN combined with LightCNN-29v2 as an end-to-end framework, and set the initial learning rate as 0.01 in LightCNN-29v2 and 0.0002 for the Comp-GAN. The Evaluation metrics. The face expression recognition task can be taken as the problem of classification tasks. To evaluate the performance, we select five evaluation metrics.\n\n(1) As used by  [73] ,  [74] , the standard recognition accuracy of each attribute as well as the label-based metric mean accuracy that overall attributes are computed to evaluate our model performance, short in mA. and acc. respectively.\n\n(2) Instance-based evaluation can capture better consistency of prediction on a given image  [75] , to appropriately evaluate the quality of different methods, following the evaluation metrics used in pedestrian attribute recognition problem  [76] , we add three more evaluation metrics, i.e. precision (prec.), recall (rec.) and F1-score (F1.).\n\n(3) Formally, the acc., mA., prec., rec. and F1. can be defined as,\n\nwhere M is the total number of attributes;",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Results Of Comp-Gan Vs. Competitors",
      "text": "We compare our model against the existing generative model, such as Cycle-GAN  [49] , Pix2Pix  [48] , VAE/GAN  [50]  and AttGAN  [52] . In particular, we highlight the following observations,",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Quality Comparison.",
      "text": "(1) Comp-GAN can well edit the facial images. As in Fig.  11 , we show several realistic images with the specified pose, desired expression generated by Comp-GAN on F 2 ED. We notice that the images have dramatically changed postures and expressions, and they can still maintain the expression and identity while changing the posture, and vice versa.   while keeping the expression-excluding information, but the AttGAN results contain some artifacts and are much blurrier than ours, while the images from Comp-GAN seem more natural and realistic.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Quantity Comparison.",
      "text": "(  As in Fig.  13 , we show the expression editing accuracy of the randomly selected 15 kinds of desired expression generated images, and compare them with other generative results. We can notice that Comp-GAN and AttGAN achieve much better accuracies than Cycle-GAN, Pix2Pix and VAE/GAN among all the expressions. As for the comparisons between the Comp-GAN and AttGAN, AttGAN can achieve better performance with a slight margin on 'Joy' expression, however, our model can get superior accuracy on the rest 14 expressions, and the generated images of Comp-GAN are much more natural and realistic as shown in Fig.  12 . And we also compare with other generative results and show the pose accuracy using 4 kinds of specified pose generated images in Fig.  14 . In contrast to the complex and varied expression modifications, the generated images from all methods are more accurate in poses, and Comp-GAN achieves the best performance on the posture accuracy. We can still see that our method can more realistically retain the original identity and expression information in the case of posture change from Fig.  12 . Furthermore, we analyze the identity preserving accuracy of all generated images shown in Fig.  15 . We can find that, in comparison, AttGAN and Comp-GAN are more accurate in the preservation of identity information, and our method gets higher accuracy for most attributes, which can be attributed to the well-designed generation network with reconstruction structure and loss function.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Evaluating Each Component Of Comp-Gan",
      "text": "We conduct extensive qualitative and quantitative experiments to evaluate the effectiveness of each component in Comp-GAN.   We also try to add the pose information as a part of expression, and train only one generator (Generator P&A) to edit posture and expression simultaneously, as shown in Fig.  16 , the generator is not conducted at all. This is the underlying reason we designed the two generators that edit pose and expression separately.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Ablation Study",
      "text": "Quality of synthesized images in terms of the amount of training data under the few-shot setting. As shown in Fig.     and 'LightCNN + G (Pose)' models have better performance compared with 'LightCNN', this further proves the validity of our generated images. On the other settings, we have similar observations. Quality of synthesized images. For implications in realworld applications, we expect the generated faces not only to look realistic but also preserve identity information. As shown in Fig.  18 , it visualizes the identity feature distributions of original and specified pose or desired expression generated data using randomly sampled 9 images via t-SNE. One color indicates one identity, and it is noticeable that the generated data are clustered around the original images with the same identities. It means our Comp-GAN can effectively preserve the identity information during the generative process.\n\nThe number of synthesized images. We choose to generate 10 synthesized images for each novel input category in the former experiments. To evaluate the relationship between the number of generated images and the recognition accuracy under the few-shot learning task, we also compare the results of generating 0,",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: F2ED has 54 different facial expression categories, which are",
      "page": 1
    },
    {
      "caption": "Figure 1: These 54 expressions are classiﬁed by referring to the recent",
      "page": 2
    },
    {
      "caption": "Figure 2: The proposed several few-shot facial expression recognition",
      "page": 2
    },
    {
      "caption": "Figure 2: , we propose several novel learning tasks,",
      "page": 3
    },
    {
      "caption": "Figure 3: Visualization of 6 semantically indistinguishable expressions",
      "page": 5
    },
    {
      "caption": "Figure 3: , which also reﬂects the difﬁculty",
      "page": 5
    },
    {
      "caption": "Figure 4: There are some facial examples of F2ED with different poses",
      "page": 6
    },
    {
      "caption": "Figure 5: Data distribution on attribute of posture in F2ED.",
      "page": 6
    },
    {
      "caption": "Figure 4: gives some examples of different poses. We collect",
      "page": 6
    },
    {
      "caption": "Figure 5: Expression. F2ED contains 54 ﬁne-grained facial expres-",
      "page": 6
    },
    {
      "caption": "Figure 6: , which indicates that F2ED has a relatively balanced",
      "page": 6
    },
    {
      "caption": "Figure 6: Data distribution on attribute of expression in F2ED.",
      "page": 7
    },
    {
      "caption": "Figure 2: we ﬁrstly deﬁne a base category set Cbase and",
      "page": 7
    },
    {
      "caption": "Figure 7: The illustration of Comp-GAN framework. It is stacked by two components, i.e., G (Exp) shown in yellow and G (Pose) as pink, and",
      "page": 8
    },
    {
      "caption": "Figure 7: , and expression classiﬁer network",
      "page": 8
    },
    {
      "caption": "Figure 7: , a stacked GAN with supervision",
      "page": 8
    },
    {
      "caption": "Figure 8: The left image shows the confusion matrix generated by",
      "page": 10
    },
    {
      "caption": "Figure 9: The left image shows the confusion matrix on FER 2013 for",
      "page": 11
    },
    {
      "caption": "Figure 8: shows that pre-training increases",
      "page": 11
    },
    {
      "caption": "Figure 9: shows that the pre-trained Comp-GAN-",
      "page": 11
    },
    {
      "caption": "Figure 5: and Fig. 6, F2ED has a relatively balanced",
      "page": 11
    },
    {
      "caption": "Figure 10: Faces of different poses/expressions in F2ED.",
      "page": 12
    },
    {
      "caption": "Figure 11: Generated samples from our Comp-GAN. The images with",
      "page": 12
    },
    {
      "caption": "Figure 12: Comparison results with other generative methods. The input",
      "page": 12
    },
    {
      "caption": "Figure 12: , our Comp-GAN",
      "page": 12
    },
    {
      "caption": "Figure 13: Comparisons among other generative models and Comp-",
      "page": 13
    },
    {
      "caption": "Figure 13: , we show the expression editing accuracy",
      "page": 13
    },
    {
      "caption": "Figure 12: Figure 14. Comparisons among other generative models and Comp-",
      "page": 13
    },
    {
      "caption": "Figure 14: In contrast to the complex and",
      "page": 13
    },
    {
      "caption": "Figure 12: Figure 15. Comparisons among other generative models and Comp-",
      "page": 13
    },
    {
      "caption": "Figure 15: We can ﬁnd",
      "page": 13
    },
    {
      "caption": "Figure 16: Ablation study of loss functions in Comp-GAN. The input",
      "page": 14
    },
    {
      "caption": "Figure 16: As can be",
      "page": 14
    },
    {
      "caption": "Figure 17: , we show the synthesized images by our Comp-",
      "page": 14
    },
    {
      "caption": "Figure 17: Generated images with different k training examples for each",
      "page": 15
    },
    {
      "caption": "Figure 18: Visualization of 9 original images (drawn as stars) and the",
      "page": 15
    },
    {
      "caption": "Figure 18: , it visualizes the identity feature distribu-",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CK+ [4]": "JAFFE [5]",
          "8": "7",
          "123": "10",
          "1": "1",
          "327": "213",
          "593": "-",
          "490 × 640": "256 × 256",
          "F": "F",
          "Controlled": "Controlled"
        },
        {
          "CK+ [4]": "KDEF [6]",
          "8": "7",
          "123": "140",
          "1": "5",
          "327": "4,900",
          "593": "-",
          "490 × 640": "562 × 762",
          "F": "FL,HL,F,FR,HR",
          "Controlled": "Controlled"
        },
        {
          "CK+ [4]": "FER2013 [7]",
          "8": "7",
          "123": "-",
          "1": "-",
          "327": "35,887",
          "593": "-",
          "490 × 640": "48 × 48",
          "F": "-",
          "Controlled": "In-the-wild"
        },
        {
          "CK+ [4]": "FER-Wild [8]",
          "8": "7",
          "123": "-",
          "1": "-",
          "327": "24,000",
          "593": "-",
          "490 × 640": "-",
          "F": "-",
          "Controlled": "In-the-wild"
        },
        {
          "CK+ [4]": "EmotionNet [9]",
          "8": "23",
          "123": "-",
          "1": "-",
          "327": "100,000",
          "593": "-",
          "490 × 640": "-",
          "F": "-",
          "Controlled": "In-the-wild"
        },
        {
          "CK+ [4]": "AffectNet [10]",
          "8": "8",
          "123": "-",
          "1": "-",
          "327": "450,000",
          "593": "-",
          "490 × 640": "-",
          "F": "-",
          "Controlled": "In-the-wild"
        }
      ],
      "page": 2
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep learning for human affect recognition: insights and new developments",
      "authors": [
        "P Rouast",
        "M Adam",
        "R Chiong"
      ],
      "year": "2001",
      "venue": "ITAC"
    },
    {
      "citation_id": "2",
      "title": "The Oxford handbook of affective computing",
      "authors": [
        "R Calvo",
        "S D'mello",
        "J Gratch",
        "A Kappas"
      ],
      "year": "2001",
      "venue": "The Oxford handbook of affective computing"
    },
    {
      "citation_id": "3",
      "title": "Affective personalization of a social robot tutor for children's second language skills",
      "authors": [
        "G Gordon",
        "S Spaulding",
        "J Westlund",
        "J Lee",
        "L Plummer",
        "M Martinez",
        "M Das",
        "C Breazeal"
      ],
      "year": "2016",
      "venue": "AAAI"
    },
    {
      "citation_id": "4",
      "title": "Comprehensive database for facial expression analysis",
      "authors": [
        "T Kanade",
        "Y Tian",
        "J Cohn"
      ],
      "year": "2000",
      "venue": "IEEE"
    },
    {
      "citation_id": "5",
      "title": "Coding facial expressions with gabor wavelets",
      "authors": [
        "M Lyons",
        "S Akamatsu",
        "M Kamachi",
        "J Gyoba"
      ],
      "year": "1998",
      "venue": "ICAFGR"
    },
    {
      "citation_id": "6",
      "title": "CD ROM from Department of Clinical Neuroscience, Psychology section",
      "authors": [
        "D Lundqvist",
        "A Flykt",
        "A Öhman"
      ],
      "year": "1998",
      "venue": "CD ROM from Department of Clinical Neuroscience, Psychology section"
    },
    {
      "citation_id": "7",
      "title": "Challenges in representation learning: Facial expression recognition challenge",
      "authors": [
        "C Pierre-Luc",
        "C Aaron"
      ],
      "year": "2001",
      "venue": "Challenges in representation learning: Facial expression recognition challenge"
    },
    {
      "citation_id": "8",
      "title": "Facial expression recognition from world wild web",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Salvador",
        "H Abdollahi",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2003",
      "venue": "CVPR"
    },
    {
      "citation_id": "9",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2003",
      "venue": "CVPR"
    },
    {
      "citation_id": "10",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2003",
      "venue": "ITAC"
    },
    {
      "citation_id": "11",
      "title": "Strong evidence for universals in facial expressions: a reply to russell's mistaken critique",
      "authors": [
        "P Ekman"
      ],
      "year": "1994",
      "venue": "Strong evidence for universals in facial expressions: a reply to russell's mistaken critique"
    },
    {
      "citation_id": "12",
      "title": "Survey on rgb, 3d, thermal, and multimodal approaches for facial expression recognition: History, trends, and affect-related applications",
      "authors": [
        "C Corneanu",
        "M Sim Ón",
        "J Cohn",
        "S Guerrero"
      ],
      "year": "2016",
      "venue": "TPAMI"
    },
    {
      "citation_id": "13",
      "title": "Handbook of cognition and emotion",
      "authors": [
        "P Ekman"
      ],
      "year": "1999",
      "venue": "Handbook of cognition and emotion"
    },
    {
      "citation_id": "14",
      "title": "Emotion, theory, research, and experience",
      "authors": [
        "R Plutchik",
        "H Kellerman"
      ],
      "year": "1980",
      "venue": "Emotion, theory, research, and experience"
    },
    {
      "citation_id": "15",
      "title": "Crowdsourcing user studies with mechanical turk",
      "authors": [
        "A Kittur",
        "E Chi",
        "B Suh"
      ],
      "year": "2001",
      "venue": "Proceedings of the SIGCHI conference on human factors in computing systems"
    },
    {
      "citation_id": "16",
      "title": "Reading what the mind thinks from how the eye sees",
      "authors": [
        "D Lee",
        "A Anderson"
      ],
      "year": "2002",
      "venue": "Psychological Science"
    },
    {
      "citation_id": "17",
      "title": "Do deep neural networks learn facial action units when doing expression recognition?",
      "authors": [
        "P Khorrami",
        "T Paine",
        "T Huang"
      ],
      "year": "2003",
      "venue": "ICCV Workshops"
    },
    {
      "citation_id": "18",
      "title": "Going deeper in facial expression recognition using deep neural networks,\" in WACV",
      "authors": [
        "A Mollahosseini",
        "D Chan",
        "M Mahoor"
      ],
      "year": "2002",
      "venue": "IEEE"
    },
    {
      "citation_id": "19",
      "title": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "authors": [
        "S Minaee",
        "A Abdolrashidi"
      ],
      "year": "2002",
      "venue": "Deep-emotion: Facial expression recognition using attentional convolutional network",
      "arxiv": "arXiv:1902.01019"
    },
    {
      "citation_id": "20",
      "title": "Attribute and simile classifiers for face verification",
      "authors": [
        "N Kumar",
        "A Berg",
        "P Belhumeur",
        "S Nayar"
      ],
      "year": "2009",
      "venue": "2009 IEEE 12th International Conference on Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Distinctive image features from scale-invariant keypoints",
      "authors": [
        "D Lowe"
      ],
      "year": "2004",
      "venue": "International journal of computer vision"
    },
    {
      "citation_id": "22",
      "title": "Gray scale and rotation invariant texture classification with local binary patterns",
      "authors": [
        "T Ojala",
        "M Pietikäinen",
        "T Mäenpää"
      ],
      "year": "2000",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "Joint pose and expression modeling for facial expression recognition",
      "authors": [
        "F Zhang",
        "T Zhang",
        "Q Mao",
        "C Xu"
      ],
      "year": "2005",
      "venue": "CVPR"
    },
    {
      "citation_id": "24",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "J Deng",
        "W Dong",
        "R Socher",
        "L.-J Li",
        "K Li",
        "L Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "25",
      "title": "Attribute-based classification for zero-shot visual object categorization",
      "authors": [
        "C Lampert",
        "H Nickisch",
        "S Harmeling"
      ],
      "year": "2014",
      "venue": "TPAMI"
    },
    {
      "citation_id": "26",
      "title": "Pose-normalized image generation for person reidentification",
      "authors": [
        "X Qian",
        "Y Fu",
        "T Xiang",
        "W Wang",
        "J Qiu",
        "Y Wu",
        "Y.-G Jiang",
        "X Xue"
      ],
      "year": "2004",
      "venue": "ECCV"
    },
    {
      "citation_id": "27",
      "title": "Identity-adaptive facial expression recognition through expression regeneration using conditional generative adversarial networks",
      "authors": [
        "H Yang",
        "Z Zhang",
        "L Yin"
      ],
      "year": "2004",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "28",
      "title": "Emotion-preserving representation learning via generative adversarial network for multi-view facial expression recognition",
      "authors": [
        "Y.-H Lai",
        "S.-H Lai"
      ],
      "year": "2004",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "29",
      "title": "Vgan-based image representation learning for privacy-preserving facial expression recognition",
      "authors": [
        "J Chen",
        "J Konrad",
        "P Ishwar"
      ],
      "year": "2004",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "30",
      "title": "Iot-based emotion recognition robot to enhance sense of community in nursing home",
      "authors": [
        "S Nagama",
        "M Numao"
      ],
      "year": "2018",
      "venue": "2018 AAAI Spring Symposium Series"
    },
    {
      "citation_id": "31",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "32",
      "title": "Artificial psychology and artificial emotion",
      "authors": [
        "W Zhiliang"
      ],
      "year": "2001",
      "venue": "CAAI Transactions on Intelligent Systems"
    },
    {
      "citation_id": "33",
      "title": "The cognitive structure of emotions",
      "authors": [
        "A Ortony",
        "G Clore",
        "A Collins"
      ],
      "year": "1990",
      "venue": "The cognitive structure of emotions"
    },
    {
      "citation_id": "34",
      "title": "A multilayer personality model",
      "authors": [
        "S Kshirsagar"
      ],
      "year": "2002",
      "venue": "Proceedings of the 2nd international symposium on Smart graphics"
    },
    {
      "citation_id": "35",
      "title": "The hundred-year emotion war: Are emotions natural kinds or psychological constructions? comment on lench, flores, and bench",
      "authors": [
        "K Lindquist",
        "E Siegel",
        "K Quigley",
        "L Barrett"
      ],
      "year": "2011",
      "venue": "The hundred-year emotion war: Are emotions natural kinds or psychological constructions? comment on lench, flores, and bench"
    },
    {
      "citation_id": "36",
      "title": "Heterogeneous knowledge transfer in video emotion recognition, attribution and summarization",
      "authors": [
        "B Xu",
        "Y Fu",
        "Y.-G Jiang",
        "B Li",
        "L Sigal"
      ],
      "year": "2002",
      "venue": "ITAC"
    },
    {
      "citation_id": "37",
      "title": "Automatic analysis of facial actions: A survey",
      "authors": [
        "B Martinez",
        "M Valstar",
        "B Jiang",
        "M Pantic"
      ],
      "year": "2003",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2003",
      "venue": "Deep facial expression recognition: A survey",
      "arxiv": "arXiv:1804.08348"
    },
    {
      "citation_id": "39",
      "title": "Recognizing facial expression: machine learning and application to spontaneous behavior",
      "authors": [
        "M Bartlett",
        "G Littlewort",
        "M Frank",
        "C Lainscsek",
        "I Fasel",
        "J Movellan"
      ],
      "year": "2005",
      "venue": "CVPR"
    },
    {
      "citation_id": "40",
      "title": "Facial expression recognition based on local binary patterns: A comprehensive study",
      "authors": [
        "C Shan",
        "S Gong",
        "P Mcowan"
      ],
      "year": "2003",
      "venue": "IVC"
    },
    {
      "citation_id": "41",
      "title": "Histograms of oriented gradients for human detection",
      "authors": [
        "N Dalal",
        "B Triggs"
      ],
      "year": "2003",
      "venue": "international Conference on computer vision & Pattern Recognition (CVPR'05)"
    },
    {
      "citation_id": "42",
      "title": "3d facial expression recognition using sift descriptors of automatically detected keypoints",
      "authors": [
        "S Berretti",
        "B Amor",
        "M Daoudi",
        "A Del Bimbo"
      ],
      "year": "2003",
      "venue": "The Visual Computer"
    },
    {
      "citation_id": "43",
      "title": "Multi-task deep neural network for joint face recognition and facial attribute prediction",
      "authors": [
        "Z Wang",
        "K He",
        "Y Fu",
        "R Feng",
        "Y.-G Jiang",
        "X Xue"
      ],
      "year": "2003",
      "venue": "ICMR"
    },
    {
      "citation_id": "44",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2002",
      "venue": "SPL"
    },
    {
      "citation_id": "45",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "R Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "46",
      "title": "Generative adversarial nets",
      "authors": [
        "I Goodfellow",
        "J Pouget-Abadie",
        "M Mirza",
        "B Xu",
        "D Warde-Farley",
        "S Ozair",
        "A Courville",
        "Y Bengio"
      ],
      "year": "2004",
      "venue": "NIPS"
    },
    {
      "citation_id": "47",
      "title": "Conditional generative adversarial nets",
      "authors": [
        "M Mirza",
        "S Osindero"
      ],
      "year": "2004",
      "venue": "Conditional generative adversarial nets",
      "arxiv": "arXiv:1411.1784"
    },
    {
      "citation_id": "48",
      "title": "Image-to-image translation with conditional adversarial networks",
      "authors": [
        "P Isola",
        "J.-Y Zhu",
        "T Zhou",
        "A Efros"
      ],
      "year": "2002",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "49",
      "title": "Unpaired image-toimage translation using cycle-consistent adversarial networks",
      "authors": [
        "J.-Y Zhu",
        "T Park",
        "P Isola",
        "A Efros"
      ],
      "year": "2002",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "50",
      "title": "Autoencoding beyond pixels using a learned similarity metric",
      "authors": [
        "A Larsen",
        "S Sønderby",
        "H Larochelle",
        "O Winther"
      ],
      "year": "2002",
      "venue": "Autoencoding beyond pixels using a learned similarity metric",
      "arxiv": "arXiv:1512.09300"
    },
    {
      "citation_id": "51",
      "title": "Facial expression recognition by de-expression residue learning",
      "authors": [
        "H Yang",
        "U Ciftci",
        "L Yin"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "52",
      "title": "Attgan: Facial attribute editing by only changing what you want",
      "authors": [
        "Z He",
        "W Zuo",
        "M Kan",
        "S Shan",
        "X Chen"
      ],
      "year": "2004",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "53",
      "title": "One-shot learning of object categories",
      "authors": [
        "L Fei-Fei",
        "R Fergus",
        "P Perona"
      ],
      "year": "2005",
      "venue": "TPAMI"
    },
    {
      "citation_id": "54",
      "title": "Model-agnostic meta-learning for fast adaptation of deep networks",
      "authors": [
        "C Finn",
        "P Abbeel"
      ],
      "year": "2001",
      "venue": "ICML"
    },
    {
      "citation_id": "55",
      "title": "Optimization as a model for few-shot learning",
      "authors": [
        "S Ravi",
        "H Larochelle"
      ],
      "year": "2005",
      "venue": "Optimization as a model for few-shot learning"
    },
    {
      "citation_id": "56",
      "title": "Prototypical networks for fewshot learning",
      "authors": [
        "J Snell",
        "K Swersky",
        "R Zemel"
      ],
      "year": "2005",
      "venue": "NIPS"
    },
    {
      "citation_id": "57",
      "title": "The expression of the emotions in man and animals",
      "authors": [
        "C Darwin",
        "P Prodger"
      ],
      "year": "1998",
      "venue": "The expression of the emotions in man and animals"
    },
    {
      "citation_id": "58",
      "title": "Expressing fear enhances sensory acquisition",
      "authors": [
        "J Susskind",
        "D Lee",
        "A Cusi",
        "R Feiman",
        "W Grabski",
        "A Anderson"
      ],
      "year": "2001",
      "venue": "Nature neuroscience"
    },
    {
      "citation_id": "59",
      "title": "Matching networks for one shot learning",
      "authors": [
        "O Vinyals",
        "C Blundell",
        "T Lillicrap",
        "D Wierstra"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "60",
      "title": "Optimization as a model for few-shot learning",
      "authors": [
        "S Ravi",
        "H Larochelle"
      ],
      "year": "2017",
      "venue": "ICLR"
    },
    {
      "citation_id": "61",
      "title": "Low-shot learning from imaginary data",
      "authors": [
        "Y.-X Wang",
        "R Girshick",
        "M Hebert",
        "B Hariharan"
      ],
      "year": "2018",
      "venue": "CVPR"
    },
    {
      "citation_id": "62",
      "title": "A light cnn for deep face representation with noisy labels",
      "authors": [
        "W Xiang",
        "H Ran",
        "Z Sun",
        "T Tan"
      ],
      "year": "2001",
      "venue": "TIFS"
    },
    {
      "citation_id": "63",
      "title": "Perceptual losses for realtime style transfer and super-resolution",
      "authors": [
        "J Johnson",
        "A Alahi",
        "L Fei-Fei"
      ],
      "year": "2003",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "64",
      "title": "Local learning to improve bag of visual words model for facial expression recognition",
      "authors": [
        "R Ionescu",
        "M Popescu",
        "C Grozea"
      ],
      "venue": "Local learning to improve bag of visual words model for facial expression recognition"
    },
    {
      "citation_id": "65",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "M.-I Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2002",
      "venue": "Local learning with deep and handcrafted features for facial expression recognition",
      "arxiv": "arXiv:1804.10892"
    },
    {
      "citation_id": "66",
      "title": "Deep neural networks with relativity learning for facial expression recognition",
      "authors": [
        "Y Guo",
        "D Tao",
        "J Yu",
        "H Xiong",
        "Y Li",
        "D Tao"
      ],
      "year": "2002",
      "venue": "ICMEW"
    },
    {
      "citation_id": "67",
      "title": "Local learning with deep and handcrafted features for facial expression recognition",
      "authors": [
        "M.-I Georgescu",
        "R Ionescu",
        "M Popescu"
      ],
      "year": "2002",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "68",
      "title": "A neural network based facial expression recognition using fisherface",
      "authors": [
        "Z Abidin",
        "A Harjoko"
      ],
      "year": "2002",
      "venue": "IJCA"
    },
    {
      "citation_id": "69",
      "title": "Automatic facial expression recognition using features of salient facial patches",
      "authors": [
        "S Happy",
        "A Routray"
      ],
      "year": "2002",
      "venue": "ITAC"
    },
    {
      "citation_id": "70",
      "title": "Image augmentation for classifying facial expression images by using deep neural network pre-trained with object image database",
      "authors": [
        "Y Shima",
        "Y Omori"
      ],
      "year": "2002",
      "venue": "ICRCA"
    },
    {
      "citation_id": "71",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "O Ronneberger",
        "P Fischer",
        "T Brox"
      ],
      "year": "2001",
      "venue": "International Conference on Medical image computing and computer-assisted intervention"
    },
    {
      "citation_id": "72",
      "title": "Deep learning face attributes in the wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2001",
      "venue": "Proceedings of the IEEE international conference on computer vision"
    },
    {
      "citation_id": "73",
      "title": "A survey on transfer learning",
      "authors": [
        "S Pan",
        "Q Yang"
      ],
      "year": "2001",
      "venue": "IEEE Transactions on knowledge and data engineering"
    },
    {
      "citation_id": "74",
      "title": "Improving person re-identification by attribute and identity learning",
      "authors": [
        "Y Lin",
        "L Zheng",
        "Z Zheng",
        "Y Wu",
        "Z Hu",
        "C Yan",
        "Y Yang"
      ],
      "year": "2001",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "75",
      "title": "A review on multi-label learning algorithms",
      "authors": [
        "M.-L Zhang",
        "Z.-H Zhou"
      ],
      "year": "2001",
      "venue": "IEEE transactions on knowledge and data engineering"
    },
    {
      "citation_id": "76",
      "title": "A richly annotated dataset for pedestrian attribute recognition",
      "authors": [
        "D Li",
        "Z Zhang",
        "X Chen",
        "H Ling",
        "K Huang"
      ],
      "year": "2001",
      "venue": "A richly annotated dataset for pedestrian attribute recognition",
      "arxiv": "arXiv:1603.07054"
    }
  ]
}