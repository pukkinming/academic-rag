{
  "paper_id": "2012.13912v1",
  "title": "Exploring Emotion Features And Fusion Strategies For Audio-Video Emotion Recognition",
  "published": "2020-12-27T10:50:24Z",
  "authors": [
    "Hengshun Zhou",
    "Debin Meng",
    "Yuanyuan Zhang",
    "Xiaojiang Peng",
    "Jun Du",
    "Kai Wang",
    "Yu Qiao"
  ],
  "keywords": [
    "CCS CONCEPTS",
    "Computer systems organization â†’ Embedded systems",
    "Redundancy",
    "Robotics",
    "â€¢ Networks â†’ Network reliability Emotion Recognition",
    "Attention Mechanism",
    "Deep learning",
    "Affective Computing",
    "Convolutional Neural Networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The audio-video based emotion recognition aims to classify a given video into basic emotions. In this paper, we describe our approaches in EmotiW 2019, which mainly explores emotion features and feature fusion strategies for audio and visual modality. For emotion features, we explore audio feature with both speech-spectrogram and Log Mel-spectrogram and evaluate several facial features with different CNN models and different emotion pretrained strategies. For fusion strategies, we explore intra-modal and cross-modal fusion methods, such as designing attention mechanisms to highlights important emotion feature, exploring feature concatenation and factorized bilinear pooling (FBP) for cross-modal feature fusion. With careful evaluation, we obtain 65.5% on the AFEW validation set and 62.48% on the test set and rank second in the challenge.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition(ER) has attracted increasing attention in academia and industry due to its wide range of applications such as human-computer interaction  [7] , clinical diagnosis  [19] , and cognitive science  [14] . Although great progress in the face and video analysis has been made  [4, 23, [26] [27] [28] [29] , audio-video emotion recognition in the wild remains a challenging problem due to the expression suffers from the large pose, illumination variance, occlusion, motion blur, etc.\n\nAudio-Video emotion recognition can be summarized as a simple pipeline shown in Fig  1 , which includes four parts, namely Video preprocessing, Feature Extraction, Feature Fusion, and Classifier. Specifically, video preprocessing refers to extract the spectrogram of the audio, the faces or landmarks of video. Feature extraction and feature fusion respectively extracts emotion features from the audio or visual signal and fuses emotion features into compact feature vectors, which are subsequently fed into a classifier for prediction.\n\nReviewing the methods of Audio-Video emotion recognition, we find that some methods emphasize feature extraction and other methods emphasize feature fusion. Yao et al  [31]  construct Holonet as discriminative feature extraction, which combines residual structure  [12]  and CReLU  [22]  to increase network depth and maintain efficiency. The EmotiW2017 winner team  [13]  gets robust feature extraction with Supervised Scoring Ensemble (SSE) which adds supervision to intermediate layers and shallow layers. Since SSE only uses high-level representations, Fan et al  [8]  further improve SSE by utilizing middle feature maps to provide more discriminative features. These methods mainly use average pooling to obtain video-level representation from frame-level.\n\nMany feature fusion strategies have been used in previous EmotiW challenges.  [9, 18, 25]  extract CNN-based frame features and use LSTM  [10]  or BLSTM  [11]  to fuse them.  [1, 15, 17]     maximum of the frame feature vectors. However, these methods ignore the importance of frames. Besides, all previous methods mainly apply score averaging or feature concatenation for audio-video fusion, which ignores the correlation between the features from different modalities.\n\nIn this paper, we exploit three types of intra-modal fusion methods, namely self-attention, relation-attention, and transformer  [24] . They are used to learn weights for frame features to highlight important frames. For cross-modal fusion, we explore feature concatenation and factorized bilinear pooling (FBP)  [32] . Besides, we evaluate different emotion features, including convolutional neural networks (CNN) for audio information with both speech-spectrogram and Log Mel-spectrogram and several facial features with different CNN models and different emotion pretrained strategies. Finally, we obtain 62.48% and rank second in the challenge.\n\nOur contributions and finds can be summarized as follows.\n\nâ€¢ We experimentally show that better face recognition CNN models and choosing suitable emotion datasets to further pretrain the face CNN models is important. â€¢ We design three kinds of attention mechanisms for visual and audio feature fusion. â€¢ We apply a Factorized Bilinear Pooling (FBP) for crossmodal feature fusion.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "The Proposed Method",
      "text": "We develop our ER system based on the pipeline of Video preprocessing-Feature Extraction-Feature Fusion-Classifier.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Video Preprocessing",
      "text": "Face detection and alignment. We apply face detection and alignment by Dlib toolbox 1 . We extend the face bounding box with a ratio of 30% and then resize the cropped faces to scale of 224 Ã— 224. We do not apply face detection and alignment 1 http://dlib.net/ for AffectNet dataset, due to the face bounding box had been provided. For AFEW dataset, If no face is detected in the picture, the entire frame is passed to the network.\n\nAudio processing and Spectrogram calculation. For each audio, the speech spectrogram and log Mel-spectrogram extraction process is consistent with  [32]  and  [3]  respectively.\n\nFor speech spectrogram, we use the Hamming window with 40 msec window size and 10 msec shift. Finally, the 200dimensional low-frequency part of the spectrogram is used as the input to the audio modality. As for log Mel-spectrogram, we calculate its deltas and delta-deltas.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Feature Extraction",
      "text": "Visual Features. We apply three CNN backbones to extract facial emotion features, namely VGGFace, ResNet18, and IR50  [4] . The dimensions are 4096, 512, and 512, respectively.\n\nAudio Feature. We extract the feature maps of the audio from the last Pooling layer of AlexNet. The size of a 3-dimensional feature map is ğ» Ã—ğ‘Š Ã—ğ¶, where the ğ» (ğ‘Š ) is the height(width) of the feature map, and ğ¶ is the number of the channel of the feature map. The feature maps are then split into n vectors(ğ‘› = ğ» Ã— ğ‘Š ). Each vector is C-dimensional.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Intra-Modal Feature Fusion",
      "text": "We apply the attention-based strategies for intra-modal feature fusion. It converts a variable number of emotion features(from audio or visual modality) into a fixed-dimension feature. We explore three attention methods, namely Selfattention, Relation-attention, and Transformer-attention. Formally, we denote a number of emotion features as {ğ‘“ 1 , â€¢ â€¢ â€¢ , ğ‘“ ğ‘› }.\n\nSelf-attention. We apply 1-dimensional Fully-Connected(FC) layer W 0 dÃ—1 and a sigmoid function ğœ for each emotion feature, the weight of the ğ‘–-th feature ğ‘“ ğ‘‡ ğ‘– is defined by:\n\n) With these self-attention weights, we aggregate all the emotion features into a global representation ğ‘“ ğ‘  as follows:\n\nRelation-attention.\n\n) With Self-attention and Relation-attention weights, all the emotion features was convert into a new feature as follows:\n\nTransformer-attention. Inspired by the works in  [32]  and  [30] , we formulate the attention weight as follows:\n\nTo reduce the dimension of the feature ğ‘“ ğ‘– , we use a ğ‘¤ Ã— ğ‘‘dimensional FC layer W 2 mÃ—d in Eq.  (5) . Then the weight of the ğ‘–-th feautre ğ‘“ ğ‘– is processed by a 1-dimensional FC layer u t , ğ‘’ğ‘¥ğ‘ () and ğ‘¡ğ‘ğ‘›â„() function in Eq.  (6) .\n\nWith these transformer-attention weights, we aggregate all the emotion features into a single feature ğ‘“ ğ‘¡ as follows:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Cross-Modal Feature Fusion",
      "text": "We apply Factorized Bilinear Pooling(FBP) for cross-modal feature fusion. Given two features in different modalities,i.e. the audio feature vector ğ’‚ âˆˆ R ğ‘š for a spectrogram and visual feature ğ’— âˆˆ R ğ‘› for frame sequence, the simplest cross-modal bilinear model is defined as follows:\n\nwhere ğ‘¾ âˆˆ R ğ‘šÃ—ğ‘› is a projection matrix, ğ’› ğ’Š âˆˆ R is the output of the bilinear model. we use the Eq.(  9 ) to obtain the output feature ğ’› = [ğ‘§ 1 , â€¢ â€¢ â€¢ , ğ‘§ ğ‘œ ]. The formula derivation from formula Eq.(  8 ) to Eq  ( 9)  was discribed in the paper  [32] . The implementation of Eq(  9 ) is illustrated in Fig2, where Å¨ğ‘‡ ğ’‚ and á¹¼ğ‘‡ ğ’— are implemented by feeding feature ğ’‚ and ğ’— to FC layers, respectively, and the function SumPooling(ğ’™, ğ‘˜) applies sum pooling with non-overlapped windows to ğ’™. Besides, Dropout is adopted to prevent over-fitting. The ğ‘™2normalization (ğ’› â† ğ’›/âˆ¥ğ’›âˆ¥) is used to normalize the energy of ğ’› to avoid the dramatical variation of the output magnitude, due to the introduced element-wise multiplication.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "In this work we use four emotion datasets to train our models, i.e. AffectNet  [20] , RAF-DB  [16] , FER+  [2] , AFEW  [5, 6] .\n\nThe human-annotated part of AffectNet dataset contains 287,651 training images and 4,000 test images, which are annotated with both emotion labels and arousal valence values. Only emotion labels are used in this task.\n\nThe RAF-DB dataset consists of 15,339 images labeled with 7-class basic emotion and 3,954 labeled with 12-class compound emotion. Only images labeled with basic emotion are used in this study.\n\nThe FER+ dataset contains 28,709 training, 3,589 validation and 3,589 test images. We combine its training data with validation data for the training split and evaluate the model performance on the test data.\n\nThe AFEW contains 773 train, 383 val and 653 test samples, which are collected from movies and TV serials with spontaneous expressions, various poses, and illuminations.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Exploration Of Emotion Features",
      "text": "We explore emotion features in two perspectives, namely CNN backbones and pretraining emotion datasets.\n\nFor the choice of the CNN model, we compare IR50  [4] , ResNet18  [12] , and VGGFace  [21]  in the Table  1 , where the former two models are pretrained on MS-Celeb-1M dataset and the last one on VGGFace dataset. We find that the large CNN, IR50, is superior to the other two models.\n\nWe use the well-trained IR50 model to extract features and only train softmax classifier using these features. The IR50 models pre-trained on FER+, RAF-DB, and AffectNet achieve 50.13%, 51.436%, and 53.78%, respectively. Therefore, we choose the IR50 model pretrain on AffectNet as our visual features in the following fusion experiments.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Exploration Of Fusion Strategies",
      "text": "We explore three intra-modal attention strategies with the FBP cross-modal fusion. We use speech spectrogram for audio CNN, which obtains 38% on AFEW validation set individally. In the Table  2 , we find the FBP improves performance for all the intra-modal fusion methods. Transformer attention for intra-modal fusion is the best for FBP. We also use log Mel-spectrogram for audio CNN, which obtains a little better performance, but the final results are very similar after intra-and cross-modal fusion. Besides, the concatenation of audio and visual vectors gets 58% accuracy in AFEW validation set with transformer attention. This is 3% lower than FBP which shows the effectiveness of FBP.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Feature Enhancement",
      "text": "In the Table  3 , the Basic Features means that we only extract one feature vector for each frame. Besides, We apply 5 kinds of feature enhancement strategies as presented in Table  3 . Specifically, for feature ğ¹ -ğ‘€ğ‘’ğ‘ğ‘›, we first obtain 18 transformation frames by using three rotations, three scales, and flipping for a frame. After that, we compute the features of these 18 transformation frames and average these 18 features as the feature ğ¹ -ğ‘€ğ‘’ğ‘ğ‘›. For the feature ğ¹ -ğ‘€ğ‘’ğ‘ğ‘›ğ‘†ğ‘¡ğ‘‘, we compute the average feature and feature standard deviation of these 18 features. We then concatenate the average feature and the standard deviation as ğ¹ -ğ‘€ğ‘’ğ‘ğ‘›ğ‘†ğ‘¡ğ‘‘. For the feature ğ¹ -ğ‘›ğ‘œğ‘Ÿğ‘šğ¹ ğ¹ğ‘‡ , we first compute the Fast Fourier transform(FFT) of the Basic Feature, and then normalize the feature and concatenate the real and imaginary parts as ğ¹ -ğ‘›ğ‘œğ‘Ÿğ‘šğ¹ ğ¹ğ‘‡ . For the feature ğ¹ -ğ´ğ‘…-ğ‘€ğ‘’ğ‘ğ‘›, ğ´ means that the features are extracted by the models pre-trained on Affectnet, and ğ‘… by the models pre-trained on RAF-DB. we concatenate these two mean features of two different pretrained models as ğ¹ -ğ´ğ‘…-ğ‘€ğ‘’ğ‘ğ‘›. Table  3  shows that the five feature enhancement methods further improve the performance of FBP where the feature F-MeanStd achieves the best result on the validation set.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results On Emotiw2019",
      "text": "In the Table  4 . The first three submitted models are trained on the training and validation set of AFEW, and the last two models are trained on the training set of AFEW. We find that it is difficult to choose models and fuse models if combining the validation set with the training set. We adopt class weight in all submissions, which means that we reweight the predicted scores by the square root of the sample numbers([0.15, 0.097, 0.129, 0.185, 0.138, 0.082, 0.215]).",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusions",
      "text": "In this paper, we exploit three types of intra-modal fusion methods, namely self-attention, relation-attention, and transformer. They are mainly used to highlight important emotion feature. For the fusion of audio and visual information, we explore feature concatenation and factorized bilinear pooling (FBP). Besides, we evaluate different emotion features, including an audio feature with both speech-spectrogram and Log Mel-spectrogram and several facial features with different CNN models and different emotion pretrained strategies.\n\nWith careful evaluation, we obtain 62.48% and rank second in the EmotiW 2019 Challenge.",
      "page_start": 2,
      "page_end": 2
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: , which includes four parts,",
      "page": 1
    },
    {
      "caption": "Figure 1: The pipeline of audio-video emotion recognition.",
      "page": 2
    },
    {
      "caption": "Figure 2: Our factorized bilinear pooling(FBP) module.",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and Technology of China, P.R. China": "2ShenZhen Key Lab of Computer Vision and Pattern Recognition, SIAT-SenseTime Joint Lab, Shenzhen Institutes of"
        },
        {
          "and Technology of China, P.R. China": "Advanced Technology, Chinese Academy of Sciences, China"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "ABSTRACT"
        },
        {
          "and Technology of China, P.R. China": "The audio-video based emotion recognition aims to classify"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "a given video into basic emotions. In this paper, we describe"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "our approaches in EmotiW 2019, which mainly explores emo-"
        },
        {
          "and Technology of China, P.R. China": "tion features and feature fusion strategies for audio and vi-"
        },
        {
          "and Technology of China, P.R. China": "sual modality. For emotion features, we explore audio feature"
        },
        {
          "and Technology of China, P.R. China": "with both speech-spectrogram and Log Mel-spectrogram and"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "evaluate several facial features with different CNN models"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "and different emotion pretrained strategies. For fusion strate-"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "gies, we explore intra-modal and cross-modal fusion meth-"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "ods, such as designing attention mechanisms to highlights"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "important emotion feature, exploring feature concatenation"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "and factorized bilinear pooling (FBP) for cross-modal fea-"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "ture fusion. With careful evaluation, we obtain 65.5% on the"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "AFEW validation set and 62.48% on the test set and rank"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "second in the challenge."
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "CCS CONCEPTS"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "â€¢ Computer systems organization â†’ Embedded systems;"
        },
        {
          "and Technology of China, P.R. China": "Redundancy; Robotics; â€¢ Networks â†’ Network reliability."
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "KEYWORDS"
        },
        {
          "and Technology of China, P.R. China": "Emotion Recognition; Attention Mechanism; Deep learning;"
        },
        {
          "and Technology of China, P.R. China": "Affective Computing; Convolutional Neural Networks"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "ACM Reference Format:"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "Hengshun Zhouâˆ—,1, Debin Mengâˆ—,2, Yuanyuan Zhang1, Xiaojiang"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "Pengâ€ ,2, Jun Du1, Kai Wang2, Yu Qiao2. 2019. Exploring Emotion"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "âˆ—Hengshun Zhou and Debin Meng contributed equally to this research."
        },
        {
          "and Technology of China, P.R. China": "â€  Xiaojiang Peng is the corresponding author. Email: xj.peng@siat.ac.cn"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "Permission to make digital or hard copies of all or part of this work for"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "personal or classroom use is granted without fee provided that copies are not"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "made or distributed for profit or commercial advantage and that copies bear"
        },
        {
          "and Technology of China, P.R. China": "this notice and the full citation on the first page. Copyrights for components"
        },
        {
          "and Technology of China, P.R. China": "of this work owned by others than ACM must be honored. Abstracting with"
        },
        {
          "and Technology of China, P.R. China": "credit is permitted. To copy otherwise, or republish, to post on servers or to"
        },
        {
          "and Technology of China, P.R. China": "redistribute to lists, requires prior specific permission and/or a fee. Request"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "permissions from permissions@acm.org."
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "ICMI â€™19, October 14â€“18, 2019, Suzhou, China"
        },
        {
          "and Technology of China, P.R. China": ""
        },
        {
          "and Technology of China, P.R. China": "Â© 2019 Association for Computing Machinery."
        },
        {
          "and Technology of China, P.R. China": "ACM ISBN 978-1-4503-6860-5/19/10. . . $15.00"
        },
        {
          "and Technology of China, P.R. China": "https://doi.org/10.1145/3340555.3355713"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Video Processing": "Extraction",
          "Feature Fusion": "Classifier"
        },
        {
          "Video Processing": "Figure 1: The pipeline of audio-video emotion recognition.",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "maximum of the frame feature vectors. However, these meth-",
          "Feature Fusion": "for AffectNet dataset, due to the face bounding box had been"
        },
        {
          "Video Processing": "ods ignore the importance of frames. Besides, all previous",
          "Feature Fusion": "provided. For AFEW dataset,\nIf no face is detected in the"
        },
        {
          "Video Processing": "methods mainly apply score averaging or feature concate-",
          "Feature Fusion": "picture, the entire frame is passed to the network."
        },
        {
          "Video Processing": "nation for audio-video fusion, which ignores the correlation",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "between the features from different modalities.",
          "Feature Fusion": "Audio processing and Spectrogram calculation. For each au-"
        },
        {
          "Video Processing": "In this paper, we exploit\nthree types of\nintra-modal\nfu-",
          "Feature Fusion": "dio, the speech spectrogram and log Mel-spectrogram ex-"
        },
        {
          "Video Processing": "sion methods, namely self-attention, relation-attention, and",
          "Feature Fusion": "traction process is consistent with [32] and [3] respectively."
        },
        {
          "Video Processing": "transformer[24]. They are used to learn weights for frame",
          "Feature Fusion": "For speech spectrogram, we use the Hamming window with"
        },
        {
          "Video Processing": "features to highlight important frames. For cross-modal fu-",
          "Feature Fusion": "40 msec window size and 10 msec shift. Finally,\nthe 200-"
        },
        {
          "Video Processing": "sion, we explore feature concatenation and factorized bilinear",
          "Feature Fusion": "dimensional low-frequency part of the spectrogram is used as"
        },
        {
          "Video Processing": "pooling (FBP) [32]. Besides, we evaluate different emotion",
          "Feature Fusion": "the input to the audio modality. As for log Mel-spectrogram,"
        },
        {
          "Video Processing": "features,\nincluding convolutional neural networks (CNN)",
          "Feature Fusion": "we calculate its deltas and delta-deltas."
        },
        {
          "Video Processing": "for audio information with both speech-spectrogram and",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "Log Mel-spectrogram and several facial features with differ-",
          "Feature Fusion": "Feature Extraction"
        },
        {
          "Video Processing": "ent CNN models and different emotion pretrained strategies.",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "",
          "Feature Fusion": "Visual Features. We apply three CNN backbones to extract"
        },
        {
          "Video Processing": "Finally, we obtain 62.48% and rank second in the challenge.",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "",
          "Feature Fusion": "facial emotion features, namely VGGFace, ResNet18, and"
        },
        {
          "Video Processing": "Our contributions and finds can be summarized as follows.",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "",
          "Feature Fusion": "IR50 [4]. The dimensions are 4096, 512, and 512, respectively."
        },
        {
          "Video Processing": "â€¢ We experimentally show that better face recognition",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "CNN models and choosing suitable emotion datasets",
          "Feature Fusion": "Audio Feature. We extract the feature maps of the audio from"
        },
        {
          "Video Processing": "",
          "Feature Fusion": "the last Pooling layer of AlexNet. The size of a 3-dimensional"
        },
        {
          "Video Processing": "to further pretrain the face CNN models is important.",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "",
          "Feature Fusion": "feature map is ğ» Ã—ğ‘Š Ã—ğ¶, where the ğ» (ğ‘Š ) is the height(width)"
        },
        {
          "Video Processing": "â€¢ We design three kinds of attention mechanisms for",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "",
          "Feature Fusion": "of\nthe channel\nthe feature map, and ğ¶ is the number of"
        },
        {
          "Video Processing": "visual and audio feature fusion.",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "",
          "Feature Fusion": "of\nthe feature map. The feature maps are then split\ninto"
        },
        {
          "Video Processing": "â€¢ We apply a Factorized Bilinear Pooling (FBP) for cross-",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "modal feature fusion.",
          "Feature Fusion": "n vectors(ğ‘› = ğ» Ã— ğ‘Š ). Each vector is C-dimensional."
        },
        {
          "Video Processing": "2\nTHE PROPOSED METHOD",
          "Feature Fusion": "Intra-modal Feature Fusion"
        },
        {
          "Video Processing": "We develop our ER system based on the pipeline of Video",
          "Feature Fusion": "We apply the attention-based strategies for intra-modal fea-"
        },
        {
          "Video Processing": "preprocessing-Feature Extraction-Feature Fusion-Classifier.",
          "Feature Fusion": "ture fusion. It converts a variable number of emotion fea-"
        },
        {
          "Video Processing": "",
          "Feature Fusion": "tures(from audio or visual modality) into a fixed-dimension"
        },
        {
          "Video Processing": "Video preprocessing",
          "Feature Fusion": "feature. We explore three attention methods, namely Self-"
        },
        {
          "Video Processing": "",
          "Feature Fusion": "attention, Relation-attention, and Transformer-attention. For-"
        },
        {
          "Video Processing": "Face detection and alignment. We apply face detection and",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "alignment by Dlib toolbox1. We extend the face bounding box",
          "Feature Fusion": "mally, we denote a number of emotion features as {ğ‘“1, Â· Â· Â·\n, ğ‘“ğ‘›}."
        },
        {
          "Video Processing": "with a ratio of 30% and then resize the cropped faces to scale",
          "Feature Fusion": ""
        },
        {
          "Video Processing": "of 224 Ã— 224. We do not apply face detection and alignment",
          "Feature Fusion": "Self-attention. We apply 1-dimensional Fully-Connected(FC)"
        },
        {
          "Video Processing": "",
          "Feature Fusion": "layer W0"
        },
        {
          "Video Processing": "",
          "Feature Fusion": "dÃ—1"
        },
        {
          "Video Processing": "1http://dlib.net/",
          "Feature Fusion": "is defined by:\nture, the weight of the ğ‘–-th feature ğ‘“ ğ‘‡"
        },
        {
          "Video Processing": "",
          "Feature Fusion": "ğ‘–"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "The implementation of Eq( 9) is illustrated in Fig2, where"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "ğ‘‡\nğ‘‡\nËœ"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "ğ‘¼\nğ’‚ and Ëœğ‘½\nğ’— are implemented by feeding feature ğ’‚ and ğ’— to"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "FC layers, respectively, and the function SumPooling(ğ’™, ğ‘˜)"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "applies sum pooling with non-overlapped windows to ğ’™."
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "Besides, Dropout is adopted to prevent over-fitting. The ğ‘™2-"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "normalization (ğ’› â† ğ’›/âˆ¥ğ’›âˆ¥) is used to normalize the energy of"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "ğ’› to avoid the dramatical variation of the output magnitude,"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "due to the introduced element-wise multiplication."
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "3\nEXPERIMENTS"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "Dataset"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "In this work we use four emotion datasets to train our models,"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "i.e. AffectNet[20], RAF-DB[16], FER+[2], AFEW[5, 6]."
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "The human-annotated part of AffectNet dataset contains"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "287,651 training images and 4,000 test\nimages, which are"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "annotated with both emotion labels and arousal valence"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "values. Only emotion labels are used in this task."
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "The RAF-DB dataset consists of 15,339 images labeled"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "with 7-class basic emotion and 3,954 labeled with 12-class"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "compound emotion. Only images labeled with basic emotion"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "are used in this study."
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "The FER+ dataset contains 28,709 training, 3,589 validation"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "and 3,589 test\nimages. We combine its training data with"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "validation data for the training split and evaluate the model"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "performance on the test data."
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "The AFEW contains 773 train, 383 val and 653 test sam-"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "ples, which are collected from movies and TV serials with"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "spontaneous expressions, various poses, and illuminations."
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "Exploration of Emotion Features"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "We explore emotion features in two perspectives, namely"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "CNN backbones and pretraining emotion datasets."
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "For the choice of the CNN model, we compare IR50[4],"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "ResNet18[12], and VGGFace[21] in the Table 1, where the"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "former two models are pretrained on MS-Celeb-1M dataset"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "and the last one on VGGFace dataset. We find that the large"
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": ""
        },
        {
          "Figure 2: Our factorized bilinear pooling(FBP) module.": "CNN, IR50, is superior to the other two models."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 3: Evaluation of five feature enhancement strate-",
      "data": [
        {
          "Table": "",
          "3: Evaluation of five": "gies. The default setting is Rotation âˆˆ",
          "feature": "",
          "enhancement": "[âˆ’2Â°, 0Â°, 2Â°], scale",
          "strate-": "âˆˆ"
        },
        {
          "Table": "",
          "3: Evaluation of five": "",
          "feature": "",
          "enhancement": "",
          "strate-": ""
        },
        {
          "Table": "[1, 1.03, 1.07]",
          "3: Evaluation of five": "",
          "feature": "",
          "enhancement": "",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "",
          "feature": "",
          "enhancement": "",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "Visual Feature",
          "feature": "Augmentation details",
          "enhancement": "AFEW Val acc",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "Basic Feature",
          "feature": "â€”-",
          "enhancement": "61.1%",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "Basic Feature_RAF-DB",
          "feature": "â€”-",
          "enhancement": "58.5%",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "F-Mean",
          "feature": "default setting",
          "enhancement": "62.14%",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "F-MeanStd",
          "feature": "default setting",
          "enhancement": "63.7%",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "",
          "feature": "",
          "enhancement": "",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "",
          "feature": "ğ‘…ğ‘œğ‘¡ğ‘ğ‘¡ğ‘–ğ‘œğ‘› âˆˆ [âˆ’15Â°, 0Â°, 15Â°]",
          "enhancement": "",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "F-MeanStd-2",
          "feature": "",
          "enhancement": "62.4%",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "",
          "feature": "",
          "enhancement": "",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "",
          "feature": "ğ‘ ğ‘ğ‘ğ‘™ğ‘’ âˆˆ [0.75, 1, 1.25]",
          "enhancement": "",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "F-NormFFT",
          "feature": "Normalized FFT",
          "enhancement": "61.35%",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "F-AR-Mean",
          "feature": "default setting",
          "enhancement": "62.92%",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "FG-Net",
          "feature": "â€”-",
          "enhancement": "59%",
          "strate-": ""
        },
        {
          "Table": "",
          "3: Evaluation of five": "",
          "feature": "",
          "enhancement": "",
          "strate-": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 3: Evaluation of five feature enhancement strate-",
      "data": [
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": "Self"
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": "54.6%"
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": "54.0%"
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": "54.8%"
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": "We also use log Mel-spectrogram for audio CNN, which"
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": "obtains a little better performance, but the final results are"
        },
        {
          "Table 2: Evaluation of intra-modal fusion methods.": ""
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[16]\nfeature. For the fusion of audio and visual information, we\nShan Li and Weihong Deng. 2019. Reliable Crowdsourcing and Deep"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Locality-Preserving Learning for Unconstrained Facial Expression\nexplore feature concatenation and factorized bilinear pooling"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Recognition.\nIEEE TIP (2019)."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "(FBP). Besides, we evaluate different emotion features,\nin-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[17] Chuanhe Liu, Tianhao Tang, Kui Lv, and Minghao Wang. 2018. Multi-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "cluding an audio feature with both speech-spectrogram and"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Feature Based Emotion Recognition for Video Clips. In Proceedings of"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Log Mel-spectrogram and several facial features with differ-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "the 2018 on International Conference on Multimodal Interaction. ACM,"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ent CNN models and different emotion pretrained strategies.\n630â€“634."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[18] Cheng Lu, Wenming Zheng, Chaolong Li, Chuangao Tang, Suyuan\nWith careful evaluation, we obtain 62.48% and rank second"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Liu, Simeng Yan, and Yuan Zong. 2018. Multiple Spatio-temporal"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "in the EmotiW 2019 Challenge."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Feature Learning for Video-based Emotion Recognition in the Wild."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "In Proceedings of the 2018 on International Conference on Multimodal"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "5\nACKNOWLEDGMENTS"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Interaction. ACM, 646â€“652."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[19] Alex J Mitchell, Amol Vaze, and Sanjay Rao. 2009. Clinical diagnosis\nThis work is partially supported by the National Natural"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "of depression in primary care: a meta-analysis. The Lancet 374, 9690\nScience Foundation of China (U1613211), Shenzhen Basic"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "(2009), 609â€“619."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Research Program (JCYJ20170818164704758), the Joint Lab"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[20] Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. 1949."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "of CAS-HK."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "AffectNet: A Database for Facial Expression, Valence, and Arousal"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Computing in the Wild.\nIEEE Transactions on Affective Computing PP,"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "99 (1949), 1â€“1.\nREFERENCES"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[21] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. 2015. Deep\n[1]\nSarah Adel Bargal, Emad Barsoum, Cristian Canton Ferrer, and Cha"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "face recognition.. In BMVC, Vol. 1. 6.\nZhang. 2016.\nEmotion recognition in the wild from videos using"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[22] Wenling Shang, Diogo Almeida, Diogo Almeida, and Honglak Lee."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "images. In ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "2016. Understanding and improving convolutional neural networks\n[2] Emad Barsoum, Cha Zhang, Cristian Canton Ferrer, and Zhengyou"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "via concatenated rectified linear units. In International Conference on\nZhang. 2016. Training Deep Networks for Facial Expression Recogni-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "International Conference on Machine Learning.\ntion with Crowd-Sourced Label Distribution. In ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[23] Lianzhi Tan, Kaipeng Zhang, Kai Wang, Xiaoxing Zeng, Xiaojiang\n[3] Mingyi Chen, Xuanji He, Jing Yang, and Han Zhang. 2018. 3-D convo-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Peng, and Yu Qiao. 2017. Group emotion recognition with individual\nlutional recurrent neural networks with attention model for speech"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "facial emotion CNNs and global\nimage based CNNs. In Proceedings"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "IEEE Signal Processing Letters 25, 10 (2018), 1440â€“"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "of the 19th ACM International Conference on Multimodal Interaction.\n1444."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ACM, 549â€“552.\n[4]\nJiankang Deng, Jia Guo, and Stefanos Zafeiriou. 2018. Arcface: Ad-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ditive angular margin loss for deep face recognition. arXiv preprint"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "arXiv:1801.07698 (2018)."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Attention Is All You Need.\n(2017).\n[5] Abhinav Dhall, Roland Goecke, Shreya Ghosh, and Tom Gedeon. 2019."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[25] Valentin Vielzeuf, StÃ©phane Pateux, and FrÃ©dÃ©ric Jurie. 2017. Temporal\nEmotiW 2019: Automatic Emotion, Engagement and Cohesion Predic-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "multimodal fusion for video emotion classification in the wild. In ACM\ntionTasks. In ACM International Conference on Mutimodal Interaction."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ICMI.\n[6] Abhinav Dhall, Roland Goecke, Simon Lucey, Tom Gedeon, et al. 2012."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[26] Kai Wang,\n, Xiaoxing Zeng, Jianfei Yang, Debin Meng, Kaipeng Zhang,\nCollecting large, richly annotated facial-expression databases from"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Xiaojiang Peng, and Yu Qiao. 2018.\nCascade Attention Networks"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "IEEE multimedia (2012)."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "For Group Emotion Recognition with Face, Body and Image Cues. In"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "In Encyclopedia of"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Proceedings of the 19th ACM International Conference on Multimodal\ndatabase systems. Springer, 1327â€“1331."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Interaction (in press). ACM.\n[8] Yingruo Fan, Jacqueline CK Lam, and Victor OK Li. 2018. Video-based"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[27] Kai Wang, Xiaojiang Peng, Jianfei Yang, Debin Meng, and Yu Qiao.\nEmotion Recognition Using Deeply-Supervised Neural Networks. In"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "2019. Region Attention Networks for Pose and Occlusion Robust Facial"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Expression Recognition. arXiv preprint arXiv:1905.04075 (2019).\n[9] Yin Fan, Xiangju Lu, Dian Li, and Yuanliu Liu. 2016. Video-based"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[28] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. 2016.\nA\nemotion recognition using CNN-RNN and C3D hybrid networks. In"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "discriminative feature learning approach for deep face recognition. In"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "European Conference on Computer Vision. Springer, 499â€“515.\n[10]\nFelix A Gers, JÃ¼rgen Schmidhuber, and Fred Cummins. 1999. Learning"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[29]\nJianfei Yang, Kai Wang, Xiaojiang Peng, and Yu Qiao. 2018. Deep\nto forget: Continual prediction with LSTM.\n(1999)."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Recurrent Multi-instance Learning with Spatio-temporal Features for\n[11] Alex Graves and JÃ¼rgen Schmidhuber. 2005.\nFramewise phoneme"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Engagement Intensity Prediction. In Proceedings of the 2018 on Inter-\nclassification with bidirectional LSTM and other neural network ar-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "national Conference on Multimodal Interaction. ACM, 594â€“598.\nchitectures. Neural Networks 18, 5-6 (2005), 602â€“610."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[30] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Eduard Hovy. 2016. Hierarchical attention networks for document"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "residual learning for image recognition. In CVPR."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "classification. In Proceedings of the 2016 Conference of the North Ameri-\n[13] Ping Hu, Dongqi Cai, Shandong Wang, Anbang Yao, and Yurong Chen."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "can Chapter of the Association for Computational Linguistics: Human\n2017. Learning supervised scoring ensemble for emotion recognition"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Language Technologies. 1480â€“1489.\nin the wild. In ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[31] Anbang Yao, Dongqi Cai, Ping Hu, Shandong Wang, Liang Sha, and\n[14] Philip Nicholas Johnson-Laird. 1980. Mental models in cognitive sci-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Yurong Chen. 2016. HoloNet: towards robust emotion recognition in"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ence. Cognitive science 4, 1 (1980), 71â€“115."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "the wild. In ACM ICMI.\n[15] Boris Knyazev, Roman Shvetsov, Natalia Efremova,\nand Artem"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[32] Yuanyuan Zhang, Zi-Rui Wang, and Jun Du. 2019. Deep Fusion: An\nKuharenko. 2018.\nLeveraging large face recognition data for emo-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Attention Guided Factorized Bilinear Pooling for Audio-video Emotion"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "tion classification. In Automatic Face & Gesture Recognition (FG 2018),"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Recognition. arXiv preprint arXiv:1901.04889 (2019).\n2018 13th IEEE International Conference on. IEEE, 692â€“696."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[16]\nfeature. For the fusion of audio and visual information, we\nShan Li and Weihong Deng. 2019. Reliable Crowdsourcing and Deep"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Locality-Preserving Learning for Unconstrained Facial Expression\nexplore feature concatenation and factorized bilinear pooling"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Recognition.\nIEEE TIP (2019)."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "(FBP). Besides, we evaluate different emotion features,\nin-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[17] Chuanhe Liu, Tianhao Tang, Kui Lv, and Minghao Wang. 2018. Multi-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "cluding an audio feature with both speech-spectrogram and"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Feature Based Emotion Recognition for Video Clips. In Proceedings of"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Log Mel-spectrogram and several facial features with differ-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "the 2018 on International Conference on Multimodal Interaction. ACM,"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ent CNN models and different emotion pretrained strategies.\n630â€“634."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[18] Cheng Lu, Wenming Zheng, Chaolong Li, Chuangao Tang, Suyuan\nWith careful evaluation, we obtain 62.48% and rank second"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Liu, Simeng Yan, and Yuan Zong. 2018. Multiple Spatio-temporal"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "in the EmotiW 2019 Challenge."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Feature Learning for Video-based Emotion Recognition in the Wild."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "In Proceedings of the 2018 on International Conference on Multimodal"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "5\nACKNOWLEDGMENTS"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Interaction. ACM, 646â€“652."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[19] Alex J Mitchell, Amol Vaze, and Sanjay Rao. 2009. Clinical diagnosis\nThis work is partially supported by the National Natural"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "of depression in primary care: a meta-analysis. The Lancet 374, 9690\nScience Foundation of China (U1613211), Shenzhen Basic"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "(2009), 609â€“619."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Research Program (JCYJ20170818164704758), the Joint Lab"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[20] Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. 1949."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "of CAS-HK."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "AffectNet: A Database for Facial Expression, Valence, and Arousal"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Computing in the Wild.\nIEEE Transactions on Affective Computing PP,"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "99 (1949), 1â€“1.\nREFERENCES"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[21] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. 2015. Deep\n[1]\nSarah Adel Bargal, Emad Barsoum, Cristian Canton Ferrer, and Cha"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "face recognition.. In BMVC, Vol. 1. 6.\nZhang. 2016.\nEmotion recognition in the wild from videos using"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[22] Wenling Shang, Diogo Almeida, Diogo Almeida, and Honglak Lee."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "images. In ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "2016. Understanding and improving convolutional neural networks\n[2] Emad Barsoum, Cha Zhang, Cristian Canton Ferrer, and Zhengyou"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "via concatenated rectified linear units. In International Conference on\nZhang. 2016. Training Deep Networks for Facial Expression Recogni-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "International Conference on Machine Learning.\ntion with Crowd-Sourced Label Distribution. In ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[23] Lianzhi Tan, Kaipeng Zhang, Kai Wang, Xiaoxing Zeng, Xiaojiang\n[3] Mingyi Chen, Xuanji He, Jing Yang, and Han Zhang. 2018. 3-D convo-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Peng, and Yu Qiao. 2017. Group emotion recognition with individual\nlutional recurrent neural networks with attention model for speech"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "facial emotion CNNs and global\nimage based CNNs. In Proceedings"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "IEEE Signal Processing Letters 25, 10 (2018), 1440â€“"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "of the 19th ACM International Conference on Multimodal Interaction.\n1444."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ACM, 549â€“552.\n[4]\nJiankang Deng, Jia Guo, and Stefanos Zafeiriou. 2018. Arcface: Ad-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[24] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ditive angular margin loss for deep face recognition. arXiv preprint"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "arXiv:1801.07698 (2018)."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Attention Is All You Need.\n(2017).\n[5] Abhinav Dhall, Roland Goecke, Shreya Ghosh, and Tom Gedeon. 2019."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[25] Valentin Vielzeuf, StÃ©phane Pateux, and FrÃ©dÃ©ric Jurie. 2017. Temporal\nEmotiW 2019: Automatic Emotion, Engagement and Cohesion Predic-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "multimodal fusion for video emotion classification in the wild. In ACM\ntionTasks. In ACM International Conference on Mutimodal Interaction."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ICMI.\n[6] Abhinav Dhall, Roland Goecke, Simon Lucey, Tom Gedeon, et al. 2012."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[26] Kai Wang,\n, Xiaoxing Zeng, Jianfei Yang, Debin Meng, Kaipeng Zhang,\nCollecting large, richly annotated facial-expression databases from"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Xiaojiang Peng, and Yu Qiao. 2018.\nCascade Attention Networks"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "IEEE multimedia (2012)."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "For Group Emotion Recognition with Face, Body and Image Cues. In"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "In Encyclopedia of"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Proceedings of the 19th ACM International Conference on Multimodal\ndatabase systems. Springer, 1327â€“1331."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Interaction (in press). ACM.\n[8] Yingruo Fan, Jacqueline CK Lam, and Victor OK Li. 2018. Video-based"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[27] Kai Wang, Xiaojiang Peng, Jianfei Yang, Debin Meng, and Yu Qiao.\nEmotion Recognition Using Deeply-Supervised Neural Networks. In"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "2019. Region Attention Networks for Pose and Occlusion Robust Facial"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Expression Recognition. arXiv preprint arXiv:1905.04075 (2019).\n[9] Yin Fan, Xiangju Lu, Dian Li, and Yuanliu Liu. 2016. Video-based"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[28] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. 2016.\nA\nemotion recognition using CNN-RNN and C3D hybrid networks. In"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "discriminative feature learning approach for deep face recognition. In"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "European Conference on Computer Vision. Springer, 499â€“515.\n[10]\nFelix A Gers, JÃ¼rgen Schmidhuber, and Fred Cummins. 1999. Learning"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[29]\nJianfei Yang, Kai Wang, Xiaojiang Peng, and Yu Qiao. 2018. Deep\nto forget: Continual prediction with LSTM.\n(1999)."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Recurrent Multi-instance Learning with Spatio-temporal Features for\n[11] Alex Graves and JÃ¼rgen Schmidhuber. 2005.\nFramewise phoneme"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Engagement Intensity Prediction. In Proceedings of the 2018 on Inter-\nclassification with bidirectional LSTM and other neural network ar-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "national Conference on Multimodal Interaction. ACM, 594â€“598.\nchitectures. Neural Networks 18, 5-6 (2005), 602â€“610."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[30] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Eduard Hovy. 2016. Hierarchical attention networks for document"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "residual learning for image recognition. In CVPR."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "classification. In Proceedings of the 2016 Conference of the North Ameri-\n[13] Ping Hu, Dongqi Cai, Shandong Wang, Anbang Yao, and Yurong Chen."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "can Chapter of the Association for Computational Linguistics: Human\n2017. Learning supervised scoring ensemble for emotion recognition"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Language Technologies. 1480â€“1489.\nin the wild. In ACM ICMI."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[31] Anbang Yao, Dongqi Cai, Ping Hu, Shandong Wang, Liang Sha, and\n[14] Philip Nicholas Johnson-Laird. 1980. Mental models in cognitive sci-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Yurong Chen. 2016. HoloNet: towards robust emotion recognition in"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "ence. Cognitive science 4, 1 (1980), 71â€“115."
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "the wild. In ACM ICMI.\n[15] Boris Knyazev, Roman Shvetsov, Natalia Efremova,\nand Artem"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "[32] Yuanyuan Zhang, Zi-Rui Wang, and Jun Du. 2019. Deep Fusion: An\nKuharenko. 2018.\nLeveraging large face recognition data for emo-"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Attention Guided Factorized Bilinear Pooling for Audio-video Emotion"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "tion classification. In Automatic Face & Gesture Recognition (FG 2018),"
        },
        {
          "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition\nICMI â€™19, October 14â€“18, 2019, Suzhou, China": "Recognition. arXiv preprint arXiv:1901.04889 (2019).\n2018 13th IEEE International Conference on. IEEE, 692â€“696."
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in the wild from videos using images",
      "authors": [
        "Adel Sarah",
        "Emad Bargal",
        "Cristian Canton Barsoum",
        "Cha Ferrer",
        "Zhang"
      ],
      "year": "2016",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "2",
      "title": "Training Deep Networks for Facial Expression Recognition with Crowd-Sourced Label Distribution",
      "authors": [
        "Emad Barsoum",
        "Cha Zhang",
        "Cristian Canton Ferrer",
        "Zhengyou Zhang"
      ],
      "year": "2016",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "3",
      "title": "3-D convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "Mingyi Chen",
        "Xuanji He",
        "Jing Yang",
        "Han Zhang"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "4",
      "title": "Arcface: Additive angular margin loss for deep face recognition",
      "authors": [
        "Jiankang Deng",
        "Jia Guo",
        "Stefanos Zafeiriou"
      ],
      "year": "2018",
      "venue": "Arcface: Additive angular margin loss for deep face recognition",
      "arxiv": "arXiv:1801.07698"
    },
    {
      "citation_id": "5",
      "title": "Automatic Emotion, Engagement and Cohesion Predic-tionTasks",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Shreya Ghosh",
        "Tom Gedeon"
      ],
      "year": "2019",
      "venue": "ACM International Conference on Mutimodal Interaction"
    },
    {
      "citation_id": "6",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "Abhinav Dhall",
        "Roland Goecke",
        "Simon Lucey",
        "Tom Gedeon"
      ],
      "year": "2012",
      "venue": "IEEE multimedia"
    },
    {
      "citation_id": "7",
      "title": "Human-computer interaction",
      "authors": [
        "Alan Dix"
      ],
      "year": "2009",
      "venue": "Encyclopedia of database systems"
    },
    {
      "citation_id": "8",
      "title": "Video-based Emotion Recognition Using Deeply-Supervised Neural Networks",
      "authors": [
        "Yingruo Fan",
        "Jacqueline Lam",
        "O Victor",
        "Li"
      ],
      "year": "2018",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "9",
      "title": "Video-based emotion recognition using CNN-RNN and C3D hybrid networks",
      "authors": [
        "Yin Fan",
        "Xiangju Lu",
        "Dian Li",
        "Yuanliu Liu"
      ],
      "year": "2016",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "10",
      "title": "Learning to forget: Continual prediction with LSTM",
      "authors": [
        "JÃ¼rgen Felix A Gers",
        "Fred Schmidhuber",
        "Cummins"
      ],
      "year": "1999",
      "venue": "Learning to forget: Continual prediction with LSTM"
    },
    {
      "citation_id": "11",
      "title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures",
      "authors": [
        "Alex Graves",
        "JÃ¼rgen Schmidhuber"
      ],
      "year": "2005",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "CVPR"
    },
    {
      "citation_id": "13",
      "title": "Learning supervised scoring ensemble for emotion recognition in the wild",
      "authors": [
        "Ping Hu",
        "Dongqi Cai",
        "Shandong Wang",
        "Anbang Yao",
        "Yurong Chen"
      ],
      "year": "2017",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "14",
      "title": "Mental models in cognitive science",
      "authors": [
        "Philip Nicholas"
      ],
      "year": "1980",
      "venue": "Cognitive science"
    },
    {
      "citation_id": "15",
      "title": "Leveraging large face recognition data for emotion classification",
      "authors": [
        "Boris Knyazev",
        "Roman Shvetsov",
        "Natalia Efremova",
        "Artem Kuharenko"
      ],
      "year": "2018",
      "venue": "Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "16",
      "title": "Reliable Crowdsourcing and Deep Locality-Preserving Learning for Unconstrained Facial Expression Recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2019",
      "venue": "IEEE"
    },
    {
      "citation_id": "17",
      "title": "Multi-Feature Based Emotion Recognition for Video Clips",
      "authors": [
        "Chuanhe Liu",
        "Tianhao Tang",
        "Kui Lv",
        "Minghao Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "18",
      "title": "Multiple Spatio-temporal Feature Learning for Video-based Emotion Recognition in the Wild",
      "authors": [
        "Cheng Lu",
        "Wenming Zheng",
        "Chaolong Li",
        "Chuangao Tang",
        "Suyuan Liu",
        "Simeng Yan",
        "Yuan Zong"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "19",
      "title": "Clinical diagnosis of depression in primary care: a meta-analysis",
      "authors": [
        "Alex J Mitchell",
        "Amol Vaze",
        "Sanjay Rao"
      ],
      "year": "2009",
      "venue": "The Lancet"
    },
    {
      "citation_id": "20",
      "title": "AffectNet: A Database for Facial Expression, Valence, and Arousal Computing in the Wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "1949",
      "venue": "IEEE Transactions on Affective Computing PP"
    },
    {
      "citation_id": "21",
      "title": "Deep face recognition",
      "authors": [
        "Andrea Omkar M Parkhi",
        "Andrew Vedaldi",
        "Zisserman"
      ],
      "year": "2015",
      "venue": "BMVC"
    },
    {
      "citation_id": "22",
      "title": "Understanding and improving convolutional neural networks via concatenated rectified linear units",
      "authors": [
        "Wenling Shang",
        "Diogo Almeida",
        "Diogo Almeida",
        "Honglak Lee"
      ],
      "year": "2016",
      "venue": "International Conference on International Conference on Machine Learning"
    },
    {
      "citation_id": "23",
      "title": "Group emotion recognition with individual facial emotion CNNs and global image based CNNs",
      "authors": [
        "Lianzhi Tan",
        "Kaipeng Zhang",
        "Kai Wang",
        "Xiaoxing Zeng",
        "Xiaojiang Peng",
        "Yu Qiao"
      ],
      "year": "2017",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "24",
      "title": "Attention Is All You Need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Lukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Attention Is All You Need"
    },
    {
      "citation_id": "25",
      "title": "Temporal multimodal fusion for video emotion classification in the wild",
      "authors": [
        "Valentin Vielzeuf",
        "StÃ©phane Pateux",
        "FrÃ©dÃ©ric Jurie"
      ],
      "year": "2017",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "26",
      "title": "Cascade Attention Networks For Group Emotion Recognition with Face, Body and Image Cues",
      "authors": [
        "Kai Wang",
        "Xiaoxing Zeng",
        "Jianfei Yang",
        "Debin Meng",
        "Kaipeng Zhang",
        "Xiaojiang Peng",
        "Yu Qiao"
      ],
      "year": "2018",
      "venue": "Proceedings of the 19th ACM International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "27",
      "title": "Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition",
      "authors": [
        "Kai Wang",
        "Xiaojiang Peng",
        "Jianfei Yang",
        "Debin Meng",
        "Yu Qiao"
      ],
      "year": "2019",
      "venue": "Region Attention Networks for Pose and Occlusion Robust Facial Expression Recognition",
      "arxiv": "arXiv:1905.04075"
    },
    {
      "citation_id": "28",
      "title": "A discriminative feature learning approach for deep face recognition",
      "authors": [
        "Yandong Wen",
        "Kaipeng Zhang",
        "Zhifeng Li",
        "Yu Qiao"
      ],
      "year": "2016",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "Deep Recurrent Multi-instance Learning with Spatio-temporal Features for Engagement Intensity Prediction",
      "authors": [
        "Jianfei Yang",
        "Kai Wang",
        "Xiaojiang Peng",
        "Yu Qiao"
      ],
      "year": "2018",
      "venue": "Proceedings of the 2018 on International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "30",
      "title": "Hierarchical attention networks for document classification",
      "authors": [
        "Zichao Yang",
        "Diyi Yang",
        "Chris Dyer",
        "Xiaodong He",
        "Alex Smola",
        "Eduard Hovy"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference of the North American Chapter"
    },
    {
      "citation_id": "31",
      "title": "HoloNet: towards robust emotion recognition in the wild",
      "authors": [
        "Anbang Yao",
        "Dongqi Cai",
        "Ping Hu",
        "Shandong Wang",
        "Liang Sha",
        "Yurong Chen"
      ],
      "year": "2016",
      "venue": "ACM ICMI"
    },
    {
      "citation_id": "32",
      "title": "Deep Fusion: An Attention Guided Factorized Bilinear Pooling for Audio-video Emotion Recognition",
      "authors": [
        "Yuanyuan Zhang",
        "Zi-Rui Wang",
        "Jun Du"
      ],
      "year": "2019",
      "venue": "Deep Fusion: An Attention Guided Factorized Bilinear Pooling for Audio-video Emotion Recognition",
      "arxiv": "arXiv:1901.04889"
    }
  ]
}