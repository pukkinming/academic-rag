{
  "paper_id": "2502.04424v2",
  "title": "Emobench-M: Benchmarking Emotional Intelligence For Multimodal Large Language Models",
  "published": "2025-02-06T18:13:35Z",
  "authors": [
    "He Hu",
    "Yucheng Zhou",
    "Lianzhong You",
    "Hongbo Xu",
    "Qianning Wang",
    "Zheng Lian",
    "Fei Richard Yu",
    "Fei Ma",
    "Laizhong Cui"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Models (MLLMs): The diagram outlines the categories of \"Foundational Emotion Recognition\", \"Conversational Emotion Understanding\", and \"Socially Complex Emotion Analysis\" along with their respective evaluation scenarios. It also presents a performance comparison of different methods on the proposed dataset EmoBench-M. The \"Random\" baseline refers to a heuristic approach that randomly selects labels from the available candidates.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotional Intelligence (EI), initially conceptualized by  Salovey and Mayer (1990) , emphasizes the ability to perceive, understand, regulate, and apply emotions in oneself and others. Recent advancements in multimodal large language models (MLLMs) have significantly improved human-computer interaction and natural language understanding, and integrating MLLMs into robotic control systems has become increasingly prevalent  (Sartor and Thompson 2024; Cheng et al. 2024a) . Incorporating EI capabilities within MLLMs is essential for improving robotic performance in real-world environments. It will enable robots to address human emotional arXiv:2502.04424v2  [cs.CL]   needs better and ensure more effective interactions.\n\nHowever, there is currently no universal benchmark to evaluate the EI capabilities of MLLMs comprehensively. Table  1  lists existing benchmarks for evaluating EI, demonstrating that most are designed for text-only or text-image EI tasks  (Sabour et al. 2024; Yang et al. 2024b; Huang et al. 2023a ), and most are not grounded in established psychological theories. Real-world MLLM-driven human-robot interactions typically occur in dynamic, multimodal environments. Unlike static text and images, videos with audio provide richer and more complex multimodal information, including dynamic facial expressions, body language, and vocal tone, which more authentically convey the flow of emotions and the interactive process. Evaluating MLLMs in multimodal environments is crucial because it allows for a more comprehensive understanding of their ability to interpret and respond to diverse emotional cues in real-world scenarios.\n\nBuilding on established psychological theories of EI  (Salovey and Mayer 1990; Huang et al. 2023b) , we explore the EI capabilities of MLLMs across three primary dimensions:\n\n(1) Foundational Emotion Recognition: This dimension focuses on accurately identifying emotional states through explicit signals such as facial expressions, vocal tone, and body language  (Ekman 1992) . It also emphasizes the extraction of emotional information from multimodal signals  (Poria et al. 2019b ). (  2 ) Conversational Emotion Understanding: Extending beyond foundational recognition, this dimension requires the ability to track emotional dynamics within conversations and to comprehend the contextual and situational meanings of emotions  (Gross 2002; Poria et al. 2019a; Liu et al. 2024 ). (3) Socially Complex Emotion Understanding: Representing an advanced level of EI, this dimension involves understanding emotions influenced not only by internal affective states but also by external social and cultural contexts. It requires AI systems to exhibit mentalizing capabilities, the ability to infer others' emotions and intentions based on environmental cues  (Frith and Frith 2006; Zhang et al. 2024c ). Building on these three dimensions, we propose a novel multimodal EI benchmark, EmoBench-M, for MLLMs. As shown in Figure  1 , our benchmark includes 13 scenarios covering diverse contexts such as music and presentation, multi-party dialogues, and social conversation. By utilizing multimodal data, i.e., video with audio, EmoBench-M enables a more comprehensive evaluation of the EI of MLLMs. Moreover, controversial samples were excluded after a thorough human review to ensure the quality of the benchmark.\n\nTo the best of our knowledge, EmoBench-M is the first comprehensive benchmark to evaluate EI at the multimodality level. We evaluate various open-source MLLMs (e.g., Video-LLaMA2  (Cheng et al. 2024b ) and InternVL2.5  (Chen et al. 2024) ) and closed-source MLLMs (e.g.,  GLM-4V (Zeng et al. 2024)  and Gemini  (Reid et al. 2024 )) on EmoBench-M. Our findings indicate that the EI capability of MLLMs in multimodal and realistic environments remains substantially below human performance in many scenarios. Moreover, we conduct an extensive evaluation of MLLMs across varying model sizes and reasoning levels. We will release our code and data to encourage further research in MLLM's EI.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal Large Language Models (MLLMs) have rapidly expanded, building upon the success of LLMs. MLLMs integrate various modalities, including images, videos, and audio  (Cheng et al. 2024b; Yao et al. 2024; Team et al. 2024) , excelling in perception, reasoning, and diverse tasks  (Zhu et al. 2024; Wang et al. 2024) . Models like Qwen2-Audio  (Chu et al. 2024) , MiniCPM-V  (Yao et al. 2024) , LongVA  (Zhang et al. 2024a) ,  GLM (Zeng et al. 2024) , InternVL  (Chen et al. 2024) , InternVideo2  (Wang et al. 2024) , and Video-LLaMA2  (Cheng et al. 2024b ) demonstrate advancements in vision and audio-video understanding. Gemini  (Anil et al. 2023; Reid et al. 2024; Team et al. 2024 ) natively supports multimodality. Further improvements are explored through visual dependency  (Zhou et al. 2024d ) and in-context learning  (Zhou et al. 2024c) . Evaluating Emotional Intelligence (EI) is crucial, with benchmarks like MERBench  (Lian et al. 2024b ), MER2023  (Lian et al. 2023a ), MC-EIU  (Liu et al. 2024) , MOSABench  (Song et al. 2024b) , and studies on GPT-4V  (Lian et al. 2023b)  focusing on multimodal emotion recognition and sentiment analysis. EmotionBench  (Huang et al. 2023b) ,  EIBench (Zhao et al. 2024) , EmoBench  (Sabour et al. 2024) , EQ-Bench  (Paech 2023) , and SOUL  (Deng et al. 2023)    \"Num\" indicates the number of samples. \"ACC\" denotes accuracy. For the \"Laughter Reasoning\", we employ an LLM as the evaluator, and evaluation prompts are shown in Appendix H. To ensure fair and consistent comparisons in future research, we adopted the open-source Qwen2.5-72B-Instruct  (Yang et al. 2024a) . Details of the specific categories in the classification tasks are provided in the Appendix B. Details of prompt are in Appendix G.\n\ndelve into deeper EI aspects, revealing gaps between LLMs and human-like emotional understanding. Moreover, numerous multimodal benchmarks assess MLLMs, including MME  (Fu et al. 2023) , MMT-Bench  (Ying et al. 2024)  for general capabilities, MultiTrust  (Zhang et al. 2024b ) for trustworthiness, and HumanVBench  (Zhou et al. 2024b) , MVBench  (Li et al. 2024c ) for video understanding. The full version can be found in Appendix A.\n\n3 EmoBench-M",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Taxonomy",
      "text": "To systematically evaluate MLLM EI capabilities, the evaluation focuses on three dimensions based on established psychological theories of EI  (Salovey and Mayer 1990) : \"Foundational Emotion Recognition\", \"Conversational Emotion Understanding\", and \"Socially Complex Emotion Analysis\". Table  2  details evaluation scenarios in each dimension.\n\nFoundational Emotion Recognition Foundational emotion recognition, a core aspect of Emotional Intelligence (EI), focuses on identifying basic emotions such as anger, happiness, and sadness  (Ekman 1992) . This dimension evaluates a Multimodal Large Language Model's (MLLMs') ability to extract and integrate emotional information from multimodal signals  (video, audio, and text)  to recognize these fundamental emotions, a crucial capability for higher-level EI. The MLLMs' proficiency in discerning emotions conveyed through speech, music, and video is assessed. Song and Speech Emotion Recognition uses data sourced from  (Livingstone and Russo 2018) , which provides video clips with audio-visual emotional cues. Opinion Sentiment Analysis utilizes data sourced from  (Zadeh et al. 2016) , focusing on speech and facial expressions in opinion videos. Emotion Intensity Analysis goes beyond simple polarity; data sourced from the CMU-MOSEI dataset  (Zadeh et al. 2018 ) is used to assess both the emotional state and its intensity from audio and video. This requires the model to identify specific emotion categories (e.g., happiness, sadness, anger) and quantify their intensity levels across diverse video content. Stock Comment Emotion Analysis employs data sourced from  (Song et al. 2024a) , analyzing emotions expressed in stock-related video comments.\n\nConversational Emotion Understanding Conversational emotion understanding requires MLLMs to track emotional dynamics and interpret their contextual significance  (Poria et al. 2019b ). This involves identifying emotional shifts in multi-party conversations, leveraging semantic and tonal cues, and adapting to dynamic contexts, including interparticipant emotional interplay.  emotion understanding is an advanced EI dimension, encompassing the ability to identify, comprehend, and respond to nuanced emotions and social intentions in intricate social scenarios. This dimension primarily evaluates emotions arising in complex social contexts, requiring deeper inference of emotions like humor, sarcasm, and latent feelings based on social interactions and norms. Humor Understanding utilizes data sourced from  (Hasan et al. 2019) . Sarcasm Detection uses data sourced from  (Castro et al. 2019) . Laughter Reasoning employs data sourced from  (Hyun et al. 2024)  to analyze the complex emotions in various social situations.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Data Collection And Processing",
      "text": "The EmoBench-M benchmark was meticulously curated to evaluate the EI capabilities of MLLMs across a diverse range of tasks. As illustrated in Figure  2 , the data collection and processing pipeline involved a rigorous, formalized procedure for data filtering and class balancing to ensure high quality and fairness.\n\nFiltering and Quality Assurance. To eliminate ambiguous or mislabeled samples from the initial datasets, we implemented a multi-reviewer verification process. Each sample underwent manual review by a panel of N = 3 graduate students. Let s be a sample from an initial dataset D initial , and let y orig (s) be its original label. Each reviewer i assigned an emotion label v i (s) from the set of possible labels L.\n\nWe formalized the reviewers' consensus using a majority vote. For any given label l ∈ L, the vote count from the reviewers for sample s is given by:\n\nwhere I(•) is the indicator function, which is 1 if the condition is true and 0 otherwise. The consensus label from the reviewers, y rev (s), is the label that receives the maximum number of votes:\n\nA sample s was retained only if the original label y orig (s) matched the reviewers' consensus label y rev (s). This filtering criterion ensures that only samples with high inter-annotator agreement and consistency with the original labels are included. Consequently, the final filtered dataset, D filtered , is defined as:\n\nThis robust procedure guarantees that EmoBench-M is built upon high-quality, unambiguous data, thereby enhancing the benchmark's reliability and validity.\n\nClass Imbalance Correction. To ensure a fair and unbiased evaluation, we addressed potential class imbalances within the filtered data. For each task, we capped the maximum number of test samples at 500. If the size of a filtered dataset for a task, |D filtered |, exceeded this threshold, we performed a targeted down-sampling procedure. This procedure iteratively removes samples from the majority class until the dataset size is reduced to 500. Specifically, in each step of the process, we identify the majority class c maj with the highest number of instances:\n\nwhere |D c | is the number of samples with label c in the current dataset. A single sample is then randomly removed from this majority class. This process is repeated until |D filtered | = 500. This method preserves the presence of minority classes while creating a more balanced class distribution, which prevents the benchmark from being skewed towards dominant emotions and promotes better model generalization across the full spectrum of emotions.\n\nDataset Statistics. EmoBench-M encompasses tasks with varying sample sizes, ranging from 80 to 500 samples per task, as detailed in Table  2 . Each task is designed to evaluate different facets of emotional intelligence, spanning from basic emotion recognition to understanding more complex social emotions. Performance metric is accuracy (ACC), and LLM-based evaluation is employed for generation tasks.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results And Findings",
      "text": "Tables  3, 4 , and 5, present the performance of various models on the EmoBench-M benchmark. Closed-source models consistently outperform open-source ones. Gemini-2.0-Flash achieves the best results in FER (61.4), CEU (53.4), and overall average (62.3). Among open-source models, Qwen2-Audio-7B-Instruct leads in FER (59.9), while Qwen2.5-VL-72B-Instruct performs best in SCEA (72.5) and shows strong results overall.\n\nAnalysis of these results reveals several factors driving the performance differences. (1) Model Scale and Modality Focus: At smaller scales (4-8B), audio-centric models like Qwen2-Audio excel in FER (Table  3 ), highlighting the primacy of speech for basic emotion analysis. At larger scales (38-78B), models with stronger vision encoders, such as InternVL, gain an advantage, suggesting that effective visualtextual fusion becomes increasingly critical as model capacity increases. (2) Native Multimodal Architecture: The superior performance of the Gemini series likely stems from its native multimodal design, which synchronously processes video and audio streams. This holistic approach is better suited for capturing the dynamic nature of emotional expression compared to models that fuse modalities at later stages.\n\n(3) Perception over Complex Reasoning: Gemini-2.0-Flash-Thinking, which adds a reasoning step, performs slightly worse than its base version (Table  5 ). This suggests that for emotion-centric tasks, effective cross-modal perception and coordination are more pivotal than complex chain-of-thoughtstyle reasoning.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With Human Performance",
      "text": "Table  5  compares the performance of MLLMs and human evaluators across the three dimensions of EmoBench-M: FER, CEU, and SCEA, along with the average score (Avg.). To ensure a fair comparison, all human evaluators followed a standardized framework, using the same emotion categories and definitions as those provided to the models. Among the models, Gemini-2.0-Flash performs the best, achieving an average score of 62.3; however, it still lags behind human performance, which averages 73.0. Humans excel in CEU with a score of 84.4, highlighting their superior contextual understanding. However, their performance in SCEA (72.7) is only marginally better than that of Gemini-2.0-Flash (72.0). The relatively lower human score in SCEA is attributed to cultural differences between the annotators and the dataset, which introduced challenges in achieving optimal performance in this dimension. Therefore, while MLLMs demonstrate promising results, they remain behind humans in tasks requiring nuanced understanding and contextual reasoning. More details on the human evaluation protocol are in Appendix D.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Analysis On Generation Metric",
      "text": "Table  6  shows cosine similarity and Pearson correlation results for traditional metrics (BLEU-4, ROUGE-L, BERTScore) and the open-source LLM Qwen2.5-72B-Instruct with human judgments. Among traditional metrics, BERTScore showed the highest consistency with human evaluation (cosine similarity: 0.8762, Pearson correlation: (j) CEIA (int.)",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Analysis On Class-Wise Performance",
      "text": "Figure  3  shows the class-wise performance of Gemini-2.0-Flash across three scenarios: Foundational Emotion Recognition (FER), Conversational Emotion Understanding (CEU), and Socially Complex Emotion Analysis (SCEA). In FER (SOER, SPER, SCEA, OSA, EIA), the model performs well on primary emotions like \"angry\" and \"neutral\", as well as sentiments like \"positive\" and \"negative\". However, it struggles with subtle or overlapping emotions, such as \"fearful\" and \"calm\", and distinctions between \"neutral\" and \"positive\". In CEU (FGDEA, PEA, FCDEA, CEIA, MPDER), the model effectively handles common emotions like \"neutral\" and structured tasks like the intent analysis. However, complex emotions like \"surprise\" and nuanced intents such as \"encouraging\" present challenges. In SCEA (HU, SD), the model achieves strong performance in HU, and SD reveals difficulties, underlining the challenges posed by subtle social emotions. Gemini-2.0-Flash demonstrates robust performance but struggles with nuanced and overlapping categories in complex scenarios. Table  7 : Stability experiment of Gemini-2.0-Flash on three dimensions, running predictions 1, 3, and 5 times, with the majority voting to obtain the final result. Since LR is a generative task, the stability experiment on the Socially Complex Emotion Analysis only involves the HU and SD scenarios.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Stability Analysis Of Mllm",
      "text": "We analyze the stability of Gemini-2.0-Flash across three dimensions: FER, CEU, and SCEA. Stability is evaluated by running predictions 1, 3, and 5 times, using majority voting for final results (shown in Table  7 ). In FER, scores are 61.4, 61.2, and 61.0, with the highest at 61.4. For CEU, scores improve from 53.4 to 54.1 across iterations. In SCEA, results are 72.0, 72.5, and 72.8, considering only HU and SD scenarios due to the generative nature of this task. These results demonstrate the model's robust stability, with minor variations and improvements in complex emotional contexts. More details can be found in Appendix E.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion And Future Work",
      "text": "We A Related Work",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "A.1 Multimodal Large Language Models",
      "text": "With the success of LLMs in various natural language processing (NLP) tasks, such as reasoning and euphemism detection  (Li et al. 2024d ), numerous efforts have been made to extend LLMs to multimodal areas, i.e., MLLMs, enabling them to process additional types of information, including images, videos, and audio  (Cheng et al. 2024b; Yao et al. 2024) .\n\nMLLMs excel in multimodal perception and reasoning and handle more diverse tasks with inputs from different multimodality  (Zhu et al. 2024; Wang et al. 2024) . For instance, Qwen2-Audio  (Chu et al. 2024)",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "A.2 Evaluation Of Emotional Intelligence",
      "text": "Given that EI is essential for understanding and responding to human emotions, many studies have focused on evaluating the EI capabilities of LLMs. MERBench  (Lian et al. 2024b ) standardizes evaluation for multimodal emotion recognition by addressing inconsistencies in feature extractors and offering a unified framework. It introduces MER2023  (Lian et al. 2023a ), a dataset focused on the Chinese language, emphasizing multi-label learning and robustness analysis. Moreover, MC-EIU  (Liu et al. 2024)",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "B Details And Case Of Datasets",
      "text": "This section provides an overview of the datasets used in our experiments, highlighting the number of test samples and the corresponding labels associated with each dataset.   (Reid et al. 2024)  0.95 40 1.0 API Gemini-2.0-Flash  (Team et al. 2024)  0.95 40 1.0 API Gemini-2.0-Flash-Thinking  (Team et al. 2024)   • UR-FUNNY and MUStARD: Both binary-labeled multimodal datasets. UR-FUNNY is used for humor detection, while MUStARD is designed for sarcasm detection, with annotations of true or false. • SMILE: A small-scale dataset focusing on humor comprehension, annotated with explanations of why the audience laughed.\n\nTable  9  provides detailed statistics on the test samples and label distributions for each dataset.",
      "page_start": 11,
      "page_end": 11
    },
    {
      "section_name": "C Model Configuration",
      "text": "The configuration details of the models evaluated on EmoBench-M are summarized in Table  8 . The table provides a detailed comparison of key hyperparameters, including top-p and top-k sampling values, temperature settings, and VRAM requirements. For models accessed via APIs, the VRAM is denoted as \"API\", reflecting their closed-source nature and cloud-based deployment. The evaluated models encompass a range of architectures, from smaller-scale models, such as InternVL2.5-4B  (Chen et al. 2024) , to large-scale variants like InternVL2.5-78B  (Chen et al. 2024) . Noteworthy configurations include Video-LLaMA2.1-AV-7B  (Cheng et al. 2024b) , which incorporates audiovisual processing, and Gemini-2.0-Flash-Thinking  (Team et al. 2024) , which features enhanced reasoning capabilities. Most models exhibit consistent sampling parameters (e.g., top-p and temperature), ensuring a standardized basis for comparison. The VRAM requirements reflect the computational demands of these models, ranging from 14 GB for smaller models to 168 GB for the largest configurations. In contrast, API-based models provide an accessible option for users, particularly when local resources are constrained, albeit at the cost of reduced transparency due to their closed-source nature.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "D Details On Comparison With Human Performance",
      "text": "Table 10 provides a detailed comparison of performance metrics between MLLMs and human participants across various evaluation scenarios. The Random baseline demonstrates consistently low performance, serving as a reference for assessing advancements in model capabilities. Among the MLLMs, Gemini-2.0-Flash exhibits the highest performance, surpassing other models such as InternVL2.5-78B and GLM-4V-PLUS in most metrics, including SOER, SCEA, and FGDEA. Despite these advancements, human participants consistently outperform the models in critical areas such as SPER, FGDEA, and PEA, with perfect scores achieved in logical reasoning (LR). This highlights the superior generalization and adaptability of humans compared to current MLLMs.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "E Details On Stability Analysis Of Mllm",
      "text": "",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "F More Analysis On Class-Wise Performance",
      "text": "For class-wise performance, confusion matrices for more models can be found in Figure  17 -32.",
      "page_start": 12,
      "page_end": 12
    },
    {
      "section_name": "Acc Waf Acc Waf Acc Waf Acc Waf Acc Waf Acc Waf",
      "text": "Open-Source Model (j) CEIA (int.)\n\nFigure  17 : Confusion matrices for Gemini-1.5-Flash on each evaluation scenario of EmoBench-M.\n\nFigure  18 : Confusion matrices for Gemini-2.0-Flash-Thinking on each evaluation scenario of EmoBench-M. (j) CEIA (int.)\n\nFigure  19 : Confusion matrices for GLM-4V-PLUS on each evaluation scenario of EmoBench-M.   (j) CEIA (int.)  (j) CEIA (int.)  (j) CEIA (int.)  (j) CEIA (int.)\n\nFigure  24 : Confusion matrices for InternVideo2-Chat-8B on each evaluation scenario of EmoBench-M. (j) CEIA (int.)  (j) CEIA (int.)",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "G Prompt",
      "text": "Prompt From RAVDESS(song)",
      "page_start": 28,
      "page_end": 28
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Taxonomy for Evaluating Emotion Intelligence (EI) Capabilities of Multimodal Large Language Models (MLLMs): The diagram",
      "page": 1
    },
    {
      "caption": "Figure 1: , our benchmark includes 13 scenarios covering",
      "page": 2
    },
    {
      "caption": "Figure 2: Data Filtering and Label Verification Process. Bar charts",
      "page": 4
    },
    {
      "caption": "Figure 2: , the data collection and pro-",
      "page": 4
    },
    {
      "caption": "Figure 3: Confusion matrices for Gemini-2.0-Flash on each evaluation scenario of EmoBench-M. Other models in Appendix F.",
      "page": 7
    },
    {
      "caption": "Figure 3: shows the class-wise performance of Gemini-2.0-",
      "page": 7
    },
    {
      "caption": "Figure 4: Example of CH-SIMSv2 dataset.",
      "page": 13
    },
    {
      "caption": "Figure 5: Example of CH-SIMS dataset.",
      "page": 15
    },
    {
      "caption": "Figure 6: Example of CMU-MOSEI dataset.",
      "page": 16
    },
    {
      "caption": "Figure 7: Example of CMU-MOSI dataset.",
      "page": 16
    },
    {
      "caption": "Figure 8: Example of FMSA-SC dataset.",
      "page": 16
    },
    {
      "caption": "Figure 9: Example of MC-EIU dataset.",
      "page": 17
    },
    {
      "caption": "Figure 10: Example of MELD dataset.",
      "page": 17
    },
    {
      "caption": "Figure 11: Example of MER2023 dataset.",
      "page": 17
    },
    {
      "caption": "Figure 12: Example of MUStARD dataset.",
      "page": 18
    },
    {
      "caption": "Figure 13: Example of RAVDSS-song dataset.",
      "page": 18
    },
    {
      "caption": "Figure 14: Example of RAVDSS-speech dataset.",
      "page": 19
    },
    {
      "caption": "Figure 15: Example of UR-FUNNY dataset.",
      "page": 19
    },
    {
      "caption": "Figure 16: Example of SMILE dataset.",
      "page": 19
    },
    {
      "caption": "Figure 17: Confusion matrices for Gemini-1.5-Flash on each evaluation scenario of EmoBench-M.",
      "page": 20
    },
    {
      "caption": "Figure 18: Confusion matrices for Gemini-2.0-Flash-Thinking on each evaluation scenario of EmoBench-M.",
      "page": 20
    },
    {
      "caption": "Figure 19: Confusion matrices for GLM-4V-PLUS on each evaluation scenario of EmoBench-M.",
      "page": 21
    },
    {
      "caption": "Figure 20: Confusion matrices for InternVL2.5-4B on each evaluation scenario of EmoBench-M.",
      "page": 21
    },
    {
      "caption": "Figure 21: Confusion matrices for InternVL2.5-8B on each evaluation scenario of EmoBench-M.",
      "page": 22
    },
    {
      "caption": "Figure 22: Confusion matrices for InternVL2.5-38B on each evaluation scenario of EmoBench-M.",
      "page": 22
    },
    {
      "caption": "Figure 23: Confusion matrices for InternVL2.5-78B on each evaluation scenario of EmoBench-M.",
      "page": 23
    },
    {
      "caption": "Figure 24: Confusion matrices for InternVideo2-Chat-8B on each evaluation scenario of EmoBench-M.",
      "page": 23
    },
    {
      "caption": "Figure 25: Confusion matrices for LongVA-7B-DPO on each evaluation scenario of EmoBench-M.",
      "page": 24
    },
    {
      "caption": "Figure 26: Confusion matrices for MiniCPM-V-2.6-8B on each evaluation scenario of EmoBench-M.",
      "page": 24
    },
    {
      "caption": "Figure 27: Confusion matrices for Qwen2-Audio-7B-Instruct on each evaluation scenario of EmoBench-M.",
      "page": 25
    },
    {
      "caption": "Figure 28: Confusion matrices for VideoLLaMA2-7B on each evaluation scenario of EmoBench-M.",
      "page": 25
    },
    {
      "caption": "Figure 29: Confusion matrices for VideoLLaMA2-7B-16F on each evaluation scenario of EmoBench-M.",
      "page": 26
    },
    {
      "caption": "Figure 30: Confusion matrices for VideoLLaMA2-72B on each evaluation scenario of EmoBench-M.",
      "page": 26
    },
    {
      "caption": "Figure 31: Confusion matrices for VideoLLaMA2.1-7B-16F on each evaluation scenario of EmoBench-M.",
      "page": 27
    },
    {
      "caption": "Figure 32: Confusion matrices for VideoLLaMA2.1-7B-AV on each evaluation scenario of EmoBench-M.",
      "page": 27
    }
  ],
  "tables": [
    {
      "caption": "Table 7: ). In FER, scores",
      "data": [
        {
          "0.02\n0.07\n0.02\n0.20\n0.66\n0.03\n0.08\n0.76\n0.12\n0.07\n0.53\n0.27\n0.04\n0.35\n0.15\n0.02\n0.28\n0.02\n0.00\n0.33\n0.07\n0.03\n0.54\n0.02": "",
          "0.00\n0.04\n0.01\n0.00\n0.05\n0.00\n0.00\n0.00\n0.00\n0.03\n0.07\n0.00": "0.07\n0.11\n0.06\n0.00\n0.41\n0.00\n0.00\n0.16\n0.11\n0.02\n0.04\n0.05",
          "0.02\n0.07\n0.00\n0.03": "0.13\n0.17\n0.22\n0.16"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.87\n0.03\n0.02\n0.01\n0.00\n0.35\n0.30\n0.00\n0.00\n0.16\n0.72\n0.00\n0.00\n0.19\n0.22\n0.33\n0.08\n0.13\n0.11\n0.09": "0.19\n0.06\n0.06\n0.06",
          "0.00\n0.06\n0.00\n0.03\n0.15\n0.00\n0.00\n0.00\n0.00\n0.04\n0.07\n0.00\n0.27\n0.24\n0.04": "0.13\n0.37\n0.00",
          "0.02\n0.17\n0.12\n0.15\n0.05": "0.15"
        },
        {
          "0.87\n0.03\n0.02\n0.01\n0.00\n0.35\n0.30\n0.00\n0.00\n0.16\n0.72\n0.00\n0.00\n0.19\n0.22\n0.33\n0.08\n0.13\n0.11\n0.09": "0.14\n0.09\n0.07\n0.05",
          "0.00\n0.06\n0.00\n0.03\n0.15\n0.00\n0.00\n0.00\n0.00\n0.04\n0.07\n0.00\n0.27\n0.24\n0.04": "0.12\n0.16\n0.09",
          "0.02\n0.17\n0.12\n0.15\n0.05": "0.28\n0.29"
        },
        {
          "0.87\n0.03\n0.02\n0.01\n0.00\n0.35\n0.30\n0.00\n0.00\n0.16\n0.72\n0.00\n0.00\n0.19\n0.22\n0.33\n0.08\n0.13\n0.11\n0.09": "0.17\n0.04\n0.17\n0.02",
          "0.00\n0.06\n0.00\n0.03\n0.15\n0.00\n0.00\n0.00\n0.00\n0.04\n0.07\n0.00\n0.27\n0.24\n0.04": "0.14\n0.13\n0.02",
          "0.02\n0.17\n0.12\n0.15\n0.05": ""
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.03\n0.05\n0.04\n0.30\n0.57\n0.02\n0.12\n0.76\n0.12\n0.03\n0.55\n0.28\n0.05\n0.36\n0.14\n0.04\n0.38\n0.11\n0.00\n0.36\n0.11\n0.01\n0.55\n0.03": "",
          "0.00\n0.02\n0.00\n0.03": "0.12\n0.00\n0.00\n0.05",
          "0.04\n0.05\n0.00\n0.10": "0.11\n0.18\n0.13\n0.02",
          "0.01\n0.02\n0.00\n0.00": "0.04\n0.05\n0.13\n0.06",
          "0.04\n0.03\n0.00\n0.00": "0.08\n0.11\n0.11\n0.14"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.02\n0.01\n0.00": "0.28\n0.35\n0.00\n0.21\n0.68\n0.00\n0.20\n0.32\n0.04\n0.05\n0.20\n0.01\n0.06\n0.06\n0.02\n0.00\n0.14\n0.00\n0.01\n0.32\n0.01",
          "0.01\n0.06\n0.01": "0.00\n0.11\n0.00\n0.00\n0.00\n0.05\n0.04\n0.20\n0.00\n0.16\n0.29\n0.00\n0.07\n0.41\n0.00\n0.07\n0.20\n0.16\n0.05\n0.18\n0.03",
          "0.00": "0.02\n0.00\n0.00\n0.00\n0.02\n0.02\n0.00"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00\n0.01\n0.00": "0.10\n0.39\n0.02\n0.00\n0.83\n0.00\n0.03\n0.47\n0.13\n0.01\n0.16\n0.08",
          "0.00": "0.00\n0.00\n0.00\n0.00",
          "0.01": "0.08\n0.00\n0.10\n0.15"
        },
        {
          "0.00\n0.01\n0.00": "0.02\n0.14\n0.06",
          "0.00": "0.00",
          "0.01": "0.16"
        },
        {
          "0.00\n0.01\n0.00": "0.00\n0.23\n0.03",
          "0.00": "0.00",
          "0.01": "0.10"
        },
        {
          "0.00\n0.01\n0.00": "0.01\n0.18\n0.01",
          "0.00": "0.00",
          "0.01": "0.07"
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00\n0.04\n0.04\n0.01\n0.02": "0.07\n0.32\n0.14\n0.04\n0.05\n0.00\n0.65\n0.09\n0.04\n0.04\n0.00\n0.24\n0.48\n0.00\n0.10\n0.00\n0.14\n0.14\n0.21\n0.07",
          "0.00": "0.00\n0.00\n0.00\n0.00",
          "0.02": "0.09\n0.04\n0.05\n0.09"
        },
        {
          "0.00\n0.04\n0.04\n0.01\n0.02": "0.02\n0.17\n0.05\n0.05\n0.22\n0.00\n0.13\n0.10\n0.19\n0.23",
          "0.00": "0.00\n0.00",
          "0.02": "0.02\n0.03"
        },
        {
          "0.00\n0.04\n0.04\n0.01\n0.02": "0.03\n0.31\n0.04\n0.11\n0.10",
          "0.00": "0.03",
          "0.02": "0.07"
        }
      ],
      "page": 24
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.03": "0.00",
          "0.00": "0.00"
        },
        {
          "0.03": "0.11",
          "0.00": "0.01"
        },
        {
          "0.03": "0.00\n0.06",
          "0.00": "0.00\n0.00"
        },
        {
          "0.03": "0.00",
          "0.00": "0.02"
        },
        {
          "0.03": "0.04",
          "0.00": "0.00"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.00\n0.05\n0.00": "0.00\n0.20\n0.02\n0.00\n0.60\n0.00\n0.00\n0.39\n0.04\n0.00\n0.10\n0.04",
          "0.00": "0.00\n0.00\n0.04\n0.11",
          "0.05": "0.13\n0.08\n0.21\n0.19\n0.26",
          "0.00\n0.04": "0.00\n0.38\n0.00\n0.28\n0.00\n0.21\n0.01\n0.26\n0.00\n0.26"
        },
        {
          "0.00\n0.05\n0.00": "0.00\n0.07\n0.00",
          "0.00": "0.00",
          "0.05": "",
          "0.00\n0.04": ""
        },
        {
          "0.00\n0.05\n0.00": "0.00\n0.15\n0.02",
          "0.00": "0.05",
          "0.05": "0.12",
          "0.00\n0.04": "0.00\n0.29"
        },
        {
          "0.00\n0.05\n0.00": "0.00\n0.13\n0.00",
          "0.00": "0.02",
          "0.05": "0.06",
          "0.00\n0.04": "0.00\n0.38"
        }
      ],
      "page": 25
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.01": "0.01",
          "0.00": "0.00",
          "0.30\n0.22\n0.29\n0.12\n0.22\n0.28": ""
        },
        {
          "0.01": "0.19\n0.31\n0.19",
          "0.00": "0.01\n0.00\n0.01",
          "0.30\n0.22\n0.29\n0.12\n0.22\n0.28": ""
        },
        {
          "0.01": "0.11",
          "0.00": "0.02",
          "0.30\n0.22\n0.29\n0.12\n0.22\n0.28": ""
        },
        {
          "0.01": "0.10",
          "0.00": "0.01\n0.36",
          "0.30\n0.22\n0.29\n0.12\n0.22\n0.28": ""
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.01\n0.08\n0.00\n0.02\n0.49\n0.00\n0.00\n0.74\n0.00\n0.04\n0.44\n0.04\n0.01\n0.15\n0.00": "0.00\n0.22\n0.00\n0.03\n0.28\n0.00\n0.00\n0.40\n0.00",
          "0.00\n0.03\n0.00\n0.00\n0.07": "0.00\n0.05\n0.01",
          "0.03\n0.07\n0.00\n0.07\n0.04": "0.11\n0.03\n0.00",
          "0.00\n0.00\n0.00\n0.00\n0.00": "0.00\n0.00\n0.00",
          "0.00\n0.03\n0.00\n0.04\n0.08": "0.00\n0.00\n0.05"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.38\n0.03\n0.01\n0.01\n0.00\n0.07\n0.30\n0.00\n0.03\n0.03\n0.09\n0.04\n0.24\n0.10\n0.10": "0.07\n0.19\n0.00\n0.26\n0.10\n0.05\n0.12\n0.07\n0.09\n0.28",
          "0.00\n0.00\n0.01": "0.00\n0.01"
        },
        {
          "0.38\n0.03\n0.01\n0.01\n0.00\n0.07\n0.30\n0.00\n0.03\n0.03\n0.09\n0.04\n0.24\n0.10\n0.10": "0.02\n0.15\n0.04\n0.00\n0.06",
          "0.00\n0.00\n0.01": "0.04"
        },
        {
          "0.38\n0.03\n0.01\n0.01\n0.00\n0.07\n0.30\n0.00\n0.03\n0.03\n0.09\n0.04\n0.24\n0.10\n0.10": "0.06\n0.01\n0.07\n0.03\n0.06",
          "0.00\n0.00\n0.01": "0.01"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.75\n0.01\n0.10\n0.01\n0.00\n0.18\n0.46\n0.00\n0.00\n0.04\n0.92\n0.00\n0.00\n0.00\n0.63\n0.07\n0.07\n0.04\n0.37\n0.01\n0.11\n0.04\n0.24\n0.00": "0.13\n0.02\n0.18\n0.00\n0.10\n0.01\n0.38\n0.00",
          "0.00\n0.00\n0.00\n0.00\n0.04\n0.00": "0.00\n0.01",
          "0.03\n0.03\n0.00\n0.13\n0.12\n0.24": "0.11\n0.00",
          "0.00\n0.11\n0.00\n0.33\n0.00\n0.04\n0.00\n0.17\n0.01\n0.34\n0.00\n0.38": "0.02\n0.53\n0.01\n0.48"
        }
      ],
      "page": 26
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.49\n0.00\n0.16": "",
          "0.03\n0.04\n0.00": "0.03\n0.04\n0.00",
          "0.00": "0.00"
        },
        {
          "0.49\n0.00\n0.16": "0.13\n0.10\n0.05",
          "0.03\n0.04\n0.00": "0.22\n0.11\n0.01\n0.00\n0.36\n0.00\n0.08\n0.25\n0.07",
          "0.00": "0.00\n0.00\n0.00"
        },
        {
          "0.49\n0.00\n0.16": "0.11",
          "0.03\n0.04\n0.00": "0.13\n0.04\n0.02",
          "0.00": "0.00"
        },
        {
          "0.49\n0.00\n0.16": "0.18",
          "0.03\n0.04\n0.00": "0.08\n0.05\n0.01",
          "0.00": "0.00"
        }
      ],
      "page": 27
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "B ( Cheng"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "; Qwen",
        "B-Instruct ( Chu"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "",
      "authors": [
        "B-Av ( Cheng"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "4",
      "title": "",
      "authors": [
        "Longva-Dpo-7b ( Zhang"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "5",
      "title": "",
      "authors": [
        "B ( Wang"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "6",
      "title": "",
      "authors": [
        "Minicpm-V- ; B ( Yao"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "7",
      "title": "",
      "authors": [
        "Emotion-Llama ( Cheng"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "8",
      "title": "",
      "authors": [
        "B ( Cheng"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "OSA: Opinion Sentiment Analysis, EIA: Emotion Intensity Analysis, SCEA: Stock Comment Emotion Analysis. Right: FGDEA: Fine-Grained Dialog Emotion Analysis, PEA: Presentation Emotion Analysis, FCDEA: Face-Centric Dialog Emotion Analysis, CEIA: Conversational Emotion & Intent Analysis, MPDER: Multi-Party Dialog Emotion Recognition. Bold and underlined indicate the best and the second best results among all models, respectively. Method HU SD LR Avg. B-4 R-L BS logic",
      "venue": "Left: SOER: Song Emotion Recognition, SPER: Speech Emotion Recognition"
    },
    {
      "citation_id": "10",
      "title": "LLaMA2-7B-16F (Cheng et al. 2024b)",
      "venue": "LLaMA2-7B-16F (Cheng et al. 2024b)"
    },
    {
      "citation_id": "11",
      "title": "Qwen2-Audio-7B-Instruct",
      "year": "2024",
      "venue": "Qwen2-Audio-7B-Instruct"
    },
    {
      "citation_id": "12",
      "title": "LLaMA2.1-7B-16F (Cheng et al. 2024b)",
      "venue": "LLaMA2.1-7B-16F (Cheng et al. 2024b)"
    },
    {
      "citation_id": "13",
      "title": "LLaMA2.1-7B-AV",
      "venue": "LLaMA2.1-7B-AV"
    },
    {
      "citation_id": "14",
      "title": "Gemini: A Family of Highly Capable Multimodal Models",
      "authors": [
        "R References Anil",
        "S Borgeaud",
        "Y Wu",
        "J Alayrac",
        "J Yu",
        "R Soricut",
        "J Schalkwyk",
        "A Dai"
      ],
      "year": "2023",
      "venue": "Gemini: A Family of Highly Capable Multimodal Models"
    },
    {
      "citation_id": "15",
      "title": "Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis",
      "authors": [
        "S Bai",
        "K Chen",
        "X Liu",
        "J Wang",
        "W Ge",
        "S Song",
        "K Dang",
        "P Wang"
      ],
      "year": "2023",
      "venue": "Leveraging ChatGPT As Text Annotation Tool For Sentiment Analysis"
    },
    {
      "citation_id": "16",
      "title": "BenchLMM: Benchmarking Cross-Style Visual Capability of Large Multimodal Models",
      "authors": [
        "R Cai",
        "Z Song",
        "D Guan",
        "Z Chen",
        "Y Li",
        "X Luo",
        "C Yi",
        "A Kot"
      ],
      "year": "2024",
      "venue": "ECCV 2024"
    },
    {
      "citation_id": "17",
      "title": "Towards Multimodal Sarcasm Detection (An Obviously Perfect Paper)",
      "authors": [
        "S Castro",
        "D Hazarika",
        "V Pérez-Rosas",
        "R Zimmermann",
        "R Mihalcea",
        "S Poria"
      ],
      "year": "2019",
      "venue": "ACL 2019"
    },
    {
      "citation_id": "18",
      "title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "authors": [
        "Z Chen",
        "W Wang",
        "Y Cao",
        "Y Liu",
        "Z Gao",
        "E Cui",
        "J Zhu",
        "S Ye",
        "H Tian",
        "Z Liu"
      ],
      "year": "2024",
      "venue": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling",
      "arxiv": "arXiv:2412.05271"
    },
    {
      "citation_id": "19",
      "title": "2024a. Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning",
      "authors": [
        "Z Cheng",
        "Z Cheng",
        "J He",
        "K Wang",
        "Y Lin",
        "Z Lian",
        "X Peng",
        "A Hauptmann"
      ],
      "year": "2024",
      "venue": "NeurIPS"
    },
    {
      "citation_id": "20",
      "title": "VideoL-LaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs",
      "authors": [
        "Z Cheng",
        "S Leng",
        "H Zhang",
        "Y Xin",
        "X Li",
        "G Chen",
        "Y Zhu",
        "W Zhang",
        "Z Luo",
        "D Zhao",
        "L Bing"
      ],
      "year": "2024",
      "venue": "VideoL-LaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs"
    },
    {
      "citation_id": "21",
      "title": "",
      "authors": [
        "Y Chu",
        "J Xu",
        "Q Yang",
        "H Wei",
        "X Wei",
        "Z Guo"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "22",
      "title": "SOUL: Towards Sentiment and Opinion Understanding of Language",
      "authors": [
        "Y Deng",
        "W Zhang",
        "S Pan",
        "L Bing"
      ],
      "year": "2023",
      "venue": "EMNLP 2023"
    },
    {
      "citation_id": "23",
      "title": "The neural basis of mentalizing",
      "authors": [
        "P Ekman",
        "C Frith",
        "U Frith"
      ],
      "year": "1992",
      "venue": "Neuron"
    },
    {
      "citation_id": "24",
      "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models",
      "authors": [
        "C Fu",
        "P Chen",
        "Y Shen",
        "Y Qin",
        "M Zhang",
        "X Lin",
        "Z Qiu"
      ],
      "year": "2023",
      "venue": "ECCV 2024"
    },
    {
      "citation_id": "25",
      "title": "Emotion regulation: Affective, cognitive, and social consequences",
      "authors": [
        "J Gross"
      ],
      "year": "2002",
      "venue": "Emotion regulation: Affective, cognitive, and social consequences"
    },
    {
      "citation_id": "26",
      "title": "UR-FUNNY: A Multimodal Language Dataset for Understanding Humor",
      "authors": [
        "M Hasan",
        "W Rahman",
        "A Zadeh",
        "J Zhong",
        "M Tanveer",
        "L Morency",
        "M Hoque"
      ],
      "year": "2019",
      "venue": "EMNLP-IJCNLP 2019"
    },
    {
      "citation_id": "27",
      "title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench",
      "authors": [
        "J Huang",
        "M Lam",
        "E Li",
        "S Ren",
        "W Wang",
        "W Jiao",
        "Z Tu",
        "M Lyu"
      ],
      "year": "2023",
      "venue": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench"
    },
    {
      "citation_id": "28",
      "title": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench",
      "authors": [
        "J Huang",
        "M Lam",
        "E Li",
        "S Ren",
        "W Wang",
        "W Jiao",
        "Z Tu",
        "M Lyu"
      ],
      "year": "2023",
      "venue": "Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench"
    },
    {
      "citation_id": "29",
      "title": "SMILE: Multimodal Dataset for Understanding Laughter in Video with Language Models",
      "authors": [
        "L Hyun",
        "K Sung-Bin",
        "S Han",
        "Y Yu",
        "T Oh"
      ],
      "year": "2024",
      "venue": "NAACL 2024"
    },
    {
      "citation_id": "30",
      "title": "SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension",
      "authors": [
        "B Li",
        "Y Ge",
        "Y Chen",
        "Y Ge",
        "R Zhang",
        "Y Shan"
      ],
      "year": "2024",
      "venue": "SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension"
    },
    {
      "citation_id": "31",
      "title": "SciQA: A Multi-Modal Multi-Document Scientific QA Benchmark for Evaluating Foundation Models",
      "authors": [
        "C Li",
        "Z Shangguan",
        "Y Zhao",
        "D Li",
        "Y Liu",
        "A Cohan"
      ],
      "year": "2024",
      "venue": "EMNLP 2024"
    },
    {
      "citation_id": "32",
      "title": "MVBench: A Comprehensive Multi-modal Video Understanding Benchmark",
      "authors": [
        "K Li",
        "Y Wang",
        "Y He",
        "Y Li",
        "Y Wang",
        "Y Liu",
        "Z Wang",
        "J Xu",
        "G Chen",
        "P Lou",
        "L Wang",
        "Y Qiao"
      ],
      "year": "2024",
      "venue": "CVPR 2024"
    },
    {
      "citation_id": "33",
      "title": "2023a. MER 2023: Multi-label Learning, Modality Robustness, and Semi-Supervised Learning",
      "authors": [
        "X Li",
        "Y Zhou",
        "L Zhao",
        "J Li",
        "F Liu",
        "Z Lian",
        "H Sun",
        "L Sun",
        "K Chen",
        "M Xu",
        "K Wang"
      ],
      "year": "2024",
      "venue": "Impromptu Cybercrime Euphemism Detection",
      "arxiv": "arXiv:2412.01413"
    },
    {
      "citation_id": "34",
      "title": "GPT-4V with Emotion: A Zeroshot Benchmark for Multimodal Emotion Understanding",
      "authors": [
        "Z Lian",
        "L Sun",
        "Y Ren",
        "H Gu",
        "H Sun",
        "L Chen",
        "B Liu",
        "J Tao",
        "Z Lian",
        "L Sun",
        "H Sun",
        "K Chen",
        "Z Wen",
        "H Gu",
        "S Chen",
        "B Liu",
        "J Tao"
      ],
      "year": "2023",
      "venue": "GPT-4V with Emotion: A Zeroshot Benchmark for Multimodal Emotion Understanding"
    },
    {
      "citation_id": "35",
      "title": "Rouge: A package for automatic evaluation of summaries",
      "authors": [
        "C.-Y Lin"
      ],
      "year": "2004",
      "venue": "Text summarization branches out"
    },
    {
      "citation_id": "36",
      "title": "Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset",
      "authors": [
        "R Liu",
        "H Zuo",
        "Z Lian",
        "X Xing",
        "B Schuller",
        "H Li"
      ],
      "year": "2024",
      "venue": "Emotion and Intent Joint Understanding in Multimodal Conversation: A Benchmarking Dataset"
    },
    {
      "citation_id": "37",
      "title": "Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup Consistent Module",
      "authors": [
        "Y Liu",
        "Z Yuan",
        "H Mao",
        "Z Liang",
        "W Yang",
        "Y Qiu",
        "T Cheng",
        "X Li",
        "H Xu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "Make Acoustic and Visual Cues Matter: CH-SIMS v2.0 Dataset and AV-Mixup Consistent Module"
    },
    {
      "citation_id": "38",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "39",
      "title": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models",
      "authors": [
        "S Paech",
        "K Papineni",
        "S Roukos",
        "T Ward",
        "W Zhu"
      ],
      "year": "2002",
      "venue": "ACL"
    },
    {
      "citation_id": "40",
      "title": "MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2019",
      "venue": "ACL 2019"
    },
    {
      "citation_id": "41",
      "title": "Emotion recognition in conversation: Research challenges, datasets, and recent advances",
      "authors": [
        "S Poria",
        "N Majumder",
        "R Mihalcea",
        "E Hovy"
      ],
      "year": "2019",
      "venue": "Emotion recognition in conversation: Research challenges, datasets, and recent advances"
    },
    {
      "citation_id": "42",
      "title": "Harnessing the Power of Large Language Models for Empathetic Response Generation: Empirical Investigations and Improvements",
      "authors": [
        "Y Qian",
        "W Zhang",
        "T Liu"
      ],
      "year": "2023",
      "venue": "EMNLP 2023"
    },
    {
      "citation_id": "43",
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "authors": [
        "M Reid",
        "N Savinov",
        "D Teplyashin",
        "D Lepikhin",
        "T Lillicrap"
      ],
      "year": "2024",
      "venue": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context"
    },
    {
      "citation_id": "44",
      "title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models",
      "authors": [
        "S Sabour",
        "S Liu",
        "Z Zhang",
        "J Liu",
        "J Zhou",
        "A Sunaryo",
        "T Lee",
        "R Mihalcea",
        "M Huang"
      ],
      "year": "2024",
      "venue": "ACL 2024"
    },
    {
      "citation_id": "45",
      "title": "Emotional intelligence. Imagination, cognition and personality",
      "authors": [
        "P Salovey",
        "J Mayer"
      ],
      "year": "1990",
      "venue": "Emotional intelligence. Imagination, cognition and personality"
    },
    {
      "citation_id": "46",
      "title": "Neural Scaling Laws for Embodied AI",
      "authors": [
        "S Sartor",
        "N Thompson"
      ],
      "year": "2024",
      "venue": "Neural Scaling Laws for Embodied AI"
    },
    {
      "citation_id": "47",
      "title": "FMSA-SC: A Fine-Grained Multimodal Sentiment Analysis Dataset Based on Stock Comment Videos",
      "authors": [
        "L Song",
        "S Chen",
        "Z Meng",
        "M Sun",
        "X Shang"
      ],
      "year": "2024",
      "venue": "IEEE Trans. Multim"
    },
    {
      "citation_id": "48",
      "title": "2024b. MOSABench: Multi-Object Sentiment Analysis Benchmark for Evaluating Multimodal Large Language Models Understanding of Complex Image",
      "authors": [
        "S Song",
        "C He",
        "S Li",
        "S Zhao",
        "C Wang",
        "T Yan",
        "X Li",
        "Q Wan",
        "J Ma",
        "J Yu",
        "X Mao"
      ],
      "venue": "2024b. MOSABench: Multi-Object Sentiment Analysis Benchmark for Evaluating Multimodal Large Language Models Understanding of Complex Image"
    },
    {
      "citation_id": "49",
      "title": "Scaling Foundation Models for Multimodal Video Understanding",
      "authors": [
        "L Team",
        "A Modi",
        "A Veerubhotla",
        "A Rysbek",
        "A Huber",
        "B Wiltshire",
        "B Veprek",
        "D Gillick",
        "D Kasenberg",
        "D Ahmed"
      ],
      "year": "2024",
      "venue": "LearnLM: Improving Gemini for Learning",
      "arxiv": "arXiv:2412.16429"
    },
    {
      "citation_id": "50",
      "title": "",
      "authors": [
        "A Yang",
        "B Yang",
        "B Zhang",
        "B Hui",
        "B Zheng",
        "B Yu",
        "C Li",
        "D Liu",
        "F Huang",
        "H Wei"
      ],
      "venue": "",
      "arxiv": "arXiv:2412.15115"
    },
    {
      "citation_id": "51",
      "title": "MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language Models on Multimodal Reasoning Tasks",
      "authors": [
        "X Yang",
        "W Wu",
        "S Feng",
        "M Wang",
        "D Wang",
        "Y Li",
        "Q Sun",
        "Y Zhang",
        "X Fu",
        "S Poria"
      ],
      "year": "2024",
      "venue": "MM-InstructEval: Zero-Shot Evaluation of (Multimodal) Large Language Models on Multimodal Reasoning Tasks"
    },
    {
      "citation_id": "52",
      "title": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone",
      "authors": [
        "Y Yao",
        "T Yu",
        "A Zhang",
        "C Wang",
        "J Cui",
        "H Zhu",
        "T Cai"
      ],
      "year": "2024",
      "venue": "MiniCPM-V: A GPT-4V Level MLLM on Your Phone"
    },
    {
      "citation_id": "53",
      "title": "MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI",
      "authors": [
        "K Ying",
        "F Meng",
        "J Wang",
        "Z Li",
        "H Lin",
        "Y Yang",
        "H Zhang"
      ],
      "year": "2024",
      "venue": "ICML 2024"
    },
    {
      "citation_id": "54",
      "title": "CH-SIMS: A Chinese Multimodal Sentiment Analysis Dataset with Fine-grained Annotation of Modality",
      "authors": [
        "W Yu",
        "H Xu",
        "F Meng",
        "Y Zhu",
        "Y Ma",
        "J Wu",
        "J Zou",
        "K Yang"
      ],
      "year": "2020",
      "venue": "ACL 2020"
    },
    {
      "citation_id": "55",
      "title": "Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L Morency"
      ],
      "year": "2018",
      "venue": "ACL 2018"
    },
    {
      "citation_id": "56",
      "title": "MOSI: Multimodal Corpus of Sentiment Intensity and Subjectivity Analysis in Online Opinion Videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L Morency",
        "A Zeng",
        "B Xu",
        "B Wang",
        "C Zhang",
        "D Yin"
      ],
      "year": "2016",
      "venue": "ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools"
    },
    {
      "citation_id": "57",
      "title": "2024a. Long Context Transfer from Language to Vision",
      "authors": [
        "P Zhang",
        "K Zhang",
        "B Li",
        "G Zeng",
        "J Yang"
      ],
      "year": "2020",
      "venue": "2024a. Long Context Transfer from Language to Vision"
    },
    {
      "citation_id": "58",
      "title": "Multitrust: A comprehensive benchmark towards trustworthy multimodal large language models",
      "authors": [
        "Y Zhang",
        "Y Huang",
        "Y Sun",
        "C Liu",
        "Z Zhao",
        "Z Fang",
        "Y Wang",
        "H Chen",
        "X Yang",
        "X Wei"
      ],
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "59",
      "title": "Both Matter: Enhancing the Emotional Intelligence of Large Language Models without Compromising the General Intelligence",
      "authors": [
        "Y Zhang",
        "C Zou",
        "Z Lian",
        "P Tiwari",
        "J Qin",
        "W Zhao",
        "Z Li",
        "S Wang",
        "Y Wang",
        "Y Hu",
        "Y Zhao",
        "C Wei",
        "B Qin",
        "M Zhou",
        "H Liang",
        "T Li",
        "Z Wu",
        "M Lin",
        "L Sun",
        "Y Zhou",
        "Y Zhang",
        "X Huang",
        "Y Chen",
        "Y Qiao",
        "W Chen",
        "B Cui",
        "Zhang"
      ],
      "year": "2024",
      "venue": "Towards Evaluating Large Language Models on Sarcasm Understanding. CoRR, abs/2408.11319"
    },
    {
      "citation_id": "60",
      "title": "HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data",
      "authors": [
        "T Zhou",
        "D Chen",
        "Q Jiao",
        "B Ding",
        "Y Li",
        "Y Shen"
      ],
      "year": "2024",
      "venue": "HumanVBench: Exploring Human-Centric Video Understanding Capabilities of MLLMs with Synthetic Benchmark Data",
      "arxiv": "arXiv:2412.17574"
    },
    {
      "citation_id": "61",
      "title": "2024c. Visual In-Context Learning for Large Vision-Language Models",
      "authors": [
        "Y Zhou",
        "X Li",
        "Q Wang",
        "J Shen"
      ],
      "venue": "ACL 2024"
    },
    {
      "citation_id": "62",
      "title": "Rethinking Visual Dependency in Long-Context Reasoning for Large Vision-Language Models",
      "authors": [
        "Y Zhou",
        "Z Rao",
        "J Wan",
        "J Shen"
      ],
      "year": "2024",
      "venue": "Rethinking Visual Dependency in Long-Context Reasoning for Large Vision-Language Models"
    },
    {
      "citation_id": "63",
      "title": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models",
      "authors": [
        "D Zhu",
        "J Chen",
        "X Shen",
        "X Li",
        "M Elhoseiny"
      ],
      "year": "2024",
      "venue": "MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"
    },
    {
      "citation_id": "64",
      "title": "",
      "authors": [
        "F ( Cheng"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "65",
      "title": "",
      "authors": [
        "Minicpm-V- ; B ( Yao"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "66",
      "title": "LLaMA2-72B (Cheng et al. 2024b)",
      "venue": "LLaMA2-72B (Cheng et al. 2024b)"
    },
    {
      "citation_id": "67",
      "title": "SOER: Song Emotion Recognition, SPER: Speech Emotion Recognition, OSA: Opinion Sentiment Analysis, EIA: Emotion Intensity Analysis, SCEA: Stock Comment Emotion Analysis. Bold and underlined indicate the best and the second best results among all models, respectively. Method FGDEA PEA FCDEA CEIA MPDER Avg",
      "venue": "ACC WAF ACC WAF ACC WAF ACC WAF ACC WAF ACC WAF Open-Source Model"
    },
    {
      "citation_id": "68",
      "title": "",
      "authors": [
        "F ( Cheng"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "69",
      "title": "",
      "authors": [
        "F ( Cheng"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "70",
      "title": "",
      "authors": [
        "B-Av ( Cheng"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "71",
      "title": "",
      "authors": [
        "Minicpm-V- ; B ( Yao"
      ],
      "year": "2024",
      "venue": ""
    },
    {
      "citation_id": "72",
      "title": "LLaMA2-72B (Cheng et al. 2024b)",
      "venue": "LLaMA2-72B (Cheng et al. 2024b)"
    },
    {
      "citation_id": "73",
      "title": "FGDEA: Fine-Grained Dialog Emotion Analysis, PEA: Presentation Emotion Analysis, FCDEA: Face-Centric Dialog Emotion Analysis, CEIA: Conversational Emotion & Intent Analysis, MPDER: Multi-Party Dialog Emotion Recognition. Bold and underlined indicate the best and the second best results among all models",
      "venue": "FGDEA: Fine-Grained Dialog Emotion Analysis, PEA: Presentation Emotion Analysis, FCDEA: Face-Centric Dialog Emotion Analysis, CEIA: Conversational Emotion & Intent Analysis, MPDER: Multi-Party Dialog Emotion Recognition. Bold and underlined indicate the best and the second best results among all models"
    }
  ]
}