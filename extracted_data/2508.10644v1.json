{
  "paper_id": "2508.10644v1",
  "title": "Conditional Information Bottleneck For Multimodal Fusion: Overcoming Shortcut Learning In Sarcasm Detection",
  "published": "2025-08-14T13:39:03Z",
  "authors": [
    "Yihua Wang",
    "Qi Jia",
    "Cong Xu",
    "Feiyu Chen",
    "Yuhan Liu",
    "Haotian Zhang",
    "Liang Jin",
    "Lu Liu",
    "Zhichun Wang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multimodal sarcasm detection is a complex task that requires distinguishing subtle complementary signals across modalities while filtering out irrelevant information. Many advanced methods rely on learning shortcuts from datasets rather than extracting intended sarcasm-related features. However, our experiments show that shortcut learning impairs the model's generalization in real-world scenarios. Furthermore, we reveal the weaknesses of current modality fusion strategies for multimodal sarcasm detection through systematic experiments, highlighting the necessity of focusing on effective modality fusion for complex emotion recognition. To address these challenges, we construct MUStARD++ R by removing shortcut signals from MUS-tARD++. Then, a Multimodal Conditional Information Bottleneck (MCIB) model is introduced to enable efficient multimodal fusion for sarcasm detection. Experimental results show that the MCIB achieves the best performance without relying on shortcut learning.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Multimodal sentiment analysis (MSA)  [50]  integrates various modalities to interpret human emotions, sentiments, and opinions. In MSA tasks, multimodal sarcasm detection  [3]  is particularly challenging due to the subtle contrast between surface meaning and underlying intent, which may convey humor, ridicule, or contempt  [24] . See Figure  1 , Sheldon's words express surprise, but his facial expression shows disgust, and his tone is mocking, conveying sarcasm. Despite advances in large language models, their performance remains capped by supervised baselines, falling short of human-level understanding  [22] . Numerous studies have explored multimodal sarcasm detection (MSD) from various perspectives. Many focus on the inherent characteristics of sarcasm or are specifically designed for prior information: such as contrasting emotional polarity  [30, 44] , or incorporate external knowledge  [49] , character profiles  [26] , emotional cues  [14] , and fine-grained visual features  [40] .\n\nAs shown by issues 1, 2, and 3 in Figure  1 , models tend to learn specific shortcuts. Those shortcuts  [15]  often fail to generalize to more diverse conditions, typically in real-world scenarios. We identify three major shortcut issues in multimodal sarcasm detection research with the representative dataset MUS-tARD++  [35] .  (1)  Heuristics in Character Role Labeling: In sarcasm datasets, some characters exhibit a speaking style that favors sarcasm, such as Sheldon in the TV show THE BIG BANG THEORY. Introducing these character labels during training causes the model to form a bias toward sarcasm associated with a specific person. (2) Label Leakage from Canned Laughter: Canned laughter (artificial background laughter) often causes label leakage. A review of MUStARD++ reveals that canned laughter frequently follows sarcastic utterances, but rarely nonsarcastic ones. Thus, the model tends to classify samples with canned laughter as sarcasm.\n\n(3) Shortcuts of Emotional Inconsistency: The dataset provides implicit/explicit emotion labels, and their contrast serves as a strong indicator of sarcasm. Some models are designed to learn implicit/explicit emotions rather than true sarcasm, limiting their ability to detect sarcasm due to over-reliance on inconsistency.\n\nThe key to understanding the intended complex information and genuinely improving performance is addressing the core multimodal fusion problem, rather than designing modules based on shortcut learning. Various fusion methods  [35, 53, 54]  do not result in significant information gains, some even reducing task performance, highlighting their poor generalization and adaptability to sarcasm tasks. Higher modal complementarity leads to poorer robustness when certain modes are absent or misaligned, and noise has a greater impact on modes with high complementarity  [25] . Multimodal sarcasm detection involves heterogeneous information with complementary but also irrelevant or misleading redundancy. Therefore, this underscores the need for an efficient fusion strategy that integrates complementary information while mitigating redundancy.\n\nTo this end, we construct MUStARD++ R by removing canned laughter, character labels, and emotional polarity from the dataset MUStARD++. Meanwhile, we propose a robust Multimodal fusion framework with a Conditional Information Bottleneck for sarcasm detection, named MCIB. The MCIB achieves the state-ofthe-art performance solely through effective multimodal fusion, without relying on additional shortcut cues. The MCIB offers two key functionalities. First, it addresses the limitations of traditional information bottlenecks by enabling the fusion of more than two information sources, overcoming the challenge of processing multiple modalities simultaneously. Second, our multimodal fusion strategy is designed to extract and separate inconsistent yet relevant information from each modality, effectively reducing inter-modal redundancy and preserving critical features for more accurate multimodal sarcasm recognition.\n\nOur main contributions:  (1)  We conduct an in-depth analysis and provide a comprehensive summary of the critical issues in multimodal sarcasm detection, addressing the issue of learning shortcuts and refactoring the benchmark. (2) We rethink multimodal fusion from a novel perspective and propose a flexible and robust multimodal conditional information bottleneck fusion method. (3) We perform extensive experiments to demonstrate the performance of our fusion method, showing that the model significantly improves the efficiency of extracting meaningful information that aids in sarcasm detection across multiple modalities.",
      "page_start": 1,
      "page_end": 4
    },
    {
      "section_name": "Related Work",
      "text": "Multimodal Sarcasm Detection has evolved from text  [16]  and image-text  [45]  to video clips, which better reflect real-life scenarios. Many methods leverage sarcasm traits to aid recognition, such as incongruity  [30] . Song et al.  [38]  propose an utterance-level attention and incongruity learning network to capture incongruity representations in sarcastic expressions. Since sarcasm is closely linked to emotions and sentiments, an adaptive representation learning model  [53]  is proposed based on an expert network to analyze the emotion and sarcasm tasks jointly. Chauhan et al.  [5]  design a multitasking collaborative framework for crosstraining and sharing attention between sarcasm, implicit/explicit emotions, and implicit/explicit sentiments. Additionally, Zhang et al.  [54]  employ a dual-gating network and three separate layers to achieve cross-modal interactive learning. Other research attempts to leverage various external information. Studies such as  [27]  and  [49]  have utilized tools like KnowleNet to implement knowledge fusion networks for MSD. Moreover, works like  [43] ,  [40] , and  [34]  enhance sarcasm recognition by incorporating conversational behavioral cues, fine-grained eye gaze information, or deep visual features acquired through self-conditioning ConvNet, respectively. While  [14, 21]  rely on pre-training with explicit and implicit emotions, the best results from  [26]  similarly through character labeling. Besides, some novel approaches  [29, 41]  suggest that quantum mechanics are well-suited for capturing the complexity and uncertainty in sarcasm through a quantum probability-driven model or quantum fuzzy neural networks.\n\nInformation Bottleneck (IB) based approaches  [37, 1]  have demonstrated their effectiveness in various tasks such as cross-modal clustering  [48]  and representation learning  [51, 9] , and have also been widely applied in multimodal fusion. For example, Zhang et al.  [52]  improve multimodal sentiment analysis performance by applying IB constraints to pairs of modalities. Mai et al.  [31]  ensure that latent modal representations can effectively handle the target task by introducing IB constraints between each modality and the predicted target. Chen et al.  [6]  adopt a similar approach, enhancing multimodal fusion by adding crossattention. Furthermore, Liu et al.  [28]  apply IB during the representation fusion stage to discard irrelevant information from individual modalities. Xiao et al.  [47]  propose a two-layer IB structure that minimizes mutual information between input and latent states while maximizing it between latent and residual states. Moreover, conditional mutual information  [46]  has demonstrated strong generalization performance in various applications, such as feature selection  [13] , modality enhancement  [23] , and multimodal selection  [20] . Gondek et al.  [17]  propose maximizing conditional mutual information to obtain relevant but novel clustering solutions, avoiding redundant information already captured by known structures or categories. Li et al.  [25]  demonstrate through CIB calculations that higher modal complementarity reduces robustness while noise impacts highly complementary modes more severely. Thus, recognizing the potential of CIB  [33]  for extracting relevant information in multimodal scenarios, we design a CIB-based method for multimodal fusion.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Task Analysis And Refactoring",
      "text": "In the MSD task, some shortcuts arise from task-specific traits, while others capture superficial cues or spurious correlations rather than the true intent of sarcasm. These easy-to-learn shortcut signals hinder the model's generalization to broader testing environments.\n\nHeuristics in Character Role Labeling. Capturing character-specific traits can improve sarcasm detection, but methods relying on character association lack generalizability, as such features are often unavailable in real-world settings. We analyzed the proportion of sarcasm and non-sarcasm in the dialogues of different characters in the MUStARD++. The chi-square test is used to examine the relationship between character and sarcasm, with the null hypothesis stating no association. The test statistic of 166.7 and p-value of 3.89 √ó 10 -27 provide strong evidence against the null hypothesis. The actors exhibited a distinct tendency towards either sarcastic or non-sarcastic behavior, suggesting that character traits strongly influence the judgment of sarcasm. The experimental findings  [3, 26]  reveal that the majority of models can enhance sarcasm recognition merely by incorporating character embeddings. However, the performance of such models trained with character embeddings will drop drastically when tested on other sentiment tasks. Thus, it is essential to design a robust model that captures the character's intended features rather than relying on character labels.\n\nLabel Leakage from Canned Laughter. Since most sarcasm datasets are derived from sitcoms, they typically include canned laughter. Canned laughter, a prerecorded sound used to enhance humor or guide audience reactions, is also prevalent in MUStARD++. It often follows ironic or humorous utterances, leading to label leakage. Most studies  [2, 42, 26]  ignore the canned laughter while processing the data. Using the pre-trained speech recognition model SpeechPrompt v2  [4] , we validated on MUStARD++ and MUStARD++ R (with and without canned laughter). Compared to MUStARD++, performance on MUStARD++ R shows a significant drop, with F1-score decreasing from 73.47 to 43.59 (a drop of 29.88) and accuracy from 78.33 to 63.03 (a drop of  15.3) . This suggests that the model learned the features associated with canned laughter, indirectly confirming the label leakage issue.\n\nShortcuts of Emotional Inconsistency. Sarcasm is often a sugar-coated bomb, expressing the opposite of its literal meaning by masking implicit emotions with explicit ones. The statistical results from the MUStARD++ dataset reveal that sarcasm is predominantly associated with different emotions, accounting for 99% of cases, while only 1% is linked to the same emotions. In contrast, non-sarcasm is mainly associated with the same emotions (94.7%), with only 5.3% corresponding to different emotions. This indicates that sarcastic sentences often exhibit discrepancies between explicit and implicit sentiment labels. The Phi coefficient analysis supports this observation, revealing a very strong association between emotional consistency and sarcasm (œï = 0.94). Many studies  [5, 36, 21]  use this pattern to detect sarcasm. But in real-world scenarios, the explicit/implicit emotion labels do not exist. Moreover, inferring explicit/implicit emotion labels may be more challenging than directly detecting sarcasm. These limitations suggest that relying on explicit/implicit emotion labels for MSD is unfeasible.\n\nTo avoid relying on shortcuts and improve the fairness, robustness, trustworthiness, and deployability of the evaluation benchmark, based on the above analysis of shortcut learning in multimodal sarcasm tasks, we reconstructed MUStARD++ R from the publicly available dataset MUStARD++. Each data sample is annotated with sarcasm labels, implicit and explicit emotions, arousal, and valence tags. The first step in modifying the dataset is to remove all shortcut labels, leaving only those related to sarcasm. Next, we eliminated video segments that contain canned laughter. Specifically, we used the timestamp of the first word in each utterance as the start of the video segment and the timestamp of the last word as the endpoint. By removing labels linked to shortcut learning, MUStARD++ R allows for a more The multimodal fusion component employs three parallel conditional information bottleneck structures to filter out irrelevant information and extract relevant information between each pair of modalities. For each pair of modalities, we first minimize the mutual information between the primary modality and the latent state to achieve filtering and compression through the information bottleneck. We then maximize the conditional mutual information among the auxiliary modality, latent state, and prediction target. Finally, the bidirectional optimization within CIB produces an intermediate representation b that encapsulates the essential information required for our prediction target.\n\naccurate evaluation of multimodal fusion models in detecting sarcasm through integrated cross-modal information.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Methodology",
      "text": "This section defines the formal optimization objective for multimodal fusion, rethinks existing fusion methods, and introduces our framework, including the MCIB algorithm and overall model architecture.\n\nNotations. We define the modalities audio, video, and text as {x 0 , x 1 , x 2 }, which are interdependent random variables. The redundant information causing interference among modalities is denoted by R, while the information gain from modality complementarity is represented by C. y denotes the ground truth of the target task. Problem formulation. Our objective is to minimize the loss between the fused multimodal information and the target y. As shown in eq. (  1 ), we aim to reduce redundancy R and maximize the utilization of complementarity C between modalities: min\n\nwhere Œ¥ sets the upper limit for allowable redundancy, and œµ defines the minimum threshold for the required level of complementarity utilization among modalities. Modal Fusion Rethink. We rethink the limited effectiveness of previous fusion methods in light of the complementary and redundant properties of multimodal data. To provide quantitative evidence, we conduct ablation experiments on different modalities (see the appendix). Ablation tests on various combinations of modality reveal limited gains from modality fusion. In some cases, adding modalities reduces performance: audio may decrease overall effectiveness. Figure  3  illustrates the reasons for ineffective modality fusion: the Redundancy region is larger than the Complementarity region, and the overlapping middle region is too small. This suggests that misleading redundant information outweighs the informational gain from the added modality. For effective modality addition, irrelevant information that may mislead predictions should be reduced, while enhancing the benefits of complementary useful information.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Our Framework",
      "text": "To extract complementary information and remove redundancy across modalities, we apply the CIB principle to build a multimodal fusion model (see Fig.  2 ). We first extract detailed modality features, then design a fusion algorithm based on multimodal pairwise strategy. Compact cross-modal representations are optimized to enhance target understanding, and a balanced, robust loss function with Lagrange constraints ensures effective model training.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Mcib Algorithm",
      "text": "To achieve the multimodal fusion objective in eq. (  1 ), we distinguish between the primary modality x p and the auxiliary modality x a , where x p , x a ‚àà {x 0 , x 1 , x 2 }. Only the information in the primary modality x p beneficial to the target is preserved, while the auxiliary modality x a provides complementary information to x p . We aim to encode as much task-related (y) information as possible in the latent state b, where b ‚àà {b 0 , b 1 , b 2 } represents generation through optimization of the conditional information bottleneck. As shown in Figure  4 , the trade-off between compressing redundant information from the primary modality and retaining complementary information provided by the auxiliary modality is optimized by minimizing mutual information while adhering to specific constraints. We reformulate this constrained optimization problem into an unconstrained Lagrangian form eq. (  2 ), known as the conditional information bottleneck:\n\nwhere I(x p ; b) encourages compression of redundancy from x p . I(b; y | x a ) ensures b retains complementary information useful for predicting y given x a . Œª balances the trade-off between compression and retention.\n\nCompress redundancy. This term penalizes the mutual information between the primary modality x p and the latent state b, encouraging b to be a compressed representation of x p . The mutual information between x p and b is: variational distribution q(b | x p ), the upper bound becomes:\n\nWe define the loss term for compression as the KL divergence between q(b | x p ) and r(b). Specifically, in designing the optimizer: choose a standard Gaussian N (0, I) as Prior r(b), while utilizing a transformer architecture to capture detailed and relevant information from x p for simulating q(b | x p ). The encoder models q(b | x p ) as a Gaussian distribution N (¬µ(x p ), œÉ 2 (x p )). The latent variable b is obtained by reparameterizing and sampling from q(b | x p ). The KL divergence is calculated as:\n\neq. (  5 ) encourages b to gather the most essential information from x p , effectively compressing the redundancy in x p .\n\nRetain complementarity. This term encourages b to retain information about y that is complementary to x a . The conditional mutual information is defined as:\n\nDirect computation is intractable. Introducing a variational distribution q(y | b, x a ), we obtain:\n\nIgnoring the constant term log p(y | x a ) (since it does not depend on b), we get the lower bound:\n\nWe define the loss term for retention as the negative expected log-likelihood.\n\nThen the Evidence Lower Bound Objective (ELBO) method is adopted for the variational approximation of q(y | b, x a ). Combining b and x a , we construct a transformer-based neural network estimator to model q(y | b, x a ). The expected log-likelihood is approximated using samples b (l) drawn from q(b | x p ):\n\nwhere L is the number of samples. Detailed derivations of the equations are elaborated in the appendix. This loss encourages the model to reconstruct y from the combined b and x a , maximizing the conditional mutual information I(b; y | x a ). By precisely formulating each step and keeping the derivation concise, we establish a tractable lower bound for I(b; y | x a ) using variational inference and ELBO methods, which effectively integrates the auxiliary modality x a with the latent representation b to enhance predictive performance. The overall loss function is:",
      "page_start": 9,
      "page_end": 11
    },
    {
      "section_name": "Overall Architecture And Optimization",
      "text": "To uncover latent information within each modality, we design a module for finegrained feature extraction. Using GENTLE  [39] , we align audio and segment audio at the word level, which then serves as a reference for aligning fine-grained features in the visual modality. Next is the multimodal feature fusion module MCIB, which integrates complementary information across modalities while filtering out redundancy. Specifically, the MCIB minimizes the mutual information between the primary modality x p and the latent state b, condensing x p and filtering out redundant information. Simultaneously, it maximizes the conditional mutual information between the auxiliary modality x a , the latent state b, and the target y, aiming to incorporate additional relevant information from x a so that b holds useful information for predicting y. The model concatenates the trained latent state b for prediction, which contains the \"redundancy-removed, effective complementarity\" information distilled through the conditional information bottleneck. As shown in Figure  5 , in the context of multimodal learning, we alternately designate each of the three modalities as the primary and auxiliary modalities to train the fusion framework based on the MCIB algorithm jointly. To better control the degree of information compression, we introduce modality-specific hyperparameters Œª 0 , Œª 1 , and Œª 2 to balance the conditional information bottleneck loss. Letting 0, 1, and 2 represent the three modalities, their respective loss functions are defined as eq. (  11 ):\n\nFurthermore, to fully exploit the fused latent state b in the conditional information bottleneck, we introduce a prediction loss L pred from b to y, where Œ± 0 , Œ± 1 , Œ± 2 and Œ≤ are the weighting coefficients. The model is trained with the following final objective:",
      "page_start": 12,
      "page_end": 13
    },
    {
      "section_name": "Experiments",
      "text": "Dataset and Evaluation Metrics. Experiments were conducted on MUStARD++ and MUStARD++ R , respectively. Given the balanced class distribution, we report results using weighted precision, weighted recall, and weighted F1-score metrics. In addition, we tested the MCIB method on multimodal sentiment analysis tasks (CMU-MOSI and CMU-MOSEI in the appendix), achieving highly competitive results, which validate its generalization capability.\n\nImplementation Details. To extract fine-grained features, we utilize several high-performance backbones. Text features are obtained using a pre-trained De-BERTa  [19] , yielding a representation size of d t = 768. For audio features, Mel Frequency Cepstral Coefficients (MFCC) and Mel spectrograms are generated with Librosa  [32] , along with prosodic features from OpenSMILE 3  [12] , resulting in a combined representation of d a = 291. The video features are extracted by processing utterance frames through the pool5 layer of a ResNet-152  [18]  pretrained on ImageNet  [8] , producing a visual representation of size d v = 2048. All experiments were conducted on Nvidia A100 GPUs with 40GB of memory. Multiple trials demonstrate that combinations of random hyperparameters within  [1, 64]  typically result in local optima after 10 searches. Experimental results were averaged over five runs. Full statistics are provided in the appendix. We compare our proposed MCIB with the following baselines on multimodal fusion and sentiment analysis. VyAnG-Net  [34]  enhances sarcasm detection by in-tegrating visual-specific attention mechanisms with text captions, while PredGaze  [40]  improves sarcasm recognition by utilizing fine-grained visual information such as eye gaze. FIES  [14]  proposes a multimodal approach that integrates audio, textual, sentiment, and emotion data to enhance sarcasm detection, while ABCA-IMI  [26]  identifies sarcasm through multiple inconsistency detection mechanisms. Some methods did not release their code; for instance, the VyAnG-Net results were obtained by training on MUStARD and validating on MUStARD++. SIB  [31, 6]  using the IB between each single modality and the target, while DIB  [52]  applies the IB to pairs of modalities, enabling back-optimization. ITHP  [47]  was designed with a two-layer IB guide to the modality information flow. TBJE  [7]  is a cross-attention-based model with high generalization for multimodal fusion.",
      "page_start": 13,
      "page_end": 15
    },
    {
      "section_name": "Comparison With Baseline Models",
      "text": "MUStARD method  [35]  leverages a collaborative gating mechanism for sarcasm and emotion recognition. Moreover, we report baseline results for the pre-trained speech model SpeechPrompt v2  [4] , and leading large language models: OpenAI's latest flagship model GPT-4o  [11] , the newest release from Google DeepMind Gemini 2.5  [10] , under multimodal configurations.\n\nAs shown in Table  1 , our approach achieved the highest F1 scores of 76.85% and 75.64% on the MUStARD++ and MUStARD++ R datasets, respectively, outperforming all baseline methods. The results indicate two aspects: the model's reliance on shortcuts (generalization ability) and its capacity to capture truly useful information (effectiveness of multimodal fusion).\n\nFirst, our method MCIB achieves strong performance without relying on tricks or dataset-specific shortcuts, demonstrating robust generalization across different data conditions. Comparing results on MUStARD++ and MUStARD++ R , we observe that, after shortcut cues (such as character labels, canned laughter, or emotional inconsistency) are removed, the performance of most conventional methods drops to varying degrees. Interestingly, GPT-4o and Gemini 2.5 perform better in the absence of these cues, possibly because character information introduces noise that misleads LLMs. These findings indicate that many methods are sensitive to shortcuts, while MCIB remains robust.\n\nSecond, using MUStARD++ R , models cannot rely on shortcuts and must depend on architecture design and multimodal fusion strategies. The results demonstrate that MCIB neither relies on nor overfits to shortcut cues and achieves the most effective fusion strategy. In real-world scenarios with limited auxiliary information, our approach outperforms other multimodal fusion methods by more effectively integrating information across modalities.\n\nAdditionally, we conducted cross-training experiments between MUStARD++ and MUStARD++ R to further investigate the impact of shortcut learning on mul-timodal sarcasm detection (see Appendix).",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Modality And Module Ablation Studies",
      "text": "This section examines modality fusion in MCIB, the impact of the transformer and fine-grained modules, and results from modality ablation and different modality combinations.",
      "page_start": 16,
      "page_end": 16
    },
    {
      "section_name": "Method",
      "text": "Precision Transformer architecture. Our transformer-based encoder captures richer patterns than the MLP, yielding a 1.32% performance gain despite increased computational cost. Fine-gained module ablation. We compare fine-grained and coarse-grained feature extraction to investigate their impact on multimodal tasks. The results show that aligning and representing the three modalities at the word level with finegrained features enhances sarcasm detection. Modal ablation. We conduct experiments with all three modalities. Among single modalities, text performs best, while vision performs worst. For dual modalities, performance is highest when text is primary and vision is auxiliary, and lowest when video is primary and audio is auxiliary. Varying primary and auxiliary modality pairs. The three modalities yield six possible sequential pairs. To maximize complementary information, we prioritize combinations that include all modalities. This results in two configurations:\n\nva + at + tv and av + vt + ta (where the first modality is primary, and the second is auxiliary). Experimental results indicate that sarcasm detection performs best when visual assists the audio modality, audio assists text, and text assists the visual modality.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Visualizations",
      "text": "V V+T 66.66 12.50 V 74.17 Figure  6  provides an intuitive comparison of the effectiveness of the proposed MCIB method in modality fusion relative to baseline models. The directional trends of the arrows indicate that MCIB optimizes in the correct direction for modality fusion. Observing the overall improvement: misclassifications due to redundant or irrelevant information are significantly reduced across all modality combinations, while complementary information between modalities is effectively utilized. The accuracy of joint predictions from added modalities has also improved. Further examples are provided in the appendix.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Conclusion",
      "text": "This paper first analyzes the shortcut learning problem existing in current multimodal sarcasm detection methods and restructures the task dataset to prevent the learning of shortcut features. Subsequently, we propose a multimodal conditional information bottleneck (MCIB) framework that effectively captures complementary inter-modal information while filtering out irrelevant and misleading redundancies. Extensive experimental results demonstrate that our model uses multi-modal data more effectively, achieving state-of-the-art performance on the multimodal sarcasm detection task. In the future, we aim to refine MCIB into an easily integrable plug-in for various backbone models in multimodal sentiment analysis, thereby boosting their performance.",
      "page_start": 17,
      "page_end": 17
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Multimodal sarcasm analysis often relies on sitcoms like Friends, which",
      "page": 2
    },
    {
      "caption": "Figure 1: , Sheldon‚Äôs words express surprise, but his facial ex-",
      "page": 2
    },
    {
      "caption": "Figure 1: , models tend to learn specific",
      "page": 2
    },
    {
      "caption": "Figure 2: The diagram illustrates the overall architecture of the MCIB model.",
      "page": 7
    },
    {
      "caption": "Figure 3: The diagram illustrates the optimization direction for multimodal fusion.",
      "page": 8
    },
    {
      "caption": "Figure 4: , the trade-off",
      "page": 9
    },
    {
      "caption": "Figure 4: The optimization process of the multimodal conditional information bot-",
      "page": 10
    },
    {
      "caption": "Figure 5: By constructing three latent state b0, b1 and b2, pertinent information",
      "page": 12
    },
    {
      "caption": "Figure 5: , in the context of multimodal learning, we alternately",
      "page": 12
    },
    {
      "caption": "Figure 6: The diagrams illustrate the loss-benefit comparison for adding new",
      "page": 17
    },
    {
      "caption": "Figure 6: provides an intuitive comparison of the effectiveness of the pro-",
      "page": 17
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Yihua Wang1, Qi Jia2, Cong Xu2, Feiyu Chen1, Yuhan Liu2,": "Haotian Zhang1, Liang Jin2, Lu Liu2, Zhichun Wang1"
        },
        {
          "Yihua Wang1, Qi Jia2, Cong Xu2, Feiyu Chen1, Yuhan Liu2,": "1School of Artificial Intelligence, Beijing Normal University, Beijing, China"
        },
        {
          "Yihua Wang1, Qi Jia2, Cong Xu2, Feiyu Chen1, Yuhan Liu2,": "2IEIT SYSTEMS Co., Ltd., Beijing, China"
        },
        {
          "Yihua Wang1, Qi Jia2, Cong Xu2, Feiyu Chen1, Yuhan Liu2,": "wangyihua@mail.bnu.edu.cn, jiaqi01@ieisystem.com,"
        },
        {
          "Yihua Wang1, Qi Jia2, Cong Xu2, Feiyu Chen1, Yuhan Liu2,": "xucong@ieisystem.com, chenfeiyuchina@mail.bnu.edu.cn,"
        },
        {
          "Yihua Wang1, Qi Jia2, Cong Xu2, Feiyu Chen1, Yuhan Liu2,": "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "Abstract"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "Multimodal\nsarcasm detection is a complex task that\nrequires distin-"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "guishing subtle complementary signals across modalities while filtering out"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "irrelevant\ninformation. Many advanced methods rely on learning shortcuts"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "from datasets rather than extracting intended sarcasm-related features. How-"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "ever, our experiments show that shortcut learning impairs the model‚Äôs gener-"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "alization in real-world scenarios. Furthermore, we reveal the weaknesses of"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "current modality fusion strategies for multimodal sarcasm detection through"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "systematic experiments, highlighting the necessity of focusing on effective"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "modality fusion for complex emotion recognition. To address these chal-"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "lenges, we construct MUStARD++R by removing shortcut signals from MUS-"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "tARD++. Then, a Multimodal Conditional Information Bottleneck (MCIB)"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "model\nis introduced to enable efficient multimodal\nfusion for sarcasm de-"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "tection. Experimental results show that\nthe MCIB achieves the best perfor-"
        },
        {
          "zxyuhan@mail.bnu.edu.cn, zhanghaotian@mail.bnu.edu.cn": "mance without relying on shortcut learning."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Label Leakage from \nHeuristics in Character": "Canned Laughter\nRole Labeling"
        },
        {
          "Label Leakage from \nHeuristics in Character": "‚ë°\n‚ë†"
        },
        {
          "Label Leakage from \nHeuristics in Character": "SHELDON: ‚ÄúWow! Well, marriage must"
        },
        {
          "Label Leakage from \nHeuristics in Character": "Text"
        },
        {
          "Label Leakage from \nHeuristics in Character": "agree with you. Well, you are just glowing.‚Äù"
        },
        {
          "Label Leakage from \nHeuristics in Character": "Modal"
        },
        {
          "Label Leakage from \nHeuristics in Character": "üòäüòäüòä\nAudio\nSarcasm"
        },
        {
          "Label Leakage from \nHeuristics in Character": "Fusion\nAudience: Laugh‚Ä¶"
        },
        {
          "Label Leakage from \nHeuristics in Character": "Video"
        },
        {
          "Label Leakage from \nHeuristics in Character": "‚ë¢\nShortcuts of Emotional"
        },
        {
          "Label Leakage from \nHeuristics in Character": "Explicit Emotion: \nImplicit Emotion:"
        },
        {
          "Label Leakage from \nHeuristics in Character": "Surprise\nDisgust\nInconsistency"
        },
        {
          "Label Leakage from \nHeuristics in Character": "Figure 1: Multimodal sarcasm analysis often relies on sitcoms like Friends, which"
        },
        {
          "Label Leakage from \nHeuristics in Character": "primarily feature character dialogues. The illustration presents the multimodal sar-"
        },
        {
          "Label Leakage from \nHeuristics in Character": "casm detection task and highlights several shortcut learning issues."
        },
        {
          "Label Leakage from \nHeuristics in Character": "1\nIntroduction"
        },
        {
          "Label Leakage from \nHeuristics in Character": "Multimodal sentiment analysis (MSA) [50] integrates various modalities to inter-"
        },
        {
          "Label Leakage from \nHeuristics in Character": "pret human emotions, sentiments, and opinions. In MSA tasks, multimodal sar-"
        },
        {
          "Label Leakage from \nHeuristics in Character": "casm detection [3] is particularly challenging due to the subtle contrast between"
        },
        {
          "Label Leakage from \nHeuristics in Character": "surface meaning and underlying intent, which may convey humor,\nridicule, or"
        },
        {
          "Label Leakage from \nHeuristics in Character": "contempt [24]. See Figure 1, Sheldon‚Äôs words express surprise, but his facial ex-"
        },
        {
          "Label Leakage from \nHeuristics in Character": "pression shows disgust, and his tone is mocking, conveying sarcasm. Despite ad-"
        },
        {
          "Label Leakage from \nHeuristics in Character": "vances in large language models, their performance remains capped by supervised"
        },
        {
          "Label Leakage from \nHeuristics in Character": "baselines, falling short of human-level understanding [22]."
        },
        {
          "Label Leakage from \nHeuristics in Character": "Numerous studies have explored multimodal sarcasm detection (MSD) from"
        },
        {
          "Label Leakage from \nHeuristics in Character": "various perspectives. Many focus on the inherent characteristics of sarcasm or"
        },
        {
          "Label Leakage from \nHeuristics in Character": "are specifically designed for prior information: such as contrasting emotional po-"
        },
        {
          "Label Leakage from \nHeuristics in Character": "larity [30, 44], or\nincorporate external knowledge [49], character profiles [26],"
        },
        {
          "Label Leakage from \nHeuristics in Character": "emotional cues [14], and fine-grained visual features [40]."
        },
        {
          "Label Leakage from \nHeuristics in Character": "As\nshown by issues 1, 2, and 3 in Figure 1, models\ntend to learn specific"
        },
        {
          "Label Leakage from \nHeuristics in Character": "2"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "tions,\ntypically in real-world scenarios. We identify three major shortcut\nissues"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "in multimodal sarcasm detection research with the representative dataset MUS-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "tARD++ [35].\n(1) Heuristics\nin Character Role Labeling:\nIn sarcasm datasets,"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "some characters exhibit a speaking style that favors sarcasm, such as Sheldon in"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "the TV show THE BIG BANG THEORY. Introducing these character labels dur-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "ing training causes the model to form a bias toward sarcasm associated with a spe-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "cific person. (2) Label Leakage from Canned Laughter: Canned laughter (artificial"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "background laughter) often causes label\nleakage. A review of MUStARD++ re-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "veals that canned laughter frequently follows sarcastic utterances, but rarely non-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "sarcastic ones. Thus, the model tends to classify samples with canned laughter as"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "sarcasm. (3) Shortcuts of Emotional Inconsistency: The dataset provides implicit/-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "explicit emotion labels, and their contrast serves as a strong indicator of sarcasm."
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "Some models are designed to learn implicit/explicit emotions rather than true sar-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "casm, limiting their ability to detect sarcasm due to over-reliance on inconsistency."
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "The key to understanding the intended complex information and genuinely"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "improving performance is addressing the core multimodal fusion problem, rather"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "than designing modules based on shortcut\nlearning. Various fusion methods [35,"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "53, 54] do not result in significant information gains, some even reducing task per-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "formance, highlighting their poor generalization and adaptability to sarcasm tasks."
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "Higher modal complementarity leads to poorer robustness when certain modes are"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "absent or misaligned, and noise has a greater impact on modes with high comple-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "mentarity [25]. Multimodal sarcasm detection involves heterogeneous informa-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "tion with complementary but also irrelevant or misleading redundancy. Therefore,"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "this underscores the need for an efficient fusion strategy that\nintegrates comple-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "mentary information while mitigating redundancy."
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "To this end, we construct MUStARD++R by removing canned laughter, char-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "acter labels, and emotional polarity from the dataset MUStARD++. Meanwhile,"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "we propose a robust Multimodal fusion framework with a Conditional Information"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "Bottleneck for sarcasm detection, named MCIB. The MCIB achieves the state-of-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "the-art performance solely through effective multimodal fusion, without relying"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "on additional\nshortcut cues. The MCIB offers\ntwo key functionalities. First,\nit"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "addresses the limitations of\ntraditional\ninformation bottlenecks by enabling the"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "fusion of more than two information sources, overcoming the challenge of pro-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "cessing multiple modalities simultaneously. Second, our multimodal fusion strat-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "egy is designed to extract and separate inconsistent yet relevant information from"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "each modality, effectively reducing inter-modal redundancy and preserving criti-"
        },
        {
          "shortcuts. Those shortcuts\n[15] often fail\nto generalize to more diverse condi-": "cal features for more accurate multimodal sarcasm recognition."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Our main contributions:\n(1) We conduct an in-depth analysis and provide a": "comprehensive summary of\nthe critical\nissues in multimodal sarcasm detection,"
        },
        {
          "Our main contributions:\n(1) We conduct an in-depth analysis and provide a": "addressing the issue of learning shortcuts and refactoring the benchmark. (2) We"
        },
        {
          "Our main contributions:\n(1) We conduct an in-depth analysis and provide a": "rethink multimodal\nfusion from a novel perspective and propose a flexible and"
        },
        {
          "Our main contributions:\n(1) We conduct an in-depth analysis and provide a": "robust multimodal conditional information bottleneck fusion method. (3) We per-"
        },
        {
          "Our main contributions:\n(1) We conduct an in-depth analysis and provide a": "form extensive experiments to demonstrate the performance of our fusion method,"
        },
        {
          "Our main contributions:\n(1) We conduct an in-depth analysis and provide a": "showing that\nthe model significantly improves the efficiency of extracting mean-"
        },
        {
          "Our main contributions:\n(1) We conduct an in-depth analysis and provide a": "ingful information that aids in sarcasm detection across multiple modalities."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "introducing IB constraints between each modality and the predicted target. Chen"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "et al. [6] adopt a similar approach, enhancing multimodal fusion by adding cross-"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "attention. Furthermore, Liu et al. [28] apply IB during the representation fusion"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "stage to discard irrelevant information from individual modalities. Xiao et al. [47]"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "propose a two-layer IB structure that minimizes mutual\ninformation between in-"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "put and latent states while maximizing it between latent and residual states. More-"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "over, conditional mutual information [46] has demonstrated strong generalization"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "performance in various applications, such as feature selection [13], modality en-"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "hancement [23], and multimodal selection [20]. Gondek et al. [17] propose maxi-"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "mizing conditional mutual information to obtain relevant but novel clustering so-"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "lutions, avoiding redundant\ninformation already captured by known structures or"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "categories. Li et al. [25] demonstrate through CIB calculations that higher modal"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "complementarity reduces robustness while noise impacts highly complementary"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "modes more severely. Thus, recognizing the potential of CIB [33] for extracting"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "relevant information in multimodal scenarios, we design a CIB-based method for"
        },
        {
          "ensure that\nlatent modal representations can effectively handle the target\ntask by": "multimodal fusion."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "intended features rather than relying on character labels."
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "Label Leakage from Canned Laughter. Since most sarcasm datasets are de-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "rived from sitcoms, they typically include canned laughter. Canned laughter, a pre-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "recorded sound used to enhance humor or guide audience reactions, is also preva-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "lent\nin MUStARD++. It often follows ironic or humorous utterances,\nleading to"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "label leakage. Most studies [2, 42, 26] ignore the canned laughter while processing"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "the data. Using the pre-trained speech recognition model SpeechPrompt v2[4], we"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "validated on MUStARD++ and MUStARD++R (with and without canned laugh-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "ter). Compared to MUStARD++, performance on MUStARD++R shows a sig-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "nificant drop, with F1-score decreasing from 73.47 to 43.59 (a drop of 29.88)"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "and accuracy from 78.33 to 63.03 (a drop of 15.3). This suggests that\nthe model"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "learned the features associated with canned laughter, indirectly confirming the la-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "bel leakage issue."
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "Shortcuts of Emotional Inconsistency. Sarcasm is often a sugar-coated bomb,"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "expressing the opposite of its literal meaning by masking implicit emotions with"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "explicit ones. The statistical results from the MUStARD++ dataset reveal that sar-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "casm is predominantly associated with different emotions, accounting for 99% of"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "cases, while only 1% is linked to the same emotions. In contrast, non-sarcasm is"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "mainly associated with the same emotions (94.7%), with only 5.3% corresponding"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "to different emotions. This indicates that sarcastic sentences often exhibit discrep-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "ancies between explicit and implicit sentiment labels. The Phi coefficient analysis"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "supports this observation, revealing a very strong association between emotional"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "consistency and sarcasm (œï = 0.94). Many studies [5, 36, 21] use this pattern to"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "detect sarcasm. But in real-world scenarios, the explicit/implicit emotion labels do"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "not exist. Moreover, inferring explicit/implicit emotion labels may be more chal-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "lenging than directly detecting sarcasm. These limitations suggest that relying on"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "explicit/implicit emotion labels for MSD is unfeasible."
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "To avoid relying on shortcuts and improve the fairness, robustness,\ntrustwor-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "thiness, and deployability of the evaluation benchmark, based on the above analy-"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "sis of shortcut learning in multimodal sarcasm tasks, we reconstructed MUStARD++R"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "from the publicly available dataset MUStARD++. Each data sample is annotated"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "with sarcasm labels, implicit and explicit emotions, arousal, and valence tags. The"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "first step in modifying the dataset\nis to remove all shortcut\nlabels,\nleaving only"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "those related to sarcasm. Next, we eliminated video segments that contain canned"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "laughter. Specifically, we used the timestamp of the first word in each utterance as"
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "the start of the video segment and the timestamp of the last word as the endpoint."
        },
        {
          "tasks. Thus,\nit\nis essential\nto design a robust model\nthat captures the character‚Äôs": "By removing labels linked to shortcut learning, MUStARD++R allows for a more"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "ùë¶\nCIB decoder"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "‚äï"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "ùë•!"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "ùë•#"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "Mùëéùë•\tùêº ùëè#, ùë¶\t\n\tùë•!)"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "Figure 2: The diagram illustrates\nthe overall architecture of\nthe MCIB model."
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "The multimodal fusion component employs three parallel conditional information"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "bottleneck structures to filter out\nirrelevant\ninformation and extract\nrelevant\nin-"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "formation between each pair of modalities. For each pair of modalities, we first"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "minimize the mutual\ninformation between the primary modality and the latent"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "state to achieve filtering and compression through the information bottleneck. We"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "then maximize the conditional mutual information among the auxiliary modality,"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "latent state, and prediction target. Finally,\nthe bidirectional optimization within"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "CIB produces an intermediate representation b that encapsulates the essential\nin-"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "formation required for our prediction target."
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "accurate evaluation of multimodal\nfusion models in detecting sarcasm through"
        },
        {
          "IB Encoder\nùúá, ‚àë\nùëè#": "integrated cross-modal information."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "dundancy R and maximize the utilization of complementarity C between modal-"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "ities:"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "min\nLoss (y, f (x0, x1, x2)) ,"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "F usion"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "(1)"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "subject to\nR(x0, x1, x2; y) ‚â§ Œ¥"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "C(x0, x1, x2; y) ‚â• œµ"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "where Œ¥ sets the upper limit for allowable redundancy, and œµ defines the minimum"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "threshold for the required level of complementarity utilization among modalities."
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "Modal Fusion Rethink. We rethink the limited effectiveness of previous fusion"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "methods in light of the complementary and redundant properties of multimodal"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "data. To provide quantitative evidence, we conduct ablation experiments on dif-"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "ferent modalities (see the appendix). Ablation tests on various combinations of"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "modality reveal limited gains from modality fusion. In some cases, adding modal-"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "ities reduces performance: audio may decrease overall effectiveness. Figure 3 il-"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "lustrates the reasons for\nineffective modality fusion:\nthe Redundancy region is"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "larger than the Complementarity region, and the overlapping middle region is too"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "small. This suggests that misleading redundant\ninformation outweighs the infor-"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "mational gain from the added modality. For effective modality addition, irrelevant"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "information that may mislead predictions should be reduced, while enhancing the"
        },
        {
          "multimodal information and the target y. As shown in eq. (1), we aim to reduce re-": "benefits of complementary useful information."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "4.1\nOur Framework": "To extract complementary information and remove redundancy across modali-"
        },
        {
          "4.1\nOur Framework": "ties, we apply the CIB principle to build a multimodal fusion model (see Fig. 2)."
        },
        {
          "4.1\nOur Framework": "We first extract detailed modality features,\nthen design a fusion algorithm based"
        },
        {
          "4.1\nOur Framework": "on multimodal pairwise strategy. Compact cross-modal representations are opti-"
        },
        {
          "4.1\nOur Framework": "mized to enhance target understanding, and a balanced, robust loss function with"
        },
        {
          "4.1\nOur Framework": "Lagrange constraints ensures effective model training."
        },
        {
          "4.1\nOur Framework": "MCIB Algorithm"
        },
        {
          "4.1\nOur Framework": "To achieve the multimodal fusion objective in eq. (1), we distinguish between the"
        },
        {
          "4.1\nOur Framework": "primary modality xp and the auxiliary modality xa, where xp, xa ‚àà {x0, x1, x2}."
        },
        {
          "4.1\nOur Framework": "to the target\nis pre-\nOnly the information in the primary modality xp beneficial"
        },
        {
          "4.1\nOur Framework": "served, while the auxiliary modality xa provides complementary information to"
        },
        {
          "4.1\nOur Framework": "information as possible in the\nxp. We aim to encode as much task-related (y)"
        },
        {
          "4.1\nOur Framework": "latent state b, where b ‚àà {b0, b1, b2} represents generation through optimization"
        },
        {
          "4.1\nOur Framework": "of\nthe conditional\ninformation bottleneck. As shown in Figure 4,\nthe trade-off"
        },
        {
          "4.1\nOur Framework": "between compressing redundant\ninformation from the primary modality and re-"
        },
        {
          "4.1\nOur Framework": "taining complementary information provided by the auxiliary modality is opti-"
        },
        {
          "4.1\nOur Framework": "mized by minimizing mutual\ninformation while adhering to specific constraints."
        },
        {
          "4.1\nOur Framework": "We reformulate this constrained optimization problem into an unconstrained La-"
        },
        {
          "4.1\nOur Framework": "grangian form eq. (2), known as the conditional information bottleneck:"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùë¶": ""
        },
        {
          "ùë¶": ""
        },
        {
          "ùë¶": ""
        },
        {
          "ùë¶": ""
        },
        {
          "ùë¶": ""
        },
        {
          "ùë¶": "latent state, and"
        },
        {
          "ùë¶": ""
        },
        {
          "ùë¶": ""
        },
        {
          "ùë¶": ""
        },
        {
          "ùë¶": ""
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "we obtain:"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "(cid:20)\n(cid:21)"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "p(y | b, xa)"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "log\nI(b;y | xa) = Ep(xa,b,y)"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "p(y | xa)\n(7)"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "‚â• Ep(xa,b,y)[logq(y | b, xa) ‚àí logp(y | xa)]."
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "Ignoring the constant\nterm log p(y | xa) (since it does not depend on b), we get"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "the lower bound:"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "(8)\nI(b; y | xa) ‚â• Ep(xa,b,y) [log q(y | b, xa)] ."
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "We define the loss\nterm for\nretention as\nthe negative expected log-likelihood."
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "Then the Evidence Lower Bound Objective (ELBO) method is adopted for\nthe"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "|\nvariational approximation of q(y\nb, xa). Combining b and xa, we construct a"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "transformer-based neural network estimator to model q(y | b, xa). The expected"
        },
        {
          "Direct computation is intractable. Introducing a variational distribution q(y | b, xa),": "log-likelihood is approximated using samples b(l) drawn from q(b | xp):"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ùë¶": "ùë•!\nùëè!\nùë•#"
        },
        {
          "ùë¶": "information\nFigure 5: By constructing three latent state b0, b1 and b2, pertinent"
        },
        {
          "ùë¶": "is facilitated. Finally,\nintegrated\ntransfer between the three modal x0, x1 and x2"
        },
        {
          "ùë¶": "data leads to the prediction of y."
        },
        {
          "ùë¶": "Overall Architecture and Optimization"
        },
        {
          "ùë¶": "To uncover latent information within each modality, we design a module for fine-"
        },
        {
          "ùë¶": "grained feature extraction. Using GENTLE [39], we align audio and segment au-"
        },
        {
          "ùë¶": "dio at\nthe word level, which then serves as a reference for aligning fine-grained"
        },
        {
          "ùë¶": "features\nin the visual modality. Next\nis\nthe multimodal\nfeature fusion module"
        },
        {
          "ùë¶": "MCIB, which integrates complementary information across modalities while fil-"
        },
        {
          "ùë¶": "tering out redundancy. Specifically, the MCIB minimizes the mutual information"
        },
        {
          "ùë¶": "between the primary modality xp and the latent state b, condensing xp and filtering"
        },
        {
          "ùë¶": "out redundant\ninformation. Simultaneously,\nit maximizes the conditional mutual"
        },
        {
          "ùë¶": "information between the auxiliary modality xa, the latent state b, and the target y,"
        },
        {
          "ùë¶": "aiming to incorporate additional relevant information from xa so that b holds use-"
        },
        {
          "ùë¶": "ful\ninformation for predicting y. The model concatenates the trained latent state"
        },
        {
          "ùë¶": "b for prediction, which contains the ‚Äùredundancy-removed, effective complemen-"
        },
        {
          "ùë¶": "tarity‚Äù information distilled through the conditional information bottleneck."
        },
        {
          "ùë¶": "As shown in Figure 5,\nin the context of multimodal\nlearning, we alternately"
        },
        {
          "ùë¶": "designate each of the three modalities as the primary and auxiliary modalities to"
        },
        {
          "ùë¶": "train the fusion framework based on the MCIB algorithm jointly. To better control"
        },
        {
          "ùë¶": "the degree of information compression, we introduce modality-specific hyperpa-"
        },
        {
          "ùë¶": "to balance the conditional\ninformation bottleneck loss.\nrameters Œª0, Œª1, and Œª2"
        },
        {
          "ùë¶": "Letting 0, 1, and 2 represent\nthe three modalities,\ntheir respective loss functions"
        },
        {
          "ùë¶": "are defined as eq. (11):"
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "L0 = LIB0 + Œª0LCIB2,": "L1 = LIB1 + Œª1LCIB0,"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "L2 = LIB2 + Œª2LCIB1."
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "Furthermore,\nto fully exploit"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "mation bottleneck, we introduce a prediction loss Lpred from b to y, where Œ±0, Œ±1,"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "Œ±2 and Œ≤ are the weighting coefficients. The model"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "final objective: Ltotal = Œ±0L0 + Œ±1L1 + Œ±2L2 + Œ≤Lpred."
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "5\nExperiments"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "Dataset and Evaluation Metrics. Experiments were conducted on MUStARD++"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "and MUStARD++R, respectively. Given the balanced class distribution, we report"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "results using weighted precision, weighted recall, and weighted F1-score metrics."
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "In addition, we tested the MCIB method on multimodal sentiment analysis tasks"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "(CMU-MOSI and CMU-MOSEI in the appendix), achieving highly competitive"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "results, which validate its generalization capability."
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": ""
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "high-performance backbones. Text features are obtained using a pre-trained De-"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "BERTa [19], yielding a representation size of dt = 768. For audio features, Mel"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "Frequency Cepstral Coefficients\n(MFCC) and Mel"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "with Librosa [32], along with prosodic features from OpenSMILE 3 [12], result-"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "ing in a combined representation of da = 291. The video features are extracted"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "by processing utterance frames through the pool5 layer of a ResNet-152 [18] pre-"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "trained on ImageNet\n[8], producing a visual"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "All experiments were conducted on Nvidia A100 GPUs with 40GB of memory."
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "Multiple trials demonstrate that combinations of random hyperparameters within"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "[1, 64] typically result in local optima after 10 searches. Experimental results were"
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "averaged over five runs. Full statistics are provided in the appendix."
        },
        {
          "L0 = LIB0 + Œª0LCIB2,": "13"
        }
      ],
      "page": 13
    },
    {
      "caption": "Table 1: Performance comparison of baseline methods on MUStARD++ and",
      "data": [
        {
          "Comparison with baseline models": "Method"
        },
        {
          "Comparison with baseline models": "PredGaze‚ó¶"
        },
        {
          "Comparison with baseline models": "VyAnG-Net‚ó¶"
        },
        {
          "Comparison with baseline models": "FIES‚ó¶"
        },
        {
          "Comparison with baseline models": "FIES‚ó¶‚ô°"
        },
        {
          "Comparison with baseline models": "ABCA-IMI‚ó¶"
        },
        {
          "Comparison with baseline models": "ABCA-IMI‚ó¶‚ô¢"
        },
        {
          "Comparison with baseline models": "SIB‚ó¶"
        },
        {
          "Comparison with baseline models": "SIB (w/o shortcuts)"
        },
        {
          "Comparison with baseline models": "DIB‚ó¶"
        },
        {
          "Comparison with baseline models": "DIB (w/o shortcuts)"
        },
        {
          "Comparison with baseline models": "ITHP ‚ó¶‚ô¢"
        },
        {
          "Comparison with baseline models": "ITHP (w/o shortcuts)"
        },
        {
          "Comparison with baseline models": "TBJE‚ó¶‚ô°"
        },
        {
          "Comparison with baseline models": "TBJE (w/o shortcuts)"
        },
        {
          "Comparison with baseline models": "MUStARD method‚ó¶"
        },
        {
          "Comparison with baseline models": "MUStARD method‚ó¶‚ô¢"
        },
        {
          "Comparison with baseline models": "MUStARD method (w/o shortcuts)"
        },
        {
          "Comparison with baseline models": "SpeechPrompt v2‚ó¶"
        },
        {
          "Comparison with baseline models": "SpeechPrompt v2 (w/o shortcuts)"
        },
        {
          "Comparison with baseline models": "GPT-4o‚ô¢"
        },
        {
          "Comparison with baseline models": "GPT-4o (w/o shortcuts)"
        },
        {
          "Comparison with baseline models": "Gemini 2.5‚ô¢"
        },
        {
          "Comparison with baseline models": "Gemini 2.5 (w/o shortcuts)"
        },
        {
          "Comparison with baseline models": "MCIB‚ó¶"
        },
        {
          "Comparison with baseline models": "MCIB‚ó¶‚ô¢"
        },
        {
          "Comparison with baseline models": "MCIB (w/o shortcuts)"
        }
      ],
      "page": 14
    },
    {
      "caption": "Table 1: , our approach achieved the highest F1 scores of 76.85%",
      "data": [
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "[40]\nimproves sarcasm recognition by utilizing fine-grained visual\ninformation"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "such as eye gaze. FIES [14] proposes a multimodal approach that integrates audio,"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "textual, sentiment, and emotion data to enhance sarcasm detection, while ABCA-"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "IMI [26] identifies sarcasm through multiple inconsistency detection mechanisms."
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "Some methods did not\nrelease their code;\nfor\ninstance,\nthe VyAnG-Net\nresults"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "were obtained by training on MUStARD and validating on MUStARD++. SIB"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "[31, 6] using the IB between each single modality and the target, while DIB [52]"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "applies the IB to pairs of modalities, enabling back-optimization. ITHP [47] was"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "designed with a two-layer\nIB guide to the modality information flow. TBJE [7]"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "is a cross-attention-based model with high generalization for multimodal fusion."
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "MUStARD method [35] leverages a collaborative gating mechanism for sarcasm"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "and emotion recognition. Moreover, we report baseline results for the pre-trained"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "speech model SpeechPrompt v2 [4], and leading large language models: OpenAI‚Äôs"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "latest flagship model GPT-4o [11],\nthe newest\nrelease from Google DeepMind"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "Gemini 2.5 [10], under multimodal configurations."
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "As shown in Table 1, our approach achieved the highest F1 scores of 76.85%"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "and 75.64% on the MUStARD++ and MUStARD++R datasets, respectively, out-"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "performing all baseline methods. The results indicate two aspects:\nthe model‚Äôs"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "reliance on shortcuts (generalization ability) and its capacity to capture truly use-"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "ful information (effectiveness of multimodal fusion)."
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "First, our method MCIB achieves strong performance without relying on tricks"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "or dataset-specific shortcuts, demonstrating robust generalization across different"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "data conditions. Comparing results on MUStARD++ and MUStARD++R, we ob-"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "serve that, after shortcut cues (such as character labels, canned laughter, or emo-"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "tional inconsistency) are removed, the performance of most conventional methods"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "drops to varying degrees. Interestingly, GPT-4o and Gemini 2.5 perform better in"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "the absence of these cues, possibly because character information introduces noise"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "that misleads LLMs. These findings indicate that many methods are sensitive to"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "shortcuts, while MCIB remains robust."
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "Second, using MUStARD++R, models cannot rely on shortcuts and must de-"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "pend on architecture design and multimodal fusion strategies. The results demon-"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "strate that MCIB neither relies on nor overfits to shortcut cues and achieves the"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "most effective fusion strategy. In real-world scenarios with limited auxiliary in-"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "formation, our approach outperforms other multimodal fusion methods by more"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "effectively integrating information across modalities."
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "Additionally, we conducted cross-training experiments between MUStARD++"
        },
        {
          "tegrating visual-specific attention mechanisms with text captions, while PredGaze": "and MUStARD++R to further investigate the impact of shortcut learning on mul-"
        }
      ],
      "page": 15
    },
    {
      "caption": "Table 2: Ablation results: using an MLP (w/o Transformer), coarse-grained fea-",
      "data": [
        {
          "timodal sarcasm detection (see Appendix).": "5.2"
        },
        {
          "timodal sarcasm detection (see Appendix).": "This section examines modality fusion in MCIB, the impact of the transformer and"
        },
        {
          "timodal sarcasm detection (see Appendix).": "fine-grained modules, and results from modality ablation and different modality"
        },
        {
          "timodal sarcasm detection (see Appendix).": "combinations."
        }
      ],
      "page": 16
    },
    {
      "caption": "Table 2: Ablation results: using an MLP (w/o Transformer), coarse-grained fea-",
      "data": [
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "fine-grained modules, and results from modality ablation and different modality"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "Method"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "w/o Transformer"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "w/o Fine-Gained"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "xv"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "xa"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "xt"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "xva"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "xat"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "xtv"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "xvt + xav + xta"
        },
        {
          "This section examines modality fusion in MCIB, the impact of the transformer and": "xva + xat + xtv"
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "va + at + tv and av + vt + ta (where the first modality is primary, and the sec-": "results indicate that sarcasm detection performs"
        },
        {
          "va + at + tv and av + vt + ta (where the first modality is primary, and the sec-": ""
        },
        {
          "va + at + tv and av + vt + ta (where the first modality is primary, and the sec-": ""
        },
        {
          "va + at + tv and av + vt + ta (where the first modality is primary, and the sec-": ""
        }
      ],
      "page": 17
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "modal data more effectively, achieving state-of-the-art performance on the multi-": "modal sarcasm detection task. In the future, we aim to refine MCIB into an easily"
        },
        {
          "modal data more effectively, achieving state-of-the-art performance on the multi-": "integrable plug-in for various backbone models in multimodal sentiment analysis,"
        },
        {
          "modal data more effectively, achieving state-of-the-art performance on the multi-": "thereby boosting their performance."
        },
        {
          "modal data more effectively, achieving state-of-the-art performance on the multi-": "References"
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "References": "[1] Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep"
        },
        {
          "References": "variational information bottleneck. In International Conference on Learning"
        },
        {
          "References": "Representations, 2022."
        },
        {
          "References": "[2] Siddhant Arora, Hayato\nFutami,\nJee-weon\nJung, Yifan\nPeng, Roshan"
        },
        {
          "References": "Sharma, Yosuke Kashiwagi, Emiru Tsunoo, and Shinji Watanabe.\nUni-"
        },
        {
          "References": "verslu: Universal spoken language understanding for diverse classification"
        },
        {
          "References": "arXiv\npreprint\nand\nsequence\ngeneration\ntasks with\na\nsingle\nnetwork."
        },
        {
          "References": "arXiv:2310.02973, 2023."
        },
        {
          "References": "[3] Santiago Castro, Devamanyu Hazarika, Ver¬¥onica P¬¥erez-Rosas, Roger Zim-"
        },
        {
          "References": "mermann, Rada Mihalcea, and Soujanya Poria. Towards multimodal sarcasm"
        },
        {
          "References": "obviously\nperfect paper).\nIn Proceedings of the 57th Annual\ndetection (an"
        },
        {
          "References": "Meeting of the Association for Computational Linguistics, pages 4619‚Äì4629,"
        },
        {
          "References": "2019."
        },
        {
          "References": "[4] Kai-Wei Chang, Yu-Kai Wang, Hua Shen, Iu-thing Kang, Wei-Cheng Tseng,"
        },
        {
          "References": "Shang-Wen Li, and Hung-yi Lee.\nSpeechprompt v2: Prompt\ntuning for"
        },
        {
          "References": "speech classification tasks. arXiv preprint arXiv:2303.00733, 2023."
        },
        {
          "References": "[5] Dushyant Singh Chauhan, SR Dhanush, Asif Ekbal,\nand Pushpak Bhat-"
        },
        {
          "References": "tacharyya.\nSentiment\nand emotion help sarcasm?\na multi-task learning"
        },
        {
          "References": "framework for multi-modal sarcasm, sentiment and emotion analysis.\nIn"
        },
        {
          "References": "Proceedings of the 58th annual meeting of the association for computational"
        },
        {
          "References": "linguistics, pages 4351‚Äì4360, 2020."
        },
        {
          "References": "[6] Xiangrui Chen, Zhendong Wu, Yu Tang, and Rong Han. Multimodal sen-"
        },
        {
          "References": "timent analysis based on information bottleneck and attention mechanisms."
        },
        {
          "References": "In 2023 2nd International Conference on Cloud Computing, Big Data Ap-"
        },
        {
          "References": "plication and Software Engineering (CBASE), pages 150‚Äì156. IEEE, 2023."
        }
      ],
      "page": 18
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[7]": "",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "Dupont. A transformer-based joint-encoding for emotion recognition and"
        },
        {
          "[7]": "",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "sentiment analysis. ACL 2020, page 1, 2020."
        },
        {
          "[7]": "[8]",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.\nIm-"
        },
        {
          "[7]": "",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "agenet: A large-scale hierarchical image database. In 2009 IEEE conference"
        },
        {
          "[7]": "",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "on computer vision and pattern recognition, pages 248‚Äì255. Ieee, 2009."
        },
        {
          "[7]": "",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "[9] Shifei Ding, Wei Du, Ling Ding, Jian Zhang, Lili Guo, and Bo An. Robust"
        },
        {
          "[7]": "",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "multi-agent communication with graph information bottleneck optimization."
        },
        {
          "[7]": "",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023."
        },
        {
          "[7]": "[10] Google DeepMind et al. Gemini 2.5: Pushing the frontier with advanced",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": ""
        },
        {
          "[7]": "",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "reasoning, multimodality, long context, and next generation agentic capabil-"
        },
        {
          "[7]": "",
          "Jean-Benoit Delbrouck, No¬¥e Tits, Mathilde Brousmiche,\nand St¬¥ephane": "ities, 2025."
        }
      ],
      "page": 19
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "the IEEE conference on\nlearning for image recognition.\nIn Proceedings of"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "computer vision and pattern recognition, pages 770‚Äì778, 2016."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "[19] Pengcheng He, Jianfeng Gao, and Weizhu Chen. Debertav3: Improving de-"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "berta using electra-style pre-training with gradient-disentangled embedding"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "sharing. arXiv preprint arXiv:2111.09543, 2021."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "[20] Yifei He, Runxiang Cheng, Gargi Balasubramaniam, Yao-Hung Hubert Tsai,"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "and Han Zhao. Efficient modality selection in multimodal learning. Journal"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "of Machine Learning Research, 25(47):1‚Äì39, 2024."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "[21] Nivin A Helal, Ahmed Hassan, Nagwa L Badr,\nand Yasmine M Afify."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "A contextual-based approach for\nsarcasm detection.\nScientific Reports,"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "14(1):15415, 2024."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "[22] He Hu, Yucheng Zhou, Lianzhong You, Hongbo Xu, Qianning Wang, Zheng"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "Lian, Fei Richard Yu, Fei Ma, and Laizhong Cui. Emobench-m: Benchmark-"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "ing emotional intelligence for multimodal large language models, 2025."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "[23] Baijun Ji, Tong Zhang, Yicheng Zou, Bojie Hu, and Si Shen.\nIncreasing"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "visual awareness in multimodal neural machine translation from an informa-"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "tion theoretic perspective. arXiv preprint arXiv:2210.08478, 2022."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "[24] Aditya Joshi, Pushpak Bhattacharyya, and Mark J Carman. Automatic sar-"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "casm detection: A survey.\nACM Computing Surveys (CSUR), 50(5):1‚Äì22,"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "2017."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "[25] Siting Li, Chenzhuang Du, Yu Huang, Longbo Huang, and Hang Zhao."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "Modality complementariness: Towards understanding multi-modal\nrobust-"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "ness, 2023."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "[26] Yangyang Li, Yuelin Li, Shihuai Zhang, Guangyuan Liu, Yanqiao Chen,"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "Ronghua Shang, and Licheng Jiao. An attention-based, context-aware mul-"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "timodal fusion method for sarcasm detection using inter-modality inconsis-"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "tency. Knowledge-Based Systems, 287:111457, 2024."
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "[27] Bin Liang, Chenwei Lou, Xiang Li, Min Yang, Lin Gui, Yulan He, Wenjie"
        },
        {
          "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual": "Pei, and Ruifeng Xu. Multi-modal sarcasm detection via cross-modal graph"
        }
      ],
      "page": 20
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "sociation for Computational Linguistics (Volume 1: Long Papers), volume 1,"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "pages 1767‚Äì1777. Association for Computational Linguistics, 2022."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "[28] Wei Liu, Shenchao Cao, and Sun Zhang. Multimodal consistency-specificity"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "Journal of\nfusion based on information bottleneck for sentiment analysis."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "King Saud University-Computer and Information Sciences, 36(2):101943,"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "2024."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "[29] Yaochen Liu, Yazhou Zhang, and Dawei Song. A quantum probability driven"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "framework for joint multi-modal sarcasm, sentiment and emotion analysis."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "IEEE Transactions on Affective Computing, 15(1):326‚Äì341, 2023."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "[30] Qiang Lu, Yunfei Long, Xia Sun, Jun Feng, and Hao Zhang. Fact-sentiment"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "Infor-\nincongruity combination network for multimodal sarcasm detection."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "mation Fusion, 104:102203, 2024."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "[31] Sijie Mai, Ying Zeng, and Haifeng Hu. Multimodal information bottleneck:"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "Learning minimal sufficient unimodal and multimodal representations. IEEE"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "Transactions on Multimedia, 25:4121‚Äì4134, 2022."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "[32] Brian McFee, Matt McVicar, Stefan Balke, Carl Thom¬¥e, Colin Raffel, Oriol"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "Nieto, Eric Battenberg, Daniel P. W. Ellis, Ryuichi Yamamoto, Josh Moore,"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "Rachel M. Bittner, Keunwoo Choi, Fabian-Robert St¬®oter, Siddhartha Kumar,"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "Simon Waloschek, Seth, Rimvydas Naktinis, Douglas Repetto, Curtis Fjord"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "Hawthorne, Cj Carr, hojinlee, Waldir Pimenta, Petr Viktorin, Paul Brossier,"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "JoÀúao Felipe Santos, JackieWu, Erik, and Adrian Holovaty.\nlibrosa/librosa:"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "0.6.0. 2018."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "[33] Sina Molavipour, Germ¬¥an Bassi, and Mikael Skoglund. Neural estimators"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "IEEE\nfor conditional mutual information using nearest neighbors sampling."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "transactions on signal processing, 69:766‚Äì780, 2021."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "[34] Ananya Pandey and Dinesh Kumar Vishwakarma.\nVyang-net: A novel"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "multi-modal sarcasm recognition model by uncovering visual, acoustic and"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "glossary features.\nIntelligent Data Analysis, page 1088467X251315637,"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "2025."
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "[35] Anupama Ray, Shubham Mishra, Apoorva Nunna,\nand Pushpak Bhat-"
        },
        {
          "convolutional network. In Proceedings of the 60th Annual Meeting of the As-": "tacharyya.\nA multimodal corpus for emotion recognition in sarcasm.\nIn"
        }
      ],
      "page": 21
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "ence, pages 6992‚Äì7003, 2022."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "[36] Sapan Shah, Sreedhar Reddy, and Pushpak Bhattacharyya.\nEmotion en-"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "riched retrofitted word embeddings. In Proceedings of the 29th International"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "Conference on Computational Linguistics, pages 4136‚Äì4148, 2022."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "[37] Noam Slonim and Naftali Tishby. Agglomerative information bottleneck."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "Advances in neural information processing systems, 12, 1999."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "[38] Liujing Song, Zefang Zhao, Yuxiang Ma, Yuyang Liu, and Jun Li. Utterance-"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "level\nincongruity learning network for multimodal\nsarcasm detection.\nIn"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "2024 26th International Conference on Advanced Communications Technol-"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "ogy (ICACT), pages 43‚Äì49. IEEE, 2024."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "[39] Hendrik Strobelt. Gentle: Robust yet\nlenient forced aligner built on kaldi."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "https://github.com/strob/gentle. Releases: 2023-05-06."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "[40] Divyank Tiwari, Diptesh Kanojia, Anupama Ray, Apoorva Nunna, and Push-"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "pak Bhattacharyya. Predict and use: Harnessing predicted gaze to improve"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "the 2023 Conference on\nmultimodal sarcasm detection.\nIn Proceedings of"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "Empirical Methods in Natural Language Processing, pages 15933‚Äì15948,"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "2023."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "[41] Prayag Tiwari, Lailei Zhang, Zhiguo Qu, and Ghulam Muhammad. Quan-"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "tum fuzzy neural network for multimodal sentiment and sarcasm detection."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "Information Fusion, 103:102085, 2024."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "[42] Mohit Tomar, Abhisek Tiwari, Tulika Saha, and Sriparna Saha. Your tone"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "speaks louder than your face! modality order infused multi-modal sarcasm"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "the 31st ACM International Conference on\ndetection.\nIn Proceedings of"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "Multimedia, pages 3926‚Äì3933, 2023."
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "[43] Mohit Singh Tomar, Tulika Saha, Abhisek Tiwari, and Sriparna Saha. Action"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "and reaction go hand in hand! a multi-modal dialogue act aided sarcasm"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "the 2024 Joint\nInternational Conference\nidentification.\nIn Proceedings of"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "on Computational Linguistics, Language Resources and Evaluation (LREC-"
        },
        {
          "Proceedings of the Thirteenth Language Resources and Evaluation Confer-": "COLING 2024), pages 298‚Äì309, 2024."
        }
      ],
      "page": 22
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "rui Li. Cross-modal incongruity aligning and collaborating for multi-modal"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "sarcasm detection.\nInformation Fusion, 103:102132, 2024."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "[45] Changsong Wen, Guoli Jia, and Jufeng Yang. Dip: Dual\nincongruity per-"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "the IEEE/CVF\nceiving network for sarcasm detection.\nIn Proceedings of"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "Conference on Computer Vision and Pattern Recognition, pages 2540‚Äì2550,"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "2023."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "[46] Aaron D Wyner. A definition of conditional mutual information for arbitrary"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "ensembles.\nInformation and Control, 38(1):51‚Äì59, 1978."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "[47] Xiongye Xiao, Gengshuo Liu, Gaurav Gupta, Defu Cao, Shixuan Li, Yax-"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "ing Li, Tianqing Fang, Mingxi Cheng, and Paul Bogdan.\nNeuro-inspired"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "information-theoretic hierarchical perception for multimodal learning, 2024."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "[48] Xiaoqiang Yan, Yiqiao Mao, Yangdong Ye, and Hui Yu. Cross-modal clus-"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "IEEE Transac-\ntering with deep correlated information bottleneck method."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "tions on Neural Networks and Learning Systems, 2023."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "[49] Tan Yue, Rui Mao, Heng Wang, Zonghai Hu, and Erik Cambria. Knowlenet:"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "Information\nKnowledge fusion network for multimodal sarcasm detection."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "Fusion, 100:101921, 2023."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "[50] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria,\nand Louis-"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "Philippe Morency. Tensor fusion network for multimodal sentiment analy-"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "sis. In Proceedings of the 2017 Conference on Empirical Methods in Natural"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "Language Processing, pages 1103‚Äì1114, 2017."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "[51] Tonghui Zhang, Changfei Dong, Jinsong Su, Haiying Zhang, and Yuzheng"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "Li.\nUnimodal and multimodal\nintegrated representation learning via im-"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "proved information bottleneck for multimodal sentiment analysis.\nIn CCF"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "International Conference on Natural Language Processing and Chinese"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "Computing, pages 564‚Äì576. Springer, 2022."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "[52] Tonghui Zhang, Haiying Zhang, Shuke Xiang, and Tong Wu.\nInformation"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "bottleneck based representation learning for multimodal sentiment analysis."
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "In Proceedings of the 6th International Conference on Control Engineering"
        },
        {
          "[44]\nJie Wang, Yan Yang, Yongquan Jiang, Minbo Ma, Zhuyang Xie, and Tian-": "and Artificial Intelligence, pages 7‚Äì11, 2022."
        }
      ],
      "page": 23
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[53] Yazhou Zhang, Yang Yu, Mengyao Wang, Min Huang, and M Shamim Hos-": ""
        },
        {
          "[53] Yazhou Zhang, Yang Yu, Mengyao Wang, Min Huang, and M Shamim Hos-": ""
        },
        {
          "[53] Yazhou Zhang, Yang Yu, Mengyao Wang, Min Huang, and M Shamim Hos-": ""
        },
        {
          "[53] Yazhou Zhang, Yang Yu, Mengyao Wang, Min Huang, and M Shamim Hos-": "[54] Yazhou Zhang, Yang Yu, Dongming Zhao, Zuhe Li, Bo Wang, Yuexian Hou,"
        },
        {
          "[53] Yazhou Zhang, Yang Yu, Mengyao Wang, Min Huang, and M Shamim Hos-": ""
        },
        {
          "[53] Yazhou Zhang, Yang Yu, Mengyao Wang, Min Huang, and M Shamim Hos-": ""
        },
        {
          "[53] Yazhou Zhang, Yang Yu, Mengyao Wang, Min Huang, and M Shamim Hos-": "tion."
        }
      ],
      "page": 24
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep variational information bottleneck",
      "authors": [
        "Ian Alexander A Alemi",
        "Joshua Fischer",
        "Kevin Dillon",
        "Murphy"
      ],
      "year": "2022",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "2",
      "title": "Universlu: Universal spoken language understanding for diverse classification and sequence generation tasks with a single network",
      "authors": [
        "Siddhant Arora",
        "Hayato Futami",
        "Jee-Weon Jung",
        "Yifan Peng",
        "Roshan Sharma",
        "Yosuke Kashiwagi",
        "Emiru Tsunoo",
        "Shinji Watanabe"
      ],
      "year": "2023",
      "venue": "Universlu: Universal spoken language understanding for diverse classification and sequence generation tasks with a single network",
      "arxiv": "arXiv:2310.02973"
    },
    {
      "citation_id": "3",
      "title": "Towards multimodal sarcasm detection (an obviously perfect paper)",
      "authors": [
        "Santiago Castro",
        "Devamanyu Hazarika",
        "Ver√≥nica P√©rez-Rosas",
        "Roger Zimmermann",
        "Rada Mihalcea",
        "Soujanya Poria"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "4",
      "title": "Prompt tuning for speech classification tasks",
      "authors": [
        "Kai-Wei Chang",
        "Yu-Kai Wang",
        "Hua Shen",
        "Iu-Thing Kang",
        "Wei-Cheng Tseng",
        "Shang-Wen Li",
        "Hung-Yi Lee"
      ],
      "year": "2023",
      "venue": "Prompt tuning for speech classification tasks",
      "arxiv": "arXiv:2303.00733"
    },
    {
      "citation_id": "5",
      "title": "Asif Ekbal, and Pushpak Bhattacharyya. Sentiment and emotion help sarcasm? a multi-task learning framework for multi-modal sarcasm, sentiment and emotion analysis",
      "authors": [
        "Dushyant Singh Chauhan",
        "S Dhanush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 58th annual meeting of the association for computational linguistics"
    },
    {
      "citation_id": "6",
      "title": "Multimodal sentiment analysis based on information bottleneck and attention mechanisms",
      "authors": [
        "Xiangrui Chen",
        "Zhendong Wu",
        "Yu Tang",
        "Rong Han"
      ],
      "year": "2023",
      "venue": "2023 2nd International Conference on Cloud Computing, Big Data Application and Software Engineering (CBASE)"
    },
    {
      "citation_id": "7",
      "title": "A transformer-based joint-encoding for emotion recognition and sentiment analysis",
      "authors": [
        "Jean-Benoit Delbrouck",
        "No√© Tits",
        "Mathilde Brousmiche",
        "St√©phane Dupont"
      ],
      "year": "2020",
      "venue": "ACL 2020"
    },
    {
      "citation_id": "8",
      "title": "Imagenet: A large-scale hierarchical image database",
      "authors": [
        "Jia Deng",
        "Wei Dong",
        "Richard Socher",
        "Li-Jia Li",
        "Kai Li",
        "Li Fei-Fei"
      ],
      "year": "2009",
      "venue": "2009 IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Robust multi-agent communication with graph information bottleneck optimization",
      "authors": [
        "Shifei Ding",
        "Wei Du",
        "Ling Ding",
        "Jian Zhang",
        "Lili Guo",
        "Bo An"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "10",
      "title": "Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities",
      "authors": [
        "Google Deepmind"
      ],
      "year": "2025",
      "venue": "Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities"
    },
    {
      "citation_id": "11",
      "title": "Gpt-4o system card",
      "authors": [
        "Openai"
      ],
      "year": "2024",
      "venue": "Gpt-4o system card"
    },
    {
      "citation_id": "12",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin W√∂llmer",
        "Bj√∂rn Schuller"
      ],
      "year": "2010",
      "venue": "Proceedings of the 18th ACM international conference on Multimedia"
    },
    {
      "citation_id": "13",
      "title": "Fast binary feature selection with conditional mutual information",
      "year": "2004",
      "venue": "Franc ¬∏ois Fleuret"
    },
    {
      "citation_id": "14",
      "title": "Improving sarcasm detection from speech and text through attention-based fusion exploiting the interplay of emotions and sentiments",
      "authors": [
        "Xiyuan Gao",
        "Shekhar Nayak",
        "Matt Coler"
      ],
      "year": "2024",
      "venue": "Proceedings of Meetings on Acoustics"
    },
    {
      "citation_id": "15",
      "title": "Shortcut learning in deep neural networks",
      "authors": [
        "Robert Geirhos",
        "J√∂rn-Henrik Jacobsen",
        "Claudio Michaelis",
        "Richard Zemel",
        "Wieland Brendel",
        "Matthias Bethge",
        "Felix Wichmann"
      ],
      "year": "2020",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "16",
      "title": "A report on the 2020 sarcasm detection shared task",
      "authors": [
        "Debanjan Ghosh",
        "Avijit Vajpayee",
        "Smaranda Muresan"
      ],
      "year": "2020",
      "venue": "Proceedings of the Second Workshop on Figurative Language Processing"
    },
    {
      "citation_id": "17",
      "title": "Conditional information bottleneck clustering",
      "authors": [
        "David Gondek",
        "Thomas Hofmann"
      ],
      "year": "2003",
      "venue": "3rd ieee international conference on data mining, workshop on clustering large data sets"
    },
    {
      "citation_id": "18",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "19",
      "title": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing",
      "authors": [
        "Pengcheng He",
        "Jianfeng Gao",
        "Weizhu Chen"
      ],
      "year": "2021",
      "venue": "Debertav3: Improving deberta using electra-style pre-training with gradient-disentangled embedding sharing",
      "arxiv": "arXiv:2111.09543"
    },
    {
      "citation_id": "20",
      "title": "Efficient modality selection in multimodal learning",
      "authors": [
        "Yifei He",
        "Runxiang Cheng",
        "Gargi Balasubramaniam",
        "Hubert Yao-Hung",
        "Han Tsai",
        "Zhao"
      ],
      "year": "2024",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "21",
      "title": "A contextual-based approach for sarcasm detection",
      "authors": [
        "A Nivin",
        "Ahmed Helal",
        "Nagwa Hassan",
        "Yasmine Badr",
        "Afify"
      ],
      "year": "2024",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "22",
      "title": "Emobench-m: Benchmarking emotional intelligence for multimodal large language models",
      "authors": [
        "He Hu",
        "Yucheng Zhou",
        "Lianzhong You",
        "Hongbo Xu",
        "Qianning Wang",
        "Zheng Lian",
        "Richard Fei",
        "Fei Yu",
        "Laizhong Ma",
        "Cui"
      ],
      "year": "2025",
      "venue": "Emobench-m: Benchmarking emotional intelligence for multimodal large language models"
    },
    {
      "citation_id": "23",
      "title": "Increasing visual awareness in multimodal neural machine translation from an information theoretic perspective",
      "authors": [
        "Baijun Ji",
        "Tong Zhang",
        "Yicheng Zou",
        "Bojie Hu",
        "Si Shen"
      ],
      "year": "2022",
      "venue": "Increasing visual awareness in multimodal neural machine translation from an information theoretic perspective",
      "arxiv": "arXiv:2210.08478"
    },
    {
      "citation_id": "24",
      "title": "Automatic sarcasm detection: A survey",
      "authors": [
        "Aditya Joshi",
        "Pushpak Bhattacharyya",
        "Mark Carman"
      ],
      "year": "2017",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "25",
      "title": "Modality complementariness: Towards understanding multi-modal robustness",
      "authors": [
        "Siting Li",
        "Chenzhuang Du",
        "Yu Huang",
        "Longbo Huang",
        "Hang Zhao"
      ],
      "year": "2023",
      "venue": "Modality complementariness: Towards understanding multi-modal robustness"
    },
    {
      "citation_id": "26",
      "title": "An attention-based, context-aware multimodal fusion method for sarcasm detection using inter-modality inconsistency",
      "authors": [
        "Yangyang Li",
        "Yuelin Li",
        "Shihuai Zhang",
        "Guangyuan Liu",
        "Yanqiao Chen",
        "Ronghua Shang",
        "Licheng Jiao"
      ],
      "year": "2024",
      "venue": "An attention-based, context-aware multimodal fusion method for sarcasm detection using inter-modality inconsistency"
    },
    {
      "citation_id": "27",
      "title": "Multi-modal sarcasm detection via cross-modal graph convolutional network",
      "authors": [
        "Bin Liang",
        "Chenwei Lou",
        "Xiang Li",
        "Min Yang",
        "Lin Gui",
        "Yulan He",
        "Wenjie Pei",
        "Ruifeng Xu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "28",
      "title": "Multimodal consistency-specificity fusion based on information bottleneck for sentiment analysis",
      "authors": [
        "Wei Liu",
        "Shenchao Cao",
        "Sun Zhang"
      ],
      "year": "2024",
      "venue": "Journal of King Saud University-Computer and Information Sciences"
    },
    {
      "citation_id": "29",
      "title": "A quantum probability driven framework for joint multi-modal sarcasm, sentiment and emotion analysis",
      "authors": [
        "Yaochen Liu",
        "Yazhou Zhang",
        "Dawei Song"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Fact-sentiment incongruity combination network for multimodal sarcasm detection",
      "authors": [
        "Qiang Lu",
        "Yunfei Long",
        "Xia Sun",
        "Jun Feng",
        "Hao Zhang"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "31",
      "title": "Multimodal information bottleneck: Learning minimal sufficient unimodal and multimodal representations",
      "authors": [
        "Sijie Mai",
        "Ying Zeng",
        "Haifeng Hu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "32",
      "title": "",
      "authors": [
        "Brian Mcfee",
        "Matt Mcvicar",
        "Stefan Balke",
        "Carl Thom√©",
        "Colin Raffel",
        "Oriol Nieto",
        "Eric Battenberg",
        "P Daniel",
        "Ryuichi Ellis",
        "Josh Yamamoto",
        "Rachel Moore",
        "Keunwoo Bittner",
        "Fabian-Robert Choi",
        "Siddhartha St√∂ter",
        "Simon Kumar",
        "Seth Waloschek",
        "Rimvydas Naktinis",
        "Douglas Repetto",
        "Curtis Hawthorne",
        "Cj Carr",
        "Waldir Pimenta",
        "Petr Viktorin",
        "Paul Brossier",
        "Felipe Jo√£o",
        "Jackiewu Santos"
      ],
      "venue": ""
    },
    {
      "citation_id": "33",
      "title": "Neural estimators for conditional mutual information using nearest neighbors sampling",
      "authors": [
        "Sina Molavipour",
        "Germ√°n Bassi",
        "Mikael Skoglund"
      ],
      "year": "2021",
      "venue": "IEEE transactions on signal processing"
    },
    {
      "citation_id": "34",
      "title": "Vyang-net: A novel multi-modal sarcasm recognition model by uncovering visual, acoustic and glossary features. Intelligent Data Analysis",
      "authors": [
        "Ananya Pandey",
        "Dinesh Kumar"
      ],
      "year": "2025",
      "venue": "Vyang-net: A novel multi-modal sarcasm recognition model by uncovering visual, acoustic and glossary features. Intelligent Data Analysis"
    },
    {
      "citation_id": "35",
      "title": "Apoorva Nunna, and Pushpak Bhattacharyya. A multimodal corpus for emotion recognition in sarcasm",
      "authors": [
        "Anupama Ray",
        "Shubham Mishra"
      ],
      "year": "2022",
      "venue": "Proceedings of the Thirteenth Language Resources and Evaluation Conference"
    },
    {
      "citation_id": "36",
      "title": "Emotion enriched retrofitted word embeddings",
      "authors": [
        "Sapan Shah",
        "Sreedhar Reddy",
        "Pushpak Bhattacharyya"
      ],
      "year": "2022",
      "venue": "Proceedings of the 29th International Conference on Computational Linguistics"
    },
    {
      "citation_id": "37",
      "title": "Advances in neural information processing systems",
      "authors": [
        "Noam Slonim",
        "Naftali Tishby"
      ],
      "year": "1999",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "38",
      "title": "Utterancelevel incongruity learning network for multimodal sarcasm detection",
      "authors": [
        "Liujing Song",
        "Zefang Zhao",
        "Yuxiang Ma",
        "Yuyang Liu",
        "Jun Li"
      ],
      "year": "2024",
      "venue": "2024 26th International Conference on Advanced Communications Technology (ICACT)"
    },
    {
      "citation_id": "39",
      "title": "Gentle: Robust yet lenient forced aligner built on kaldi",
      "authors": [
        "Hendrik Strobelt"
      ],
      "venue": "Gentle: Robust yet lenient forced aligner built on kaldi"
    },
    {
      "citation_id": "40",
      "title": "Apoorva Nunna, and Pushpak Bhattacharyya. Predict and use: Harnessing predicted gaze to improve multimodal sarcasm detection",
      "authors": [
        "Divyank Tiwari",
        "Diptesh Kanojia",
        "Anupama Ray"
      ],
      "year": "2023",
      "venue": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "41",
      "title": "Quantum fuzzy neural network for multimodal sentiment and sarcasm detection",
      "authors": [
        "Prayag Tiwari",
        "Lailei Zhang",
        "Zhiguo Qu",
        "Ghulam Muhammad"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "42",
      "title": "Your tone speaks louder than your face! modality order infused multi-modal sarcasm detection",
      "authors": [
        "Mohit Tomar",
        "Abhisek Tiwari",
        "Tulika Saha",
        "Sriparna Saha"
      ],
      "year": "2023",
      "venue": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    {
      "citation_id": "43",
      "title": "Action and reaction go hand in hand! a multi-modal dialogue act aided sarcasm identification",
      "authors": [
        "Mohit Singh Tomar",
        "Tulika Saha",
        "Abhisek Tiwari",
        "Sriparna Saha"
      ],
      "year": "2024",
      "venue": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)"
    },
    {
      "citation_id": "44",
      "title": "Cross-modal incongruity aligning and collaborating for multi-modal sarcasm detection",
      "authors": [
        "Jie Wang",
        "Yan Yang",
        "Yongquan Jiang",
        "Minbo Ma",
        "Zhuyang Xie",
        "Tianrui Li"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "45",
      "title": "Dip: Dual incongruity perceiving network for sarcasm detection",
      "authors": [
        "Changsong Wen",
        "Guoli Jia",
        "Jufeng Yang"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "46",
      "title": "A definition of conditional mutual information for arbitrary ensembles",
      "authors": [
        "D Aaron",
        "Wyner"
      ],
      "year": "1978",
      "venue": "Information and Control"
    },
    {
      "citation_id": "47",
      "title": "Neuro-inspired information-theoretic hierarchical perception for multimodal learning",
      "authors": [
        "Xiongye Xiao",
        "Gengshuo Liu",
        "Gaurav Gupta",
        "Defu Cao",
        "Shixuan Li",
        "Yaxing Li",
        "Tianqing Fang",
        "Mingxi Cheng",
        "Paul Bogdan"
      ],
      "year": "2024",
      "venue": "Neuro-inspired information-theoretic hierarchical perception for multimodal learning"
    },
    {
      "citation_id": "48",
      "title": "Cross-modal clustering with deep correlated information bottleneck method",
      "authors": [
        "Xiaoqiang Yan",
        "Yiqiao Mao",
        "Yangdong Ye",
        "Hui Yu"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "49",
      "title": "Knowlenet: Knowledge fusion network for multimodal sarcasm detection",
      "authors": [
        "Tan Yue",
        "Rui Mao",
        "Heng Wang",
        "Zonghai Hu",
        "Erik Cambria"
      ],
      "year": "2023",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "50",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "Amir Zadeh",
        "Minghai Chen",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing"
    },
    {
      "citation_id": "51",
      "title": "Unimodal and multimodal integrated representation learning via improved information bottleneck for multimodal sentiment analysis",
      "authors": [
        "Tonghui Zhang",
        "Changfei Dong",
        "Jinsong Su",
        "Haiying Zhang",
        "Yuzheng Li"
      ],
      "year": "2022",
      "venue": "CCF International Conference on Natural Language Processing and Chinese Computing"
    },
    {
      "citation_id": "52",
      "title": "Information bottleneck based representation learning for multimodal sentiment analysis",
      "authors": [
        "Tonghui Zhang",
        "Haiying Zhang",
        "Shuke Xiang",
        "Tong Wu"
      ],
      "year": "2022",
      "venue": "Proceedings of the 6th International Conference on Control Engineering and Artificial Intelligence"
    },
    {
      "citation_id": "53",
      "title": "Self-adaptive representation learning model for multi-modal sentiment and sarcasm joint analysis",
      "authors": [
        "Yazhou Zhang",
        "Yang Yu",
        "Mengyao Wang",
        "Min Huang",
        "M Shamim Hossain"
      ],
      "year": "2024",
      "venue": "ACM Transactions on Multimedia Computing, Communications and Applications"
    },
    {
      "citation_id": "54",
      "title": "Learning multi-task commonness and uniqueness for multi-modal sarcasm detection and sentiment analysis in conversation",
      "authors": [
        "Yazhou Zhang",
        "Yang Yu",
        "Dongming Zhao",
        "Zuhe Li",
        "Bo Wang",
        "Yuexian Hou",
        "Prayag Tiwari",
        "Jing Qin"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Artificial Intelligence"
    }
  ]
}