{
  "paper_id": "2203.12531v3",
  "title": "Multi-Label Classification With Transformers For Action Unit Detection",
  "published": "2022-03-23T16:46:09Z",
  "authors": [
    "Gauthier Tallec",
    "Edouard Yvinec",
    "Arnaud Dapogny",
    "Kevin Bailly"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Action Unit (AU) Detection is the branch of affective computing that aims at recognizing unitary facial muscular movements. It is key to unlock unbiased computational face representations and has therefore aroused great interest in the past few years. One of the main obstacles toward building efficient deep learning based AU detection system is the lack of wide facial image databases annotated by AU experts. In that extent the ABAW challenge paves the way toward better AU detection as it involves a 2M frames AU annotated dataset. In this paper, we present our submission to the ABAW3 challenge. In a nutshell, we applied a multi-label detection transformer that leverage multi-head attention to learn which part of the face image is the most relevant to predict each AU.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The ABAW3 Challenge  [6]  is the third edition  [6, 7, 13]  of Affect Analysis in the wild  [26] . We participated in the AU detection part of the challenge. In previous editions, top ranked methods took benefit from the heterogeneous annotations of ABAW3  [8, 9, 10, 11, 12]  and used multi-task learning to jointly learn the different affective computing tasks corresponding to the different challenges. This year, the use of such strategy was restricted to the multi-task challenge competitors. We therefore focused on the AU detection problem specifities.\n\nAction Units (AUs) are a dictionary of unitary muscular activations that was designed by psychologists to anatomically describe the mechanics of facial expressions. Theoretically  [4] , accurate detection of such activations enables unbiased computational description of human faces and could therefore help improve face analysis applications altogether.\n\nFrom a machine learning standpoint, AU detection is a multi-label detection problem that comes with three main specifities: First, AUs activations are very local changes of skin texture. Consequently, several methods tried to design locally adapted features  [29]  with a notable focus on facial landmark-based AU attention  [14, 20]\n\nOverview of the Multi-Label Transformer heavily dependant from one another (mostly for physiological reasons), hence multiple approaches tried to model those dependencies using label prior dependency learning methods  [22, 24] . Finally, AU are events of short duration, therefore video-based datasets, which include all publicly available AU detection datasets, are bound to be heavily imbalanced toward the absence of activation. This problem has mostly been tackled by weighting the binary cross-entropy with AU frequency based coefficients  [21]  and adding a dice score contribution to the loss  [20] .\n\nIn this paper, we design a transformer-based architecture for multi-label detection that we apply to the action unit detection problem. More precisely, our method learns local features adapted to each AU using a) self-attention on encoded patches of human faces to produce the features and b) cross-attention between those features and learned AU token for selecting which feature are most informative about each AU. Figure  1  summarizes the architecture of our model. The research work in action unit detection is centered around two main streams, that corresponds to the two main specificities of the problem, namely the local aspect of AUs and the strong dependencies that AUs displays between one another.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Exploiting the local aspect of action units require the extraction of local features. For that purpose, seminal work  [29]  proposed convolutional layers with region-wise shared filters to extract features that are adapted to each face region. Nevertheless, for such method to be efficient face parts need to always fall in the same rectangular region. To overcome this limitation, several works  [14, 19, 20]  guided AU related feature using facial landmarks, either by learning from predefined landmark neighborhood crops  [14] , or by using landmark based AU attention maps  [21] .\n\nTo take advantage of the strong relationships between AU, several works focused on explicitly modeling those dependencies. In particular, backpropagation through a probabilistic graphical model (PGM) was adopted in  [1] , and an hybrid message passing strategy was used in  [22] . Others leveraged the local nature of AU to assume that label dependencies imply dependencies between local face zones. Attempts at capturing such spatial dependencies include attention map learning  [21] , LSTM-based spatial pooling  [18]  and more recently transformer-like architectures  [5] .",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Transformers For Multi-Label Detection.",
      "text": "The arrival of transformers  [25]  has significantly modified the landscape of state of the art deep learning architectures, first in natural language processing  [2]  and more recently in computer vision  [3] . So far, transformerbased multi-label leveraged cross-attention between labelwise learnable tokens  [15]  and encoded input parts to learn which portions of the inputs are useful for predicting each label. In this work, we propose to apply this very architecture to AU detection.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methodology",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Background",
      "text": "Most transformer architectures consists in a combination of Multi-Head Attention modules (MHA) and Multi-Layer Perceptron (MLP) blocks interspersed by layer normalization (missing quote)and dropout (missing quote)denoted LN and D respectively.\n\nThe Multi-Head Attention module consists in an ensemble of N h attention head. Each of these heads computes attention between n q queries q stored in Q ∈ R nq×d and n keys K ∈ R n×d for combining n values V ∈ R n×d . For that purpose, for each head h, query, key and values are encoded using dense layers\n\nThen each head performs encoded values combination based on encoded key/query comparison:\n\nThe N h resulting vectors are concatenated and projected using dense layers\n\n(3) To simplify the notations, we use d k = d v = d/N h (missing quote)and further define self and cross attention layers as follows:\n\nThe MLP Block is composed of two dense layers with dropout on top of each of them: W g ∈ R d×d mlp with gelu activation (missing quote)and W l ∈ R d mlp ×d with linear activation. Formally for input queries Q:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Multi-Label Transformer",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Self Attention On Input Image",
      "text": "Recent work  [3]  on transformer architectures unlocked the use of self attention mechanism on images. However it has been shown that applying such mechanism on raw images requires large training datasets with a lot of variability which is only partially the case for ABAW3 database. For that purpose we use a pretrained encoder g WI to first encode the raw images X in a patch-based fashion:\n\nSecond we encode positions by adding a different token to each encoded patch:\n\nwhere W nx ∈ R nx×d stores each patch position embedding. Finally, we apply N x layers of self-attention on the extracted patches. Layer l consists in first, input self attention:\n\nfollowed by MLP block based encoding:",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Cross Attention On Label Tokens",
      "text": "Previous works  [15]  on multi-label transformers leverage N l layers of multi-head attention between learnable label tokens and input parts in order to learn which portion of the input is the more informative about each label. Formally, we denote T (0) = I L W t ∈ R L×d the L labels ids encoded with dense layer W t ∈ R L×d and X (Nx) the n x considered parts extracted from input. Layer l consists in, first, token self attention:\n\nSecond, token image cross attention:\n\nand finally, MLP block based encoding:\n\nAt the end of the N l -th layer, the vector of predictions is computed by projecting the L encoded tokens using a dense layer W p ∈ R d×1 with sigmoid activation σ:\n\nTask t distribution is then estimated as follows:\n\nwhere BCE stands for binary cross entropy and W englobes all the network parameters.\n\nTraining is done under the assumption that tasks are independent given the input. Therefore it consists in the minimization of the following maximum likelihood based loss:\n\nLastly, the ABAW3 challenge is based on videos, i.e. on continuous frame sequences. As AUs are segment-level events as opposed to frame-level events (i.e. activation of a specific AU usually lasts for at least a couple of frames): to impose such structure in the transformer prediction, we simply smoothed these predictions using a moving mean of window size w s = 20 frames.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Network Architecture",
      "text": "For all our experiments we use an (Imagenet-pretrained) InceptionV3 backbone from which the pooling layer is removed and replaced by a 1D convolution with 128 output channels, on top of which we add a single self-attention layer with N h = 8. As far as cross attention layers are concerned we use N l = 2 as well as a dropout rate of 0.1.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Training Strategy",
      "text": "Actions units detection is a heavily imbalanced problem, therefore we use a number of solutions to mitigate this problem while learning: we weight each example (term of the sum in Equation  14 ) with frequency based coefficients. Furthermore, we add a Dice score contribution  [20]  to the final loss in order to adapt to the F1Score based evaluation.\n\nFor loss optimization we use AdamW optimizer  [16] . For the Inception part of the network we use an exponentially decaying learning rate with initial value 5e -4 and decay rate 0.99. For the transformer part of our network we use the learning schedule in  [25] . We scale it with respect to the number of patches n x = 64 in the input image self attention part, and with respect to the number of labels L, in the label tokens cross-attention part.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pretraining And Data Augmentation",
      "text": "We investigate pretraining the network on already available data, e.g. BP4D  [28]  and DISFA  [17]  databases. However, these datasets do not have exatcly the same AU annotation as in ABAW3 dataset: Hence, we handle this by pretraining on the reunion of the 3 different AU sets and masking the bce loss for the AU that are not annotated for each example of each dataset.\n\nFurthermore, we apply data-augmentation by combining geometrical (random rotation, horizontal flips, random zoom) and color-based (channel drop, random brightness) augmentations. Furthermore we make use of label smoothing  [23]  and image mixup  [27] .\n\nTable  4  reports the results of our different submissions to the challenge.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Acknowledgements",
      "text": "",
      "page_start": 3,
      "page_end": 3
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of the Multi-Label Transformer",
      "page": 1
    },
    {
      "caption": "Figure 1: summarizes the architecture of our",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Submission ID": "1\n2\n3\n4\n5",
          "moving mean": "✗\nX\nX\nX\nX",
          "frequency weights": "X\nX\n✗\n✗\n✗",
          "Dice": "X\nX\n✗\nX\nX",
          "DISFA/BP4D pretraining": "✗\n✗\nX\nX\nX",
          "mixup": "X\nX\n✗\n✗\nX",
          "Valid F1": "52.7\n53.8\n50.3\n51.0\n51.7",
          "Test Mean F1": "-\n-\n-\n-\n-"
        }
      ],
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Deep structure inference network for facial action unit recognition",
      "authors": [
        "Ciprian Corneanu",
        "Meysam Madadi",
        "Sergio Escalera"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "2",
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "3",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "Alexey Dosovitskiy",
        "Lucas Beyer",
        "Alexander Kolesnikov",
        "Dirk Weissenborn",
        "Xiaohua Zhai",
        "Thomas Unterthiner",
        "Mostafa Dehghani",
        "Matthias Minderer",
        "Georg Heigold",
        "Sylvain Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "4",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "Rosenberg Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "5",
      "title": "Facial action unit detection with transformers",
      "authors": [
        "Miriam Geethu",
        "Bjorn Jacob",
        "Stenger"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "arxiv": "arXiv:2202.10659"
    },
    {
      "citation_id": "7",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "8",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "9",
      "title": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multitask learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "10",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "11",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "12",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "13",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "Eac-net: A region-based deep enhancing and cropping approach for facial action unit detection",
      "authors": [
        "Wei Li",
        "Farnaz Abtahi",
        "Zhigang Zhu",
        "Lijun Yin"
      ],
      "year": "2017",
      "venue": "2017 12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "15",
      "title": "Query2label: A simple transformer way to multi-label classification",
      "authors": [
        "Shilong Liu",
        "Lei Zhang",
        "Xiao Yang",
        "Hang Su",
        "Jun Zhu"
      ],
      "year": "2021",
      "venue": "Query2label: A simple transformer way to multi-label classification",
      "arxiv": "arXiv:2107.10834"
    },
    {
      "citation_id": "16",
      "title": "Decoupled weight decay regularization",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2019",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "17",
      "title": "Disfa: A spontaneous facial action intensity database",
      "authors": [
        "Mohammad Mohammad Mavadati",
        "Kevin Mahoor",
        "Philip Bartlett",
        "Jeffrey Trinh",
        "Cohn"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "18",
      "title": "Local relationship learning with personspecific shape regularization for facial action unit detection",
      "authors": [
        "Xuesong Niu",
        "Hu Han",
        "Songfan Yang",
        "Yan Huang",
        "Shiguang Shan"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Deep adaptive attention for joint facial action unit detection and face alignment",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Lizhuang Ma"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "20",
      "title": "Jaanet: Joint facial action unit detection and face alignment via adaptive attention",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Lizhuang Ma"
      ],
      "year": "2021",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "21",
      "title": "Facial action unit detection using attention and relation learning",
      "authors": [
        "Zhiwen Shao",
        "Zhilei Liu",
        "Jianfei Cai",
        "Yunsheng Wu",
        "Lizhuang Ma"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "Hybrid message passing with performance-driven structures for facial action unit detection",
      "authors": [
        "Tengfei Song",
        "Zijun Cui",
        "Wenming Zheng",
        "Qiang Ji"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "Christian Szegedy",
        "Vincent Vanhoucke",
        "Sergey Ioffe",
        "Jon Shlens",
        "Zbigniew Wojna"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "24",
      "title": "Multiorder networks for action unit detection",
      "authors": [
        "Arnaud Gauthier Tallec",
        "Kevin Dapogny",
        "Bailly"
      ],
      "year": "2022",
      "venue": "Multiorder networks for action unit detection",
      "arxiv": "arXiv:2202.00446"
    },
    {
      "citation_id": "25",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "26",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW), 2017 IEEE Conference on"
    },
    {
      "citation_id": "27",
      "title": "mixup: Beyond empirical risk minimization",
      "authors": [
        "Hongyi Zhang",
        "Moustapha Cisse",
        "Yann Dauphin",
        "David Lopez-Paz"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "28",
      "title": "A high-resolution spontaneous 3d dynamic facial expression database",
      "authors": [
        "Xing Zhang",
        "Lijun Yin",
        "Shaun Jeffrey F Cohn",
        "Michael Canavan",
        "Andy Reale",
        "Peng Horowitz",
        "Liu"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE international conference and workshops on automatic face and gesture recognition (FG)"
    },
    {
      "citation_id": "29",
      "title": "Deep region and multi-label learning for facial action unit detection",
      "authors": [
        "Kaili Zhao",
        "Wen-Sheng Chu",
        "Honggang Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    }
  ]
}