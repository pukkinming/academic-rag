{
  "paper_id": "2003.00809v1",
  "title": "Vision Based Body Gesture Meta Features For Affective Computing",
  "published": "2020-02-10T14:38:16Z",
  "authors": [
    "Indigo J. D. Orton"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Title: Vision based body gesture meta features for Affective Computing Early detection of psychological distress is key to effective treatment. Automatic detection of distress, such as depression, is an active area of research. Current approaches utilise vocal, facial, and bodily modalities. Of these, the bodily modality is the least investigated, partially due to the difficulty in extracting bodily representations from videos, and partially due to the lack of viable datasets. Existing body modality approaches use automatic categorization of expressions to represent body language as a series of specific expressions, much like words within natural language. In this dissertation I present a new type of feature, within the body modality, that represents meta information of gestures, such as speed, and use it to predict a non-clinical depression label. This differs to existing work by representing overall behaviour as a small set of aggregated meta features derived from a person's movement. In my method I extract pose estimation from videos, detect gestures within body parts, extract meta information from individual gestures, and finally aggregate these features to generate a small feature vector for use in prediction tasks. No existing, publicly available, dataset for distress analysis contains source videos of participants or pose data extracted from the source videos. As such, I introduce a new dataset of 65 video recordings of interviews with self-evaluated distress, personality, and demographic labels. This dataset enables the development of features utilising the whole body in distress detection tasks. I evaluate my newly introduced meta-features for predicting depression, anxiety, perceived stress, somatic stress, five standard personality measures, and gender. A linear regression based classifier using these features achieves a 82.70% F1 score for predicting depression within my novel dataset. My results suggest this feature type has value for distress prediction and, more broadly, has potential within the affective computing domain, as they appear to be useful aggregate representations of human behaviour.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Declaration",
      "text": "This dissertation is the result of my own work and includes nothing which is the outcome of work done in collaboration except as declared in the Preface and specified in the text. It is not substantially the same as any that I have submitted, or am concurrently submitting, for a degree or diploma or other qualification at the University of Cambridge or any other University or similar institution except as declared in the Preface and specified in the text. I further state that no substantial part of my dissertation has already been submitted, or is being concurrently submitted, for any such degree, diploma or other qualification at the University of Cambridge or any other University or similar institution except as declared in the Preface and specified in the text. This dissertation does not exceed the prescribed limit of 15 000 words.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Indigo Jay Dennis Orton",
      "text": "June, 2019\n\nChapter 1",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Introduction",
      "text": "Mental health is an increasingly prominent area of discussion and research in society, it is a main contributor to overall disease burden globally  [41] . Awareness of its effects and symptoms, destigmatization of its results, consideration of its causes, and approaches to its treatment and diagnosis, in the case of illness, are all rapidly evolving. According to the UK Mental Health Foundation  [41] , depression is the \"predominant mental health problem worldwide\". Within the UK in 2014, 19.7% of people over the age of 16 showed symptoms of depression or anxiety. Early detection is paramount to long term health, 10% of children aged 5-16 years have a clinically diagnosable mental health issue, of children who have experienced mental health problems 70% do not have their issues detected or addressed early enough  [41] .",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Automatic Distress Detection -Why And How",
      "text": "Automatic distress detection enables large scale early screening. Early screening can be used to enable mitigation of distress at an earlier stage that it might otherwise be identified and also prioritization of services. For example, Crisis Text Line  [12]  (CTL) provides a free mental health counseling service via text messages. The communication medium means consumers can message without a counselor being exclusively available. To better support those consumers most in need (e.g. those most at risk of suicide), CTL analyses text messages in real time to triage consumers for counselor attention  [15] . While the CTL example is a specific use case with a single modality, text, the broader area of distress detection uses many modalities including facial, eye, head, vocal, bodily, and speech. Existing work has investigated usage of psychology coding systems such as FACS levels of activity in eye and head movement  [1, 55, 16] , orientation of eye gaze and head  [46] , fundamental frequencies and biomarkers of vocal recordings  [43, 2] , automatic categorization of body expressions  [36, 34, 51] , and natural language artifacts from speech  [13] . Moreover, many methods incorporate multiple modalities and achieve significantly better results.\n\nOne of the difficulties in this area is the lack of available and relevant datasets. Distress focused datasets are not readily shared given the sensitive nature of the subject. Those that are shared rarely include source data, but rather provide processed outputs, again due to the sensitive of the subject. Finally, each modality has different source data requirements and datasets are generally designed with a specific method in mind. All of this creates a barrier to the development of novel features and thus research within this area is concentrated on the modalities available within the few public datasets.",
      "page_start": 13,
      "page_end": 14
    },
    {
      "section_name": "Body Modality -A Research Gap",
      "text": "The body modality includes the whole body from the neck down, sometimes the head is also included. It is the least researched of the modalities. This is, partially, due to the dataset barrier. No public distress dataset, that I am aware of, provides body modality data, or the source video recordings to extract such data. The majority of relevant body modality based automatic distress detection work is based on private clinical datasets.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Existing Approaches",
      "text": "Within these body modality methods there are two primary approaches: body expression categorization and hand-crafted distress behaviour descriptors. The first approach uses unsupervised learning to cluster spatio-temporal descriptors of movement to define expression codebooks and then uses these codebooks to predict depression severity  [36, 34, 51] . The second approach defines hand-crafted descriptors of behaviour psychology literature has shown to correlate to distress  [23] , such as self-adaptors and fidgeting  [46, 40] , and to model the intensity of such behaviours to indicate distress severity.",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "Thesis Contributions",
      "text": "My core research question is: is meta information from gestures predictive of psychological distress?\n\nIn this dissertation I present novel generic body gesture meta features for automatic distress detection. These features are based on the movement, frequency, and duration of body gestures. Per-gesture these features are cheap to compute and relatively simplistic, aggregated across all gestures they aim to represent general behavioural characteristics of a subject.\n\nI evaluate the use of these features for detecting depression (non-clinical) and find that they provide useful predictive information within linear models. I then evaluate the generalisability of these features by applying them to classification tasks for other distress and personality labels.\n\nAs I noted earlier, a core barrier to the development of new features within the body modality is the lack of available distress datasets that expose the required source data. As such I introduce a novel audio-visual dataset gathered for this dissertation in collaboration with Dr. Marwa Mahmoud. The dataset contains audio-visual recordings of semi-structured interviews and labels based on established psychology self-evaluation questionnaires for distress and personality measures.",
      "page_start": 14,
      "page_end": 15
    },
    {
      "section_name": "Dissertation Structure",
      "text": "Chapter 2 contains a literature review covering related distress detection research and relevant technology for working with the body modality specifically. Chapter 3 introduces a novel dataset for depression detection. I present my method in Chapter 4. I evaluate my features' validity in Chapter 5. Finally, I conclude and outline potential future work in Chapter 6.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Chapter 2",
      "text": "",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "Literature Review",
      "text": "This review is structured in two parts: automatic depression detection and body modality technology. In the first component I review existing approaches to automatic depression detection and related fields, and the variety of modalities used by these approaches. This defines the research space this dissertation occupies. The second component outlines the current methods for working with the body modality and the technology involved in these methods, such as pose estimation. This provides the basis for the core data I use within my method.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Automatic Detection Of Distress",
      "text": "Distress is expressed through all modalities. Many approaches have been developed to automatically detect distress using behavioural cues, these include both mono-modal and multi-modal approaches. For example, face analysis methods are one of the common, and powerful, techniques enabled by the development of tools for extracting accurate positional information, such as facial landmarks, and tools for automatic interpretation based on existing psychology approaches such as FACS  [21] .\n\nI review uses of the primary modalities, cross-modal considerations, multi-modal approaches, and the expanding use of deep learning within automatic distress detection. The face, head, and body modalities are the most relevant, though I briefly provide examples of text and vocal modal usage.",
      "page_start": 17,
      "page_end": 17
    },
    {
      "section_name": "Text Modality",
      "text": "Dang et al.  [13]  use linguistic attributes, auxiliary speech behaviour, and word affect features to predict depression severity and emotional labels within the DAIC dataset  [26] . Linguistic attribute features include total number of words, unique words, pronouns, lexical proficiency, among others. Auxiliary speech behaviour cover parallel actions, such as laughing or sighing, and meta information such as word repeats and average phrase length. Word features are determined by a collection of annotated corpora that assign ngrams categorizations or ratings relating to their affective semantic. For example, assigning an emotion type (anger, disgust, joy, etc) to a word or rating words from 0 to 10 on affective attributes such as arousal, valence, dominance, and pleasure. These three feature types are used to predict depression measures.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Audio Modality",
      "text": "The audio modality can be a strong predictive source as non-verbal features of speech can be predictive of distress irrespective of the content of a person's speech  [43] .\n\nFeatures from audio data commonly include prosody, jitter, intensity, loudness, fundamental frequency, energy, Harmonic-to-Noise-Ratio (HNR), among others. Ozdas et al.  [43]  present a method for analysing fluctuations in the fundamental frequency of a person's voice to assess their risk of suicide. Alghowinem et al.  [2]  explore the use of a broad selection of vocal features, extracted using the \"openSMILE\" toolkit  [22]  to detect depression. Dibeklioglu et al.  [16]  use vocal prosody to detect depression.\n\nIn their investigation of psychomotor retardation caused by depressive states, Syed et al.  [55]  use low-level descriptors to model turbulence in subject speech patterns. By profiling the turbulence of depressed and non-depressed participants with a depression dataset they develop a model for predicting depression severity based on the level of turbulence.",
      "page_start": 18,
      "page_end": 18
    },
    {
      "section_name": "Facial Modality",
      "text": "Joshi et al.  [36]  present a \"bag of facial dynamics\" depression detection method based on the same expression clustering as their \"bag of body dynamics\" method described in more depth below. For this facial method the space-time interest points (STIPs) are generated for face aligned versions of the source videos.\n\nWhile Joshi et al. use a categorization approach, Dibeklioglu et al.  [16]  use generic features representing facial movement dynamics for depression detection. This method involves generating statistical derivations from movement features such as velocity, acceleration, and facial displacement over a time period and then modeling their effect.\n\nWhilst Dibeklioglu et al. take a generic approach to facial movement, Syed et al.  [55]  attempt to model behaviour discussed in psychology literature, using features representing psychomotor retardation to predict depression severity. Psychomotor retardation has been shown to be linked to depression  [50] . In particular, Syed et al. generate features to capture craniofacial movements that represent psychomotor retardation, and thus indicate depression. To capture the target movements they design features that represent muscular tightening, a depressed subject is expected to have impaired muscle movements. They model three types of movement: head movement, mouth movement, and eyelid movement. These movements are represented by temporal deltas to define the amount of movement in the region. From these localised movement deltas the authors aim to represent specific actions such as blinking or contorting of the mouth. Relatively simplistic features derived from these actions, such as blink rate, can be indicative of depression  [3, 19] . The nature of human behaviour is that these kinds of simplistic features can contribute useful information to distress detection models. Moreover, modeling specific facial actions has been examined as well. For example, Scherer et al.  [46]  use smile features such as intensity and duration, along with other modalities, to detect depression.\n\nYang et al.  [58]  present a novel facial descriptor, a \"Histogram of Displacement Range (HDR)\", which describes the amount of movement of facial landmarks. The histogram counts the number of occurrences of a displacement within a certain range of movement. Where Syed et al. represented the amount of movement of certain facial features to measure psychomotor retardation, Yang et al. represent the number of times the face is distorted, so to speak, by landmarks moving a certain amount.\n\nEye Sub-Modality While Syed et al.  [55]  explored the use of eye lid features, eye gaze features have also been shown to be effective in predicting distress. This modality has become viable as eye tracking technology has progressed sufficiently to enable accurate processing of eye features.\n\nAlghowinem et al.  [1]  use eye gaze/activity to perform binary classification of depression in cross-cultural datasets. They extract iris and eyelid movements to extract features such as blink rate, duration of closed eyes, and statistical \"functionals\" (i.e. simple derivations) of the amount of activity. However, activity is not the only indicator, Scherer et al.  [46]  use average eye gaze vertical orientation (among other features), in the span  [-60, 60]  degrees.",
      "page_start": 19,
      "page_end": 19
    },
    {
      "section_name": "Head Modality",
      "text": "Joshi et al.  [36]  present a \"histogram of head movements\" depression detection method that models movement of a person's head over time. They use three facial landmarks, the corner of each eye and the tip of the nose, to compute the orientation of the subject's head. The histogram uses orientation bins of width 10 within the range [-90, 90] degrees for windows of time within a video. These windowed histograms are then averaged over the full length of the video. The resulting average histogram is a descriptor of the amount of movement within the video by representing the variety of angles the head orients to. This method achieves comparable performance to their \"bag of facial dynamics\" method.\n\nA number of the methods using eye activity features also incorporate head activity in their models. Alghowinem et al.  [1]  model head activity similarly to their modeling of eye activity. As with eye activity, they extract statistical derivations of movement and angular shift. They also include the duration of the head at different orientations, the rate of change of orientation, and the total number of orientation changes. Scherer et al.  [46]  use head vertical orientation, similar to their eye gaze orientation feature, as a feature for predicting depression. They use the average pitch of the head within a 3D head orientation model. Dibeklioglu et al.  [16]  also model head movement dynamics in a similar fashion to their facial movement dynamics features. Similar to  Alghowinem et",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "Body Modality",
      "text": "The body modality is the least researched of the modalities reviewed. This is due to a number of factors including the difficulty of dataset creation and the available technology for extracting raw body data. Contrast this with the relative ease of working with the other modalities and it is no surprise they received more attention. However, much of the relevant body modality based automatic distress detection research appeared in the early 2010s as some private clinical datasets were gathered and the parallel technological ecosystem expanded to support generation of body modality features.",
      "page_start": 20,
      "page_end": 20
    },
    {
      "section_name": "The Case For Further Investigation",
      "text": "De Gelder  [14]  presents the case for further research of bodily expressions within affective neuroscience. Though a parallel field, the core argument is much the same for affective computing's investigation of bodily expressions. Specifically, at the time of writing (2009) de Gelder asserts that 95% of \"social and affective neuroscience\" focuses on faces and that the remaining 5% is mostly split between vocal, musical, and environmental modalities with a very few number of papers investigating the body modality. This was similar to the state of automatic distress detection in the early 2010s, though the vocal modality has a more prominent position in the literature and is more evenly balanced with the facial modality.\n\nAffectation control and robust automatic detection Non-verbal features provide discriminative information regardless of a person's conscious communication, this is particularly important for automatic distress detection. Different modalities can be consciously controlled to varying degrees. For example, facial expressions are more easily controlled than bodily expressions  [14] . By including more modalities, and representations of those modalities, automatic distress detection could become more robust to conscious affectation modification of modalities.\n\nFurther to robustness, one of the advantages of the face and body modalities is their ability to detect micro-expressions. Micro-expressions are instinctual reactions to some stimulus that can be predictive of emotional and distress state  [20, 28] . They are significantly harder to control than general expressions.",
      "page_start": 21,
      "page_end": 21
    },
    {
      "section_name": "Expression Categorization",
      "text": "Much of the body modality research has approached the problem as a transfer of the methods from the facial modality by modeling body movements as expressions in much the same way facial expressions are  [33, 36, 34] . Differences in these methods have been centred around: what tracklets form the basis of the body data  [36] , the use of deep learning vs manual descriptor definition  [46] , and process for generating categories of expressions  [51] .\n\nJoshi et al.  [36]  demonstrate the discriminative power of bodily expressions for predicting clinically based depression measures. They use STIPs from recordings of a participant's upper body and generate a \"Bag of Body Dynamics (BoB)\" based on codebooks of expression representations. STIPs are generated for a video, then Histograms of Gradient (HoG) and Optical Flow (HoF) are computed spatio-temporally around the STIPs, these histograms are then clustered within a sample, the cluster centres form a representation of the movements occurring in the video. The cluster centres from all videos in a training set are clustered again to generate the codebook of expressions. A BoB feature vector is generated for each sample by counting the number of cluster centres within the sample that fit within each codebook expression. Finally, these BoB feature vectors are used by an SVM to detect depression.\n\nJoshi et al.  [34]  extend on this approach by combining a \"holistic body analysis\", similar to the BoB method, this method uses STIPs for whole body motion analysis and adds relative body part features. These features represent the movement of the head and limbs relative to the participant's trunk, represented as polar histograms.\n\nApplying the same essential method as Joshi et al.,  Song et al. [51]  present a method for learning a codebook of facial and bodily micro-expressions. They identify micro-expressions by extracting STIPs over very short time intervals (e.g. a few hundred milliseconds), then, as Joshi et al. do, they compute local spatio-temporal features around the STIPs and learn a codebook of expressions based on these local features. Finally, for each sample they generate a Bag-of-Words style feature vector based on the codified micro-expressions present in the sample.",
      "page_start": 21,
      "page_end": 22
    },
    {
      "section_name": "Distress Behaviour Descriptors",
      "text": "Psychology literature describes specific behaviours that are correlated with psychological distress and disorders. For example, Fairbanks et al.  [23]  find self-adaptors and fidgeting behaviour to be correlated to psychological disorders. Based on this work, Scherer et al.  [46]  evaluate the use of these behaviours for automatic distress detection. Specifically, they manually annotate their dataset for hand self-adaptor behaviours and fidgets, including hand tapping, stroking, grooming, playing with hands or hair, and similar behaviours. To identify whether regions are relevant to these behaviours they also annotate these behaviours with categories such as head, hands, arms, and torso, and then extract statistical information such as the average duration of self-adaptors in each region. They also annotate leg fidgeting behaviour such as leg shaking and foot tapping, again they use statistical derivations of these behaviours as features for detection.\n\nWhilst Scherer et al. manually annotated self-adaptors and fidgets, Mahmoud et al.  [40]  present an automatic detector of fidgeting, and similar behaviours, based on a novel rhythmic motion descriptor. They extract SURF interest point tracklets from colour and depth data and then apply their novel rhythmic measure to check similarity among cyclic motion across tracklets. Rhythmic motion is then localised based on Kinect skeletal regions and classified as one of four classes: Non-Rhythmic, Hands, Legs, and Rocking.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Multi-Modal Fusion",
      "text": "Combining modalities for prediction has proven effective when combining a variety of modalities  [58, 35, 16] . There are four primary types of fusion: feature fusion such as feature vector concatenation, decision fusion such as majority vote, hybrid fusion which uses both, and deep learning fusion which merges inner representations of features within a deep learning architecture. The deep learning fusion method differs from feature fusion as the features are provided to separate input layers and only merged after inner layers, but before decision layers.\n\nSong et al.  [51]  combine micro-expressions from facial and bodily modalities with sample-level audio features. They evaluate three methods, early fusion by concatenating audio features to features from each visual frame, early fusion using a CCA  [30]  kernel, and late fusion based on voting, where the per-frame predictions from the visual modalities are averaged over the sample and then combined with the audio prediction. Dibeklioglu et al.  [16]  fuse facial, head, and vocal modalities using feature concatenation. They extend on this by performing feature selection on the concatenated vector, rather than the source vectors, using the Min-Redundancy Max-Relevance algorithm  [45] .\n\nAlghowinem et al.  [1]  perform hybrid modality fusion, both combining feature vectors from modalities and performing a majority vote on individual modality classification predictions. The vote fusion is based on three classifiers, two mono-modal classifiers and one feature fusion classifier.\n\nHuang et al.  [31]  train long-short-term memory (LSTM) models on facial, vocal, and text modalities and then use a decision level fusion, via a SVR, to predict the final regression values. This paper differs from many deep learning approaches as it uses the decision level fusion, rather than having the deep learning models find patterns across feature types.  [25]  present another approach to usage of multiple modalities where the text modality provides contextualisation for features from the audio-visual modalities. They apply a topic modeling method for depression detection using vocal and facial modalities, where features are grouped based on the topic being responded to within an interview. The authors suggest that without topic modeling the features are averaged over too large a time period such that all temporal information is lost. By segmenting the samples they aim to retain some of the temporal information. Arbitrary segmentation would not necessarily be useful, thus their use of logical segmentation based on topic.",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Temporal Contextualisation Gong & Poellabauer",
      "text": "",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Deep Learning",
      "text": "Much of the recent work in distress detection leverages advances in deep learning, especially advances related to recurrent architectures, such as LSTMs, which can model sequence data well. In distress detection most modalities provide sequence data, audio-visual streams, natural language, or sequence descriptors of the data (such as FACS AUs).\n\nChen et al.  [9]  present a method utilizing text, vocal, and facial modalities for emotion recognition. They explore the use of existing vocal features such as fundamental frequency analysis, auto-learnt features based on pre-trained CNNs to extract vocal and facial features, and word embedding features for the text. The auto-learnt facial features are derived from existing facial appearance data already extracted from the raw videos (i.e. they do not have their CNNs process raw frames to extract features). They then experiment with SVR and LSTM models to evaluate the temporal value of the LSTM model. They find that fused auto-learnt features from all modalities in combination with the LSTM model provides the greatest performance.\n\nYang et al.  [59]  present an interesting multi-level fusion method that incorporates text, vocal, and facial modalities. They design a Deep Convolutional Neural Network (DCNN) to Deep Neural Network (DNN) regression model that is trained, separately, for audio and video modalities. They also trained their regression models separately for depressed and non-depressed participants, resulting in four separate DCNN -DNN models. They use the openSMILE toolkit for their audio features, this is common among many of the vocal modality methods (e.g. Alghowinem et al. from above), and FACS AUs for their visual features. They derive a temporally relevant feature vector from the set of all AUs by calculating the change in AUs over time. Their text modality model uses Paragraph Vectors in combination with SVMs and random forests and predicts a classification task rather than a regression task. Finally, they fuse, using DNNs, the audio and visual model predictions per training set, i.e. the depressed participant trained models are fused and the non-depressed models are fused. They then use another DNN to fuse the two fused regression predictions (i.e. depressed and non-depressed) and the classification prediction from the text modality. While they use DNNs to fuse decisions at multiple levels, this is a decision-level fusion method, not a deep learning fusion method, as they fuse the regression predictions from each model rather than the inner layer outputs.\n\nYang et al.  [58]  present a second paper which utilises the same structure of DCNNs and DNNs, with two significant changes: firstly, the video features are changed to a new global descriptor they present, the \"Histogram of Displacement Range (HDR)\" which describes the amount of movement of facial landmarks, and secondly, the text modality now uses the same DCNN -DNN architecture to perform regression based on text data. Having changed the output of the text model the final fusion is a regression fusion using the same method as the audio visual models were fused in the first paper.\n\nFinally, no method, that I am aware of, uses deep learning end-to-end for automatic distress detection such that features are learnt from raw data for predicting distress. All methods apply deep learning on top of audio-visual descriptors and hand-crafted features. One of the core difficulties is the relative sparsity of existing data and thus the restricted ability to learn interesting features. Therefore, continued development of features and approaches to behavioural representation is valuable and able to contribute to methods that use deep learning.",
      "page_start": 23,
      "page_end": 23
    },
    {
      "section_name": "Body Gestures",
      "text": "Body gestures present a number of challenges including deriving features directly from pixels, extracting positional data, detecting gestures within the data, and designing representative features based on gestures. This section focuses on previous work on body gesture and pose representation, which forms the basis of my feature definition and generation.\n\nThere are three primary approaches to body gesture representation within the affective computing literature: traditional computer vision feature detection algorithms such as STIPs  [36, 34] , pose estimation  [8]  from standard video recordings, and use of specialised 3D capture equipment such as Kinects  [40] .",
      "page_start": 24,
      "page_end": 24
    },
    {
      "section_name": "Recording Type -2D Vs 3D",
      "text": "The first two approaches use standard 2D video recordings to extract the base data for calculating features. The third approach uses 3D video recordings to enable more detailed analysis of body movement via depth. The most common 3D capture equipment used within related work is Kinects. Kinects have an added benefit that they provide skeletal key points (i.e. joint locations) within a recording, thus enabling more accurate representation of body data.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Extracting Body Representations",
      "text": "Given a video recording, 2D or 3D, with or without skeletal information, the next phase is feature extraction. Feature extraction approaches fall into two primary categories, which apply to both 2D and 3D videos: generic video feature representations and body modality specific representations.",
      "page_start": 25,
      "page_end": 25
    },
    {
      "section_name": "Generic Interest Points",
      "text": "The first approach does not target specific body areas or movement, instead it assumes that the only subject within the video is the subject the model is concerned with, i.e. the participant, and extracts generic video features from the recording to represent the body and gestures. Examples of these generic features are Space-Time Interest Points (STIPs), SURF, Histogram of Gradients (HOGs) and Optical Flow (HOFs), among others. These features can be extracted from colour and depth recordings such that they are applicable to both 2D and 3D recordings.\n\nWhile some approaches apply these generic features (or rather, derivations of these features) directly to prediction tasks  [36, 51]  (examples are discussed in Section 2.1), others aim to incorporate heuristics and information based on body parts. For example, interest points can be segmented based on location within the frame to heuristically distinguish body parts (e.g. head and arms)  [34] .\n\nAnother example is Mahmoud et al.  [40]  who use SURF keypoints from colour and depth images across a video to define tracklets for their rhythmic motion measure. Their dataset is based on Kinect captured videos to provide depth, this also provides skeletal regions such as feet and head. They use these Kinect skeletal regions to localise their keypoints' motion.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Body Modality Specific",
      "text": "The second approach extracts body modality specific interest points (e.g. skeletal models) to calculate features from. There are two primary methods for extracting these interest points: joint estimations from specialised capture equipment such as Kinects and pose estimation from existing video recordings. Such skeletal models have gained popularity in the past few years for action recognition tasks  [54, 17, 10, 48, 56, 18] .\n\nIn this dissertation I use pose estimation to extract body interest points (e.g. joints) from each frame in a two-dimensional video. In the past three to four years there has been substantial work on these pose estimation systems  [57, 7, 49, 60, 39] . The current state-of-the-art is OpenPose by Cao et al.  [8] . OpenPose uses skeletal hierarchy and part affinity fields to estimate pose interest points (i.e. joints) relative to each other and determine both part and orientation based on the direction of the proposed limb. The state-of-the-art model, at the time of writing, generates 25 interest points identifying the location of all major joints and more detailed information regarding head and feet. OpenPose can also be used to extract facial landmarks and detailed hand models.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Summary",
      "text": "I have reviewed methods for automatic depression detection and the variety of modalities and models used within the field. I have also discussed, in broad terms, methods used for working with body data within related fields, from variation in capture equipment to difference in core interest points.\n\nThe first component of the review outlined the gap within the existing use of the body modality that my proposed methodology addresses. While the second component outlined methods for working with the body modality, and specifically technology that I use to implement my methodology.",
      "page_start": 26,
      "page_end": 26
    },
    {
      "section_name": "Chapter 3 Dataset",
      "text": "To develop whole body features I need a dataset containing video recordings and distress labels. However, due to the sensitive nature of this data no existing dataset makes both the source video and distress labels publicly available. Though some datasets make video derived data available, such as facial landmarks, none include derived data for the participant's whole body. Given this I introduce a new dataset containing audio-visual recordings of interviews and labeled with distress and personality measures. This is not a clinical dataset, the interviews are performed by a computer science researcher and the labels are based on established psychology self-evaluation questionnaires.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Collaborators",
      "text": "Dr. Marwa Mahmoud from the Department of Computer Science at the University of Cambridge was the interviewer. Dr. Gabriela Pavarini from the Department of Psychiatry at the Univeristy of Oxford provided advice regarding the interview procedure for collection.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Existing Related Datasets",
      "text": "There are a number of relevant existing datasets, however none satisfy the requirements of my research. Namely that they: are available, include psychological distress labels, and include source videos.\n\nI provide an overview of four existing datasets, two clinical datasets, a distress focused dataset using facial, auditory, and speech modalities, and one affective computing body gesture dataset. Each of these datasets, and their related work, provide useful insight for my data collection and model development research.",
      "page_start": 27,
      "page_end": 27
    },
    {
      "section_name": "Clinical",
      "text": "Joshi et al. describe a clinical dataset  [33]  for depression analysis that contains recordings of participants' upper bodies and faces during a structured interview. It includes clinical assessment labels for each participant, including depression severity. This dataset was collected at the Black Dog Institute [6], a clinical research institute in Australia.\n\nJoshi et al.  [34]  describe a clinical dataset collected at the University of Pittsburgh containing full body recordings of participants in interviews, it uses Major Depressive Disorder (MDD)  [4]  diagnosis and Hamilton Rating Scale of Depression (HRSD)  [29]  as labels. The authors validate the use of full body features for depression prediction within this dataset.\n\nGiven the clinical nature of these datasets they are not publicly available.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Audio-Visual Distress",
      "text": "Gratch et al. introduce the Distress Analysis Interview Corpus (DAIC) dataset  [26]  containing audio-visual recordings of interviews with participants with varying levels of distress. Participants are assessed for depression, post traumatic stress disorder (PTSD), and anxiety based on self-evaluation questionnaires. The dataset provides audio data (and derivations such as transcripts) and extracted facial landmark data, though they do not provide source video data nor pose data. Source video recordings are rarely shared in distress focused datasets due to the sensitive of the data. The dataset contains four participant interview structures: face-to-face interviews with a human, teleconference interviews with a human interviewer, \"Wizard-of-Oz\" interviews with a virtual agent controlled by an unseen interviewer, and automated interviews with a fully autonomous virtual agent.\n\nThis dataset informs my dataset collection via the labels it contains and their interview structures. The DAIC method has participants complete a set of questionnaires, then participants are interviewed and recorded, and finally the participant completes another set of questionnaires. Within the interview the interviewer asks a few neutral questions to build rapport, then asks questions related to the participant's distress symptoms, and finally asks some neutral questions so the participant can relax before the interview finishes.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Body Gestures",
      "text": "Palazzi et al. introduce a audio-visual dataset of dynamic conversations between different ethnicities annotated with prejudice scores. Videos in this dataset contain two participants interacting in an empty confined space. Participants move around the space throughout the session, enabling analysis of body language affected during conversation. This dataset highlights, and focuses on, the effect attributes of a counterpart, such as race, gender, and age, have on a person's behaviour. In this dataset counterparts are explicitly non-uniform. Whereas, in my dataset the interviewer (i.e. counterpart for a participant) is the same for all interviewees. This is useful in controlling for the counterpart variable in human behaviour, supporting the isolation of correlations between distress and behaviour.\n\nDatasets such as this one are not useful for my current research question, however, as they provide source videos they present opportunities for future work investigating my features generalisability to other domains.",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 28,
      "page_end": 28
    },
    {
      "section_name": "Design",
      "text": "This dataset is designed to enable investigation of the body modality for use in automatic detection of distress for early screening. This is a non-clinical dataset.\n\nIts source data is audio-visual recordings of conversational interviews. The recordings capture the whole body of participants to enable features based on a whole body modality. These interviews involve questions related to distress to elicit emotive responses from participants, however the responses to these questions are irrelevant to the core data. The interviews use a conversational style to best enable naturalistic gestures from participants.\n\nLabels are scored results from established self-evaluation questionnaires for assessing distress and personality traits, as well as demographic labels such as gender. The distress questionnaires are: the PHQ-8  [38, 37]  for depression, GAD-7  [52]  for anxiety, SSS-8  [24]  for somatic symptoms, and the PSS  [11]  for perceived stress. Personality traits are measured using the Big Five Inventory  [32] .",
      "page_start": 29,
      "page_end": 29
    },
    {
      "section_name": "Participants",
      "text": "Recruitment I advertised for participants via University of Cambridge email lists, student social media groups, classified sections of websites, such as Gumtree  [27] , specific to the Cambridge area, and paper fliers posted around the University of Cambridge. Participants were directed to a website 1  describing the study along with a participant registration form.\n\nThe registration form captured demographic data and two self-evaluation psychological distress questionnaires. Demographic data captured includes gender, age, ethnicity, and nationality. Gender and age were required while ethnicity and nationality were not. The two psychological distress questionnaires were the PHQ-8  [38, 37]  and GAD-7  [52] .",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Selection",
      "text": "In total 106 people registered to participate and 35 were invited to the face to face session. The participant population is balanced with regards to distress levels and gender 2  . Distress level balancing aims to include participants at the extents of the distress spectrum such that there is a distinct difference between the high and low distress populations. Participant distress level is selected based on PHQ-8 and GAD-7 questionnaire responses such that participants are balanced between high (i.e. major or severe) and low (i.e. mild) distress. Of the invited participants, there are 18 with high distress and 17 with low distress.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Compensation",
      "text": "Participants are compensated for their time with a Â£15 voucher.",
      "page_start": 30,
      "page_end": 30
    },
    {
      "section_name": "Face To Face Session",
      "text": "During the face to face session participants sign a research consent form outlining the interview process, complete a battery of five established psychology questionnaires evaluating distress levels and personality traits, are interviewed by a researcher, and finally sign a debrief research consent form that outlines the full purpose of the study. Participants are not aware of the focus of the research (i.e. body modality analysis) before the interview such their affectations are natural.\n\nTo achieve the conversational interview dynamic the interviewer asks general questions regarding the participant's life and further encourages the participant to elaborate. For example, the interviewer asks \"can you tell me about one time in your life you were particularly happy?\" and then asks follow up questions regarding the example the participant provides. The interview style and structure is inspired by the DAIC dataset. In developing the interview structure and questions I also drew on training documents provided by Peer2Peer Cambridge [44], an organisation dedicated to peer support for mental health which trains students in general counseling.\n\nSo as to avoid influencing participants' behaviour the interviewer remains as neutral as possible during the interview, while still responding naturally such that the participant is comfortable in engaging in the questions. Furthermore, to ensure neutrality the interviewer is explicitly not aware of the distress level of participants before the interview and has no prior relationships with any participant.",
      "page_start": 30,
      "page_end": 31
    },
    {
      "section_name": "Technical Faults",
      "text": "16 interviews are interrupted due to technical faults 3  . Recordings that are interrupted are treated as multiple individual samples within the dataset (though they remain connected by their participant ID).",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Preliminary Analysis",
      "text": "The dataset contains a total of 35 interviewed participants with a total video duration of 7 hours 50 minutes and 8 seconds. Each participant provides responses to 5 questionnaires, including 2 responses to both the PHQ-8 and GAD-7 questionnaires as participants completed both during registration and the face-to-face session.\n\nThough significantly more people registered for participation, I include only those interviewed in this analysis.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Validation Criteria",
      "text": "There are three primary criteria the dataset should satisfy with regards to label results:\n\n1. The psychological measures statistically match previous work and published norms  4 (i.e. the distribution within the participant population is similar to that of the general population).\n\n2. There are no confounding correlations. For example, gender correlating highly to depression would indicate a poorly balanced dataset and would be confounding for depression analysis.\n\n3. The labels are well balanced to enable machine learning.\n\nAs the common measure of similar distress detection research is depression I focus primarily on it for this validation.\n\nGeneral statistics regarding the questionnaire and demographic results within the dataset are provided in Table  3 .1. Covariance is presented as normalized covariance values, also known as the correlation coefficient.",
      "page_start": 31,
      "page_end": 31
    },
    {
      "section_name": "Published Norms",
      "text": "A comparison of the mean values for distress and personality measures between my dataset and the published norms is presented in Table  3   are generally in line with the published norms. The dataset has slightly higher mean distress scores, though a substantially higher mean perceived stress score. Depression, extraversion, and neuroticism measures are particularly close to their published norms. While the dataset mean for agreeableness and openness are substantially greater than the published norms (over 10% over the technical range for those measures).",
      "page_start": 31,
      "page_end": 32
    },
    {
      "section_name": "Confounding Correlations",
      "text": "While the other distress measures (anxiety, perceived stress, and somatic stress) are strongly correlated with depression, the personality measures have below 50% covariance with the exception of neuroticism which has an 80% covariance. Furthermore, the demographic measures, gender and age, are negligibly correlated, with 9.47% and -11.09% covariance, respectively. This suggests that the labels are not confounding of each other.",
      "page_start": 32,
      "page_end": 32
    },
    {
      "section_name": "Label Balance",
      "text": "There are 17 participants below the mean depression result (7.43) and 18 participants above. The mean depression score of the group below the overall mean is 2.18 while the score for those above is 12.39. Ideally for machine learning the dataset's distribution would include more participants at the severe depression end of the spectrum, though the present  distribution still places the below group firmly in the \"mild\" category and the above group in the \"major depression\" category. There are 18 male and 17 female participants. As the gender covariance shows, the split on the depression measure and the split on gender are not the same participants (gender is balanced across the distress spectrum).",
      "page_start": 32,
      "page_end": 33
    },
    {
      "section_name": "Difference From Registration",
      "text": "Participants complete the PHQ-8 and GAD-7 questionnaires during registration and during the interview process. These questionnaires are temporal, specifically, they relate to the participant's mental state in the past two weeks. Given this, some difference between registration and interview results is expected.\n\nWith the exception of a small number of outliers, participants were generally consistent in self-evaluation between registration and interview. PHQ-8 responses have a mean difference of 0.89 while GAD-7 responses have a mean difference of 0.63. This supports the selection of participants based on temporal self-evaluation questionnaire results.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Interview Meta Statistics",
      "text": "There is a total of 7 hours 50 minutes and 8 seconds of participant interview recordings, with a mean interview duration of 13 minutes and 25 seconds. The standard deviation of interview duration is 3 minutes and 20 seconds and the median interview duration is 13 minutes and 8 seconds. Depression score and interview duration are not correlated, with a covariance of 6.95%. Furthermore, interview duration is not correlated with any questionnaire result (i.e. distress or personality measure), all absolute covariance values are below 25%, which provides confidence in the reliability of the data.",
      "page_start": 33,
      "page_end": 33
    },
    {
      "section_name": "Summary",
      "text": "I have introduced a new audio-visual dataset containing recordings of conversational interviews between participants and a researcher, and annotated with established psychology self-evaluation questionnaires for depression, anxiety, somatic symptoms, perceived stress, and personality traits. This dataset involves 35 participants and 65 recordings (due to recording interruptions) with a total video duration of 7 hours 50 minutes and 8 seconds.\n\nThere are a number of relevant existing datasets including clinical datasets which contain body gestures but are inaccessible beyond their home institute, distress datasets that contain facial expressions and speech modalities but no body gestures or source videos, and video datasets containing body gestures but lacking distress labels. While these datasets inform my dataset design and collection, no dataset I am aware of satisfies the criteria for research on body gesture modalities for predicting distress.\n\nAn analysis of the questionnaire results in the dataset show they are aligned with psychology literature published norms, they are not confounded by factors such as gender or age, and have a useful and balanced distribution across the distress spectrum. Having collected the dataset, I now describe my methodology for extracting gesture meta features and applying them to automatic distress detection. To this end I define four core stages of my methodology, from video to prediction (as shown in  Pose estimation extracts per-frame approximations of skeletal joint locations. This enables more accurate gesture analysis than direct pixel based approaches (such as STIPs per Joshi et al.'s method  [36] ).",
      "page_start": 34,
      "page_end": 35
    },
    {
      "section_name": "Chapter 4 Method",
      "text": "",
      "page_start": 35,
      "page_end": 35
    },
    {
      "section_name": "Pose Estimation",
      "text": "I process each video to extract per-frame skeletal pose data using OpenPose",
      "page_start": 36,
      "page_end": 36
    },
    {
      "section_name": "Data Preparation",
      "text": "I perform three data preparation steps: filtering, recovery, and smoothing. Filtering smooths dataset-level noise, recovery fixes outlier noise (where outliers are detection failures for specific pose points, but not the whole body), and smoothing reduces detection noise.\n\nExtracted pose estimation data has two primary forms of noise: individual frames, or short segments of frames, where detection of a person, or part of a person, is lost, and detection points moving around slightly even if the person is static. Manual review of a selection of detection loss cases shows no consistent cause (for both complete detection loss and partial loss). It appears to be the deep learning model failing inexplicably on some frames.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Filtering",
      "text": "The absence of, or low representation of, gestures is relevant information for prediction tasks (simply put, if more activity is relevant then less activity must be too). However, the absence of gestures can also be due to a lack of opportunity to express gestures, such as when a sample is too short. These short samples lead to dataset-level sample noise that can hinder predictive models.\n\nSamples shorter than 1 minute are excluded as it is difficult to provide enough opportunity for gesture dynamics in less than 1 minute of video. 12 out of 65 samples within the dataset are shorter than 1 minute.",
      "page_start": 37,
      "page_end": 37
    },
    {
      "section_name": "Detection Recovery",
      "text": "Pose estimation within my dataset contains frames where certain pose points (e.g. an arm or a leg) are not detected. In these cases OpenPose returns 0 for each pose point not detected. Manual review of a number of instances shows that the joint does not moved much, if at all, during the lost frames. However, the pose point \"moving\" to position 0 causes significant noise in feature calculation.\n\nTherefore, I perform detection \"recovery\" to infer the position of the pose point in the missing frames, thus providing a smoother pose point movement. I recover the position by linearly interpolating the pose point's position between the two closest detected frames temporally surrounding the lost frame(s).\n\nIt is worth noting that this pose point detection failure is different to full detection failure where the whole participant is not detected within a frame. I do not attempt to recover such full failure cases as the failure is more serious and cause is ambiguous. I do not want to introduce stray data by \"recovering\" significantly incorrect data. Partial failures suggest a simple failure of OpenPose to extend through its body hierarchy. Since other pose points are still detected I am more confident that it is not a \"legitimate\" failure to do with participant position. Instead, full failure cases are treated as \"separators\" within a sample. Gesture detection occurs on either side but not across such separators.",
      "page_start": 37,
      "page_end": 38
    },
    {
      "section_name": "Detection Smoothing",
      "text": "To extract more accurate and relevant features I smooth the pose data by removing high frequency movement within pose points. Such high frequency movement of pose points is caused by OpenPose's detection noise (i.e. the exact pose point might move back and forth by a few pixels each frame while its target joint is static). Thus smoothing is not smoothing human movement, but rather smoothing pose extraction noise. To smooth the data I apply a fourier transform filter to each dimension for each pose point. The smoothing steps are:\n\n1. Separate the data into x and y positions and smooth them (i.e. apply the following steps) independently.\n\n2. I convert the position sequence data using a fourier transform with a window length of 64.\n\n3. Set the medium and high frequency values (all frequencies above the first five) to 0.\n\n4. Invert the fourier transform on the updated fourier values to reconstruct the smoothed pose data.\n\n5. Concatenate the smoothed windows.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Feature Extraction",
      "text": "I extract two types of features: aggregate features across the whole body and localised features for specific body parts, which I term \"body localisations\". The localisations are: head, hands, legs, and feet. As the participants are seated the body trunk does not move substantially such that a gesture might be detected. The whole-body features include aggregations of the body localisation features as well as features incorporating the whole body.\n\nBy including localised and non-localised features I can model the information provided by individual body parts and also the overall behaviour.",
      "page_start": 38,
      "page_end": 38
    },
    {
      "section_name": "Gesture Definition",
      "text": "I define a gesture as a period of sustained movement within a body localisation. Multiple body localisations moving at the same time are treated as multiple individual, overlapping, gestures.\n\nA gesture is represented as a range of frames in which the target body localisation has sustained movement.\n\nFor example, if a participant waves their hand while talking it would be a hand gesture. If they were to cross their legs it would register as a gesture in both the leg and feet localisations.",
      "page_start": 39,
      "page_end": 39
    },
    {
      "section_name": "Whole Body Features",
      "text": "â¢ Average frame movement -the per-frame average movement of every tracked pose point (i.e. whole body). This is the only feature that is not based on detected gestures.\n\nâ¢ Proportion of total movement occurring during a gesture -the proportion of total movement (i.e. the whole body) that occurred while some body localisation was affecting a gesture.\n\nâ¢ Average gesture surprise -gesture surprise is calculated per-gesture as the elapsed proportional time since the previous gesture in the same localisation, or the start of the sample (proportional to length of the sample) for the first gesture. This overall feature averages the surprise value calculated for every gesture across all tracked localisations. I use the term \"surprise\" as the feature targets the effect on a gesture level basis, rather than the sample level. This is not a measure of how much of a sample no gesture is occurring as it is normalised on both the sample length and the number of gestures 2 .\n\nâ¢ Average gesture movement standard deviation -the standard deviation of per-frame movement within a gesture is averaged across all detected gestures. This is intended to indicate the consistency of movement intensity through a gesture.\n\nâ¢ Number of gestures -total number of detected gestures across all tracked localisations.\n\n2 To illustrate further: if 2 gestures occurred within a sample such that 80% of the sample duration had no gesture occurring, the average gesture surprise would be 80 2 = 40. Whereas, if there were 100 gestures, still with 80% of the sample with no gesture occurring, the average surprise be 0.8%, even though both samples had the same proportion without any gesture occurring. This matches the intuition that each gesture within 100 evenly spaced gestures would be unsurprising as they were regularly occurring, whereas the 2 evenly spaced gestures would be surprising because nothing was happening in between.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Localised Features",
      "text": "Whole body and localised features are concatenated in the same feature vector. Localised features are included in the final feature vector for each localisation included in the vector.\n\nâ¢ Average length of gestures -the average number of frames per gesture.\n\nâ¢ Number of gestures -the total number of gestures, irrespective of gesture length.\n\nâ¢ Average per-frame gesture movement -the average movement across all gestures.\n\nâ¢ Total movement in gestures -the total amount of movement affected by the detected gestures.\n\nâ¢ Average gesture surprise -the average surprise across all gestures.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Normalisation",
      "text": "All features are normalised such that the length of the sample does not affect the results. I normalise sum based features (e.g. gesture length, gesture count, total movement, etc) against the total number of frames in the sample and against the total number of gestures for gesture average values. For example, gesture surprise is normalised against the total number of frames and normalised a second time against the total number of gestures.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Absent Features",
      "text": "If a feature has no inputs (such as when no gesture was detected within a body localisation) its value is set to -1 to enable models to incorporate the absence of movement in their predictions.",
      "page_start": 40,
      "page_end": 40
    },
    {
      "section_name": "Body Localisation",
      "text": "Gestures in different body localisations provide distinct information. Aggregating gestures from different localisations provides a general representation of this information, however, having features localised to specific body localisations provides further information, without significantly increasing the dimensionality.\n\nI define four localisations, hands, head, legs, and feet, based on specific pose estimation points.\n\nHands I use the finger tip points (including thumb) detected by OpenPose as the gesture detection points. This means wrist based gestures (e.g. rolling of a hand) are detected. Each hand is processed separately, that is, I detect gestures and calculate individual gestures independently in each hand, these gestures are then aggregated into a single body localisation feature vector. This makes the final features side agnostic. This ensures differences in dominant hand between participants will not affect the result.\n\nHead While OpenPose includes face detection, providing detailed facial landmarks, I use the general head position points provided by the standard pose detection component.\n\nLegs I represent legs using the knee pose points from OpenPose. As with hands, I process gestures in each leg independently and then aggregate to a single feature vector.\n\nFeet Each foot is comprised of four pose points within OpenPose. Aggregation is the same as hands and legs.\n\nTrunk I do not include a \"trunk\" localisation as there is minimal movement in the trunk, given the seated nature of the dataset interviews. Though some participants may lean forwards and backwards, these movements are not represented well within my data as the camera faces the participants directly such that forwards and backwards leaning would be towards the camera, thus requiring depth perception which is not included in my data. Side to side leaning is restricted by the arms of the participant's chair. As such the localisations that are relatively free to move, those other than the trunk, are intuitively the most likely areas to provide predictive information.",
      "page_start": 41,
      "page_end": 41
    },
    {
      "section_name": "Gestures",
      "text": "",
      "page_start": 41,
      "page_end": 41
    },
    {
      "section_name": "Gesture Detection",
      "text": "To detect gestures within a set of pose points (e.g. finger tips, knees, feet, head, etc) I scan the activity of the target points for sustained periods of movement. The gesture detection step takes cleaned per-frame pose estimations and outputs a collection of ranges of non-overlapping frames that contain gestures within the localisation.\n\nFirst, the per-frame absolute movement delta is calculated for each pose point. The movement is then averaged across all localisation pose points. Movement deltas are L 2 distances. Formally, M p,t = P p,t -P p,t-1\n\nwhere M p,t is the amount of movement for pose point p at time t, P p,t is the position value of pose point p at time t, and F t is the averaged per-frame movement across all points. Second, I average the movement of each frame within a window such that a small number of frames do not have a disproportionate effect on the detection. That is,\n\nwhere W i is the windowed average at window index i, l is the length of the window, and F t is the average movement at frame t, from Equation  4 .1. In this dissertation I use l = 10, i.e. a second of movement is represented by 3 windows, this is experimentally chosen. Third, the detector iterates through the averaged windows until a window with an average movement above a threshold is found. The first frame of this initial window is considered the beginning of the gesture. The gesture continues until n consecutive windows (I use 3, i.e. 30 frames, as an approximate of a second) are found below the movement threshold. The last frame of the final window above the movement threshold is considered the end of the gesture. This is provided more formally in Algorithm 1.\n\nHaving detected gestures in each body localisation I extract the features described above from each gesture and aggregate them to form the final feature vector.",
      "page_start": 41,
      "page_end": 42
    },
    {
      "section_name": "Feature Space Search",
      "text": "I have described a collection of novel features whose individual and mutual predictive value is as yet unknown. Some features are potentially unhelpful to predictive models. Therefore, distinguishing useful features from unhelpful features is a key operation to enable development of accurate models, and thus validate these features. To this end, I perform an exhaustive search of the feature space to identify the combination of features with the best performance.\n\nAlghowinem et al.  [1]  demonstrate the benefits of a variable feature set, achieving accuracy improvements of up to 10%. Their classification method is based on SVMs and the dimensionality reduction enabled by feature filtering is significant. They use a statistical T-threshold based approach to feature selection. However, Dibeklioglu et al.  [16]  argue that basing selection on optimization of mutual information will achieve better results than individual information based selection. I follow this mutual information approach and thus feature selection is based on the results achievable given a combination of features, rather than features' individual relevance.\n\nI define few enough features such that a brute force feature combination space search (i.e. test every permutation) is viable. As each feature can be included or excluded the space has 2 n permutations, where n is the number of features being searched. if W â¥ m then // Start the gesture on the first window that exceeds the movement threshold.",
      "page_start": 42,
      "page_end": 42
    },
    {
      "section_name": "9:",
      "text": "if start â¡ N U LL then 10:\n\nbelowT hresholdCount â 0 12:\n\nbelowT hresholdCount â belowT hresholdCount + 1 // A gesture is completed after n consecutive windows below the movement threshold.",
      "page_start": 43,
      "page_end": 43
    },
    {
      "section_name": "14:",
      "text": "if belowT hresholdCount â¡ n then // The end of a gesture is the final window that exceeds the threshold.",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "15:",
      "text": "end â i -n 16:\n\nif end -start â¥ l then 17:\n\ngestures append [start, end] // Reset to find the next gesture. gestures append [start, end] I iterate over every permutation and perform three fold cross validation using the combination of features, the permutation with the greatest average cross validation F1 score is taken as the best permutation, which enables testing and evaluating the effectiveness of the proposed features.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Summary",
      "text": "In this chapter I have described my methodology for extracting gesture meta features from videos. The four core stages are: pose estimation, data preparation, feature extraction, and finally classifier training. I use OpenPose to extract pose data per-frame as it is the stateof-the-art in pose estimation. I then filter samples and perform two operations to reduce noise within the remaining samples: recovery of partial detection failures and smoothing of high frequency detection noise. Features are extracted by detecting individual gestures, calculating per-gesture features such as speed, and aggregating per-gesture features within their localisations and over the whole body. Classifier choice is an evaluation detail and discussed in the next chapter.",
      "page_start": 44,
      "page_end": 44
    },
    {
      "section_name": "Chapter 5 Evaluation",
      "text": "Having described my gesture meta features, I now evaluate their predictive capability on the dataset introduced in Chapter 3. Section 5.1 describes implementation details of my evaluations. Section 5.2 defines a simple baseline using a single feature. In Section 5.3 I evaluate the features' potential using the exhaustive feature search described in Section 4.4. Section 5.4 investigates the influence of different body localisations on the features' effectiveness. I then demonstrate the features' generalisability beyond the primary depression detection task in Section 5.5. In Section 5.6 I compare the features' automatic depression detection performance with an existing body-modality method and a straightforward face-modality method. In Section 5.7 I experiment with a multi-modal classifier that combines the gesture meta features with face-modality features. Finally, I summarize the results in Section 5.8.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Implementation Details",
      "text": "Before presenting evaluation results, I outline the details of the evaluation setup.",
      "page_start": 45,
      "page_end": 45
    },
    {
      "section_name": "Evaluation",
      "text": "I perform evaluations using a three fold cross validation. The training and test samples are split in a participant-independent manner and use stratified folding to balance them with regards to labels. Cross validating with more folds leads to fewer test samples per fold. Given the small size of the dataset, this can lead to more erratic performance (i.e. more extremes in cross validation results). I assess results based on the average cross validation F1 score and the standard deviation between fold F1 results. Average F1 provides an objective measure of model quality. Whilst the standard deviation provides an indicator of the consistency of the model. Table  5 .1: Binary classification thresholds for distress and personality labels.",
      "page_start": 45,
      "page_end": 46
    },
    {
      "section_name": "Classifier Models",
      "text": "I evaluate four types of classifiers on all tasks:\n\nâ¢ A linear regression based classifier (denoted as lin) using a classification threshold of 0.5.\n\nâ¢ A logistic regression based classifier (denoted as log) using the L-BFGS solver.\n\nâ¢ A linear kernel SVM (denoted as svm) with balanced class weighting (without balancing the class weightings the classifier consistently chooses to predict a single class on every fold).\n\nâ¢ A random forest (denoted as rf) with 40 trees, feature bootstrapping, a minimum of 3 samples per leaf, a maximum depth of 5, balanced class weighting, and exposing 80% of features per node. These parameters were chosen experimentally.",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Labels",
      "text": "I only evaluate binary classification tasks (i.e. high vs. low depression score) within this dissertation. However, the dataset contains continuous values for distress and personality measures, thus a constant threshold is required for each label (similar to the participation selection criteria discussed in Section 3.2.2). These thresholds are chosen such that the resulting binary classes are as balanced as possible. Given the small size of the dataset, balancing the classes is important to classifier training. Per-label thresholds are reported in Table  5  original questionnaire scoring categories could be applied to the regressed predictions. Though I evaluate the features' predictive capability against multiple labels, my primary focus is on depression detection, as this is the area that most of the related work discusses. An evaluation the features' capability on other labels is presented in Section 5.5.",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Localisations",
      "text": "Though I evaluate four localisation types: head, hands, legs, and feet, the feet localisation has a negative effect on performance, shown in Section 5.4. As such, all other sections use the three useful localisations: head, hands, and legs, in their evaluations.",
      "page_start": 47,
      "page_end": 47
    },
    {
      "section_name": "Feature Reporting Notation",
      "text": "To concisely report features used by different classification models I describe a brief notation for enumerating features. The notation defines tokens for localisations, feature type, and how the lin model interprets the feature. The structure is \"[localisation]-[feature type][linear polarity]\". Localisation and feature type token mappings are provided in Table  5 .2.\n\nI define an indicator of how a model interprets a feature based on its contribution to a positive classification. A greater value (e.g. more activity) contributing to a positive classification is denoted by \"+\". Conversely a greater value contributing to a negative classification is denoted by \"Â¬\". A value which has a negligible effect (defined as a linear Model F1 avg F1 std lin 34.43% 11.45% log 34.43% 11.45% svm 33.82% 10.73% rf 64.29% 5.83% Table  5 .3: F1 aggregate scores for models using the baseline feature on the depression detection task. The rf model achieves the best performance. In this evaluation only one feature is used, this means that the feature is available to every decision node in the random forest, enabling strong overfitting. model applying a near-zero coefficient) is denoted by \"/\". Finally, if the lin classifiers for each cross-validation fold are inconsistent in usage of the feature the \"?\" indicator is used.\n\nFor example, F-GSÂ¬ would denote that a greater amount of surprise in feet gestures indicates a negative classification.",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Baseline",
      "text": "As a baseline I evaluate the use of the only non-gesture based feature, average per frame movement (O-FM). Results are given in Table  5 .  3 .\n\nWhile the rf model achieves the best results, it is inconsistent across its folds. Its 3 F1 scores are: 70.06%, 60.00%, and 47.06%. However, it does suggest that the movement feature alone is valuable for prediction. Though, the worse than random results achieved by the other models suggest that movement is not a linear indicator. It is possible, indeed likely, that the rf model's result is reflective an overfitting of the dataset.",
      "page_start": 48,
      "page_end": 48
    },
    {
      "section_name": "Feature Search",
      "text": "I evaluate the effect of the feature space search to identify the best feature combination.",
      "page_start": 49,
      "page_end": 49
    },
    {
      "section_name": "All Features",
      "text": "Table  5 .4 presents results for each model when provided with the full feature vector unmodified. Given all features the lin, log, and svm models all improve on their baselines, while the rf model is worse than its baseline. The reduction in performance by the rf model can be attributed to the overfitting ability of random forests. This ability is especially prevalent with single feature problems as the feature must be made available to every node within the forest, thus enabling direct fitting of multiple decision trees to the   5 .5: Performance of the best feature combination as determined by an exhaustive feature space search using the lin classifier to detect depression. This demonstrates the power, and the linearity, of the gesture meta features as the lin classifier is able to achieve a high F1 score.\n\ndataset. The lin model achieves significantly better-than-random performance, indicating that the features have some linear predictive capability.",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Searched Features",
      "text": "I perform an exhaustive feature space search to identify the best combination of features (determined by the average cross-validation F1 score). This provides two important outcomes: the best accuracy possible for a given model using these features within my dataset and the most relevant features to predict depression within my dataset. A good accuracy from the first outcome validates the features I have developed. The second outcome then provides a basis for further analysis of these features and their relation to depression within my dataset.\n\nThis search requires a model fast enough to train that the full space can be searched in a viable amount of time and, preferably, a model that is interpretable. For these reasons I use the lin model to search the feature space, and then evaluate the best feature combination with the other classifier types.",
      "page_start": 49,
      "page_end": 49
    },
    {
      "section_name": "Feature Search Improves Best Performance",
      "text": "The best performance when applying the feature search, again from the lin classifier, is significantly better, 82.70%, than the classifier's baseline of 34.43% and all features baseline of 66.81%. This demonstrates the importance of reducing dimensionality and feature confusion, especially when using relatively direct methods such as linear regression. Results are provided in Table  5 .5, a comparison of these results to the baseline results is presented in Figure  5 .1.",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Chosen Features",
      "text": "The full feature combination is: {O-FM?, O-GM+, O-GC?, Hn-GC?, Hn-GTÂ¬, Hn-GS?, He-GL?, He-GC?, He-GA+, He-GTÂ¬, He-GS?, L-GLÂ¬, L-GC?, L-GT+}. Analysing this feature set, we can derive some intuition as to information indicative of depression. The overall (O-*) features suggest that the number of gestures (O-GC) and the amount of movement within those gestures (O-GM) is relevant to depression. The O-GM+ token suggests that more movement within gestures relative to all other movement is indicative of depression. The localised features suggest that the length of gestures (*-GL) has a correlation with depression, however, this correlation differs between localisations. The head localisation is ambiguous as to whether shorter or longer gestures (He-GL?) is indicative of depression. Whilst longer gestures in the legs localisation (L-GLÂ¬) is indicative of less depression. Within this model, less total movement of the hands (Hn-GTÂ¬) is indicative of distress.",
      "page_start": 50,
      "page_end": 50
    },
    {
      "section_name": "Negative Performance Impact On Other Models And Overfitting",
      "text": "The identified feature set is chosen using the lin model, so it is unsurprising it has a greater improvement than any other model. While the log classifier's performance does not change much, the svm and rf classifiers have reduced performance compared to their all features and one feature baselines, respectively. There are two aspects to consider here: the value of pre-model filtering to each model and the potential for each model to overfit. Focusing on the rf classifier as it has a more distinct reduction in performance; random forests have inbuilt feature discretion, so the pre-model filtering of features does not have as great a complexity reducing effect as it does on the other models. My hyper-parameters give the rf model a relatively large selection of features (80%) per node, thus it should generally filter, to some extent, those naturally unhelpful features. Random forests, as decision tree ensemble methods, have a propensity for overfitting data. By reducing the number of features available I reduce the available surface for overfitting. Moreover, when only using one feature, as in the baseline, every decision node in the random forest has access to the feature, thus enabling particularly strong overfitting of a small dataset.",
      "page_start": 51,
      "page_end": 51
    },
    {
      "section_name": "Dimensionality Compression",
      "text": "I do not perform any dimensionality compression operations in my presented evaluations. However, I have experimented with Principle Component Analysis (PCA) both independently and in combination with a feature search. Neither approach achieved especially interesting results, all were worse than when not applying PCA. Given this, and the already relatively low number of dimensions, I do not see it as a critical path of investigation for this dissertation. Figure  5 .2: Comparison of the best F1 average scores from localisation combinations using the lin classifier using all features. The most interesting results are the bottom four (vertically) localisation results: head -legs, hands -head -legs, feet -head -legs, and hands -feet -head -legs. Specifically, the feet localisation impairs the performance of the localisation combinations when included. This trend is also seen in feet -legs and feet -head.",
      "page_start": 51,
      "page_end": 51
    },
    {
      "section_name": "Body Localisations",
      "text": "Not all localisations are necessarily beneficial. Identifying which localisations are helpful is made more difficult by the localisations' interactions within a classifier and the effect they have on the overall features. Though a localisation may generate features that are chosen using feature search, they may reduce overall accuracy by obfuscating predictive information in the overall features.\n\nGiven this, I experiment with localisations included individually and in varying combinations. I also provide an example of a localisation, feet, that negatively effects performance when included, even when all other localisations are also included (and are thus providing the same predictive information). A comparison of the best F1 scores for localisation combinations is presented in Figure  5 .2.",
      "page_start": 52,
      "page_end": 52
    },
    {
      "section_name": "Localisation Inclusion",
      "text": "Clearly not all of the features generated per localisation provide useful information. Inclusion of more localisations, and thus a larger feature space, does not guarantee a better optimal result is available within the space. As the overall features (those aggregated across Model F1 avg F1 std lin 80.53% 4.04% log 59.52% 8.91% svm 65.40% 8.40% rf 51.32% 5.76% Table  5 .6: Performance of models when using features chosen via exhaustive search with source features from the head -legs configuration. This configuration achieves close to the best performance of the standard configuration (Table  5 .5). The lin classifier also has more consistent performance across cross validation folds than it does on the standard configuration, with a standard deviation of 4.04% compared to 8.95%. However, these results do not clearly define which configuration is generally better as the differences are quite minor.\n\nall localisations) are effected by each localisation, the quality of the feature space can be degraded with the inclusion of localisations. For example, this occurs regularly in Figure  5 .2 when including the feet localisation. In particular, the best base configuration, headlegs using the lin classifier, achieves a 70.88% F1 score, when the feet localisation is included this drops to 49.84%. I have not identified any psychology literature, or clear intuition, as to why the feet localisation hinders performance. I see three probable explanations: 1) some literature does support this and I have simply not identified the literature, 2) this is accurately representing that feet movement meta information does not distinctively change with distress, but no literature has explicitly investigated this, and 3) this is simply a attribute of the dataset that is not reflective of any broader trend, either due to the dataset size or the nature of the interview dynamic.",
      "page_start": 52,
      "page_end": 53
    },
    {
      "section_name": "Best Base Performance Configuration",
      "text": "Though the head -legs configuration achieves the best performance when all features are used, it does not achieve the best performance when features are chosen based on an exhaustive search. While my primary configuration, hands -head -legs, achieves 82.70% F1 average, the head -legs configuration achieves 80.53%, results are presented in Table  5 .6.",
      "page_start": 54,
      "page_end": 54
    },
    {
      "section_name": "Generalisability",
      "text": "I have demonstrated the gesture meta features' predictive value with regards to depression detection. I now evaluate their predictive capability for other labels including participants' gender, personality measures, anxiety, perceived stress, and somatic stress. I apply the best feature combination identified in Section 5.3 to each of the labels, Figure  5 .3: Comparison of F1 average scores when using the optimal feature combination for the depression label vs. the optimal feature combination for each label. Each label has a significant performance improvement when using its optimal feature combination.\n\npresented in Table  5 .7. I also perform a feature space search for each label, using the same process as Section 5.3, to provide greater insight into the effect of features and their reliability across labels, presented in Table  5 .8. A comparison is shown in Figure  5 .3. Consistent identification of features across uncorrelated labels reinforces the hypothesis that they provide predictive information beyond a specific label and dataset (i.e. are less likely to be overfitting).",
      "page_start": 55,
      "page_end": 55
    },
    {
      "section_name": "Results",
      "text": "The depression feature combination does not generalise well to the other distress measures for the lin model, though it achieves 62-71% F1 scores for neuroticism, agreeableness, and conscientiousness. Interestingly, the other labels' best results using the depression combination are above 60% F1, with the exception of the other distress measures, which only achieve a best of 53-54% F1. This is surprising as the distress labels are strongly correlated to the depression label. However, when features are chosen on a per-label basis the results improve significantly. All labels 1  achieve 76%+ (all but one are above 80%) average F1 score with their best  These results suggest that the depression chosen feature combination does not generalise particularly well. These results are also surprising as the distress labels (anxiety, perceived stress, and somatic stress), which are correlated to the depression label, perform poorly, whilst uncorrelated labels (such as agreeableness, openness, and gender) perform better. This may be due to overfitting of feature profiles to labels (i.e. the optimal features for the label) or truly distinct feature profiles between the correlated distress labels, though the former appears more probable.  classifier. The best classifier for all labels is the lin classifier, which is to be expected given it is the search classifier and previous evaluation has shown the features provide useful linear information.",
      "page_start": 54,
      "page_end": 54
    },
    {
      "section_name": "Fitting Features To Labels",
      "text": "There are two potential explanations for the substantial improvement in performance when using label specific feature combinations: each label could legitimately have a unique feature profile through which its distinctive attributes are expressed or the labels could be experiencing a level of overfitting due to the small size of the dataset. While it is likely to be a combination of the two reasons, labels that are relatively uncorrelated with depression are still able to achieve good performance using the depression chosen features, suggesting it is not just overfitting. For example, agreeableness, extraversion, and conscientiousness all have covariances of less than 50% with depression, yet they achieve 72.61%, 67.95%, and 79.19% average F1 scores, respectively, with the depression features. Openness which has a 4.29% covariance with depression achieves 65.08% with the depression chosen features. Furthermore, as Table  5 .9 shows, the openness feature combination shares only 6 out of its 9 features with the depression combination, whilst it also excludes 8 out of 14 of the depression combination's features. Therefore, it can be reasonably suggested that the features do generalise, acknowledging that fitting the features directly to a label achieves the best performance, as would be expected with all prediction tasks.",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Cross Classifier Generalisability",
      "text": "Within the depression feature combination results, the lin, log, and rf models all achieve the best results on at least 2 labels each. The svm classifier performs close to the best on many labels, such as conscientiousness where it achieves 78.77% compared to the best result of 79.19%. The svm model performs better when classifying conscientiousness, extraversion, and agreeableness, using the depression feature combination, than it does when predicting the depression label. It is also better at transferring the depression feature combination to other labels than the lin model that the feature combination was chosen with. Its performance improves on most labels when using the label targeted feature sets, such as on gender where it achieves 76.71% and somatic stress with 68.08% F1, whilst with the depression feature combination its F1 result was less than 50% for both. Interestingly, whilst the svm model performed particularly well on agreeableness using the depression feature combination, it performed worse when using the label targeted feature combination.",
      "page_start": 56,
      "page_end": 56
    },
    {
      "section_name": "Personality",
      "text": "Within the results for the depression feature combination the conscientiousness label achieves the best average F1 score across all classifiers, 75.55%, with an average standard deviation of 6.45%. Unlike the distress evaluation questionnaires, the BFI (personality) questionnaire is designed such that responses should remain relatively consistent over time. Future work could investigate whether the features are able to consistently predict personality measures over time for a participant. Do the features remain predictive of personality as a person's temporal distress measures change?",
      "page_start": 57,
      "page_end": 57
    },
    {
      "section_name": "Chosen Features",
      "text": "Table  5 .9 presents the feature sets resulting from the per-label feature searches. There are some consistencies and intuitive inversions within the feature sets. For example, the head localised average gesture movement (He-GA) is relevant for many of the labels, though in some it is inconsistent whether greater or lesser is indicative of the label (instances of inconsistencies are denoted as He-GA? within Table  5 .9). The feature usage is inverted between neuroticism, where a faster average speed is indicative of a positive classification, and extraversion and agreeableness, where a slower average speed is indicative of positive classification.\n\nFurthermore, as stated above, features that are chosen by uncorrelated labels support the hypothesis that these features are providing useful information. For example, of the 7 features the gender label chooses, 5 are also chosen by the anxiety label, including hand total gesture movement (Hn-GT), hand gesture surprise (Hn-GS), and leg gesture surprise (L-GS). Whilst gender and anxiety have a covariance of 7.23% within the dataset.",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "Movement",
      "text": "I represent four measures of movement: average movement per-frame (i.e. speed), standard deviation of movement (i.e. consistency of speed), total movement both over an entire sample and over individual gestures, and proportion of total movement that occurs during gestures (i.e. how much/little movement is there outside of gestures). The amount and speed of movement is intuitively correlated to distress and personality. This intuition is validated as these movement representations are included in a variety of feature sets resulting from feature space searches, across multiple labels. Indeed, Table  5 .9 shows that the speed of the head during a gesture (i.e. He-GA) is correlated with positive classifications for labels including neuroticism, depression, somatic stress, while it is inversely correlated with other classifications including extraversion and agreeableness.",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "Surprise",
      "text": "One of the less initially intuitive features is \"gesture surprise\". This feature represents the distance between a gesture and the previous gesture (or beginning of the sample) as a proportion of the total length of the sample. This per-gesture value is then averaged across all gestures, this means it is not a measure of the proportion of the sample when no gesture occurs (as discussed in Section 4.3). The intuition is to represent whether the participant's gestures occur regularly or after a period of stillness (i.e. how \"surprising\" is the gesture). Gesture surprise is included in every feature combination resulting from a feature search of a label, shown in Table  5 .9, suggesting it provides useful information.",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "Linearity",
      "text": "The features are, in general, linearly correlated with different labels, as shown by the effectiveness of the lin classifier, which has been the focus of the feature development and evaluation. Using this classifier provides interpretability of the features and integrity that the features are providing real, useful, information, not simply a platform for overfitting.\n\nHowever, not all features are consistently linear with regards to certain labels. Some features are learnt as correlated and inversely correlated on the same label in different cross validation rounds. This inconsistency denoted with the usage indicator \"?\". For example, while the hands localised gesture surprise feature (Hn-GS) is linearly correlated with somatic stress, anxiety, perceived stress, and gender, it is inconsistently correlated with conscientiousness and depression. These inconsistencies within feature correlation are the exception, not the rule. Indeed, all of the features that experience inconsistency on a label are shown to be linearly correlated to another label, or linearly correlated within another localisation.",
      "page_start": 58,
      "page_end": 58
    },
    {
      "section_name": "Comparison With Related Work",
      "text": "I evaluate two comparison methods: a linear SVM model based on Bag-of-Words features of FACS  [21]  Action Units (AUs), and the Bag-of-Body Dynamics (BoB) method presented by Joshi et al.  [36] . The first method provides a comparison of modality (facial-modality) within my dataset and a cross-dataset benchmark with the closest related dataset, DAIC.\n\nThe second method provides a comparison to an existing body-modality method that uses an automatic expression categorization approach to predict depression.",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "Au Svm",
      "text": "This method uses basic facial expression analysis to predict a binary depression label. I implement a simple linear kernel SVM based on a Bag-of-Words (BoW) feature vector of FACS AUs to predict the binary depression label in my dataset and, separately, a binary depression label within DAIC. As with my dataset, I apply a threshold to DAIC's PHQ-8 score labels such that the dataset is as balanced as possible. The resulting threshold is 5 (i.e. 6 and above is considered \"depressed\"), this results in 68 of 139 samples classed as \"depressed\". This is a lower threshold than used in my dataset (which is 7).\n\nWithin the BoW feature vector, binary AUs are counted for each frame they are detected in, whilst intensity measured AUs (i.e. continuous value measures) are summed across all frames. The sums and counts are then normalised against the number of frames in which the face was successfully detected.\n\nThe DAIC dataset provides per-frame predictions for 20 AUs. I use OpenFace  [5]  to extract per-frame AUs from my dataset. OpenFace predicts 35 AUs per-frame, substantially more than provided by DAIC. While this method does not use the gesture features, I still only evaluate it on the samples that pass the gesture filtering step (Section 4.2.1) so that the evaluation is consistent with previous results.",
      "page_start": 59,
      "page_end": 59
    },
    {
      "section_name": "Bag-Of-Body Dynamics",
      "text": "Joshi et al.  [36]  use a BoW approach to predict depression in clinically assessed participants from their BlackDog dataset. The BoW feature vector is comprised of categorized body expressions based on K-Means clustering of histograms surrounding STIPs within a video of the subject. Their method is:\n\n1. Extract Space-Time Interest Points (STIPs).\n\n2. Calculate histograms of gradients (HoG) and optic flow (HoF) around the STIPs.\n\n3. Cluster the histograms per-sample using K-Means, the resulting cluster centres define the sample's \"key interest points\" (KIPs).\n\n4. Cluster KIPs across all samples to define a BoB codebook, also using K-Means.\n\n5. Generate a BoB feature vector for each sample by fitting its KIPs to the codebook.\n\n6. Finally, apply a non-linear SVM to the resulting feature vectors to predict depression.\n\nThey use a radial basis function kernel (RBF) for their SVM.\n\nDifferences in Method In the original paper Joshi et al. experiment with multiple KIP counts, codebook sizes, and perform an extensive grid search for their SVM parameters. I use a single KIP count and codebook size, 1,000 and 500, which they also experiment with. While I perform a grid search on the RBF parameters, it may not be as extensive as their search. I also exclude samples that generate fewer STIPs than the KIP count. This results in 45 out of 65 samples being included.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Results",
      "text": "The results for each model are presented in Table  5 .10, a comparison of methods applied to my dataset is shown in Figure  5 .4. I compare the best depression detection model from my previous evaluations (i.e. the feature combination identified in Section 5.3 with the lin classifier), and a lin classifier using all gesture meta features, with two BoB based SVMs and a FACS AUs based linear SVM for predicting depression. The lin model performs best, achieving an 82.70% F1 average with the optimal feature combination and 66.81% with all features, compared to the next best at 63.71% from the FACS AUs SVM model and 61.10% from the BoB RBF SVM model. Results for all models are based on the same three fold cross-validation mean F1 as previously.\n\nThe FACS AUs SVM model performs better on my dataset than the DAIC dataset. This is almost certainly due to the difference in quantity and quality of available FACS AU features. The DAIC dataset provides 20 AU predictions per frame while my dataset provides 35 AU predictions.",
      "page_start": 60,
      "page_end": 60
    },
    {
      "section_name": "Multi-Modal",
      "text": "Given the success of the FACS AUs SVM classifier in Section 5.6, I also evaluate a multi-modal method that fuses my gesture meta features and FACS AUs.\n\nI perform multiple multi-modal experiments:\n\n1. Feature vector fusion including all features, evaluated across all four classifier types.\n\n2. Feature vector fusion with a feature search on my gesture meta features, though all AU features are retained in every search iteration.  The lin model using the optimal feature combination identified in Section 5.3 achieves the best results. The lin model using all features (i.e. the all feature baseline from Section 5.3) also beats the comparison methods.",
      "page_start": 61,
      "page_end": 61
    },
    {
      "section_name": "Results",
      "text": "The best result was by the feature search method with an svm classifier (2.b), achieving 81.94% F1 average. The feature search with lin classifier (2.a) was second best with an 80.93% F1 average. However, these are worse than the best depression detection score of 82.70% when using the gesture meta features alone with the lin model. Moreover, the hybrid fusion approaches achieved F1s in the mid-70s, so rather than correcting errors by the meta features, it averaged the success rate between the meta classifier and the AUs classifier, resulting in a worse F1.",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Potential Future Fusion Approaches",
      "text": "These approaches to fusion are somewhat simplistic. More sophisticated approaches, such as deep learning fusion, may achieve better results than the meta features alone. An interesting route for future work is to use recurrent deep learning to fuse temporally aligned meta features with AUs, rather than fusing them post-aggregation.",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Summary",
      "text": "In this chapter I have evaluated the introduced gesture meta features and found they provide linear predictive information. To achieve the best performance I perform an exhaustive search of the feature space to identify the best feature combination. This demonstrates the potential of the features, however, it also introduces a risk of overfitting. This risk is mitigated by two factors: the best performance is achieved by a linear regression based classifier (i.e. a classifier not prone to overfitting) and many features are present in optimal feature combinations for multiple uncorrelated labels.\n\nI compared my method to a basic facial-modality method and an existing body-modality method based on STIPs (i.e. generic analysis of video data). My method outperforms both when using the lin classifier, both when using all possible features and when using the optimal feature combination for the depression label.\n\nFinally, I perform a multi-modal experiment utilising the facial-modality comparison method and my novel gesture meta features. Despite testing multiple approaches to fusion, no method I evaluated beat the mono-modal gesture meta features classifiers. However, all related work that I am aware of achieves better performance when incorporating multiple modals, as such it is likely that further investigation of multi-modal approaches incorporating the gesture meta features will identify a multi-modal approach that does improve upon my mono-modal results.",
      "page_start": 62,
      "page_end": 62
    },
    {
      "section_name": "Chapter 6 Conclusion",
      "text": "I have presented a novel set of generic body modality features based on meta information of gestures. To develop and evaluate these features I also introduced a novel non-clinical audio-visual dataset containing recordings of semi-structured interviews along with distress and personality labels.\n\nI evaluated these features for detecting depression as a binary classification task based on PHQ-8 scores. A linear regression based classifier achieved a 82.70% average F1 score, suggesting these features are both useful for depression detection and provide linear information. I further evaluated the features' generalisability via binary classification tasks for other distress labels, such as anxiety, personality measures, and gender. All generalisation labels achieved better than 80% average F1 scores, with the exception of one label, personality neuroticism, which achieved 76.39%.\n\nThese are novel features as previous works within the body modality for automatic distress detection are based on either unsupervised definitions of expressions or handcrafted descriptors of distress indicative behaviour. These features are similar to some of the work within the eye and head modalities which are based on modeling generic activity levels within these modalities, among other features.\n\nFinally, these features exist within a broader ecosystem of affect based features. The best results for automatic distress detection, and similar tasks, is achieved by integrating multiple modalities and multiple representations of modalities. Future work may apply these features to new tasks, extend the feature type by extracting similar meta features, and develop methods for combining them with other modalities to amplify the information they provide.",
      "page_start": 65,
      "page_end": 66
    },
    {
      "section_name": "Future Work Dataset",
      "text": "Future work could expand the introduced dataset and increase its quality via manual annotations. For example, annotating time frames based on the topic of conversation could enable sample segmentation methods such as  Gong & Poellabauer [25] . This could improve the feature modeling and classifier stages, another route of future work is gesture definition and detection. For example, manual annotation of gestures would enable auto-learnt gesture detectors.",
      "page_start": 66,
      "page_end": 66
    },
    {
      "section_name": "Regression Tasks",
      "text": "All labels I evaluate, with the exception of gender, are scales based on self-evaluation questionnaires relating to distress or personality. The distress scales define multiple levels of severity while the personality scales do not. Both types of labels are prime for regression prediction tasks. Moreover, distress classification prediction could be provided by the defined severity levels once a regression had been performed. However, this requires a larger dataset (initial experiments with regression on the dataset support this assertion).",
      "page_start": 66,
      "page_end": 66
    },
    {
      "section_name": "Features",
      "text": "Applying the presented features within larger datasets could further illuminate the properties of certain features, either confirming their inconsistency, or solidifying the linearity in one direction. Such properties should then be tested with regards to the psychology literature.",
      "page_start": 66,
      "page_end": 66
    },
    {
      "section_name": "Trunk",
      "text": "I do not evaluate a \"trunk\" localisation (i.e. hips to shoulders), though it may prove useful in future work. In my dataset participants are seated during the whole interview, thus the two-dimensional range of motion in the trunk (i.e. up/down and side to side) is small. As I do not include depth recording the greatest range of motion, forward and backward, is difficult to incorporate. Future work that either includes depth sensing or has participants in a wider variety of scenarios, where they may be standing or more physically engaged, may find some use in a trunk localisation.",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Cross Localisation Co-Occurrence",
      "text": "Co-occurrence of gestures across localisations is not considered in this dissertation, though it is an area for future work. Applying deep learning to co-occurrence modeling to identify relevant patterns would be particularly interesting.",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Improved Surprise",
      "text": "In future work this feature could be extended to be more indicative of \"surprise\", rather than simply an average of distance since last gesture. For example, a gesture that occurs within a regular pattern of gestures, regardless of their distance, might be considered to have low surprise, while a gesture interrupting that pattern by occurring in the middle of the pattern's silent period would be very surprising. This more sophisticated measure of surprise could account for naturalistic behaviour such as rhythmic movement/gestures. These types of extensions of features are exciting for two reasons: firstly, they are interpretable, they do not come from a black box model, and thus can be supported by psychology literature. Secondly, they are immediately measurable and their design is based on traditional statistical techniques such as repetition modeling and can therefore be iterated on more readily than deep learning based features.",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Generalisation",
      "text": "Future work, some of which has been discussed here, could extend these features to achieve better results in more diverse datasets. A greater variety of scenarios where the participant is more constantly physically engaged such that movement is more constant, e.g walking or a standing conversation, would challenge the design of the presented gesture meta features. Indeed, the current design would have trouble as it relies on the default state being a lack of movement.",
      "page_start": 67,
      "page_end": 67
    },
    {
      "section_name": "Deep Learning For Feature Generation",
      "text": "As I discussed in Chapter 2, deep learning has been applied to the depression detection task, but still relies on hand-crafted features and descriptors. As with many fields it is likely that deep learning methods will, in the next few years, achieve state-of-the-art results, far exceeding the potential of more traditional approaches. However, to reach this point the deep learning models need large enough datasets to train on.\n\nAssuming no especially large dataset is developed specifically for automatic depression detection, one potential for future work is the use of transfer learning (as Chen et al.  [9]  explore for vocal and facial features) for body expressions. For example, CNNs could be trained on general body expression datasets to learn inner representations of body language. The output of inner layers could then be used in depression detection tasks (Razavian et al.  [47]  present a transfer learning approach to image recognition tasks and achieve very good results on niche tasks using a generic pre-trained image CNN).",
      "page_start": 67,
      "page_end": 68
    },
    {
      "section_name": "Classifiers And Deep Learning",
      "text": "In this dissertation I have used two traditional statistical models, linear and logistic regression, and two more advanced machine learning methods, support vector machines and random forests. However, I have purposefully avoided more complex machine learning methods such as neural networks, and especially deep learning. These more sophisticated methods have achieved significant improvements on the state-of-the-art in many domains, however, they suffer from a lack of interpretability and a propensity to overfit data. As my primary focus has been validating gesture meta features I define interpretability as a core requirement and, given the size of my dataset, avoiding methods that overfit is important.\n\nHowever, having validated these features, deep learning presents opportunities with regards to learning more sophisticated aggregation functions (this is slightly different to the feature generating deep learning discussed above). I aggregate three core features of gestures: their movement, duration, and \"surprise\". I perform aggregation via averaging and standard deviations. However, given a larger dataset, a deep learning model could foreseeably learn both more sophisticated aggregation functions and core meta-features. It is important to emphasize here the burden on dataset size that exists when attempting to learn valuable features using deep learning.",
      "page_start": 68,
      "page_end": 68
    },
    {
      "section_name": "Multi-Modal Approaches With Deep Learning",
      "text": "Deep learning can learn useful modality integration functions. While I experimented with a multi-modal approach in Section 5.7 it did not achieve better results than my mono-modal method. I believe this is due to the rather direct approaches to multi-modal integration I experimented with. Applying deep learning to modal integration thus presents an opportunity for further work.\n\nFurthermore, integrating a greater diversity of modalities and modality representations would be interesting. For example, applying the Bag-of-Body Dynamics features along with my gesture meta features and FACS AUs. With regards to the Bag-of-Body Dynamics method, future work could apply the core clustering concept to pose estimation data rather than STIPs data and may achieve better results from this.",
      "page_start": 68,
      "page_end": 68
    }
  ],
  "figures": [
    {
      "caption": "Figure 4: 1: High level pipeline description of",
      "page": 35
    },
    {
      "caption": "Figure 4: 2: Example of OpenPose estimation output of the participant interview position.",
      "page": 36
    },
    {
      "caption": "Figure 4: 2 presents an example of extracted pose points.",
      "page": 36
    },
    {
      "caption": "Figure 5: 1: Comparison of baseline results to feature searched results. This shows that",
      "page": 50
    },
    {
      "caption": "Figure 5: 2: Comparison of the best F1 average scores from localisation combinations",
      "page": 52
    },
    {
      "caption": "Figure 5: 3: Comparison of F1 average scores when using the optimal feature combination",
      "page": 54
    },
    {
      "caption": "Figure 5: 4. I compare the best depression detection model from",
      "page": 60
    },
    {
      "caption": "Figure 5: 4: Comparison of method F1 scores for predicting the depression label in my",
      "page": 61
    }
  ],
  "tables": [
    {
      "caption": "Table 3: 1. Covariance is presented as normalized covariance values,",
      "page": 31
    },
    {
      "caption": "Table 3: 2. While there are diï¬erences, the measures",
      "page": 31
    },
    {
      "caption": "Table 3: 1: General statistics regarding questionnaire and demographic results within the",
      "page": 32
    },
    {
      "caption": "Table 3: 2: Comparison of the mean questionnaire values within my dataset to the published",
      "page": 33
    },
    {
      "caption": "Table 5: 1: Binary classiï¬cation thresholds for distress and personality labels.",
      "page": 46
    },
    {
      "caption": "Table 5: 2: Feature notation tokens.",
      "page": 47
    },
    {
      "caption": "Table 5: 3: F1 aggregate scores for models using the baseline feature on the depression",
      "page": 48
    },
    {
      "caption": "Table 5: 4 presents results for each model when provided with the full feature vector",
      "page": 48
    },
    {
      "caption": "Table 5: 4: Classiï¬er F1 performance for detecting depression within my dataset given all",
      "page": 49
    },
    {
      "caption": "Table 5: 5: Performance of the best feature combination as determined by an exhaustive",
      "page": 49
    },
    {
      "caption": "Table 5: 5, a comparison of these results to the baseline results is",
      "page": 50
    },
    {
      "caption": "Table 5: 6: Performance of models when using features chosen via exhaustive search with",
      "page": 53
    },
    {
      "caption": "Table 5: 5). The lin classiï¬er also has",
      "page": 53
    },
    {
      "caption": "Table 5: 7. I also perform a feature space search for each label, using the",
      "page": 54
    },
    {
      "caption": "Table 5: 8. A comparison is shown in Figure 5.3.",
      "page": 54
    },
    {
      "caption": "Table 5: 7: Performance of models using the feature combination identiï¬ed for the depression",
      "page": 55
    },
    {
      "caption": "Table 5: 8: Performance of models for predicting a variety of labels when using features",
      "page": 55
    },
    {
      "caption": "Table 5: 7, achieved via label speciï¬c feature combinations. The",
      "page": 55
    },
    {
      "caption": "Table 5: 9 shows, the openness feature combination shares only 6 out of",
      "page": 56
    },
    {
      "caption": "Table 5: 9: Features chosen for each label when features are searched speciï¬cally for the",
      "page": 57
    },
    {
      "caption": "Table 5: 2 for the full notation mapping.",
      "page": 57
    },
    {
      "caption": "Table 5: 9 presents the feature sets resulting from the per-label feature searches. There are",
      "page": 57
    },
    {
      "caption": "Table 5: 9). The feature usage is inverted",
      "page": 57
    },
    {
      "caption": "Table 5: 9 shows that",
      "page": 58
    },
    {
      "caption": "Table 5: 9, suggesting it provides useful information.",
      "page": 58
    },
    {
      "caption": "Table 5: 10, a comparison of methods applied",
      "page": 60
    },
    {
      "caption": "Table 5: 10: Comparison of my lin model, FACS AUs based SVM models, and the BoB",
      "page": 62
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Cross-cultural detection of depression from nonverbal behaviour",
      "authors": [
        "Sharifa Alghowinem",
        "Roland Goecke",
        "Michael Jeffrey F Cohn",
        "Gordon Wagner",
        "Michael Parker",
        "Breakspear"
      ],
      "year": "2015",
      "venue": "FG"
    },
    {
      "citation_id": "2",
      "title": "Cross-Cultural Depression Recognition from Vocal Biomarkers",
      "authors": [
        "Sharifa Alghowinem",
        "Roland Goecke",
        "Julien Epps",
        "Michael Wagner",
        "Jeffrey Cohn"
      ],
      "year": "2016",
      "venue": "ISCA"
    },
    {
      "citation_id": "3",
      "title": "Multimodal Depression Detection: Fusion Analysis of Paralinguistic, Head Pose and Eye Gaze Behaviors",
      "authors": [
        "Sharifa Alghowinem",
        "Roland Goecke",
        "Michael Wagner",
        "Julien Epps",
        "Matthew Hyett",
        "Gordon Parker",
        "Michael Breakspear"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Diagnostic and Statistical Manual of Mental Disorders",
      "year": "1994",
      "venue": "Am Psychiatr Assoc"
    },
    {
      "citation_id": "5",
      "title": "Open-Face 2.0: Facial Behavior Analysis Toolkit",
      "authors": [
        "Tadas BaltruÅ¡aitis",
        "Amir Zadeh",
        "Chong Lim",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018)"
    },
    {
      "citation_id": "6",
      "title": "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields",
      "authors": [
        "Zhe Cao",
        "Tomas Simon",
        "Shih-En Wei",
        "Yaser Sheikh"
      ],
      "year": "2016",
      "venue": "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields"
    },
    {
      "citation_id": "7",
      "title": "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. arXiv.org",
      "authors": [
        "Zhe Cao",
        "Gines Hidalgo",
        "Tomas Simon",
        "Shih-En Wei",
        "Yaser Sheikh",
        "Openpose"
      ],
      "year": "2018",
      "venue": "Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields. arXiv.org"
    },
    {
      "citation_id": "8",
      "title": "Multimodal Multi-task Learning for Dimensional and Continuous Emotion Recognition",
      "authors": [
        "Shizhe Chen",
        "Qin Jin",
        "Jinming Zhao",
        "Shuai Wang"
      ],
      "year": "2017",
      "venue": "AVEC@ACM Multimedia"
    },
    {
      "citation_id": "9",
      "title": "Motion feature augmented recurrent neural network for skeleton-based dynamic hand gesture recognition",
      "authors": [
        "Xinghao Chen",
        "Hengkai Guo",
        "Guijin Wang",
        "Li Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "10",
      "title": "",
      "authors": [
        "Sheldon Cohen",
        "Tom Kamarck",
        "Robin Mermelstein"
      ],
      "year": "1983",
      "venue": ""
    },
    {
      "citation_id": "11",
      "title": "Investigating Word Affect Features and Fusion of Probabilistic Predictions Incorporating Uncertainty in AVEC 2017. AVEC@ACM Multimedia",
      "authors": [
        "Ting Dang",
        "Brian Stasak",
        "Zhaocheng Huang",
        "Sadari Jayawardena",
        "Mia Atcheson",
        "Munawar Hayat",
        "Ngoc Phu",
        "Vidhyasaharan Le",
        "Roland Sethu",
        "Julien Goecke",
        "Epps"
      ],
      "year": "2017",
      "venue": "Investigating Word Affect Features and Fusion of Probabilistic Predictions Incorporating Uncertainty in AVEC 2017. AVEC@ACM Multimedia"
    },
    {
      "citation_id": "12",
      "title": "Why bodies? Twelve reasons for including bodily expressions in affective neuroscience",
      "authors": [
        "Gelder De"
      ],
      "year": "1535",
      "venue": "Philosophical Transactions of the Royal Society B: Biological Sciences"
    },
    {
      "citation_id": "13",
      "title": "Detecting Crisis: An AI Solution",
      "year": "2018",
      "venue": "Detecting Crisis: An AI Solution"
    },
    {
      "citation_id": "14",
      "title": "Multimodal Detection of Depression in Clinical Interviews",
      "authors": [
        "Hamdi Dibeklioglu",
        "Zakia Hammal",
        "Ying Yang",
        "Jeffrey Cohn"
      ],
      "year": "2015",
      "venue": "ICMI"
    },
    {
      "citation_id": "15",
      "title": "Skeleton based action recognition with convolutional neural network",
      "authors": [
        "Yong Du",
        "Yun Fu",
        "Liang Wang"
      ],
      "year": "2015",
      "venue": "2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)"
    },
    {
      "citation_id": "16",
      "title": "Hierarchical recurrent neural network for skeleton based action recognition",
      "authors": [
        "Yong Du",
        "Wei Wang",
        "Liang Wang"
      ],
      "year": "2015",
      "venue": "CVPR"
    },
    {
      "citation_id": "17",
      "title": "Eye-blink rates and depression: Is the antidepressant effect of sleep deprivation mediated by the dopamine system?",
      "authors": [
        "Ebert"
      ],
      "year": "1996",
      "venue": "Neuropsychopharmacology"
    },
    {
      "citation_id": "18",
      "title": "Telling lies: Clues to deceit in the marketplace, politics, and marriage",
      "authors": [
        "Ekman"
      ],
      "year": "2009",
      "venue": "Telling lies: Clues to deceit in the marketplace, politics, and marriage"
    },
    {
      "citation_id": "19",
      "title": "Facial coding action system (FACS): A technique for the measurement of facial actions",
      "authors": [
        "P Ekman",
        "W V Friesen"
      ],
      "year": "1978",
      "venue": "Facial coding action system (FACS): A technique for the measurement of facial actions"
    },
    {
      "citation_id": "20",
      "title": "Opensmile: the munich versatile and fast open-source audio feature extractor. the munich versatile and fast open-source audio feature extractor",
      "authors": [
        "Florian Eyben",
        "Martin WÃ¶llmer",
        "BjÃ¶rn Schuller"
      ],
      "year": "2010",
      "venue": "Opensmile: the munich versatile and fast open-source audio feature extractor. the munich versatile and fast open-source audio feature extractor"
    },
    {
      "citation_id": "21",
      "title": "Nonverbal interaction of patients and therapists during psychiatric interviews",
      "authors": [
        "Lynn Fairbanks",
        "Michael Mcguire",
        "Candace Harris"
      ],
      "year": "1982",
      "venue": "Journal of Abnormal Psychology"
    },
    {
      "citation_id": "22",
      "title": "The Somatic Symptom Scale-8 (SSS-8)",
      "authors": [
        "Benjamin Gierk",
        "Sebastian Kohlmann",
        "Kurt Kroenke",
        "Lena Spangenberg",
        "Markus Zenger",
        "Elmar BrÃ¤hler",
        "Bernd LÃ¶we"
      ],
      "year": "2014",
      "venue": "The Somatic Symptom Scale-8 (SSS-8)"
    },
    {
      "citation_id": "23",
      "title": "Topic Modeling Based Multi-modal Depression Detection",
      "authors": [
        "Yuan Gong",
        "Christian Poellabauer"
      ],
      "year": "2017",
      "venue": "AVEC@ACM Multimedia"
    },
    {
      "citation_id": "24",
      "title": "The Distress Analysis Interview Corpus of human and computer interviews. LREC",
      "authors": [
        "Jonathan Gratch",
        "Ron Artstein",
        "M Gale",
        "Giota Lucas",
        "Stefan Stratou",
        "Angela Scherer",
        "Rachel Nazarian",
        "Jill Wood",
        "David Boberg",
        "Stacy Devault",
        "Marsella",
        "Skip David R Traum",
        "Louis-Philippe Rizzo",
        "Morency"
      ],
      "year": "2014",
      "venue": "The Distress Analysis Interview Corpus of human and computer interviews. LREC"
    },
    {
      "citation_id": "25",
      "title": "",
      "authors": [
        "Gumtree"
      ],
      "year": "2019",
      "venue": ""
    },
    {
      "citation_id": "26",
      "title": "Micromomentary facial expressions as indicators of ego mechanisms in psychotherapy",
      "authors": [
        "Ernest Haggard",
        "Kenneth Isaacs"
      ],
      "year": "1966",
      "venue": "Methods of Research in Psychotherapy"
    },
    {
      "citation_id": "27",
      "title": "Development of a Rating Scale for Primary Depressive Illness",
      "authors": [
        "Max Hamilton"
      ],
      "year": "1967",
      "venue": "British Journal of Social and Clinical Psychology"
    },
    {
      "citation_id": "28",
      "title": "Canonical Correlation Analysis: An Overview with Application to Learning Methods. dx",
      "authors": [
        "Sandor David R Hardoon",
        "John Szedmak",
        "Shawe-Taylor"
      ],
      "year": "2006",
      "venue": "Canonical Correlation Analysis: An Overview with Application to Learning Methods. dx"
    },
    {
      "citation_id": "29",
      "title": "Continuous Multimodal Emotion Prediction Based on Long Short Term Memory Recurrent Neural Network",
      "authors": [
        "Jian Huang",
        "Ya Li",
        "Jianhua Tao",
        "Zheng Lian",
        "Zhengqi Wen",
        "Minghao Yang",
        "Jiangyan Yi"
      ],
      "year": "2017",
      "venue": "AVEC@ACM Multimedia"
    },
    {
      "citation_id": "30",
      "title": "The Big Five Trait Taxonomy: History, Measurement, and Theoretical Perspectives",
      "authors": [
        "P Oliver",
        "Sanjay John",
        "Srivastava"
      ],
      "year": "1999",
      "venue": "Handbook of personality Theory and research"
    },
    {
      "citation_id": "31",
      "title": "Neural-Net Classification For Spatio-Temporal Descriptor Based Depression Analysis",
      "authors": [
        "Jyoti Joshi",
        "Abhinav Dhall",
        "Roland Goecke",
        "Michael Breakspear",
        "Gordon Parker"
      ],
      "year": "2012",
      "venue": "Neural-Net Classification For Spatio-Temporal Descriptor Based Depression Analysis"
    },
    {
      "citation_id": "32",
      "title": "Relative Body Parts Movement for Automatic Depression Analysis",
      "authors": [
        "Jyoti Joshi",
        "Abhinav Dhall",
        "Roland Goecke",
        "Jeffrey Cohn"
      ],
      "year": "2013",
      "venue": "ACII"
    },
    {
      "citation_id": "33",
      "title": "Multimodal assistive technologies for depression diagnosis and monitoring",
      "authors": [
        "Jyoti Joshi",
        "Roland Goecke",
        "Sharifa Alghowinem",
        "Abhinav Dhall",
        "Michael Wagner",
        "Julien Epps",
        "Gordon Parker",
        "Michael Breakspear"
      ],
      "year": "2013",
      "venue": "J. Multimodal User Interfaces"
    },
    {
      "citation_id": "34",
      "title": "Can body expressions contribute to automatic depression analysis?",
      "authors": [
        "Jyoti Joshi",
        "Roland Goecke",
        "Gordon Parker",
        "Michael Breakspear"
      ],
      "year": "2013",
      "venue": "2013 10th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2013)"
    },
    {
      "citation_id": "35",
      "title": "The PHQ-9: Validity of a Brief Depression Severity Measure",
      "authors": [
        "Kurt Kroenke",
        "Robert Spitzer",
        "Janet B W Williams"
      ],
      "year": "2001",
      "venue": "Journal of General Internal Medicine"
    },
    {
      "citation_id": "36",
      "title": "The PHQ-8 as a measure of current depression in the general population",
      "authors": [
        "Kurt Kroenke",
        "Tara Strine",
        "Robert Spitzer",
        "Janet B W Williams",
        "Joyce Berry",
        "Ali Mokdad"
      ],
      "year": "2009",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "37",
      "title": "A survey of human pose estimation: The body parts parsing based methods",
      "authors": [
        "Zhao Liu",
        "Jianke Zhu",
        "Jiajun Bu",
        "Chun Chen"
      ],
      "year": "2015",
      "venue": "Journal of Visual Communication and Image Representation"
    },
    {
      "citation_id": "38",
      "title": "Automatic multimodal descriptors of rhythmic body movement",
      "authors": [
        "Marwa Mahmoud",
        "Louis-Philippe Morency",
        "Peter Robinson"
      ],
      "year": "2013",
      "venue": "Automatic multimodal descriptors of rhythmic body movement"
    },
    {
      "citation_id": "39",
      "title": "Mental Health Foundation",
      "year": "2019",
      "venue": "Mental Health Foundation"
    },
    {
      "citation_id": "40",
      "title": "National Study of Chronic Disease Self-Management: Six-Month Outcome Findings",
      "authors": [
        "Marcia Ory",
        "Sangnam Ahn",
        "Luohua Jiang",
        "Kate Lorig",
        "Phillip Ritter",
        "Diana Laurent",
        "Nancy Whitelaw",
        "Matthew Smith"
      ],
      "year": "2013",
      "venue": "Journal of Aging and Health"
    },
    {
      "citation_id": "41",
      "title": "Analysis of fundamental frequency for near term suicidal risk assessment",
      "authors": [
        "R G Ozdas",
        "S E Shiavi",
        "M K Silverman",
        "D M Silverman",
        "Wilkes"
      ],
      "year": "2000",
      "venue": "IEEE International Conference on Systems, Man, and Cybernetics"
    },
    {
      "citation_id": "42",
      "title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy",
      "authors": [
        "Hanchuan Peng",
        "Fuhui Long",
        "C Ding"
      ],
      "year": "2005",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Automatic audiovisual behavior descriptors for psychological disorder analysis",
      "authors": [
        "Stefan Scherer",
        "Giota Stratou",
        "Gale Lucas",
        "Marwa Mahmoud",
        "Jill Boberg",
        "Jonathan Gratch",
        "Albert Rizzo",
        "Louis-Philippe Morency"
      ],
      "year": "2014",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "44",
      "title": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition",
      "authors": [
        "Ali Sharif Razavian",
        "Hossein Azizpour",
        "Josephine Sullivan",
        "Stefan Carlsson"
      ],
      "year": "2014",
      "venue": "CNN Features Off-the-Shelf: An Astounding Baseline for Recognition"
    },
    {
      "citation_id": "45",
      "title": "Recurrent Neural Network Based Action Recognition from 3D Skeleton Data",
      "authors": [
        "Parul Shukla",
        "K Kanad",
        "Prem Biswas",
        "Kalra"
      ],
      "year": "2017",
      "venue": "2017 13th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)"
    },
    {
      "citation_id": "46",
      "title": "Hand Keypoint Detection in Single Images Using Multiview Bootstrapping",
      "authors": [
        "Tomas Simon",
        "Hanbyul Joo",
        "Iain Matthews",
        "Yaser Sheikh"
      ],
      "year": "2017",
      "venue": "Hand Keypoint Detection in Single Images Using Multiview Bootstrapping"
    },
    {
      "citation_id": "47",
      "title": "Psychomotor symptoms of depression",
      "authors": [
        "Christina Sobin",
        "Harold Sackeim"
      ],
      "year": "2019",
      "venue": "Psychomotor symptoms of depression"
    },
    {
      "citation_id": "48",
      "title": "Learning a sparse codebook of facial and body microexpressions for emotion recognition",
      "authors": [
        "Yale Song",
        "Louis-Philippe Morency",
        "Randall Davis"
      ],
      "year": "2013",
      "venue": "the 15th ACM"
    },
    {
      "citation_id": "49",
      "title": "A Brief Measure for Assessing Generalized Anxiety Disorder",
      "authors": [
        "Kurt Robert L Spitzer",
        "Kroenke",
        "B W Janet",
        "Bernd Williams",
        "LÃ¶we"
      ],
      "year": "2006",
      "venue": "Archives of Internal Medicine"
    },
    {
      "citation_id": "50",
      "title": "Development of personality in early and middle adulthood: Set like plaster or persistent change?",
      "authors": [
        "Sanjay Srivastava",
        "Samuel Oliver P John",
        "Jeff Gosling",
        "Potter"
      ],
      "year": "2003",
      "venue": "Journal of Personality and Social Psychology"
    },
    {
      "citation_id": "51",
      "title": "Human action recognition method based on hierarchical framework via Kinect skeleton data",
      "authors": [
        "Benyue Su",
        "Huang Wu",
        "Min Sheng"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Machine Learning and Cybernetics (ICMLC)"
    },
    {
      "citation_id": "52",
      "title": "Depression Severity Prediction Based on Biomarkers of Psychomotor Retardation",
      "authors": [
        "Kirill Zafi Sherhan Syed",
        "Sidorov",
        "Marshall David"
      ],
      "year": "2017",
      "venue": "AVEC@ACM Multimedia"
    },
    {
      "citation_id": "53",
      "title": "Human skeleton tree recurrent neural network with joint relative motion feature for skeleton based action recognition",
      "authors": [
        "Shenghua Wei",
        "Yonghong Song",
        "Yuanlin Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Image Processing (ICIP)"
    },
    {
      "citation_id": "54",
      "title": "Convolutional Pose Machines",
      "authors": [
        "Shih-En Wei",
        "Varun Ramakrishna",
        "Takeo Kanade",
        "Yaser Sheikh"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "55",
      "title": "Multimodal Measurement of Depression Using Deep Learning Models",
      "authors": [
        "Le Yang",
        "Dongmei Jiang",
        "Xiaohan Xia",
        "Ercheng Pei",
        "Meshia CÃ©dric Oveneke",
        "Hichem Sahli"
      ],
      "year": "2017",
      "venue": "AVEC@ACM Multimedia"
    },
    {
      "citation_id": "56",
      "title": "Hybrid Depression Classification and Estimation from Audio Video and Text Information",
      "authors": [
        "Le Yang",
        "Hichem Sahli",
        "Xiaohan Xia",
        "Ercheng Pei",
        "Meshia CÃ©dric Oveneke",
        "Dongmei Jiang"
      ],
      "year": "2017",
      "venue": "AVEC@ACM Multimedia"
    },
    {
      "citation_id": "57",
      "title": "A Survey on Human Pose Estimation. Intelligent Automation and Soft Computing",
      "authors": [
        "Bo Hong",
        "Qing Zhang",
        "Lei",
        "Neng Bi",
        "Ji Zhong",
        "Jia Du",
        "Peng"
      ],
      "year": "2016",
      "venue": "A Survey on Human Pose Estimation. Intelligent Automation and Soft Computing"
    }
  ]
}