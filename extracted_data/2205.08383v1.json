{
  "paper_id": "2205.08383v1",
  "title": "Bias And Fairness On Multimodal Emotion Detection Algorithms",
  "published": "2022-05-11T20:03:25Z",
  "authors": [
    "Matheus Schmitz",
    "Rehan Ahmed",
    "Jimi Cao"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Numerous studies have shown that machine learning algorithms can latch onto protected attributes such as race and gender and generate predictions that systematically discriminate against one or more groups. To date the majority of bias and fairness research has been on unimodal models. In this work, we explore the biases that exist in emotion recognition systems in relationship to the modalities utilized, and study how multimodal approaches affect system bias and fairness. We consider audio, text, and video modalities, as well as all possible multimodal combinations of those, and find that text alone has the least bias, and accounts for the majority of the models' performances, raising doubts about the worthiness of multimodal emotion recognition systems when bias and fairness are desired alongside model performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Neural Networks are already ubiquitously used for classification and recognition in countless domains, including humans and their emotions. Yet in the rush to continuously advance those systems, researchers have not always paid attention to unequal development in the system capabilities  [14] . Oftentimes those systems are found to work better in certain demographic groups than in others, with the \"others\" category most often being minorities  [6] . As a result, theses systems can potentially show prejudice in their judgement and consequently exacerbate inequality.\n\nEmotion Recognition is an important area of research to enable effective human-computer interaction. Human emotions can be detected using speech signal, facial expressions, body language, and electroencephalography (EEG)  [9] . Our goal is to analyze and try to mitigate gender bias in current state-of-the-art transfer learning algorithms for different modalities including audio, text, and video. We define model bias as the difference in the predictive performance of a model across different groups. Throughout this work the groups considered are gender-based, that is, male and female.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Literature Review",
      "text": "A survey on bias and fairness in machine learning by Mehrabi et al.  [14]  points to the various sub-areas of Machine Learning in which the bias thematic has been explored by researchers, and, among other findings, reveals that the majority of published work has been on unimodal models. In the realm of multimodal learning, work by Shen Yan et al.  [25]  focusing on personality assessments showed that different modalities display numerous bias patterns, and that the data fusion stage also introduces further model bias. Similarly, Ramakrishna et al.  [19]  revealed how gender, race, and age can affect the way characters are portrayed in media. Buolamwini et al.  [3]  found substantial disparities in the accuracy rate of classifying gender of dark-skin females in existing commercial gender classification systems.\n\nResearch by Yoon et al.  [26]  worked on audio-text bimodal emotion recognition but there was no emphasis on assessing or mitigating bias. Tripathi et al.  [24]  expanded this line of research by exploring audio-textvideo trimodal emotion recognition, but still leave out any considerations about bias and fairness. Meanwhile, Domnich and Anbarjafari  [7]  worked on gender bias assessment in emotion recognition using unimodal learning from facial imagery.\n\nGiven this paper analyzes bias, we need to have a definition of fairness and how to measure and compare multiple modalities in their fairness. Kleinberg et al.  [11]  have shown that it is simply not possible to satisfy all definitions of fairness at the same time. Hence here we opt to utilize the fairness definition proposed by Bellamy et al.  [2] , which are composed of two metrics: statistical parity difference, and equality of opportunity difference  [18] .\n\nStatistical parity is a fairness metric that measures if the prediction is independent of the protected attribute and can be calculated with the following equation:\n\nEquality of Opportunity is a fairness metric which measures if the prediction is conditionally independent to the protected attribute, given the value of predictor is true and can be calculated with the following equation:\n\nAs this literature review shows, individually there is prior work in multimodal emotion recognition, fairness in multimodal learning, and gender bias in machine learning. This work advances current research by exploring the intersection of these, namely: gender bias in state-of-the-art transfer learning models for emotion recognition, considering both unimodal as well as multimodal fusion architectures.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "We used the IEMOCAP dataset  [4] . This dataset contains both a scripted and an improvised section, and we opt to utilize both in our analysis so as to maximize our sample sizes during model training. In including acted data there is a risk the findings are not representative of day-to-day life, but given there are no large datasets of non-acted multimodal emotion recognition data covering audio, text, and video, we opt for IEMOCAP as a solution for covering all modalities that is often utilized for emotion recognition research (  [24, 10, 15, 20, 8] ). While it would be possible to utilize only the non-acted part of IEMOCAP, developing models on small data has numerous side-effects which would also result in questionable findings. For this reason we choose to utilize the full collection of samples available in IEMOCAP. The dataset originally contains 10039 utterances, with a class breakdown as per table  1 . Those samples have are split roughly equally amongst 5 sessions, each containing one male and one female participant.\n\nIn IEMOCAP, \"xxx\" identifies samples for which there was no agreement amongst annotators, while \"other\" means annotators all agree on some emotion, but that emotion is not part of the predefined emotions for the dataset. On the basis of not being useful labels we remove all samples labeled as \"xxx\" and \"other\". On the basis of having too small of a sample size we remove the samples labeled as: \"surprised\", \"fearful\", \"disgusted. Lastly, similar to previous work on the IEMOCAP dataset (  [17, 16, 23] ), we merge the \"excited\" and \"happy\" classes into a single class for which we use the \"happy\" label. As a result, the final dataset contains 7380 samples, with a class breakdown shown in table  1 . All classes are balanced, with a roughly 50/50 gender split.\n\nUtilizing the filtered data, we create the following train-validation-test split: Sessions 1-4 are training data, on Session 5 the even numbered utterances are validation data, and the odd numbered utterances are test data. This implies roughly a split of 80% training, 10% validation and 10% test.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Feature Extractions",
      "text": "We chose three different state of the art deep learning models for each of the three modalities available in the dataset: audio, video, and text. We leverage each models to extract embeddings from the data. Audio. We leverage the utterance level data that comes in IEMOCAP and generate sliding time windows from which embeddings can be extracted. Utterance lengths vary from 0.6 seconds up to 34.1 seconds, with the 80th percentile being 6 seconds, and hence that is the chosen length to which all audios were standardized. The full utterance length distribution is shown in figure  1 . For larger files the middle section of the data is kept, and for smaller files we employ front-padding, which we found to greatly improve model performance in comparison to back-padding. The audio is downsampled to 16kHz, and we utilize a 25 millisecond sliding window with a 10 millisecond step size, thus the resulting embeddings are of shape (timesteps, features) = (600, 400).\n\nThe unimodal model is utilized to compare five different feature extraction approaches: MFCC, Mel-Spectrogram, Wav2Vec2  [1] , TRILL  [22]  and WavLM  [5] . Unimodal test dataset F1-Scores are as follows: MFCC (24.9%), Mel-Spectrogram (26.3%), Wav2Vec (28.1%), TRILL (27.8%), WavLM (28.6%). Based on this analysis WavLM is the chosen embedding model. Text. We use EmoBERTa, a technique proposed by Kim et al  [10]  that is a simple yet expressive scheme of emotion recognition in conversation task. It starts with RoBERTa  [12]  for sequence classification and adds a randomly initialized linear layer with the softmax nonlinearity to the last layer of the pretrained model.\n\nThe authors propose multiple models, such as including past and future utterances for speaker state aware emotion recognition. Our work uses the pre-trained EmoBERTa-base model without any past nor future utterances, to keep it consistent with audio and video modality techniques in using only the utterance itself to recognize emotion.\n\nThe model was then further fine-tuned on our filtered IEMOCAP train split for 30 epochs using the training configurations proposed in the paper with 12 attention heads, 12 hidden layers, a Vocab Size of 50265 and 512 Max Position Embeddings. To use for fusion in multimodal techniques, we then extracted sentencelevel embeddings by using the approach proposed by McCormick and Ryan (2019)  [13] . For each individual token in the sentence, we summed the vectors from the last four layers. Next, we averaged all the extracted token vectors in the sentence to get a standardized sentence-level embedding for each sentence.\n\nVideo. FaceNet  [21]  is the approach chosen to extract embeddings from the video files. We split the videos based on the time frame of the utterances in the audio section. This resulted in video spanning from 18 to 1024 frames with the 90th percentile being 263 frames, at 30 frames per second. IEMOCAP's videos contain two speakers at any given moment. The video files contain labels regarding in which halves of the video the male and female subjects were, and also on which gender was speaking at any given utterance. Using these information we were able to identify and isolate the active speaker. Then, using MTCNN  [27] , a pretrained neural network for face detection, we detected faces and extracted 512-features-long embeddings for the detected face through FaceNet for each frame of a video. For frames where faces were not detected, an empty array was used in its place instead. Finally, for videos with less than 263 frames, we pre-padded the frames. For videos with more than 263 frames, we kept the middle section.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Training",
      "text": "We chose to utilize a standardized model architecture to make comparisons more direct. The utilized neural network structure is shown in figure  2 . With this structure, we train one model with each possible combination of modalities. For unimodal and bimodal training, the modalities not being utilized simply forward a vector of zeros to the fusion layer. The chosen optimizer is AdaBelief  [28]  with a learning rate of 1e-3, and the loss function is Categorical Cross Entropy. All layers have a LeakyReLU activation, except for the final layer which is a Softmax.\n\nWe employ early stopping to terminate training for any model that doesn't see a reduction in validation set loss for 20 sequential epochs. All models are allowed to train until the early stopping trigger halts training, and the models are then restored to the state (weights) they had on the best epoch, as judged by validation loss.\n\nThe specific choices for normalization and dropout on the individual modalities' branches were derived experimentally on the unimodal models associated with each modality, during this process a Softmax layer was utilized in place if the pre-concatenation linear layer. The scores from those reduced models were not utilized for evaluation, merely for architecture design. Once the individual modalities' branches were locked, the postconcatenation section of the architecture was derived in a similar manner considering the trimodal model. When the final architecture was locked in all modality variations were trained on the same neural network structure.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Performance",
      "text": "We compared models across three different metrics: Accuracy, F1-Score, and ROC-AUC. Table  2  summarizes the results. We can observe that across all metrics the \"audio + text\" model was the highest performing one. The unimodal \"video\" model had the worst performance across all metrics, which might explain why adding it to the best model results in a worse trimodal model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Bias And Fairness",
      "text": "We used a number of different metrics to compare how model performance varied across the two gender groups.\n\nTable  3  shows how each modality performs on: (1) Average F1 Score difference, i.e. the difference across genders in F1 Score; (2) Average Equality of Opportunity (EO) difference, i.e. the average difference across genders in how conditionality independent the prediction is to the protected attribute; and (3) Average Statistical Parity (SP) difference, i.e. the average difference across genders in selection rates of each emotion. For all these metrics lower is better. We can see that unimodal audio and text are the least biased across all measures. Among unimodal approaches Video is significantly more biased. Considering bimodal models, fusing audio and video significantly increases bias in the system by increasing statistical parity and equality of opportunity.\n\nIn figure  3  males is higher. Using this measure for fairness, we could see Text and audio were the least biased unimodal models while video was the most biased according to both the fairness measures, especially for happiness and frustration. The trimodal approach yielded decent results, similar to text and audio unimodal approaches.\n\nResults for Equality of Opportunity are highly similar, hence we opted to place them in the appendix section.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusion",
      "text": "We surveyed different unimodal and multimodal approaches for emotion recognition, and showed that using text alone was superior to other approaches at emotion recognition by achieving a good balance between accuracy and fairness measures. Audio, although significantly worse than text, was still a lot better than video, as the latter wasn't able to obtain good accuracy while also having the worst performance in fairness measures. Amongst bimodal models, an audio + video fusion is the best performing model, and the addition of video results in a worse trimodal model in both predictive performance and fairness. We note that compared to text alone, the F1 Score gain from the audio-video bimodal model is rather small in comparison to the more accentuated worsening of fairness metrics.\n\nAdditionally, this work shines light on how different modalities varied on recognizing different emotions. Both unimodal and multimodal models which included text features were worse at detecting anger in females compared to males, displaying a gap in F1-Score of roughly 10%, as shown in the appendix. When fusing all the modalities, we observed that the trimodal model's performance was similar to text, failing to improve significantly on either accuracy or fairness.\n\nThe results presented give raise to one important ethical dilemma: One the one hand fusing modalities can potentially improve model performance, on the other hand adding modalities makes people more identifiable. This is relevant given the observation that text alone drives nearly all the model performance, while being the least biased. The latter arguably is a by-product of the fact that among text, audio, and video, text contains the least cues about someone's gender. This combination creates the predicament of deciding whether the addition of audio and video data to emotion recognition models is warranted in face of the small performance boost and the disproportional worsening of gender bias.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Future Work",
      "text": "Without the aid of more complex training techniques, in this paper we see the performance of models that do not include text is low (the random guess performance for 5 classes would be 20%). Hence there remains the possibility that part of the bias currently observed for non-text modalities, especially video, is a result of their poor learning ability overall. For this reason we recommend that future work emphasizes raising performance for audio and video, by, for example, incorporating dataset fusion, data augmentation, and other techniques that reduce over-fitting and increase the model's learning capacity, as better performing models would enable a clearer isolation of factors between simple overall poor performance and bias. We also observe a peculiarly consistent effect where all model variations that include text data show a significant performance gap in predicting anger for men and women. The consistency of this tendency through all fusion models makes it unlikely to be a one time fluke. Yet we cannot, based on the current work, identify whether this is associated with the dataset used, with differences in vocabulary choices between genders, with the performance gap between modalities, or with other reasons. Thus such quandary provides an interesting avenue for further exploration.",
      "page_start": 5,
      "page_end": 5
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Utterance length by quantile",
      "page": 3
    },
    {
      "caption": "Figure 2: Architecture for the neural network utilized",
      "page": 4
    },
    {
      "caption": "Figure 3: Statistical Parity diﬀerence by emotion and modality",
      "page": 6
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Those samples have are",
      "page": 2
    },
    {
      "caption": "Table 1: All classes are balanced, with a roughly 50/50",
      "page": 2
    },
    {
      "caption": "Table 1: Class breakdown on the dataset used",
      "page": 2
    },
    {
      "caption": "Table 2: summarizes",
      "page": 4
    },
    {
      "caption": "Table 2: Modality performance on the test dataset",
      "page": 4
    },
    {
      "caption": "Table 3: shows how each modality performs on: (1)",
      "page": 4
    },
    {
      "caption": "Table 3: Comparison of fairness measures across modal-",
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "authors": [
        "A Baevski",
        "H Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
      "doi": "10.48550/ARXIV.2006.11477"
    },
    {
      "citation_id": "2",
      "title": "Rama-Figure 3: Statistical Parity difference by emotion and modality murthy",
      "authors": [
        "R Bellamy",
        "K Dey",
        "M Hind",
        "S Hoffman",
        "S Houde",
        "K Kannan",
        "P Lohia",
        "J Martino",
        "S Mehta",
        "A Mojsilovic",
        "S Nagar",
        "K Richards",
        "J Saha",
        "D Sattigeri",
        "P Singh",
        "M Varshney",
        "K Zhang"
      ],
      "year": "2018",
      "venue": "An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias"
    },
    {
      "citation_id": "3",
      "title": "Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification",
      "authors": [
        "J Buolamwini",
        "T Gebru"
      ],
      "year": "2018",
      "venue": "Proceedings of the 1st Conference on Fairness, Accountability and Transparency"
    },
    {
      "citation_id": "4",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "E Kazemzadeh",
        "E Provost",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "5",
      "title": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "authors": [
        "S Chen",
        "C Wang",
        "Z Chen",
        "Y Wu",
        "S Liu",
        "Z Chen",
        "J Li",
        "N Kanda",
        "T Yoshioka",
        "X Xiao",
        "J Wu",
        "L Zhou",
        "S Ren",
        "Y Qian",
        "Y Qian",
        "J Wu",
        "M Zeng",
        "X Yu",
        "F Wei"
      ],
      "year": "2021",
      "venue": "WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing",
      "doi": "10.48550/ARXIV.2110.13900"
    },
    {
      "citation_id": "6",
      "title": "The measure and mismeasure of fairness: A critical review of fair machine learning",
      "authors": [
        "S Corbett-Davies",
        "S Goel"
      ],
      "year": "2018",
      "venue": "The measure and mismeasure of fairness: A critical review of fair machine learning",
      "arxiv": "arXiv:1808.00023"
    },
    {
      "citation_id": "7",
      "title": "Responsible AI: Gender bias assessment in emotion recognition",
      "authors": [
        "A Domnich",
        "G Anbarjafari"
      ],
      "year": "2021",
      "venue": "Responsible AI: Gender bias assessment in emotion recognition"
    },
    {
      "citation_id": "8",
      "title": "Speech emotion recognition with deep convolutional neural networks",
      "authors": [
        "D Issa",
        "M Demirci",
        "A Yazici"
      ],
      "year": "2020",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "9",
      "title": "Human-computer interaction: Overview on state of the art",
      "authors": [
        "F Karray",
        "M Alemzadeh",
        "J Abou Saleh",
        "M Arab"
      ],
      "year": "2017",
      "venue": "International journal on smart sensing and intelligent systems"
    },
    {
      "citation_id": "10",
      "title": "EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa",
      "authors": [
        "T Kim",
        "P Vossen"
      ],
      "year": "2021",
      "venue": "EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa"
    },
    {
      "citation_id": "11",
      "title": "Inherent Trade-Offs in the Fair Determi-nation of Risk Scores",
      "authors": [
        "J Kleinberg",
        "S Mullainathan",
        "M Raghavan"
      ],
      "year": "2016",
      "venue": "Inherent Trade-Offs in the Fair Determi-nation of Risk Scores"
    },
    {
      "citation_id": "12",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
    },
    {
      "citation_id": "13",
      "title": "BERT Word Embeddings Tutorial",
      "authors": [
        "C Mccormick",
        "N Ryan"
      ],
      "year": "2019",
      "venue": "BERT Word Embeddings Tutorial"
    },
    {
      "citation_id": "14",
      "title": "A survey on bias and fairness in machine learning",
      "authors": [
        "N Mehrabi",
        "F Morstatter",
        "N Saxena",
        "K Lerman",
        "A Galstyan"
      ],
      "year": "2021",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "15",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "E Barsoum",
        "C Zhang"
      ],
      "year": "2017",
      "venue": "2017 IEEE International conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "16",
      "title": "Improving Speech Emotion Recognition with Unsupervised Representation Learning on Unlabeled Speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2019",
      "venue": "ICASSP 2019 -2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis",
      "authors": [
        "S Poria",
        "I Chaturvedi",
        "E Cambria",
        "A Hussain"
      ],
      "year": "2016",
      "venue": "IEEE 16th International Conference on Data Mining (ICDM)"
    },
    {
      "citation_id": "18",
      "title": "bias and fairness in AI",
      "authors": [
        "S Prince"
      ],
      "year": "2019",
      "venue": "bias and fairness in AI"
    },
    {
      "citation_id": "19",
      "title": "ArXiv URL",
      "venue": "ArXiv URL"
    },
    {
      "citation_id": "20",
      "title": "Linguistic analysis of differences in portrayal of movie characters",
      "authors": [
        "A Ramakrishna",
        "V Martínez",
        "N Malandrakis",
        "K Singla",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P17-1153"
    },
    {
      "citation_id": "21",
      "title": "Efficient Emotion Recognition from Speech Using Deep Learning on Spectrograms",
      "authors": [
        "A Satt",
        "S Rozenberg",
        "R Hoory"
      ],
      "year": "2017",
      "venue": "Interspeech"
    },
    {
      "citation_id": "22",
      "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
      "authors": [
        "F Schroff",
        "D Kalenichenko",
        "J Philbin"
      ],
      "year": "2015",
      "venue": "FaceNet: A Unified Embedding for Face Recognition and Clustering"
    },
    {
      "citation_id": "23",
      "title": "Towards Learning a Universal Non-Semantic Representation of Speech",
      "authors": [
        "J Shor",
        "A Jansen",
        "R Maor",
        "O Lang",
        "O Tuval",
        "F De Chaumont Quitry",
        "M Tagliasacchi",
        "I Shavitt",
        "D Emanuel",
        "Y Haviv"
      ],
      "year": "2020",
      "venue": "Interspeech 2020. ISCA",
      "doi": "10.21437/interspeech.2020-1242"
    },
    {
      "citation_id": "24",
      "title": "Analyzing the Influence of Dataset Composition for Emotion Recognition",
      "authors": [
        "A Sutherland",
        "S Magg",
        "C Weber",
        "S Wermter"
      ],
      "year": "2021",
      "venue": "Analyzing the Influence of Dataset Composition for Emotion Recognition"
    },
    {
      "citation_id": "25",
      "title": "Multimodal emotion recognition on iemocap dataset using deep learning",
      "authors": [
        "S Tripathi",
        "S Tripathi",
        "H Beigi"
      ],
      "year": "2018",
      "venue": "Multimodal emotion recognition on iemocap dataset using deep learning",
      "arxiv": "arXiv:1804.05788"
    },
    {
      "citation_id": "26",
      "title": "Mitigating Biases in Multimodal Personality Assessment",
      "authors": [
        "S Yan",
        "D Huang",
        "M Soleymani"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 International Conference on Multimodal Interaction",
      "doi": "10.1145/3382507.3418889"
    },
    {
      "citation_id": "27",
      "title": "Multimodal Speech Emotion Recognition Using Audio and Text",
      "authors": [
        "S Yoon",
        "S Byun",
        "K Jung"
      ],
      "year": "2018",
      "venue": "Multimodal Speech Emotion Recognition Using Audio and Text"
    },
    {
      "citation_id": "28",
      "title": "Joint Face Detection and Alignment using Multitask Cascaded Convolutional Networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "Joint Face Detection and Alignment using Multitask Cascaded Convolutional Networks"
    },
    {
      "citation_id": "29",
      "title": "AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients",
      "authors": [
        "J Zhuang",
        "T Tang",
        "Y Ding",
        "S Tatikonda",
        "N Dvornek",
        "X Papademetris",
        "J Duncan"
      ],
      "year": "2020",
      "venue": "AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients",
      "doi": "10.48550/ARXIV.2010.07468"
    }
  ]
}