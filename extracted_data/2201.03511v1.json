{
  "paper_id": "2201.03511v1",
  "title": "A Study On Cross-Corpus Speech Emotion Recognition And Data Augmentation",
  "published": "2022-01-10T18:08:24Z",
  "authors": [
    "Norbert Braunschweiler",
    "Rama Doddipatla",
    "Simon Keizer",
    "Svetlana Stoyanchev"
  ],
  "keywords": [
    "speech emotion recognition",
    "cross-corpus",
    "data augmentation",
    "CNN-RNN bi-directional LSTM",
    "deep learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Models that can handle a wide range of speakers and acoustic conditions are essential in speech emotion recognition (SER). Often, these models tend to show mixed results when presented with speakers or acoustic conditions that were not visible during training. This paper investigates the impact of cross-corpus data complementation and data augmentation on the performance of SER models in matched (test-set from same corpus) and mismatched (test-set from different corpus) conditions. Investigations using six emotional speech corpora that include single and multiple speakers as well as variations in emotion style (acted, elicited, natural) and recording conditions are presented. Observations show that, as expected, models trained on single corpora perform best in matched conditions while performance decreases between 10-40% in mismatched conditions, depending on corpus specific features. Models trained on mixed corpora can be more stable in mismatched contexts, and the performance reductions range from 1 to 8% when compared with single corpus models in matched conditions. Data augmentation yields additional gains up to 4% and seem to benefit mismatched conditions more than matched ones.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition is attracting wide research interest in a variety of areas of which the most dominant one aims to improve humanmachine interaction, for example, to generate more adequate responses in content and style for dialogue systems. Moreover, it is used in other domains such as supporting doctors to tailor therapeutic techniques for patients or in the recording and analysis of emotional states during meetings and interviews. While there has been a lot of progress in emotion recognition, especially in speech emotion recognition (SER), there are still unresolved challenges to improve the performance as well as the robustness of models towards speakers and conditions not seen in training. Deep learning models may perform reasonably well on a single corpus, but often exhibit significantly lower performance when faced with new speakers in different acoustic environments.\n\nCopyright 2021 IEEE. Published in the 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU) (ASRU 2021), scheduled for 14-18 December 2021 in Cartagena, Colombia. Personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966.\n\nApproaches to increase the performance of SER models on individual corpora are plentiful in the literature, but the situation of handling a broad range of speaking styles and acoustic scenes is still challenging. Also, when focusing on single corpora there is no need to consider the details of different annotation schemes as mentioned in  [1] .\n\nIn contrast, the area of cross-corpus speech emotion recognition, in which data from multiple corpora is used to train models, is looking into the aspects of how multiple and often diverse corpora can help to improve SER performance in general and robustness against unseen data in particular. The diversity of corpora ranges from features such as language (cross-lingual), emotion style (acted, elicited, natural) to acoustic scene (studio recording, regular room recording, recordings with variable environments and background noises).\n\nOne of the questions in cross-corpus SER is whether the combination of corpora can improve or degrade the performance. The paper addresses this question on a selection of six emotional speech corpora with a broad range of attributes including multi-speaker and single speaker corpora; acted, elicited and what is deemed to be natural emotions; acoustic scenes ranging from high quality studio recordings up to mixed quality recordings with variable background noises.\n\nUsing these diverse corpora a state-of-the-art CNN-RNN emotion recognition model is trained and then tested in matched and mismatched conditions. Furthermore, corpora are combined to measure the impact of cross-corpus complementation on emotion classification performance in matched and mismatched conditions. Results are analyzed considering corpus specific characteristics and more detailed comparisons are conducted for features such as scripted vs. improvised emotions and studio recordings vs. regular office recordings.\n\nThe paper presents related work next, followed by the description of the deep learning model and the augmentation methods. Then, the individual corpora and their features are presented; the experimental set-up is laid out and finally results are presented and discussed.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "There are previous studies related to cross-corpus emotion recognition from  [2]  who use six corpora and different types of normalization to handle variations observed across these corpora.  [3]  introduce a domain-adaptive subspace learning method to reduce the feature space differences between source and target speech. In  [4]  a bidirectional LSTM with attention mechanism is used for classifying emotions across various corpora. To generalize to emotions across corpora authors suggest to use models trained on out-of-domain data and conduct adaptation to the missing corpus or use domain adversarial training (DAT)  [5] .  [6]  use a conditional cycle emotion generative adversarial network to generate synthetic data from the unlabeled target corpus to increase the variability in the source cor-pus and subsequently improve performance in cross-corpus emotion recognition.  [7]  present an approach aimed at learning more generalized features of emotional speech which uses a triplet network. Most recently  [8]  proposed a subspace learning method called joint distribution adaptive regression (JDAR) to reduce the feature distribution differences between samples from the training and test sets.\n\nMany of these works are based on relatively small emotional speech corpora or mix different languages. While the introduction of larger corpora such as CMU-MOSEI  [9]  which are also aimed at including more natural emotions has increased the amount of training data, it is still interesting to investigate whether combining different corpora can improve recognition results on other corpora as mentioned by  [4] . The current study adds 3 large single speaker corpora into the pool of available corpora for cross-corpus experiments thus enabling the study of speaker individual influences in combination with multi-speaker corpora of variable sizes and variable emotion types. This, in combination with a new state-of-the-art deep learning model architecture provides a fertile ground for interesting experiments.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "To investigate the impact of cross-corpus complementation and data augmentation, first, a state-of-the-art deep learning model was established. Then, focusing on the 4 emotion classes angry, happy, sad, neutral, which are the ones most widely found in emotional speech corpora, corpus-specific models were trained and evaluated in matched and mismatched conditions, followed by the creation of cross-corpus models and a combination with data augmentation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Model Architecture",
      "text": "Inspired by the emotion recognition model presented in  [10]  who derived their model from the triple attention network described in  [11] , who in turn based their model on work by  [12] , a similar architecture was tested using a bi-directional LSTM (BLSTM)  [13]  model with attention mechanism (henceforth called BLSTMATTsim). The model takes log-Mel filterbank features as input and encodes them in a BLSTM with 2 layers of 512 nodes followed by an attention mechanism which is then projected down to the emotions under consideration. As initial experiments with this architecture did not achieve the same results as published by  [10] , a new model design was chosen, motivated by the consideration that a combination of the strengths of CNNs (analysing spatial data) and RNNs (analysing sequential data) could improve performance further. The new model architecture feeds the same log-Mel filterbank features into a series of CNN and RNN layers followed by a stack of fully connected layers and an attention mechanism (see Figure  1  and henceforth called CNNRN-NATT). The bi-directional LSTM network architecture is chosen as it considers a temporal feature distribution over the whole input sequence to be useful for SER, as mentioned in  [10] .\n\nFor model training the machine learning toolkit LUDWIG 1  was used which is built on the tensorflow library 2    [14]  and uses python's SoundFile library  3  to read sound-files.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Data Augmentation",
      "text": "Data augmentation aims to improve model performance through an increase of the amount of training data which is typically done by altering existing training material in the frequency or time domain (e.g. Fig.  1 . Illustration of the CNNRNNATT model architecture  [15] ). Often signal based methods are used where modifications such as speed or volume perturbation are applied (e.g. in ASR  [16, 17] ) and new signals are generated, but there are also features based methods in which features derived from signals are altered (e.g.  [18] ). For this study, two data augmentation methods were tested: 1) speed and 2) volume perturbation.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Corpora",
      "text": "The following six speech corpora are used in the experiments: IEMOCAP  [19] , RAVDESS  [20] , CMU-MOSEI  [9]  and three single speaker expressive speech corpora entitled TF1, TF2, and TM1 ('F' = female, 'M' = male speaker).\n\nThe first three corpora are publicly available corpora often used in emotion recognition research, while the remaining three corpora are in-house expressive speech corpora. IEMOCAP, RAVDESS and CMU-MOSEI provide multi-speaker recordings of which CMU-MOSEI is the only one to include what is deemed to be natural emotions since it was recorded from YouTube videos. IEMO-CAP is considered to include 'elicited' emotions which are induced by scripted and improvised sessions between two actors (always male/female pairs). RAVDESS includes acted emotions, which is also the case for the three single speaker corpora. In terms of recording quality the three single speaker corpora are high quality professional studio recordings, followed by RAVDESS, IEMOCAP and CMU-MOSEI.\n\nCorpus selection was based on covering a wide range of emotional expressions from acted to natural and also the presence of the 4 emotion classes angry, happy, sad as well as neutral, which are the ones considered in this approach. Also, the presence of single speaker corpora and multi-speaker corpora was intended enabling a separate evaluation of combinations of them. In the following, each corpus is introduced in more detail and corpus specific splits into train/test-sets are explained.\n\nTable  1  provides an overview of the 6 corpora along with their number of utterances in total and by emotion. RAV: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song) includes North Amercian speech from 24 actors (12 male, 12 female) who recorded two sentences in different emotions including angry, happy, sad and neutral (only speech was used, no song samples). Speakers used two levels of intensity (nor-mal=1, strong=2) to realize emotions. Both levels were merged to indicate the presence of an emotion as opposed to absence. To split the data into 5 folds for cross-validation always 19 speakers were used for training and 5 for testing and the speakers were shifted in each fold.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iem Rav Mos Tf1 Tf2 Tm1",
      "text": "IEM: IEMOCAP (Interactive Emotional Dyadic Motion Capture)  [19]  a widely used benchmark corpus in speech emotion recognition research; contains 10 speakers (5 male, 5 female) recorded in 5 sessions with either scripted or improvised conversations in North American English. It provides about 12 hours of audiovisual recordings of which only speech was used. The corpus contains more than 9 emotions annotated by multiple labelers. It was recorded in a regular room with the microphone positioned between the two speakers, therefore individual speech files associated with a single-speaker can still include audible parts of the conversation partner in the background. Many papers  [4, 10, 21]  conduct evaluations on IEMO-CAP by selecting a subset of 4 emotions: angry, happy, neutral, and sad and build the happy class by combining 595 happy utterances and 1041 excited utterances, resulting in 1636 happy utterances; the same procedure was applied in this study. The 5 cross-validation sets were created with a leave-one-session-out method, i.e. set 1 used sessions 1-4 for training and session 5 for testing.\n\nMOS: CMU-MOSEI Carnegie Mellon University -Multimodal Opinion Sentiment and Emotion Intensity dataset  [9]  is a large corpus of more than 23.5k utterances extracted from more than 1000 YouTube videos which have been crowd-annotated with sentiment and emotion. The language is English but a number of accents are present and there is a wide variety of acoustic conditions including different domestic background noises. Since the data was not specifically designed for the purposes of emotion recognition, the emotions are considered to be natural. Each utterance was annotated by 3 crowd-workers using a range of 0-3 for each emotion in the set of anger, disgust, fear, happiness, sadness, and surprise. For this study, only the emotions angry, happy, sad and neutral (which is inferred by all emotions set to zero) are used. In addition, since utterances can have multiple emotion labels, only utterances for which there was a unequivocal annotation were chosen. For creating 5 folds for cross-validation, the first fold was based on the training, validation and test splits used for the ACL 2018 conference 4 and in other folds split randomly but proportionally to the number of utterances per emotion class.\n\nTF1,TF2,TM1: The three in-house, single speaker expressive speech corpora include high quality studio recordings which were conducted with detailed instructions for the speakers to deliver a consistent and distinctive speaking style for individual emotions. TF1 includes natural speech recorded from a British English female voice talent, TF2 provides expressive speech from a North American English female voice talent, and TM1 contains expressive speech from a North American English male voice talent. The data was split with a ratio of 80/20 into train/test-sets respectively and in each set split randomly but proportionally to the number of utterances in each emotion class.\n\nTable  2  shows the number of utterances used for training and testing for each of the six corpora as an example in the first fold.\n\nIn addition to the above mentioned corpora, the eNTERFACE  [22]  corpus was used as an example for a completely unseen corpus to test various models on. The corpus includes 44 speakers (8 female) of which only 43 speakers were used because speaker 6 was unsegmented. Each speaker produced 5 recordings of each emotion (acted) and only angry, happy, sad were used as there are no neutral recordings included. Speakers are speaking in different English accents.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Set-Up",
      "text": "Using the CNNRNNATT model architecture, corpus-dependent models were trained and performance variations were observed between models tested on matched test-sets (held-out data from the same corpus) and mismatched test-sets (held-out data from other corpora). To avoid the influence of particular splits into train/testsets, 5-fold-cross validation was used for each corpus. This also enables the evaluation of performance variations across splits. Corpus specific aspects were taken into consideration to create separate train/test-sets in the 5 folds as mentioned in Section 3 for each corpus.\n\nAfter evaluating single corpus models, corpora were combined and the impact on model performance measured. A model trained on all corpora was built and compared with the single corpus models as well as with subsets of corpora.\n\nTo compare the impact of data augmentation methods, speed and volume perturbation was used to augment corpora, effectively adding 2 copies of each corpus. The combination of data complementation and data augmentation was also tested, by training a model on the original data from all corpora plus the augmented data.\n\nFor measuring the performance of models the widely used unweighted accuracy (UA) and weighted accuracy (WA) metrics were used, as calculated by the following equations:\n\nwhere tp = true positives, tn = true negatives, fp = false positives, and fn = false negatives. UA considers each class to have the same weight, while WA considers the number of instances in each class to weigh its contribution. Because there is a large number of comparisons which could not all be fitted into the space restrictions in this article, results are mainly reported in UA.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "In an initial experiment the model architecture was established by first using a similar architecture as presented in  [10] , i.e. the BLST-MATTsim model and then developing the new CNNRNN architecture which are both introduced in 2.1.\n\nIEMOCAP  [19]  was used to evaluate model performance. To enable comparison with  [10]  the identical split into train/test-sets was used, i.e. 4290 utterances (sessions 1-4) for training and 1241 utterances (session 5) for testing.\n\nAudio files were converted into a log Mel-spectral representation using the 'fbank' setting in LUDWIG. The parameters for the spectrum extraction were: analysis window length = 26 ms, window shift = 9 ms, number of filter bands = 23. The audio file length limit was set to 7.0 seconds which truncates files longer than that and applies zero-padding to shorter files. Normalization was applied using the 'per file' setting which uses z-norm on a 'per file' level. All audio-files had a sampling rate of 16 kHz.\n\nResults are summarized in Table  3  which shows three models from the literature above the divider for comparison to our own models below the divider. Results show that the BLSTMATTsim architecture resulted in lower performance than the figures reported in  [10]  for the BLSTMATT model. The CNNRNNATT architecture achieved similar results as  [10] . The model named \"AttentionPooling\" from  [21]  uses an attention pooling based representation learning method for SER. While,  [23]  uses both text and audio in a multimodal model that combines attention modeling with a bi-directional gated recurrent unit (GRU). To get a more representative number for the CNNRNNATT model performance across different train/testsplits of IEMOCAP, 5-fold cross validation with leave-one-sessionout splits was used and results are also shown in Table  3 , which serves as our baseline for subsequent experiments.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Model",
      "text": "UA WA AttentionPooling  [21]  71.8 -Speech + Text  [23]  78.0 -BLSTMATT  [10]  80.1 73. The CNNRNNATT model takes the log-Mel filterbank features as input and maps them to a tensor which is then passed through a stack of 6 convolutional layers, followed by a stack of recurrent layers (here just 1), and a stack of 4 fully connected layers (sequence of nodes: 512-512-256-128) with batch normalization after each layer and dropout set to 0.2. The reduce function was set to null which means that the full vector was output. The output layer is fed into the Bahdanau attention mechanism  [24]  which passes its output to the emotion classifier projecting to the 4 classes. The loss function was selected as sampled softmax cross entropy.\n\nTo ensure a level playing field for the cross-corpus experiments all models were trained with the same CNNRNNATT architecture using identical hyper-parameter settings. Each model was trained for 200 epochs using the Adam optimiser  [25]  with an initial learning rate of 0.0001 and a batch size of 186. Following  [10]  the settings for reducing the learning rate when a plateau of validation measure is reached were 4 epochs for patience with a reduction rate of 0.8.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Augmentation",
      "text": "To test the impact of data augmentation techniques, first, the IEMO-CAP corpus was used. For each augmentation technique one copy of the original audio data was created by a) randomly altering the speed of the speech using the sox (v14.4.1, sox.sourceforge.net) speed effect within a factor-range of 0.6 and 1.5 (< 1 slows down, > 1 speeds up), and b) randomly varying the volume within the same factor range using the sox vol effect.\n\nTable  4  shows the results in average UA and average WA of 5-fold cross-validation when applying data augmentation to IEMO-CAP. The combination of speech and volume perturbation was also tested by generating 2 variants of each method and combining them with the original data thereby effectively increasing the original training data five times (shown in row entitled '2sp-2vol').\n\nTo check whether additional data created by other sox effects would result in further performance improvements, the effects bass (boost/cut lower frequencies), treble (boost/cut upper frequencies), overdrive (non-linear distortion), and tempo (changing playback speed but not pitch) were used to generate further variants which were used in the '7vars' model shown in Table  4  and increases the corpus size eight times.\n\nResults show that data augmentation with the mentioned effects can improve classification performance and additive improvements are observed when combining effects as seen in the '2sp-2vol' and '7vars' models. In addition, augmentation also seems to reduce the variance across folds as can be seen by the reduced standard deviations in augmented models. Not all effects are beneficial for performance boosting, especially effects which cause more extreme signal distortions or modulation factors which are too wide. It was also found that introducing more variation by random modulation factors seemed to boost performance more then choosing fixed factors.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Corpus",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Single Corpus And Cross-Corpus Models",
      "text": "Results for single corpus models and a model trained on all six corpora (All6) are shown in Table  5 . In addition, Table  6  shows results when corpora are augmented with speed and volume perturbation, and when again, all six are combined and speed and volume augmentation is applied (All6aug). The results for single corpus models show that all of them perform best on matched test-sets and there are performance decreases of different extends in mismatched conditions, i.e. generally, the multi-speaker, larger corpora IEM and MOS show smaller performance reductions than the single speaker corpora and the smaller, multi-speaker RAV corpus. Best performances are achieved by single speaker corpora with clean studio recordings, acted emotions and larger amounts of utterances for each emotion class (TF1, TF2, TM1), followed by multispeaker, acted emotions corpus RAV and the lowest performances are computed on the multi-speaker corpora MOS (natural emotions, variable recording conditions) and IEM (elicited and acted emotions, normal room recordings with occasionally audible cross-talk).\n\nThe model trained on all 6 corpora (All6), as expected, shows a significant improvement in overall performance across all test-sets with an average UA of 87.5%, while it does not achieve the same level for the matched test-sets as seen in the single corpus modelsan indication that the model fine-tunes very much onto a particular training corpus.\n\nAdding more training data by data augmentation (All6aug) shows another, albeit small performance gain.\n\nLooking at the distances in matched and mismatched conditions, average UA across all corpora for the 6 matched conditions is 89.8%, while for the 30 mismatched conditions it is 65.3%. This distance   7 .\n\nAverage unweighted accuracy (standard deviation in brackets) for models trained on IEMOCAP scripted (IEMscript) and improvised (IEMimpro) and on a combination of RAV+TF1+TF2+TM1 for the StudioScript corpus. Bold numbers are indicating matched conditions. is larger in single speaker corpora than in multi-speaker corpora. When adding augmented data the average UA across all corpora in matched conditions improves slightly to 90.1%, while the average UA across all mismatched conditions improves to 66.8%, i.e. a larger improvement than in the matched case. An indication that performance of single corpus models in mismatched conditions can be slightly boosted by data augmentation.\n\nTo investigate the influence of scripted vs. improvised subsets in IEMOCAP as well as recording conditions in IEMOCAP vs. studio recordings another experiment was carried out. 3 models are compared: IEMscript trained on the scripted utterances (2078 in training); IEMimpro trained in the improvised sessions (2212 in training); and StudioScript trained on a combination of scripted studio recordings including utterance-balanced subsets of the 4 corpora RAV+TF1+TF2+TM1 with the following number of utterances by corpus: RAV=450, TF1=518, TF2=518, TM1=518.\n\nTable  7  shows results when testing these models on the full IEM test-sets and on the sub-test-sets from scripted and improvised sessions. IEMimpro models on mismatched test-sets. As can be seen the overall performance of IEMimpro is better than IEMscript on the full IEM test-sets and also on mismatched test-sets from RAV, TF1, TF2 but not on TM1, an indication that training on the improvised version provided better performance in both matched and mismatched conditions. It is also interesting to observe a speaker dependent influence when looking at the performance differences between IEMscript and IEMimpro models on the mismatched single speaker corpora in Table 8: IEMimpro works more than 5% better on TF1 than IEMscript and 2% better on TF2, but on TM1 the IEMscript model is more than 3% better than IEMimpro.\n\nTo further test the performance on a completely unseen corpus both single corpus as well as the multi-corpus model (All6) were tested on the unseen eNTERFACE corpus. Results are shown in Table 9 and might be influenced by the special properties of eNTER-FACE, i.e. reverberation in recordings and multiple speakers speaking English in different accents. While the All6 model performs noticably better than the single-corpus models MOS, TF1 and TM1, it is slightly below IEM and TF2, and a bit less than 2% below RAV. An indication that just combining corpora does not solve the problem. Since the 3 large single speaker corpora share the same recording conditions it enables a more focused evaluation on the impacts of mixing corpora and testing in matched and mismatched conditions. These corpora were analyzed by training models on all combination variants and testing in matched and mismatched conditions. Results of mixtures of corpora show that in mismatched conditions adding one corpus generally improved performance despite not reaching the same levels as in the matched conditions, e.g. training on TF1+TF2 and testing on unseen speaker TM1 results in average UA of 71.1%, which is better than the single corpus mismatched cases of TF1 tested on TM1 with 66.8% and TF2 tested on TM1 with 65.3%, but still far lower than the single corpus matched case TM1 tested on TM1 with 99.6%. Table  10  shows an overview of the results for training on the single speaker corpora and combinations thereof.\n\nThere could be a gender influence when looking at the mismatched results of the single speaker corpora in which TM1, when tested on female speakers TF1 and TF2 performs 5-8% lower than testing the female speakers on the male speaker. However, TF1 actually performs slightly better on TM1 than it does on TF2, indicating that other aspects, likely to do with the style in which emotions are delivered, are at play here.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusions",
      "text": "A cross-corpus study for speech emotion recognition was presented to test performances of deep learning models in matched (test-set from same corpus) and mismatched (test-set from different corpus) conditions. Six corpora including variations along the dimensions of number of speakers, emotion type (acted, elicited, natural) and acoustic scene (studio recording, regular room recording, variable recording conditions including mixed background noise) were used in the study. Additionally, data augmentation was applied in the form of speed and volume perturbation.\n\nThe study has produced quantifiable evidence that the chosen model fine-tunes to the training data and when confronted with unseen data (mismatched condition) performs 10-40% worse than for seen data (matched condition).\n\nSingle speaker corpora with acted emotions recorded in studios perform best showing unweighted accuracy values up to 99.9% while there is a noticeable performance decline in the range of 10-24% for multi-speaker corpora with more variable recording conditions down to unweighted accuracy figures of 76.4%.\n\nRecording conditions are also influential with clean studio recordings easier to classify than regular office room recordings or recordings with mixed conditions in aspects such as environments, background noise, etc.\n\nOn the question whether the combination of corpora improves or degrades performance there is a mixed picture. The results showed that adding more data is generally beneficial in mismatched conditions and does not significantly decrease performance in matched conditions. However, there are corpus dependent variations, e.g. improvements were observed for single speaker corpora with added data from other single speaker corpora and when tested on unseen single speaker corpora, but not always when tested on multi-speaker corpora.\n\nAnother conclusion is, that if the test speaker is not in the training data then performance is significantly lower compared to models trained on single speaker corpora which see the speaker in training and test on held-out sets in testing.\n\nFurthermore, experiments on IEMOCAP showed that training on improvised data showed to be more beneficial than training on scripted data even when tested on scripted data.\n\nHowever, a model trained on all six corpora achieved the highest overall performance across all test-sets showing only small performance reductions of 1-7% compared to dedicated single corpus models.\n\nThe results on data augmentation indicate that additional performance gains can be achieved and especially single corpus models in mismatched conditions seem to benefit most. Performing data augmentation on the model trained on all corpora resulted in the best overall performance across all test-sets, showing that the combination of different corpora plus the addition of augmented data proved to have an additive effect on performance.\n\nThe study has shown, that the challenge of handling unseen speakers and different recording conditions in speech emotion recognition is still unresolved. More investigations are required to understand the variability of emotions and to create a model which is robust to speakers and recording conditions.",
      "page_start": 6,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: and henceforth called CNNRN-",
      "page": 2
    },
    {
      "caption": "Figure 1: Illustration of the CNNRNNATT model architecture",
      "page": 2
    }
  ],
  "tables": [
    {
      "caption": "Table 7: Average unweighted accuracy (standard deviation",
      "data": [
        {
          "Model trained on": "IEM\nRAV\nMOS\nTF1\nTF2\nTM1"
        },
        {
          "Model trained on": "76.4 (1.9)\n65.0 (1.4)\n66.2 (1.5)\n67.7 (1.6)\n65.9 (1.1)\n63.8 (1.0)\n85.9 (3.0)\n69.4 (1.1)\n64.4 (1.1)\n68.0 (1.7)\n68.1 (1.8)\n67.6 (2.7)\n77.4 (2.9)\n68.7 (1.7)\n65.7 (3.0)\n60.5 (1.1)\n64.2 (0.8)\n64.2 (1.3)\n99.4 (0.2)\n63.8 (4.8)\n63.4 (1.1)\n71.1 (2.1)\n67.8 (0.3)\n59.5 (2.0)\n99.9 (0.0)\n60.1 (2.3)\n64.9 (1.2)\n66.2 (2.9)\n65.6 (0.4)\n60.9 (0.8)\n99.6 (0.4)\n60.8 (2.8)\n66.6 (1.5)\n68.0 (2.7)\n66.8 (0.7)\n65.3 (0.3)"
        },
        {
          "Model trained on": "66.5 (6.1)\n68.6 (8.5)\n68.8 (4.7)\n71.3 (14.0)\n71.8 (13.8)\n69.3 (15.1)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 7: Average unweighted accuracy (standard deviation",
      "data": [
        {
          "Model trained on": "IEMaug\nRAVaug\nMOSaug\nTF1aug\nTF2aug\nTM1aug"
        },
        {
          "Model trained on": "76.5 (1.2)\n66.0 (0.9)\n66.4 (2.0)\n69.5 (1.3)\n67.9 (0.8)\n65.4 (0.6)\n87.3 (1.5)\n71.3 (3.4)\n64.2 (1.3)\n70.6 (1.8)\n70.6 (1.1)\n68.4 (3.2)\n77.8 (1.5)\n68.2 (4.1)\n62.9 (2.7)\n61.5 (1.2)\n68.3 (1.9)\n65.2 (4.1)\n99.4 (0.1)\n64.2 (5.5)\n65.6 (2.8)\n72.1 (2.0)\n72.2 (3.2)\n63.7 (1.2)\n99.9 (0.1)\n61.5 (0.8)\n65.1 (2.7)\n68.7 (4.0)\n63.5 (0.4)\n61.9 (2.0)\n99.5 (0.4)\n62.5 (3.0)\n67.0 (1.6)\n67.8 (1.4)\n72.0 (1.3)\n69.1 (1.6)"
        },
        {
          "Model trained on": "67.4 (5.8)\n69.0 (9.1)\n69.5 (4.8)\n72.8 (13.7)\n74.7 (12.5)\n70.7 (14.3)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 10: Average classification results in unweighted accuracy Onthequestionwhetherthecombinationofcorporaimprovesor",
      "data": [
        {
          "Model trained on": "TF1+TF2\nTF1+TM1\nTF2+TM1"
        },
        {
          "Model trained on": "77.0 (3.5)\n99.2 (0.1)\n99.0 (0.5)\n68.6 (2.0)\n99.9 (0.1)\n99.8 (0.2)\n71.1 (0.7)\n98.6 (1.8)\n99.3 (0.5)"
        },
        {
          "Model trained on": "90.1 (16.4)\n88.7 (17.4)\n92.0 (13.0)"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Demonstrating and modelling systematic time-varying annotator disagreement in continuous emotion annotation",
      "authors": [
        "M Atcheson",
        "V Sethu",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Demonstrating and modelling systematic time-varying annotator disagreement in continuous emotion annotation"
    },
    {
      "citation_id": "3",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "B Vlasenko",
        "F Eyben",
        "M Wollmer",
        "A Stuhlsatz",
        "A Wendemuth",
        "G Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Unsupervised cross-corpus speech emotion recognition using domain-adaptive subspace learning",
      "authors": [
        "N Liu",
        "Y Zong",
        "B Zhang",
        "L Liu",
        "J Chen",
        "G Zhao",
        "J Zhu"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    {
      "citation_id": "5",
      "title": "A crosscorpus study on speech emotion recognition",
      "authors": [
        "R Milner",
        "M Jalal",
        "R Ng",
        "T Hain"
      ],
      "year": "2019",
      "venue": "Proc. of ASRU, IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "6",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "M Abdelwahab",
        "C Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "7",
      "title": "A conditional cycle emotion GAN for cross corpus speech emotion recognition",
      "authors": [
        "B.-H Su",
        "C.-C Lee"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "8",
      "title": "Domain generalization with triplet network for cross-corpus speech emotion recognition",
      "authors": [
        "S.-W Lee"
      ],
      "year": "2021",
      "venue": "2021 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "9",
      "title": "Crosscorpus speech emotion recognition using joint distribution adaptive regression",
      "authors": [
        "J Zhang",
        "L Jiang",
        "Y Zong",
        "W Zheng",
        "L Zhao"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "10",
      "title": "Multi-attention recurrent network for human communication comprehension",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "P Vij",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "11",
      "title": "Empirical interpretation of speech emotion perception with attention based model for speech emotion recognition",
      "authors": [
        "M Jalal",
        "R Milner",
        "T Hain"
      ],
      "year": "2020",
      "venue": "Proc. of Interspeech"
    },
    {
      "citation_id": "12",
      "title": "Multi-modal sequence fusion via recursive attention for emotion recognition",
      "authors": [
        "R Beard",
        "R Das",
        "R Ng",
        "P Keerthana Gopalakrishnan",
        "L Eerens",
        "P Swietojanski",
        "O Miksik"
      ],
      "year": "2018",
      "venue": "Computational Natural Language Learning"
    },
    {
      "citation_id": "13",
      "title": "Dual attention networks for multimodal reasoning and matching",
      "authors": [
        "H Nam",
        "J.-W Ha",
        "J Kim"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Long short-term memory",
      "authors": [
        "Sepp Hochreiter",
        "Jurgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "15",
      "title": "TensorFlow: learning functions at scale",
      "authors": [
        "M Abadi"
      ],
      "year": "2016",
      "venue": "ACM Sigplan Notices"
    },
    {
      "citation_id": "16",
      "title": "Spoken language recognition using Xvectors",
      "authors": [
        "D Snyder",
        "D Garcia-Romero",
        "A Mccree",
        "G Sell",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2018",
      "venue": "Proc. Odyssey"
    },
    {
      "citation_id": "17",
      "title": "Audio augmentation for speech recognition",
      "authors": [
        "T Ko",
        "V Peddinti",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Exploring effective data augmentation with TDNN-LSTM neural network embedding for speaker recognition",
      "authors": [
        "C Huang"
      ],
      "year": "2019",
      "venue": "IEEE Automatic Speech Recognition and Understanding Workshop"
    },
    {
      "citation_id": "19",
      "title": "SpecAugment: A simple data augmentation method for automatic speech recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "SpecAugment: A simple data augmentation method for automatic speech recognition"
    },
    {
      "citation_id": "20",
      "title": "IEMOCAP: Interactive Emotional Dyadic Motion Capture Database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "21",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "22",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "P Li",
        "Y Song",
        "I Mcloughlin",
        "W Guo",
        "L Dai"
      ],
      "year": "2018",
      "venue": "Proc. of Interspeech, Hyderabad"
    },
    {
      "citation_id": "23",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "International Conference on Data Engineering Workshops, ICDE"
    },
    {
      "citation_id": "24",
      "title": "Conversational emotion analysis via attention mechanisms",
      "authors": [
        "Z Lian",
        "J Tao",
        "B Liu",
        "J Huang"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "25",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "D Bahdanau",
        "K Cho",
        "Y Bengio"
      ],
      "year": "2015",
      "venue": "Neural machine translation by jointly learning to align and translate"
    },
    {
      "citation_id": "26",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "Proc. of the 3rd International Conference for Learning Representations"
    }
  ]
}