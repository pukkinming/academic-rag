{
  "paper_id": "2503.23257v1",
  "title": "Fiesta: Fisher Information-Based Efficient Selective Test-Time Adaptation",
  "published": "2025-03-29T23:56:32Z",
  "authors": [
    "Mohammadmahdi Honarmand",
    "Onur Cezmi Mutlu",
    "Parnian Azizian",
    "Saimourya Surabhi",
    "Dennis P. Wall"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Robust facial expression recognition in unconstrained, \"inthe-wild\" environments remains challenging due to significant domain shifts between training and testing distributions. Test-time adaptation (TTA) offers a promising solution by adapting pre-trained models during inference without requiring labeled test data. However, existing TTA approaches typically rely on manually selecting which parameters to update, potentially leading to suboptimal adaptation and high computational costs. This paper introduces a novel Fisher-driven selective adaptation framework that dynamically identifies and updates only the most critical model parameters based on their importance as quantified by Fisher information. By integrating this principled parameter selection approach with temporal consistency constraints, our method enables efficient and effective adaptation specifically tailored for video-based facial expression recognition. Experiments on the challenging Af-fWild2 benchmark demonstrate that our approach significantly outperforms existing TTA methods, achieving a 7.7% improvement in F1 score over the base model while adapting only 22,000 parameters-more than 20 times fewer than comparable methods. Our ablation studies further reveal that parameter importance can be effectively estimated from minimal data, with sampling just 1-3 frames sufficient for substantial performance gains. The proposed approach not only enhances recognition accuracy but also dramatically reduces computational overhead, making test-time adaptation more practical for real-world affective computing applications.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Robust facial expression recognition in unconstrained, \"inthe-wild\" environments has become increasingly critical with the advent of applications such as driver-monitoring systems, interactive educational tools, and mental health di-* Corresponding author agnostics. In these real-world settings, the wide variability in factors such as illumination, camera quality, pose, and occlusions induces significant domain shifts between the training and testing distributions. As a consequence, models trained on well-curated datasets often fail to generalize to the diverse conditions encountered in practical deployments, necessitating robust adaptation strategies that can operate under such challenging circumstances.\n\nDomain shift remains a formidable challenge as the statistical properties of test data frequently deviate from those observed during training, leading to considerable performance degradation in deep learning models. Traditional domain adaptation and generalization techniques often rely on access to target domain data or extensive retraining, which may be impractical in dynamic or privacy-sensitive environments. Test-time adaptation (TTA) has emerged as a promising alternative that allows a pre-trained model to refine its parameters at inference using unlabeled test samples. This approach mitigates the need for large-scale reannotation and re-training, enabling the model to continuously adjust to evolving conditions during deployment.\n\nA promising direction within TTA leverages the temporal consistency inherent in video data. Methods that enforce temporal coherence assume that, in high frame-rate videos, consecutive frames exhibit gradual changes in appearance and, consequently, in model predictions  [38] . Such temporal consistency provides a self-supervision signal that can be exploited to adapt model parameters at test time. However, prior work in this domain typically relies on manually selecting a fixed subset of parameters. This manual selection, while effective in certain scenarios, may be computationally expensive and suboptimal, as it does not account for the dynamic importance of different parameters under varying test conditions.\n\nIn contrast, the sensitivity of a network's loss with respect to its parameters, as characterized by the Fisher Information Matrix  [12] , offers a principled metric for parameter importance. Fisher-based measures have been successfully employed in continual learning to mitigate catastrophic forgetting and have recently been explored in the context of test-time adaptation  [40] . By quantifying the contribution of each weight to the overall loss, Fisher scores enable a data-driven, selective adaptation strategy.\n\nThis paper introduces a novel Fisher-driven selective adaptation framework for TTA. The proposed method integrates Fisher information into the temporal consistency paradigm. By leveraging Fisher scores to dynamically assess parameter importance, our approach selectively updates only those weights that are most critical to model performance under test conditions. This principled, data-driven selection mechanism not only streamlines the adaptation process but also addresses the rigidity and inefficiency of manual layer selection.\n\nThe proposed Fisher-driven selective adaptation framework offers several key advantages. First, by focusing on dynamically selected, high-importance weights, the method significantly reduces computational overhead compared to approaches that update a large, fixed subset of parameters. Second, the selective updating process enhances model robustness, as evidenced by improved F1 scores in facial expression recognition tasks under diverse domain shifts. Finally, the approach is inherently well-suited for in-thewild affective behavior analysis, providing a scalable and efficient solution for real-world applications. Experimental results on the AffWild2 benchmark dataset  [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] 53] , demonstrate that the proposed approach yields competitive performance improvements over both the base supervised model and the base TTA method.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Facial Expression Recognition",
      "text": "Facial expression recognition (FER) is a central problem in affective computing, with applications ranging from user engagement in interfaces and gaming to driver monitoring, mental health assessment, and security systems. In realworld settings, FER models must cope with large intra-class variations caused by changes in lighting, head pose, occlusions (e.g. glasses, hands), and individual differences. Faces can appear in varied head poses and under diverse illumination, often partially occluded by objects or selfblocking (e.g. hand on face). Subject-specific differences (identity, age, gender) further modulate facial appearance, meaning the same expression can look different on different people, while different expressions can sometimes look similar. Such intra-class variance and inter-class ambiguity demand FER models that generalize across pose, lighting, occlusion, and identity variations. Handling these factors is critical for FER systems to perform reliably in unconstrained settings.\n\nEarly deep learning approaches for video-based FER predominantly employed convolutional neural networks (CNNs) to learn robust representations in unconstrained en-vironments  [4, 27, 31, 43] . More recently, transformerbased architectures have also been leveraged to capture long-range dependencies and relational features among facial regions, further enhancing recognition accuracy  [52] . Minimal work has addressed test-time adaptation (TTA) for FER, despite its promise in adapting to dynamic conditions such as lighting, pose, and occlusion without extra labeled data  [38] . In this work, we extend the temporal-consistencybased TTA paradigm to develop a more robust and adaptive FER system for real-world, unconstrained environments.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Test-Time Adaptation",
      "text": "Test-time adaptation (TTA) methods adjust a model during inference using only unlabeled test data, thereby addressing domain shifts that static training cannot anticipate. Early approaches in this space updated the running statistics of batch normalization (BN) layers  [30, 46]  to reflect the new test data distribution. Later, methods employed auxiliary self-supervised tasks to adapt backbone parameters  [47] , while others refined this strategy by minimizing output entropy and selectively updating BN weights-capitalizing on their high expressive power  [6, 48] . More recent techniques have also explored meta-learning solutions  [54]  and combined image augmentation with entropy minimization to handle larger domain shifts  [33, 55] .\n\nWhile effective on static data, these approaches generally overlook the rich temporal correlations inherent in video data, which are crucial for tasks like FER. Only a few works, such as  [48] , have ventured into continual adaptation for online streams, with  [49]  proposing augmentation consistency and  [7]  introducing a novel normalization layer to address non-independent and non-identically distributed (non-i.i.d.) test conditions.\n\nTemporal-consistency-based approaches have demonstrated that leveraging smooth transitions in sequential frames can yield more stable predictions  [38] . However, existing temporal-based methods often rely on manual or naïve selection of the subset of parameters to update, lacking a principled strategy to determine which weights are most beneficial to adapt.\n\nIn this work, we extend the temporal-consistency paradigm by introducing dynamic selection of model parameters. This approach eliminates the need for manual weight selection and avoids blindly choosing a subset, leading to computationally more efficient and better-adapted updates specifically tailored for video data.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Fisher Information Matrix",
      "text": "The Fisher Information Matrix (FIM) quantifies the sensitivity of a model's loss to changes in each parameter, making it an effective measure of weight importance. This concept is rooted in Bayesian statistics and was initially popularized through the use of Laplace approximations in neural network learning, as discussed by MacKay  [35] . Under this approximation, the posterior distribution of model parameters is approximated by a Gaussian whose precision (inverse variance) is given by the FIM's diagonal elements. As noted by Pascanu and Bengio  [41] , the diagonal elements of the FIM effectively capture the curvature of the loss surface near a minimum. A higher Fisher value for a weight implies that the model's loss is highly sensitive to changes in that parameter, suggesting that it is significantly contributing to the learned function. This idea of parameter importance estimation via Fisher information has been widely applied to preserve important knowledge in various learning paradigms. In the realm of continual learning, Kirkpatrick et al.  [12]  introduced Elastic Weight Consolidation (EWC), which uses the Fisher information of each parameter to regularize learning on new tasks. Further extended by works such as  [32] ,  [44] ,  [5] ,  [39] ,  [36] , and  [51] , Fisher-based regularization is employed to prevent catastrophic forgetting by constraining critical weights from deviating too far from their previously learned values.\n\nMore recently, the concept has has been adopted in testtime adaptation frameworks. Niu et al.  [40]  proposed an efficient test-time adaptation method that explicitly addresses forgetting by computing Fisher information on-the-fly for the test distribution. In a similar vein, Brahma et al.  [1]  leverage Fisher information in a lifelong test-time adaptation framework. They stochastically update the model on a stream of target data and use a Fisher-based criterion to decide when to reset certain parameters back to their source values, rather than resetting weights at random.\n\nThese works demonstrate that Fisher information is a powerful tool for selective learning under domain shift. Motivated by this, our approach applies a Fisher-based selection strategy within a test-time adaptation framework, enabling the model to adapt to new facial expression domains while updating only the parameters most vital to its base performance. To the best of our knowledge, there has been no prior work on selective test-time adaptation driven by Fisher scores. Existing TTA solutions either treat all parameters equally or rely on manual selection to decide which weights to update, often leading to suboptimal or computationally heavy outcomes. Our proposed method addresses this gap by systematically computing Fisher scores for each parameter at test time, then selectively updating only the most critical parameters. This strategy not only minimizes computational overhead but also robustly updates the core knowledge necessary for accurate facial expression recognition under domain shifts, representing a novel contribution to the field.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Method",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets And Preprocessing",
      "text": "To build a comprehensive and diverse training set, several well-known datasets from the facial expression recognition literature were merged, including Affwild2  [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] 53] , Affectnet  [37] , and the Real-world Affective Faces Database (RAF-DB)  [28, 29] . The model is designed to classify images into seven basic emotions-commonly referred to as Ekman emotions [3]-along with an additional \"other\" category for expressions that do not fall into these predefined classes. Affwild2 is notably larger than the other datasets and exhibits a significant imbalance in label distribution. To address this issue, a random sampling strategy is applied, limiting the number of frames per video per emotion category to 300, following the approach in  [38] . The specific label distribution after this sampling is detailed in Table 1. Since Affwild2 already provides cropped and aligned images, and the remaining datasets are available only as cropped images, no further spatial preprocessing is necessary. All images are then resized to 112px × 112px using antialiasing. Additionally, we implement common image augmentation techniques during training, such as random horizontal flip, color jitter, channel dropout, brightness and contrast shift, blur, and histogram equalization.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Model",
      "text": "The model architecture and training strategy follow prior work on test-time adaptation for facial expression recognition  [38] . Predictions are made on individual video frames, allowing the use of well-established image-processing architectures. Given their reliable performance and stable training, models from the ResNet  [8]  family are employed, incorporating enhancements such as aggregated residual transformations  [50]  and squeeze-and-excitation blocks  [10] . The resulting embeddings are then passed through two fully connected layers, with the final (output) layer undergoing both weight and input normalization [45] to mitigate overconfidence, ensure smoothness, and enhance generalization.\n\nA critical challenge in this setting is the significant class imbalance, which must be addressed for effective supervised learning. Traditional solutions such as label weighting, up-sampling, or down-sampling often prove inadequate in certain scenarios. To address this, Label-Distribution-Aware Margin Loss (LDAM)  [2]  is adopted. Unlike standard sample weighting, which scales the loss multiplicatively, LDAM adjusts the class margins based on the frequency of each class. The precise formulation is given in Eq. (  1 ), where z is the unnormalized prediction vector, y is the ground truth label, n j denotes the number of samples in class j, and C is a temperature-like hyperparameter that modulates the margin's impact. By enforcing larger margins for minority classes, LDAM enhances the model's robustness and reduces overfitting.\n\nSupervised training is then conducted using backpropagation with the defined LDAM loss to account for the skewed label distribution. Optimization is performed using the Adam  [11]  optimizer with weight decay  [34] , following a step-decay schedule for the learning rate. The modeling and training processes are implemented in the PyTorch  [42]  framework on NVIDIA V100 GPUs.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Temporal Smoothing Domain Adaptation",
      "text": "Following the methodology in  [38] , we adopt a temporalconsistency-based domain adaptation strategy to enhance prediction stability across video frames. Since 2D CNNs are trained on individual static images rather than video sequences, they lack an inherent bias for smooth and consistent predictions across consecutive frames. It has been observed that these models tend to produce outputs with pronounced high-frequency components. However, when a low-pass filter is applied to the predictions, the resulting outputs appear more coherent. This observation is leveraged by using the filtered predictions as a supervision signal to tune the network, thereby encouraging temporal consistency. Specifically, the model's outputs are first smoothed over time using a low-pass filter. This smoothed version is designated as the target, and the mean-squared error between the original and filtered predictions is computed. This error signal is then backpropagated to update a selected subset of model parameters.\n\nLet x (t) ∈ R 112×112×3 denote the t th frame of a video, and let f (.) : R 112×112×3 → R 8 represent the neural network. It is hypothesized that ensuring coherence between predictions on consecutive frames can serve as an implicit Jacobian regularizer. Previous work  [9]  has shown that regularizing the Frobenius norm of the network's input-output Jacobian can help achieve flatter minima and improve robustness to input variations. When the video's frame rate is sufficiently high, the Jacobian can be approximated as follows:\n\nThus, minimizing the Frobenius norm of the Jacobian essentially amounts to reducing the difference between consecutive frame predictions:\n\nEmpirical analysis reveals that the distribution of prediction differences is heavy-tailed. This behavior is mainly due to abrupt changes in predictions caused by issues like improper cropping or sudden activation shifts stemming from model imperfections. Directly using the formulation in Eq. (  3 ) can lead to instability during training because these outliers disrupted the optimization process. To mitigate this, an equivalent formulation is adopted: all frames are first processed to obtain a set of unnormalized scores y (t) ∈ R 8 and then a self-supervision loss function is defined as follows:\n\nHere, LP F (.) represents a low-pass filter (a median filter is used in experiments for its robustness to outliers). For long videos, processing every frame for adaptation can be computationally expensive. To address this, a sliding window is employed to count changes in the model's predictions, selecting only those regions exhibiting the most significant variations to form the training batch. The loss function is then updated as in Eq. (  5 ), where R denotes the set of selected regions and r represents a range of frames within a region:\n\nBecause this loss is differentiable, backpropagation can be used to update model parameters. The selection of which parameters to update is critical, as it impacts the model's expressivity and the effectiveness of the adaptation. In this work, we investigate whether a more systematic and dynamic approach to parameter selection exists, instead of relying on manual choices. To achieve this, we employ a Fisher scoring weight selection strategy, which is discussed in depth in Section 3.4. Finally, the adaptation process is carried out using the AdamW optimizer with a learning rate of 0.0001, and we perform 4 gradient steps-a setting that we found to be empirically optimal.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Fisher Scoring Weight Selection",
      "text": "In this section, we propose a novel Fisher scoring-based strategy to identify and select the most important model weights, updating only these during backpropagation. Let ϕ(θ i ) represent the importance of the parameter θ i , which we estimate using the diagonal Fisher information matrix as done in elastic weight consolidation  [12] . Typically, computing the Fisher information requires a set of labeled indistribution (ID) training samples. However, in our setting we do not have access to the training data, and our test samples are unlabeled, making it challenging to assess weight importance.\n\nInspired by the method introduced in  [40] , we address this challenge by first collecting a small set of N unlabeled sample frames x (t) from the test video. We then use the originally trained model f Θ (•) to predict these samples, thereby obtaining pseudo-labels ŷ(t) . With these, we construct a pseudo-labeled set of sampled frames Q. We calculate the Fisher importance of each model weight θ i as an average of its Fisher scores over this set using the following formula:\n\nwhere L CE denotes the cross-entropy loss. We experimented with different numbers of frames sampled N (see Section 4 for detailed results), but in most cases a 1-3 frames per video were sufficient. After calculating the Fisher scores as specified in Eq. 6, we rank the model's weights by importance and select the top k% of weights for updating during backpropagation. In this work, we experimented with adapting different percentages of model weights, both from early layers and across all layers, with detailed results provided in Section 4. The Fisher-driven selected weights then undergo the temporal smoothing domain adaptation module, as described in Section 3.3, and the adapted model is subsequently used for label prediction. A comprehensive overview of our complete methodology and framework is presented in Fig.  1 .",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experiments",
      "text": "We evaluate our Fisher-driven selective adaptation framework on the challenging task of facial expression recognition (FER) in-the-wild using the AffWild2  [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] 53]  validation dataset. In our experiments, we report the macro F1 score calculated across predictions on all frames of all videos, which is a standard evaluation metric for expression recognition tasks, especially in cases with class imbalance.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "Our base model employs an SE-ResNeXt-101  [10]  backbone trained on a combination of facial expression recognition datasets (AffWild2  [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24] [25] [26] 53] , AffectNet  [37] , and RAF-DB  [28, 29] ). This approach provides a strong foundation model that has already learned to recognize expressions across a diverse range of subjects and conditions, serving as a challenging baseline to improve upon.\n\nIn our experiments, we compare several test-time adaptation methods. First, we evaluate TENT  [48] , which leverages entropy minimization and has shown competitive performance on various benchmarks. We also assess Temporal Smoothing Adaptation, derived from the TempT framework  [38] . For this approach, we explore multiple weight selection strategies in addition to our newly proposed Fisher Scoring Weight Selection method.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Performance Comparison",
      "text": "Table  2  summarizes the comparative performance of all methods, including the F1 scores achieved on the validation set and the number of weights adapted during the test-time adaptation process. The results reveal several key findings. First, we observe that TENT  [48] , despite its success in other test-time adaptation scenarios, significantly degrades performance on facial expression recognition, with a substantial 17.2% decrease in F1 score compared to the base model. This highlights the challenges of applying generalpurpose adaptation methods to the emotionally nuanced and temporally sensitive domain of facial expressions.\n\nFor the regular Temporal Smoothing Adaptation approach, we find that performance is heavily dependent on the selection of which subset of model layers and weights to adapt. In our experiments, we manually selected different subsets for adaptation: • Early layers: Including up to the end of the first layer of the SE-ResNeXt model • Mid layers: Including the 2nd and 3rd layers • Late layers: Including the 4th layer until the end of the model • All layers: The entire model\n\nThe results demonstrate the critical impact of layer selection. While adapting early layers yields a 6.2% improvement over the base model, adapting mid layers causes an 11.4% performance drop. Adapting late layers provides a modest 1.54% improvement, and adapting all layers actually degrades performance by 7.7%. This inconsistency underscores a significant limitation of the regular adaptation approach: it requires extensive experimentation to identify which layers to adapt for optimal performance, making it impractical for real-world deployment. Furthermore, most variations of regular temporal smoothing adaptation require adapting a substantial number of weights-in the order of millions-with the exception of the early layers version, which still necessitates updating 456,000 parameters. This high computational demand presents a significant barrier for deployment in resource-constrained environments or real-time applications.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Fisher-Based Selective Adaptation",
      "text": "Our proposed Fisher scoring weight selection method addresses both the performance inconsistency and computational inefficiency of previous approaches. The method selects the most important weights to adapt using Fisher scores based on pseudo labels of the model on the target domain at test time. This approach personalizes the weight selection process to each individual video at test time, making the adaptation both more effective and computationally efficient. We implemented our Fisher-based selection using two strategies:\n\n• Selecting the top 0.2% of weights across the entire model based on Fisher scores • Selecting the top 5% of weights from the early layers of the model Both strategies yield impressive results. When selecting from the entire model, our method achieves an F1 score of 0.341, representing a 4.9% improvement over the base model. When focusing on early layers, it reaches an F1 score of 0.350, a 7.7% improvement that surpasses all other adaptation approaches. This performance is particularly notable considering that it outperforms the base SE-ResNeXt-101 model, which is already a very large and powerful network for facial expression recognition.\n\nPerhaps most significantly, our Fisher-based approach dramatically reduces the computational burden of adaptation. When selecting from early layers, it requires updating only 22,000 weights out of the model's total 92,000,000 parameters-more than 20 times fewer than the regular early layers adaptation and over 4,000 times fewer than adapting the entire model. Even when selecting from all layers, our method still maintains a minimal footprint, adapting just 183,000 weights.\n\nThese results demonstrate that our Fisher-driven selective adaptation framework not only improves recognition performance but does so with substantially greater efficiency than existing approaches. By dynamically identifying the most important weights for each test video, the method eliminates the need for manual layer selection while achieving superior performance with minimal computational overhead.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Ablation Studies",
      "text": "To further investigate the effectiveness of our Fisher-driven selective adaptation approach, we conducted ablation studies examining two critical aspects of the method: (1) the impact of the percentage threshold used for weight selection, and (2) the effect of averaging Fisher scores across multiple frames when estimating parameter importance.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Impact Of Percentage Threshold",
      "text": "Fig.  2  presents the performance trends of our approach across different percentage thresholds for both adaptation strategies: selecting from early layers only (left plot) and selecting from all model weights (right plot). The horizontal dashed line represents the base model performance (F1 = 0.325), with the green region above indicating improvement and the red region below showing performance degradation.\n\nWe observe that performance remains robust across a wide range of percentage thresholds when using a small number of frames  (1) (2) (3)  for Fisher score calculation. This relative insensitivity to the exact percentage threshold suggests that a very limited subset of weights is responsible for the majority of the adaptation benefits. This finding strongly supports our core hypothesis that selective adaptation of the most critical parameters can yield substantial performance improvements while minimizing computational overhead.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Effect Of Frame Sampling",
      "text": "Perhaps counterintuitively, our results demonstrate that averaging Fisher scores over multiple frames does not provide additional benefits and frequently leads to performance degradation. As shown in Fig.  2 , models using Fisher scores calculated from 5, 10, or 15 frames consistently underperform compared to those using only 1 or 3 frames, with many configurations falling below baseline performance.\n\nThis phenomenon can be explained by considering the nature of Fisher information in the context of facial expression recognition. Fisher scores measure the sensitivity of the loss with respect to parameters for specific inputs. When averaging across multiple frames, especially in videos with expression transitions or significant variations, the resulting averaged Fisher scores may no longer accurately reflect the most critical parameters for any particular frame. This \"dilution effect\" appears to diminish the precision of the parameter importance estimation, leading to suboptimal adaptation.\n\nIn contrast, using Fisher scores from a single frame or a very small number of frames (1-3) maintains the specificity of the importance estimation. The slight performance fluctuations observed with these lower frame counts across different percentage thresholds likely reflect the sensitivity of Fisher scores to the specific content of the sampled frames. Despite these fluctuations, the performance remains consistently above the baseline, highlighting the robustness of the overall approach.\n\nThe superior performance of low-frame sampling validates our selective adaptation strategy while suggesting that parameter importance can be effectively estimated from minimal data. This has significant practical implications, as it enables extremely efficient adaptation with minimal computational overhead-sampling just 1-3 frames and updating less than 0.5% of the model's parameters is sufficient to achieve substantial performance improvements.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we introduced a novel Fisher-driven selective adaptation framework for test-time domain adaptation in facial expression recognition. Our approach addresses critical limitations of existing test-time adaptation methods by employing Fisher information to dynamically identify and update only the most important model parameters during adaptation.\n\nOur experimental results on the challenging AffWild2 benchmark demonstrate that the proposed method consistently outperforms both the baseline model and existing adaptation techniques. This performance gain is particularly significant considering that it surpasses methods that adapt substantially more parameters, including those that update the entire network.\n\nThe computational efficiency of our approach represents a major advancement for practical applications. By reducing the number of adapted parameters from millions to a few thousand, our method enables efficient adaptation even in resource-constrained environments. This dramatic reduction in computational overhead, combined with improved performance, makes our approach particularly well-suited for real-world deployment.\n\nOur ablation studies revealed several important insights. Most notably, we found that averaging Fisher scores across multiple frames often degrades performance compared to using scores from just a single frame or very few frames. This finding suggests that parameter importance estimation is most effective when focused on specific instances rather than averaged across diverse inputs-particularly in domains like facial expression recognition where subtle temporal variations carry significant semantic meaning.\n\nBeyond facial expression recognition, the proposed selective adaptation framework offers a principled approach to test-time adaptation that could be extended to other computer vision tasks facing domain shift challenges. By eliminating the need for manual layer selection and providing a data-driven mechanism for identifying adaptation-critical parameters, our method addresses the fundamental limitations of existing approaches that rely on heuristics or exhaustive experimentation.\n\nWhile our work demonstrates significant improvements, several avenues for future research remain open. Investigating dynamic adjustment of the percentage threshold based on domain characteristics, exploring alternative parameter importance metrics beyond Fisher information, and extending the approach to more diverse visual understanding tasks could further enhance the method's applicability. Additionally, investigating how selective adaptation might be combined with other complementary strategies such as selfsupervision or entropy minimization represents a promising direction for future exploration.\n\nIn conclusion, our Fisher-driven selective adaptation framework represents a significant step toward more efficient, effective, and practical test-time adaptation for facial expression recognition in unconstrained environments, opening new possibilities for robust affective computing in real-world applications.",
      "page_start": 8,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Overview of our proposed selective test-time domain adaptation framework using Fisher information. The method consists of",
      "page": 4
    },
    {
      "caption": "Figure 1: 4. Experiments",
      "page": 6
    },
    {
      "caption": "Figure 2: presents the performance trends of our approach",
      "page": 7
    },
    {
      "caption": "Figure 2: , models using Fisher scores",
      "page": 7
    },
    {
      "caption": "Figure 2: Ablation study on the effect of percentage threshold and frame sampling on adaptation performance. Left: Results when selecting",
      "page": 8
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Happiness": "2622\n5670\n347",
          "Surprise": "5540\n19325\n846",
          "Neutral": "44676\n55670\n3096",
          "Disgust": "7851\n19650\n2390",
          "Anger": "32962\n118605\n5771",
          "Fear": "9730\n11647\n1571",
          "Sadness": "3296\n3626\n865",
          "Other": "31412\n0\n0",
          "Total": "138089\n234193\n14886"
        },
        {
          "Happiness": "8639",
          "Surprise": "25711",
          "Neutral": "103442",
          "Disgust": "29891",
          "Anger": "157338",
          "Fear": "22948",
          "Sadness": "7787",
          "Other": "31412",
          "Total": "387168"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "A probabilistic framework for lifelong test-time adaptation",
      "authors": [
        "Dhanajit Brahma",
        "Piyush Rai"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "2",
      "title": "Learning imbalanced datasets with label-distribution-aware margin loss",
      "authors": [
        "Kaidi Cao",
        "Colin Wei",
        "Adrien Gaidon",
        "Nikos Arechiga",
        "Tengyu Ma"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "3",
      "title": "Basic emotions. Handbook of cognition and emotion",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1999",
      "venue": "Basic emotions. Handbook of cognition and emotion"
    },
    {
      "citation_id": "4",
      "title": "Facial expression recognition with deeplysupervised attention network",
      "authors": [
        "Yingruo Fan",
        "O Victor",
        "Jacqueline Li",
        "Lam"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "Orthogonal gradient descent for continual learning",
      "authors": [
        "Mehrdad Farajtabar",
        "Navid Azizan",
        "Alex Mott",
        "Ang Li"
      ],
      "venue": "International conference on artificial intelligence and statistics"
    },
    {
      "citation_id": "6",
      "title": "Training batchnorm and only batchnorm: On the expressive power of random features in cnns",
      "authors": [
        "Jonathan Frankle",
        "David Schwab",
        "Ari Morcos"
      ],
      "year": "2020",
      "venue": "Training batchnorm and only batchnorm: On the expressive power of random features in cnns",
      "arxiv": "arXiv:2003.00152"
    },
    {
      "citation_id": "7",
      "title": "Note: robust continual test-time adaptation against temporal correlation",
      "authors": [
        "Taesik Gong",
        "Jongheon Jeong",
        "Taewon Kim",
        "Yewon Kim",
        "Jinwoo Shin",
        "Sung-Ju Lee"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "8",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "9",
      "title": "Robust learning with jacobian regularization",
      "authors": [
        "Judy Hoffman",
        "Daniel Roberts",
        "Sho Yaida"
      ],
      "year": "2019",
      "venue": "Robust learning with jacobian regularization",
      "arxiv": "arXiv:1908.02729"
    },
    {
      "citation_id": "10",
      "title": "Squeeze-andexcitation networks",
      "authors": [
        "Jie Hu",
        "Li Shen",
        "Gang Sun"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "11",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2014",
      "venue": "Adam: A method for stochastic optimization",
      "arxiv": "arXiv:1412.6980"
    },
    {
      "citation_id": "12",
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": [
        "James Kirkpatrick",
        "Razvan Pascanu",
        "Neil Rabinowitz",
        "Joel Veness",
        "Guillaume Desjardins",
        "Andrei Rusu",
        "Kieran Milan",
        "John Quan",
        "Tiago Ramalho",
        "Agnieszka Grabska-Barwinska"
      ],
      "year": "2005",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "13",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "14",
      "title": "Abaw: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "15",
      "title": "Multi-label compound expression recognition: C-expr database & network",
      "authors": [
        "Dimitrios Kollias"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "16",
      "title": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, affect, action unit recognition: Aff-wild2, multi-task learning and arcface",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "17",
      "title": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect analysis in-the-wild: Valence-arousal, expressions, action units and a unified framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "18",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "19",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "Kollias",
        "E Schulc",
        "Hajiyev",
        "Zafeiriou"
      ],
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "20",
      "title": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2019",
      "venue": "Face behavior a la carte: Expressions, affect and action units in a single network",
      "arxiv": "arXiv:1910.11111"
    },
    {
      "citation_id": "21",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Björn Zhao",
        "Irene Schuller",
        "Stefanos Kotsia",
        "Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "22",
      "title": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2021",
      "venue": "Distribution matching for heterogeneous multi-task learning: a large-scale face study",
      "arxiv": "arXiv:2105.03790"
    },
    {
      "citation_id": "23",
      "title": "Abaw: Valencearousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alice Baird",
        "Alan Cowen",
        "Stefanos Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Distribution matching for multi-task learning of classification tasks: a large-scale study on faces & beyond",
      "authors": [
        "Dimitrios Kollias",
        "Viktoriia Sharmanska",
        "Stefanos Zafeiriou"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "The 6th affective behavior analysis in-the-wild (abaw) competition",
      "authors": [
        "Dimitrios Kollias",
        "Panagiotis Tzirakis",
        "Alan Cowen",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Alice Baird",
        "Chris Gagne",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "26",
      "title": "7th abaw competition: Multi-task learning and compound expression recognition",
      "authors": [
        "Dimitrios Kollias",
        "Stefanos Zafeiriou",
        "Irene Kotsia",
        "Abhinav Dhall",
        "Shreya Ghosh",
        "Chunchang Shao",
        "Guanyu Hu"
      ],
      "year": "2024",
      "venue": "7th abaw competition: Multi-task learning and compound expression recognition",
      "arxiv": "arXiv:2407.03835"
    },
    {
      "citation_id": "27",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "28",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for unconstrained facial expression recognition",
      "authors": [
        "Shan Li",
        "Weihong Deng"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "29",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "Shan Li",
        "Weihong Deng",
        "Junping Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "30",
      "title": "Revisiting batch normalization for practical domain adaptation",
      "authors": [
        "Yanghao Li",
        "Naiyan Wang",
        "Jianping Shi",
        "Jiaying Liu",
        "Xiaodi Hou"
      ],
      "year": "2016",
      "venue": "Revisiting batch normalization for practical domain adaptation",
      "arxiv": "arXiv:1603.04779"
    },
    {
      "citation_id": "31",
      "title": "Occlusion aware facial expression recognition using cnn with attention mechanism",
      "authors": [
        "Yong Li",
        "Jiabei Zeng",
        "Shiguang Shan",
        "Xilin Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "32",
      "title": "Learning without forgetting",
      "authors": [
        "Zhizhong Li",
        "Derek Hoiem"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "33",
      "title": "Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation",
      "authors": [
        "Jian Liang",
        "Dapeng Hu",
        "Jiashi Feng"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "34",
      "title": "Fixing weight decay regularization in adam",
      "authors": [
        "Ilya Loshchilov",
        "Frank Hutter"
      ],
      "year": "2017",
      "venue": "Fixing weight decay regularization in adam"
    },
    {
      "citation_id": "35",
      "title": "A practical bayesian framework for backpropagation networks",
      "authors": [
        "J David",
        "Mackay"
      ],
      "year": "1992",
      "venue": "Neural computation"
    },
    {
      "citation_id": "36",
      "title": "Essentials for class incremental learning",
      "authors": [
        "Sudhanshu Mittal",
        "Silvio Galesso",
        "Thomas Brox"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "37",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "Ali Mollahosseini",
        "Behzad Hasani",
        "Mohammad Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "38",
      "title": "Tempt: Temporal consistency for test-time adaptation",
      "authors": [
        "Cezmi Onur",
        "Mohammadmahdi Mutlu",
        "Saimourya Honarmand",
        "Dennis Surabhi",
        "Wall"
      ],
      "year": "2006",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "Disturbance-immune weight sharing for neural architecture search",
      "authors": [
        "Shuaicheng Niu",
        "Jiaxiang Wu",
        "Yifan Zhang",
        "Yong Guo",
        "Peilin Zhao",
        "Junzhou Huang",
        "Mingkui Tan"
      ],
      "year": "2021",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "40",
      "title": "Efficient test-time model adaptation without forgetting",
      "authors": [
        "Shuaicheng Niu",
        "Jiaxiang Wu",
        "Yifan Zhang",
        "Yaofo Chen",
        "Shijian Zheng",
        "Peilin Zhao",
        "Mingkui Tan"
      ],
      "year": "2022",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "41",
      "title": "Revisiting natural gradient for deep networks",
      "authors": [
        "Razvan Pascanu",
        "Yoshua Bengio"
      ],
      "year": "2013",
      "venue": "Revisiting natural gradient for deep networks",
      "arxiv": "arXiv:1301.3584"
    },
    {
      "citation_id": "42",
      "title": "Pytorch: An imperative style, highperformance deep learning library",
      "authors": [
        "Adam Paszke",
        "Sam Gross",
        "Francisco Massa",
        "Adam Lerer",
        "James Bradbury",
        "Gregory Chanan",
        "Trevor Killeen",
        "Zeming Lin",
        "Natalia Gimelshein",
        "Luca Antiga"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "43",
      "title": "Computer vision estimation of emotion reaction intensity in the wild",
      "authors": [
        "Yang Qian",
        "Ali Kargarandehkordi",
        "Cezmi Onur",
        "Saimourya Mutlu",
        "Mohammadmahdi Surabhi",
        "Dennis Honarmand",
        "Peter Wall",
        "Washington"
      ],
      "year": "2023",
      "venue": "Computer vision estimation of emotion reaction intensity in the wild"
    },
    {
      "citation_id": "44",
      "title": "Experience replay for continual learning",
      "authors": [
        "David Rolnick",
        "Arun Ahuja",
        "Jonathan Schwarz",
        "Timothy Lillicrap",
        "Gregory Wayne"
      ],
      "year": "2019",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "45",
      "title": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks",
      "authors": [
        "Tim Salimans",
        "P Durk",
        "Kingma"
      ],
      "year": "2016",
      "venue": "Weight normalization: A simple reparameterization to accelerate training of deep neural networks"
    },
    {
      "citation_id": "46",
      "title": "Improving robustness against common corruptions by covariate shift adaptation",
      "authors": [
        "Steffen Schneider",
        "Evgenia Rusak",
        "Luisa Eck",
        "Oliver Bringmann",
        "Wieland Brendel",
        "Matthias Bethge"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "47",
      "title": "Test-time training with self-supervision for generalization under distribution shifts",
      "authors": [
        "Yu Sun",
        "Xiaolong Wang",
        "Zhuang Liu",
        "John Miller",
        "Alexei Efros",
        "Moritz Hardt"
      ],
      "year": "2020",
      "venue": "International conference on machine learning"
    },
    {
      "citation_id": "48",
      "title": "Tent: Fully test-time adaptation by entropy minimization",
      "authors": [
        "Dequan Wang",
        "Evan Shelhamer",
        "Shaoteng Liu",
        "Bruno Olshausen",
        "Trevor Darrell"
      ],
      "year": "2020",
      "venue": "Tent: Fully test-time adaptation by entropy minimization",
      "arxiv": "arXiv:2006.10726"
    },
    {
      "citation_id": "49",
      "title": "Continual test-time domain adaptation",
      "authors": [
        "Qin Wang",
        "Olga Fink",
        "Luc Van Gool",
        "Dengxin Dai"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "50",
      "title": "Aggregated residual transformations for deep neural networks",
      "authors": [
        "Saining Xie",
        "Ross Girshick",
        "Piotr Dollár",
        "Zhuowen Tu",
        "Kaiming He"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "51",
      "title": "Forget me not: Reducing catastrophic forgetting for domain adaptation in reading comprehension",
      "authors": [
        "Ying Xu",
        "Xu Zhong",
        "Jimeno Yepes",
        "Jey Lau"
      ],
      "year": "2020",
      "venue": "2020 International joint conference on neural networks (IJCNN)"
    },
    {
      "citation_id": "52",
      "title": "Transfer: Learning relation-aware facial expression representations with transformers",
      "authors": [
        "Fanglei Xue",
        "Qiangchang Wang",
        "Guodong Guo"
      ],
      "year": "2021",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "53",
      "title": "Aff-wild: Valence and arousal 'inthe-wild'challenge",
      "authors": [
        "Stefanos Zafeiriou",
        "Dimitrios Kollias",
        "A Mihalis",
        "Athanasios Nicolaou",
        "Guoying Papaioannou",
        "Irene Zhao",
        "Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "54",
      "title": "Adaptive risk minimization: Learning to adapt to domain shift",
      "authors": [
        "Marvin Zhang",
        "Henrik Marklund",
        "Nikita Dhawan",
        "Abhishek Gupta",
        "Sergey Levine",
        "Chelsea Finn"
      ],
      "year": "2021",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "55",
      "title": "Memo: Test time robustness via adaptation and augmentation",
      "authors": [
        "Marvin Zhang",
        "Sergey Levine",
        "Chelsea Finn"
      ],
      "year": "2022",
      "venue": "Advances in Neural Information Processing Systems"
    }
  ]
}