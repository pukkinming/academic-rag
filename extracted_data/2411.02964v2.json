{
  "paper_id": "2411.02964v2",
  "title": "Speaker Emotion Recognition: Leveraging Self-Supervised Models For Feature Extraction Using Wav2Vec2 And Hubert",
  "published": "2024-11-05T10:06:40Z",
  "authors": [
    "Pourya Jafarzadeh",
    "Amir Mohammad Rostami",
    "Padideh Choobdar"
  ],
  "keywords": [
    "Speaker Emotion Recognition",
    "Self-supervised learning",
    "Transformer Models",
    "Wav2Vec2",
    "HuBERT"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Speech is the most natural way of expressing ourselves as humans. Identifying emotion from speech is a nontrivial task due to the ambiguous definition of emotion itself. Speaker Emotion Recognition (SER) is essential for understanding human emotional behavior. The SER task is challenging due to the variety of speakers, background noise, complexity of emotions, and speaking styles. It has many applications in education, healthcare, customer service, and Human-Computer Interaction (HCI). Previously, conventional machine learning methods such as SVM, HMM, and KNN have been used for the SER task. In recent years, deep learning methods have become popular, with convolutional neural networks and recurrent neural networks being used for SER tasks. The input of these methods is mostly spectrograms and hand-crafted features. In this work, we study the use of selfsupervised transformer-based models, Wav2Vec2 and HuBERT, to determine the emotion of speakers from their voice. The models automatically extract features from raw audio signals, which are then used for the classification task. The proposed solution is evaluated on reputable datasets, including RAVDESS, SHEMO, SAVEE, AESDD, and Emo-DB. The results show the effectiveness of the proposed method on different datasets. Moreover, the model has been used for real-world applications like call center conversations, and the results demonstrate that the model accurately predicts emotions.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech is one of the primary ways of expressing emotions. Therefore, a system that can recognize, interpret, and respond to the emotions expressed in speech is highly valuable. Emotions influence both the vocal characteristics and the linguistic content of speech. In recent years, enormous efforts have been devoted to developing methods for automatically identifying human emotions from speech signals, a field known as speaker emotion recognition (SER). SER systems have many applications including human-machine interactions  [11, 41] , medicine  [32] , and psychology  [60] . Moreover, evaluation of the conversations in a call centre is one of the major applications of SER, the manager of a call centre can analyse the performance of the operators. The operators are able to consider the feeling of the customers during conversation to handle necessary cases.\n\nIn all of the classification problems one of the main step is feature extraction. Before using deep learning most of the conventional machine learning methods used hand-crafted features speech recognition including Mel-frequency cepstral coefficients (MFCCs, pitch, zero-crossing, Fourier transform, and energy of signals  [51, 43, 1, 44, 40, 46] . After growing deep learning usage in speech processing tasks, the models extracted features automatically by deep-based models. They used convolutional neural networks (CNNs) to extract spatial features from input, the input was mostly spectrograms. Also, they used recurrent neural networks (RNNs) to evaluate temporal features. The models extracted local information and long-term contextual dependencies  [3, 33, 57, 2, 58, 30, 56, 10, 36, 53] . Additionally, the attention mechanism is another popular method that has been used in different fields such as speech recognition  [10]  , visual object classification  [36]  and document classification  [53] . It is a powerful concept that helps models focus on specific parts of the input sequence when generating each part of the output sequence. In the SER problem, models try to ignore silence frames and other parts of the utterance that do not carry emotional content  [9, 35, 21, 22] . Our contribution in this study lies in solving speaker emotion recognition by using transformer-based models, HuBERT  [19]  and Wav2vVec  [4] , which trained in a self-supervised method on the huge amount of data. These models are very powerful for feature extraction, so we used the models to extract features from raw audio data. We will delve deeper into the models in the next sections.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Works",
      "text": "Different machine learning algorithms have been used to construct a good classifier for emotion classification. As mentioned, conventional machine learning methods such as support vector machines (SVM)  [20, 8, 46, 45] , k-nearest neighbours (KNN)  [27] , and decision tree  [29]  have been used for emotion recognition. In  [27] , two utterances were aligned in the emotion space defined by emotograms using Multi-Dimensional Dynamic Time Warping (MD-DTW)  [18] . A KNN classifier was then applied to assign a final emotion class label based on the MD-DTW measure. KNN assigns a label to a test utterance based on the labels of its k nearest neighbors, with the final label determined by a majority vote among the neighbors. Each of the classifiers has its own advantages and disadvantages. These methods mainly use hand-crafted features such as pitch, energy, zero-crossing rate, Melfilterbank features, and MFCCs. These are often referred to as Low-Level Descriptors (LLD). Since MFCC models the human auditory perception system, it is the most popular spectral feature. Moreover, CNN-based models take the signal's spectrogram as input, which is a visual representation of the spectrum of frequencies of a signal as it varies with time  [2, 58] . In  [2] , spectrograms were generated from the input speech signal, followed by the use of three convolutional layers and three fully connected layers to extract features from the spectrogram images. A spectrogram is a visual representation of signal strength over time at various frequencies present in a waveform. It is depicted as a twodimensional graph, with time on the horizontal axis, frequency on the vertical axis, and the amplitude of the frequency components at a specific time represented by the intensity or color at each point. The spectrogram is computed from the speech signal by applying the Fast Fourier Transform (FFT), resulting in a time-frequency representation. Some methods apply CNNs to extract local features from the input, then use LSTM layers to learn long-term dependencies from the extracted local features  [57, 9] . In  [57] , two CNN-LSTM networks were constructed: one 1D CNN-LSTM network and one 2D CNN-LSTM network, designed to learn local and global emotion-related features from speech and log-mel spectrograms, respectively. Both networks share a similar architecture, consisting of four Local Feature Learning Blocks (LFLBs) and an LSTM layer. Each LFLB, which primarily includes one convolutional layer and one max-pooling layer, is designed to capture local correlations while extracting hierarchical features. The LSTM layer is then used to learn longterm dependencies from these local features. These networks, which combine the strengths of CNNs and LSTMs, capitalize on the advantages of both architectures while mitigating their individual limitations. RNNs, however, are inherently sequential models that do not allow parallelization of their computations, this bottleneck is particularly evident when processing large datasets with long sequences  [47] . The authors in  [3, 33]  have found that spectrograms are more effective than LLD features because LLD features are already decorrelated. One of the advantages of deep-based models is feature extraction; they can extract useful and important features dur-ing the training stages. In  [3] , the authors analyzed speech using spectrograms and proposed rectangular kernels of varying shapes and sizes, combined with max pooling in rectangular neighborhoods, to extract discriminative features. This approach effectively learns discriminative features from the speech spectrograms.\n\nThe mentioned features are affected by noise and environmental changes, which have a direct impact on the performance of the system. For example, most datasets are recorded in studios by actors, resulting in high-quality audio. If we train a model on this data, it may not perform well in real-world applications due to noise. It would be better to extract more robust features from the audio file. To tackle these problems, we suggest using transformer-based models to extract features from raw audio. These models are trained on a huge amount of data using self-supervised methods, allowing them to learn useful features from raw audio due to the self-attention mechanism in transformers",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Proposed Methodology",
      "text": "A key element of emotion recognition is the ability to extract more distinguishing speech features. In the proposed framework, features are extracted by transformers instead of traditional feature engineering techniques. In this study HuBERT and Wav2Vec2.0 have been used for feature extraction. Both models are transformer-based, and they extract both acoustic features and language modelling from raw audio signals in their architectures. In the following sections we go into more details about two models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Hubert",
      "text": "Hidden-Unit BERT (HuBERT) provides aligned target labels for BERT-like prediction losses using an offline clustering step (figure  2 ). It is proposed to overcome three distinct problems in the selfsupervised approach: the presence of multiple sound units per input utterance, the absence of lexicon during the pre-training phase, and the absence of explicit segmentation of sound units  [19] . In HuBERT, prediction loss is applied to masked regions, forcing a combined acoustic and language model to be learned for unmasked inputs. The model is pretrained on the Librispeech  [39]  (960h) and Librilight  [26]  (60Kh) benchmarks and it outperformed state-of-the-art methods, it presented in three differ-Figure  2 : Hubert model architecture ent models three model sizes pre-trained with Hu-BERT: BASE (90M parameters), LARGE (300M), and X-LARGE (1B). The authors in  [19]  made two decisions regarding mask prediction: how to mask and where to apply the prediction loss. For the first decision,p% of the timesteps are randomly selected as start indices and spans of l steps are masked using the same strategies as SpanBERT  [25]  and wav2vec 2.0  [4] . To address the second decision, cross-entropy loss computed over masked and unmasked time steps asLm andLu. Similarly to language modelling, if the loss is computed only over masked time steps, the model must predict the targets for the unseen frames based on the context. It forces the model to learn both the acoustic representation of unmasked segments and the long-range temporal structure of speech data.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Wav2Vec 2.0",
      "text": "The model is a transformer-based model trained with a self-supervised method to extract features from raw audio signals  [4]  (figure  3 ). It operates by first converting raw audio waveforms into meaningful feature representations using a CNN and a GELU activation function  [16]  to capture latent speech representations z 1 , z 2 , . . . , z T for T time steps. These initial features are then fed into a transformer network  [5] , which is trained using a contrastive loss objective to differentiate between correct and incorrect quantized representations of the audio signal  [17] . Using this training method, Wav2Vec 2.0 can learn rich, contextualised embeddings from unlabeled speech data, effectively building context representations over continuous speech and capturing Figure  3 : Wav2Vec2.0 model architecture dependencies over the entire sequence of latent representations. We utilised the pre-trained Wav2Vec 2.0 model to extract high-level features from raw audio inputs, which were then used as inputs for our subsequent processing stages. This approach not only reduces the need for hand-engineered features but also leverages the powerful representations learned by Wav2Vec 2.0, leading to improved performance in our specific application.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Model Architecture",
      "text": "The input to our model is a raw audio file, which is sent to the feature extractor module (HUBERT or Wav2Vec 2.0). After extracting the features, we have a matrix with the size of n×768. Depending on the length of the input audio, the length varies. The longer the audio file, the more features we have with the size of 768. In order to project the output of the feature extractor, we calculated the mean of the features across each row, so we will have a vector with a size of 768. Then, we apply two feed-forward layers for classification. The size of the final layer is equal to the number of classes that are compatible with the dataset's classes (figure  1 ). In the following section, we will report the performance of the proposed model on reputable datasets.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experimental Results",
      "text": "We performed SER on five reputable datasets that encompass different languages and emotions to assess the performance of transformer-based models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Datasets",
      "text": "There are 1440 speech audio samples in the RAVEDESS  [31]  dataset, recorded by 24 profes-sional actors. Two semantically neutral US English phrases are read while revealing eight emotions (neutral, calm, happiness, sadness, anger, fear, disgust, surprise). AESDD (Actuated Emotional Speech Dynamic Database)  [49, 50]  is a publicly available dataset for speech emotion recognition. There are utterances of acted emotional speech in the Greek language. There are approximately 500 utterances from five actors. The database contains utterances with five emotions: anger, disgust, fear, happiness, and sadness. EMODB  [7]  is a German database with high-quality audio recordings from 10 actors (5 male and 5 female) who produce 10 German utterances (5 short and 5 long sentences) with 7 emotions (anger, neutral, anger, boredom, happiness, sadness, disgust). Some emotional expressions have two versions recorded by the same author. Thus, the database provides about 535 sentences. The Surrey Audio-Visual Expressed Emotion (SAVEE)  [23]  database has been recorded as a prerequisite for the development of an automatic emotion recognition system. The database consists of recordings from 4 male actors of 7 different emotions (anger, disgust, fear, happiness, sadness, surprise, and neutral), and 480 British English utterances in total. The sentences were chosen from the standard TIMIT corpus and phonetically balanced for each emotion. The data were recorded in a visual media lab with high-quality audio-visual equipment, processed and labelled. The SHEMO  [38]  dataset includes 3000 semi-natural utterances, equivalent to 3 h and 25 min of speech data extracted from online radio plays. The dataset covers speech samples of 87 native-Persian speakers for five basic emotions including anger, fear, happiness, sadness, and surprise, as well as a neutral. It should be noted that we excluded the fear utterances from our test due to their small number in the database, as was done by the authors in  [38] .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Experiments And Results",
      "text": "Several experiments were carried out on the explained dataset by the proposed model. We split the datasets into 80% and 20% for train and test respectively, we kept the ratio of the classes in the test set. The confusion matrix is one of the best methods for showing the evaluation of the methods for the SER problem since we can analyze the missed classifications. For this study, we applied large models of HUBERT and Wave2Vec 2.0.\n\nOur work aims to recognize speech emotion   [13]  73.50 -MFCC + Pitch + Energy + ZCR + DWT  [28]  -85 CNN  [37]  80 79 GResNets  [55]  -68.48 CNN+BLSTM  [24]  69.4% 68.10   [48]  82.40 -eGeMAPs feature set  [15]  76.90 -OpenSmile features + ADAN  [54]  83.74 -RESNET MODEL + Deep BiLSTM  [42]  85.57 -Complementary Features + KELM  [14]  84.49 -ADRNN  [34]  85.39 -DCNN + DTPM  [56]  87.31 -DCNN + CFS + MLP  [13]  90.50 -CNN + LSTM  [57]  -95.89 MSFs  [52]  -85.     This section provides a detailed analysis of certain misclassifications observed in the tables. In table 3, the Hubert-based model misclassified boredom as neutral due to the similar tone of voice in the test data. Similarly, in table 4, anger was frequently misclassified as wonder, and calm as neutral, likely due to the tonal similarities between these classes. The Hubert-based model misclassified happy voices as wonder. After listening to the samples, we observed a similarity between the tones of happy and wonder emotions. In the samples that were sad but recognized as fearful, the tone of the speakers closely resembled that of the fearful emotion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ravdess",
      "text": "In table 5, the Wav2Vec2-based model struggled with the wonder class; upon examining the score of the incorrect class (fear), we found it to be close to 0.60, indicating that the model might benefit from additional training data. Lastly, in table 6, both models misclassified wonder from neutral. After analyzing the common misclassified samples, we concluded that the tone in the wonder class closely resembled neutral and that these samples tended to have shorter durations. Furthermore, the Hubertbased model misclassified some happy samples as neutral. After listening to these samples, we found a high degree of similarity between the tones of happy and neutral speech.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Conclusion",
      "text": "This paper proposes transformer-based feature extractors as a way of solving the SER problem. Us-ing transformer-based models, we can learn many useful temporal and spatial features from raw audio files that are very useful for emotion recognition. Our claim is supported by experimental results from benchmark datasets. The two models performed well on standard and famous datasets, and their performance was acceptable. This shows the significance and effectiveness of the proposed system for SER using self-supervised methods. Moreover, our model performed well on noisy data such as that from call centers, which is relevant to realworld applications. Although the self-supervised models presented in this paper have demonstrated improved performance in speaker emotion recognition, there are still opportunities for further enhancing the model. We can use CNN models in the following of the output of the feature extraction part. In this case, we calculated the mean of the features. By using CNN models we can capture a more meaningful relationship between the features rather than a mean measurement. Also, performance could be further improved by augmenting the training dataset with noise, SpecAugment, and room impulse responses (RIRs).",
      "page_start": 1,
      "page_end": 1
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The high-level description of our model. The input is a raw audio signal, in the second stage, features",
      "page": 2
    },
    {
      "caption": "Figure 2: Hubert model architecture",
      "page": 3
    },
    {
      "caption": "Figure 3: ). It operates by",
      "page": 3
    },
    {
      "caption": "Figure 3: Wav2Vec2.0 model architecture",
      "page": 4
    },
    {
      "caption": "Figure 1: ). In the follow-",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table 2: The confusion matrixes of SER on AESDD Table 4: The confusion matrixes of SER on",
      "data": [
        {
          "Dataset": "SAVEE",
          "Model": "Proposed Method (Hubert)",
          "Weighted Accuracy": "91.66 ± 1.2",
          "Unweighted Accuracy": "90.48 ± 1.1"
        },
        {
          "Dataset": "",
          "Model": "Proposed Method (Wav2Vec2)",
          "Weighted Accuracy": "83.34 ± 1.3",
          "Unweighted Accuracy": "80.95 ± 1.1"
        },
        {
          "Dataset": "",
          "Model": "Ensemble softmax regression [48]",
          "Weighted Accuracy": "51.50",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "eGeMAPs feature set [15]",
          "Weighted Accuracy": "42.40",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "DCNN + CFS + MLP [13]",
          "Weighted Accuracy": "66.90",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "CNN [33]",
          "Weighted Accuracy": "73.6",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "RAVDESS",
          "Model": "Proposed Method (Hubert)",
          "Weighted Accuracy": "92.82 ± 1.5",
          "Unweighted Accuracy": "93.78 ± 1.6"
        },
        {
          "Dataset": "",
          "Model": "Proposed Method (Wav2Vec2)",
          "Weighted Accuracy": "97.67 ± 1.2",
          "Unweighted Accuracy": "98.02 ± 1.3"
        },
        {
          "Dataset": "",
          "Model": "DCNN + CFS + MLP [13]",
          "Weighted Accuracy": "73.50",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "MFCC + Pitch + Energy + ZCR + DWT [28]",
          "Weighted Accuracy": "-",
          "Unweighted Accuracy": "85"
        },
        {
          "Dataset": "",
          "Model": "CNN [37]",
          "Weighted Accuracy": "80",
          "Unweighted Accuracy": "79"
        },
        {
          "Dataset": "",
          "Model": "GResNets [55]",
          "Weighted Accuracy": "-",
          "Unweighted Accuracy": "68.48"
        },
        {
          "Dataset": "",
          "Model": "CNN+BLSTM [24]",
          "Weighted Accuracy": "69.4%",
          "Unweighted Accuracy": "68.10"
        },
        {
          "Dataset": "",
          "Model": "Bagged SVM [6]",
          "Weighted Accuracy": "-",
          "Unweighted Accuracy": "75.69"
        },
        {
          "Dataset": "EMODB",
          "Model": "Proposed Method (Hubert)",
          "Weighted Accuracy": "97.83± 1.4",
          "Unweighted Accuracy": "98.21 ± 1.3"
        },
        {
          "Dataset": "",
          "Model": "Proposed Method (Wav2Vec2)",
          "Weighted Accuracy": "99 ± 0.2",
          "Unweighted Accuracy": "99 ± 0.3"
        },
        {
          "Dataset": "",
          "Model": "Ensemble softmax regression [48]",
          "Weighted Accuracy": "82.40",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "eGeMAPs feature set [15]",
          "Weighted Accuracy": "76.90",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "OpenSmile features + ADAN [54]",
          "Weighted Accuracy": "83.74",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "RESNET MODEL + Deep BiLSTM [42]",
          "Weighted Accuracy": "85.57",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "Complementary Features + KELM [14]",
          "Weighted Accuracy": "84.49",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "ADRNN [34]",
          "Weighted Accuracy": "85.39",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "DCNN + DTPM [56]",
          "Weighted Accuracy": "87.31",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "DCNN + CFS + MLP [13]",
          "Weighted Accuracy": "90.50",
          "Unweighted Accuracy": "-"
        },
        {
          "Dataset": "",
          "Model": "CNN + LSTM [57]",
          "Weighted Accuracy": "-",
          "Unweighted Accuracy": "95.89"
        },
        {
          "Dataset": "",
          "Model": "MSFs [52]",
          "Weighted Accuracy": "-",
          "Unweighted Accuracy": "85.5"
        },
        {
          "Dataset": "",
          "Model": "CNN [59]",
          "Weighted Accuracy": "-",
          "Unweighted Accuracy": "85.2"
        },
        {
          "Dataset": "",
          "Model": "Fuzzy C-Means [12]",
          "Weighted Accuracy": "-",
          "Unweighted Accuracy": "92.2"
        },
        {
          "Dataset": "AESDD",
          "Model": "Proposed Method (Hubert)",
          "Weighted Accuracy": "98.36 ± 1.2",
          "Unweighted Accuracy": "98.33± 1.4"
        },
        {
          "Dataset": "",
          "Model": "Proposed Method (Wav2Vec2)",
          "Weighted Accuracy": "98.36 ± 1.1",
          "Unweighted Accuracy": "98.33± 1.1"
        },
        {
          "Dataset": "SHEMO",
          "Model": "Proposed Method (Hubert)",
          "Weighted Accuracy": "83.77 ± 1.7",
          "Unweighted Accuracy": "71.38 ± 1.4"
        },
        {
          "Dataset": "",
          "Model": "Proposed Method (Wav2Vec2)",
          "Weighted Accuracy": "95.52± 2.1",
          "Unweighted Accuracy": "91.21± 1.3"
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Shape-based modeling of the fundamental frequency contour for emotion detection in speech",
      "authors": [
        "J Arias",
        "B Carlos",
        "B Nestor"
      ],
      "year": "2014",
      "venue": "Computer Speech and Language"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition from spectrograms with deep convolutional neural network",
      "authors": [
        "A Badshah"
      ],
      "year": "2017",
      "venue": "2017 International Conference on Platform Technology and Service"
    },
    {
      "citation_id": "4",
      "title": "Deep features-based speech emotion recognition for smart affective services",
      "authors": [
        "A Badshah"
      ],
      "year": "2019",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "6",
      "title": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "authors": [
        "A Baevski",
        "S Schneider",
        "M Au"
      ],
      "year": "2019",
      "venue": "vq-wav2vec: Self-supervised learning of discrete speech representations",
      "arxiv": "arXiv:1910.05453"
    },
    {
      "citation_id": "7",
      "title": "Bagged support vector machines for emotion recognition from speech",
      "authors": [
        "A Bhavan",
        "P Chauhan",
        "R Sha"
      ],
      "year": "2019",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "8",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt"
      ],
      "year": "2005",
      "venue": "In Interspeech"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition using support vector machine",
      "authors": [
        "Y Chavhan",
        "M Dhore",
        "P Yes"
      ],
      "year": "2010",
      "venue": "International Journal of Computer Applications"
    },
    {
      "citation_id": "10",
      "title": "3-d convolutional recurrent neural networks with attention model for speech emotion recognition",
      "authors": [
        "M Chen"
      ],
      "year": "2018",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "11",
      "title": "Attention-based models for speech recognition",
      "authors": [
        "J Chorowsk"
      ],
      "year": "2015",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition in human-computer interaction",
      "authors": [
        "R Cowie",
        "E Douglas-Cowie",
        "N Tsapatsoulis",
        "G Votsis",
        "S Kollias",
        "W Fellenz",
        "J Taylor"
      ],
      "year": "2001",
      "venue": "IEEE Signal Processing Magazine"
    },
    {
      "citation_id": "13",
      "title": "Application of fuzzy c-means clustering algorithm to spectral features for emotion classification from speech",
      "authors": [
        "S Demircan",
        "H Kahramanl"
      ],
      "year": "2018",
      "venue": "Neural Computing and Applications"
    },
    {
      "citation_id": "14",
      "title": "Impact of feature selection algorithm on speech emotion recognition using deep convolutional neural network",
      "authors": [
        "M Farooq"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "15",
      "title": "Exploration of complementary features for speech emotion recognition based on kernel extreme learning machine",
      "authors": [
        "L Guo"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "16",
      "title": "Emotion recognition in lowresource settings: An evaluation of automatic feature selection methods",
      "authors": [
        "F Haider"
      ],
      "year": "2021",
      "venue": "Computer Speech and Language"
    },
    {
      "citation_id": "17",
      "title": "Gaussian error linear units (gelus)",
      "authors": [
        "D Hendrycks",
        "K Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus)",
      "arxiv": "arXiv:1606.08415"
    },
    {
      "citation_id": "18",
      "title": "Product quantization for nearest neighbor search",
      "authors": [
        "J Herve",
        "M Douze",
        "C Schmid"
      ],
      "year": "2010",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "19",
      "title": "Multi-dimensional dynamic time warping for gesture recognition",
      "authors": [
        "G Holt",
        "M Reinders",
        "E Hendriks"
      ],
      "year": "2007",
      "venue": "Thirteenth Annual Conference of the Advanced School for Computing and Imaging"
    },
    {
      "citation_id": "20",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "21",
      "title": "Gmm supervector based svm with spectral features for speech emotion recognition",
      "authors": [
        "H Hu",
        "X Ming-Xing",
        "W Wu"
      ],
      "year": "2007",
      "venue": "2007 IEEE International Conference on Acoustics, Speech and Signal Processing -ICASSP'07"
    },
    {
      "citation_id": "22",
      "title": "Attention assisted discovery of sub-utterance structure in speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2016",
      "venue": "Attention assisted discovery of sub-utterance structure in speech emotion recognition"
    },
    {
      "citation_id": "23",
      "title": "Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition",
      "authors": [
        "C.-W Huang",
        "S Narayanan"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Multimedia and Expo (ICME)"
    },
    {
      "citation_id": "24",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2014",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "25",
      "title": "Learning temporal clusters using capsule routing for speech emotion recognition",
      "authors": [
        "M Jalal"
      ],
      "year": "2019",
      "venue": "Proceedings of Interspeech"
    },
    {
      "citation_id": "26",
      "title": "Spanbert: Improving pretraining by representing and predicting spans",
      "authors": [
        "M Joshi"
      ],
      "year": "2020",
      "venue": "Transactions of the Association for Computational Linguistics"
    },
    {
      "citation_id": "27",
      "title": "Libri-light: A benchmark for asr with limited or no supervision",
      "authors": [
        "J Kahn"
      ],
      "year": "2020",
      "venue": "ICASSP 2020 -IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "Emotion classification via utterance-level dynamics: A patternbased approach to characterizing affective expressions",
      "authors": [
        "Y Kim",
        "E Provost"
      ],
      "year": "2013",
      "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "29",
      "title": "Feature extraction algorithms to improve the speech emotion recognition rate",
      "authors": [
        "A Koduru",
        "H Valiveti",
        "A Budati"
      ],
      "year": "2020",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition using a hierarchical binary decision tree approach",
      "authors": [
        "C.-C Lee"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "31",
      "title": "Speech emotion recognition using convolutional and recurrent neural networks",
      "authors": [
        "W Lim",
        "J Daeyoung",
        "L Taejin"
      ],
      "year": "2016",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "32",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PloS One"
    },
    {
      "citation_id": "33",
      "title": "Emotion in medicine",
      "authors": [
        "B Maier",
        "W Shibles"
      ],
      "year": "2011",
      "venue": "The Philosophy and Practice of Medicine and Bioethics: A Naturalistic-Humanistic Approach"
    },
    {
      "citation_id": "34",
      "title": "Learning salient features for speech emotion recognition using convolutional neural networks",
      "authors": [
        "Q Mao"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "35",
      "title": "Speech emotion recognition from 3d log-mel spectrograms with deep learning network",
      "authors": [
        "H Meng"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "36",
      "title": "Automatic speech emotion recognition using recurrent neural networks with local attention",
      "authors": [
        "S Mirsamadi",
        "B Emad",
        "Z Cha"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "37",
      "title": "Recurrent models of visual attention",
      "authors": [
        "V Mnih",
        "N Heess",
        "A Graves"
      ],
      "year": "2014",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "38",
      "title": "A cnn-assisted enhanced audio signal processing for speech emotion recognition",
      "authors": [
        "S Mustaqeem",
        "Kwon"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "39",
      "title": "Shemo: A large-scale validated database for persian speech emotion detection",
      "authors": [
        "O Nezami",
        "P Lou",
        "M Karami"
      ],
      "year": "2019",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "40",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "V Panayotov"
      ],
      "year": "2015",
      "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "41",
      "title": "Mandarin emotional speech recognition based on svm and nn",
      "authors": [
        "T.-L Pao"
      ],
      "year": "2016",
      "venue": "18th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "42",
      "title": "Speech emotion recognition approaches in human computer interaction",
      "authors": [
        "S Ramakrishnan",
        "M Ibrahiem"
      ],
      "year": "2013",
      "venue": "Telecommunication Systems"
    },
    {
      "citation_id": "43",
      "title": "Clustering-based speech emotion recognition by incorporating learned features",
      "authors": [
        "M Sajjad",
        "S Kwon"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "44",
      "title": "Emotion recognition in the noise applying large acoustic feature sets",
      "authors": [
        "B Schuller",
        "A Dejan",
        "W Frank",
        "R Gerhard"
      ],
      "year": "2006",
      "venue": "Proceedings of the 2006 IEEE International Conference"
    },
    {
      "citation_id": "45",
      "title": "Hidden markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "R Gerhard",
        "L Manfred"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing"
    },
    {
      "citation_id": "46",
      "title": "Emotion speech recognition using mfcc and svm",
      "authors": [
        "S Shambhavi",
        "V Nitnaware"
      ],
      "year": "2015",
      "venue": "International Journal of Engineering Research and Technology"
    },
    {
      "citation_id": "47",
      "title": "Automatic speech emotion recognition using support vector machine",
      "authors": [
        "P Shen",
        "Z Changjun",
        "X Chen"
      ],
      "year": "2011",
      "venue": "Proceedings of the 2011 International Conference on Electronic and Mechanical Engineering and Information Technology"
    },
    {
      "citation_id": "48",
      "title": "Attention is all you need in speech separation",
      "authors": [
        "C Subakan"
      ],
      "venue": "ICASSP 2021 -IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "49",
      "title": "Ensemble softmax regression model for speech emotion recognition",
      "authors": [
        "Y Sun",
        "G Wen"
      ],
      "year": "2017",
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "50",
      "title": "Speech emotion recognition for performance interaction",
      "authors": [
        "N Vryzas"
      ],
      "year": "2018",
      "venue": "Journal of the Audio Engineering Society"
    },
    {
      "citation_id": "51",
      "title": "Subjective evaluation of a speech emotion recognition interaction framework",
      "authors": [
        "N Vryzas"
      ],
      "year": "2018",
      "venue": "Proceedings of the Audio Mostly 2018 on Sound in Immersion and Emotion"
    },
    {
      "citation_id": "52",
      "title": "Speech emotion recognition using fourier parameters ieee transactions on affective computing",
      "authors": [
        "K Wang",
        "A Ning",
        "N Bing",
        "Z Yanyong"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "53",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "H Tiago",
        "W.-Y Chan"
      ],
      "year": "2011",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "54",
      "title": "Hierarchical attention networks for document classification",
      "authors": [
        "Z Yang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 2016 Conference of the North American Chapter"
    },
    {
      "citation_id": "55",
      "title": "Adversarial data augmentation network for speech emotion recognition",
      "authors": [
        "L Yi",
        "M Man-Wai"
      ],
      "year": "2019",
      "venue": "Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "56",
      "title": "Spectrogram based multi-task audio classification. Multimedia Tools and Applications",
      "authors": [
        "Y Zeng"
      ],
      "year": "2019",
      "venue": "Spectrogram based multi-task audio classification. Multimedia Tools and Applications"
    },
    {
      "citation_id": "57",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "58",
      "title": "Speech emotion recognition using deep 1d and 2d cnn lstm networks",
      "authors": [
        "J Zhao",
        "M Xia",
        "C Lijiang"
      ],
      "year": "2019",
      "venue": "Biomedical Signal Processing and Control"
    },
    {
      "citation_id": "59",
      "title": "An experimental study of speech emotion recognition based on deep convolutional neural networks",
      "authors": [
        "W Zheng",
        "J Yu",
        "Y Zou"
      ],
      "year": "2015",
      "venue": "2015 International Conference on Affective Computing and Intelligent Interaction (ACII)"
    },
    {
      "citation_id": "60",
      "title": "Speech emotion recognition using cnn",
      "authors": [
        "H Zhengwei",
        "D Ming",
        "M Qirong",
        "Z Yongzhao"
      ],
      "year": "2014",
      "venue": "Association for Computing Machinery"
    },
    {
      "citation_id": "61",
      "title": "Emotion recognition in schizophrenic and depressed inpatients",
      "authors": [
        "D Zuroff",
        "A Sally"
      ],
      "year": "1986",
      "venue": "Journal of Clinical Psychology"
    }
  ]
}