{
  "paper_id": "2502.10435v2",
  "title": "Ramer: Reconstruction-Based Adversarial Model For Multi-Party Multi-Modal Multi-Label Emotion Recognition",
  "published": "2025-02-09T07:46:35Z",
  "authors": [
    "Xudong Yang",
    "Yizhang Zhu",
    "Hanfeng Liu",
    "Zeyi Wen",
    "Nan Tang",
    "Yuyu Luo"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Conventional Multi-modal multi-label emotion recognition (MMER) assumes complete access to visual, textual, and acoustic modalities. However, real-world multi-party settings often violate this assumption, as non-speakers frequently lack acoustic and textual inputs, leading to a significant degradation in model performance. Existing approaches also tend to unify heterogeneous modalities into a single representation, overlooking each modality's unique characteristics. To address these challenges, we propose RAMer (Reconstruction-based Adversarial Model for Emotion Recognition), which refines multi-modal representations by not only exploring modality commonality and specificity but crucially by leveraging reconstructed features, enhanced by contrastive learning, to overcome data incompleteness and enrich feature quality. RAMer also introduces a personality auxiliary task to complement missing modalities using modality-level attention, improving emotion reasoning. To further strengthen the model's ability to capture label and modality interdependency, we propose a stack shuffle strategy to enrich correlations between labels and modality-specific features. Experiments on three benchmarks, i.e., MEmoR, CMU-MOSEI, and M 3 ED, demonstrate that RAMer achieves state-of-the-art performance in dyadic and multi-party MMER scenarios.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Emotion recognition from videos is crucial for advancing human-computer interaction and social intelligence. Multimodal multi-label emotion recognition (MMER) leverages visual, textual, and acoustic signals to identify multiple emotions (e.g., happy, sad) simultaneously  [Zhang et al., 2020; Zhang et al., 2021a] . Conventional MMER methods, as shown in Figure  1 (a), typically focus on monologue or dyadic settings, assuming all modalities are fully available. However, real-world conversations often involve multiple participants (i.e., multi-party) with incomplete modality data for non-speakers who always lack acoustic and textual signals. Multi-party MMER, a more complex and practical setting, introduces three key challenges. Firstly, handling incomplete modalities is a significant challenge, which requires robust methods to reconstruct or infer missing information. Most existing works  [Zhang et al., 2021a; Zhang et al., 2022; Ge et al., 2023]  assume complete modality access and encode each modality independently, overlooking missing data. While some methods  [Ghosal et al., 2019; Hu et al., 2021]  leverage speaker-aware context modeling, their performance degrades in multi-party settings where nonspeakers often lack critical modalities. Secondly, representing diverse modalities effectively remains challenging, often requiring techniques that can not only integrate disparate information but also reconstruct rich, complete representations from potential modalities. Current fusion strategies, such as aggregation-based methods (e.g., concatenation, averaging)  [Shen et al., 2020]  and hybrid approaches  [Manzoor et al., 2023] , project modalities into a shared subspace, often neglecting their unique characteristics and reducing dis-criminative ability. Recent methods  [Zhang et al., 2022]  attempt to separate modality-specific and shared features but often suffer from information loss due to inadequate handling of inter-modal correlations. Similarly, methods preserving modality-specific information  [Peng et al., 2023]  may overlook cross-modal commonalities, limiting their ability to fully capture inter-modal relationships. Finally, multi-label learning presents challenges in modeling robust label correlations and capturing complex interdependency between modalities and labels. Existing approaches  [Cisse et al., 2013; Ma et al., 2021]  often fail to fully exploit collaborative label relationships. Moreover, emotions vary across modalities, and different emotions rely on distinct modality features, further complicating the task.\n\nTo address these issues, we propose RAMer, a novel framework designed to tackle the challenges of the Multiparty MMER problem. RAMer integrates multimodal representation learning with multi-label modeling to effectively handle incomplete modalities in multi-party settings.\n\nAs illustrated in Figure  1 (b), RAMer addresses the challenge of multi-party MMER by following techniques. To address the challenge of incomplete modalities, we propose an auxiliary task that incorporates external knowledge, such as personality traits, to complement the existing modalities. Leveraging this, we employ modality-level attention mechanisms to capture both inter-and intra-personal features. A reconstruction-based network is utilized to recover and enrich the features of any modality by leveraging information from the other modalities.\n\nTo represent diverse modalities effectively and capture discriminative features, we design an adversarial network that extracts commonality across modalities while amplifying the specificity inherent to each one. This helps ensure minimal information loss during the fusion process.\n\nAdditionally, to model robust interconnections between modalities and labels, we propose a novel modality shuffle strategy. This strategy enriches the feature space by shuffling both samples and modalities, based on the commonality and specificity of the modalities, improving the model's ability to capture label correlations and modality-to-label relationships.\n\nIn summary, the contributions of this work are:\n\n• A Novel Model for the Multi-party MMER Problem. We present RAMer, a new framework that centrally integrates feature reconstruction within an adversarial learning paradigm. RAMer adeptly captures both commonality and specificity across modalities, crucially utilizing robustly reconstructed features to significantly improve emotion recognition, especially even with incomplete modality data.\n\n• Optimization Techniques. To enhance the robustness of multi-party emotion recognition, RAMer employs contrastive learning to enrich reconstructed features and integrates a personality auxiliary task to capture modalitylevel attention. We also propose a stack shuffle strategy, enhancing the modeling of label correlations and modality-to-label relationships by leveraging the commonality and specificity of different modalities.\n\n• Extensive Experiments. We conduct comprehensive ex-periments on three benchmarks, i.e., MEmoR, CMU-MOSEI, and M 3 ED, across various conversation scenarios. Results show that RAMer surpasses existing approaches and achieves state-of-the-art performance in both dyadic and multi-party MMER problems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Multi-modal Representation Learning. Emotion recognition has progressed from uni-modal approaches  [Huang et al., 2021; Saha et al., 2020]  to multi-modal methods  [Lv et al., 2021; Zhang et al., 2022]  that exploit complementary features across modalities. While uni-modal approaches often face recognition biases  [Huang et al., 2021] , multi-modal learning has gained significant attention, with a key challenge being the effective integration of heterogeneous modalities. Early fusion methods, such as concatenation  [Ngiam et al., 2011a] , tensor fusion  [Liu et al., 2018a] , and averaging  [Hazirbas et al., 2017] , struggle with modality gaps that hinder effective feature alignment. To address this, attentionbased methods  [Ge et al., 2023; Tsai et al., 2019]     [Tsai and Lee, 2020]  to explore label correlations. Some noteworthy strategies  [Zhang et al., 2021b; Zhang et al., 2022]  focus on modeling labelfeature correlations through label-specific representations enabled by visual attention  [Chen et al., 2019]  and transformers  [Zhang et al., 2022] . Beyond monologue settings, MMER in conversations has gained interest, using GCN  [Ghosal et al., 2019]  and memory networks  [Hazarika et al., 2018]     et al., 2023] applied AT to reduce modal and data biases in MMER tasks. However, Zhang et al.  [Zhang et al., 2022]  implemented AT to extract multi-modal commonality and diversity, but suffered a significant loss of modality information due to inadequate cross-modal information fusion.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Problem Formulation",
      "text": "In this section, we introduce the notations used and formally define the Multi-party Multi-modal Multi-label Emotion Recognition (Multi-party MMER) problem.\n\nNotations. We use lowercase letters for scalars (e.g., v), uppercase letters for vectors (e.g., Y ), and boldface for matrices (e.g., X). A data sample is represented by the tuple (V, P t , S r , E t,r ), where:\n\nrefers to the set of target persons, and\n\nrepresents the target segments, each annotated with an emotion moment.\n\n• E t,r denotes the labeled emotion for person P t in S r .\n\nEach sample involves modalities such as visual (v), acoustic (a), textual (t), and personality traits (p).\n\nFor each modality m ∈ {v, a, t, p}, the corresponding features are represented as\n\nwith N data samples, where:\n\n(1) X m τ ∈ X m represents the features for each modality m in sample τ , and (2)\n\nζ is a multi-hot vector indicating the presence (1) or absence (0) of emotion labels, where Y υ τ = 1 indicates that sample τ belongs to class υ, and Y υ τ = 0 otherwise. The goal of the Multi-party MMER problem is to learn a function F :\n\nY that predicts the target emotion E t,r for person P t in segment S r , leveraging contextual information from multiple modalities. Discussion. It is important to note that the target person P t may have incomplete modality information, meaning they may not simultaneously possess visual, textual, or acoustic representations. This introduces uncertainty in the modality of the target segment S r , making the prediction task more challenging.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methodology",
      "text": "Figure  2  shows the framework of RAMer, which consists of three components: auxiliary uni-modal embedding, reconstruction-based adversarial Learning, and stack shuffle feature augmentation.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Auxiliary Uni-Modal Embedding",
      "text": "To extract contextual information from each modality, we employ four independent transformer encoders  [Vaswani et al., 2017] , each dedicated to a specific modality m.Each encoder consists of n m identical layers to ensure consistent and deep representation. For multi-party conversation videos with T participants with incomplete modalities, we introduce an optional auxiliary task leveraging personality to complement missing modalities. Specifically, we concatenate personality embedding X p with each modality X m ∈ {v, t, a} to enrich the feature space. We then apply the scaled dot-product attention to compute inter-person attention across the person dimension within each segment, and intra-person attention along the segment dimension for each individual  [Shen et al., 2020] . This modality-level attention mechanism is designed to enhance the model's emotion reasoning ability by effectively capturing both interpersonal dynamics and temporal patterns within the data. In this way, we obtain personality enhanced representation X m α ∈ R l×d .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Reconstruction-Based Adversarial Learning",
      "text": "The second component leverages multiple modalities by capturing inter-modal commonalities while preserving modalityspecific features. To address the limitations of adversarial networks  [Goodfellow et al., 2014; Zhang et al., 2022] , which can result in information loss and difficulty in learning modality-label dependencies, we introduce a reconstruction-based approach. It employs contrastive learning to learn modality-independent but label-relevant representations while reconstructing missing modalities during training to enhance robustness.\n\nAdversarial Training. To balance modality specificity and commonality, we adopt adversarial training to extract discriminative features. The uni-modal embeddings X m α are fed to three fully connected networks f m to extract specificity S m , m ∈ {v, a, t}. In parallel, X m α are also passed through a reconstruction network, which is coupled with a contrastive learning network, followed by a generator G (•; θ G ) to derive the commonality C m . Both specificity and commonality are then passed through linear layers with softmax activation in the discriminator D (•; θ D ) that is designed to distinguish which modality the inputs come from. The generator captures commonality C m by projecting different reconstructed embedding X m γ into a shared latent subspace, ensuring distributional alignment across modalities. Consequently, this architecture encourages the generator G(•; θ G ) to produce outputs that challenge the discriminator D (•; θ D ) by obscuring the source modality of C m . The generator and discriminator are jointly trained in a game-theoretic setup to enhance feature robustness against modality-specific biases. Both the commonality adversarial loss L C and the specificity adversarial loss L S are calculated by cross-entropy loss as,\n\nwhere U ∈ {U v , U t , U a } represents the ground truth label corresponding to the discriminator's input.\n\nIn the shared subspace, it is advantageous to employ a unified representation of various modalities to facilitate multilabel classification. This representation is designed to eliminate redundant information and extract the elements common to the different modalities, thereby introducing a common semantic loss defined as,\n\nwhere ŷυ,m τ is predicted with C m and y υ τ is the ground-truth label. To encode diverse aspects of multi-modal data, we introduce an orthogonal loss L orth that encourages the commonality C m and specificity S m subspaces to remain distinct by minimizing their overlap.\n\nwhere ∥•∥ F is Frobenius norm. Hereby, the objective of adversarial training L adv is\n\nwhere λ a , λ o and λ c are trade-off parameters.\n\nMulti-modal Feature Reconstruction. To reconstruct the features of any modality by leveraging information from the other modalities. We employ a reconstruction network that is composed of modality-specific encoders ε m , decoders d m , and a two-level reconstruction process utilizing multi-layer linear networks g(•). Given input X m α from different modality, three encoders ε m that consist of MLPs are utilized to project X m α into latent embedding Z m α within the latent space S z . Subsequently, three corresponding decoders d m transform these latent vectors into the decoded vectors X m α . At the first level of reconstruction network, the intrinsic vector D m that derived from contrastive learning network and semantic features X {v,t,a}\\m α are concatenated to form the input, which is processed to produce X m β used for the second-level reconstruction network. Hereby, the reconstruction network can be formulated as,\n\nThe obtained three embedding X m α , X m β , and X m γ from three distinct feature spaces are fed into fully connected network followed by max pooling. We can formulate the reconstruction loss L rec and classification loss L lsr cls as,\n\nwhere ∥•∥ F is the Frobenius norm, λ α,β,γ are trade-off parameters, L B is the binary cross entropy (BCE) loss.\n\nTo capture the feature distributions of different modalities and use them to guide the restoration of incomplete modalities, intrinsic vectors D m obtained through a supervised contrastive learning network  [Khosla et al., 2020]  are incorporated into the reconstruction network. The encoders ε m project input X m σ to contrastive embeddings Z m σ , σ ∈ {α, β, γ}. Given a contrastive embedding set Z = Z {v,t,a} σ , an anchor vector z i ∈ Z and assuming the prototype vector updated during the training process based on the moving average is µ m j,k , where modality m ∈ {v, t, a}, label category j ∈ [ζ], label polarity k ∈ {pos, neg}, then the intrinsic vector D m can be derived from:\n\n(10) The loss of contrastive learning network is defined as,\n\nwhere P (i) is the positive set, η ∈ R + is a temperature parameter, and A(i) = Z \\ {i}.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Stack Shuffle For Feature Augmentation",
      "text": "To construct more robust correlations among labels and model the complex interconnections between modalities and labels, we propose a multi-modal feature augmentation strategy that incorporates a stack shuffle mechanism. After obtaining the commonality and specificity representations, we perform sample-wise and modality-wise shuffling processes sequentially on a batch of samples. To strengthen the correlations between labels, we first apply a sample-wise shuffle.\n\nThe features derived from C m and S m are split into k stacks along the sample dimension, with the top elements of each stack cyclically popped and appended to form new vectors.\n\nNext, a modality-wise shuffle is introduced to help the model capture and integrate information across different modalities.\n\nFor each sample, features are divided into stacks along the modality dimension, and iterative pop-and-append operations are applied. Finally, the shuffled samples V are used to finetune the classifier c ζ with the binary cross-entropy (BCE) loss.\n\nCombing the Eq.(  5 ), Eq.(  7 ) ∼ Eq.(  11 ) and Eq.(  12 ), the final objective function L is formulated as,\n\nwhere λ r , λ s are trade-off parameters.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Setup",
      "text": "Datasets and Metrics. We evaluated RAMer on three benchmark datasets: MEmoR  [Shen et al., 2020] , a multiparty conversation dataset that includes personality, and CMU-MOSEI  [Zadeh et al., 2018]  and M 3 ED  [Zhao et al., 2022] , which are dyadic conversation datasets that do not include personality information. The evaluation is conducted under the protocols of these datasets. For CMU-MOSEI and M 3 ED, we employed four commonly used evaluation metrics: Accuracy (Acc), Micro-F1, Precision (P), and Recall (R). For MEmoR, we followed the benchmark's protocol and used Micro-F1, Macro-F1, and Weighted-F1 metrics.\n\nBaselines. For the MEmoR dataset, we compare RAMer with multi-party conversation baselines, including MDL, MDAE  [Ngiam et al., 2011b] , BiLSTM+TFN  [Zadeh et al., 2017] , BiLSTM+LMF  [Liu et al., 2018b] , Dia-logueGCN  [Ghosal et al., 2019] , DialogueCRN  [Hu et al., 2021] , and AMER  [Shen et al., 2020] . We further assess its robustness against recent dyadic models, including  CARAT [Peng et al., 2023]  and TAILOR  [Zhang et al., 2022] . For the CMU-MOSEI and M 3 ED datasets, we test three categories of methods. 1) Classic methods. CC  [Read et al., 2011] , which concatenates all available modalities as input for binary classifiers. 2) Deep-based methods. ML-GCN [Chen et al., 2019], using Graph Convolutional Networks to map label representations and capture label correlations. 3) Multi-modal multi-label methods. These include MulT  [Tsai et al., 2019]  for cross-modal interactions, MISA  [Hazarika et al., 2020]  for learning modalityinvariant and modality-specific features, and methods like MMS2S  [Zhang et al., 2020] , HHMPN  [Zhang et al., 2021a] , TAILOR  [Zhang et al., 2022] , AMP  [Ge et al., 2023] , and  CARAT [Peng et al., 2023] .",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Comparison With The State-Of-The-Art",
      "text": "We present the performance comparisons of RAMer on the MEmoR, CMU-MOSEI, and M3ED datasets in Table  1 , Table 2, and Table  3 , respectively, with following observations. 1) On the MEmoR dataset, RAMer outperforms all baselines by a significant margin. While TAILOR achieves a high weighted-F1 score in the fine-grained setting, its overall performance is weaker due to biases toward frequent and easierto-recognize classes. RAMer consistently delivers strong results across all settings, demonstrating its ability to learn more effective representations. 2) On the CMU-MOSEI and M3ED datasets, RAMer surpasses state-of-the-art methods on all metrics except recall, which is less critical compared to accuracy and Micro-F1 in these contexts. 3) Deep-based methods outperform classical ones, highlighting the importance of capturing label correlations for improved classification performance. 4) Multimodal methods like HHMPN and AMP significantly outperform the unimodal ML-GCN, emphasizing the necessity of multimodal interactions. 5) Models optimized for dyadic conversations, such as CARAT, experience a notable performance drop in multi-party settings with incomplete modalities. In contrast, RAMer excels in both scenarios, achieving substantial improvements in Micro-F1 and Macro-F1 scores on the MEmoR dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "To better understand the importance of each component of RAMer, we compared various ablated variants.\n\nAs shown in Table  4 , we make the following observations:      F1 than variant (11) . This indicates that jointly learning specificity and commonality yields superior performance, underscoring the importance of capturing both modality-specific specificity and shared commonality.\n\n• Contrastive learning benefits the MMER. The inclusion of loss functions L scl in adversarial training leads to progressive performance improvements, as evidenced by the superior results of (4). • Feature reconstruction net benefits MMER. Variants (  5 ), (  6 ), (  7 ) are worse than (11), and (8) shows an 0.045 decrease in Micro-F1, which indicates that feature reconstruction can improve model performance. When the entire reconstruction process is omitted, the performance of (8) declines even more compared to (  6 ) and (  7 ), confirming the effectiveness of multi-level feature reconstruction in achieving multi-modal fusion. • Changing the fusion order leads to poor performance, variants (  9 ) and (  10 ) perform worse than (11). It validates the rationality and optimality of feature fusion.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Qualitative Analysis",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Visualization Of Modality-To-Label Correlations",
      "text": "To explore the relationship between modalities and labels, we visualized the correlation of labels with their most relevant modalities. As shown in Figure  4 , regardless of the presence of adversarial training, different emotion label is influenced by different modalities. For instance, surprise is predominantly correlated with the acoustic modality, while anger is primarily associated with the visual modality. This indicates that each modality captures the distinguishable semantic information of the labels from distinct perspectives.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Case Study",
      "text": "To demonstrate RAMer's robustness in complex scenarios, Figure  5  shows an example of MMER on the MEmoR dataset where specific target persons have incomplete modality signals. The top three rows display different modalities from a video clip, segmented semantically with aligned multimodal signals. Key observations include: 1) The target moment requires recognizing emotions for both the speaker (e.g., Howard) and non-speakers (e.g., Penny and Leonard).\n\nWhile the speaker typically has complete multi-modal signals, non-speakers often lack certain modalities. TAILOR, limited by missing modalities, yields partial predictions as its self-attention mechanism struggles to align labels with missing features. 2) Limitations of single or incomplete modali- ties. A single modality, such as text, is often insufficient for accurate emotion inference (e.g., only Howard's Joy is detectable from text alone). Although CARAT attempts to reconstruct missing information, it fails to capture cross-modal commonality, leading to incorrect predictions. 3)Inter-person interactions and external knowledge (e.g., personality traits) play important roles. Inter-person attention helps compensate for missing data, while personality-aware reasoning improves emotion inference across participants, highlighting the synergy between user profiling and emotion recognition. Experimental results demonstrate that RAMer achieves superior robustness and effectiveness in complex real-world scenarios.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we proposed RAMer, a framework that refines multi-modal representations using reconstruction-based adversarial learning to address the Multi-party Multi-modal Multi-label Emotion Recognition problem. RAMer captures both the commonality and specificity across modalities using an adversarial learning module, with reconstruction and contrastive learning enhancing its ability to differentiate emotion labels, even with missing data. We also introduce a personality auxiliary task to complement incomplete modalities, improving emotion reasoning through modality-level attention. Furthermore, the stack shuffle strategy enriches the feature space and strengthens correlations between labels and modalities. Extensive experiments on three datasets demonstrate that RAMer consistently outperforms state-of-the-art methods in both dyadic and multi-party MMER scenarios.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: (a), typically focus on monologue or dyadic",
      "page": 1
    },
    {
      "caption": "Figure 1: (a) shows the conventional approach for MMER in mono-",
      "page": 1
    },
    {
      "caption": "Figure 1: (b), RAMer addresses the chal-",
      "page": 2
    },
    {
      "caption": "Figure 2: The framework of RAMer. Given incomplete multi-modal inputs, RAMer first encodes each individual modality through an",
      "page": 3
    },
    {
      "caption": "Figure 2: shows the framework of RAMer, which con-",
      "page": 3
    },
    {
      "caption": "Figure 3: t-SNE visualizations of modality embeddings. (a)(b): Specificity and commonality features without/with adversarial training.",
      "page": 6
    },
    {
      "caption": "Figure 3: (a), without adversarial training,",
      "page": 6
    },
    {
      "caption": "Figure 3: (b) shows clearer separation of",
      "page": 6
    },
    {
      "caption": "Figure 3: (c) demonstrates that without the reconstruction net (RN)",
      "page": 6
    },
    {
      "caption": "Figure 4: The correlation of modality-to-label dependencies.",
      "page": 7
    },
    {
      "caption": "Figure 4: , regardless of the presence",
      "page": 7
    },
    {
      "caption": "Figure 5: shows an example of MMER on the MEmoR dataset",
      "page": 7
    },
    {
      "caption": "Figure 5: An example of the case study results.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Performance comparison on the MEmoR dataset under primary and fine-grained settings. With various modality combinations",
      "data": [
        {
          "Methods\nModality": "",
          "Primary": "Micro-F1\nMacro-F1\nWeighted-F1",
          "Fine-grained": "Micro-F1\nMacro-F1\nWeighted-F1"
        },
        {
          "Methods\nModality": "v, a, t, p\nMDL with Personality\nv, a, t, p\nMDAE\nv, a, t, p\nBiLSTM+TFN\nv, a, t, p\nBiLSTM+LMF\nv, a, t, p\nDialogueGCN\nv, a, t\nAMER w/o Personality\nv, a, t, p\nAMER\nv, a, t, p\nDialogueCRN\nv, a, t, p\nTAILOR\nv, a, t, p\nCARAT",
          "Primary": "0.429\n0.317\n0.423\n0.421\n0.303\n0.410\n0.470\n0.310\n0.454\n0.449\n0.294\n0.432\n0.441\n0.310\n0.425\n0.446\n0.339\n0.440\n0.477\n0.353\n0.465\n0.441\n0.310\n0.425\n0.341\n0.287\n0.326\n0.399\n0.224\n0.422",
          "Fine-grained": "0.363\n0.217\n0.345\n0.363\n0.219\n0.341\n0.366\n0.207\n0.350\n0.364\n0.198\n0.351\n0.373\n0.229\n0.373\n0.401\n0.246\n0.379\n0.419\n0.262\n0.400\n0.373\n0.229\n0.373\n0.490\n0.303\n0.069\n0.346\n0.090\n0.483"
        },
        {
          "Methods\nModality": "v, a, t, p\nRAMer",
          "Primary": "0.499\n0.402\n0.503",
          "Fine-grained": "0.431\n0.299\n0.404"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: Performance comparison on the MEmoR dataset under primary and fine-grained settings. With various modality combinations",
      "data": [
        {
          "Methods": "",
          "Aligned": "Acc\nP\nR\nMicro-F1",
          "Unaligned": "Acc\nP\nR\nMicro-F1"
        },
        {
          "Methods": "CC\nML-GCN\nMulT\nMISA\nMMS2S\nHHMPN\nTAILOR\nAMP\nCARAT",
          "Aligned": "0.225\n0.306\n0.523\n0.386\n0.411\n0.546\n0.476\n0.509\n0.445\n0.619\n0.465\n0.531\n0.582\n0.43\n0.453\n0.509\n0.475\n0.629\n0.504\n0.56\n0.459\n0.602\n0.496\n0.556\n0.488\n0.641\n0.512\n0.569\n0.484\n0.643\n0.511\n0.569\n0.494\n0.661\n0.518\n0.581",
          "Unaligned": "0.235\n0.320\n0.550\n0.404\n0.437\n0.573\n0.482\n0.524\n0.423\n0.636\n0.445\n0.523\n0.571\n0.398\n0.371\n0.45\n0.447\n0.619\n0.462\n0.529\n0.434\n0.591\n0.476\n0.528\n0.46\n0.639\n0.452\n0.529\n0.462\n0.642\n0.459\n0.535\n0.466\n0.652\n0.466\n0.544"
        },
        {
          "Methods": "RAMer",
          "Aligned": "0.505\n0.668\n0.604\n0.551",
          "Unaligned": "0.469\n0.660\n0.560\n0.486"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Learning multi-label scene classification",
      "authors": [
        "Boutell"
      ],
      "year": "2004",
      "venue": "Pattern recognition"
    },
    {
      "citation_id": "2",
      "title": "Multi-label image recognition with graph convolutional networks",
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "3",
      "title": "A simple framework for contrastive learning of visual representations",
      "year": "2020",
      "venue": "In International conference on machine learning"
    },
    {
      "citation_id": "4",
      "title": "Robust bloom filters for large multilabel classification tasks",
      "authors": [
        "Cisse"
      ],
      "year": "2013",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "5",
      "title": "Learning robust multi-modal representation for multi-label emotion recognition via adversarial masking and perturbation",
      "authors": [
        "Ge"
      ],
      "year": "2023",
      "venue": "Proceedings of the ACM Web Conference 2023"
    },
    {
      "citation_id": "6",
      "title": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "authors": [
        "Ghosal"
      ],
      "year": "2019",
      "venue": "Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation",
      "arxiv": "arXiv:1908.11540"
    },
    {
      "citation_id": "7",
      "title": "Generative adversarial nets",
      "authors": [
        "Goodfellow"
      ],
      "year": "2014",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "8",
      "title": "Misa: Modality-invariant and-specific representations for multimodal sentiment analysis",
      "authors": [
        "Hazarika"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. North American Chapter"
    },
    {
      "citation_id": "9",
      "title": "Fusenet: Incorporating depth into semantic segmentation via fusion-based cnn architecture",
      "authors": [
        "Hazirbas"
      ],
      "year": "2016",
      "venue": "Computer Vision-ACCV 2016: 13th Asian Conference on Computer Vision"
    },
    {
      "citation_id": "10",
      "title": "Contextual Reasoning Networks for Emotion Recognition in Conversations. arXiv e-prints",
      "authors": [
        "Hu"
      ],
      "year": "2021",
      "venue": "Contextual Reasoning Networks for Emotion Recognition in Conversations. arXiv e-prints",
      "arxiv": "arXiv:2106.01978"
    },
    {
      "citation_id": "11",
      "title": "Audiooriented multimodal machine comprehension via dynamic inter-and intra-modality attention",
      "authors": [
        "Huang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "12",
      "title": "Supervised Contrastive Learning",
      "authors": [
        "Khosla"
      ],
      "year": "2020",
      "venue": "Supervised Contrastive Learning",
      "arxiv": "arXiv:2004.11362"
    },
    {
      "citation_id": "13",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Liu"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "14",
      "title": "Efficient low-rank multimodal fusion with modality-specific factors",
      "authors": [
        "Liu"
      ],
      "year": "2018",
      "venue": "Efficient low-rank multimodal fusion with modality-specific factors",
      "arxiv": "arXiv:1806.00064"
    },
    {
      "citation_id": "15",
      "title": "Progressive modality reinforcement for human multimodal emotion recognition from unaligned multimodal sequences",
      "authors": [
        "Lv"
      ],
      "year": "2011",
      "venue": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing",
      "arxiv": "arXiv:1605.07725"
    },
    {
      "citation_id": "16",
      "title": "Towards emotion-aided multi-modal dialogue act classification",
      "authors": [
        "Read"
      ],
      "year": "2011",
      "venue": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "Memor: A dataset for multimodal emotion reasoning in videos",
      "authors": [
        "Shen"
      ],
      "year": "2020",
      "venue": "Proceedings of the 28th ACM international conference on multimedia"
    },
    {
      "citation_id": "18",
      "title": "Order-free learning alleviating exposure bias in multi-label classification",
      "authors": [
        "Lee ; Che-Ping Tsai",
        "Hung-Yi Tsai",
        "; Lee",
        "Tsai"
      ],
      "year": "2018",
      "venue": "Proceedings of the conference. Association for computational linguistics. Meeting",
      "arxiv": "arXiv:1806.06176"
    },
    {
      "citation_id": "19",
      "title": "Attention is all you need",
      "authors": [
        "Vaswani"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "20",
      "title": "Generative adversarial networks: introduction and outlook",
      "authors": [
        "Wang"
      ],
      "year": "2017",
      "venue": "IEEE/CAA Journal of Automatica Sinica"
    },
    {
      "citation_id": "21",
      "title": "Cross-modality attention with semantic graph embedding for multi-label classification",
      "authors": [
        "Wu"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "22",
      "title": "Multimodal language analysis in the wild: Cmumosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "Zadeh"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "23",
      "title": "Multimodal multi-label emotion detection with modality and label dependence",
      "authors": [
        "Zhang"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)"
    },
    {
      "citation_id": "24",
      "title": "Multi-modal multi-label emotion recognition with heterogeneous hierarchical message passing",
      "authors": [
        "Zhang"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "25",
      "title": "Tailor versatile multi-modal learning for multi-label emotion recognition",
      "authors": [
        "Zhang"
      ],
      "year": "2021",
      "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD)",
      "arxiv": "arXiv:2201.05834"
    }
  ]
}