{
  "paper_id": "2308.03300v1",
  "title": "Do You Remember? Overcoming Catastrophic Forgetting For Fake Audio Detection",
  "published": "2023-08-07T05:05:49Z",
  "authors": [
    "Xiaohui Zhang",
    "Jiangyan Yi",
    "Jianhua Tao",
    "Chenglong Wang",
    "Chuyuan Zhang"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Current fake audio detection algorithms have achieved promising performances on most datasets. However, their performance may be significantly degraded when dealing with audio of a different dataset. The orthogonal weight modification to overcome catastrophic forgetting does not consider the similarity of genuine audio across different datasets. To overcome this limitation, we propose a continual learning algorithm for fake audio detection to overcome catastrophic forgetting, called Regularized Adaptive Weight Modification (RAWM). When fine-tuning a detection network, our approach adaptively computes the direction of weight modification according to the ratio of genuine utterances and fake utterances. The adaptive modification direction ensures the network can effectively detect fake audio on the new dataset while preserving its knowledge of old model, thus mitigating catastrophic forgetting. In addition, genuine audio collected from quite different acoustic conditions may skew their feature distribution, so we introduce a regularization constraint to force the network to remember the old distribution in this regard. Our method can easily be generalized to related fields, like speech emotion recognition. We also evaluate our approach across multiple datasets and obtain a significant performance improvement on cross-dataset experiments.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "With the development of speech synthesis and voice conversion technology  (Wang et al., 2018; 2021) , the models can generate human-like speech, which makes it difficult for most people to distinguish the generated audio from the real one. Although this technology has brought great convenience to human life, it has also brought great safety hazards to the country and society. Therefore, fake audio detection has attracted increasing attention in recent years. A series of challenges have been organized to detect fake audio, such as the ASVspoof challenge  (Wu et al., 2015; Kinnunen et al., 2017; Todisco et al., 2019; Yamagishi et al., 2021)  and the Audio Deep Synthesis Detection (ADD) challenge  (Yi et al., 2022) . In these competitions, deep neural networks have achieved great success. Currently, large-scale pre-trained models have gradually been applied to fake audio detection and achieved state-of-the-art results on several public fake audio detection datasets  (Tak et al., 2022; Martín-Doñas & Álvarez, 2022; Lv et al., 2022; Wang & Yamagishi, 2021) . Although fake audio detection achieves promising performance, it may be significantly degraded when dealing with audio of another dataset. The diversity of audio proposes a significant challenge to fake audio detection across datasets  (Zhang et al., 2021b; a) . Some approaches have been proposed to improve detection performance across datasets. An ensemble learning method is proposed to improve the detection ability of the model for unseen audio  (Monteiro et al., 2020)  and a dualadversarial domain adaptive network (DDAN) is designed to learn more generalized features for different datasets  (Wang et al., 2020) . Both methods require some audio from the old dataset, but in some practical situations, it is almost impossible to obtain them. For instance, a pre-trained model proposed by a company has been released to the public. It is unfeasible for the public to fine-tune it using the data belonging to the original company. In addition, a data augmentation method is proposed to extract more robust features for detection across datasets  (Zhang et al., 2021b) , which is only suitable for the datasets with similar feature distribution. In continual learning, a method called Detecting Fake Without Forgetting (DFWF) is proposed for fake audio detection  (Ma et al., 2021) . Although the above meth- Figure  1 : Schematic of SGD, OWM, and RAWM. (a), With RAWM, the optimization process searches for configurations that lead to great performance on both old (blue area) and new (green area) datasets. The center parts of the two areas represent better recognition performance than the other, and can be regarded as subspaces of the area mentioned by the OWM. A successful optimized configuration θrawm stops inside the overlapping subspace. However, the configuration θsgd obtained by SGD is optimized without considering forgetting, and the configuration θowm obtained by orthogonal weight modification can reach the overlapping area but not the overlapping subspace. (b), the RAWM adaptively modifies weight direction by introducing a projector that is orthogonal to the projector P proposed by OWM.\n\nods are effective, they still have some limitations, like the acquisition of old data in the ensemble learning method and the DDAN and deteriorating learning performance in the DFWF. This paper, however, aims to overcome catastrophic forgetting while exerting a positive influence on acquiring new knowledge without any previous samples.\n\nMost fake audio detection datasets are under clean conditions, where the genuine audio has a more similar feature distribution than the fake audio  (Ma et al., 2021) . A few datasets, however, are collected under different acoustic conditions  (Müller et al., 2022) , which makes a difference in their feature distributions of genuine audio  (Ma et al., 2022) . If we modify the model weights as the orthogonal weight modification (OWM) method  (Zeng et al., 2019)  which introduces a new weight direction orthogonal to all old data, most genuine audio with similar feature distribution across datasets can not be trained effectively. The reason is that new genuine audio is supposed by the OWM to damage learned knowledge, so it modifies new weight direction orthogonal to the old one regardless the new and old genuine audio have similar feature distribution and they can be seen as a whole from the same dataset. Based on the above inference, it is more effective for genuine audio on new datasets to be trained with a direction close to the previous one, rather than orthogonal to it. To address these issues, we propose a continual learning approach, named Regularized Adaptive Weight Modification (RAWM). In our method, if the proportion of fake audio is larger, the modified direction is closer to the orthogonal projector of the subspace spanned by all old input; if the proportion of genuine audio is larger, the modification is closer to the old input subspace. However, old and new datasets are collected from different acoustic conditions in some cases, where genuine audio may have quite different feature distributions. We address this issue by introducing a regularization constraint. This constraint forces the model to remember the old feature distribution. In addition, compared with the experience-replay-based method, RAWM does not require old data, which makes it suitable for most situations. The optimization process of RAWM is compared with that of the Stochastic Gradient Descent search (SGD) and OWM in Figure  1a .",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Contributions:",
      "text": "We propose a regularized adaptive weight modification algorithm to overcome catastrophic forgetting.\n\nThere are two essential steps in our method: adaptive weight modification (AWM) and regularization. The former AWM is proposed for continual learning in most situations where genuine audio has similar feature distribution and the latter regularization is introduced to ease the problem that genuine audio may have different feature distribution in a few cases. Although our method is inspired by the feature distribution similarity in fake audio detection, it can also be used in other related tasks, such as speech emotion recognition.\n\nThe experimental results show that our proposed method outperforms several continual learning methods in acquiring new knowledge and overcoming forgetting, including Elastic Weight Consolidation (EWC)  (Kirkpatrick et al., 2017) , Learning without Forgetting (LwF)  (Li & Hoiem, 2017) , OWM, and DFWF. The code of our method has been released in Regularized Adaptive Weight Modification.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "In continual learning, overcoming catastrophic forgetting methods can be divided into the following categories. The regularization methods perform a regularization on the objection function or regulate important weights that are essential for previous tasks  (Kinnunen et al., 2017; Zenke et al., 2017b; Aljundi et al., 2018; 2019; Mallya & Lazebnik, 2018; Serra et al., 2018) . The dynamic architecture methods reserve their previous knowledge by introducing additional layers or nodes and grow model architecture  (Rusu et al., 2016; Schwarz et al., 2018) ;  (Yoon et al., 2017) . The memory-based methods remember their previous data to prevent gradient updates from damage on their learned knowledge.  (Lopez-Paz & Ranzato, 2017; Castro et al., 2018; Wu et al., 2019; Lee et al., 2019) . The natural gradient descent methods approximate the Fisher information matrix in EWC using the generalized Gauss-Newton method to fast gradient descent  (Tseran et al., 2018; Chen et al., 2019) .\n\nAlthough the mainstream continual learning methods, such as the EWC, LwF and OWM, have achieved great success in many fields including image classification  (Zeng et al., 2019; Kirkpatrick et al., 2017) , object detection  (Perez-Rua et al., 2020) , semantic segmentation  (Cermelli et al., 2020) , lifelong language learning (de Masson D'Autume et al., 2019) and sentence representation  (Liu et al., 2019) . However, the approximation of regularization methods will produce error accumulation in continual learning  (Zenke et al., 2017a; Huszár, 2017; Ma et al., 2021) . In contrast, our proposed method only needs the current inputs, which leads to a better performance than others in error accumulation. Besides, we relax the regularized constraint in the DFWF and introduce a direction modification to solve the deteriorating learning performance problem.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Background",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Orthogonal Weight Modification",
      "text": "The OWM algorithm overcomes catastrophic forgetting by modifying the direction of weights on the new task. The modified direction P , which is a square matrix, is orthogonal to the subspace spanned by all inputs of the previous task. The orthogonal projector is constructed by an iterative method similar to the Recursive Least Square (RLS) algorithm  (Shah et al., 1992) , which hardly requires any previous samples.\n\nWe consider a feed-forward network consisting of L + 1 layers, indexed by l = 0, 1, • • • , L with the same activation function g(•). The x l (i, j) ∈ R s represents the output of the lth layer in response to the mean of the ith batch inputs on jth dataset, and the x l (i, j) T is the transpose matrix of the x l (i, j). The modified direction P can be calculated as:\n\nwhere α is a hyperparameter decaying with the number of tasks.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Learning Without Forgetting",
      "text": "The LwF algorithm is inspired by the idea of model distillation, where old knowledge is viewed as a penalty term to regulate the new model representation similar to the old.\n\nThe model trained on old datasets is replicated into two models with the same parameters. The two models are named teacher and student models in the LwF. In process of training on new datasets, the parameters of the teacher model are frozen to produce its features as \"soft labels\". The student model is trained by the loss function as:\n\nwhere λ 0 is a ratio coefficient representing the importance of learned knowledge; y o is the \"soft label\" produced by the teacher model and y n is the ground truth of new data; Both ŷo and ŷn are the softmax output of the student model. Both L old and L new are cross-entropy loss. The former L old regulates the output probabilities ŷo to be close to the recorded output y o from the teacher model and the latter L new encourages predictions ŷn to be consistent with the ground truth y n .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Proposed Method",
      "text": "On most fake audio detection datasets, under the same acoustic conditions, feature distributions of genuine audio are relatively more concentrated than the fake, which means the feature distribution of genuine audio has a smaller variance than that of fake audio  (Ma et al., 2021; Yan et al., 2022) .\n\nBesides, there are also a few datasets whose genuine audio has quite different feature distributions from others  (Ma et al., 2022; Müller et al., 2022) .\n\nBased on the observations, we propose a continual learning method, named Regularized Adaptive Weight Modification (RAWM), to overcome catastrophic forgetting. There are two essential steps in our method: adaptive direction modification (AWM) and regularization. The AWM is proposed for most situations where genuine audio has similar feature distribution. As shown in Figure  1b , by introducing an extra projector, which is a square matrix orthogonal to the projector proposed by the OWM, our method could adaptively modify weight direction closer to the previous inputs subspace. As for those genuine audio collected from quite different acoustic conditions, it is detrimental for learned knowledge to modify weight according to the rule we mentioned above, because their feature distribution is distinct from others. To address this issue, we introduce a regularization term to force the new distribution of inference to be similar to the old one. Our method does not require any replay of previous samples. In addition, our method is inspired by fake audio detection but it can easily be generalized to other related tasks. The reason is that most of them have one or more classes, like neutral emotion in speech emotion recognition (SER)  (Sharma, 2022) , with relatively similar feature distribution between different datasets. We also take SER as an example to present how our method generalizes to other fields in Sec. 4.3 and show the process of our algorithm in Algorithm 1.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Adaptive Weight Modification",
      "text": "We start by introducing an adaptive modification of weight direction according to the ratio β of classes with similar feature distribution between different datasets and others in batch data, which is essential for sequence training on multi-datasets. We first consider a feed-forward network like that described in Sec. 3.1. Then, we introduce a square matrix Q as a projector that is orthogonal to the P proposed by the OWM algorithm. This orthogonal projector can be written as Eq 3:\n\nwhere the projector P , which is orthogonal to the subspace spanned by all previous inputs, can be calculated as Eq 1 and I is an identity matrix. The construction of the orthogonal projector Q is mathematically sound  (Haykin, 2002; Ben-Israel & Greville, 2003; Bengio & LeCun, 2007) . To verify the modification direction according to the essential ratio β, we introduce the β defined as:\n\nrepresents the number of batch samples of b classes with relatively similar feature distributions on old and new datasets, respectively; the N t , t ∈ [b + 1, c] represents the number of batch samples of other c classes. By adding one to both the numerator and denominator, β can be calculated when all the batch data belong to classes in the numerator. As illustrated in Eq 3, the norm of projector Q is proportional to the ratio β. Our approach defines the modified direction R of weights as: R = Pnorm +mQnorm (5)\n\nwhere m is a constant to constrain the norm of projector Q to prevent gradient explosion or gradient vanishing in the backward process; P norm and Q norm are identity matrices normalized by P and Q, respectively. Normalization is to prevent the case that the change of β has little effect on the modified direction because of the large norm gap between P and Q. In the back-propagate (BP) process, the direction of network weights is modified as:\n\nwhere W l (i, j) ∈ R s×v represents the connection weights between the lth layer and the (l+1)th layer; γ represents the learning rate of this network; ∆W BP l (i, j) represents the standard BP gradient; R represents the modification projector in our method. In Eq 7, we can easily observe Algorithm 1 Regularized Adaptive Weight Modification 1: Require: Training data from different datasets, γ (learning rate), m (constant hyperparameter), T reg (constant hyperparameter).\n\n2: for every dataset j do 3:\n\nfor every batch i do 4:\n\nend for 22: end for that we modify weight direction adaptively by multiplying the BP gradient ∆W BP l (i, j) with our projector R whose direction is varied according to the ratio β of classes with similar feature distribution between different datasets and others.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Regularization",
      "text": "There are also a few datasets where genuine audio is collected from quite different acoustic conditions compared with others. In this case, it is unreasonable to use the above method directly. As for these utterances, we introduce an extra regularization forcing the model to remember the previous inference distribution.\n\nWe first replicate the pre-trained model into two models with the same parameters, one is the teacher model and the other one is the student model. The parameter of the teacher model is frozen in the process of training on the new dataset and the parameter of the student model is fine-tuned. Like the operation in the LwF, we view the softmax output y o from the teacher model as \"soft labels\" and use the loss function to slash the distinction between the \"soft labels\" y o and the softmax output y n of the student model, thus forcing the student model to remember the learned knowledge. The loss function, which is a modified cross-entropy loss, can be written as:\n\nwhere T reg is a constant hyperparameter. The y o , y n are softmax outputs of teacher and student models, respectively;\n\nThe ŷ is a normalized form of the y; The ŷ and y are one item of ŷ and y, respectively. The weight modification of this regularization ∆W BP lreg can be written as Eq 10.\n\n∆W BP lreg = ∇Lreg (10)",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Regularized Adaptive Weight Modification",
      "text": "In brief, our method RAWM is proposed for general continual learning conditions by modifying weight direction according to the ratio β of classes with similar feature distribution between different datasets and others in batch data. By introducing a regularized restriction, our method eases the problem that a few data belonging to classes in the numerator of the Eq 4 may have distinct feature distributions because they are collected from quite different conditions. Our method is inspired by fake audio detection where the ratio β in the Eq 4 can be written as:\n\nin which N g and N f represent the number of genuine and fake audios in a batch, respectively. As for another research area, for example, speech emotion recognition including happy, sad, angry, and neutral, the essential ratio can be written as:\n\nbecause the neutral emotion has a relatively more similar feature distribution than others between different datasets.\n\nThe N neu , N ang , N sad , and N hap represent the number of neutral, angry, sad, and happy data in a batch, respectively.\n\nConsidering a continual learning situation, the BP process of regularized adaptive weight modification can be written as Eq 13.\n\nCompared with the Eq 7, our method introduces a regularization constraint to the adaptive weight modification. The importance of the regularization depends on the hyperparameter η which is a coefficient measuring the attention degree of the knowledge acquired from old datasets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We conduct our experiments on four fake audio datasets, including the ASVspoof2019LA (S), ASVspoof2015 (T 1 ), VCC2020 (T 2 ), and In-the-Wild (T 3 ). The models are firstly trained using the training set of the S and are finetuned on the training sets of the other three datasets. All of the experiments are evaluated using two or four evaluation sets in these datasets. The final model in the study refers to the model that was trained after the entire training process and then evaluated on each dataset.\n\nASVspoof 2019 LA Dataset  (Todisco et al., 2019)  is the sub-challenge dataset (30 males and 37 females) containing three subsets: training, development, and evaluation. The training set and development share the same attack including four TTS and two VC algorithms. The bonafide audio is collected from the VCTK corpus  (Veaux et al., 2017) . The evaluation set contains totally different attacks.\n\nASVspoof2015 dataset  (Wu et al., 2015)  is an opensource standard dataset of genuine and synthetic speech in the ASVspoof2015 challenge. The genuine speech was recorded from 106 speakers (45 males and 61 females) with no significant channel or background noise effects. The spoofing speech is generated using a variety of speech synthesis and voice conversion algorithms.\n\nVCC2020 dataset  (Zhao et al., 2020)  is collected from Voice Conversion Challenge 2020. This dataset contains two subsets: a set of genuine audio provided by organizers and a set of fake audio provided by participating teams. Different from the previous three datasets, VCC2020 is a multilingual fake audio dataset, including English, Finnish, German and Mandarin.\n\nIn-the-Wild dataset  (Müller et al., 2022)  contains a set of deep fake audio (and corresponding real audio) of 58 politicians and other public figures collected from publicly available sources, such as social networks and video streaming platforms. In total, 20.8 hours of genuine audio and 17.2 hours of fake audio were collected. On average, each speaker had 23 minutes of genuine audio and 18 minutes of fake audio.\n\nWe divide the genuine and fake audios of the VCC2020 dataset into four subsets. A quarter is used to build the evaluation set, a quarter to build the development set, and the rest to be used as the training set. The In-the-Wild dataset is divided in the same way as the VCC2020. The detailed statistics of the datasets are presented in Table  1a . The Equal Error Rate (EER), which is widely used for fake audio detection and speaker verification, is applied to evaluate the experimental performance.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Experimental Setup",
      "text": "Fake Audio Detection Model: We use the pre-trained model Wav2vec 2.0  (Baevski et al., 2020)  as the feature extractor and the self-attention convolutional neural network (S-CNN) as the classifier. The parameters of Wav2vec 2.0 is loaded from the pre-train model XLSR-53  (Conneau et al., 2020) . The classifier S-CNN contains three 1D-Convolution layers, one self-attention layer, and two full connection layers, according to the forward process. The input dimension of the first convolution layer is 256 and the hidden dimension of all convolution layers is 80. The kernel size and stride are set to 5 and 1, respectively. The hidden dimension of all full connection layers is 80 and the output dimension is 2.\n\nTraining Details: We fine-tune the model weights including the pre-trained model XLSR-53 and the classifier S-CNN. All of the parameters are trained by the Adam optimizer with a batch size of 2 and a learning rate γ of 0.0001. The constant m and T reg in RAWM are set to 0.1 and 2, respectively. The α is initialized to 0.00001 for convolution layers, 0.0001 for the self-attention layer, and 0.1 for full connection layers. The norm in normalization of projector P and Q is the L 2 norm. In addition, we present the results\n\nof training all datasets (Tain-on-All) that is considered to be the lower bound to all continual learning methods we mentioned  (Parisi et al., 2019) . All results are (re)produced by us and averaged over 7 runs with standard deviations.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Baseline",
      "text": "We first train our model on the training set of the S dataset.\n\nTable  1b  shows the detection performance of our baseline on multiple evaluation sets which is very close to the stateof-the-art result  (Nautsch et al., 2021)  in the same dataset.\n\nAlthough the model achieves promising performance on S, its detection accuracy degrades significantly on other datasets. In addition, our baseline achieves the lowest crossdatasets EER on T 1 dataset among three unseen datasets, which verifies that the detection model will have better performance when facing genuine audio with more similar feature distribution. Apart from that, the results with different training steps are presented in Table  8  in the appendix.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Effects Of The Η For Our Method",
      "text": "Sequence training between two datasets: We start by performing some experiments to evaluate the effectiveness of η in RAWM, which represents the attention degree to learned knowledge. In Table  2 , we can easily observe that the RAWM achieves great performance on both old and new datasets, especially in the experiment on S → T 1 . By comparing the results of three cross-datasets, we observe that when the new and old datasets have similar feature distribu- tion (Table  2a ), there is an improvement in the performance of both acquiring new knowledge and overcoming forgetting with the increasing of η (η < 1); When the feature distribution of the new and old datasets is different (Table  2b , Table  2c ), it is the model when η = 0.50 that achieves the best result, which shows that the regularization we introduced is also of benefit to performance on both learning and overcoming forgetting.\n\nSequence training on four datasets: We also present the results on multiple evaluation sets about different η in Table  2d . It can be observed that our method slashes performance degradation when training across datasets. The RAWM achieves the lowest EER among the results when η = 0.50, which demonstrates that the same attention degree to both old and new datasets is the best choice for learning and overcoming forgetting. In addition, the results of S, T 1 and T 2 show that the model with larger η is more effective in overcoming forgetting.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Ablation Studies For Our Method",
      "text": "Sequence training between two datasets: In this section, we compare our proposed method with adaptive weight modification without regularization (-REG) and orthogonal weight modification without regularization (-AWM). Table  3  presents their EER on three evaluation sets. We observe that RAWM achieves similar EER to -REG on the new dataset, both of them are superior significantly to -AWM, which shows that the adaptive weight modification has a significant positive impact on acquiring knowledge, while regularization impacts little. As for overcoming forgetting, when the feature distribution of the new and old datasets is similar (Table  3a ), the EER of the -REG on the old datasets is much lower than that of the -AWM and higher than that of the RAWM, which shows that the adaptive weight modification and regularization can significantly reduce the forgetting in this case. When the languages of the new and old datasets are different (Table  3b ), the EER of RAWM in the old datasets is similar to that of the -REG and much lower than that of the -AWM, which also proves that the adaptive weight modification has a significant positive impact on overcoming forgetting. When the feature distribution of the new and old datasets is quite different (Table  3c ), the EER of the -REG is similar to that of the -AWM and much higher than that of the RAWM, which shows that in this case, regularization is of great benefit to overcoming forgetting, while the effect of adaptive weight modification is not obvious.\n\nSequence training on four datasets: In this section, we present the results of the ablation study on four evaluation sets in Table  3d . We observe that the EER of -REG to -AWM degrades more obviously than that of RAWM to -REG on all evaluation sets, which indicates that adaptive weight modification has a more obvious benefit in learning and overcoming forgetting than regularization for sequence training on multiple datasets.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Comparison Of Our Method With Other Methods",
      "text": "Sequence training between two datasets: We compare our method with several methods in Table  4 . The EWC, LwF, and OWM as three mainstream continual learning methods achieve great success in many fields. The DFWF is the first continual learning method to overcome forgetting for fake audio detection. The results demonstrate that fine-tuning without modification (Fine-tune) forgets previous knowledge obviously. The forgetting of RAWM is one-tenth that of Fine-tune on Table  4a  and the EER on the new dataset of RAWM is also half that of Fine-tune. We also observe that the Fine-tune, EWC and OWM achieve similar performance in three experiments and the performance of LwF outperforms theirs on the new dataset. The DFWF is more effective in overcoming forgetting than the above methods, but its performance on the new dataset is inferior to others. Compared with others, our method achieves lower EER on both old and new datasets of all experiments, which demonstrates that both overcoming forgetting and learning could definitely benefit from our method when training across datasets, regardless of whether the datasets have similar feature distributions (Table  4a , Table  4b ) or same languages (Table  4c ).\n\nSequence training on four datasets: In addition, We compare our method with several methods for sequence training on four datasets in Table  4d . The results show that most methods achieve lower EERs than fine-tuning, and the best result for overcoming forgetting and learning is our proposed method, which indicates that the RAWM is superior to others for sequence training on both two and multiple datasets.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "The Performance Of The Rawm With A Few Samples",
      "text": "We also present some results of the model when training on a few samples of new datasets. In our experiments, only 100 samples randomly selected from new datasets T 1 were used for fine-tuning or continual learning. All models are first trained on the S datasets and then fine-tuned or continually learned on the T 1 dataset. All models are trained on the new dataset within five steps. From the results, we can observe that our method RAWM also achieves the best performance on both old and new datasets and the learning performance is very close to the result of Train-on-All which is the the lower bound to all continual learning methods. By comparing the results in Table  4a  and Table  5 , we can easily find that reducing the number of samples has only a little damage to our method.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "The Rawm For Speech Emotion Recognition",
      "text": "Our method is inspired by fake audio detection and it can be easily used in other related tasks. We take speech emotion recognition as an example to evaluate the performance of the RAWM in other fields. In this regard, the previous result shows that neutral emotion achieved the highest recognition accuracy across thirteen emotion datasets  (Sharma, 2022) . So we infer that neutral speech has a more similar feature distribution than that of happy, sad, and angry, thus the ratio β of our method can be written as Eq 12. Based on this observation, we conduct some experiments for speech emotion recognition. We choose four emotional classes, including neutral, happy, angry, and sad. The feature extractor and classifier are as same as that in fake audio detection. The results have been shown in Table  6 . It could be easily observed that our method still achieves the highest accuracy on both datasets. The effect of our method in overcoming forgetting is most obvious and its learning performance is very close to the result of Train-on-All.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "The Rawm For Image Recognition",
      "text": "We also have performed evaluation experiments on the image recognition domain in the CLEAR benchmark  (Lin et al., 2021)  to explore the broader applicability of our method. The CLEAR-10 benchmark for continual learning consists of 10 image recognition experiences, each comprising 11 classes such as camera, baseball, laptop, etc. To evaluate the effectiveness of our method, we selected several widely used continual learning algorithms, including the Replay, EWC, LwF, GDumbFinetune (GDF)  (Prabhu et al., 2020) , CopyWeights with Re-init (CWR)  (Lomonaco & Maltoni, 2017) , and OWM methods. Table  7  presents the results of the comparative analysis. We treated the Replay method, which corresponds to the \"Train-on-all\" approach in our paper, as the upper bound of accuracy for all continual learning methods. The EWC and LwF methods have been introduced in our paper. In the GDF algorithm, we set the memory size to be the same as the number of training data in one bucket, and for CWR, the cwr layer was positioned as the final layer of the model. To extract features, we employed a pre-trained ResNet-50  (He et al., 2016)  as a feature extractor, producing 2048-dimensional feature vectors. A linear layer with input and output dimensions of 2048 and 11, respectively, was used as the downstream classifier. Our experiments were conducted with a batch size of 512, an initial learning rate of 1 (decayed by a factor of 0.1 after 60 epochs), and the SGD optimizer with a momentum of 0.9. The experimental results in Table  7  demonstrate that our proposed method, referred to as RAWM, consistently achieved the best performance across most tasks. In particular, in Exp 3 and Exp 8 , the performance of our method closely approached the highest accuracy achieved among all the evaluated methods.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In this work, we propose a continual learning algorithm to overcome catastrophic forgetting, called RAWM, that could adaptively modify the weight direction in process of training on new datasets. We also introduce a regularization to deal with the situation when old and new datasets are collected from quite different conditions. The experimental results demonstrate that our method outperforms four continual learning methods in learning and overcoming forgetting in scenarios of sequence training on both two and multiple datasets. The result shows that our method still achieves the best performance among the above methods when training on a few samples. Besides, our method is inspired by fake audio detection and the results show that it can be easily generalized to other fields, like speech emotion recognition. In addition, our method does not require previous data; thus it can be applied to most classification networks. We have yet to study how to make the model learn the weight direction gradually in the process of training on new datasets without any constraint, and exploring generalization to related tasks will form the focus of our future studies.",
      "page_start": 9,
      "page_end": 9
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Schematic of SGD, OWM, and RAWM. (a), With RAWM, the optimization process searches for configurations",
      "page": 2
    },
    {
      "caption": "Figure 1: b, by introducing an ex-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "Abstract",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "1. Introduction"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "With the development of speech synthesis and voice con-"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "version technology (Wang et al., 2018; 2021), the models"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "Current\nfake\naudio detection algorithms have",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "can generate human-like speech, which makes it difficult"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "achieved\npromising\nperformances\non\nmost",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "for most people to distinguish the generated audio from the"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "datasets.\nHowever,\ntheir performance may be",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "real one. Although this technology has brought great conve-"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "significantly degraded when dealing with audio of",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "nience to human life, it has also brought great safety hazards"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "a different dataset. The orthogonal weight modi-",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "to the country and society. Therefore, fake audio detection"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "fication to overcome catastrophic forgetting does",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "has attracted increasing attention in recent years. A series of"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "not consider the similarity of genuine audio across",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "challenges have been organized to detect fake audio, such as"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "different datasets. To overcome this limitation, we",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "the ASVspoof challenge (Wu et al., 2015; Kinnunen et al.,"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "propose a continual\nlearning algorithm for fake",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "2017; Todisco et al., 2019; Yamagishi et al., 2021) and the"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "audio detection to overcome catastrophic forget-",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "Audio Deep Synthesis Detection (ADD) challenge (Yi et al.,"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "ting, called Regularized Adaptive Weight Modi-",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "2022).\nIn these competitions, deep neural networks have"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "fication (RAWM). When fine-tuning a detection",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "achieved great success. Currently,\nlarge-scale pre-trained"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "network, our approach adaptively computes the",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "models have gradually been applied to fake audio detection"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "direction of weight modification according to the",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "and achieved state-of-the-art results on several public fake"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "ratio of genuine utterances and fake utterances.",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "audio detection datasets (Tak et al., 2022; Mart´ın-Do˜nas &"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "The adaptive modification direction ensures the",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "´"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "Alvarez, 2022; Lv et al., 2022; Wang & Yamagishi, 2021)."
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "network can effectively detect fake audio on the",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "Although fake audio detection achieves promising perfor-"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "new dataset while preserving its knowledge of old",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "mance, it may be significantly degraded when dealing with"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "model, thus mitigating catastrophic forgetting. In",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "audio of another dataset. The diversity of audio proposes a"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "addition, genuine audio collected from quite dif-",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "significant challenge to fake audio detection across datasets"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "ferent acoustic conditions may skew their feature",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "(Zhang et al., 2021b;a)."
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "distribution, so we introduce a regularization con-",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "straint to force the network to remember the old",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "Some approaches have been proposed to improve detec-"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "distribution in this regard. Our method can easily",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "tion performance across datasets. An ensemble learning"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "be generalized to related fields, like speech emo-",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "method is proposed to improve the detection ability of the"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "tion recognition. We also evaluate our approach",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "model for unseen audio (Monteiro et al., 2020) and a dual-"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "across multiple datasets and obtain a significant",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "adversarial domain adaptive network (DDAN) is designed"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "performance improvement on cross-dataset exper-",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "to learn more generalized features for different datasets"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "iments.",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "(Wang et al., 2020). Both methods require some audio from"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "the old dataset, but\nin some practical situations,\nit\nis al-"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "most impossible to obtain them. For instance, a pre-trained"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "model proposed by a company has been released to the pub-"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "1State Key Laboratory of Multimodal Artificial Intelligence",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "lic.\nIt\nis unfeasible for the public to fine-tune it using the"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "Systems, Institute of Automation, Chinese Academy of Sciences,",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "data belonging to the original company. In addition, a data"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "Beijing, China 2School of Computer and Information Technology,",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "University of Beijing Jiaotong, Beijing, China 3Department of",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "augmentation method is proposed to extract more robust"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "Automation, Tsinghua University, Beijing, China 4University of",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "features for detection across datasets (Zhang et al., 2021b),"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "Science and Technology of China, Beijing, China. Correspondence",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "which is only suitable for the datasets with similar feature"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "to: Jiangyan Yi <jiangyan.yi@nlpr.ia.ac.cn>.",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "distribution. In continual learning, a method called Detect-"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "ing Fake Without Forgetting (DFWF) is proposed for fake"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "Proceedings of\nthe 40 th International Conference on Machine",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "Learning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": "audio detection (Ma et al., 2021). Although the above meth-"
        },
        {
          "Xiaohui Zhang 1 2\nJiangyan Yi 1": "2023 by the author(s).",
          "Jianhua Tao 3 Chenglong Wang 1 4 Chuyuan Zhang 1": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "SGD"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "(a)\n(b)"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "Figure 1: Schematic of SGD, OWM, and RAWM. (a), With RAWM, the optimization process searches for configurations"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "that\nlead to great performance on both old (blue area) and new (green area) datasets. The center parts of the two areas"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "represent better recognition performance than the other, and can be regarded as subspaces of the area mentioned by the"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "OWM. A successful optimized configuration ˆθrawm stops inside the overlapping subspace. However, the configuration ˆθsgd"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "obtained by SGD is optimized without considering forgetting, and the configuration ˆθowm obtained by orthogonal weight"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "modification can reach the overlapping area but not the overlapping subspace. (b), the RAWM adaptively modifies weight"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "direction by introducing a projector that is orthogonal to the projector P proposed by OWM."
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "ods are effective, they still have some limitations, like the\nold feature distribution.\nIn addition, compared with the"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "acquisition of old data in the ensemble learning method and\nexperience-replay-based method, RAWM does not require"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "the DDAN and deteriorating learning performance in the\nold data, which makes it suitable for most situations. The"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "DFWF. This paper, however, aims to overcome catastrophic\noptimization process of RAWM is compared with that of"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "forgetting while exerting a positive influence on acquiring\nthe Stochastic Gradient Descent search (SGD) and OWM"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "new knowledge without any previous samples.\nin Figure 1a."
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "Most fake audio detection datasets are under clean condi-\nContributions: We propose a regularized adaptive weight"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "tions, where the genuine audio has a more similar feature\nmodification algorithm to overcome catastrophic forgetting."
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "distribution than the fake audio (Ma et al., 2021). A few\nThere are two essential steps in our method: adaptive weight"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "datasets, however, are collected under different acoustic\nmodification (AWM) and regularization. The former AWM"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "conditions (M¨uller et al., 2022), which makes a difference\nis proposed for continual learning in most situations where"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "in their feature distributions of genuine audio (Ma et al.,\ngenuine audio has similar feature distribution and the latter"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "2022).\nIf we modify the model weights as the orthogonal\nregularization is introduced to ease the problem that gen-"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "weight modification (OWM) method (Zeng et al., 2019)\nuine audio may have different feature distribution in a few"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "which introduces a new weight direction orthogonal to all\ncases. Although our method is inspired by the feature distri-"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "old data, most genuine audio with similar\nfeature distri-\nbution similarity in fake audio detection, it can also be used"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "bution across datasets can not be trained effectively. The\nin other related tasks, such as speech emotion recognition."
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "reason is that new genuine audio is supposed by the OWM\nThe experimental results show that our proposed method"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "to damage learned knowledge, so it modifies new weight\noutperforms several continual learning methods in acquir-"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "direction orthogonal to the old one regardless the new and\ning new knowledge and overcoming forgetting, including"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "old genuine audio have similar feature distribution and they\nElastic Weight Consolidation (EWC)\n(Kirkpatrick et al.,"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "can be seen as a whole from the same dataset. Based on\n2017), Learning without Forgetting (LwF) (Li & Hoiem,"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "the above inference, it is more effective for genuine audio\n2017), OWM, and DFWF. The code of our method has been"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "on new datasets to be trained with a direction close to the\nreleased in Regularized Adaptive Weight Modification."
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "previous one, rather than orthogonal to it. To address these"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "issues, we propose a continual\nlearning approach, named"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "2. Related Work"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "Regularized Adaptive Weight Modification (RAWM).\nIn"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "our method,\nif the proportion of fake audio is larger,\nthe\nIn continual\nlearning, overcoming catastrophic forgetting"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "modified direction is closer to the orthogonal projector of\nmethods can be divided into the following categories. The"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "the subspace spanned by all old input; if the proportion of\nregularization methods perform a regularization on the ob-"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "genuine audio is larger, the modification is closer to the old\njection function or regulate important weights that are es-"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "input subspace. However, old and new datasets are collected\nsential\nfor previous tasks (Kinnunen et al., 2017; Zenke"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "from different acoustic conditions in some cases, where gen-\net al., 2017b; Aljundi et al., 2018; 2019; Mallya & Lazeb-"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "uine audio may have quite different feature distributions.\nnik, 2018; Serra et al., 2018). The dynamic architecture"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "We address this issue by introducing a regularization con-\nmethods reserve their previous knowledge by introducing ad-"
        },
        {
          "መ\nOWM\n𝜃𝑟𝑎𝑤𝑚": "straint. This constraint forces the model to remember the\nditional layers or nodes and grow model architecture (Rusu"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "et al., 2016; Schwarz et al., 2018); (Yoon et al., 2017). The"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "memory-based methods remember their previous data to pre-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "vent gradient updates from damage on their learned knowl-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "edge. (Lopez-Paz & Ranzato, 2017; Castro et al., 2018; Wu"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "et al., 2019; Lee et al., 2019). The natural gradient descent"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "methods approximate the Fisher information matrix in EWC"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "using the generalized Gauss-Newton method to fast gradient"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "descent (Tseran et al., 2018; Chen et al., 2019)."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Although the mainstream continual learning methods, such"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "as the EWC, LwF and OWM, have achieved great success in"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "many fields including image classification (Zeng et al., 2019;"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Kirkpatrick et al., 2017), object detection (Perez-Rua et al.,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "2020), semantic segmentation (Cermelli et al., 2020), life-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "long language learning (de Masson D’Autume et al., 2019)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "and sentence representation (Liu et al., 2019). However, the"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "approximation of regularization methods will produce error"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "accumulation in continual\nlearning (Zenke et al., 2017a;"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Husz´ar, 2017; Ma et al., 2021).\nIn contrast, our proposed"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "method only needs the current inputs, which leads to a better"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "performance than others in error accumulation. Besides, we"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "relax the regularized constraint in the DFWF and introduce"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "a direction modification to solve the deteriorating learning"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "performance problem."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "3. Background"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "3.1. Orthogonal Weight Modification"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "The OWM algorithm overcomes catastrophic forgetting by"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "modifying the direction of weights on the new task. The"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "modified direction P , which is a square matrix, is orthog-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "onal to the subspace spanned by all inputs of the previous"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "task. The orthogonal projector is constructed by an itera-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "tive method similar to the Recursive Least Square (RLS)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "algorithm (Shah et al., 1992), which hardly requires any"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "previous samples."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "We consider a feed-forward network consisting of L + 1"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "layers, indexed by l = 0, 1, · · ·\n, L with the same activation"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "function g(·). The xl(i, j) ∈ Rs represents the output of"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "the lth layer in response to the mean of the ith batch inputs"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "on jth dataset, and the xl(i, j)T is the transpose matrix of"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "the xl(i, j). The modified direction P can be calculated as:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Pl(i, j) = Pl(i−1, j) − kl(i, j)xl−1(i, j)T Pl(i−1, j)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "(1)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Pl(i−1, j)xl−1(i, j)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "kl(i, j) ="
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "α + xl−1(i, j)T Pl(i−1, j)xl−1(i, j)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "where α is a hyperparameter decaying with the number of"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "tasks."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "3.2. Learning without Forgetting"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "The LwF algorithm is inspired by the idea of model distil-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "lation, where old knowledge is viewed as a penalty term"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "to regulate the new model representation similar to the old."
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "4.1. Adaptive Weight Modification"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "We start by introducing an adaptive modification of weight"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "direction according to the ratio β of classes with similar"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "feature distribution between different datasets and others"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "in batch data, which is essential for sequence training on"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "multi-datasets. We first consider a feed-forward network"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "like that described in Sec. 3.1. Then, we introduce a square"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "matrix Q as a projector that is orthogonal to the P proposed"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "by the OWM algorithm. This orthogonal projector can be"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "written as Eq 3:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Q = β[I −P (P T P )−1P T ]\n(3)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "where the projector P , which is orthogonal to the subspace"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "spanned by all previous inputs, can be calculated as Eq 1 and"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "I is an identity matrix. The construction of the orthogonal"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "projector Q is mathematically sound (Haykin, 2002; Ben-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Israel & Greville, 2003; Bengio & LeCun, 2007). To verify"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "the modification direction according to the essential ratio β,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "we introduce the β defined as:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "b(cid:80) t\nNt + 1"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "=1"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "β =\n(4)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "b+c"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "(cid:80)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Nt + 1"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "t=b+1"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "in which the Nt, t ∈ [1, b] represents the number of batch"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "samples of b classes with relatively similar feature distri-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "butions on old and new datasets, respectively; the Nt, t ∈"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "[b + 1, c] represents the number of batch samples of other c"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "classes. By adding one to both the numerator and denom-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "inator, β can be calculated when all the batch data belong"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "to classes in the numerator. As illustrated in Eq 3, the norm"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "of projector Q is proportional to the ratio β. Our approach"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "defines the modified direction R of weights as:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "(5)\nR = Pnorm +mQnorm"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Q\nP"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ",\n(6)\nQnorm =\nPnorm ="
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "||P ||\n||I −P (P T P )−1P ||"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "with the same parameters, one is the teacher model and the"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "other one is the student model. The parameter of the teacher"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "model is frozen in the process of training on the new dataset"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "and the parameter of the student model is fine-tuned. Like"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "the operation in the LwF, we view the softmax output yo"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "from the teacher model as ”soft\nlabels” and use the loss"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "function to slash the distinction between the ”soft labels” yo"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "and the softmax output yn of the student model, thus forcing"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "the student model to remember the learned knowledge. The"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "loss function, which is a modified cross-entropy loss, can"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "be written as:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "(8)\nLreg(ˆyo, ˆyn) = −ˆyo · log ˆyn"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "y1/Treg\ny1/Treg"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ",\n(9)\nyo =\nyn ="
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "(cid:80) y1/Treg\n(cid:80) y1/Treg"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "where Treg is a constant hyperparameter. The yo, yn are"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "softmax outputs of teacher and student models, respectively;"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "The ˆy is a normalized form of the y; The ˆy and y are one"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "item of ˆy and y, respectively. The weight modification of"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "this regularization ∆W BP\ncan be written as Eq 10."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "lreg"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "∆W BP\n(10)\nlreg = ∇Lreg"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "4.3. Regularized Adaptive Weight Modification"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "In brief, our method RAWM is proposed for general con-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "tinual\nlearning conditions by modifying weight direction"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "according to the ratio β of classes with similar feature dis-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "tribution between different datasets and others in batch data."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "By introducing a regularized restriction, our method eases"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "the problem that a few data belonging to classes in the nu-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "merator of the Eq 4 may have distinct feature distributions"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "because they are collected from quite different conditions."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Our method is inspired by fake audio detection where the"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "ratio β in the Eq 4 can be written as:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Ng + 1"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "β =\n(11)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Nf + 1"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "in which Ng and Nf represent the number of genuine and"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "fake audios in a batch, respectively. As for another research"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "area,\nfor example, speech emotion recognition including"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "happy, sad, angry, and neutral,\nthe essential ratio can be"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "written as:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Nneu + 1"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "β =\n(12)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Nang + Nhap + Nsad + 1"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "because the neutral emotion has a relatively more similar"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "feature distribution than others between different datasets."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "The Nneu, Nang, Nsad, and Nhap represent the number of"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "neutral, angry, sad, and happy data in a batch, respectively."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Considering a continual learning situation, the BP process"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "of regularized adaptive weight modification can be written"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "as Eq 13."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "j = 1\nWl(i, j) = Wl(i−1, j)+G"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "j > 1\nWl(i, j) = Wl(i−1, j)+ H"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "(13)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "G = γ(i, j)∆W BP\n(i, j)"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "H = (1−η)Rl(j−1)G+η∆W BP\nlreg (i, j)"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: a. The be the lower bound to all continual learning methods we",
      "data": [
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": ""
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "η"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "Baseline"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "0.00"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "0.20"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "0.25"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "0.50"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "0.75"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "1.00"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: a. The be the lower bound to all continual learning methods we",
      "data": [
        {
          "detection and speaker verification, is applied to evaluate the": "experimental performance.",
          "by us and averaged over 7 runs with standard deviations.": ""
        },
        {
          "detection and speaker verification, is applied to evaluate the": "",
          "by us and averaged over 7 runs with standard deviations.": "5.3. Baseline"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "5.2. Experimental Setup",
          "by us and averaged over 7 runs with standard deviations.": ""
        },
        {
          "detection and speaker verification, is applied to evaluate the": "",
          "by us and averaged over 7 runs with standard deviations.": "We first train our model on the training set of the S dataset."
        },
        {
          "detection and speaker verification, is applied to evaluate the": "Fake Audio Detection Model: We use the pre-trained",
          "by us and averaged over 7 runs with standard deviations.": "Table 1b shows the detection performance of our baseline"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "model Wav2vec 2.0 (Baevski et al., 2020) as the feature",
          "by us and averaged over 7 runs with standard deviations.": "on multiple evaluation sets which is very close to the state-"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "extractor and the self-attention convolutional neural network",
          "by us and averaged over 7 runs with standard deviations.": "of-the-art result (Nautsch et al., 2021) in the same dataset."
        },
        {
          "detection and speaker verification, is applied to evaluate the": "(S-CNN) as the classifier. The parameters of Wav2vec 2.0 is",
          "by us and averaged over 7 runs with standard deviations.": "Although the model achieves promising performance on"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "loaded from the pre-train model XLSR-53 (Conneau et al.,",
          "by us and averaged over 7 runs with standard deviations.": "S,\nits detection accuracy degrades significantly on other"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "2020). The classifier S-CNN contains three 1D-Convolution",
          "by us and averaged over 7 runs with standard deviations.": "datasets. In addition, our baseline achieves the lowest cross-"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "layers, one self-attention layer, and two full connection lay-",
          "by us and averaged over 7 runs with standard deviations.": "datasets EER on T1 dataset among three unseen datasets,"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "ers, according to the forward process. The input dimension",
          "by us and averaged over 7 runs with standard deviations.": "which verifies that the detection model will have better per-"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "of the first convolution layer is 256 and the hidden dimen-",
          "by us and averaged over 7 runs with standard deviations.": "formance when facing genuine audio with more similar"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "sion of all convolution layers is 80. The kernel size and",
          "by us and averaged over 7 runs with standard deviations.": "feature distribution. Apart from that, the results with differ-"
        },
        {
          "detection and speaker verification, is applied to evaluate the": "stride are set to 5 and 1, respectively. The hidden dimension",
          "by us and averaged over 7 runs with standard deviations.": "ent training steps are presented in Table 8 in the appendix."
        },
        {
          "detection and speaker verification, is applied to evaluate the": "of all full connection layers is 80 and the output dimension",
          "by us and averaged over 7 runs with standard deviations.": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 3: a), the EER of the –REG on",
      "data": [
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "(a)"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "Method"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "Baseline"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "Train-on-All"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "Fine-tune"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "EWC"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "OWM"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "LwF"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "DFWF"
        },
        {
          "S → T1 → T2 → T3 and is evaluated using evaluation sets.": "RAWM(Ours)"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 3: a), the EER of the –REG on",
      "data": [
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "tion (Table 2a), there is an improvement in the performance",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "has a significant positive impact on acquiring knowledge,"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "of both acquiring new knowledge and overcoming forget-",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "while regularization impacts little. As for overcoming for-"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "ting with the increasing of η (η < 1); When the feature",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "getting, when the feature distribution of the new and old"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "distribution of the new and old datasets is different (Table",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "datasets is similar\n(Table 3a),\nthe EER of\nthe –REG on"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "2b, Table 2c), it is the model when η = 0.50 that achieves",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "the old datasets is much lower than that of the –AWM and"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "the best result, which shows that the regularization we in-",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "higher than that of the RAWM, which shows that the adap-"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "troduced is also of benefit to performance on both learning",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "tive weight modification and regularization can significantly"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "and overcoming forgetting.",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "reduce the forgetting in this case. When the languages of"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "the new and old datasets are different (Table 3b), the EER of"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "Sequence training on four datasets: We also present the",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "RAWM in the old datasets is similar to that of the –REG and"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "results on multiple evaluation sets about different η in Table",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "much lower than that of the –AWM, which also proves that"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "2d. It can be observed that our method slashes performance",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "the adaptive weight modification has a significant positive"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "degradation when training across datasets.\nThe RAWM",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "impact on overcoming forgetting. When the feature distri-"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "achieves the lowest EER among the results when η = 0.50,",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "bution of the new and old datasets is quite different (Table"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "which demonstrates that the same attention degree to both",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "3c), the EER of the –REG is similar to that of the –AWM"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "old and new datasets is the best choice for\nlearning and",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "and much higher than that of the RAWM, which shows that"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "overcoming forgetting. In addition, the results of S, T1 and",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "in this case, regularization is of great benefit to overcoming"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "T2 show that the model with larger η is more effective in",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "forgetting, while the effect of adaptive weight modification"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "overcoming forgetting.",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "is not obvious."
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "5.5. Ablation studies for our method",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "Sequence training on four datasets:\nIn this section, we"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "present the results of the ablation study on four evaluation"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "Sequence training between two datasets: In this section,",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "sets in Table 3d. We observe that\nthe EER of –REG to"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "we compare our proposed method with adaptive weight",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "–AWM degrades more obviously than that of RAWM to –"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "modification without regularization (–REG) and orthogo-",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "REG on all evaluation sets, which indicates that adaptive"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "nal weight modification without\nregularization (–AWM).",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "weight modification has a more obvious benefit in learning"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "Table 3 presents their EER on three evaluation sets. We",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "and overcoming forgetting than regularization for sequence"
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "observe that RAWM achieves similar EER to –REG on",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": ""
        },
        {
          "RAWM(Ours)\n0.666\n0.247\nRAWM(Ours)\n1.237\n3.721": "",
          "RAWM(Ours)\n4.942\n3.249\n1.508\n0.641\n3.850\n3.163\nRAWM(Ours)": "training on multiple datasets."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table 5: The EER(%) of few samples experiments. All Table 6: The Acc(%) of various continual learning meth-",
      "data": [
        {
          "experiments are evaluated using the evaluation set on S and": "",
          "Podcast → IEMOCAP and are evaluated using the eval-": "uation set on MSP-Podcast and IEMOCAP"
        },
        {
          "experiments are evaluated using the evaluation set on S and": "Method",
          "Podcast → IEMOCAP and are evaluated using the eval-": "MSP-Podcast"
        },
        {
          "experiments are evaluated using the evaluation set on S and": "Baseline",
          "Podcast → IEMOCAP and are evaluated using the eval-": ""
        },
        {
          "experiments are evaluated using the evaluation set on S and": "",
          "Podcast → IEMOCAP and are evaluated using the eval-": "54.446"
        },
        {
          "experiments are evaluated using the evaluation set on S and": "Train-on-All",
          "Podcast → IEMOCAP and are evaluated using the eval-": ""
        },
        {
          "experiments are evaluated using the evaluation set on S and": "",
          "Podcast → IEMOCAP and are evaluated using the eval-": "54.396"
        },
        {
          "experiments are evaluated using the evaluation set on S and": "Fine-tune",
          "Podcast → IEMOCAP and are evaluated using the eval-": ""
        },
        {
          "experiments are evaluated using the evaluation set on S and": "",
          "Podcast → IEMOCAP and are evaluated using the eval-": "24.094"
        },
        {
          "experiments are evaluated using the evaluation set on S and": "EWC",
          "Podcast → IEMOCAP and are evaluated using the eval-": ""
        },
        {
          "experiments are evaluated using the evaluation set on S and": "",
          "Podcast → IEMOCAP and are evaluated using the eval-": "35.819"
        },
        {
          "experiments are evaluated using the evaluation set on S and": "OWM",
          "Podcast → IEMOCAP and are evaluated using the eval-": ""
        },
        {
          "experiments are evaluated using the evaluation set on S and": "",
          "Podcast → IEMOCAP and are evaluated using the eval-": "32.267"
        },
        {
          "experiments are evaluated using the evaluation set on S and": "LwF",
          "Podcast → IEMOCAP and are evaluated using the eval-": ""
        },
        {
          "experiments are evaluated using the evaluation set on S and": "DFWF",
          "Podcast → IEMOCAP and are evaluated using the eval-": "38.800"
        },
        {
          "experiments are evaluated using the evaluation set on S and": "RAWM(Ours)",
          "Podcast → IEMOCAP and are evaluated using the eval-": "41.995"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 5: The EER(%) of few samples experiments. All Table 6: The Acc(%) of various continual learning meth-",
      "data": [
        {
          "RAWM(Ours)\n0.923\n0.312": "5.6. Comparison of our method with other methods",
          "RAWM(Ours)\n41.995\n54.229": ""
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "",
          "RAWM(Ours)\n41.995\n54.229": "trained on the S datasets and then fine-tuned or continually"
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "Sequence training between two datasets: We compare our",
          "RAWM(Ours)\n41.995\n54.229": "learned on the T1 dataset. All models are trained on the"
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "method with several methods in Table 4. The EWC, LwF,",
          "RAWM(Ours)\n41.995\n54.229": "new dataset within five steps.\nFrom the results, we can"
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "and OWM as three mainstream continual learning methods",
          "RAWM(Ours)\n41.995\n54.229": "observe that our method RAWM also achieves\nthe best"
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "achieve great success in many fields. The DFWF is the first",
          "RAWM(Ours)\n41.995\n54.229": "performance on both old and new datasets and the learning"
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "continual learning method to overcome forgetting for fake",
          "RAWM(Ours)\n41.995\n54.229": "performance is very close to the result of Train-on-All which"
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "audio detection. The results demonstrate that fine-tuning",
          "RAWM(Ours)\n41.995\n54.229": "is the the lower bound to all continual learning methods. By"
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "without modification (Fine-tune) forgets previous knowl-",
          "RAWM(Ours)\n41.995\n54.229": "comparing the results in Table 4a and Table 5, we can easily"
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "edge obviously. The forgetting of RAWM is one-tenth that",
          "RAWM(Ours)\n41.995\n54.229": "find that reducing the number of samples has only a little"
        },
        {
          "RAWM(Ours)\n0.923\n0.312": "of Fine-tune on Table 4a and the EER on the new dataset",
          "RAWM(Ours)\n41.995\n54.229": "damage to our method."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": ""
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": ""
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": "Exp2"
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": "93.64"
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": "90.00"
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": "84.95"
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": "88.89"
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": "91.62"
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": "91.72"
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": "92.12"
        },
        {
          "Table 7: The Accuracy(%) on the CLEAR experiences.": "92.53"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "classes such as camera, baseball,\nlaptop, etc. To evaluate",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "scenarios of sequence training on both two and multiple"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "the effectiveness of our method, we selected several widely",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "datasets. The result shows that our method still achieves the"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "used continual\nlearning algorithms,\nincluding the Replay,",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "best performance among the above methods when training"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "EWC, LwF, GDumbFinetune (GDF) (Prabhu et al., 2020),",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "on a few samples. Besides, our method is inspired by fake"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "CopyWeights with Re-init (CWR) (Lomonaco & Maltoni,",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "audio detection and the results show that it can be easily gen-"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "2017), and OWM methods. Table 7 presents the results of",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "eralized to other fields, like speech emotion recognition. In"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "the comparative analysis. We treated the Replay method,",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "addition, our method does not require previous data; thus it"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "which corresponds to the ”Train-on-all” approach in our",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "can be applied to most classification networks. We have yet"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "paper, as the upper bound of accuracy for all continual",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "to study how to make the model learn the weight direction"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "learning methods. The EWC and LwF methods have been",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "gradually in the process of training on new datasets without"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "introduced in our paper. In the GDF algorithm, we set the",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "any constraint, and exploring generalization to related tasks"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "memory size to be the same as the number of training data",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "will form the focus of our future studies."
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "in one bucket, and for CWR, the cwr layer was positioned",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": ""
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "as the final layer of the model. To extract features, we em-",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": ""
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "Acknowledgements"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "ployed a pre-trained ResNet-50 (He et al., 2016) as a feature",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": ""
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "extractor, producing 2048-dimensional feature vectors. A",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "This\nwork\nis\nsupported\nby\nthe\nNational\nKey\nRe-"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "linear layer with input and output dimensions of 2048 and",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "search\nand\nDevelopment\nProgram\nof\nChina\nunder"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "11, respectively, was used as the downstream classifier. Our",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "Grant No.2020AAA0140003,\nthe National Natural Sci-"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "experiments were conducted with a batch size of 512, an",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "ence\nFoundation\nof\nChina\n(NSFC)\n(No.61831022,"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "initial\nlearning rate of 1 (decayed by a factor of 0.1 after",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "No.U21B2010, No.62101553, No.61971419, No.62006223,"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "60 epochs), and the SGD optimizer with a momentum of",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "No.62276259, No.62201572, No.62206278), Beijing Mu-"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "0.9. The experimental results in Table 7 demonstrate that",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "nicipal Science\nand Technology Commission, Admin-"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "our proposed method, referred to as RAWM, consistently",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "istrative Commission\nof\nZhongguancun\nScience\nPark"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "achieved the best performance across most tasks. In partic-",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "No.Z211100004821013, Open Research Projects of Zhe-"
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "ular, in Exp3 and Exp8, the performance of our method",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "jiang Lab (NO.2021KH0AB06)."
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "closely approached the highest accuracy achieved among",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": ""
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "all the evaluated methods.",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": ""
        },
        {
          "92.12\n92.53\n91.41\nRAWM (Ours)": "",
          "93.74\n91.82\n92.42\n92.53\n92.22\n92.53\n95.25": "References"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Baevski, A.,\nZhou, Y., Mohamed, A.,\nand Auli, M."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "wav2vec 2.0: A framework for self-supervised learning"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "of speech representations.\nIn Larochelle, H., Ranzato,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "M., Hadsell, R., Balcan, M.,\nand Lin, H.\n(eds.), Ad-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "vances in Neural\nInformation Processing Systems 33:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Annual Conference on Neural\nInformation Processing"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Systems 2020, NeurIPS 2020, December 6-12, 2020,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "https://proceedings.\nvirtual,\n2020.\nURL"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "neurips.cc/paper/2020/hash/"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "html."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Ben-Israel, A. and Greville, T. N. Generalized inverses:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "theory and applications, volume 15. Springer Science &"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Business Media, 2003."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Bengio, Y. and LeCun, Y.\nScaling learning algorithms"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "towards AI.\nIn Large Scale Kernel Machines. MIT Press,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "2007."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Castro, F. M., Mar´ın-Jim´enez, M. J., Guil, N., Schmid, C.,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "and Alahari, K.\nEnd-to-end incremental\nlearning.\nIn"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Proceedings of\nthe European conference on computer"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "vision (ECCV), pp. 233–248, 2018."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Cermelli, F., Mancini, M., Bulo, S. R., Ricci, E., and Caputo,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "B. Modeling the background for incremental learning in"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "semantic segmentation.\nIn Proceedings of the IEEE/CVF"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Conference on Computer Vision and Pattern Recognition,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "pp. 9233–9242, 2020."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Chen, Y., Diethe, T., and Lawrence, N. Facilitating bayesian"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "continual learning by natural gradients and stein gradients."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "arXiv preprint arXiv:1904.10644, 2019."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Conneau, A., Baevski, A., Collobert, R., Mohamed, A.,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "and Auli, M.\nUnsupervised cross-lingual\nrepresenta-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "arXiv preprint\ntion learning for\nspeech recognition."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "arXiv:2006.13979, 2020."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "de Masson D’Autume, C., Ruder, S., Kong, L., and Yo-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "gatama, D. Episodic memory in lifelong language learn-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "ing. Advances in Neural Information Processing Systems,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "32, 2019."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Haykin, S. S. Adaptive filter theory.\nPearson Education"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "India, 2002."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "the IEEE\ning for image recognition.\nIn Proceedings of"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "conference on computer vision and pattern recognition,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "pp. 770–778, 2016."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Husz´ar, F. On quadratic penalties in elastic weight con-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "solidation. CoRR, abs/1712.03847, 2017. URL http:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "//arxiv.org/abs/1712.03847."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Ma, H., Yi, J., Tao, J., Bai, Y., Tian, Z., and Wang, C. Con-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "tinual learning for fake audio detection. arXiv preprint"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "arXiv:2104.07286, 2021."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Ma, H., Yi, J., Wang, C., Yan, X., Tao, J., Wang, T., Wang,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "S., Xu, L., and Fu, R. FAD: A chinese dataset for fake"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "audio detection. CoRR, abs/2207.12308, 2022. doi: 10."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "48550/arXiv.2207.12308. URL https://doi.org/"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "10.48550/arXiv.2207.12308."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Mallya, A. and Lazebnik, S. Packnet: Adding multiple tasks"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "to a single network by iterative pruning.\nIn Proceedings"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "of the IEEE conference on Computer Vision and Pattern"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Recognition, pp. 7765–7773, 2018."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Mart´ın-Do˜nas, J. M. and ´Alvarez, A. The vicomtech au-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "dio deepfake detection system based on wav2vec2 for"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "the 2022 add challenge.\nIn ICASSP 2022-2022 IEEE In-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "ternational Conference on Acoustics, Speech and Signal"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Processing (ICASSP), pp. 9241–9245. IEEE, 2022."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Monteiro, J., Alam, M. J., and Falk, T. H. An ensemble"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "based approach for generalized detection of spoofing at-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "tacks to automatic speaker recognizers.\nIn 2020 IEEE"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "International Conference on Acoustics, Speech and Sig-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "nal Processing,\nICASSP 2020, Barcelona, Spain, May"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "4-8, 2020, pp. 6599–6603. IEEE, 2020.\ndoi: 10.1109/"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "ICASSP40776.2020.9054558.\nURL https://doi."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "org/10.1109/ICASSP40776.2020.9054558."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "M¨uller, N. M., Czempin, P., Dieckmann, F., Froghyar, A.,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "and B¨ottinger, K. Does audio deepfake detection general-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "ize? arXiv preprint arXiv:2203.16263, 2022."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Nautsch, A., Wang, X., Evans, N. W. D., Kinnunen, T. H.,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Vestman, V., Todisco, M., Delgado, H., Sahidullah,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "M., Yamagishi,\nJ.,\nand Lee, K. A.\nAsvspoof 2019:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Spoofing countermeasures for\nthe detection of synthe-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "IEEE Trans.\nsized,\nconverted and replayed speech."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Biom. Behav.\nIdentity Sci., 3(2):252–265, 2021.\ndoi:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "10.1109/TBIOM.2021.3059479. URL https://doi."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "org/10.1109/TBIOM.2021.3059479."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., and Wermter,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "S. Continual lifelong learning with neural networks: A"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "review. Neural Networks, 113:54–71, 2019. doi: 10.1016/"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "j.neunet.2019.01.012. URL https://doi.org/10."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "1016/j.neunet.2019.01.012."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Perez-Rua, J.-M., Zhu, X., Hospedales, T. M., and Xiang,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "T.\nIncremental few-shot object detection.\nIn Proceedings"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "of\nthe IEEE/CVF Conference on Computer Vision and"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Pattern Recognition, pp. 13846–13855, 2020."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Prabhu, A., Torr, P. H. S., and Dokania, P. K.\nGdumb:"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "A simple approach that questions our progress in con-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "tinual\nlearning.\nIn Vedaldi, A., Bischof, H., Brox, T.,"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Virtual Event, Shanghai, China, 25-29 October 2020,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "pp. 1086–1090. ISCA, 2020. doi: 10.21437/Interspeech."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "2020-1255. URL https://doi.org/10.21437/"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Interspeech.2020-1255."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Wang, T., Fu, R., Yi, J., Tao, J., Wen, Z., Qiang, C., and"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Wang, S. Prosody and voice factorization for few-shot"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "speaker adaptation in the challenge m2voc 2021. ICASSP"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "2021 - 2021 IEEE International Conference on Acoustics,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Speech and Signal Processing (ICASSP), pp. 8603–8607,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "2021."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Wang, X. and Yamagishi, J.\nInvestigating self-supervised"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "front ends for speech spoofing countermeasures. arXiv"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "preprint arXiv:2111.07725, 2021."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Wang, Y., Stanton, D., Zhang, Y., Skerry-Ryan, R. J., Batten-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "berg, E., Shor, J., Xiao, Y., Ren, F., Jia, Y., and Saurous,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "R. A. Style tokens: Unsupervised style modeling, control"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "and transfer in end-to-end speech synthesis.\nIn Interna-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "tional Conference on Machine Learning, 2018."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Wu, Y., Chen, Y., Wang, L., Ye, Y., Liu, Z., Guo, Y., and"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Fu, Y. Large scale incremental learning.\nIn Proceedings"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "of\nthe IEEE/CVF Conference on Computer Vision and"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Pattern Recognition, pp. 374–382, 2019."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Wu, Z., Kinnunen, T., Evans, N., Yamagishi, J., Hanilc¸i, C.,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Sahidullah, M., and Sizov, A. Asvspoof 2015:\nthe first"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "automatic speaker verification spoofing and countermea-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "the\nsures challenge.\nIn Sixteenth annual conference of"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": ""
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "international speech communication association, 2015."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Yamagishi,\nJ., Wang, X., Todisco, M., Sahidullah, M.,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Patino, J., Nautsch, A., Liu, X., Lee, K. A., Kinnunen, T.,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Evans, N., et al. Asvspoof 2021: accelerating progress in"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "spoofed and deepfake speech detection. arXiv preprint"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "arXiv:2109.00537, 2021."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Yan, X., Yi,\nJ.,\nTao,\nJ., Wang, C., Ma, H., Wang,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "T., Wang,\nS.,\nand\nFu,\nR.\nAn\ninitial\ninvestiga-"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "tion for detecting vocoder fingerprints of\nfake audio."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "CoRR,\nabs/2208.09646,\n2022.\ndoi:\n10.48550/arXiv."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "2208.09646. URL https://doi.org/10.48550/"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "arXiv.2208.09646."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Yi, J., Fu, R., Tao, J., Nie, S., Ma, H., Wang, C., Wang,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "T., Tian, Z., Bai, Y., Fan, C., et al. Add 2022:\nthe first"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "audio deep synthesis detection challenge.\nIn ICASSP"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "2022-2022 IEEE International Conference on Acoustics,"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Speech and Signal Processing (ICASSP), pp. 9216–9220."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "IEEE, 2022."
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "Yoon,\nJ., Yang, E., Lee,\nJ., and Hwang, S. J.\nLifelong"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "learning with dynamically expandable networks. arXiv"
        },
        {
          "Do You Remember? Overcoming Catastrophic Forgetting for Fake Audio Detection": "preprint arXiv:1708.01547, 2017."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Evaluation Sets": ""
        },
        {
          "Evaluation Sets": "T2"
        },
        {
          "Evaluation Sets": "7.670"
        },
        {
          "Evaluation Sets": "10.000"
        },
        {
          "Evaluation Sets": "26.165"
        },
        {
          "Evaluation Sets": "46.503"
        },
        {
          "Evaluation Sets": "44.741"
        },
        {
          "Evaluation Sets": "49.726"
        }
      ],
      "page": 13
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Memory aware synapses: Learning what (not) to forget",
      "authors": [
        "R Aljundi",
        "F Babiloni",
        "M Elhoseiny",
        "M Rohrbach",
        "T Tuytelaars"
      ],
      "year": "2018",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV)"
    },
    {
      "citation_id": "2",
      "title": "Taskfree continual learning",
      "authors": [
        "R Aljundi",
        "K Kelchtermans",
        "T Tuytelaars"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "3",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020"
    },
    {
      "citation_id": "4",
      "title": "Generalized inverses: theory and applications",
      "authors": [
        "A Ben-Israel",
        "T Greville"
      ],
      "year": "2003",
      "venue": "Generalized inverses: theory and applications"
    },
    {
      "citation_id": "5",
      "title": "Scaling learning algorithms towards AI",
      "authors": [
        "Y Bengio",
        "Y Lecun"
      ],
      "year": "2007",
      "venue": "Large Scale Kernel Machines"
    },
    {
      "citation_id": "6",
      "title": "End-to-end incremental learning",
      "authors": [
        "F Castro",
        "M Marín-Jiménez",
        "N Guil",
        "C Schmid",
        "K Alahari"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "7",
      "title": "Modeling the background for incremental learning in semantic segmentation",
      "authors": [
        "F Cermelli",
        "M Mancini",
        "S Bulo",
        "E Ricci",
        "B Caputo"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "8",
      "title": "Facilitating bayesian continual learning by natural gradients and stein gradients",
      "authors": [
        "Y Chen",
        "T Diethe",
        "N Lawrence"
      ],
      "year": "2019",
      "venue": "Facilitating bayesian continual learning by natural gradients and stein gradients",
      "arxiv": "arXiv:1904.10644"
    },
    {
      "citation_id": "9",
      "title": "Unsupervised cross-lingual representation learning for speech recognition",
      "authors": [
        "A Conneau",
        "A Baevski",
        "R Collobert",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Unsupervised cross-lingual representation learning for speech recognition",
      "arxiv": "arXiv:2006.13979"
    },
    {
      "citation_id": "10",
      "title": "Episodic memory in lifelong language learning",
      "authors": [
        "C De Masson D'autume",
        "S Ruder",
        "L Kong",
        "D Yogatama"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "11",
      "title": "Adaptive filter theory",
      "authors": [
        "S Haykin"
      ],
      "year": "2002",
      "venue": "Adaptive filter theory"
    },
    {
      "citation_id": "12",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "13",
      "title": "On quadratic penalties in elastic weight consolidation",
      "authors": [
        "F Huszár"
      ],
      "year": "2017",
      "venue": "On quadratic penalties in elastic weight consolidation"
    },
    {
      "citation_id": "14",
      "title": "The asvspoof 2017 challenge: Assessing the limits of replay spoofing attack detection",
      "authors": [
        "T Kinnunen",
        "M Sahidullah",
        "H Delgado",
        "M Todisco",
        "N Evans",
        "J Yamagishi",
        "K Lee"
      ],
      "year": "2017",
      "venue": "Interspeech 2017, 18th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "15",
      "title": "Overcoming catastrophic forgetting in neural networks",
      "authors": [
        "J Kirkpatrick",
        "R Pascanu",
        "N Rabinowitz",
        "J Veness",
        "G Desjardins",
        "A Rusu",
        "K Milan",
        "J Quan",
        "T Ramalho",
        "A Grabska-Barwinska"
      ],
      "year": "2017",
      "venue": "Proceedings of the national academy of sciences"
    },
    {
      "citation_id": "16",
      "title": "Crafting papers on machine learning",
      "authors": [
        "P Langley"
      ],
      "year": "2000",
      "venue": "Proceedings of the 17th International Conference on Machine Learning (ICML 2000)"
    },
    {
      "citation_id": "17",
      "title": "Overcoming catastrophic forgetting with unlabeled data in the wild",
      "authors": [
        "K Lee",
        "K Lee",
        "J Shin",
        "H Lee"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision"
    },
    {
      "citation_id": "18",
      "title": "Learning without forgetting",
      "authors": [
        "Z Li",
        "D Hoiem"
      ],
      "year": "2017",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "19",
      "title": "The clear benchmark: Continual learning on real-world imagery",
      "authors": [
        "Z Lin",
        "J Shi",
        "D Pathak",
        "D Ramanan"
      ],
      "year": "2021",
      "venue": "Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track"
    },
    {
      "citation_id": "20",
      "title": "Continual learning for sentence representations using conceptors",
      "authors": [
        "T Liu",
        "L Ungar",
        "J Sedoc"
      ],
      "year": "2019",
      "venue": "Continual learning for sentence representations using conceptors",
      "arxiv": "arXiv:1904.09187"
    },
    {
      "citation_id": "21",
      "title": "Core50: a new dataset and benchmark for continuous object recognition",
      "authors": [
        "V Lomonaco",
        "D Maltoni"
      ],
      "year": "2017",
      "venue": "1st Annual Conference on Robot Learning"
    },
    {
      "citation_id": "22",
      "title": "Gradient episodic memory for continual learning",
      "authors": [
        "D Lopez-Paz",
        "M Ranzato"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "23",
      "title": "Fake audio detection based on unsupervised pretraining models",
      "authors": [
        "Z Lv",
        "S Zhang",
        "K Tang",
        "P Hu"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "24",
      "title": "Continual learning for fake audio detection",
      "authors": [
        "H Ma",
        "J Yi",
        "J Tao",
        "Y Bai",
        "Z Tian",
        "C Wang"
      ],
      "year": "2021",
      "venue": "Continual learning for fake audio detection",
      "arxiv": "arXiv:2104.07286"
    },
    {
      "citation_id": "25",
      "title": "FAD: A chinese dataset for fake audio detection",
      "authors": [
        "H Ma",
        "J Yi",
        "C Wang",
        "X Yan",
        "J Tao",
        "T Wang",
        "S Wang",
        "L Xu",
        "R Fu"
      ],
      "venue": "FAD: A chinese dataset for fake audio detection",
      "doi": "10.48550/arXiv.2207.12308"
    },
    {
      "citation_id": "26",
      "title": "Adding multiple tasks to a single network by iterative pruning",
      "authors": [
        "A Mallya",
        "S Lazebnik",
        "Packnet"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "27",
      "title": "The vicomtech audio deepfake detection system based on wav2vec2 for the 2022 add challenge",
      "authors": [
        "J Martín-Doñas",
        "A Álvarez"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "28",
      "title": "An ensemble based approach for generalized detection of spoofing attacks to automatic speaker recognizers",
      "authors": [
        "J Monteiro",
        "M Alam",
        "T Falk"
      ],
      "year": "2020",
      "venue": "2020 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP40776.2020.9054558"
    },
    {
      "citation_id": "29",
      "title": "Does audio deepfake detection generalize?",
      "authors": [
        "N Müller",
        "P Czempin",
        "F Dieckmann",
        "A Froghyar",
        "K Böttinger"
      ],
      "year": "2022",
      "venue": "Does audio deepfake detection generalize?",
      "arxiv": "arXiv:2203.16263"
    },
    {
      "citation_id": "30",
      "title": "Spoofing countermeasures for the detection of synthesized, converted and replayed speech",
      "authors": [
        "A Nautsch",
        "X Wang",
        "N Evans",
        "T Kinnunen",
        "V Vestman",
        "M Todisco",
        "H Delgado",
        "M Sahidullah",
        "J Yamagishi",
        "K Lee",
        "Asvspoof"
      ],
      "year": "2019",
      "venue": "IEEE Trans. Biom. Behav. Identity Sci",
      "doi": "10.1109/TBIOM.2021.3059479"
    },
    {
      "citation_id": "31",
      "title": "Continual lifelong learning with neural networks: A review",
      "authors": [
        "G Parisi",
        "R Kemker",
        "J Part",
        "C Kanan",
        "S Wermter"
      ],
      "year": "2019",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2019.01.012"
    },
    {
      "citation_id": "32",
      "title": "Incremental few-shot object detection",
      "authors": [
        "J.-M Perez-Rua",
        "X Zhu",
        "T Hospedales",
        "T Xiang"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "33",
      "title": "A simple approach that questions our progress in continual learning",
      "authors": [
        "A Prabhu",
        "P Torr",
        "P Dokania",
        "Gdumb"
      ],
      "year": "2020",
      "venue": "Computer Vision -ECCV 2020 -16th European Conference",
      "doi": "10.1007/978-3-030-58536-5_31"
    },
    {
      "citation_id": "34",
      "title": "Progressive neural networks",
      "authors": [
        "A Rusu",
        "N Rabinowitz",
        "G Desjardins",
        "H Soyer",
        "J Kirkpatrick",
        "K Kavukcuoglu",
        "R Pascanu",
        "R Hadsell"
      ],
      "year": "2016",
      "venue": "Progressive neural networks",
      "arxiv": "arXiv:1606.04671"
    },
    {
      "citation_id": "35",
      "title": "Progress & compress: A scalable framework for continual learning",
      "authors": [
        "J Schwarz",
        "W Czarnecki",
        "J Luketina",
        "A Grabska-Barwinska",
        "Y Teh",
        "R Pascanu",
        "R Hadsell"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "36",
      "title": "Overcoming catastrophic forgetting with hard attention to the task",
      "authors": [
        "J Serra",
        "D Suris",
        "M Miron",
        "A Karatzoglou"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "37",
      "title": "Optimal filtering algorithms for fast learning in feedforward neural networks",
      "authors": [
        "S Shah",
        "F Palmieri",
        "M Datum"
      ],
      "year": "1992",
      "venue": "Neural networks"
    },
    {
      "citation_id": "38",
      "title": "Multi-lingual multi-task speech emotion recognition using wav2vec 2.0",
      "authors": [
        "M Sharma"
      ],
      "year": "2022",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP43922.2022.9747417"
    },
    {
      "citation_id": "39",
      "title": "Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation",
      "authors": [
        "H Tak",
        "M Todisco",
        "X Wang",
        "J.-W Jung",
        "J Yamagishi",
        "N Evans"
      ],
      "year": "2022",
      "venue": "Automatic speaker verification spoofing and deepfake detection using wav2vec 2.0 and data augmentation",
      "arxiv": "arXiv:2202.12233"
    },
    {
      "citation_id": "40",
      "title": "Future horizons in spoofed and fake audio detection",
      "authors": [
        "M Todisco",
        "X Wang",
        "V Vestman",
        "M Sahidullah",
        "H Delgado",
        "A Nautsch",
        "J Yamagishi",
        "N Evans",
        "T Kinnunen",
        "K Lee",
        "Asvspoof"
      ],
      "year": "2019",
      "venue": "Future horizons in spoofed and fake audio detection",
      "arxiv": "arXiv:1904.05441"
    },
    {
      "citation_id": "41",
      "title": "Natural variational continual learning",
      "authors": [
        "H Tseran",
        "M Khan",
        "T Harada",
        "T Bui"
      ],
      "year": "2018",
      "venue": "Continual Learning Workshop@ NeurIPS"
    },
    {
      "citation_id": "42",
      "title": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit",
      "authors": [
        "C Veaux",
        "J Yamagishi",
        "K Macdonald"
      ],
      "year": "2017",
      "venue": "Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit"
    },
    {
      "citation_id": "43",
      "title": "Dual-adversarial domain adaptation for generalized replay attack detection",
      "authors": [
        "H Wang",
        "H Dinkel",
        "S Wang",
        "Y Qian",
        "K Yu"
      ],
      "year": "2020",
      "venue": "Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event",
      "doi": "10.21437/Interspeech.2020-1255"
    },
    {
      "citation_id": "44",
      "title": "Prosody and voice factorization for few-shot speaker adaptation in the challenge m2voc 2021",
      "authors": [
        "T Wang",
        "R Fu",
        "J Yi",
        "J Tao",
        "Z Wen",
        "C Qiang",
        "S Wang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "45",
      "title": "Investigating self-supervised front ends for speech spoofing countermeasures",
      "authors": [
        "X Wang",
        "J Yamagishi"
      ],
      "year": "2021",
      "venue": "Investigating self-supervised front ends for speech spoofing countermeasures",
      "arxiv": "arXiv:2111.07725"
    },
    {
      "citation_id": "46",
      "title": "Style tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis",
      "authors": [
        "Y Wang",
        "D Stanton",
        "Y Zhang",
        "R Skerry-Ryan",
        "E Battenberg",
        "J Shor",
        "Y Xiao",
        "F Ren",
        "Y Jia",
        "R Saurous"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "47",
      "title": "Large scale incremental learning",
      "authors": [
        "Y Wu",
        "Y Chen",
        "L Wang",
        "Y Ye",
        "Z Liu",
        "Y Guo",
        "Y Fu"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "Asvspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge",
      "authors": [
        "Z Wu",
        "T Kinnunen",
        "N Evans",
        "J Yamagishi",
        "C Hanilc ¸i",
        "M Sahidullah",
        "A Sizov"
      ],
      "year": "2015",
      "venue": "Sixteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "49",
      "title": "Asvspoof 2021: accelerating progress in spoofed and deepfake speech detection",
      "authors": [
        "J Yamagishi",
        "X Wang",
        "M Todisco",
        "M Sahidullah",
        "J Patino",
        "A Nautsch",
        "X Liu",
        "K Lee",
        "T Kinnunen",
        "N Evans"
      ],
      "year": "2021",
      "venue": "Asvspoof 2021: accelerating progress in spoofed and deepfake speech detection",
      "arxiv": "arXiv:2109.00537"
    },
    {
      "citation_id": "50",
      "title": "An initial investigation for detecting vocoder fingerprints of fake audio",
      "authors": [
        "X Yan",
        "J Yi",
        "J Tao",
        "C Wang",
        "H Ma",
        "T Wang",
        "S Wang",
        "R Fu"
      ],
      "venue": "An initial investigation for detecting vocoder fingerprints of fake audio",
      "doi": "10.48550/arXiv.2208.09646"
    },
    {
      "citation_id": "51",
      "title": "Add 2022: the first audio deep synthesis detection challenge",
      "authors": [
        "J Yi",
        "R Fu",
        "J Tao",
        "S Nie",
        "H Ma",
        "C Wang",
        "T Wang",
        "Z Tian",
        "Y Bai",
        "C Fan"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "52",
      "title": "Lifelong learning with dynamically expandable networks",
      "authors": [
        "J Yoon",
        "E Yang",
        "J Lee",
        "S Hwang"
      ],
      "year": "2017",
      "venue": "Lifelong learning with dynamically expandable networks",
      "arxiv": "arXiv:1708.01547"
    },
    {
      "citation_id": "53",
      "title": "Continual learning of context-dependent processing in neural networks",
      "authors": [
        "G Zeng",
        "Y Chen",
        "B Cui",
        "S Yu"
      ],
      "year": "2019",
      "venue": "Nature Machine Intelligence"
    },
    {
      "citation_id": "54",
      "title": "Continual learning through synaptic intelligence",
      "authors": [
        "F Zenke",
        "B Poole",
        "S Ganguli"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning, ICML 2017"
    },
    {
      "citation_id": "55",
      "title": "Continual learning through synaptic intelligence",
      "authors": [
        "F Zenke",
        "B Poole",
        "S Ganguli"
      ],
      "year": "2017",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "56",
      "title": "One-class learning towards synthetic voice spoofing detection",
      "authors": [
        "Y Zhang",
        "F Jiang",
        "Z Duan"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "57",
      "title": "An empirical study on channel effects for synthetic voice spoofing countermeasure systems",
      "authors": [
        "Y Zhang",
        "G Zhu",
        "F Jiang",
        "Z Duan"
      ],
      "year": "2021",
      "venue": "An empirical study on channel effects for synthetic voice spoofing countermeasure systems",
      "arxiv": "arXiv:2104.01320"
    },
    {
      "citation_id": "58",
      "title": "Voice conversion challenge 2020: Intra-lingual semi-parallel and cross-lingual voice conversion",
      "authors": [
        "Y Zhao",
        "W.-C Huang",
        "X Tian",
        "J Yamagishi",
        "R Das",
        "T Kinnunen",
        "Z Ling",
        "T Toda"
      ],
      "year": "2020",
      "venue": "Voice conversion challenge 2020: Intra-lingual semi-parallel and cross-lingual voice conversion",
      "arxiv": "arXiv:2008.12527"
    }
  ]
}