{
  "paper_id": "2308.14245v1",
  "title": "A Comparison Of Personalized And Generalized Approaches To Emotion Recognition Using Consumer Wearable Devices: Machine Learning Study",
  "published": "2023-08-28T01:21:22Z",
  "authors": [
    "Joe Li",
    "Peter Washington"
  ],
  "keywords": [
    "deep learning",
    "machine learning",
    "emotion recognition",
    "wearable technology",
    "affective computing",
    "stress detection",
    "neural network",
    "affect detection",
    "digital health"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Background: Studies have shown the potential adverse health effects, ranging from headaches to cardiovascular disease, associated with long-term negative emotions and chronic stress. Since many indicators of stress are imperceptible to observers, the early detection and intervention of stress remains a pressing medical need. Physiological signals offer a non-invasive method of monitoring emotions and are easily collected by smartwatches. Existing research primarily focuses on developing generalized machine learning-based models for emotion classification. Objective: We aim to study the differences between personalized and generalized machine learning models for three-class emotion classification (neutral, stress, and amusement) using wearable biosignal data. Methods: We developed a convolutional encoder for the three-class emotion classification problem using data from WESAD, a multimodal dataset with physiological signals for 15 subjects. We compared the results between a subject-exclusive generalized, subjectinclusive generalized, and personalized model. Results: For the three-class classification problem, our personalized model achieved an average accuracy of 95.06% and F1-score of 91.71, our subject-inclusive generalized model achieved an average accuracy of 66.95% and F1-score of 42.50, and our subject-exclusive generalized model achieved an average accuracy of 67.65% and F1-score of 43.05. Conclusions: Our results emphasize the need for increased research in personalized emotion recognition models given that they outperform generalized models in certain contexts. We also demonstrate that personalized machine learning models for emotion classification are viable and can achieve high performance.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Stress and negative affect can have long-term consequences on physical and mental health, such as chronic illness, higher mortality rates, and major depression  [1] [2] [3] . Therefore, the early detection and corresponding intervention of stress and negative emotions greatly reduces the risk of detrimental health conditions appearing later in life  [4] . Since negative stress and affect can be difficult for humans to observe  [10] [11] [12] , automated emotion recognition models can play an important role in healthcare. Affective computing can also facilitate digital therapy and advance the development of assistive technologies for autism  [5, 6, [27] [28] [29] [30] .\n\nPhysiological signals, including electrocardiography (ECG), electrodermal activity (EDA), and photoplethysmography (PPG), have been shown to be robust indicators of emotions  [7] [8] [9] . The non-invasive nature of physiological signal measurement makes it a practical and convenient method for emotion recognition. Wearable devices such as smartwatches have become increasingly popular, and products such as Fitbit have already integrated the continuous sensing of heart rate, ECG, and EDA data into their smartwatches. The accessibility of wearable devices indicates that an emotion recognition model based on physiological data can have practical applications in healthcare.\n\nThe vast majority of research in recognizing emotions from physiological data involves machine learning models that are generalizable, which means that models were trained on one group of subjects and tested on a separate group of subjects  [13, 15, [20] [21] [22] [23] [41] [42] [43] [44] [45] [46] . However, generalized models require large amounts of annotated data to train, and individuals inherently experience and react to affective stimuli differently. This creates inter-subject data variance, which degrades model accuracy  [16] . Studies emphasize the need for personalized or subject-dependent models  [17, 25] , but it is still unclear how their performance compares against generalized models.\n\nWe present a personalized machine learning model for the three-class emotion classification problem (neutral, stress, and amusement) on the Wearable Stress and Affect Dataset (WESAD), which is the only publicly available dataset that includes both stress and emotion data  [15] . To the best of our knowledge, there has been no study comparing the performance of a generalized model to a personalized model for this task. Therefore, we compare the performances of a personalized and a generalized model. We establish two generalized model baselines: one is subject-inclusive and the other subject-exclusive. Ultimately, we conclude that a personalized machine learning approach outperforms a generalized approach.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Overview",
      "text": "To classify physiological data into the neutral, stress, and amusement classes, we developed a machine learning framework and evaluated the framework using data from the Wearable Stress and Affect Dataset (WESAD). Our machine learning pipeline consists of data pre-processing, a convolutional encoder for feature extraction, and a feed-forward neural network for supervised prediction (Figure  1 ). Using this model architecture, we compared generalized and personalized approaches to the three-class emotion classification task (neutral, stress, and amusement).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Dataset",
      "text": "We selected the Wearable Stress and Affect Dataset (WESAD) because, to the best of our knowledge, it is the only publicly available dataset that combines both stress and emotion annotations. WESAD consists of multimodal physiological data in the form of continuous time-series data for 15 subjects and corresponding annotations of four affective states: neutral, stress, amusement, and meditation. However, we only considered the neutral, stress, and amusement classes since the objective of WESAD is to provide data for the three-class classification problem, and the benchmark model in WESAD ignores the meditation state as well. Our model incorporated data from eight modalities recorded in WESAD: ECG, EDA, EMG, RESP, TEMP, and ACC (x, y, and z axes). Measurements for each of the eight modalities were sampled at 700 Hz to enforce uniformity, and data were collected for approximately 36 minutes per subject.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Pre-Processing And Partitioning",
      "text": "Each data modality was normalized with a mean of zero and a standard deviation of 1. We used a sliding window algorithm to partition each modality into intervals consisting of 64 data points, with a 50% overlap between consecutive intervals. We ensured that all 64 data points within an interval shared a common annotation, which allowed us to assign a single affective state to each interval. The process of normalization, followed by a sliding window partition, is illustrated in Figure  1 . Then, these intervals were partitioned into training, validation, and testing sets.\n\nFor the personalized model, we partitioned the training, validation, and testing sets as follows. Each subject in the dataset had their own model, trained, validated, and tested independently of other subjects. For each affective state (neutral, stress, and amusement), we allocated the initial 70% of intervals with that affective state for training, the next 15% for validation, and the final 15% for testing. This guaranteed that the relative frequencies of each affective state were consistent across all three sets. Simply using the first 70% of all intervals for the training data would skew the distribution of affective states given the nature of the WESAD dataset. Furthermore, our partitioning of intervals according to sequential time order, rather than random selection, helped prevent overfitting by guaranteeing that two adjacent intervals with similar features would be in the same set. The partitioning of training, validation, and testing sets for the personalized model is shown in Figure  2 .\n\nStandard generalized models partition the training, validation, and testing sets by subject  [15] . We denote these standard models as subject-exclusive generalized models, as shown in Figure  2 . Through this partitioning method, it is impossible to compare the performances of generalized and personalized models since they are solving two separate tasks. Therefore, we present a modified subject-exclusive generalized model that solves the same task as the personalized model. The testing set for our subject-exclusive generalized model consisted of the last 15% of intervals for each affective state for one subject. The training set consisted of the first 70% of intervals for each affective state for all subjects except the one subject in the testing set, and the validation set consisted of the next 15% of intervals for all subjects except the one subject in the testing set. The training and testing sets for this approach contained data from mutually exclusive sets of subjects-this is where the name of the model, subject-exclusive, is derived from. Since the testing sets for the subject-exclusive generalized and personalized model are equivalent, it is possible to compare generalized and personalized approaches. This subject-exclusive generalized model served as our first generalized model baseline.\n\nA second generalized model baseline was created, called the subject-inclusive generalized model. Like the testing sets for the subject-exclusive generalized and personalized models, the testing set for this model contained the last 15% of intervals for each affective state for a single subject. The training set consisted of the first 70% of intervals for each affective state for all subjects, and the validation set consisted of the next 15%. The set of subjects in the training and testing sets overlapped by one subject-the subject in the testing set-which is why this model is called the subject-inclusive generalized model. This is illustrated in Figure  2 .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Model Architecture",
      "text": "The model architecture consisted of an encoder network followed by a feed forward head, which is shown in Figure  1 . Eight channels, representing the eight modalities we used from WESAD, served as input into an encoder network, which was modeled after the encoder section of U-Net  [18] . The encoder network had three blocks, with each block consisting of two one-dimensional convolutional layers (kernel size of three) followed by a onedimensional max pooling (kernel size of two). The output of each convolution operation was passed through a Sigmoid Linear Unit (SiLU) activation function. Between each block, we doubled the number of channels and added a dropout layer (15%) to prevent overfitting. The output of the encoder was flattened and passed through two fully connected layers with SiLU activation to produce a three-class probability distribution. Table  1  shows the hyperparameters that determine the model structure. These were consistent between the subject-exclusive generalized, subject-inclusive generalized, and personalized models.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Model Training",
      "text": "We trained the two generalized baseline models and the personalized model under the same hyperparameters to guarantee a fair comparison. Both models were trained with cross-entropy loss using the AdamW optimization. All models were written using PyTorch  [31] . Within 1000 epochs, models with the lowest validation loss were saved for testing. A Nvidia GeForce RTX 4090 GPU was used for training. A separate personalized model was trained for each of the 15 subjects. The subject-exclusive generalized model was trained 15 times, and the subject-inclusive generalized model was trained once. For model comparison, all models were tested on each of the 15 subjects.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "For the three-class emotion classification task (neutral, stress, and amusement), Figure  3  and Figure  4  illustrate the accuracy and F1-score of the personalized and generalized models when tested on each of the 15 subjects. In order to guarantee a fair comparison between the models, they had the same random seeds for model initialization, and their architecture and hyperparameters were the same. The accuracy and F1-score for the personalized model exceeded that of the subject-inclusive generalized model for all subjects except subject 1, and the personalized model outperformed the subject-exclusive generalized model in terms of accuracy and F1-score for all subjects. Table  2  shows the average and standard deviation of the accuracies and F1-scores across all subjects for the three models. We achieved an average accuracy of 95.06%, 66.95%, and 67.65% for the personalized, subject-inclusive generalized, and subject-exclusive generalized models, respectively. We also achieved an average F1-score of 91.72, 42.50, and 43.05 for the personalized, subject-inclusive generalized, and subject-exclusive generalized models, respectively. Observing the error margins in Table  2 , the differences in accuracy and F1score between the personalized model and both generalized models are statistically significant.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Discussion",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Principal Findings",
      "text": "Using a machine learning approach with a convolutional encoder, we demonstrated that a personalized model outperforms a generalized model in both the accuracy and F1-score metrics for the three-class emotion classification task. By establishing two generalized model baselines through the subject-inclusive and subject-exclusive models, we created an alternative approach to the standard generalization technique of separating training and testing sets by subject, and as a result, we were able to compare personalized and generalized approaches. Our personalized model achieved an accuracy of 95.06% and F1score of 91.72, while our subject-inclusive generalized model achieved an accuracy of 66.95% and F1-score of 42.50 and our subject-exclusive generalized model achieved an accuracy of 67.65 and F1-score of 43.05.\n\nOur work indicates that personalized models for emotion recognition should be further explored in the realm of healthcare. Machine learning methods for emotion classification are clearly viable and can achieve high accuracies, as shown by our personalized model. Furthermore, given that numerous wearable technologies collect physiological signals, data acquisition is both straightforward and non-invasive. Combined with the popularity of consumer wearable technology, it is feasible to scale emotion recognition systems. This can ultimately play a major role in the early detection of stress and negative emotions, thus serving as a preventative measure for serious health problems ranging from major depression to heart attacks.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Comparison With Prior Work",
      "text": "",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Generalized Models",
      "text": "The vast majority of studies developed generalized approaches to the emotion classification task. Schmidt et al  [15] , the pioneer of WESAD, created several feature extraction models and achieved accuracies up to 80% for the three-class classification task. Huynh et al  [23]  developed a deep neural network, trained on WESAD wrist signals, to outperform past approaches by 8.22%. Albaladejo-González et al  [35]  achieved a F1-score of 88.89 using an unsupervised local outlier factor model and 99.03 using a supervised multilayer perceptron. Additionally, they analyzed the transfer learning capabilities of different models between the WESAD and SWELL-KW  [36]  datasets. Ghosh et al  [37]  achieved 94.8% accuracy using WESAD chest data by encoding time-series data into Gramian Angular Field images and employing deep learning techniques. Bajpai et al  [38]  investigated the k-nearest neighbor algorithm to explore the tradeoff between performance and total number of nearest neighbors using WESAD. Through federated learning, Almadhor et al  [39]  achieved 86.82% accuracy on data in WESAD using a deep neural network. Behinaein et al  [40]  developed a novel transformer approach and achieved state-of-the-art performance using only one modality from WESAD.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Personalized Models",
      "text": "Sah and Ghasemzadeh  [25]  developed a generalized approach using a convolutional neural network using one modality from WESAD. For the three-class classification problem, they achieved an average accuracy of 92.85%. They used the leave-one-subject-out (LOSO) analysis to highlight the need of personalization. Indikawati and Winiarti  [33]  directly developed a personalized approach for the four-class classification problem in WESAD (neutral, stress, amusement, meditation). Using different feature extraction machine learning models, they achieved accuracies ranging from 88-99% for the 15 subjects. Liu et al  [34]  developed a federated learning approach using data from WESAD with the goal of preserving user privacy. In doing so, they developed a personalized model as a baseline, which achieved an average accuracy of 90.2%. Despite studies in both the generalized and personalized approaches, there exists no direct comparison between a generalized and personalized model. Our paper therefore bridges this gap and concretely demonstrates how personalization outperforms generalization.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Limitations And Future Work",
      "text": "One limitation of our work is that we only evaluated data for 15 subjects from a single dataset (WESAD). It is valuable to reproduce results on additional physiological signal datasets for emotion analysis, such as DEAP  [14]  and CLAS  [19] . Furthermore, data from WESAD were collected under controlled laboratory environments, which may not generalize to the real world. Therefore, analyzing emotions in a real-world context through datasets such as K-EmoCon  [24] , which contain physiological data collected during naturalistic conversations, may be useful. Emotions in the K-EmoCon dataset were categorized into 18 different classes, so exploring this dataset could also help us better assess the benefits of personalization for a broader range of emotions. A major goal of this approach is to provide support for personalized digital interventions for neuropsychiatry which could benefit a variety of applications such as video-based digital therapeutics for children with autism  [47] [48] [49] [50] [51] [52] [53] [54] .\n\nFor personalized models, individuals must annotate their own data, which can be timeconsuming and expensive. To reduce the burden of manual labeling, self-supervised learning approaches can be explored, where a model first pre-trains on a large set of unlabeled data. Then, the pre-trained model can be fine-tuned to a small amount of labeled data provided by the user. This self-supervised learning framework follows that of natural language processing models such as BERT  [27] . Studies have begun to explore the tradeoff between the amount of labels and model accuracy. For example, Khan and Sarkar  [32]  compared a supervised algorithm to a semi-supervised generative adversarial network trained on partially-labeled data from WESAD, and they demonstrated that partial labeling can still maintain accurate results.",
      "page_start": 9,
      "page_end": 10
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ). Using this model architecture, we",
      "page": 2
    },
    {
      "caption": "Figure 1: Overview of our model architecture for the three-class emotion classification task.",
      "page": 3
    },
    {
      "caption": "Figure 2: A comparison of different generalized and personalized approaches to the three-",
      "page": 3
    },
    {
      "caption": "Figure 1: Then, these intervals were partitioned into training,",
      "page": 4
    },
    {
      "caption": "Figure 2: Standard generalized models partition the training, validation, and testing sets by subject",
      "page": 4
    },
    {
      "caption": "Figure 2: Through this partitioning method, it is impossible to compare the",
      "page": 4
    },
    {
      "caption": "Figure 2: Model Architecture",
      "page": 5
    },
    {
      "caption": "Figure 1: Eight channels, representing the eight modalities we used from",
      "page": 5
    },
    {
      "caption": "Figure 3: and Figure 4 illustrate the accuracy and F1-score of the personalized and generalized",
      "page": 6
    },
    {
      "caption": "Figure 3: A comparison of model accuracy between the personalized and generalized",
      "page": 7
    },
    {
      "caption": "Figure 4: A comparison of F1-score between the personalized and generalized models.",
      "page": 7
    }
  ],
  "tables": [
    {
      "caption": "Table 1: shows the hyperparameters that determine the model structure. These were",
      "data": [
        {
          "Hyperparameter": "Encoder Depth (Number of blocks)",
          "Value": "3"
        },
        {
          "Hyperparameter": "Dropout Rate",
          "Value": "15%"
        },
        {
          "Hyperparameter": "Number of Fully Connected Layers",
          "Value": "2"
        },
        {
          "Hyperparameter": "Convolutional Kernel Size",
          "Value": "3"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 2: shows the",
      "data": [
        {
          "Model type": "Personalized",
          "Accuracy (SD; %)": "95.06 (9.24)",
          "F1-score (SD; %)": "91.72 (15.33)"
        },
        {
          "Model type": "Subject-Inclusive Generalized",
          "Accuracy (SD; %)": "66.95 (13.76)",
          "F1-score (SD; %)": "42.50 (17.37)"
        },
        {
          "Model type": "Subject-Exclusive Generalized",
          "Accuracy (SD; %)": "67.65 (13.48)",
          "F1-score (SD; %)": "43.05 (17.20)"
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Causal relationship between stressful life events and the onset of major depression",
      "authors": [
        "K Kendler",
        "L Karkowski",
        "C Prescott"
      ],
      "year": "1999",
      "venue": "Am J Psychiatry"
    },
    {
      "citation_id": "2",
      "title": "Affective reactivity to daily stress and 20-year mortality risk in adults with chronic illness: Findings from the National Study of Daily Experiences",
      "authors": [
        "J Chiang",
        "N Turiano",
        "D Mroczek",
        "G Miller"
      ],
      "year": "2018",
      "venue": "Health Psychol"
    },
    {
      "citation_id": "3",
      "title": "Let it go: Lingering negative affect in response to daily stressors is associated with physical health years later",
      "authors": [
        "K Leger",
        "S Charles",
        "D Almeida"
      ],
      "year": "2018",
      "venue": "Psychol Sci"
    },
    {
      "citation_id": "4",
      "title": "Mental health literacy: empowering the community to take action for better mental health",
      "authors": [
        "A Jorm"
      ],
      "year": "2012",
      "venue": "Am Psychol"
    },
    {
      "citation_id": "5",
      "title": "Affective computing and autism",
      "authors": [
        "R El Kaliouby",
        "R Picard",
        "S Baron-Cohen"
      ],
      "year": "2006",
      "venue": "Ann N Y Acad Sci"
    },
    {
      "citation_id": "6",
      "title": "The digital therapeutic alliance and human-computer interaction",
      "authors": [
        "D' Alfonso",
        "S Lederman",
        "R Bucci",
        "S Berry"
      ],
      "year": "2020",
      "venue": "JMIR Ment Health"
    },
    {
      "citation_id": "7",
      "title": "Basic emotions are associated with distinct patterns of cardiorespiratory activity",
      "authors": [
        "P Rainville",
        "A Bechara",
        "N Naqvi",
        "A Damasio"
      ],
      "year": "2006",
      "venue": "Int J Psychophysiol"
    },
    {
      "citation_id": "8",
      "title": "Bodily maps of emotions",
      "authors": [
        "L Nummenmaa",
        "E Glerean",
        "R Hari",
        "J Hietanen"
      ],
      "year": "2014",
      "venue": "Proc Natl Acad Sci U S A",
      "doi": "10.1073/pnas.1321664111"
    },
    {
      "citation_id": "9",
      "title": "Analysis of physiological signals for recognition of boredom, pain, and surprise emotions",
      "authors": [
        "E Jang",
        "B Park",
        "M Park",
        "S Kim",
        "J Sohn"
      ],
      "year": "2015",
      "venue": "J Physiol Anthropol"
    },
    {
      "citation_id": "10",
      "title": "Individual differences in cognitive reappraisal: experiential and physiological responses to an anger provocation",
      "authors": [
        "I Mauss",
        "C Cook",
        "J Cheng",
        "J Gross"
      ],
      "year": "2007",
      "venue": "Int J Psychophysiol"
    },
    {
      "citation_id": "11",
      "title": "Misery has more company than people think: underestimating the prevalence of others' negative emotions",
      "authors": [
        "A Jordan",
        "B Monin",
        "C Dweck",
        "B Lovett",
        "O John",
        "J Gross"
      ],
      "year": "2011",
      "venue": "Pers Soc Psychol Bull"
    },
    {
      "citation_id": "12",
      "title": "Levels of emotional awareness: Theory and measurement of a socio-emotional skill",
      "authors": [
        "R Lane",
        "R Smith"
      ],
      "year": "2021",
      "venue": "J Intell"
    },
    {
      "citation_id": "13",
      "title": "A machine learning model for emotion recognition from physiological signals",
      "authors": [
        "J Domínguez-Jiménez",
        "K Campo-Landines",
        "J Martínez-Santos",
        "E Delahoz",
        "S Contreras-Ortiz"
      ],
      "year": "2020",
      "venue": "Biomed Signal Process Control",
      "doi": "10.1016/j.bspc.2019.101646"
    },
    {
      "citation_id": "14",
      "title": "DEAP: A database for emotion analysis ;using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani"
      ],
      "year": "2012",
      "venue": "IEEE Trans Affect Comput",
      "doi": "10.1109/t-affc.2011.15"
    },
    {
      "citation_id": "15",
      "title": "Introducing wesad, a multimodal dataset for wearable stress and affect detection",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Duerichen",
        "C Marberger",
        "K Laerhoven"
      ],
      "year": "2018",
      "venue": "Proceedings of the 20th ACM international conference on multimodal interaction",
      "doi": "10.1145/3242969.3242985"
    },
    {
      "citation_id": "16",
      "title": "A survey on physiological signal-based emotion recognition",
      "authors": [
        "Z Ahmad",
        "N Khan"
      ],
      "year": "2022",
      "venue": "Bioengineering (Basel)"
    },
    {
      "citation_id": "17",
      "title": "Wearable-based affect recognitiona review",
      "authors": [
        "P Schmidt",
        "A Reiss",
        "R Dürichen",
        "K Laerhoven"
      ],
      "year": "2019",
      "venue": "Sensors",
      "doi": "10.3390/s19194079"
    },
    {
      "citation_id": "18",
      "title": "U-net: Convolutional networks for biomedical image segmentation",
      "authors": [
        "O Ronneberger",
        "P Fischer",
        "T Brox"
      ],
      "year": "2015",
      "venue": "Medical Image Computing and Computer-Assisted Intervention-MICCAI"
    },
    {
      "citation_id": "19",
      "title": "Clas: A database for cognitive load, affect and stress recognition",
      "authors": [
        "V Markova",
        "T Ganchev",
        "K Kalinkov"
      ],
      "year": "2019",
      "venue": "International Conference on Biomedical Innovations and Applications (BIA)",
      "doi": "10.1109/BIA48344.2019.8967457"
    },
    {
      "citation_id": "20",
      "title": "An emotion recognition system based on physiological signals obtained by wearable sensors. Wearable Sensors and Robots",
      "authors": [
        "C He",
        "Y Yao",
        "X Ye"
      ],
      "year": "2015",
      "venue": "Proceedings of International Conference on Wearable Sensors and Robots"
    },
    {
      "citation_id": "21",
      "title": "",
      "authors": [
        "China Hangzhou"
      ],
      "year": "2017",
      "venue": "",
      "doi": "10.1007/978-981-10-2404-7_2"
    },
    {
      "citation_id": "22",
      "title": "Fused CNN-LSTM deep learning emotion recognition model using electroencephalography signals",
      "authors": [
        "M Ramzan"
      ],
      "year": "2023",
      "venue": "Int J Neurosci"
    },
    {
      "citation_id": "23",
      "title": "A comparative study of machine learning techniques for emotion recognition from peripheral physiological signals. 2020 31st Irish Signals and Systems Conference (ISSC)",
      "authors": [
        "S Vijayakumar",
        "R Flynn",
        "N Murray"
      ],
      "year": "2020",
      "venue": "A comparative study of machine learning techniques for emotion recognition from peripheral physiological signals. 2020 31st Irish Signals and Systems Conference (ISSC)",
      "doi": "10.1109/ISSC49989.2020.9180193"
    },
    {
      "citation_id": "24",
      "title": "Affect state and stress detection using neural architecture search",
      "authors": [
        "L Huynh",
        "T Nguyen",
        "T Nguyen",
        "S Pirttikangas",
        "P Siirtola",
        "Stressnas"
      ],
      "year": "2021",
      "venue": "Adjunct Proceedings of the 2021 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2021 ACM International Symposium on Wearable Computers",
      "doi": "10.1145/3460418.3479320"
    },
    {
      "citation_id": "25",
      "title": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations. Scientific Data",
      "authors": [
        "C Park",
        "N Cha",
        "S Kang",
        "A Kim",
        "A Khandoker",
        "L Hadjileontiadis",
        "A Oh",
        "Y Jeong",
        "U Lee"
      ],
      "year": "2020",
      "venue": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations. Scientific Data",
      "doi": "10.6084/m9.figshare.12618797"
    },
    {
      "citation_id": "26",
      "title": "Stress classification and personalization: Getting the most out of the least. arXiv. Preprint posted online July 12",
      "authors": [
        "R Sah",
        "H Ghasemzadeh"
      ],
      "venue": "Stress classification and personalization: Getting the most out of the least. arXiv. Preprint posted online July 12",
      "doi": "10.48550/arXiv.2107.05666"
    },
    {
      "citation_id": "27",
      "title": "Pre-training of deep bidirectional transformers for language understanding. arXiv. Preprint posted online October 11",
      "authors": [
        "J Devlin",
        "M Chang",
        "K Lee",
        "K Toutanova",
        "Bert"
      ],
      "year": "2018",
      "venue": "Pre-training of deep bidirectional transformers for language understanding. arXiv. Preprint posted online October 11",
      "doi": "10.48550/arXiv.1810.04805"
    },
    {
      "citation_id": "28",
      "title": "A Review of and roadmap for data science and machine learning for the neuropsychiatric phenotype of autism",
      "authors": [
        "P Washington",
        "D Wall"
      ],
      "year": "2023",
      "venue": "Annu Rev Biomed Data Sci"
    },
    {
      "citation_id": "29",
      "title": "Data-driven diagnostics and the potential of mobile artificial intelligence for digital therapeutic phenotyping in computational psychiatry",
      "authors": [
        "P Washington",
        "N Park",
        "P Srivastava"
      ],
      "year": "2020",
      "venue": "Biol Psychiatry Cogn Neurosci Neuroimaging"
    },
    {
      "citation_id": "30",
      "title": "Effect of wearable digital intervention for improving socialization in children with autism spectrum disorder: A Randomized Clinical Trial",
      "authors": [
        "C Voss",
        "J Schwartz",
        "J Daniels"
      ],
      "year": "2019",
      "venue": "JAMA Pediatr"
    },
    {
      "citation_id": "31",
      "title": "SuperpowerGlass: a wearable aid for the at-home therapy of children with autism",
      "authors": [
        "P Washington",
        "C Voss",
        "A Kline",
        "N Haber",
        "J Daniels",
        "A Fazel",
        "T De",
        "C Feinstein",
        "T Winograd",
        "D Wall"
      ],
      "year": "2017",
      "venue": "Proceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies",
      "doi": "10.1145/3130977"
    },
    {
      "citation_id": "32",
      "title": "Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga",
        "A Desmaison"
      ],
      "year": "2019",
      "venue": "Pytorch: An imperative style, highperformance deep learning library. Advances in neural information processing systems",
      "doi": "10.5555/3454287.3455008"
    },
    {
      "citation_id": "33",
      "title": "Semi-supervised generative adversarial network for stress detection using partially labeled physiological data. arXiv. Preprint posted online June 30",
      "authors": [
        "N Khan",
        "N Sarkar"
      ],
      "year": "2022",
      "venue": "Semi-supervised generative adversarial network for stress detection using partially labeled physiological data. arXiv. Preprint posted online June 30",
      "doi": "10.48550/arXiv.2206.14976"
    },
    {
      "citation_id": "34",
      "title": "Stress detection from multimodal wearable sensor data",
      "authors": [
        "F Indikawati",
        "S Winiarti"
      ],
      "year": "2020",
      "venue": "IOP Conference Series: Materials Science and Engineering",
      "doi": "10.48550/arXiv.2107.05666"
    },
    {
      "citation_id": "35",
      "title": "Learning from others without sacrificing privacy: Simulation comparing centralized and federated machine learning on mobile health data. JMIR mHealth and uHealth",
      "authors": [
        "J Liu",
        "J Goetz",
        "S Sen",
        "A Tewari"
      ],
      "year": "2021",
      "venue": "Learning from others without sacrificing privacy: Simulation comparing centralized and federated machine learning on mobile health data. JMIR mHealth and uHealth"
    },
    {
      "citation_id": "36",
      "title": "Evaluating different configurations of machine learning models and their transfer learning capabilities for stress detection using heart rate",
      "authors": [
        "M Albaladejo-González",
        "Ruipérez Valiente",
        "Gómez Mármol"
      ],
      "year": "2023",
      "venue": "Journal of Ambient Intelligence and Humanized Computing",
      "doi": "10.1007/s12652-022-04365-z"
    },
    {
      "citation_id": "37",
      "title": "The swell knowledge work dataset for stress and user modeling research",
      "authors": [
        "S Koldijk",
        "M Sappelli",
        "S Verberne",
        "M Neerincx",
        "W Kraaij"
      ],
      "year": "2014",
      "venue": "Proceedings of the 16th international conference on multimodal interaction",
      "doi": "10.1145/2663204.2663257"
    },
    {
      "citation_id": "38",
      "title": "Classification of mental stress from wearable physiological sensors using image-encoding-based deep neural network",
      "authors": [
        "S Ghosh",
        "S Kim",
        "M Ijaz",
        "P Singh"
      ],
      "year": "2022",
      "venue": "Biosensors"
    },
    {
      "citation_id": "39",
      "title": "Evaluating KNN performance on WESAD dataset",
      "authors": [
        "D Bajpai",
        "L He"
      ],
      "year": "2020",
      "venue": "12th International Conference on Computational Intelligence and Communication Networks",
      "doi": "10.1109/CICN49253.2020"
    },
    {
      "citation_id": "40",
      "title": "Wrist-Based Electrodermal Activity Monitoring for Stress Detection Using Federated Learning",
      "authors": [
        "A Almadhor",
        "G Sampedro",
        "M Abisado",
        "S Abbas",
        "Y Kim",
        "M Khan",
        "J Baili",
        "J Cha"
      ],
      "year": "2023",
      "venue": "Sensors"
    },
    {
      "citation_id": "41",
      "title": "A transformer architecture for stress detection from ecg",
      "authors": [
        "B Behinaein",
        "A Bhatti",
        "D Rodenburg",
        "P Hungler",
        "A Etemad"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 ACM International Symposium on Wearable Computers",
      "doi": "10.1145/3460421.3480427"
    },
    {
      "citation_id": "42",
      "title": "Feature selection framework for XGBoost based on electrodermal activity in stress detection",
      "authors": [
        "C Hsieh",
        "Y Chen",
        "W Beh",
        "A Wu"
      ],
      "year": "2019",
      "venue": "IEEE International Workshop on Signal Processing Systems",
      "doi": "10.1109/SiPS47522.2019.9020321"
    },
    {
      "citation_id": "43",
      "title": "Stress detection by machine learning and wearable sensors",
      "authors": [
        "P Garg",
        "J Santhosh",
        "A Dengel",
        "S Ishimaru"
      ],
      "year": "2021",
      "venue": "26th International Conference on Intelligent User Interfaces-Companion",
      "doi": "10.1145/3397482.3450732"
    },
    {
      "citation_id": "44",
      "title": "Intelligent stress monitoring assistant for first responders",
      "authors": [
        "K Lai",
        "S Yanushkevich",
        "V Shmerko"
      ],
      "year": "2021",
      "venue": "IEEE Access",
      "doi": "10.1109/ACCESS.2021.3057578"
    },
    {
      "citation_id": "45",
      "title": "Continuous stress detection using the sensors of commercial smartwatch",
      "authors": [
        "P Siirtola"
      ],
      "year": "2019",
      "venue": "Adjunct Proceedings of the 2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedings of the 2019 ACM International Symposium on Wearable Computers",
      "doi": "10.1145/3341162.3344831"
    },
    {
      "citation_id": "46",
      "title": "Stress detection with machine learning and deep learning using multimodal physiological data",
      "authors": [
        "P Bobade",
        "M Vani"
      ],
      "year": "2020",
      "venue": "Second International Conference on Inventive Research in Computing Applications",
      "doi": "10.1109/ICIRCA48905.2020.9183244"
    },
    {
      "citation_id": "47",
      "title": "Hierarchical deep neural network for mental stress state detection using IoT based biomarkers",
      "authors": [
        "A Kumar",
        "K Sharma",
        "A Sharma"
      ],
      "year": "2021",
      "venue": "Pattern Recognition Letters",
      "doi": "10.1016/j.patrec.2021.01.030"
    },
    {
      "citation_id": "48",
      "title": "Exploratory study examining the at-home feasibility of a wearable tool for social-affective learning in children with autism",
      "authors": [
        "J Daniels",
        "J Schwartz",
        "C Voss",
        "N Haber",
        "A Fazel",
        "A Kline",
        "P Washington",
        "C Feinstein",
        "T Winograd",
        "D Wall"
      ],
      "year": "2018",
      "venue": "NPJ digital medicine",
      "doi": "10.1038/s41746-018-0035-3"
    },
    {
      "citation_id": "49",
      "title": "Feasibility testing of a wearable behavioral aid for social learning in children with autism",
      "authors": [
        "J Daniels",
        "N Haber",
        "C Voss",
        "J Schwartz",
        "S Tamura",
        "A Fazel",
        "A Kline",
        "P Washington",
        "J Phillips",
        "T Winograd",
        "C Feinstein"
      ],
      "year": "2018",
      "venue": "Applied clinical informatics"
    },
    {
      "citation_id": "50",
      "title": "Labeling images with facial emotion and the potential for pediatric healthcare. Artificial intelligence in medicine",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "P Washington",
        "Q Tariq",
        "K Dunlap",
        "J Schwartz",
        "D Wall"
      ],
      "year": "2019",
      "venue": "Labeling images with facial emotion and the potential for pediatric healthcare. Artificial intelligence in medicine"
    },
    {
      "citation_id": "51",
      "title": "A mobile game for automatic emotion-labeling of images",
      "authors": [
        "H Kalantarian",
        "K Jedoui",
        "P Washington",
        "D Wall"
      ],
      "year": "2018",
      "venue": "IEEE transactions on games",
      "doi": "10.1109/TG.2018.2877325"
    },
    {
      "citation_id": "52",
      "title": "Guess What? Towards Understanding Autism from Structured Video Using Facial Affect",
      "authors": [
        "H Kalantarian",
        "P Washington",
        "J Schwartz",
        "J Daniels",
        "N Haber",
        "D Wall"
      ],
      "year": "2019",
      "venue": "Journal of healthcare informatics research"
    },
    {
      "citation_id": "53",
      "title": "A gamified mobile system for crowdsourcing video for autism research",
      "authors": [
        "H Kalantarian",
        "P Washington",
        "J Schwartz",
        "J Daniels",
        "N Haber",
        "D Wall"
      ],
      "year": "2018",
      "venue": "2018 IEEE international conference on healthcare informatics (ICHI)",
      "doi": "10.1109/ICHI.2018.00052"
    },
    {
      "citation_id": "54",
      "title": "Superpower glass. GetMobile: Mobile Computing and Communications",
      "authors": [
        "A Kline",
        "C Voss",
        "P Washington",
        "N Haber",
        "H Schwartz",
        "Q Tariq",
        "T Winograd",
        "C Feinstein",
        "D Wall"
      ],
      "year": "2019",
      "venue": "Superpower glass. GetMobile: Mobile Computing and Communications",
      "doi": "10.1145/3130977"
    },
    {
      "citation_id": "55",
      "title": "A mobile game platform for improving social communication in children with autism: a feasibility study",
      "authors": [
        "Y Penev",
        "K Dunlap",
        "A Husic",
        "C Hou",
        "P Washington",
        "E Leblanc",
        "A Kline",
        "J Kent",
        "A Ng-Thow-Hing",
        "B Liu",
        "C Harjadi"
      ],
      "year": "2021",
      "venue": "Applied clinical informatics"
    }
  ]
}