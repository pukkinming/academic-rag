{
  "paper_id": "2503.20428v1",
  "title": "Evaluating Facial Expression Recognition Datasets For Deep Learning: A Benchmark Study With Novel Similarity Metrics",
  "published": "2025-03-26T11:01:00Z",
  "authors": [
    "F. Xavier Gaya-Morey",
    "Cristina Manresa-Yee",
    "Célia Martinie",
    "Jose M. Buades-Rubio"
  ],
  "keywords": [
    "Facial Expression Recognition",
    "Deep Learning",
    "Computer Vision"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "This study investigates the key characteristics and suitability of widely used Facial Expression Recognition (FER) datasets for training deep learning models. In the field of affective computing, FER is essential for interpreting human emotions, yet the performance of FER systems is highly contingent on the quality and diversity of the underlying datasets. To address this issue, we compiled and analyzed 24 FER datasets-including those targeting specific age groups such as children, adults, and the elderly-and processed them through a comprehensive normalization pipeline. In addition, we enriched the datasets with automatic annotations for age and gender, enabling a more nuanced evaluation of their demographic properties. To further assess dataset efficacy, we introduce three novel metrics-Local, Global, and Paired Similarity-which quantitatively measure dataset difficulty, generalization capability, and cross-dataset transferability. Benchmark experiments using state-of-the-art neural networks reveal that large-scale, automatically collected datasets (e.g., AffectNet, FER2013) tend to generalize better, despite issues with labeling noise and demographic biases, whereas controlled datasets offer higher annotation quality but limited variability. Our findings provide actionable recommendations for dataset selection and design, advancing the development of more robust, fair, and effective FER systems.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Facial Expression Recognition (FER) is a key research area within affective computing, focused on identifying and interpreting human emotions through facial cues. While facial expressions themselves do not equate to emotions [1], they serve as vital visual indicators of emotional states, facilitating nonverbal communication and enhancing social interactions  [2] . Although the universality of facial expressions across cultures remains a topic of debate  [3] , Ekman's foundational work on six fundamental expressions-anger, happiness, surprise, disgust, sadness, and fear-continues to be a cornerstone in this field  [4] . The increasing relevance of FER is reflected in its projected market growth, estimated to reach $682.2 billion by 2032, driven by applications in medical diagnostics  [5] , human behavior analysis  [6] , and human-computer interaction  [7] .\n\nF. Xavier Gaya-Morey, Cristina Manresa-Yee and Jose M. Buades-Rubio are with the Computer Graphics and Vision and AI Group (UGIVIA), at the Universitat de les Illes Balears, Carretera de Valldemossa, km 7.5, Palma, 07122, Illes Balears, Spain (e-mail: francesc-xavier.gaya@uib.es; cristina.manresa@uib.es; josemaria.buades@uib.es).\n\nCélia Martinie is with the ICS-IRIT, at the University Toulouse 3, Paul Sabatier, 118 Rte de Narbonne, Toulouse, 31062, France (e-mail: celia.martinie@irit.fr).\n\nTraditional machine learning methods for facial expression recognition relied on Action Units (AUs) and handcrafted feature descriptors  [8] . AUs represent facial muscle activations and were used by human observers in the Facial Action Coding System (FACS)  [9]  to describe emotional expressions with high accuracy. However, with the arrival of Deep Learning (DL) methods-capable of automatically extracting feature representations from raw images and mimicking human visual perception-these approaches have become the standard in FER due to their superior accuracy and generalization capabilities  [8] ,  [10] . Given the data-intensive nature of DL, numerous FER datasets have been developed. However, the performance of trained models is highly dependent on the dataset used for training, making dataset selection a critical factor. Characteristics such as dataset size, recording conditions, labeling methodology, and participant demographics must be carefully considered to mitigate potential biases. For instance, significant differences in facial expression recognition exist between age groups, such as children, adults, and the elderly  [11] . Despite this, large-scale datasets like AffectNet  [12]  do not provide age distribution information, and most existing datasets focus on specific age groups. For example, LIRIS-CSE  [13]  targets children, KDEF  [14]  focuses on adults, and ElderReact  [15]  is designed for elderly individuals.\n\nTo address these gaps, we have compiled and analyzed 24 FER datasets, including those tailored to specific age groups. These datasets were normalized through multiple processing steps, and we enriched them with additional annotations-such as estimated age and gender-using deep learning models. This allowed for an in-depth examination of current FER dataset characteristics. Additionally, we introduced three novel metrics to assess different aspects of the datasets and conducted a benchmark by training state-of-the-art deep learning models on each dataset. To our knowledge, this study constitutes the most comprehensive compendium of FER datasets to date, providing an extensive evaluation that will be valuable for future research. For transparency and reproducibility, we have made publicly available the list of images used from each database, the computed automatic annotations, and the code 1 .\n\nThe remainder of this paper is structured as follows. First, we review previous FER benchmarks and commonly used datasets. We then outline the primary and secondary research questions guiding our study. Next, we describe our methodology, including dataset collection and normalization, metric definitions, and benchmark settings. The results are subsequently presented and analyzed, grouped according to the research questions. Finally, we discuss key findings, provide recommendations for dataset selection and construction, and conclude the study.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work A. Common Facial Expression Recognition Datasets",
      "text": "Facial expression recognition has been a focus of research for many years, not only for developing automated methods but also for psychological experiments, leading to the availability of numerous datasets. In our literature review, we identified 28 common datasets within the field, with particular attention to those including older adults and children-two underrepresented age groups due to their higher vulnerability. These datasets, which serve both automated FER research and controlled psychological experiments, vary widely in terms of collection methods, demographics, and expression types.\n\nIn Table  I , we summarize the key characteristics of the datasets, published between 1997 and 2022. This compilation of datasets illustrates the diversity and evolution of FER resources, ranging from controlled laboratory collections to large-scale, \"in the wild\" data. The varied characteristicsincluding subject demographics, expression types, collection methods, and imaging modalities-underscore the importance of dataset selection and curation in developing robust and generalizable facial expression recognition systems.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Facial Expression Recognition Benchmarks",
      "text": "Despite the growing interest in facial expression recognition, there are few comprehensive benchmarks available in the literature. Some existing studies compare newly proposed datasets with previously established ones or conduct experiments across multiple datasets. For instance, Manresa et al.  [39]  explored cross-dataset learning using five datasets-BU-4DFE, CK+, FEGA, JAFFE, and WSEFEP-demonstrating that generalization improves when training incorporates multiple datasets. Similarly, Chen et al.  [40]  conducted experiments across multiple datasets-CK+, JAFFE, SFEW, FER2013, ExpW, RAF-DB, and AFE-evaluating 14 algorithms to analyze and enhance cross-domain learning strategies.\n\nSeveral literature reviews have also compiled performance results across different datasets. For example, Li et al.  [10]  and Kopalidis et al.  [41]  present the performance of multiple methods on six and ten datasets, respectively. However, these approaches present several limitations. First, performance is often measured solely using accuracy, which is not a reliable metric for unbalanced datasets  [42] . Additionally, the number of expression classes varies across studies, meaning that the baseline performance of a random classifier is inconsistent. Furthermore, validation procedures differ between studies, employing methods such as leave-one-subject-out (LOSO), kfold cross-validation with different values of k, or pre-defined training-validation-test splits, which affects the performance estimation. Lastly, preprocessing techniques also vary, leading to inconsistencies in dataset evaluation. These factors underscore the need for a benchmark that normalizes datasets under the same conditions for a fair comparison.\n\nMany studies proposing new FER methods evaluate their approaches using multiple datasets. For instance, Dhall et al.  [31]  introduced SFEW, a static version of AFEW, and validated their method on SFEW, JAFFE, and Multi-PIE  [43] . Similarly, Wu et al.  [44]  proposed a graph convolutional neural network and tested it on CK+ and JAFFE, while Li et al.  [45]  designed a FER-specific transformer architecture and evaluated it using RAF-DB, SFEW, and FER+  [46] .\n\nAnother relevant approach focuses on addressing misclassification errors in FER datasets. For example, Escobar et al.  [47]  tackled this issue by applying iterative training and automatic reclassification techniques on FER2013, NHFI, and AffectNet. Similarly, Liu et al.  [48]  proposed auxiliary action unit graphs to mitigate misclassification errors, conducting experiments on RAF-DB and AffectNet.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Research Questions",
      "text": "In this section, we present the research questions addressed in this study, as outlined in Table  II , and briefly explain the approach used to answer them. These questions are categorized into primary and secondary research questions.\n\nThe first primary research question (RQ1) investigates the main characteristics of widely used FER datasets. Its associated secondary research questions explore specific aspects relevant to other studies, including the conditions under which the datasets were collected (e.g., sourced from the Internet, recorded with actors, etc.), their demographic characteristics (e.g., age, gender distributions), the labeled expression classes and their balance, and the key data characteristics, such as color information and whether the dataset consists of images or videos. The second primary research question (RQ2) focuses on evaluating and comparing datasets for training deep learning models. The secondary research questions aim to assess three key aspects: the difficulty level of each dataset for deep learning models, the generalization capability of models trained on one dataset when tested on others, and the similarity between datasets in terms of model performance.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Iv. Methods",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Datasets Collection And Normalization",
      "text": "We aimed to obtain as many datasets as listed in Table I as possible. When datasets were publicly available, we accessed them via web forms, and for those without specified access procedures, we directly contacted the responsible individuals or the original authors via email. Despite our efforts, access to four datasets-Tsinghua, RAF-DB, Affective Interaction, and CAFE-was not granted, either due to a lack of response or denial of permission. Despite this, a public version of the RAF-DB dataset containing 15k images was found and used instead 2 .\n\nThe datasets presented several challenges due to inherent differences, such as varying numbers of classes, inconsistent data annotations (e.g., age, gender, head pose), different image resolutions, and differing face locations within images. To standardize and harmonize the data from these diverse sources, a six-step process was followed:\n\n1) Frame sampling. 2 https://www.kaggle.com/datasets/shuvoalok/raf-db-dataset 1) Frame Sampling: Among the collected datasets, nine contain video data-ElderReact, EmoReact, BioVidEmo, AFEW, MMI, CK+, LIRIS-CSE, and BU-4DFE. Since this work focuses on facial expression recognition from still images, these datasets were sampled, extracting only a subset of frames, depending on the dataset type. For videos that transition between a neutral expression and one of the six basic emotions, three frames were extracted per video: one displaying the neutral expression and two depicting the target expression. Only a single neutral frame was taken from each video, as the neutral expression was consistently present, unlike the others. This sampling method was applied to the CK+, MMI, and BioVidEmo datasets. For videos in which the same expression is maintained throughout, five frames were sampled at equal intervals from the beginning to the end of each video. This approach was used for the ElderReact, EmoReact, and LIRIS-CSE datasets. For the AFEW dataset, we utilized its official static version, SFEW, eliminating the need for frame extraction. Similarly, the BU-4DFE dataset, as provided by the authors, was already pre-sampled and available in image format.\n\n2) Class Unification: The datasets used in the benchmark included different classes, some of which referred to the same expression. The following classes were merged under the same label:\n\n• Anger: \"arrabbiato\" (FEGA), \"annoyed\" (Lifespan), and \"grumpy\" (Lifespan) • Disgust: \"disgusto\" (FEGA) • Fear: \"afraid\" (DDCF, NIMH-ChEFS, KDEF), \"fearful\" (RaFD), and \"paura\" (FEGA) • Happiness: \"joy\" (WSEFEP), \"allegria\" (FEGA), \"amusement\" and (BioVidEmo) • Sadness: \"tristezza\" (FEGA) • Surprise: \"sorpresa\" (FEGA) • Neutral: \"neutra\" (FEGA), and \"profile\" (Lifespan) 3) Automatic Labeling: The available information across the datasets varied significantly, necessitating the use of multiple deep learning approaches to automatically annotate missing features such as age, gender, and head pose in the images.\n\nThe MiVOLO transformer model  [49]  was selected for age and gender estimation due to its demonstrated state-ofthe-art performance. To obtain face crops, the default face detector from its official implementation, YOLOv8  [50] , was employed. Since age and gender are constant attributes for a single user or video clip (each containing only one individual), estimations were made for every image or frame associated with each user. The final age and gender labels were refined by taking the mode of the gender and the median of the age values across all images for each user, thus enhancing accuracy and resolving inconsistencies within the data.\n\nFor head pose estimation, InsightFace  [51]  was utilized, employing ResNet50  [52]  for facial alignment. For consistency with the existing dataset annotations, images and frames were categorized into \"front\", \"half left\", \"half right\", \"full left\", \"full right\" and \"back\" classes. These categories correspond to approximate orientations of frontal (0 degrees), 45 degrees (left or right), 90 degrees (left or right), or over 90 degrees rotation of the head, respectively.\n\n4) Age Group Extraction: We established three main age groups: 17 years or less (children), between 18 and 59 years (adults), and 60 years or more (elderly). We used the age annotations of the datasets when available to classify the images into one of these three groups. In many datasets, participants' age was not available, but the age group was, so we used it, matching it with the defined three age groups. In cases where neither age nor age group information was present in the dataset, the automatic age labels described in Section IV-A3 were used.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "5) Image Exclusion:",
      "text": "We excluded those images in which YOLOv8 failed to detect a face, as this step is essential for subsequent image preprocessing. Additionally, we introduced an exclusion criterion for non-frontal images. Specifically, images with head poses labeled as \"full left,\" \"full right,\" and \"back,\" as defined in Section IV-A3, along with those lacking head pose annotations (indicating failure of automatic estimation), were removed. This step aimed to exclude challenging samples with poor camera perspectives that could negatively impact recognition performance.\n\n6) Preprocessing: Before utilizing the images for training, a preprocessing step was applied to normalize the faces and facilitate the learning process of the models. Following the methodology outlined in previous studies  [16] ,  [39] ,  [53] ,  [54] , facial landmarks were first detected using the estimator described in Section IV-A3 to obtain the coordinates of key facial points. The images were then rotated to horizontally align the eyes, using a line drawn between the coordinates of the eyes. Finally, the images were cropped, resized to a resolution of 224 × 224 pixels, and converted to grayscale to standardize the input format for the models.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Metrics Definition",
      "text": "In this study, we not only aimed to explore different datasets but also to assess their suitability for training deep learning models. To achieve this, we propose three new metrics that we have named Local Similarity, Global Similarity, and Paired Similarity, each capturing different aspects of the datasets based on a set of deep learning models. For simplicity, we use the following notations: Local Similarity function LS(d) quantifies the performance of models trained and tested on the same dataset d, thus providing a measure of how challenging it is for the models to learn the task from the dataset. It is computed using Equation  1 .\n\nGlobal Similarity function GS(d train ) measures how well models trained on dataset d train perform when evaluated on all other datasets. It indicates the dataset's ability to produce models with strong cross-dataset performance, i.e. it reflects how well a model trained on one dataset generalizes to others. The metric is computed using Equation  2 .\n\nC. Benchmark Settings 1) Hardware: All experiments were conducted on a computer equipped with an Intel Core i9-9900KF CPU (3.60 GHz), a NVIDIA RTX 4090 GPU (24GB), and 32GB of RAM, provided by the University of the Balearic Islands. The hardware specifications were considered sufficient to perform all necessary computations within a reasonable timeframe.\n\n2) Models: Two deep learning models were selected for the experiments, to include different architectural choices: Swin Transformer  [55] , and ConvNext  [56] . The two of them are fairly recent and have demonstrated excelling results in the image classification task, with results above 80% accuracy on the ImageNet dataset.\n\nThe Swin Transformer is a hierarchical vision transformer that processes images using shifted windows, improving efficiency and scalability. It constructs multi-scale feature maps by splitting images into patches and merging them at deeper layers, enabling strong performance in object detection and segmentation. The shifted window approach balances local self-attention with cross-window connections, allowing Swin Transformer to excel in various vision tasks.\n\nConvNeXt revisits ConvNets by integrating transformerinspired design choices, such as large kernel sizes and inverted bottlenecks, into a ResNet-based architecture. This modernization enables ConvNeXt to surpass transformer models like Swin Transformer in tasks such as object detection and segmentation while maintaining the efficiency of ConvNets. It demonstrates that well-designed ConvNets remain highly competitive for modern vision tasks.\n\n3) Training Process: We conducted five training sessions for each dataset, following a cross-validation procedure. This resulted in a total of 240 training sessions (24 datasets × 2 networks × 5 cross validations = 240). The models were implemented using the PyTorch framework, leveraging pretrained weights from ImageNet. Data augmentation techniques were applied uniformly, including random horizontal flipping, rotation, translation, scaling, and adjustments to brightness and contrast. Each model was trained for a maximum of 20 epochs, with early stopping applied when validation accuracy did not improve by at least 1% for 5 consecutive epochs. Furthermore, since some of the datasets used for training were unbalanced, we weighted the loss function (cross entropy loss) to account for class imbalance.\n\n4) Testing Process: Every training was evaluated on every other dataset. In total, 240 trainings × 24 datasets = 5, 760 evaluations were conducted. For fairness, since not all seven expression classes were present in all datasets, only those present in the training were used for its testing on the remaining datasets.\n\nAfter computing all evaluations, we measured three key characteristics for each dataset: Local Similarity, Global Similarity, and Paired Similarity, described in Section IV-B. As measure of performance θ, we used the F1 score metric, a very common metric for image classification tasks that stays fair for unbalanced problems, conversely to accuracy  [42] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "V. Results",
      "text": "In this section, we present the results obtained to answer the primary and secondary research questions outlined in Section III. Each subsection corresponds to a specific research question.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Rq1: Datasets Exploration",
      "text": "To address the first primary research question, we examine the key characteristics of the collected datasets, considering various aspects. Figure  1  shows the number of images in each dataset after applying the normalization process described in Section IV-A. Because they share common image sources, duplicate images may appear across these datasets. Additionally, due to their automated nature, these datasets tend to be large but may contain labeling errors, duplicate images, and non-facial images  [39] ,  [47] ,  [48] . In contrast, FE-Test, ElderReact, and EmoReact were manually curated from online sources. FE-Test was hand-picked to create a small test set for evaluating models trained on other datasets. ElderReact and EmoReact were sourced from the \"React\" YouTube channel  3  , then clipped and annotated. Notably, SFEW is a static version of the AFEW dataset, created by extracting frames from movie clips using subtitles to identify specific expressions.\n\nThe remaining datasets were captured in controlled environments using one or more cameras. This setup allows for greater control over factors such as face orientation, gaze, lighting conditions, and other key variables. Additionally, since these datasets feature posed expressions, the labels correspond to the intended expressions of the actors, reducing the likelihood of labeling errors compared to automatically collected datasets. These datasets also tend to include demographic annotations (e.g., age, gender, and ethnicity) and offer multiple expression samples per individual, making them suitable for cross-subject experiments.\n\nFigure  2a  presents the number of individuals included in each dataset, while Figure  2b  shows the average number of images per user. Note that automatically collected datasets lack user demographic labels, and hence were excluded from the figures. Both the number of users and the number of images per user vary significantly across datasets, although datasets with a large number of users generally contain fewer images per user, e.g. Lifespan and DDCF. Among the collected datasets, only four-ElderReact, EmoReact, LIRIS-CSE, and BioVidEmo-were explicitly designed to capture spontaneous expressions. These datasets were recorded in controlled environments while attempting to elicit natural reactions using multimedia stimuli. In contrast, Internet-sourced datasets such as AffectNet and FER2013 may contain a mix of natural and posed expressions, whereas the remaining datasets feature posed expressions.\n\n2) RQ1.2: Demographic Characteristics: Figure  3  presents the age histogram across all datasets. A total of 55% of the images feature individuals between 20 and 40 years old, while only 21%, 17% and 7% correspond to individuals between 40 and 60, under 18, and over 60 years old, respectively. This indicates a significant age-related bias, with a much higher representation of adults compared to children and older adults. The availability of images for elderly individuals is particularly limited and decreases further with increasing age. Interestingly, the data appears to form three distinct clusters corresponding to different age groups: around 10 years old (children), 30 years old (adults), and 60 years old (elderly). A more detailed analysis of age distribution per dataset reveals significant variability, as illustrated in the right plot of Figure  4 . Large-scale automated datasets-AffectNet, FER2013, RAF-DB, SFEW, ExpW, and NHFI-contain images spanning all age groups but tend to be biased toward the adult category. Conversely, manually curated datasets often focus on specific age groups. For instance, LIRIS-CSE, EmoReact, DDCF, DEFSS, and NIMH-ChEFS primarily include children, while ElderReact is dedicated to elderly individuals. Other datasets, such as WSEFEP, MMI, KDEF, JAFFE, FEGA, and BU-4DFE, predominantly feature adult subjects. Notably, two manually curated datasets, Lifespan and FACES, cover a broader age range, from young adults to older individuals. These datasets were specifically designed to explore the impact of aging on facial expression recognition, though they lack images of children, preventing full-spectrum age coverage. Additionally, some acted datasets primarily consist of adult subjects but include a small proportion of children (RaFD, DEFSS, and CK+) and elderly individuals (BioVidEmo).\n\nThe left plot of Figure  4  illustrates the gender distribution across datasets. In most cases, the difference between male and female representation is less than 20%. However, some datasets show a stronger gender imbalance. Specifically, RAFD and MMI have a male predominance, whereas JAFFE, BU-4DFE, ElderReact, NIMH-ChEFS, DEFSS, CK+, and EmoReact have a higher proportion of female images. This suggests a slight overall bias toward female representation, with JAFFE and BU-4DFE exhibiting the most pronounced imbalance (100% and 70% female subjects, respectively). Regarding ethnicity, only WSEFEP, JAFFE, DEFSS, Lifespan, FEGA, FACES, KDEF, DDCF, and RaFD include explicit ethnicity labels. There is a strong predominance of the Caucasian ethnicity (also referred to as \"White\" in some datasets), although some datasets provide more diversity. For example, RaFD includes images of Moroccan individuals, JAFFE features Japanese subjects, and Lifespan includes images from individuals categorized as \"Black\" and Indian. For datasets without ethnicity labels, some insights can be gathered from their respective publications. For example, in NIMH-ChEFS, most participants are Caucasian, with four girls and one boy identified as non-Caucasian. The BU-4DFE dataset contains diverse ethnic representation: Asian (28 participants), Black (8), Hispanic/Latino (3), and White (62). In CK+, 81% of subjects are Euro-American, 13% Afro-American, and 6% from other groups. The MMI dataset comprises 10 European, 3 South American, and 12 Asian participants. The remaining datasets neither include ethnicity labels nor provide relevant information in their documentation.\n\n3) RQ1.3: Classes: The most commonly found classes across the collected datasets correspond to the six basic expressions  [4]  plus the neutral expression. However, some datasets also include additional expressions. For instance, the \"contempt\" expression appears in AffectNet, NHFI, RaFD, and CK+; \"pleased\" is present in DDCF; \"amusement\" in BioVidEmo; and \"joy\" in WSEFEP. Moreover, EmoReact provides labels for \"curiosity\", \"uncertainty\", \"excitement\", and \"frustration\"; Lifespan includes \"annoyance\", \"grumpiness\", and \"profile\"; while MMI contains \"scream\", \"boredom\", and \"sleepiness\". Notably, some of these expressions, such as \"amusement\" and \"joy\", are closely related to a basic expression-in this case, \"happiness\".\n\nAlthough most datasets include labels for the six basic expressions plus the neutral one, their distributions are often highly imbalanced. Figure  5  presents the class distribution across datasets. As shown, \"happiness\" and \"neutral\" are frequently the two most prevalent classes, whereas \"fear\" and \"disgust\" tend to be underrepresented in many datasets, including AffectNet, EmoReact, ExpW, Lifespan, and RAF-DB. In smaller datasets, minority classes may contain an insufficient number of images for effective training. For example, the Lifespan dataset includes only 7 images for the \"disgust\" class.\n\nAdditionally, some datasets lack certain expressions: LIRIS-CSE, FACES, BU-4DFE, ElderReact, and MMI do not include the \"neutral\" expression; Lifespan lacks \"fear\"; DEFSS and NIMH-ChEFS omit \"disgust\" and \"surprise\"; BioVidEmo excludes \"surprise\" and \"neutral\"; and EmoReact does not contain \"anger\", \"neutral\", or \"sadness\".   III  summarizes the characteristics of the collected datasets, including format (image or video), resolution, color, whether the images are cropped to the face, and whether they include variations in perspective, background, and illumination conditions.\n\nOnly seven datasets are available in video format, all of which were collected under controlled conditions (none were automatically sourced from the Internet). As a result, these video datasets contain a limited number of clips, posing challenges for expression recognition based on dynamic sequences rather than static images. Notably, SFEW is the static version of AFEW, which is originally in video format. The datasets exhibit a wide range of resolutions. With the exception of AffectNet, ExpW, and Lifespan, they contain images in a fixed resolution, ranging from low resolutions, such as FER2013 (48 × 48) and RAF-DB (100 × 100), to high resolutions, such as FACES (2835 × 3543) and NIMH-ChEFS (1960 × 3008). Most datasets provide images in RGB, except for CK+, FER2013, FE-Test, JAFFE, and NHFI, which are in grayscale. Additionally, most datasets contain face-centered images, except for BioVidEmo, ElderReact, EmoReact, FEGA, LIRIS-CSE, and SFEW, which provide uncropped images. Notably, ExpW includes uncropped images along with bounding box annotations for cropping.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Table Iii: Data Characteristics Of The Collected Datasets",
      "text": "Several datasets include variations in facial perspective, background, and illumination conditions. As expected, datasets collected from the Internet-AffectNet, ExpW, RAF-DB, SFEW, NHFI, FE-Test, and FER2013-naturally exhibit these variations, as the images were captured in diverse contexts. Video-format datasets generally offer variations in facial perspective due to head movements throughout the videos, as seen in BioVidEmo, ElderReact, EmoReact, and LIRIS-CSE. Additionally, MMI, DDCF, KDEF, and RaFD deliberately incorporated different facial viewpoints during data collection. The majority of datasets created using actors maintained consistent background and illumination conditions across all images. This was the case for BioVidEmo, BU-4DFE, DEFSS, FACES, JAFFE, KDEF, Lifespan, MMI, NIMH-ChEFS, RaFD, and WSEFEP. In contrast, DDCF and CK+ included variations in illumination intensity, while ElderReact, EmoReact, FEGA, and LIRIS-CSE featured recordings in different scenarios. Notably, DDCF's actors posed wearing a black beanie, while BioVidEmo's actors wore an EEG cap.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Rq2: Benchmark",
      "text": "In this section, we present the results of the benchmarking process conducted on the collected datasets to address RQ2 and its secondary research questions.\n\n1) RQ2.1: Dataset Difficulty: The numerical results for the Local Similarity metric are shown in the second column of Table  IV . Figure  6  presents the results for this metric, broken down by network. This metric evaluates the performance of networks trained and tested on the same dataset, providing insights into the relative difficulty of each dataset.\n\nThe highest Local Similarity values were observed in RaFD, FACES, WSEFEP, and NIMH-ChEFS, all exceeding 0.9. These datasets were collected under controlled conditions, likely resulting in fewer variations in images and, consequently, making the recognition task easier. Among the datasets collected automatically from the Internet, RAF-DB and FER2013 achieved scores above 0.7, while NHFI,  AffectNet, and ExpW had lower scores. The lowest scores were recorded for ElderReact (0.2224), BioVidEmo (0.3603), LIRIS-CSE (0.4318), and SFEW (0.4681). Interestingly, all of these datasets were originally in video format and were converted into image sets via frame sampling. However, two other video-format datasets, CK+ and MMI, performed significantly better, with values above 0.6. The highest variability in results across networks was observed in JAFFE and FE-Test, both containing fewer than 400 samples. In contrast, other similarly sized datasets, such as WSEFEP and DEFSS, did not exhibit such variations. Among cross-validation iterations, the highest difference was on JAFFE by far, followed by Lifespan, and FEGA.\n\n2) RQ2.2: Dataset Generalization: The results for the Global Similarity metric are shown in the third column of Table  IV . Figure  7  illustrates the results, broken down by network. Unlike Local Similarity, which evaluates networks on the same dataset they were trained on, this metric measures performance when networks are tested on all datasets except the one used for training, thereby assessing generalization capabilities. Fig.  7 : Global similarity computed per network.\n\nThe highest Global Similarity score was achieved by Af-fectNet (0.6095), which was 0.1 higher than the second-best dataset, FER2013 (0.5008). Other high-performing datasets included NHFI (0.4773), ExpW (0.4782), and RAF-DB (0.4601), all of which were collected from the Internet and are among the largest datasets. In contrast, datasets collected under controlled conditions performed worse in terms of generalization. The lowest scores were obtained by EmoReact (0.0994), followed by ElderReact (0.1667), Lifespan (0.1769), and LIRIS-CSE (0.1934). Once again, video-format datasets occupied the lowest rankings, with the exception of CK+ and MMI. The greatest variation in network performance was observed in FEGA and FE-Test.\n\n3) RQ2.3: Dataset Redundancy: Figure  8  presents the Paired Similarity of the datasets, where each training dataset is evaluated against all other datasets and normalized by its Local Similarity. The results are displayed in matrix form, where rows represent the training dataset and columns represent the testing dataset. The diagonal entries are always equal to one, as they correspond to the paired similarity of each dataset with itself. However, values greater than one indicate cases where training on one dataset results in better performance on another dataset than on itself. This metric provides insights into how much information from one dataset can be learned from another.\n\nThere were two cases where the Paired Similarity exceeded 1: training on AffectNet resulted in better performance on FE-Test, and MMI. This suggests that AffectNet may provide more useful training features than those two datasets themselves, likely due to factors such as dataset size and similarity. Additionally, AffectNet achieved Paired Similarity scores above 0.90 for FEGA, WSEFEP, JAFFE, KDEF, NIMH-ChEFS, and RaFD; FER2013 for FE-Test; KDEF for RaFD and WSEFEP; and NHFI for FE-Test. These findings align with the Global Similarity results discussed in the previous section, where these datasets-particularly AffectNetdemonstrated strong generalization capabilities.\n\nConversely, EmoReact, ElderReact, BioVidEmo, Lifespan, and LIRIS-CSE consistently exhibited the lowest scores, both as training and testing datasets, which aligns with their poor Global Similarity performance. Notably, FE-Test generally",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Vi. Discussion",
      "text": "The exploration and benchmarking of the collected datasets revealed common characteristics, strengths, and weaknesses. In this section, we use the obtained results to explore these aspects in depth and list recommendations for future studies in the facial expression recognition field.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "A. Collection Process, Demographic Biases And Expression Distribution",
      "text": "The main difference between datasets stems from their collection process: automatic collection from the Internet or manual recording with actors in a controlled setup. This difference significantly impacts the characteristics of the images in the dataset. Automatic datasets tend to be larger and exhibit greater variation in illumination conditions, backgrounds, face perspectives, and demographic diversity. However, unlike manually recorded datasets, they lack demographic information such as age, gender, and ethnicity and may contain lower-quality expression annotations since they were labeled a posteriori, meaning that the labels may be influenced by labelers' subjectivity. Another key distinction between datasets is the nature of the expressions they contain, either acted or spontaneous. Only four datasets-ElderReact, EmoReact, LIRIS-CSE, and BioVidEmo-offer spontaneous expressions, elicited by showing videos to participants. Nevertheless, these datasets performed poorly across all three metrics, indicating the increased difficulty of recognizing spontaneous expressions and their unsuitability for expression recognition from static images.\n\nDatasets lacking demographic information were automatically labeled using computational tools. This analysis revealed a significant age imbalance in all datasets, with a predominance of adult samples over children and elderly individuals. Since crucial facial differences exist between these age groups that affect facial expression recognition  [11] , the underrepresentation of minority groups may lead to biased training. The same issue arises with ethnicity, where datasets are overwhelmingly dominated by Caucasian (or white) participants. On the other hand, gender distribution was generally balanced across datasets.\n\nWhile custom datasets typically maintain a balanced number of samples per expression, automatically collected datasets tend to be heavily biased toward happiness and neutral expressions, as these are more commonly found on the Internet and typically easier to distinguish from other expressions. Addressing this imbalance is crucial to avoid biases in training, with one straightforward approach being the use of a weighted loss function, as implemented in this study.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "B. Benchmarking With Similarity Metrics",
      "text": "Numerous metrics exist in information theory to assess the similarity of datasets, such as entropy-related metrics. However, in this study, our goal was not to compare datasets based on image similarity but rather in terms of the knowledge deep learning models can extract from them. In this regard, the three proposed metrics-Local, Global, and Paired Similarityoffer valuable insights when given a performance metric, a set of datasets, and a set of networks. Local Similarity helped identify easy and difficult datasets, often influenced by dataset size, variability, and annotation quality. Global Similarity pinpointed datasets that enabled strong generalization across unseen data or, conversely, exhibited high specificity. Paired Similarity highlighted specific dataset pairs that were either very similar or highly dissimilar in terms of the knowledge networks could transfer between them.\n\nDespite being automatically collected and containing potential label errors, duplicated images, and missing demographic information  [39] ,  [47] ,  [48] , the largest datasets demonstrated superior generalization. These datasets consistently ranked among the top performers in the Global Similarity metric, likely due to their high variability in image sources and conditions. However, networks struggled to achieve high performance on these datasets, as reflected in their low Local Similarity scores. This suggests that the very factor contributing to their strong generalization-their diversity-also makes them particularly challenging test datasets. Despite these difficulties, some automatic datasets even outperformed models trained and tested on the same dataset. Notably, AffectNet, the largest dataset in the benchmark, achieved the best results on FE-Test, and MMI, even surpassing models trained directly on those datasets. These findings suggest that large-scale, diverse datasets are best suited for real-life, uncontrolled scenarios. Conversely, datasets recorded with actors in controlled settings exhibited higher Local Similarity scores, likely due to their lower variability, but at the cost of reduced Global Similarity and weaker generalization.\n\nThe size of a dataset significantly influences training outcomes, even when using the same network. Greater variations were observed in cross-validation iterations when training on smaller datasets such as JAFFE, and FE-Test, potentially leading to overfitting. Nevertheless, FE-Test, and WSEFEP performed reasonably well on average. Additionally, images extracted from videos yielded worse results across all metrics compared to static photos. This may be due to limited variation between consecutive frames and the inclusion of low-intensity expressions or transitional frames, which are harder to classify than peak-intensity expressions in static image datasets.\n\nAnalyzing the three similarity metrics-Local, Global, and Paired Similarity-allowed us to identify the overall weakestperforming datasets. The most evident cases are ElderReact, EmoReact, LIRIS-CSE, and BioVidEmo, which performed poorly across all metrics. ElderReact had the lowest Local Similarity score (≈ 0.2) and a similarly low Global Simi-larity score, indicating that networks failed to learn meaningful distinctions between its facial expressions and that training on this dataset did not generalize well to others. EmoReact had the lowest Global Similarity (≈ 0.1), while LIRIS-CSE and BioVidEmo also exhibited poor performance (Local Similarity < 0.5, Global Similarity < 0.2). Notably, these results were close to those of a random classifier, which would achieve a performance of approximately 0.14 in a 7class classification task. The poor results of these datasets may be attributed to their original video format, where expressions do not persist throughout the entire clip. Consequently, after frame sampling, some images may be labeled with an expression they do not truly depict, increasing the difficulty of expression recognition from still images.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "C. Recommendations For Future Fer Studies",
      "text": "Based on our dataset exploration and benchmarking results, we provide the following recommendations for future FER studies:\n\n• Avoid small datasets. Training on datasets with too few samples may lead to overfitting and poor generalization. Instead, use datasets with at least 500 samples.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "D. Recommendations For New Fer Datasets",
      "text": "For studies aiming to construct new FER datasets, we recommend:\n\n• Opt for large-scale data collection. Controlled recordings with actors can be useful for specific studies but often limit dataset size. Automatic collection methods may be simpler and more effective for acquiring large datasets.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "This study presents a comprehensive evaluation of 24 widely used FER datasets to uncover their intrinsic characteristics and determine their suitability for training deep learning models. By employing a rigorous normalization process and enriching the datasets with additional automatic demographic annotations, we have been able to examine key factors such as demographic biases, class imbalances, and data variability. Additionally, three metrics were introduced-Local, Global, and Paired Similarity-that allowed for a detailed quantitative analysis of each dataset's learning difficulty, generalization capability, and the extent of knowledge transfer between datasets, offering a fair and thorough benchmark of current FER datasets.\n\nDespite containing label errors, duplicates, and missing demographic details, large-scale automatically collected datasets demonstrate strong generalization (high Global Similarity) due to their high variability. However, this same diversity poses challenges for deep learning models, leading to lower Local Similarity scores. Conversely, datasets recorded with actors in controlled settings yield higher Local Similarity due to consistent conditions, yet they exhibit reduced Global Similarity and weaker generalization, making them less effective in real-life, uncontrolled scenarios. Smaller datasets, such as JAFFE, and FE-Test, can lead to overfitting, while images extracted from videos generally perform worse than static photos because they often include transitional frames or low-intensity expressions. Notably, datasets like ElderReact, EmoReact, LIRIS-CSE, and BioVidEmo perform poorly across all metrics, underscoring the challenges associated with video formats in FER tasks.\n\nThe exploration and benchmarking of FER datasets allowed us to identify both strengths and weaknesses, from which we derived several recommendations for future FER research. For example, large-scale datasets (with at least 500 samples) such as AffectNet and FER2013 to ensure better generalization, while addressing class imbalances and ensuring demographic diversity to mitigate biases. Additionally, researchers are advised to avoid relying on video-sampled frames due to their inconsistent expression intensities and to perform Paired Similarity analyses when combining datasets to prevent redundancy. For new dataset construction, employing automatic data collection to capture extensive variability, incorporating diverse lighting, settings, and perspectives, including rich metadata, and eliminating duplicate images are crucial steps to enhance both model performance and fairness.\n\nOverall, this study contributes a robust framework for FER dataset evaluation and offers valuable guidelines that will inform future research aimed at developing more reliable and equitable facial expression recognition systems.",
      "page_start": 11,
      "page_end": 12
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: shows the number of images in each",
      "page": 5
    },
    {
      "caption": "Figure 1: Number of images per dataset, in logarithmic scale.",
      "page": 5
    },
    {
      "caption": "Figure 2: a presents the number of individuals included in",
      "page": 5
    },
    {
      "caption": "Figure 2: b shows the average number of",
      "page": 5
    },
    {
      "caption": "Figure 2: (a) Number of users by dataset. (b) Average number",
      "page": 5
    },
    {
      "caption": "Figure 3: Age histogram of the combined datasets.",
      "page": 6
    },
    {
      "caption": "Figure 4: Large-scale automated datasets–AffectNet,",
      "page": 6
    },
    {
      "caption": "Figure 4: illustrates the gender distribu-",
      "page": 6
    },
    {
      "caption": "Figure 4: (Left) Gender distribution across datasets. (Right) Age",
      "page": 6
    },
    {
      "caption": "Figure 5: presents the class distribution",
      "page": 6
    },
    {
      "caption": "Figure 5: Class distribution across datasets.",
      "page": 7
    },
    {
      "caption": "Figure 6: presents the results for this metric, broken",
      "page": 7
    },
    {
      "caption": "Figure 6: Local similarity computed per network.",
      "page": 8
    },
    {
      "caption": "Figure 7: illustrates the results, broken down by",
      "page": 8
    },
    {
      "caption": "Figure 7: Global similarity computed per network.",
      "page": 8
    },
    {
      "caption": "Figure 8: presents the",
      "page": 8
    },
    {
      "caption": "Figure 8: Paired similarity of the collected datasets.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data\nUsers Ages Type Samples": "51 21-66 Image 2,856\nN/A N/A Image 210\nN/A N/A Image 5,558\n110 18-76 Image 1,128\n46 N/A Video 1,323\n12 4-12 Video 208\nN/A N/A Image 91,793\nN/A N/A Image 440,000\n116 8-30 Image 404\nN/A 0-70 Image 29,672\n16 N/A Video 810\n90 18-65 Video 430\n63 4-14 Video 1,102\n154 2-8 Image 1,192\n30 20-30 Image 210\n80 6-16 Image 6,366\nN/A N/A Image 35,887\n330 1-70 Video 1,426\n59 10-17 Image 482\n95 1-70 Image 700\n123 18-50 Video 593\n171 19-80 Image 2,052\n67 N/A Image 8,040\n101 18-45 Video 606\n75 19-62 Video 2,900\n576 18-93 Image 1,354\n70 20-30 Image 4,900\n10 N/A Image 219"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Data\nFormat Resolution Color F.crop": "Image Variable RGB Yes\nVideo 1388×1038 RGB No\nVideo 312×418 RGB Yes\nVideo 640×490 Grayscale Yes\nImage 900×900 RGB Yes\nImage 850×947 RGB Yes\nVideo 1280×720 RGB No\nVideo 1280×720 RGB No\nImage Variable RGB Both\nImage 2835×3543 RGB Yes\nImage 640×480 RGB No\nImage 48×48 Grayscale Yes\nImage 150×150 Grayscale Yes\nImage 256×256 Grayscale Yes\nImage 562×762 RGB Yes\nImage Variable RGB Yes\nVideo 800×600 RGB No\nVideo 720×576 RGB Yes\nImage 224×224 Grayscale Yes\nImage 1960×3008 RGB Yes\nImage 681×1024 RGB Yes\nImage 100×100 RGB Yes\nImage 720×576 RGB No\nImage 1725×1168 RGB Yes"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "2",
      "title": "",
      "authors": [
        "Nhfi"
      ],
      "year": "2020",
      "venue": ""
    },
    {
      "citation_id": "3",
      "title": "AffectNet",
      "year": "2017",
      "venue": "AffectNet"
    },
    {
      "citation_id": "4",
      "title": "Image",
      "year": "2017",
      "venue": "Image"
    },
    {
      "citation_id": "5",
      "title": "16 N/A Video 810 ✓ ✓ ✓ ✓ ✓",
      "authors": [
        "Aff",
        "Int"
      ],
      "year": "2016",
      "venue": "16 N/A Video 810 ✓ ✓ ✓ ✓ ✓"
    },
    {
      "citation_id": "6",
      "title": "FER2013",
      "year": "2013",
      "venue": "FER2013"
    },
    {
      "citation_id": "7",
      "title": "",
      "authors": [
        "Nimh-Chefs"
      ],
      "year": "2011",
      "venue": ""
    },
    {
      "citation_id": "9",
      "title": "Lifespan 2004 576 18-93 Image 1,354 ✓ ✓ ✓ ✓ ✓ ✓",
      "venue": "Lifespan 2004 576 18-93 Image 1,354 ✓ ✓ ✓ ✓ ✓ ✓"
    },
    {
      "citation_id": "10",
      "title": "Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest"
    },
    {
      "citation_id": "11",
      "title": "The bodily expressive action stimulus test (beast). construction and validation of a stimulus basis for measuring perception of whole body expression of emotions",
      "authors": [
        "B Gelder",
        "J Van Den",
        "Stock"
      ],
      "year": "2011",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "12",
      "title": "Emotional Expressions Reconsidered: Challenges to Inferring Emotion From Human Facial Movements",
      "authors": [
        "L Barrett",
        "R Adolphs",
        "S Marsella",
        "A Martinez",
        "S Pollak"
      ],
      "year": "2019",
      "venue": "Psychological Science in the Public Interest"
    },
    {
      "citation_id": "13",
      "title": "An argument for basic emotions",
      "authors": [
        "P Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "14",
      "title": "Emotional expression in psychiatric conditions: New technology for clinicians",
      "authors": [
        "K Grabowski",
        "A Rynkiewicz",
        "A Lassalle",
        "S Baron-Cohen",
        "B Schuller",
        "N Cummins",
        "A Baird",
        "J Podgórska-Bednarz",
        "A Pienia",
        "I Łucka"
      ],
      "year": "2019",
      "venue": "Psychiatry and Clinical Neurosciences"
    },
    {
      "citation_id": "15",
      "title": "Application of facial expression studies on the field of marketing",
      "authors": [
        "A Barreto"
      ],
      "year": "2017",
      "venue": "Application of facial expression studies on the field of marketing"
    },
    {
      "citation_id": "16",
      "title": "Using a Social Robot to Evaluate Facial Expressions in the Wild",
      "authors": [
        "S Ramis",
        "J Buades",
        "F Perales"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "17",
      "title": "Facial expression recognition using machine learning and deep learning techniques: A systematic review",
      "authors": [
        "M Mohana",
        "P Subashini"
      ],
      "venue": "SN Computer Science"
    },
    {
      "citation_id": "18",
      "title": "Manual for the facial action coding system",
      "authors": [
        "P Ekman",
        "W Friesen"
      ],
      "year": "1978",
      "venue": "Manual for the facial action coding system"
    },
    {
      "citation_id": "19",
      "title": "Deep facial expression recognition: A survey",
      "authors": [
        "S Li",
        "W Deng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "20",
      "title": "Facial age affects emotional expression decoding",
      "authors": [
        "M Fölster",
        "U Hess",
        "K Werheid"
      ],
      "year": "2014",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "21",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "22",
      "title": "A novel database of children's spontaneous facial expressions (liris-cse)",
      "authors": [
        "R Khan",
        "A Crenn",
        "A Meyer",
        "S Bouakaz"
      ],
      "year": "2019",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "23",
      "title": "Karolinska directed emotional faces",
      "year": "1998",
      "venue": "PsycTESTS Dataset"
    },
    {
      "citation_id": "24",
      "title": "Elderreact: A multimodal dataset for recognizing emotional response in aging adults",
      "authors": [
        "K Ma",
        "X Wang",
        "X Yang",
        "M Zhang",
        "J Girard",
        "L Morency"
      ],
      "year": "2019",
      "venue": "ICMI 2019 -Proceedings of the 2019 International Conference on Multimodal Interaction"
    },
    {
      "citation_id": "25",
      "title": "A novel approach to cross dataset studies in facial expression recognition",
      "authors": [
        "S Ramis",
        "J Buades",
        "F Perales",
        "C Manresa-Yee"
      ],
      "venue": "Multimedia Tools and Applications"
    },
    {
      "citation_id": "26",
      "title": "Natural human face images for emotion recognition",
      "authors": [
        "S Vaidya"
      ],
      "year": "2020",
      "venue": "Natural human face images for emotion recognition"
    },
    {
      "citation_id": "27",
      "title": "Tsinghua facial expression database -a database of facial expressions in chinese young and older women and men: Development and validation",
      "authors": [
        "T Yang",
        "Z Yang",
        "G Xu",
        "D Gao",
        "Z Zhang",
        "H Wang",
        "S Liu",
        "L Han",
        "Z Zhu",
        "Y Tian",
        "Y Huang",
        "L Zhao",
        "K Zhong",
        "B Shi",
        "J Li",
        "S Fu",
        "P Liang",
        "M Banissy",
        "P Sun"
      ],
      "venue": "PLOS ONE"
    },
    {
      "citation_id": "28",
      "title": "From facial expression recognition to interpersonal relation prediction",
      "authors": [
        "Z Zhang",
        "P Luo",
        "C Loy",
        "X Tang"
      ],
      "year": "2018",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "29",
      "title": "The creation and validation of the developmental emotional faces stimulus set",
      "authors": [
        "A Meuwissen",
        "J Anderson",
        "P Zelazo"
      ],
      "year": "2017",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "30",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "31",
      "title": "A database for emotional interactions of the elderly",
      "authors": [
        "K Wang",
        "Z Zhu",
        "S Wang",
        "X Sun",
        "L Li"
      ],
      "year": "2016",
      "venue": "2016 IEEE/ACIS 15th International Conference on Computer and Information Science (ICIS)"
    },
    {
      "citation_id": "32",
      "title": "Biovid emo db: A multimodal database for emotion analyses validated by subjective ratings",
      "authors": [
        "L Zhang",
        "S Walter",
        "X Ma",
        "P Werner",
        "A Al-Hamadi",
        "H Traue",
        "S Gruss"
      ],
      "year": "2016",
      "venue": "2016 IEEE Symposium Series on Computational Intelligence"
    },
    {
      "citation_id": "33",
      "title": "Emoreact: a multimodal approach and dataset for recognizing emotional responses in children",
      "authors": [
        "B Nojavanasghari",
        "T Baltrušaitis",
        "C Hughes",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ser. ICMI '16"
    },
    {
      "citation_id": "34",
      "title": "The child affective facial expression (cafe) set: validity and reliability from untrained adults",
      "authors": [
        "V Lobue",
        "C Thrasher"
      ],
      "year": "2015",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "35",
      "title": "Warsaw set of emotional facial expression pictures: A validation study of facial display photographs",
      "authors": [
        "M Olszanowski",
        "G Pochwatko",
        "K Kuklinski",
        "M Scibor-Rylski",
        "P Lewinski",
        "R Ohme"
      ],
      "year": "2014",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "36",
      "title": "The dartmouth database of children's faces: Acquisition and validation of a new face stimulus set",
      "authors": [
        "K Dalrymple",
        "J Gomez",
        "B Duchaine"
      ],
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "37",
      "title": "Challenges in representation learning: A report on three machine learning contests",
      "authors": [
        "I Goodfellow",
        "D Erhan",
        "P Carrier",
        "A Courville",
        "M Mirza",
        "B Hamner",
        "W Cukierski",
        "Y Tang",
        "D Thaler",
        "D.-H Lee",
        "Y Zhou",
        "C Ramaiah",
        "F Feng",
        "R Li",
        "X Wang",
        "D Athanasakis",
        "J Shawe-Taylor",
        "M Milakov",
        "J Park",
        "R Ionescu",
        "M Popescu",
        "C Grozea",
        "J Bergstra",
        "J Xie",
        "L Romaszko",
        "B Xu",
        "Z Chuang",
        "Y Bengio"
      ],
      "year": "2013",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "38",
      "title": "Collecting large, richly annotated facial-expression databases from movies",
      "authors": [
        "A Dhall",
        "S Member",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2007",
      "venue": "Collecting large, richly annotated facial-expression databases from movies"
    },
    {
      "citation_id": "39",
      "title": "The nimh child emotional faces picture set (nimh-chefs): A new set of children's facial emotion stimuli",
      "authors": [
        "H Egger",
        "D Pine",
        "E Nelson",
        "E Leibenluft",
        "M Ernst",
        "K Towbin",
        "A Angold"
      ],
      "year": "2011",
      "venue": "International Journal of Methods in Psychiatric Research"
    },
    {
      "citation_id": "40",
      "title": "Static facial expression analysis in tough conditions: Data, evaluation protocol and benchmark",
      "authors": [
        "A Dhall",
        "R Goecke",
        "S Lucey",
        "T Gedeon"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)"
    },
    {
      "citation_id": "41",
      "title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression",
      "authors": [
        "P Lucey",
        "J Cohn",
        "T Kanade",
        "J Saragih",
        "Z Ambadar",
        "I Matthews"
      ],
      "year": "2010",
      "venue": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression"
    },
    {
      "citation_id": "42",
      "title": "Faces-a database of facial expressions in young, middle-aged, and older women and men: Development and validation",
      "authors": [
        "N Ebner",
        "M Riediger",
        "U Lindenberger"
      ],
      "year": "2010",
      "venue": "Behavior Research Methods"
    },
    {
      "citation_id": "43",
      "title": "Presentation and validation of the radboud faces database",
      "authors": [
        "O Langner",
        "R Dotsch",
        "G Bijlstra",
        "D Wigboldus",
        "S Hawk",
        "A Van Knippenberg"
      ],
      "year": "2010",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "44",
      "title": "A high-resolution 3d dynamic facial expression database",
      "authors": [
        "L Yin",
        "X Chen",
        "Y Sun",
        "T Worm",
        "M Reale"
      ],
      "year": "2008",
      "venue": "2008 8th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "45",
      "title": "Web-based database for facial expression analysis",
      "authors": [
        "M Pantic",
        "M Valstar",
        "R Rademaker",
        "L Maat"
      ],
      "year": "2005",
      "venue": "2005 IEEE International Conference on Multimedia and Expo"
    },
    {
      "citation_id": "46",
      "title": "A lifespan database of adult facial stimuli",
      "authors": [
        "M Minear",
        "D Park"
      ],
      "year": "2004",
      "venue": "Behavior Research Methods, Instruments, & Computers"
    },
    {
      "citation_id": "47",
      "title": "Coding facial expressions with gabor wavelets (ivc special issue)",
      "authors": [
        "M Lyons",
        "M Kamachi",
        "J Gyoba"
      ],
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "48",
      "title": "Analysis of gender differences in facial expression recognition based on deep learning using explainable artificial intelligence",
      "authors": [
        "C Manresa-Yee",
        "S Ramis",
        "J Buades"
      ],
      "venue": "International Journal of Interactive Multimedia and Artificial Intelligence"
    },
    {
      "citation_id": "49",
      "title": "Cross-domain facial expression recognition: A unified evaluation benchmark and adversarial graph learning",
      "authors": [
        "T Chen",
        "T Pu",
        "H Wu",
        "Y Xie",
        "L Liu",
        "L Lin"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "50",
      "title": "Advances in facial expression recognition: A survey of methods, benchmarks, models, and datasets",
      "authors": [
        "T Kopalidis",
        "V Solachidis",
        "N Vretos",
        "P Daras"
      ],
      "year": "2024",
      "venue": "Information"
    },
    {
      "citation_id": "51",
      "title": "A survey of predictive modeling on imbalanced domains",
      "authors": [
        "P Branco",
        "L Torgo",
        "R Ribeiro"
      ],
      "year": "2016",
      "venue": "ACM Comput. Surv"
    },
    {
      "citation_id": "52",
      "title": "Multi-pie",
      "authors": [
        "R Gross",
        "I Matthews",
        "J Cohn",
        "T Kanade",
        "S Baker"
      ],
      "year": "2008",
      "venue": "Image and Vision Computing"
    },
    {
      "citation_id": "53",
      "title": "Facial expression recognition using convolutional neural network on graphs",
      "authors": [
        "C Wu",
        "L Chai",
        "J Yang",
        "Y Sheng"
      ],
      "year": "2019",
      "venue": "2019 Chinese Control Conference (CCC)"
    },
    {
      "citation_id": "54",
      "title": "Fer-former: Multimodal transformer for facial expression recognition",
      "authors": [
        "Y Li",
        "M Wang",
        "M Gong",
        "Y Lu",
        "L Liu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "55",
      "title": "Training deep networks for facial expression recognition with crowd-sourced label distribution",
      "authors": [
        "E Barsoum",
        "C Zhang",
        "C Ferrer",
        "Z Zhang"
      ],
      "year": "2016",
      "venue": "Proceedings of the 18th ACM International Conference on Multimodal Interaction, ser. ICMI '16"
    },
    {
      "citation_id": "56",
      "title": "Towards a better performance in facial expression recognition: A data-centric approach",
      "authors": [
        "C Mejia-Escobar",
        "M Cazorla",
        "E Martinez-Martin"
      ],
      "year": "2023",
      "venue": "Computational Intelligence and Neuroscience"
    },
    {
      "citation_id": "57",
      "title": "Uncertain label correction via auxiliary action unit graphs for facial expression recognition",
      "authors": [
        "Y Liu",
        "X Zhang",
        "J Kauttonen",
        "G Zhao"
      ],
      "year": "2022",
      "venue": "2022 26th International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "58",
      "title": "Mivolo: Multi-input transformer for age and gender estimation",
      "authors": [
        "M Kuprashevich",
        "I Tolstykh"
      ],
      "year": "2024",
      "venue": "Analysis of Images, Social Networks and Texts"
    },
    {
      "citation_id": "59",
      "title": "Ultralytics yolo",
      "authors": [
        "G Jocher",
        "A Chaurasia",
        "J Qiu"
      ],
      "year": "2024",
      "venue": "Ultralytics yolo"
    },
    {
      "citation_id": "60",
      "title": "Insightface: 2d and 3d face analysis project",
      "authors": [
        "J Guo",
        "J Deng",
        "X An",
        "J Yu",
        "B Gecer"
      ],
      "year": "2024",
      "venue": "Insightface: 2d and 3d face analysis project"
    },
    {
      "citation_id": "61",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "62",
      "title": "Unveiling the human-like similarities of automatic facial expression recognition: An empirical exploration through explainable ai",
      "authors": [
        "F Gaya-Morey",
        "S Ramis-Guarinos",
        "C Manresa-Yee",
        "J Buades-Rubio"
      ],
      "venue": "Unveiling the human-like similarities of automatic facial expression recognition: An empirical exploration through explainable ai"
    },
    {
      "citation_id": "63",
      "title": "Pain assessment tools in adults with communication disorders: systematic review and meta-analysis",
      "authors": [
        "Á Sabater-Gárriz",
        "J Molina-Mula",
        "P Montoya",
        "I Riquelme"
      ],
      "venue": "BMC Neurology"
    },
    {
      "citation_id": "64",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "65",
      "title": "A convnet for the 2020s",
      "authors": [
        "Z Liu",
        "H Mao",
        "C.-Y Wu",
        "C Feichtenhofer",
        "T Darrell",
        "S Xie"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    }
  ]
}