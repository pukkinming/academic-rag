{
  "paper_id": "2003.11136v1",
  "title": "Joint Deep Cross-Domain Transfer Learning For Emotion Recognition",
  "published": "2020-03-24T22:30:42Z",
  "authors": [
    "Dung Nguyen",
    "Sridha Sridharan",
    "Duc Thanh Nguyen",
    "Simon Denman",
    "Son N. Tran",
    "Rui Zeng",
    "Clinton Fookes"
  ],
  "keywords": [
    "emotional knowledge transfer",
    "emotion recognition",
    "facial expression recognition",
    "speech emotion recognition",
    "transfer learning",
    "cross-domain transfer",
    "joint leaning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Deep learning has been applied to achieve significant progress in emotion recognition. Despite such substantial progress, existing approaches are still hindered by insufficient training data, and the resulting models do not generalize well under mismatched conditions. To address this challenge, we propose a learning strategy which jointly transfers the knowledge learned from rich datasets to source-poor datasets. Our method is also able to learn cross-domain features which lead to improved recognition performance. To demonstrate the robustness of our proposed framework, we conducted experiments on three benchmark emotion datasets including eNTERFACE, SAVEE, and EMODB. Experimental results show that the proposed method surpassed state-of-the-art transfer learning schemes by a significant margin.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The well-known challenge underpinning automatic emotion recognition is the lack of sufficient labelled data to train robust models for classifying emotion. Collecting and accurately labelling emotion categories for large-scale datasets are not only costly and time consuming, but also require specific skills and knowledge  Zhou et al. (2017) . Remedies to overcome this limitation are sought to exploit the full potential of advances in deep learning techniques to improve the recognition performance on available limited datasets.\n\nTo address the problem of data scarcity in emotion classification, transfer learning has been widely adopted  Zhang et al. (2017) ;  Ng et al. (2015) ;  Jung et al. (2015) ;  Hu et al. (2018) . As shown in the literature, existing methods have made use of pretrained models that have been well trained on one dataset and fine-tune them on novel ones. Experimental results show that the knowledge captured by pre-trained models on non-target datasets can be well transferred to target ones via fine-tuning. However, these efforts have only attempted to transfer the learned emotional knowledge across various datasets within a single domain. It is shown that different domains, e.g., visual and auditory domain, provide complementary information for understanding human's emotion and thus could enrich emotion recognition models  Hu et al. (2018) ;  Ji et al. (2019) ;  Zhang et al. (2017) . However, transferring knowledge across various domains, e.g., from visual to auditory domain and vice versa, is a challenging task. This kind of transfer learning also poses a greater challenge when the training/testing is performed on different datasets; considerable drop in performance is often observed due to distribution shift across datasets  Torralba and Efros (2011) .\n\nTo tackle the task of transferring emotional knowledge learned across multiple domains and on multiple resource-poor datasets without suffering from distribution shift, we propose a joint deep cross-domain learning method that aims to learn cross-domain knowledge and jointly transfer the learned knowledge from rich datasets to sourcepoor datasets to improve the performance of emotion recognition. The transferring of features from rich datasets to source-poor datasets is performed concurrently on the source-poor datasets in an end-to-end fashion.\n\nOur method is inspired by the face recognition method proposed in  Sun et al. (2014) , which simultaneously learns facial features using two supervisory channels: face identification and face verification. The framework enables learning compact yet discriminative features that reduce intra-person variations while enlarging inter-person differences. However, unlike  Sun et al. (2014) , in our method, pairs of training samples are taken from two different resource-poor datasets, rather than from the same rich dataset. Our proposed joint loss function has quite similar objectives to that proposed by  [Ji et al. (2019) ]. This means that the main focus of these loss functions is to minimize intra-class variations, and maximize inter-class variations to be able to learn effectively across multiple datasets. To develop a novel idea, we propose a joint deep cross-domain learning algorithm, in which three supervisory signals including two emotion classifiers and an emotion matching signal are simultaneously jointly learned in a single framework. The focus of these two emotion classifiers is to accurately detect all types of emotions from different domains using cross-entropy loss, whereas the aim of emotion matching is to match a pair of samples to determine if they correspond to the same emotion or not relying on contrastive loss. The network proposed by  [Ji et al. (2019) ] composes an Intra-category Common feature representation channel (IC), an Inter-category Distinction feature representation channel (ID) for facial expression representation, and a fusion network combining two channel features for facial expression recognition in cross databases. While the IC channel learns common features of intra-category facial expressions for the common representation by minimizing the distance between such extracted common features, the ID channel learns Distinction features of different categories for the representation of facial expressions by maximizing distances of samples in different categories. Therefore, the main difference between our approach and one proposed by  [Ji et al. (2019) ] is that while the contrastive loss is exploited to match a paired sample,  [Ji et al. (2019) ] have utilized the distance method. In addition to this, three supervisory signals of our architecture are concurrently jointly learned, while the IC channel, the ID channel, and the ICID fusion network are sequentially learned. As shown in our experimental results, the proposed learning framework is able to effectively learn cross-domain features and well generalize on different poorly resourced and disjoint datasets. To this end, we make the following contributions,\n\n• We investigate a relatively unexplored problem: how to effectively train an emotion recognition model on various multi-domain datasets when some or all the datasets are resource poor, in an end-to-end fashion. We address this problem by proposing a joint deep cross-domain learning method. Unlike existing works, e.g.,  Hu et al. (2018) ;  Ji et al. (2019) ;  Zhang et al. (2017) , our learning method is jointly performed on multiple datasets and thus is able to learn not only crossdomain features but also cross-dataset features.\n\n• We investigate Group Normalization (GN) technique  Wu and He (2018)  with different mini batches for emotion recognition. Furthermore, we report an interesting finding that GN achieves very competitive recognition accuracy with small mini batches of size of 2. We found that using small size batches did not negatively affect cross-domain transfer, yet significantly saved memory consumed in training without sacrificing the recognition accuracy.\n\n• We conduct extensive experiments and develop various baselines, including face expression model, speech emotion model, and cross-domain transfer model using various off-the-shelf models to validate the proposed method.\n\nThe remainder of this paper is organized as follows: Section 2 briefly reviews related work; Section 3 describes our proposed method; Section 4 presents experiments and results; and Section 5 concludes our paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Fine-tuning: Transfer learning has been extensively applied in emotion recognition on visual data  Zhang et al. (2017) ;  Ng et al. (2015) . The most common way is to fine-tune a pre-trained model such as ResNet or AlexNet on specific visual emotion datasets. Such approach was inspired by self-taught learning  He et al. (2015) , and aims to exploit rich representations learned in a source dataset to improve the generalization in a target dataset, thereby alleviating over-fitting when training is done from scratch and with a small amount of training data. For example, in  Ng et al. (2015) , emotion recognition models were fine-tuned from models pre-trained on ImageNet. In  Zhang et al. (2017) , 3D convolutional networks, encoding both spatial and temporal information, were intially trained on a large-scale video dataset and subsequently fine-tuned on a much smaller emotion dataset to learn both audio and visual features.\n\nCross-domain transfer: Cross-domain transfer was investigated in  Hu et al. (2018) ;  Ji et al. (2019) . In particular,  Hu et al. (2018)  initially trained their model on the Largescale Subtle Emotions and Mental States in the Wild database, and then transferred the learned knowledge to a traditional (non-subtle) expression dataset. Similarly, the pretrained model in  Ji et al. (2019)  was learned on two different domains and fine-tuned by fusing the pre/post-trained models with a classification loss. In  Albanie et al. (2018) , facial features learned from face image dataset were transferred to speech domain using distillation  Gupta et al. (2016) .\n\nTraining with multiple datasets: While exploiting cross-domain transfer was a key component of  Hu et al. (2018) ;  Ji et al. (2019)  to overcome insufficient data, the rest of their architecture focused on addressing the domain shift between multiple datasets. Specifically, distribution alignment was adopted in  Hu et al. (2018)  to leverage tasks including subtle facial expression recognition and landmark detection on disjoint datasets. It was pointed in  Zeng et al. (2018)  that a straightforward combination of multiple datasets could not lead to any improvement of the recognition performance due to the bias and inconsistency in the annotation of the datasets and the large amount of unlabelled data. To address this issue, an Inconsistent Pseudo Annotations to Latent Truth (IPA2LT) scheme was proposed in  Zeng et al. (2018) . In this scheme, each sample was initially assigned to more than one label by manually annotating or automatically predicting. An end-to-end LTNet was then developed to discover the latent truth from input face images and inconsistent pseudo labels.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Proposed Method",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Pipeline",
      "text": "Our algorithm is designed to learn emotional knowledge across visual and auditory domain, and transfer the cross-domain knowledge to multiple source-poor datasets.\n\nLet D Sv be a source dataset of visual data and D Sa be a source dataset of audio data. These source datasets are used for initial training the model, i.e., building pretrained model. We also have N visual target datasets denoted as D Tv 1 , ..., D Tv N . All target datasets are resource poor, i.e., they contain small numbers of annotated data.\n\nWe first train an initial model P p v using the visual dataset D Sv . This initial model is also considered as pre-trained model and then fine-tuned using one of the target datasets D Tv 1 , ..., D Tv N . This step results in a cross-data fine-tuned model F v . To incorporate audio features, the model F v is adapted onto the auditory dataset D Sa and achieves a cross-domain fine-tuned model G v,a . We finally adapt G v,a to the N target datasets D Tv 1 , ..., D Tv N resulting in N cross-domain fine-tuned models. In order to transfer common knowledge shared by all target domains, the final N cross-domain fine-tuned models are jointly trained. The pipeline of our method is illustrated in Fig.  1 .\n\nWhen training the initial model, the source dataset D Sv is chosen as it contains the largest number of annotated samples. The choice of domain for building the pre-trained model relies on the availability of the domain data. Visual domain data is usually more accessible than audio domain data, and the model can learn better on a domain which has more annotated samples, thereby achieving a better set of parameters for further transferring/fine-tuning.\n\nThe emotional knowledge learned in the pre-trained models can be reused in crossdomain transferring steps. The reason we conduct this cross-domain transferring, i.e., transferring the learned emotional knowledge from the pre-trained model from visual domain to the auditory emotion domain prior to carrying out joint learning is because there is complementary information between visual and auditory domain. It therefore can accumulate useful emotional knowledge to the current model. The learned emotional knowledge of this model is then transferred to multiple datasets using our proposed joint learning algorithm which concurrently minimizes intra-class emotion variance and maximizes inter-class emotion variance on these resource-poor datasets. Our proposed joint learning algorithm is explained in detail in Section 3.3.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Data Pre-Processing",
      "text": "Video stream: After extracting frames from videos, face regions are extracted using an improved Viola-Jones algorithm  Nguyen et al. (2018) . Finally, detected face regions are resized to 64×64×3.\n\nAudio Stream:  Zhang et al. (2018)  extracted three channels of log Mel-spectrograms from segments over all utterances. They fine-tuned a pre-trained AlexNet model on such features, achieving significantly better performance than with hand-crafted features. Inspired by this, we also extract three channel features as follows. First, Melspectrogram segments with size 64 × 64 × 3 (F = 64, T = 64, C = 3) are generated from 1-D speech signals, with F , T , and C denoting the number of Mel-filter banks, the segment length corresponding to the frame number in a context window, and the number of channels of the Mel-spectrogram, respectively. For cross-domain transfer we select values of F and T to match the pre-processed video, while the 3 channels of the Mel-spectrograms are the static, delta, and delta-delta coefficients, similar to  Zhang et al. (2018) . Next, we convert 64 Mel-filter banks from 20 to 8000 Hz into a log Melspectrogram using a 25 ms Hamming window with a 10ms overlap for an utterance. A context window of 64 frames (length 10 ms × 63 + 25 ms = 655 ms) is then applied to the whole log Mel-spectrogram to extract the static 2-D Mel-spectrogram segments (64 × 64) with an overlap size of 30 frames  Zhang et al. (2018) .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Joint Learning",
      "text": "To accumulate the emotional knowledge from visual domain and auditory domain which is subsequently transferred and re-used as initial knowledge for our joint learning Input: Two target domains: D T i = (x i , l i ) and D T j = (x j , l j ), parameters θ class1 , θ class2 , and θ match transferred from our pre-trained models through continuous fine-tuning, learning rate η(t), t ← 0 1 while not converge do 2 t ← t + 1 sample a number of pair of training samples (x i , x j , l i , l j , y ij ) from two mini batches which are correspondingly taken from D T i and D T j ;\n\n3\n\n, where y ij = 1 if l i = l j , and y ij = 0 otherwise.   1 ). During pre-training and (continuous) fine-tuning, these models are learned by maximizing the posterior probability of the ground-truth, focusing on separating features from different classes  Wang et al. (2018) . Given an input feature vector x with its corresponding ground-truth label c, the cross-entropy loss is formulated as follows:\n\nWhere f = Conv(x, θ e ), Conv(•) is defined by our proposed convolutional neural network (CNN) and is then used as the feature extraction function, x is the input face image or input speech segment for cross-domain transfer, and θ e denotes our CNN parameters to be learned which are randomly initialized when pre-training and are transferred from our pre-trained models when fine-tuning; c and θ class denote the target class and the softmax layer parameters, respectively; p i is the target probability distribution, p i = 0 for all i except p c = 1 for the target class c, pi is the predicted probability distribution.\n\nThe set of accumulated parameters of our model fine-tuning on the final domain is now transferred and reused as initial knowledge for our joint learning on two different visual domains by simultaneously optimizing two cross-entropy losses of E.q.\n\n( 1) and a contrastive loss, shown in E.q.( 2), as originally proposed by  Hadsell et al. (2006)  for dimensionality reduction (see Fig.  1 ).\n\nThe contrastive loss is calculated as follows,\n\nwhere f i = Conv(x i , θ e ), f j = Conv(x j , θ e ), x i and x j are taken from D Tv i and D Tv j , and m is the predefined margin. If x i and x j are from the same emotion, then y ij = 1. In this case, E.q. (2) minimizes the L2 distance between the two feature vectors: f i and f j . If x i and x j are from different emotions, then y ij = 0. A mathematical framework similar to ours is proposed in  Nagrani et al. (2018) ;  Sun et al. (2014) ;  Nagrani et al. (2018) . However, x i and x j are a pair of face images always taken from one domain and θ e is randomly initialized in  Sun et al. (2014) . In contrast, in our joint learning algorithm, (x i , x j ) is a pair of samples taken from D Tv i and D Tv j , and θ e is transferred from a set of learned parameters of our model fine-tuning on the final auditory domain. Likewise, only a cross-entropy loss function in E.q.( 1) was optimized in  Nagrani et al. (2018)  and only a contrastive loss function was optimized by  Nagrani et al. (2018) , whereas all supervisory signals consisting of two cross-entropy loss functions (L 1 and L 2 ) (E.q.( 1)) and a contrastive loss function (L c ) (E.q.( 2)) are jointly optimized in our framework.\n\nTherefore, the training loss function L joint for our joint learning algorithm is defined as follows,\n\nwhere λ 1 , λ 2 , and λ 3 are hyper-parameters. What is the motivation for such joint learning? While the aim of classification signals is to classify each sample from multiple domains into different types of emotions by maximizing inter-class variations, the objective of the matching signal is to predict whether a pair of samples belongs to the same emotion by reducing intra-class variations.\n\nOur goal is to learn the parameters θ e in the feature extraction function Conv(•), while θ match , θ class1 , and θ class2 are parameters introduced to propagate two emotion classification signals and an emotion matching signal. The parameters are updated by stochastic gradient descent. The emotion classification and emotion matching gradients are weighted by a hyperparameter. Our joint learning algorithm is summarized in Algorithm 1. During testing, θ e is used for feature extraction.\n\nHow to generate paired samples for our joint learning algorithm? Paired samples are input for the contrastive loss, and are usually generated from two datasets. However, this approach may create a huge number of pairs, consequently slowing finetuning. We instead propose to generate paired samples from each mini batch. We suppose that two mini batches of size K from two corresponding datasets are fed into our framework at every iteration for joint learning, and a total of K 2 pairs of samples are generated. Our model can learn better if all pairs of samples are generated from Our joint learning using continuous fine-tuning A EMO Model (Fine-tuned) on visual SAVEE and visual eNTERFACE by optimizing our joint loss function each pair of mini batches at each iteration. Therefore, it is required for our joint learning algorithm to use a small mini batch since in addition to saving memory, all pairs of samples can be generated without significantly increasing the number of pairs.\n\nUnfortunately, batch normalization (BN)  Ioffe and Szegedy (2015)  performs well only with a large mini batch, while with a small mini batch it inaccurately estimates the mini batch statistics, increasing model error  Wu and He (2018) . In contrast, our experimental results further confirm that Group Normalization (GN)  Wu and He (2018)  achieves similar performance with batch sizes from 2 to 512. Following this finding, we exploit GN with a mini batch-size of 2 for our approach. We note that GN with a mini batch-size of 2 does not affect a natural transfer process since GN is independently computed along mini batches, while with a mini batch-size of 2 BN becomes a linear layer y = γ σ (x -µ + β), where µ and σ were previously computed from the pretrained model and frozen  He et al. (2015) . These are the main reasons why we initially investigate different mini batch-sizes for GN before carrying out the extensive crossdomain transfer and joint learning experiments.\n\nOur general joint deep cross-domain learning framework was formulated for N visual and M audio source databases and J visual and K audio target databases. To demonstrate and evaluate the performance our method we use a small number for N , M , J, and K. Specifically N = 1, M = 1, J = 1, and K = 3. For our joint learning algorithm, our system was formulated for learning on 2 different visual datasets.",
      "page_start": 5,
      "page_end": 8
    },
    {
      "section_name": "Experiments & Results",
      "text": "Dataset Details: The eNTERFACE dataset  Martin et al. (2006)  is an audio-visual dataset which has 44 subjects and includes a total of 1293 video sequences in which the proportion of sequences corresponding to women and men are 23% and 77%, re-spectively. Subjects were asked to express 6 discrete emotions: anger, disgust, fear, happiness, sadness, and surprise  Martin et al. (2006) .\n\nThe SAVEE dataset  Haq and Jackson (2009)  is an audio-visual dataset which was recorded by higher degree researchers (aged from 27 to 31 years) at the University of Surrey, and four native male British speakers. All of them were also required to speak and express seven discrete emotions: anger, disgust, fear, happiness, sadness, surprise, and neutral. The dataset contains 120 utterances per speaker  Haq and Jackson (2009) .\n\nThe EMO-DB dataset  Burkhardt et al. (2005)  is an acted speech corpus containing 535 emotional utterances with seven different acted emotions listed: anger, disgust, fear, happiness, sadness, surprise, and neutral. These emotions were stimulated by five male and five female professional native German-speaking actors, generating five long and five short German utterances used in daily communication. These actors were asked to read predefined sentences in the targeted emotions.\n\nNetwork architecture: The network we use for all stages is VGG-16. The network includes four convolutional layers with 64, 128, 256, and 512 3×3 filters and stride 1, respectively, the output of each convolutional layer is activated using leaky rectified linear units (LReLU)  Maas et al. (2013)  before being normalized by Group Normalization  Wu and He (2018) , afterwhich the network has four fully-connected layers with hidden units of dimensionality 512, 128, 32, and 6, respectively. A LReLU activation function is also exploited after each fully-connected layer.\n\nImplementation details: For pre-training, we apply the variance scaling initialiser of  Zhang et al. (2018)  that has been recently proposed for network weight initialization for all convolutions. Group Normalization  Wu and He (2018)  and Dropout (0.5) are exploited in all convolutional layers. We train our model for 100,000 iterations with a learning rate of 10e-5. For cross-domain transfer using (continuous) fine-tuning, we fine-tune only fully-connected layers when transferring between visual domains, and continuously fine-tune all layers when transferring from visual to audio domains. For joint learning using continuous fine-tuning, we set λ 1 = 1, λ 2 = 1, and λ 3 = 0.01, and reduce the learning rate to 10e-6. The models are trained for 20,000 iterations and the mini batch-size is fixed at 2. In all sets of our experiments, we apply k-fold cross-validation, the original training data is randomly divided into k equal parts. Of the k-parts, one of them is fixed as the validation data for testing the model, and the other k-1 parts are used as training data. The cross-validation process is then repeated 5 times. We only focus on recognizing 6 emotions: anger, disgust, fear, happiness, sadness, and surprise.\n\nVideo emotion recognition results: Experimental results of video emotion recognition models: V eNTER Model, which is pre-trained on video eNTERFACE, and V SAV Model (Fine-tuned), which fine-tunes only fully-connected layers of the pretrained V eNTER Model on video SAVEE are illustrated in Table  2  and Table 4 , respectively. As shown in Table  2  and Table 4 , the V SAV Model (Fine-tuned) and the V eNTER Model achieve state-of-the-art performances with approximately 99% and 93% recognition accuracy for the within-corpus scenario on visual SAVEE and visual eNTERFACE, accordingly. However, these performances decrease significantly  when evaluating these models in regard to a cross-corpus scenario as illustrated in Table  7 . For example, the V SAV Model (Fine-tuned) shows a significant drop of 79% in emotion recognition accuracy when evaluated on visual eNTERFACE. Similarly, V eNTER Model achieves only 35% emotion recognition accuracy when testing on visual SAVEE (see Table  7 ). This significant drop is attributed to the distribution shift across datasets.\n\nCross-domain transfer results: We continuously fine-tune all layers of our V SAV Model (Fine-tuned) on audio SAVEE (see Table  3 ), and then on audio EMODB (see Table  5 ) in an end-to-end fashion as described in detail in Table  1 . As shown in Table 3, the A SAV Model (Fine-tuned) achieves very promising recognition accuracy (62%) standing in second place, which is around 3% higher than the accuracy obtained by the A SAV Model which is trained from scratch. Similarly, the A EMO Model (Fine-tuned) demonstrates best performance (89%) in comparison with other state-ofthe-art speech emotion recognition models and is significantly better than that of the A EMO Model (67%), as shown in Table  5 .\n\nTo further show the effectiveness of our proposed model with respect to handling insufficient data, we conduct two additional sets of experiments as follows: 1) We fine-tune fully-connected layers of our V SAV Model (Fine-tuned) on 1-part training data of video eNTERFACE (see Table  2 ), and 2) We fine-tune fully-connected layers of our pre-trained V eNTER Model on 1-part training data of video SAVEE (see the   Noroozi et al. (2019)  0.56 CNN Model  Noroozi et al. (2019)  0.97 V SAV Model (Fine-tuned, 1-part) 0.96 V SAV Model (Fine-tuned, BS=2) 0.99 V SAV Model (Fine-tuned, BS=512) 0.99   4 , despite being fine-tuned on only 1-part of the training data, the V eNTER Model (Fine-tuned, 1-part) and V SAV Model (Fine-tuned, 1-part) achieve very competitive recognition accuracies (92% and 96%) on video eNTERFACE and video SAVEE, respectively in comparison with those when these models learn on 4-parts training data. We re-implement other cross-domain transfer approaches using off-the-shelf models including all versions of ResNet  He et al. (2016b ), of Inception Szegedy et al. (2015) , of  MobileNet Howard et al. (2017) , and we also re-implement meta transfer learning  Nguyen et al. (2018)  to further compare and thoroughly demonstrate the efficiency of our proposed model. As shown in the Table  6 , our model achieves the lowest errors when fine-tuned on video eNTERFACE and on video SAVEE, accordingly. Moreover, while ResNet  He et al. (2016b) , Inception  Szegedy et al. (2015) , and MobileNet  Howard et al. (2017)  over-fit (shown as '-' in the Table  6 ) when fine-tuned on video eNTERFACE, our model performs very well, achieving model error of 0.136. When fine-tuned on video SAVEE, these off-the-shelf models achieve quite competitive model errors (approximately 1.8) compared to those obtained by Cifar  Ouyang and Wang (2013)  and Vgg 19  Simonyan and Zisserman (2014) , which are however significantly higher than our model error (0.041).\n\nOur cross-domain systems outperforms significantly the recent state-of-the-art emotion recognition systems relying on the transfer learning method which fine-tunes offthe-shelf/pre-trained models. This can be explained that the representational structures that are unrelated to emotion are still remained in off-the-shelf/pre-trained models and the extracted features are usually vulnerable to identity variations, leading to degrading the performance of these emotion recognition systems fine-tuning off-the-shelf/pretrained models on the emotion dataset.\n\nAs analysed earlier, our models achieved from the cross-domain transfer stage still poorly perform on new domains However, our objective of this stage is to accumulatively learn emotional knowledge from visual domains and from auditory domains which is then transferred to our joint learning on multiple visual domains.\n\nJoint learning results: As explained earlier, the transfer learning task poses a greater challenge when the training/testing is performed on different datasets; significant drop in performance is often observed owning to distribution shift across datasets. To address the transferring emotional knowledge task learned across multiple domains and on multiple resource-poor datasets without suffering from distribution shift, we have proposed a joint deep cross-domain learning approach, focusing on learning crossdomain knowledge. To demonstrate the effectiveness of our joint deep cross-domain learning method, we conduct three baseline systems: 1) Our network architecture is learned only on eNTERFACE training set using classification loss and evaluated on eNTERFACE testing set and SAVEE testing set, 2) Our network architecture is learned only on SAVEE training set using classification loss and evaluated on SAVEE testing set and eNTERFACE testing set, and 3) Our network architecture is trained on both eN-TERFACE training set and SAVEE training set using classification loss and evaluated on eNTERFACE testing set and SAVEE testing set (see Table  1  for further description of each experiment and notation). Whereas joint learning experiments are to train our network architecture on both eNTERFACE training set and SAVEE training set using our proposed joint loss function and evaluate on eNTERFACE testing set and SAVEE testing set (see Table  1 ) . As shown in Table  7 , the V SAV eNTER Model does not show a significant improvement in recognition accuracy, despite learning with multiple datasets enlarged by simply combining visual eNTERFACE and visual SAVEE. This model achieves only 64% recognition accuracy which is the same as that in V eNTER Model and 4% higher than that achieved by the V SAV Model (Finetuned) while validating on both video eNTERFACE and video SAVEE. Although, the V SAV eNTER Model is learned by jointly optimizing two cross-entropy losses, this model still suffers from a distribution shift across datasets. In contrast, as can be seen in Table  7 , the performance of our proposed model is vastly improved when learning with our proposed joint learning algorithm as described in detail in Table  1 , which simultaneously optimizes two cross-entropy losses and a contrastive loss. Through this approach, our proposed joint learning model achieves 66% and 94% emotion recognition accuracy tested on visual SAVEE and visual eNTERFACE accordingly corresponding an average accuracy of 80%, which shows the best performance compared to our baseline systems (V SAV Model (Fine-tuned), V eNTER Model, and V SAV eNTER Model) (see Table  7 ).\n\nThus this demonstrates that our proposed model can generalize well across multiple datasets, hence successfully tackling distribution shift across datasets. Moreover, it is worth noting that our proposed joint learning method is fine-tuned on our pre-trained model with a small mini batch-size of 2 rather than training from scratch, therefore, saving a huge amount of time and memory resources for training without sacrificing emotion recognition accuracy. The choice of a mini batch-size of 2 is due to the following motivation: the V SAV Model (Fine-tuned) and the V eNTER Model using a mini batch-size of 2 obtain very competitive performance compared to a mini batch-size of 512. Thus, with a small mini batch-size, we still can carry out extensive experiments with a limited memory/GPU and the number of paired samples for the contrastive loss is only 4 for each mini batch-size instead of K 2 (K > 2). The reason we are unable to compare our proposed joint learning algorithm with the state-of-the-art models is that as far as we know this is the first research in addressing a training problem with multiple insufficient datasets where there is a distribution shift across datasets. As such, we only compare our joint learning results with those of our baseline models.",
      "page_start": 9,
      "page_end": 13
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper, we have developed a framework which is able to learn well with multiple poorly resourced and disjoint emotion datasets by simultaneously minimiz-ing intra-class variance and maximizing inter-class variance. By integrating the crossdomain transfer using a continuous fine-tuning strategy, our proposed framework has successfully transferred emotional learned knowledge between modalities such as from one visual domain to another visual domain, from the visual domain to the audio domain, and then to multiple domains. To the best of our knowledge, our joint learning algorithm is the first study aimed at resolving the training problem with multiple poorly resourced emotion datasets. To validate the effectiveness of our proposed framework in learning, extensive experiments have been conducted in visual and speech emotion recognition and demonstrate that our framework performs significantly better than other state-of-the-art approaches involving three emotion datasets: eNTERFACE, SAVEE, and EMODB.",
      "page_start": 13,
      "page_end": 13
    },
    {
      "section_name": "Acknowledgement",
      "text": "This research was supported by an Australian Research Council (ARC) Discovery grant DP140100793.",
      "page_start": 14,
      "page_end": 14
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Pipeline of our proposed method. We initially pre-train our model on one visual domain then",
      "page": 4
    },
    {
      "caption": "Figure 1: When training the initial model, the source dataset DSv is chosen as it contains the",
      "page": 5
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Set of experiment": "VeNTER Model",
          "Description": "Pre-training on visual eNTERFACE dataset"
        },
        {
          "Set of experiment": "(Fine-\nVSAV Model\ntuned)",
          "Description": "Fine-tuning\nfully-connected\nlayers\nof\nthe\npre-trained\nVeNTER Model on visual SAVEE dataset"
        },
        {
          "Set of experiment": "ASAV Model",
          "Description": "Pre-training on audio SAVEE"
        },
        {
          "Set of experiment": "(Fine-\nASAV Model\ntuned)",
          "Description": "Fine-tuning VSAV Model (Fine-tuned) on audio SAVEE"
        },
        {
          "Set of experiment": "AEMO Model",
          "Description": "Pre-training on audio EMODB"
        },
        {
          "Set of experiment": "AEMO Model (Fine-\ntuned)",
          "Description": "Fine-tuning ASAV Model (Fine-tuned) on audio EMODB"
        },
        {
          "Set of experiment": "VeNTER+SAV Model",
          "Description": "Pre-training on visual eNTERFACE and visual SAVEE"
        },
        {
          "Set of experiment": "Our\njoint\nlearning\nalgorithm",
          "Description": "Our joint\nlearning using continuous ﬁne-tuning AEMO Model\n(Fine-tuned) on visual SAVEE and visual eNTERFACE by op-\ntimizing our joint loss function"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table 3: ), and then on audio EMODB (see Ta-",
      "data": [
        {
          "CNN Model Noroozi et al. (2019)": "Fine-Tuned Pre-trained C3D Zhang et al. (2017)",
          "0.64": "0.54"
        },
        {
          "CNN Model Noroozi et al. (2019)": "Video C3D + DBN Nguyen et al. (2017)",
          "0.64": "0.83"
        },
        {
          "CNN Model Noroozi et al. (2019)": "V eNTER Model (Fine-tuned, 1-part)",
          "0.64": "0.92"
        },
        {
          "CNN Model Noroozi et al. (2019)": "V eNTER Model (BS=2)",
          "0.64": "0.93"
        },
        {
          "CNN Model Noroozi et al. (2019)": "V eNTER Model (BS=512)",
          "0.64": "0.93"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 3: ), and then on audio EMODB (see Ta-",
      "data": [
        {
          "SVM Noroozi et al. (2019)": "SVM-PCA Noroozi et al. (2019)",
          "0.49": "0.43"
        },
        {
          "SVM Noroozi et al. (2019)": "RF Noroozi et al. (2019)",
          "0.49": "0.56"
        },
        {
          "SVM Noroozi et al. (2019)": "RF-PCA Noroozi et al. (2019)",
          "0.49": "0.53"
        },
        {
          "SVM Noroozi et al. (2019)": "AlexNet Zhang et al. (2018)",
          "0.49": "0.69"
        },
        {
          "SVM Noroozi et al. (2019)": "A SAV Model",
          "0.49": "0.59"
        },
        {
          "SVM Noroozi et al. (2019)": "A SAV Mode (Fine-tuned)",
          "0.49": "0.62"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table 4: ). It is noted that 4-parts of training data are used for pre-training and fine-",
      "data": [
        {
          "SVM-PCA Noroozi et al. (2019)": "RF Noroozi et al. (2019)",
          "0.52": "0.56"
        },
        {
          "SVM-PCA Noroozi et al. (2019)": "CNN Model Noroozi et al. (2019)",
          "0.52": "0.97"
        },
        {
          "SVM-PCA Noroozi et al. (2019)": "V SAV Model (Fine-tuned, 1-part)",
          "0.52": "0.96"
        },
        {
          "SVM-PCA Noroozi et al. (2019)": "V SAV Model (Fine-tuned, BS=2)",
          "0.52": "0.99"
        },
        {
          "SVM-PCA Noroozi et al. (2019)": "V SAV Model (Fine-tuned, BS=512)",
          "0.52": "0.99"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 4: ). It is noted that 4-parts of training data are used for pre-training and fine-",
      "data": [
        {
          "ComParE set 2016": "Alexnet-DTPM 2018",
          "0.86": "0.76"
        },
        {
          "ComParE set 2016": "DCNN-DTPM 2018",
          "0.86": "0.84"
        },
        {
          "ComParE set 2016": "Fine-Tuned Alexnet-Average 2018",
          "0.86": "0.83"
        },
        {
          "ComParE set 2016": "Fine-Tuned Alexnet-DTPM 2018",
          "0.86": "0.87"
        },
        {
          "ComParE set 2016": "A EMO Model",
          "0.86": "0.67"
        },
        {
          "ComParE set 2016": "A EMO Model (Fine-tuned)",
          "0.86": "0.89"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table 6: The training error of our cross-domain transfer models in comparison with other model errors",
      "data": [
        {
          "Cifar 2013\nInception v1 2015\nMobilenet v2 2017\nResnet v1 152 2016b\nResnet v2 50 2016a\nVgg 19 2014\nTransfer learning 2018": "Our proposed model",
          "1.773\n1.812\n1.796\n1.886\n1.787\n1.792\n0.44": "0.041",
          "1.3263\n-\n-\n-\n-\n1.56\n0.91": "0.136"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotion recognition in speech using cross-modal transfer in the wild",
      "authors": [
        "S Albanie",
        "A Nagrani",
        "A Vedaldi",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Emotion recognition in speech using cross-modal transfer in the wild"
    },
    {
      "citation_id": "2",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Proceedings of Interspeech, Lissabon"
    },
    {
      "citation_id": "3",
      "title": "The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing",
      "authors": [
        "F Eyben",
        "K Scherer",
        "B Schuller",
        "J Sundberg",
        "E Andr",
        "C Busso",
        "L Devillers",
        "J Epps",
        "P Laukka",
        "S Narayanan",
        "K Truong"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Cross modal distillation for supervision transfer",
      "authors": [
        "S Gupta",
        "J Hoffman",
        "J Malik"
      ],
      "year": "2016",
      "venue": "Cross modal distillation for supervision transfer"
    },
    {
      "citation_id": "5",
      "title": "Dimensionality reduction by learning an invariant mapping",
      "authors": [
        "R Hadsell",
        "S Chopra",
        "Y Lecun"
      ],
      "year": "2006",
      "venue": "Dimensionality reduction by learning an invariant mapping"
    },
    {
      "citation_id": "6",
      "title": "Speaker-dependent audio-visual emotion recognition",
      "authors": [
        "S Haq",
        "P Jackson"
      ],
      "year": "2009",
      "venue": "Proc. Int. Conf. on Auditory-Visual Speech Processing (AVSP'08)"
    },
    {
      "citation_id": "7",
      "title": "Identity mappings in deep residual networks",
      "authors": [
        "K He",
        "Z Xiangyu",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Identity mappings in deep residual networks"
    },
    {
      "citation_id": "8",
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2015",
      "venue": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification"
    },
    {
      "citation_id": "9",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Deep residual learning for image recognition"
    },
    {
      "citation_id": "10",
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "authors": [
        "A Howard",
        "M Zhu",
        "B Chen",
        "D Kalenichenko",
        "W Wang",
        "T Weyand",
        "M Andreetto",
        "H Adam"
      ],
      "year": "2017",
      "venue": "Mobilenets: Efficient convolutional neural networks for mobile vision applications"
    },
    {
      "citation_id": "11",
      "title": "Deep multi-task learning to recognise subtle facial expressions of mental states",
      "authors": [
        "G Hu",
        "L Liu",
        "Y Yuan",
        "Z Yu",
        "Y Hua",
        "Z Zhang",
        "F Shen",
        "L Shao",
        "T Hospedales",
        "N Robertson",
        "Y Yang"
      ],
      "year": "2018",
      "venue": "Deep multi-task learning to recognise subtle facial expressions of mental states"
    },
    {
      "citation_id": "12",
      "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
      "authors": [
        "S Ioffe",
        "C Szegedy"
      ],
      "year": "2015",
      "venue": "Batch normalization: Accelerating deep network training by reducing internal covariate shift"
    },
    {
      "citation_id": "13",
      "title": "Cross-domain facial expression recognition via an intra-category common feature and inter-category distinction feature fusion network",
      "authors": [
        "Y Ji",
        "Y Hu",
        "Y Yang",
        "F Shen",
        "H Shen"
      ],
      "year": "2019",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "14",
      "title": "Joint fine-tuning in deep neural networks for facial expression recognition",
      "authors": [
        "H Jung",
        "S Lee",
        "J Yim",
        "S Park",
        "J Kim"
      ],
      "year": "2015",
      "venue": "Joint fine-tuning in deep neural networks for facial expression recognition"
    },
    {
      "citation_id": "15",
      "title": "Rectifier nonlinearities improve neural network acoustic models",
      "authors": [
        "A Maas",
        "A Hannun",
        "A Ng"
      ],
      "year": "2013",
      "venue": "ICML Workshop on Deep Learning for Audio, Speech and Language Processing"
    },
    {
      "citation_id": "16",
      "title": "The eNTERFACE' 05 audio-visual emotion database, in: Data Engineering Workshops",
      "authors": [
        "O Martin",
        "I Kotsia",
        "B Macq",
        "I Pitas"
      ],
      "year": "2006",
      "venue": "Proceedings. 22nd International Conference on"
    },
    {
      "citation_id": "17",
      "title": "Learnable pins: Cross-modal embeddings for person identity",
      "authors": [
        "A Nagrani",
        "S Albanie",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "Learnable pins: Cross-modal embeddings for person identity"
    },
    {
      "citation_id": "18",
      "title": "Seeing voices and hearing faces: Crossmodal biometric matching",
      "authors": [
        "A Nagrani",
        "S Albanie",
        "A Zisserman"
      ],
      "year": "2018",
      "venue": "CVPR 2018",
      "doi": "10.1109/CVPR.2018.00879"
    },
    {
      "citation_id": "19",
      "title": "Deep learning for emotion recognition on small datasets using transfer learning",
      "authors": [
        "H Ng",
        "V Nguyen",
        "V Vonikakis",
        "S Winkler"
      ],
      "year": "2015",
      "venue": "ICMI"
    },
    {
      "citation_id": "20",
      "title": "Meta transfer learning for facial emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "I Abbasnejad",
        "D Dean",
        "C Fookes"
      ],
      "year": "2018",
      "venue": "Meta transfer learning for facial emotion recognition"
    },
    {
      "citation_id": "21",
      "title": "Deep spatiotemporal feature fusion with compact bilinear pooling for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "D Dean",
        "C Fookes"
      ],
      "year": "2018",
      "venue": "Deep spatiotemporal feature fusion with compact bilinear pooling for multimodal emotion recognition"
    },
    {
      "citation_id": "22",
      "title": "Deep spatio-temporal features for multimodal emotion recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "A Ghasemi",
        "D Dean",
        "C Fookes"
      ],
      "year": "2017",
      "venue": "2017 IEEE Winter Conference on Applications of Computer Vision (WACV)"
    },
    {
      "citation_id": "23",
      "title": "Audiovisual emotion recognition in video clips",
      "authors": [
        "F Noroozi",
        "M Marjanovic",
        "A Njegus",
        "S Escalera",
        "G Anbarjafari"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2017.2713783"
    },
    {
      "citation_id": "24",
      "title": "Joint deep learning for pedestrian detection",
      "authors": [
        "W Ouyang",
        "X Wang"
      ],
      "year": "2013",
      "venue": "Joint deep learning for pedestrian detection",
      "doi": "10.1109/ICCV.2013.257"
    },
    {
      "citation_id": "25",
      "title": "Very deep convolutional networks for large-scale image recognition",
      "authors": [
        "K Simonyan",
        "A Zisserman"
      ],
      "year": "2014",
      "venue": "Very deep convolutional networks for large-scale image recognition"
    },
    {
      "citation_id": "26",
      "title": "Deep learning face representation by joint identification-verification",
      "authors": [
        "Y Sun",
        "Y Chen",
        "X Wang",
        "X Tang"
      ],
      "year": "2014",
      "venue": "Deep learning face representation by joint identification-verification"
    },
    {
      "citation_id": "27",
      "title": "Going deeper with convolutions",
      "authors": [
        "C Szegedy",
        "W Liu",
        "Y Jia",
        "P Sermanet",
        "S Reed",
        "D Anguelov",
        "D Erhan",
        "V Vanhoucke",
        "A Rabinovich"
      ],
      "year": "2015",
      "venue": "Going deeper with convolutions",
      "doi": "10.1109/CVPR.2015.7298594"
    },
    {
      "citation_id": "28",
      "title": "Unbiased look at dataset bias",
      "authors": [
        "A Torralba",
        "A Efros"
      ],
      "year": "2011",
      "venue": "Unbiased look at dataset bias"
    },
    {
      "citation_id": "29",
      "title": "Cosface: Large margin cosine loss for deep face recognition",
      "authors": [
        "H Wang",
        "Y Wang",
        "Z Zhou",
        "X Ji",
        "D Gong",
        "J Zhou",
        "Z Li",
        "W Liu"
      ],
      "year": "2018",
      "venue": "CVPR 2018",
      "doi": "10.1109/CVPR.2018.00552"
    },
    {
      "citation_id": "30",
      "title": "Group normalization",
      "authors": [
        "Y Wu",
        "K He"
      ],
      "year": "2018",
      "venue": "Group normalization"
    },
    {
      "citation_id": "31",
      "title": "Facial expression recognition with inconsistently annotated datasets",
      "authors": [
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2018",
      "venue": "Facial expression recognition with inconsistently annotated datasets"
    },
    {
      "citation_id": "32",
      "title": "Speech emotion recognition using deep convolutional neural network and discriminant temporal pyramid matching",
      "authors": [
        "S Zhang",
        "T Huang",
        "W Gao"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "33",
      "title": "Learning affective features with a hybrid deep model for audio-visual emotion recognition",
      "authors": [
        "S Zhang",
        "T Huang",
        "W Gao",
        "Q Tian"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Circuits and Systems for Video Technology PP"
    },
    {
      "citation_id": "34",
      "title": "Mask r-cnn with feature pyramid attention for instance segmentation",
      "authors": [
        "X Zhang",
        "G An",
        "Y Liu"
      ],
      "year": "2018",
      "venue": "14th IEEE International Conference on Signal Processing (ICSP)",
      "doi": "10.1109/ICSP.2018.8652371"
    },
    {
      "citation_id": "35",
      "title": "Fine-tuning convolutional neural networks for biomedical image analysis: Actively and incrementally",
      "authors": [
        "Z Zhou",
        "J Shin",
        "L Zhang",
        "S Gurudu",
        "M Gotway",
        "J Liang"
      ],
      "year": "2017",
      "venue": "Fine-tuning convolutional neural networks for biomedical image analysis: Actively and incrementally"
    }
  ]
}