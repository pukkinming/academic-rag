{
  "paper_id": "2308.16207v2",
  "title": "Masa-Tcn: Multi-Anchor Space-Aware Temporal Convolutional Neural Networks For Continuous And Discrete Eeg Emotion Recognition",
  "published": "2023-08-30T04:49:24Z",
  "authors": [
    "Yi Ding",
    "Su Zhang",
    "Chuangao Tang",
    "Cuntai Guan"
  ],
  "keywords": [
    "-Temporal convolutional neural networks (TCN)",
    "emotion recognition",
    "electroencephalogram (EEG) I. INTRODUCTION E MOTION recognition",
    "also known as emotional artificial intelligence [1]",
    "[2]",
    "leverages machine learning to understand human emotions",
    "crucial for addressing emotion-related mental disorders like anxiety",
    "depression",
    "and autism spectrum disorder (ASD). It employs both categorical and dimensional models",
    "with the valence-arousal-dominance (VAD) model [1] being prominent for evaluating"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition from electroencephalogram (EEG) signals is a critical domain in biomedical research with applications ranging from mental disorder regulation to human-computer interaction. In this paper, we address two fundamental aspects of EEG emotion recognition: continuous regression of emotional states and discrete classification of emotions. While classification methods have garnered significant attention, regression methods remain relatively under-explored. To bridge this gap, we introduce MASA-TCN, a novel unified model that leverages the spatial learning capabilities of Temporal Convolutional Networks (TCNs) for EEG emotion regression and classification tasks. The key innovation lies in the introduction of a space-aware temporal layer, which empowers TCN to capture spatial relationships among EEG electrodes, enhancing its ability to discern nuanced emotional states. Additionally, we design a multi-anchor block with attentive fusion, enabling the model to adaptively learn dynamic temporal dependencies within the EEG signals. Experiments on two publicly available datasets show that MASA-TCN achieves higher results than the state-of-theart methods for both EEG emotion regression and classification tasks. The code is available at https://github.com/yiding-cs/MASA-TCN Index Terms-Temporal convolutional neural networks (TCN), emotion recognition, electroencephalogram (EEG) I. INTRODUCTION E MOTION recognition, also known as emotional artificial intelligence [1], [2], leverages machine learning to understand human emotions, crucial for addressing emotion-related mental disorders like anxiety, depression, and autism spectrum disorder (ASD). It employs both categorical and dimensional models, with the valence-arousal-dominance (VAD) model [1] being prominent for evaluating emotions across valence (negative to positive), arousal (passive to active), and dominance",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Preliminaries A. Problem Formulation",
      "text": "There are two types of EEG emotion recognition tasks to be addressed in this paper: CER and DEC. We provide a more formal description on the data annotation of CER and DEC. Given N trials of continuous EEG signal [X 0 , ..., X N -1 ], X ∈ R C×T , where C presents EEG electrode numbers, T is the number of temporal data points. Typically, the entire trial is divided into shorter segments, denoted by Xi , i ∈ [0, 1, ..., n-1], using a sliding window with or without overlap to train the neural networks. For CER, the labels are y CER = [y 0 , ..., y n-1 ], y ∈ R 1×T /f y s , where f y s is the sampling rate of the continues labels. Because the label of each trial in CER is continuous in the temporal dimension, the label is also cut into shorter segments as is done for the EEG data. The target of CER is to learn f (Θ) : X i → y CER , which can:\n\nwhere Θ is the trainable parameters of f (•) and Ψ(•) is the regression loss.\n\nFor DEC, the labels are y DEC = [y 0 , ..., y n-1 ], y ∈ R. Because there is one label for each trial in DEC, all the segments within one trial share the same label. The target of DEC is to learn f (Θ) : X i → y DEC , which can:\n\nwhere Θ is the trainable parameters of f (•) and Υ(•) is the cross-entropy loss.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Neural Networks For Temporal Pattern Recognition",
      "text": "This section introduces two neural networks for temporal pattern recognition: RNN and TCN. RNNs, distinct from feedforward networks, leverage previous outputs as inputs, incorporating internal states to learn temporal dynamics. LSTM  [45] , a variant of RNN, efficiently models sequential patterns using a cell state for information retention and gates for regulating data updates. Bidirectional LSTMs enhance pattern learning by processing sequences in reverse order. Gated recurrent units (GRU)  [46] , a simpler LSTM alternative, achieves comparable performance with fewer gates. LSTM's capability in temporal pattern extraction from flattened PSD vectors for CER was demonstrated in  [44] . TCN, introduced for action segmentation  [47] , employs causal and dilated convolutions for sequential modeling, with residual connections enhancing its performance  [48] . Zhang et al.  [8]  applied TCN with rPSD features for CER, surpassing LSTM's results  [44] .\n\nHowever, the spatial relations among electrodes remain underexplored with both LSTM and TCN relying on flattened spectral features. To address this, we introduce a space-aware\n\nFig.  2 . Space-aware temporal convolutional layer. The SAT has two types of convolutional kernels: context kernels that extract the spectral patterns channel by channel and spatial fusion kernels that learn spatial patterns across all the channels. A context kernel of size (4, 3) is utilized for example. And a four-EEG-channel sample with four spectral features in each EEG channel is used as the example. Zero padding is added to make the context kernel a causal kernel along temporal dimension.\n\nOnly one kernel for each type of CNN kernels is shown in the diagram for better view, the final output (on the top) consists of the outputs from more kernels (4 is utilized as the number of kernels of each type for demonstration purpose). Ch-n represents n-th EEG channel, and fn is the n-th frequency band. Best viewed in color.\n\ntemporal convolutional layer for TCN to effectively learn spatial patterns in CER tasks. Considering emotion's temporal variability  [49] ,  [50] , a multi-anchor attentive fusion block is devised to enhance temporal dynamics modeling in affective EEG data. Unlike previous approaches using varying dilation rates, we employ 1D causal convolutional kernels of different lengths to capture the dynamic temporal dependencies in emotional processes. In addition to CER, which necessitates the model's ability to capture temporally continuous changes, this design can also be applied to DEC. DEC demands one overall prediction per input, which can be achieved by employing a mean fusion strategy across predictions from all sub-segments within each input segment.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Method",
      "text": "In this section, the detailed introduction of each functional component in MASA-TCN is presented. TCN has superior sequential pattern modeling ability. However, EEG data has spatial and temporal patterns to be extracted for the regression and classification tasks. Previous works use flattened rPSD features as the input to TCN directly, which can not extract the spatial pattern effectively. A space-aware temporal convolutional layer (SAT) is proposed to extract spatial-spectral patterns of EEG using TCN. Besides, to better learn the temporal dynamics underlying emotional cognitive processes that might appear in different time scales, we design a MAAF. The MAAF consists of three parallel SATs with different lengths of 1D causal convolution kernels. The outputs of these parallel SATs are attentively fused as the input to several TCN layers which learns the higher-level temporal patterns and generates the final hidden embedding. For the CER tasks, a linear layer is utilized as a regressor to map the hidden embedding to the continuous labels. For the DEC tasks, because these segments in time order share one label of that trial, a sum fusion layer is utilized to generate the final output instead of using a linear layer to get a single output. The structure of MASA-TCN is demonstrated in Fig.  1 .",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Input Construction",
      "text": "The construction of the network input is illustrated first to better understand the algorithm. As mentioned in Section 2.1, the EEG data of each trial is cut into shorter segments. Note that the sampling rates of the EEG data and continuous label are different, the former is much higher than the latter, e.g. 256Hz vs 4Hz in MAHNOB-HCI. Then the segments are further segmented into sub-segments along temporal dimension. Sliding windows with overlaps are applied to make sure subsegments are synchronized to each value of the continuous label for CER. For each sub-segment, it is still a 2D matrix, which has spatial and temporal dimensions. Because the subsegments are in time orders, they can be regarded as frames in a video. For each sub-segments, averaged rPSDs in 6 frequency bands are calculated as described in  [8] . We flatten the rPSDs along the EEG channel dimension, resulting in a feature vector:\n\nwhere p is the averaged rPSD, C equals to the EEG channel number, F equals to the total number of the frequency bands, and [•] denotes the concatenation. p f0 c0 represents the averaged rPSD of the channel c 0 in frequency band f 0 . Hence, one input to the neural networks would be:\n\nwhere t represents the total number of the rPSD vectors within one segment. TCN utilizes 1D CNN along the temporal dimension and treats the feature vector that contains spectral and spatial patterns as the channel dimension of 1D CNN. Hence, the spectral patterns across EEG channels as well as the spatial relations among EEG electrodes are not capably learned. Instead of treating the feature vector dimension as the channel dimension of 1D CNN, we treat the input to TCN as a 2D matrix, whose dimensions are feature and time.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "B. Space-Aware Temporal Convolutional Layer",
      "text": "The SAT has two types of convolutional kernels: context kernels that extract the spectral patterns channel by channel and spatial fusion kernels that learn spatial patterns across all the channels. The structure of SAT is shown in Fig.  2 .\n\nGiven the input x = [v 0 , ..., v t-1 ] T , v ∈ R 1×C * f introduced in Section 3.1, the first type of the CNN kernels in SAT is the 2D causal convolutional kernel whose size, step, and dilation are (f, k), (f, 1), and (1, 2), where f is the number of frequency bands used to calculate rPSDs and k is the length of the CNN kernel in temporal dimension. Note that the default dilation step is 1 instead of 0 in PyTorch  [51]  library, which means there is no dilation in that dimension if the dilation step is set as 1. Because the step in the feature dimension is the same as the height of the kernel, it can learn spectral contextual patterns across EEG channels. Hence, it is named the context kernel. The context kernel can learn spectral patterns as well as temporal dynamics at the same time due to its 2D shape. Different from WaveNet  [52]  that has dilation steps of 1, 2, 4, ...2 n-1 , where n is the number of layers, the first layer of MASA-TCN has a dilation of 2 in the temporal dimension. There are two reasons. The first reason is that the higher dilation step helps to get more discriminative information, considering the fact that the rPSD is calculated using overlapped sliding windows so that the adjacent vectors are highly correlated. The second reason is that discarding the TCN layer with a dilation step of 1 leads to smaller model size, without any compromises on the receptive field. Due to the causal convolution, the temporal dimension of the input and output are the same. Hence, we can get the output\n\n, where s is the number of context kernels, C is the number of EEG channels, and t is the total number of the rPSD vectors within one segment. H context can be calculated by:\n\nwhere Conv2D represents the 2D convolution applied to the input x, with kernel s ize, strides, and dilation as the parameters for the CNN operation. Note that these parameters are set to their default values in the PyTorch library unless specified otherwise. Given that f denotes the number of frequency bands used for rPSD (refer to Section IV-B), f MAHNOB-HCI is 6 and f DEAP is 5. We set k to values in the set  [3, 5, 15] , and an analysis to evaluate the effects of k is conducted in Section V-D.\n\nThe output of the context kernels is spatially fused by spatial fusion kernels to learn the spatial patterns of EEG channels. The size, stride, and dilation of the spatial kernels are (C, 1), (1, 1),  (1, 1) , respectively. This is the same as the commonly used spatial kernels of CNNs in BCI domains  [20] ,  [22] . Besides, it can be treated as an attentive fusion of all the EEG channels, with the weights of the 1D CNN kernel being the attention scores. After s spatial fusion kernels, the size of the hidden embedding H SF becomes (s × 1 × t). This process can be described as:\n\nwhere the default values of strides (1, 1) and dilation (1, 1) are utilized.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "C. Multi-Anchor Attentive Fusion Block",
      "text": "There are two steps in the MAAF: 1) parallel SATs with different temporal kernel lengths and 2) attentive fusion of the output from these SATs. The architecture of MAAF is shown in Fig.  1 . TSception  [7]  utilizes multi-scale temporal convolutional kernels to capture temporal dynamics that might happen at different time scales. Emotion varies from time to time, especially over longer duration  [50] . The duration of emotions varies from a few seconds to several hours  [49] .\n\nThree parallel SATs with different temporal kernel sizes are utilized to capture those temporal dynamics in different time scales. In this paper, the temporal lengths of the context kernels are set to k =  [3, 5, 15] , respectively. The longer the temporal length, the larger the temporal receptive field. Because the weights of the context kernels are distributed along the time dimension with the help of dilation steps, each weight is like an anchor on the time axis. Hence, we name these parallel SATs multi-anchor SATs. Besides, from a causal dependence perspective, different temporal kernel sizes may incorporate various previous outcomes to determine the subsequent output. We hypothesize that it can increase the robustness of the causal dependence in the temporal dimension underlying the continuous emotional cognitive process. The results in ablation studies also support the effectiveness of the multi-anchor design. The multi-anchor SATs can be described as:\n\nwhere SAT contains the sequential operation of Eq. 5 and Eq. 6.\n\nDifferent from TSception that directly concatenates the output of different scale kernels, an attentive fusion operation is adopted to combine the output from different SATs. First, the three outputs are concatenated along the kernel dimension (channel dimension of CNNs). Given\n\n. Then, a one-by-one convolutional layer with s CNN kernels of size (1, 1) serves as both an attentive fusion layer and a dimension reducer, returning the concatenated dimensions back to their original size. Hence, the\n\n. The output of the attentive fusion layer can be described as:\n\nwhere [•] is the concatenation along the kernel dimension.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Temporal Convolutional Layer",
      "text": "TCNs are further stacked to learn the temporal dependencies on top of the space-aware temporal patterns learned from MAAF. TCNs enhance temporal sequence learning by stacking causal convolution layers, utilizing dilated 1D CNN kernels. Bai et al.  [53]  improved the sequence modeling capabilities of TCNs by introducing weight normalization, residual connections, and nonlinear activation functions. The enhanced TCN model can be expressed as:\n\nwhere m denotes the layer index, f (•) represents the filter, k is the kernel size, strd is the stride, and d is the dilation factor. strd-d•i indicates the direction of the past. Φ(•) is the PReLU activation function. By stacking layers of TCNs, the temporal receptive field can be increased. The receptive field size can be calculated by:\n\nwhere m is the number of the convolutional block with residual connection, k is the kernel size, d m is the dilation of the m-th convolutional block with the residual connection.\n\nWhen the dilation factor increases exponentially by 2 as the number of TCN layers increases, the receptive field can be calculated as:\n\nBecause SAT, the first layer of MASA-TCN, has dilation of 2, the receptive field of MASA-TCN can be calculated as:",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "E. Output Layer For Regression And Classification Tasks",
      "text": "Given the learned temporal representation, H m , a linear layer is utilized to project it to the desired output for regression and classification, respectively. H m is a sequence of learned embeddings of the sub-segments. Since regression tasks involve n-to-n mapping, a linear layer projects the embedding of each sub-segment into a scalar, which represents the predicted emotional value of that sub-segment. Hence, the regression output, y CER , can be calculated as follows:\n\nwhere LP(•) represents the linear projection layer, and t is the number of sub-segments in a sample. For classification tasks, the entire sequence of sub-segments corresponds to a single label. A mean fusion is applied to aggregate the predictions from all the sub-segments. Hence, the classification output, y DEC , can be calculated as follows:\n\nIV. EXPERIMENTS",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Datasets",
      "text": "Two publicly available datasets are utilized in this paper: MAHNOB-HCI  [5]  for CER and DEAP  [4]  for DEC.\n\nMAHNOB-HCI 1  is a multimodal dataset to study human emotional responses and the implicit tagging of emotions. 30 subjects participated in the data collection experiments. Each subject watched 20 film clips, during which the synchronized recording of multi-angle facial videos, audio signals, EOG, EEG, respiration amplitude, and skin temperature were recorded. A subset  [44]  of the MAHNOB-HCI database that contains 24 participants' 239 trials and the continuous labels in valence from several experts was utilized for the CER task. The final labels were determined by taking the averages of the experts' annotations. The EEG signals have 32 electrodes and the sampling rate is 256 Hz. The annotations are of 4 Hz resolution.\n\nDEAP 2  is a multimodal dataset studying human affective states. 32 subjects participated in the experiments. Each of them watched 40 1-min-long music videos while their EEG, facial expressions, and galvanic skin response (GSR) were recorded simultaneously. Self-assessments on arousal, valence, dominance, and liking from the subjects were utilized as the labels. A continuous 9-point scale was adopted to measure the levels of those dimensions, which was projected into low and high classes using a threshold of 5. The valence dimension was utilized in DEC task to be consistent with CER task. The EEG signals have 32 channels and the sampling rate is 512 Hz.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "B. Preprocessing",
      "text": "We follow the pre-processing steps  [8]  for MAHNOB-HCI. For each EEG trial, the first and last 30 seconds of non-stimuli durations are removed, after which an average reference is conducted. The entire trial is split into shorter segments using a 2s' sliding window with 0.25s' overlap. Then the average rPSD of (0.3-5Hz), (5-8Hz), (8-12Hz), (12-18Hz), , and (30-45Hz) is calculated using Welch's method. By doing so, the 32 × 6 = 192-D rPSD features which have a frequency of 4Hz can be synchronized with the continuous labels. When training the neural networks, another sliding window whose length and step are 96 and 32 is applied to get the temporal sequence of the calculated rPSD vectors as described in  [8] . Hence, the size of the input to the neural networks is  (batch, 192, 96) .\n\nFor DEAP, we follow  [7]  to do the same pre-processing steps. For each trial of EEG, the first 3s' baseline is removed. The data is downsampled to 128 Hz. EOG artifacts were removed following the method described in  [4] . A band-pass filter from 4Hz to 45Hz is applied to remove low and highfrequency noise. Average reference is then conducted. Because MASA-TCN is designed for regression, it needs to learn from a temporal sequence of rPSD vectors. Each EEG trial is split into segments of 8 seconds, with a 4-second overlap, to create a temporal sequence for applying MASA-TCN to the DEC task and for comparison with SOTA DEC methods that utilize shorter EEG segments as input. Then the longer segments are further split into 2s' shorter segments with 0.25s' overlap to get the rPSDs in (4-8Hz), (8-12Hz), (12-18Hz), , and (30-45Hz) five frequency bands. Note that the segment length in  [7]  is 4 seconds; for a fair comparison, we rerun all the compared methods using 8s' segments with 4s' overlap.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Evaluation Metrics",
      "text": "The evaluation metrics for CER are the same as those in  [8] : root mean square error (RMSE), Pearson's correlation coefficient (PCC), and concordance correlation coefficient (CCC). Given the prediction ŷ, and the continuous label y, RMSE, PCC, and CCC can be calculated as:\n\nwhere N is the number of elements in the prediction/label vector, σ ŷy is the covariance, σ ŷ and σ y are the variances, and µ ŷ and µ y are the means. The evaluation metrics for DEC are the same as described in  [7] : accuracy (ACC) and F1 score.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "D. Experiment Settings",
      "text": "For CER tasks, a leave one subject out (LOSO) strategy is utilized as described in  [8] . In the LOSO strategy, one subject's data is selected as test data, while the remaining subjects' data serve as training data. Within the training data, 80% is randomly selected as training data, and the rest 20% is utilized as validation data. We repeat this process until each subject has been the test subject once. The purposes of including an additional validation set are: 1) to provide criteria (best CCC on the validation/development set) for model and hyperparameter selection; 2) to evaluate the model's generalization ability by testing it on unseen subject data, which is assessed only once. The mean RMES, PCC, and CCC are reported as the final results.\n\nWe adhere to the settings described in  [7]  for DEC tasks, employing a trial-wise 10-fold cross-validation for subjectspecific experiments. DEC, under a generalized setting, remains challenging even in the context of subject-specific experiments  [7] . Emotion is a component of a continuous cognitive process, wherein adjacent segments within each trial demonstrate high correlations. Randomly shuffling these segments across different trials before the training-test split can lead to test data leakage, as the highly correlated adjacent segments within each trial might be present in both the training and test sets. To address such data leakage issues and adopt a more generalized evaluation methodology, we utilize trial-wise randomization to divide each subject's trials into ten folds, following the procedure outlined in  [7] . In each iteration of the 10-fold cross-validation process, one fold is allocated as the test dataset, while the remaining nine folds are divided into training and validation datasets at an 80% and 20% ratio, respectively. The final results are presented as the mean accuracy (ACC) and F1 score across all subjects.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "E. Implementation Details",
      "text": "For the CER task, we follow the same training strategy as described in  [8] . CCC loss is utilized to guide the training.\n\nThe network is trained using the Adam optimizer, with an initial learning rate of 1e-4 and a weight decay of 1e-4. A ReduceLROnPlateau learning rate scheduler, with a patience of 5 and a reduction factor of 0.5, is also used. The maximum training epoch is set to 15 and the early stopping patient is set to 10. The batch size is set to 2. The kernel size of MASA-TCN is set to  [3, 5, 15] . We tune the depth and width of MASA-TCN based on the overall performance on validation data. When the depth is 2 and the width is 64, MASA-TCN gives the best results on validation data. The dropout rate is set as 0.15 for TCNs and 0.4 for RNNs (RNN, LSTM and GRU) as suggested in  [44] . For baseline methods, we use the same training strategy and parameters as the ones of MASA-TCN for fair comparison. We also compare our results with the ones reported in the existing literatures for the same task.\n\nFor the DEC task, based on the training strategy described in  [7] , we further reduce the maximum training epochs from 500 to 100 and add early stopping with the patient being 10 to avoid over-fitting. Besides, a two-stage training strategy is adopted. It contains two stages. In stage I, we train the model using training data and evaluate it on the validation data. The model with the best validation ACC is saved. In stage II, we combine the training and validation data as new training data and re-train the saved model on the combined dataset for at maximum 50 epochs and stop training when the training loss reaches the stopping criteria. During the training stage, the training loss of the epoch with best validation ACC is saved as the stopping criteria in the second stage. The learning rate is 1e-3 and the batch size is 32. The dropout of TCN is still 0.15 because there is a dropout operation in every TCN layer. However, this is too small for the baseline methods. Hence, the dropout rate of baseline methods is still 0.5 which is suggested in  [7] . Label smoothing with a factor of 0.1 is added to further overcome the overfitting problems. The depth and width of MASA-TCN are set to 3, and 16 based on the performance on validation data. All the baselines are re-trained using the same training strategy and the same segment length of data as MASA-TCN for a fair comparison.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Results And Analysis",
      "text": "In this section, we first report the CER results of MASA-TCN against several baselines, as well as the SOTA results reported in recently published papers  [8] ,  [30] . Then the ablation study results are presented to analyze the contribution of each functional component of MASA-TCN. After that, five types of analysis experiments are conducted to analyze the effects of 1) the starting dilation, 2) the kernel size, 3) the model depth and width, 4) different fusion strategies in MAAF, and 5) early and late spatial learning. Lastly, the results for DEC tasks and the effect of mean fusion in last fully-connected layer are reported.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Cer Results On Mahnob-Hci",
      "text": "We first compare the proposed MASA-TCN with several temporal learning neural networks; then, we compare the CER results of MASA-TCN with the SOTA results reported in the existing literature  [8] ,  [30]  that use the same experimental settings. Table  I  shows the CER results of RNN, LSTM, GRU, TCN, Chen et al.  [40] , and MASA-TCN under the LOSO experimental setting. Table II lists the reported SOTA results alongside those of MASA-TCN.\n\nTable  I  shows that MASA-TCN outperforms all compared methods in RMSE, PCC, and CCC on both validation and test sets. Specifically, MASA-TCN exhibits a 14.29% lower RMSE (an absolute drop of 0.01), a 0.043 higher PCC, and a 0.046 higher CCC than TCN. Against the RNN family, it achieves a 10.45% lower RMSE (an absolute drop of 0.007), a 0.015 higher PCC, and a 0.031 higher CCC than the best-performing LSTM. Comparatively, MASA-TCN also outperforms SOTA results in  [8]  with a 9.09% lower RMSE (an absolute drop of 0.006), a 0.033 higher PCC, and a 0.04 higher CCC. Furthermore, it significantly surpasses Soleymani's methods detailed in  [8] , with a 25.93% lower RMSE (an absolute drop of 0.021), a 0.08 higher PCC, and a 0.111 higher CCC. Although MASA-TCN has a slightly higher RMSE than the one in  [30] , it improves PCC and CCC by a large margin (0.037 for PCC and 0.021 for CCC).",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "B. Ablation Studies",
      "text": "Several ablation studies are conducted to understand how each component of MASA-TCN contributes to the improvements of CER results. Starting from the baseline TCN, SAT and MAAF are gradually added to see the effects of them. The results are shown in Table  III .\n\nAccording to Table  III , adding SAT and MAAF can incrementally improve all the three metrics. By adding SAT alone, the performances of TCN can be improved by 0.008 on RMSE, 0.022 on PCC, and 0.023 on CCC. The results are further improved from 0.062 to 0.060 on RMSE, from 0.486 to 0.507 on PCC, and from 0.394 to 0.417 when MAAF is also added. The results indicate the effectiveness of all those functional blocks in MASA-TCN.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "C. Effect Of The Starting Dilation",
      "text": "In this section, the effects of the starting dilation in the SAT are discussed. As mentioned in Section 3.2, the dilation of SAT starts from 2 instead of 1 which is used in TCN. There are two reasons. First, the EEG sub-segments are overlapped to synchronize them with the continuous labels, leading to some redundant information in the adjacent sub-segments. Using higher dilation at the SAT can learn the discriminative patterns effectively. Second, higher starting dilation in SAT can increase the receptive field which can reduce the number of TCN layers to get the same temporal receptive field, resulting in a more compact model size. To evaluate those effects, the starting dilation of SAT in MASA-TCN is set to 1, 2, and 4. The results are shown in Table  IV .\n\nThe results show that increasing the dilation to a certain degree can improve the performance, and further increase of dilation can not provide gains on the CER results. When the dilation of SAT is 2, MASA-TCN has the best performance on both validation and test set. When the value is increased further to 4, the results slightly drop on both validation and test set. The possible reasons are a dilation of 2 and a TCN layer of 2 can give enough temporal receptive field and increase more can lose certain information among the adjacent sub-segments.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "D. Effect Of The Kernel Size",
      "text": "This section explores the impact of varying the maximum kernel size within the MAAF, adjusting it from 3 to 15 in increments of 2. The findings, presented in Table  V , reveal minimal differences in overall performance as measured by RMSE. However, larger kernel sizes are associated with improvements in both PCC and CCC, indicating that they may enhance performance in these specific metrics.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "E. Effect Of The Model Depth And Width",
      "text": "Experiments about the effects of model depth and width are conducted to better understand MASA-TCN. For model depth studies, the SAT is regarded as 2 layers due to the sequential operation of two types of CNN kernels. And there are 2 causal convolutional layers in one TCN layer. Hence the depths are set as 2, 4, 6, 8, and 10. For the width, it is the number of ↓: the lower the better; ↑: the higher the better. ↓: the lower the better; ↑: the higher the better. SA: Space-aware temporal convolutional layer; MA: Multi-anchor attentive fusion block. ↓: the lower the better; ↑: the higher the better. kernels in each CNN layer. The widths are set as 8, 16, 32, and 64. The results are shown in Fig.  3 .\n\nAccording to the results, depth is not sensitive when it is higher than 4, and the width more sensitively affects the model performance compared with the depth. From Fig.  3  (a), only having SAT and MAAF can not provide good performance. This is because the temporal receptive field is not enough. When the depth is 4, MASA-TCN achieves the best performance on both validation and test sets. However, when the depth increases to higher than 4, the performances drop a little bit and become stable. This is due to that enough temporal receptive field is achieved and a deeper model is relatively harder to train than the shallow one  [54] . From Fig.  3  (b), the performances are positively related to the width. And when the width is 64, MASA-TCN achieves the best results on both validation and test sets. Note that we also conducted an experiment that use 128 as the width, but the model gave very low performances, which indicates the wider model is also harder to train. ↓: the lower the better; ↑: the higher the better. ↓: the lower the better; ↑: the higher the better.",
      "page_start": 7,
      "page_end": 9
    },
    {
      "section_name": "F. Effect Of Different Fusion Strategies In Maaf",
      "text": "The effects of different fusion strategies in MAAF are analyzed and discussed in this section. Because three SATs with different kernel lengths are parallelly utilized in MAAF, the output needs to be fused for the subsequent TCNs. Three types of fusion mechanisms are studied: concatenation, mean, and attentive fusion.\n\nBased on the results in Table VI, all three types of fusion methods achieve relatively acceptable performances, and with attentive fusion, MASA-TCN has the best performances on both validation and test sets. This indicates the effectiveness of attention fusion in MAAF. ↓: the lower the better; ↑: the higher the better.  ↓: the lower the better; ↑: the higher the better.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "G. Effect Of Early And Late Spatial Learning",
      "text": "The order of spatial learning is studied in this part. As illustrated in Section 2.2, there are spatial, spectral, and temporal patterns that need to be recognized for EEG data. Typically spatial patterns can be learned by a 1D CNN kernel whose size is (c, 1), where c is the number of EEG channels. In MASA-TCN, the spatial learning is done in SAT, which is regarded as early spatial learning. The spatial patterns can also be learned after the last several TCN layers, which is termed late spatial learning. We compare both early and late spatial learning. The results are shown in Table  VII .\n\nEarly spatial learning is more effective than late spatial learning according to the results. It is noticeable that late spatial learning cannot even has comparable performance with the one using early spatial learning. More analyses should be done in the future to better understand the reason.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "H. Dec Results On Deap",
      "text": "1) Comparison with baselines for DEC: MASA-TCN achieves SOTA performances on CER tasks, we further explore the possibility of extending it to DEC tasks and compare it with the SOTA methods of DEC tasks, SVM (2012)  [4] , DeepConvNet (2017)  [20] , EEGNet (2018)  [22] , TSception (2022)  [7] , and MEET (2023)  [33]  in this section. Because MASA-TCN is mainly designed for CER tasks, a regressor is utilized to generate a 1D output that has the same length as the continuous labels. One way to adapt MASA-TCN to DEC is to change the output size of the regressor from 1D to binary output and the regressor becomes a normal classifier in most deep learning methods for classification. However, we can also extend MASA-TCN to DEC by using a mean fusion on the output of the regressor as a kind of classifier ensemble which can increase the robustness. Hence, in MASA-TCN, we choose the latter to extend it from the CER tasks to the DEC tasks. Note that for a fair comparison, all the methods use the same data preprocessing steps, the same segment length (8s)   IX , the MASA-TCN model delivers superior or comparable performance in emotion classification tasks. Specifically, MASA-TCN exhibits the highest accuracy and F1 score in the valence dimension. While the differences in accuracy among various deep learning approaches are not markedly significant, MASA-TCN and MEET, the latter securing the second place, outshine their counterparts in terms of F1 scores. In the context of the arousal dimension, MASA-TCN attains the highest accuracy, whereas TSception leads with the best F1 score.\n\n2) Effect of mean fusion in the fully connected layer for DEC:\n\nThis section examines the impact of mean fusion in the final fully connected (FC) layer through two experiments: (1) removing mean fusion from MASA-TCN and (2) implementing mean fusion in the last FC layer of MASA-TCN. Results in Table X reveal that incorporating mean fusion into MASA-TCN enhances the ACC and F1 score by 1.63% and 2.7%, respectively, highlighting the benefit of mean fusion in MASA-TCN.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Vi. Discussion",
      "text": "The CER tasks are relatively more comprehensive to study human emotions. CER tasks require the model to predict the temporally continuous labels of emotions using EEG signals, which are rarely explored in the existing literatures  [8] . Emotion is a continuous neural cognitive process of the brain  [49] . In general EEG collection experiments in the studies for emotions  [4] ,  [5] , the subjects are required to watch and listen to the affective stimuli for a certain duration. And the emotional states are not consistent during the entire trial  [50] . By refining the label of shorter segments using the  continuous label instead of the single label of one trial in DEC, improvements in classification are observed  [55] . Despite the importance exploring novel methods for EEG CER tasks, only a few works  [8] ,  [44]  have proposed some algorithms. And all of them use flattened feature vectors as input while not effectively learning the spatial patterns across EEG channels. MASA-TCN is proposed in this paper to enable TCN to learn spatial, spectral, and temporal patterns simultaneously for the CER tasks. The main functional block for spatial learning is the SAT layer that consists of context kernel and spatial fusion kernel two types of CNN kernels. With the help of zero padding and dilation along the temporal dimension, SAT can also learn the temporal causal dependencies. Because EEG contains abundant temporal information that is related to the brain's emotional activities changes from time to time, and the temporal dependencies might happen in different time scales  [7] ,  [49] , a MAAF block is further designed to capture those temporal dynamics with the help of multiple temporal kernel lengths as well as an attentive fusion layer. Extensive experiments on a publicly available dataset have been done to evaluate the proposed method. The results demonstrate the effectiveness of MASA-TCN for CER tasks and we set new SOTA results against the recently published results in  [8] . We further extend MASA-TCN from regression tasks to classification tasks by adding a mean fusion in the final fully-connected layer (regressor). It also achieves higher classification results over several SOTA methods in DEC tasks. To the best of our knowledge, this is the first work to propose a unified model for both CER and DEC tasks. The experiment also indicates that calculating the mean of the output of a regressor as the classification output can yield a certain improvement in F1 scores.\n\nBesides the analysis experiments we conducted and discussed in Section 5, some discussions on the output of MASA-TCN for CER are given here. Four representative samples for well, moderately, and poorly regressed trials are selected to show the differences between the prediction and ground truth. They are shown in Fig.  4 . The discussions are two-fold: the performance of MASA-TCN for CER and the differences among the three evaluation metrics.\n\nWe first discuss the performance of MASA-TCN. According to Fig.  4  (a) and (b), MASA-TCN can well regress the relatively smaller absolute value (> 0.15) while the predictions of larger-value labels, especially the ones with sudden changes, are not well addressed. In the future, some regularization terms can be added to the output of MASA-TCN to reduce the amplitude after sudden changes. It is also noticeable that MASA-TCN handles the positive labels better than the negative ones by comparing Fig.  4  (a), (b), and (c). Based on Fig.  4  (d), it can be seen that MASA-TCN can not well regress the details of sudden short-term fluctuations. RMSE can punish the distance between prediction and label pointwise, hence, it is worth trying to guide the training using a weighted combination of RMSE and CCC for better regression of the details instead of using CCC loss only. Next, we give some discussions on the evaluation metrics.\n\nCCC is a better evaluation metric for CER compared with RMSE and PCC. RMSE focuses on point-to-point precision, while the correlation between the predictions and labels is not effectively measured. As shown in Fig.  4 , when the trial is well regressed (Fig.  4  (a)), RMSE is still larger than the ones of the poorly regressed ones (Fig.  4  (c) and Fig.  4 (d) ). That's because the labels are in relatively lower amplitudes in Fig.  4  (c) and (d) than the ones in Fig.  4 (a) . Even though the trends are not well regressed, the point-to-point distances are still small. However, CCCs of those two poorly regressed outputs are much lower than the well-regressed ones because CCC also measures the correlation between the two vectors. Although PCC can measure the correlations, it ignores the absolute distances among points of the outputs and labels. Hence, we can see the PCC is still high in Fig.  4  (b) even though there are long drifts between the outputs and labels. CCC can reflect those drifts as well, hence, the CCC of Fig.  4  (a) is much higher than the one of Fig.  4",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "(B).",
      "text": "There are also some limitations in this work that need to be discussed. The first one is the lack of datasets for CER tasks. This is a common problem for EEG CER tasks. Because preparing a dataset for the EEG CER tasks needs well-designed experiment protocols as well as the efforts of a number of experts to continuously annotate the corresponding trials. In the future, more datasets need to be created to further boost this research area. Besides, more interpretability methods should be applied to better understand why early spatial learning is much better than late spatial learning. At last, in this paper, we follow  [8]  that only uses CCC in the loss function to guide the training process. In the future, using a weighted combination of RMSE, PCC, and CCC in the loss function is expected to provide certain improvements.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vii. Conclusion",
      "text": "In this paper, MASA-TCN is proposed to improve the SOTA results of the CER and DEC tasks using EEG. Compared with the SOTA methods  [8] ,  [44]  that don't effectively learn the spatial patterns among EEG channels, a novel SAT layer is designed to enable TCN to capture spatial, spectral, and temporal patterns simultaneously. A MAAF block is further proposed to capture the temporal dynamics that might happen different time scales underlying emotional cognitive processes. By adding a mean fusion in the output of the regressor of MASA-TCN, we further extend MASA-TCN from CER to DEC, making it a unified model for both the CER and DEC tasks using EEG. Extensive experiments on two public emotion datasets show the effectiveness of the proposed methods for both CER and DEC. New SOTA results are achieved by MASA-TCN for those tasks.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The architecture of our MASA-TCN. There are four main parts of MASA-TCN: feature extraction block, MAAF block, TCN block, and",
      "page": 3
    },
    {
      "caption": "Figure 2: Space-aware temporal convolutional layer. The SAT has two",
      "page": 3
    },
    {
      "caption": "Figure 1: A. Input construction",
      "page": 3
    },
    {
      "caption": "Figure 2: Given the input x = [v0, ..., vt−1]T , v ∈R1×C∗f introduced",
      "page": 4
    },
    {
      "caption": "Figure 1: TSception [7] utilizes multi-scale temporal",
      "page": 4
    },
    {
      "caption": "Figure 3: According to the results, depth is not sensitive when it",
      "page": 8
    },
    {
      "caption": "Figure 3: (a), only having SAT and MAAF can not provide good",
      "page": 8
    },
    {
      "caption": "Figure 3: (b), the performances are positively related to the width. And",
      "page": 8
    },
    {
      "caption": "Figure 3: Effect of the depth and width of MASA-TCN.",
      "page": 9
    },
    {
      "caption": "Figure 4: Four representative samples of well and poorly regressed trials",
      "page": 10
    },
    {
      "caption": "Figure 4: The discussions are two-fold:",
      "page": 10
    },
    {
      "caption": "Figure 4: (a) and (b), MASA-TCN can well regress the",
      "page": 10
    },
    {
      "caption": "Figure 4: (a), (b), and (c). Based",
      "page": 10
    },
    {
      "caption": "Figure 4: (d), it can be seen that MASA-TCN can not well",
      "page": 10
    },
    {
      "caption": "Figure 4: , when the trial",
      "page": 10
    },
    {
      "caption": "Figure 4: (a)), RMSE is still larger than the",
      "page": 10
    },
    {
      "caption": "Figure 4: (c) and Fig. 4 (d)).",
      "page": 10
    },
    {
      "caption": "Figure 4: (c) and (d) than the ones in Fig. 4 (a). Even though",
      "page": 10
    },
    {
      "caption": "Figure 4: (a) is much higher than the one of Fig. 4 (b).",
      "page": 10
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Features\nTime\nsub-segments@(ch x t )": "",
          "Column_2": "e=k) AF : Attentiv",
          "Column_3": "e fusion : Causal convolution layer + :",
          "Column_4": "Element-wise addition R : Regressor(",
          "Column_5": "MLP) M : Mean"
        }
      ],
      "page": 3
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Emotions recognition using EEG signals: A survey",
      "authors": [
        "S Alarcão",
        "M Fonseca"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "2",
      "title": "Decoupled multimodal distilling for emotion recognition",
      "authors": [
        "Y Li",
        "Y Wang",
        "Z Cui"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "3",
      "title": "Self-supervised representation learning from videos for facial action unit detection",
      "authors": [
        "Y Li",
        "J Zeng",
        "S Shan",
        "X Chen"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "4",
      "title": "DEAP: A database for emotion analysis using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "5",
      "title": "A multimodal database for affect recognition and implicit tagging",
      "authors": [
        "M Soleymani",
        "J Lichtenauer",
        "T Pun",
        "M Pantic"
      ],
      "year": "2012",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Unsupervised learning of brain state dynamics during emotion imagination using highdensity EEG",
      "authors": [
        "S.-H Hsu",
        "Y Lin",
        "J Onton",
        "T.-P Jung",
        "S Makeig"
      ],
      "year": "2022",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "7",
      "title": "TSception: Capturing temporal dynamics and spatial asymmetry from EEG for emotion recognition",
      "authors": [
        "Y Ding",
        "N Robinson",
        "S Zhang",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Visual-to-EEG cross-modal knowledge distillation for continuous emotion recognition",
      "authors": [
        "S Zhang",
        "C Tang",
        "C Guan"
      ],
      "year": "2022",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Identifying stable patterns over time for emotion recognition from EEG",
      "authors": [
        "W.-L Zheng",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "EEG based emotion recognition by combining functional connectivity network and local activations",
      "authors": [
        "P Li",
        "H Liu",
        "Y Si",
        "C Li",
        "F Li",
        "X Zhu",
        "X Huang",
        "Y Zeng",
        "D Yao",
        "Y Zhang",
        "P Xu"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "11",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "12",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly",
        "J Uszkoreit",
        "N Houlsby"
      ],
      "year": "2021",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "13",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "M.-W Kenton",
        "L Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of NAACL-HLT"
    },
    {
      "citation_id": "14",
      "title": "Language models are few-shot learners",
      "authors": [
        "T Brown",
        "B Mann",
        "N Ryder",
        "M Subbiah",
        "J Kaplan",
        "P Dhariwal",
        "A Neelakantan",
        "P Shyam",
        "G Sastry",
        "A Askell",
        "S Agarwal",
        "A Herbert-Voss",
        "G Krueger",
        "T Henighan",
        "R Child",
        "A Ramesh",
        "D Ziegler",
        "J Wu",
        "C Winter",
        "C Hesse",
        "M Chen",
        "E Sigler",
        "M Litwin",
        "S Gray",
        "B Chess",
        "J Clark",
        "C Berner",
        "S Mccandlish",
        "A Radford",
        "I Sutskever",
        "D Amodei"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "15",
      "title": "Glob-alWoZ: Globalizing MultiWoZ to develop multilingual task-oriented dialogue systems",
      "authors": [
        "B Ding",
        "J Hu",
        "L Bing",
        "M Aljunied",
        "S Joty",
        "L Si",
        "C Miao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "16",
      "title": "Is GPT-3 a good data annotator",
      "authors": [
        "B Ding",
        "C Qin",
        "L Liu",
        "Y Chia",
        "B Li",
        "S Joty",
        "L Bing"
      ],
      "year": "2023",
      "venue": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "17",
      "title": "Learning adaptive multiresolution transforms via meta-framelet-based graph convolutional network",
      "authors": [
        "T Luo",
        "Z Mo",
        "S Pan"
      ],
      "year": "2024",
      "venue": "The Twelfth International Conference on Learning Representations"
    },
    {
      "citation_id": "18",
      "title": "Semi-supervised classification with graph convolutional networks",
      "authors": [
        "T Kipf",
        "M Welling"
      ],
      "year": "2017",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "19",
      "title": "Fast graph generation via spectral diffusion",
      "authors": [
        "T Luo",
        "Z Mo",
        "S Pan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Deep learning with convolutional neural networks for EEG decoding and visualization",
      "authors": [
        "R Schirrmeister",
        "J Springenberg",
        "L Fiederer",
        "M Glasstetter",
        "K Eggensperger",
        "M Tangermann",
        "F Hutter",
        "W Burgard",
        "T Ball"
      ],
      "year": "2017",
      "venue": "Human Brain Mapping"
    },
    {
      "citation_id": "21",
      "title": "Learning temporal information for brain-computer interface using convolutional neural networks",
      "authors": [
        "S Sakhavi",
        "C Guan",
        "S Yan"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "22",
      "title": "EEGNet: a compact convolutional neural network for EEG-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of Neural Engineering"
    },
    {
      "citation_id": "23",
      "title": "A transformer-based deep neural network model for SSVEP classification",
      "authors": [
        "J Chen",
        "Y Zhang",
        "Y Pan",
        "P Xu",
        "C Guan"
      ],
      "year": "2023",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "24",
      "title": "Subject-independent brain-computer interfaces based on deep convolutional neural networks",
      "authors": [
        "O.-Y Kwon",
        "M.-H Lee",
        "C Guan",
        "S.-W Lee"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "25",
      "title": "LGGNet: Learning from local-global-graph representations for brain-computer interface",
      "authors": [
        "Y Ding",
        "N Robinson",
        "C Tong",
        "Q Zeng",
        "C Guan"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "26",
      "title": "A survey of deep learning-based classification methods for steady-state visual evoked potentials",
      "authors": [
        "J Yudong Pan",
        "Y Zhang"
      ],
      "year": "2023",
      "venue": "Brain-Apparatus Communication: A Journal of Bacomics"
    },
    {
      "citation_id": "27",
      "title": "Deep convolutional neural networks for mental load classification based on EEG data",
      "authors": [
        "Z Jiao",
        "X Gao",
        "Y Wang",
        "J Li",
        "H Xu"
      ],
      "year": "2018",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "28",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "29",
      "title": "MSFR-GCN: A multi-scale feature reconstruction graph convolutional network for EEG emotion and cognition recognition",
      "authors": [
        "D Pan",
        "H Zheng",
        "F Xu",
        "Y Ouyang",
        "Z Jia",
        "C Wang",
        "H Zeng"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "30",
      "title": "GIGN: Learning graph-in-graph representations of EEG signals for continuous emotion recognition",
      "authors": [
        "Y Ding",
        "C Guan"
      ],
      "year": "2023",
      "venue": "2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "31",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cybernetics"
    },
    {
      "citation_id": "32",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for EEG emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "33",
      "title": "MEET: A multi-band EEG transformer for brain states decoding",
      "authors": [
        "E Shi",
        "S Yu",
        "Y Kang",
        "J Wu",
        "L Zhao",
        "D Zhu",
        "J Lv",
        "T Liu",
        "X Hu",
        "S Zhang"
      ],
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "34",
      "title": "Transformers for EEGbased emotion recognition: A hierarchical spatial information learning model",
      "authors": [
        "Z Wang",
        "Y Wang",
        "C Hu",
        "Z Yin",
        "Y Song"
      ],
      "year": "2022",
      "venue": "IEEE Sensors Journal"
    },
    {
      "citation_id": "35",
      "title": "A dual-branch dynamic graph convolution based adaptive transformer feature fusion network for EEG emotion recognition",
      "authors": [
        "M Sun",
        "W Cui",
        "S Yu",
        "H Han",
        "B Hu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "SECT: A method of shifted EEG channel transformer for emotion recognition",
      "authors": [
        "Z Bai",
        "F Hou",
        "K Sun",
        "Q Wu",
        "M Zhu",
        "Z Mao",
        "Y Song",
        "Q Gao"
      ],
      "year": "2023",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "37",
      "title": "EEG emotion recognition using dynamical graph convolutional neural networks and broad learning system",
      "authors": [
        "X.-H Wang",
        "T Zhang",
        "X -M. Xu",
        "L Chen",
        "X -F. Xing",
        "C Chen"
      ],
      "year": "2018",
      "venue": "2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)"
    },
    {
      "citation_id": "38",
      "title": "Hierarchical attention-based temporal convolutional networks for EEG-based emotion recognition",
      "authors": [
        "C Li",
        "B Chen",
        "Z Zhao",
        "N Cummins",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "ICASSP 2021 -2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "39",
      "title": "SAE+LSTM: A new framework for emotion recognition from multi-channel EEG",
      "authors": [
        "X Xing",
        "Z Li",
        "T Xu",
        "L Shu",
        "B Hu",
        "X Xu"
      ],
      "year": "2019",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "40",
      "title": "Continuous bimanual trajectory decoding of coordinated movement from EEG signals",
      "authors": [
        "Y.-F Chen",
        "R Fu",
        "J Wu",
        "J Song",
        "R Ma",
        "Y.-C Jiang",
        "M Zhang"
      ],
      "year": "2022",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "41",
      "title": "EEGbased emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "Analysing affective behavior in the second ABAW2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)"
    },
    {
      "citation_id": "43",
      "title": "AVEC 2019 workshop and challenge: State-of-mind, detecting depression with AI, and cross-cultural affect recognition",
      "authors": [
        "F Ringeval",
        "B Schuller",
        "M Valstar",
        "N Cummins",
        "R Cowie",
        "L Tavabi",
        "M Schmitt",
        "S Alisamir",
        "S Amiriparian",
        "E.-M Messner",
        "S Song",
        "S Liu",
        "Z Zhao",
        "A Mallol-Ragolta",
        "Z Ren",
        "M Soleymani",
        "M Pantic"
      ],
      "year": "2019",
      "venue": "Proceedings of the 9th International on Audio/Visual Emotion Challenge and Workshop, ser. AVEC '19"
    },
    {
      "citation_id": "44",
      "title": "Analysis of EEG signals and facial expressions for continuous emotion detection",
      "authors": [
        "M Soleymani",
        "S Asghari-Esfeden",
        "Y Fu",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "45",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "46",
      "title": "On the properties of neural machine translation: Encoder-decoder approaches",
      "authors": [
        "K Cho",
        "B Van Merrienboer",
        "D Bahdanau",
        "Y Bengio"
      ],
      "year": "2014",
      "venue": "On the properties of neural machine translation: Encoder-decoder approaches"
    },
    {
      "citation_id": "47",
      "title": "Temporal convolutional networks for action segmentation and detection",
      "authors": [
        "C Lea",
        "M Flynn",
        "R Vidal",
        "A Reiter",
        "G Hager"
      ],
      "year": "2017",
      "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "48",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling"
    },
    {
      "citation_id": "49",
      "title": "Determinants of emotion duration and underlying psychological and neural mechanisms",
      "authors": [
        "P Verduyn",
        "P Delaveau",
        "J.-Y Rotgé",
        "P Fossati",
        "I Mechelen"
      ],
      "year": "2015",
      "venue": "Emotion Review"
    },
    {
      "citation_id": "50",
      "title": "EEGbased emotion recognition with emotion localization via hierarchical self-attention",
      "authors": [
        "Y Zhang",
        "H Liu",
        "D Zhang",
        "X Chen",
        "T Qin",
        "Q Zheng"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "51",
      "title": "PyTorch: An imperative style, highperformance deep learning library",
      "authors": [
        "A Paszke",
        "S Gross",
        "F Massa",
        "A Lerer",
        "J Bradbury",
        "G Chanan",
        "T Killeen",
        "Z Lin",
        "N Gimelshein",
        "L Antiga",
        "A Desmaison",
        "A Kopf",
        "E Yang",
        "Z Devito",
        "M Raison",
        "A Tejani",
        "S Chilamkurthy",
        "B Steiner",
        "L Fang",
        "J Bai",
        "S Chintala"
      ],
      "year": "2019",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "52",
      "title": "WaveNet: A generative model for raw audio",
      "authors": [
        "A Van Den Oord",
        "S Dieleman",
        "H Zen",
        "K Simonyan",
        "O Vinyals",
        "A Graves",
        "N Kalchbrenner",
        "A Senior",
        "K Kavukcuoglu"
      ],
      "year": "2016",
      "venue": "ISCA Speech Synthesis Workshop"
    },
    {
      "citation_id": "53",
      "title": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "authors": [
        "S Bai",
        "J Kolter",
        "V Koltun"
      ],
      "year": "2018",
      "venue": "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling"
    },
    {
      "citation_id": "54",
      "title": "Deep residual learning for image recognition",
      "authors": [
        "K He",
        "X Zhang",
        "S Ren",
        "J Sun"
      ],
      "year": "2016",
      "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "55",
      "title": "Emotion recognition with refined labels for deep learning",
      "authors": [
        "S Zhang",
        "C Guan"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    }
  ]
}