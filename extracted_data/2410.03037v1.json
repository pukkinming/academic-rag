{
  "paper_id": "2410.03037v1",
  "title": "Disentangling Textual And Acoustic Features Of Neural Speech Representations",
  "published": "2024-10-03T22:48:04Z",
  "authors": [
    "Hosein Mohebbi",
    "Grzegorz Chrupała",
    "Willem Zuidema",
    "Afra Alishahi",
    "Ivan Titov"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Neural speech models build deeply entangled internal representations, which capture a variety of features (e.g., fundamental frequency, loudness, syntactic category, or semantic content of a word) in a distributed encoding. This complexity makes it difficult to track the extent to which such representations rely on textual and acoustic information, or to suppress the encoding of acoustic features that may pose privacy risks (e.g., gender or speaker identity) in critical, real-world applications. In this paper, we build upon the Information Bottleneck principle to propose a disentanglement framework that separates complex speech representations into two distinct components: one encoding content (i.e., what can be transcribed as text) and the other encoding acoustic features relevant to a given downstream task. We apply and evaluate our framework to emotion recognition and speaker identification downstream tasks, quantifying the contribution of textual and acoustic features at each model layer. Additionally, we explore the application of our disentanglement framework as an attribution method to identify the most salient speech frame representations from both the textual and acoustic perspectives.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "The internal activation vectors of most modern deep learning systems, including Neural Speech Models (NSM) such as Wav2Vec2  (Baevski et al., 2020) , HuBERT  (Hsu et al., 2021) , and Whisper  (Radford et al., 2022) , are highly entangled. This means that distinct input characteristics -such as fundamental frequency, loudness, syntactic category, or semantic features of a spoken word-are not separated into individual dimensions within the model's latent space -but are instead intertwined within the same ones. Entanglement is a major obstacle for our ability to interpret and to intervene; disentanglement, to the extent that it is possible and even if imperfect, is therefore often highly desirable. For instance, when state-of-the-art NSMs are used in critical situations, we may want to be able to guarantee that information about the speaker's identity, gender, or health characteristics are not used in downstream applications. However, the entangled nature of the NSM's internal representation makes it difficult to surgically suppress such acoustic information.\n\nIn this paper, we investigate the extent to which we can learn to disentangle neural representations. Although our proposed disentanglement framework is not modality-specific, in this paper we focus on two speech-related downstream tasks. As our first case study, we choose emotion recognition as the target task: while the content of an utterance is a strong cue for detecting the emotion of the speaker, the additional acoustic signals offer significant advantages over text-based models. For example, consider the utterances \"I'm so happy\" and \"I'm fine,\" where the former explicitly conveys happiness through its content, whereas the latter can represent a wide range of emotions depending on prosody and tone. Our second case study closely follows the setup of the first, but we choose speaker identification as the target task. Here we hypothesize that text offers only limited information, and the acoustic information will play an even greater role in revealing the identity of the speaker than in the case of emotion recognition.\n\nIn Section 2, we propose a two-stage disentanglement framework (sketched in Figure1) based on the Information Bottleneck principle  (Tishby et al., 2000; Alemi et al., 2016)  to disentangle textual and acoustic features encoded in NSMs. In stage 1 of our framework, we train a decoder with two objectives: to map the internal representation of an existing speech model to text, but also minimize the presence of irrelevant information in these representations. The goal is to ensure that the latent representation preserves only the speech features necessary for accurate transcription, while filtering out any extraneous characteristics. In stage 2, we train a second decoder on the same speech representations. This decoder also has access to the latent 'textual' representation learned in stage 1, and is again trained with 2 objectives: to predict our target labels (emotion or speaker IDs), and to minimize the amount of information encoded in the vector. This objective should ensure that the representation learned in stage 2 avoids encoding textual information -since the decoder already has access to it and the information minimization term discourages redundancy. Instead, it is expected to capture additional acoustic characteristics that are beneficial for the specific task.\n\nIn Section 3 we describe the models, dataset and training regime we use. In Sections 4 and 5 we evaluate our framework. We obtain highly compressed latent representations, that yield strong performance on both the standard transcription and our two target tasks. Moreover, our probing  (Alain & Bengio, 2016)  experiments show that the representations we obtain are almost perfectly disentangled from each other: while textual latent representations can predict transcriptions as effectively as the original speech representations, they exhibit random performance when predicting acoustic features (e.g., pitch or speaker identity). In contrast, acoustic representations excel at predicting these features but show random performance for transcriptions, validating the effectiveness of our disentanglement approach. The textual latent representations produced in the first stage are independent of the target task and can be easily applied to new downstream tasks.  1 Finally, we analyze the emergence of the two types of representations in the original speech model. In Section 6, we trace back the contributions of the different layers of pre-trained and fine-tuned versions of the Wav2Vec2  (Baevski et al., 2020)  to the representation of emotion. We find that as we progress through the layers, the acoustic contribution to emotion recognition significantly decreases in models fine-tuned on ASR, while the textual contribution increases, as they benefit from more accurate transcription and understanding of word polarity. Additionally, in Section 7, we qualitatively demonstrate that our disentanglement framework can serve as a feature attribution method to highlight the most significant frame representations for a given target task. Unlike existing gradient-based methods  (Sundararajan et al., 2017) , our approach allows us to determine whether a frame's contribution is textual or acoustic. Such disentangled attribution techniques can have many applications, e.g., in psychiatric research where the mismatch between textual and acoustic emotion expression was shown predictive of mood and disorders  (Niu et al., 2023)  or in bias control within speech agents.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Disentanglement Framework",
      "text": "This section explains how we build upon the Information Bottleneck principle to disentangle textual and acoustic information within speech representations that contribute to a targeted downstream task.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Variational Information Bottleneck",
      "text": "The core idea of an Information Bottleneck (IB,  Tishby et al., 2000)  is to learn a stochastic encoding Z that maximizes the prediction of a target variable Y , while minimizing the information retained about the input H. Accordingly, the loss function to be optimized based on this principle can be defined as follows:\n\nwhere I(., .) represents mutual information that measures the dependence between two variables. The coefficient β ≥ 0 controls the trade-off between retaining information about either H or Y in Z.\n\nThe exact computation of IB loss, however, is intractable. To address this, a Variational Information Bottleneck  (VIB, Alemi et al., 2016)  has been proposed, it establishes a lower bound on the IB loss    in Equation  1 . The VIB loss is defined as 2\n\nInformation loss\n\n(2)\n\nwhere N denotes the sample size, p ϕ (y|z) acts as the decoder, a neural network that predicts the label y from the latent representation z, 3  while p θ (z|h) serves as a stochastic encoder, mapping the input h to the representation z; r(z) approximates the marginal p(z). The first term in the loss function encourages the encoder to preserve information relevant to the label, while the second term, the KL divergence, pushes it to discard as much information as possible. The approximate marginal r(z) is typically assumed to be a spherical Gaussian  (Alemi et al., 2016) . The encoder p(z|h) is parameterized using an MLP to predict the mean µ and the diagonal covariance matrix Σ: p θ (z|h) = N (z|µ θ (h), Σ θ (h)). The optimization is performed using the reparameterization trick (Kingma & Welling, 2013).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Our Proposed Method",
      "text": "Consider a sequence of speech representations (h 1 , ..., h T ), produced by a pre-trained speech model, where h t represents the speech frame representation at timestamp t, and T denotes the total number of frames in a given utterance. Given a target task, our goal is to decompose each frame representation into two distinct latent representations: z textual t and z acoustic t . Our approach to disentangling these two latent representations involves two stages, sketched in Figure  1 . Both stages use frozen speech representations obtained from the same speech model as input.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Stage 1",
      "text": "In the first stage, we aim to distill the textual content from speech frame representations, while discarding other non-textual features. The textual capability of a speech model is typically evaluated using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which assesses how accurately the model can transcribe spoken utterances based on their representations.\n\nWe extract all speech frame representations for a given audio from a pre-trained speech model and compute a weighted average across model layers, with the weights learned during training. These frame representations are then given as input to a bottleneck encoder, which compresses the information into low-dimensional latent frame representations. To decode transcriptions from the latent frame representations, we employ the Connectionist Temporal Classification (CTC,  Graves et al., 2006)  loss as the task loss term in Equation 2, thus minimizing the following loss function:\n\nIn this way, we force the bottleneck encoder p(z textual |h) to retain only the information necessary for transcription (as encouraged by the CTC loss), while discarding irrelevant features in the original representation (h) (constrained by the information loss 4  ). We refer to the latent frame representation for frame t learned at this stage as z textual t . Intuitively, these latent representations capture the minimum statistics of speech representations needed for decoding transcriptions.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Stage 2",
      "text": "Our goal in the second stage is to capture acoustic features that complement the textual features learned in stage 1 and contribute to the downstream task. To achieve this, we replace the task loss in Equation 2 with the Cross-Entropy loss (CE) over the target class labels, thus, minimizing the following loss function:\n\nUsing labeled data for our target task in this stage, we extract speech frame representations from the same speech model and again learn a weighted average over layers. These representations are then fed into a bottleneck encoder p(z acoustic |h) -with the same architecture as the encoder in stage 1 -to form the complimentary latent representations (z acoustic t\n\n). We apply the information loss to the frame-wise latent representations at the output of the encoder. Subsequently, we pass these latent representations through an attention layer 5 to have latent representations at the utterance level, since labels for our target tasks are assigned to the whole utterance. Finally, the pooled latent representation z acoustic is concatenated with the frozen textual latent representations z textual (previously trained in Stage 1) to decode the target task. This conditional setup encourages the trainable latent representations to retain only non-textual features, particularly those absent in the textual latent representations. During the training process in this stage, no gradient updates are directed back to the z textual t learned in stage 1.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training Details",
      "text": "For training VIB, we follow Alemi et al. (  2016 ) and model r(z) and p(z|h) as multivariate Gaussian distributions: r(z) = N (z|µ = 0, Σ = 1) and p(z|h) = N (z|µ(h), Σ(h)). The bottleneck encoders to estimate µ and Σ consist of two shared linear layers with the same dimensionality as the original hidden representations (h t ), followed by independent d-dimensional layers for each, with GELU  (Hendrycks & Gimpel, 2016)  activation functions in between. We experiment with different bottleneck dimensions d = {16, 32, 64, 128, 256} as the output size of the bottleneck encoders. To estimate the gradients, we employ the reparameterization trick (Kingma & Welling, 2013): z t = µ(h t ) + Σ(h t ) ⊙ ϵ, where ϵ is sampled from the normal distribution N (0, 1). During inference, we use z t = µ(h t ). In our implementation, we gradually increase the β coefficient linearly from 0.1 to 1 during training. For decoding, we utilize a randomly initialized linear projection into C classes, where C = 32 for transcription (corresponding to the number of target characters), C = 4 for emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed in Appendix A.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Target Models",
      "text": "To obtain speech representations, we use two prominent self-supervised speech models: Wav2Vec2  (Baevski et al., 2020)  and HuBERT  (Hsu et al., 2021)  in their different sizes: Base (12-Transformer layers, 768-hidden size) and Large (24, 1024), obtained from the HuggingFace  (Wolf et al., 2020)  library. Our experiments include both pre-trained (on raw speech) and fine-tuned 6  (on transcribed speech) versions of these models. Both models employ the Transformer  (Vaswani et al., 2017)  architecture and learn speech representations through masked prediction in a self-supervised manner. Wav2Vec2 employs a contrastive loss to identify the masked speech frame among distractors, while HuBERT uses k-means clustering to create prediction targets. The models are further fine-tuned with additional labeled speech data by optimizing a linear classifier using CTC loss to decode transcription.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data",
      "text": "Transcription. For training in stage 1, we use subsets of two widely used read speech corpora: LibriSpeech  (Panayotov et al., 2015)  and Mozilla's Common Voice 17.0  (Ardila et al., 2019) . The former is derived from audiobooks, while the latter is recorded by contributors reading sentences displayed on a screen. We randomly select 5,000 examples from each corpus, ensuring an equal representation of gender and speaker ID, with each sample having a maximum duration of 14 seconds. Following  (Baevski et al., 2020) , we remove non-spoken special characters (e.g., commas and periods) from transcriptions, as these are not included in our target vocabulary. We reserve 1000 examples from each corpus as test data, leaving the remaining for training, which results in 17.4, and 3.2 hours of transcribed speech data, in total, for training and test sets, respectively.\n\nTarget tasks. For emotion data in stage 2, we utilize IEMOCAP  (Busso et al., 2008)  database, which consists of five dyadic sessions involving ten speakers (5 male, 5 female). Following prior research  (Li et al., 2021; 2022) , we exclude utterances without transcripts and combine Happy and Excited labels to form a 4-way classification task. We then undersample the dataset to balance emotion classes, resulting in 4,064 utterances (∼ 5 hours of audio, with an average duration of 4.4 seconds per segment). Each utterance is assigned one emotion from the label set: {Angry, Happy, Neutral, Sad}. For Speaker Identity as our target task, we utilize a subset of Mozilla's Common Voice 17.0 dataset  (Ardila et al., 2019) , consisting of 4,000 training and 1,000 test samples, stratified by two genders (male and female) and 24 speaker identities. All audio files in this study are resampled to 16 kHz to match the sampling rate used for the pre-training data of the target models.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Task Performance After Vib Training",
      "text": "In this section, we test the effectiveness of the disentangled representations against the original entangled representations on the downstream tasks. We will verify the disentanglement in the next section. For comparison, we also train identically structured decoders which rely on the original hidden states (h t ); the difference with VIB training is that the information loss is excluded during the training process. The performance of these classifiers (which can be viewed as a repurposed form of probing  (Alain & Bengio, 2016; Tenney et al., 2019) ) serves as a strong baseline, representing the performance achievable relying on the hidden states, without compressing them into latent representations.\n\nTable  1  reports both VIB and probing performances, averaged over three runs with different random seeds, for transcription, emotion recognition, and speaker identification tasks. For the transcription task at stage 1, VIB demonstrates a similar or sometimes even lower word error rate (WER) compared to probing classifiers, implying the success of VIB training in compressing essential information for audio transcription (WER = 100 serving as the performance of the random baseline). 7  As expected, representations derived from fine-tuned models show better transcription performance compared to those from pre-trained models (for both VIB and probing) as they are specifically tuned for transcription. The same success is evident in decoding our target tasks in stage 2, where we report the accuracy of both VIB and probing classifiers for emotion recognition and speaker identification tasks (the random baseline accuracy is 25 and 4.1, respectively). Overall, the classifier benefits more from the specialized, compressed representations than from the original hidden states, as VIB encourages retaining only task-relevant information, resulting in more robust representations.\n\nWe also found VIB performance consistent across various bottleneck dimensions; see the results in Appendix B. Both VIB and probing learn weights for layer averaging (see Figure  1 ). These weights are necessary because the information is not uniformly distributed across the NSM layers. As shown in Appendix D, both approaches use these weights; the weights provide insights into the contribution of individual layers to the acoustic and textual features.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Evaluation Of Disentanglement",
      "text": "",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Sanity Check Probing",
      "text": "The product of our training procedure (described in Sec. 2) are disentangled latent representations z textual t and z acoustic t for each speech frame representation h t . In this section, we investigate these latent representations to validate if they are truly disentangled by probing them for various types of textual and acoustic information. Given an audio input, we expect that z textual t , which is specialized for text, should not encode any aspects of the acoustic characteristics of the audio. In contrast, z acoustic t , which is specialized for audio features, should not contain any information about the audio transcription.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Setup",
      "text": "To assess textual capability, we train a linear probing classifier on frozen latent representations to predict their transcriptions. To estimate acoustic capability, we train a set of probing classifiers followed by an attention layer on latent representations to predict various acoustic features at the utterance level, including Mean Intensity, Mean Pitch, Gender, and Speaker Identity. For comparison. we also probe the original hidden states for the same objectives with the identically-structured classifiers. We utilize a subset of Mozilla's Common Voice 17.0 dataset  (Ardila et al., 2019) , consisting of 4,000 training and 1,000 test samples, stratified by two genders (male and female) 8  and 24 speakers. We extract labels for acoustic features directly from the raw audio waveforms using the Parselmouth toolkit  (Jadoul et al., 2018) . We then discretize them into four equally sized buckets based on quantiles to cast the task as a four-way classification problem. Gender (Accuracy)  ) learned at stages 1 and 2, along with hidden states derived from Large models for transcription and a set of audio features. For the WER metric, the lower score is better, while for other metrics, the higher is better. The dashed line in each plot represents the random baseline.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results",
      "text": "Figure  2  illustrates the classification results for Large models. Dashed lines represent the random baseline performance. As we can see, acoustic latent representations (for both target tasks; z acoustic emotion and z acoustic speaker ) exhibit no awareness of the textual content of the audio as their performance for CTC matches the random baseline (WER = 1). Conversely, textual latent representations are as effective as the original hidden states and -for pre-trained models -even outperform them at decoding transcription.\n\nLooking into predicting acoustic features, textual latent representations consistently show random performance, suggesting no acoustic features are encoded within them. Acoustic latent representations, however, show substantial probing performance despite not having any explicit acoustic objective in their training at stage 2. Interestingly, acoustic latent representations for the task of speaker identification are better at encoding acoustic features than those of emotion recognition. It is likely due to acoustic information playing a greater role in revealing the speaker identity.\n\nAdditionally, in contrast to z acoustic speaker , acoustic latent representations for emotion (z acoustic emotion ) do not match the performance of the original hidden states. For example, for the Wav2Vec2 model, the probing performance for Speaker ID based on hidden states is 0.98, while it is 0.26 for acoustic latent representations (the random baseline is 1/24 ≈ 0.04). This disparity suggests that not all those acoustic features encoded in the hidden representations are crucial for recognizing emotion, thus, not all of those features were retained in stage 2. These findings could be important in real-world scenarios where, for privacy protection, encoding acoustic features without precisely identifying the speaker can be essential. The pattern of the probing results is also similar for Base models and for various bottleneck dimensions, as reported in Appendix E.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Qualitative Evaluation",
      "text": "Next, we visualize the latent representations to gain insight into how they have been encoded in the representation space for emotion recognition. We make use of RAVDESS (Livingstone & Russo, 2018) dataset, which contains only two identical statements spoken by 24 actors (12 male, 12 female) with 4 different emotions. This makes it ideal for this analysis, as the linguistic content remains constant across utterances with different emotions. We select examples with matching emotion labels from the IEMOCAP dataset, resulting in 384 utterances (each averaging 3.7 seconds). We obtain the speech representations from the large Wav2Vec2 model and generate their corresponding latent representations using the bottleneck encoders trained in stages 1 and 2 by doing only inference without any further training. Then, we compute the average of frame representations over all frames in each utterance and apply t-SNE.\n\nFigure  3  shows a 2D projection of these latent representations with data points marked and colored according to their transcription and emotion labels, respectively. The textual latent representations (learned in stage 1) are perfectly clustered according to their transcriptions. In contrast, the acoustic latent representations (learned in stage 2) are not separated by transcription, suggesting that no textual information is retained there. Instead, these acoustic representations are roughly clustered by emotion, which nicely aligns with our desired goal. The pattern is also the same for the Base model, reported in Appendix F.1.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "Layerwise Emotion Contribution",
      "text": "With our disentangled framework established and validated, we can now quantify, separately, the extent to which each layer in a speech model contributes textually and acoustically to the target task of emotion recognition. We first train the latent representations using the same setup as we described in Section 2, but instead of using a weighted layer average in the input, we use the frame representations from a specific layer of the model. The layerwise results for stages 1 and 2 are shown in Figure  4 , with the black horizontal dashed line representing random performances.\n\nFor HuBERT, the transcription ability improves as we move through the layers. In contrast, Wav2Vec2 shows a U-shaped pattern, with the best performance in the middle layers and the final layers being unable to decode transcription. For Wav2Vec2-FT, however, transcription performance improves sharply in the last layers, which is expected given that the model is fine-tuned for ASR, indicating substantial changes in the last layers during fine-tuning. In stage 2 (middle panel), models perform best at emotion recognition in the middle layers. Compared to pre-trained, the fine-tuned model (Wav2Vec2-FT) shows a significant decline in emotion recognition performance in the final layers, where these layers lose acoustic information in favor of encoding text transcription.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "Layerwise Emotion Probing",
      "text": "We next train a probing classifier (followed by an attention layer) on top of frozen z textual t and z acoustic t latent representations to decode emotion. Each latent representation is specialized to preserve either textual or acoustic features from the original hidden states of each model layer. Therefore, the probing performance reveals the extent to which these textual and acoustic features contribute to emotion recognition. For comparison, we also train the same probing classifier on the original hidden states across each layer.\n\nFigure  4  (right) illustrates emotion probing performances. Comparing pre-trained and fine-tuned models, the latent acoustic representations learned from the final layers of Wav2Vec-FT contribute significantly less to emotion recognition. This is likely due to the model losing some acoustic information during fine-tuning in favor of transcription capabilities. We can see, however, that these layers benefit more from textual features in predicting emotion, as their representations offer more accurate transcriptions. Also, Compared to Wav2Vec2, HuBERT shows a greater contribution to emotion recognition; acoustically in the middle layers, and textually in the last layer.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Localizing Salient Textual And Acoustic Frames",
      "text": "In this section, we use our disentanglement framework to localize salient input features for the target tasks. The attention layer in stage 2 of our framework (see Figure  1 ) can be used to identify those frames in the original audio input whose latent representations contribute most to our target tasks. Crucially, the disentanglement mechanism allows us to clearly separate the contributions of acoustic features from those of textual features, providing insight into their individual roles. This disentangled attribution could be particularly useful in fields like psychiatry, where differences between textual and acoustic emotional expressions can aid in the diagnosis of disorders  (Niu et al., 2023) , or in detecting bias in speech agents' responses to user requests. While a comprehensive evaluation of disentangled attribution is beyond the scope of this work, we do conduct a preliminary investigation.\n\nFinding salient input features for model decisions is often done by computing the gradient of the model's output with respect to the inputs  (Sundararajan et al., 2017; Ancona et al., 2018; Yuan et al., 2019; Samek et al., 2019) . To compare our attention scores with gradient-based attribution scores, we train another classifier for the target task on the original hidden states of the Wav2Vec2 model and use Integrated Gradients (IG) to compute attribution to individual frames  (Sundararajan et al., 2017) .\n\nWe normalize the IG scores to sum to 1, ensuring a fair comparison with attention scores.\n\nWe then obtain frame-by-frame distributions of both acoustic and sentiment features to compare them directly with attribution results. For acoustic features, we focus on intensity and pitch, identifying and representing peaks and valleys in their temporal patterns. To analyze sentiment, we use the spaCy toolkit  (Honnibal & Montani, 2017)  to annotate word-level polarity (positivity, negativity, or neutrality, with 0 representing neutrality) within the utterances. Using Montreal Forced Aligner  (McAuliffe et al., 2017) , we can map these word-level labels to frames (see details in Appendix G). As a result, each feature vector for a speech frame includes the following dimensions: (1) the presence of a peak or valley in intensity, (2) the same for pitch, and (3) the presence of a 'sentiment-laden' word, all normalized to the range [0, 1].\n\nWe then compute the dot product between the frame-wise attribution scores and the corresponding feature vectors. Figure  5  presents these results, averaged across all examples in the IEMOCAP test set. The figure demonstrates that acoustic attention effectively captures peaks and valleys in acoustic features (intensity and pitch), while textual attention focuses on word polarity. Both have higher agreement with features than Integrated Gradient scores (see Appendix H for qualitative examples). Note that textual attention exhibits fairly high similarity with acoustic features as polar words are often pronounced with emotional emphasis.",
      "page_start": 9,
      "page_end": 10
    },
    {
      "section_name": "Related Work",
      "text": "Learning disentangled representations of complex data, such that distinct factors are separable and controllable has been the focus of many studies.  Bengio et al. (2013)  motivate this approach within the context of deep learning, while  Wang et al. (2024)  provide a general up-to-date overview.\n\nFor speech, disentanglement has been used for controllable style transfer in voice conversion, where the goal is to modify para-linguistic information while preserving linguistic content. Speaker conversion is a canonical example of this line of work: for example van den Oord et al. (  2017 ) evaluate their modality-agnostic discrete representation learning framework on this task, while  Polyak et al. (2021)  apply discrete representation learning specifically for speech re-synthesis. In an alternative approach,  Qian et al. (2019)  propose AutoVC, as a style transfer autoencoder framework, that disentangles speaker identity information from content by training an autoencoder conditioned on speaker identity representations (encoded by another model).  Nagrani et al. (2020)  exploit videos containing speaker faces as a self-supervision signal for learning disentangled speech embeddings. SpeechSplit  (Qian et al., 2020)  extends AutoVC by constraining latent representation dimensions and adding noise, splitting speech representations into four specific attributes (content, pitch, timbre, and rhythm).\n\nWith the assumption that most voice synthesis tasks can be defined by controlling pitch, amplitude, linguistic content, and timbre,  Choi et al. (2021; 2022)  develop a framework for manipulating and synthesizing voice signals by autoencoding these four properties and reconstructing the waveform using a combination of information perturbation and bottleneck techniques. In a different approach  Wang et al. (2021)  trained an adversarial network to minimize correlations between the representations of content, pitch, timbre and rhythm.  Wang et al. (2023)  propose a multi-modal information bottleneck approach aimed at disentangling relevant visual and textual features to enhance the interpretability of vision-language models.\n\nQu et al. (  2024 ) build upon the quantization module of HuBERT  (Hsu et al., 2021)  and propose Prosody2Vec, which consists of four modules: a unit encoder that employs a deep network to map the discrete speech representations produced by the HuBERT's quantization module to continuous latent representations; a pre-trained deep network to encode speaker; a prosody encoder; and a decoder that reconstructs mel-spectrograms from the outputs of the three encoders. The trainable parameters are first pre-trained on spontaneous speech and then fine-tuned for emotion recognition. In the current study, our goal is to propose a general method which accommodates many possible factors of variations and makes it possible to configure them. Unlike Prosody2Vec, our approach requires fewer trainable parameters, is directly trained on the target task, and is applicable to any speech model.",
      "page_start": 10,
      "page_end": 10
    },
    {
      "section_name": "Conclusions",
      "text": "Our work presents a disentanglement framework based on the Information Bottleneck principle, which effectively separates entangled representations of neural speech models into distinct textual and acoustic components, while retaining only features relevant to the target tasks. In our experiments with the emotion recognition and speaker identification tasks, we have demonstrated that the framework can effectively isolate key features, improving interpretability, whilst maintaining the performance of the original model. We believe the framework holds potential for a range of applications where disentanglement is key. One important class of such applications are speech recognition systems where privacy-preservation is a key concern for responsible deployment (and sometimes even a legal requirement); separating sensitive speaker-specific information from task-relevant information is essential in such systems. The framework is also proving useful, as we showed, in providing a route to perform disentangled feature attribution, revealing the most significant speech frame representations from both textual and acoustic perspectives. We hope our findings thus help advance neural speech technology, by helping to improve transparency and control.\n\nA unique aspect of our framework is its modality-independent architecture: in principle, any type of entangled multi-modal representation can be disentangled as long as the right pair of tasks are selected for training in stage 1 and 2. This opens up the application of the framework beyond just neural speech recognition, for instance to grounded language models, where we might want to disentangle linguistic and visual information. We leave further exploration of these possibilities for future work.\n\nA TRAINING HYPERPARAMETERS\n\nIn our experiments, all models were trained for 50 epochs using the AdamW optimizer with gradient norm clipping. The learning rate was set to 1e-3 and 1e-4 for Base and Large models, respectively, with a warmup ratio of 0.1 and a cosine decay scheduler, together with weight decay. Specifically for training transcription prediction using CTC loss, we use a batch size of 1; the effective batch size here is the number of frame representations since the CTC loss is computed across all frames. For other training objectives, we use a batch size of 8.",
      "page_start": 10,
      "page_end": 14
    },
    {
      "section_name": "B Ablation On The Bottleneck Dimension",
      "text": "",
      "page_start": 14,
      "page_end": 14
    },
    {
      "section_name": "D Layer Weight Averaging",
      "text": "In the input of both VIB and probing training setups in Section 4, we train weights for layer averaging (see Figure  1 ). Let us have a look at these trained layer weights to see the contribution of each layer at each stage. Figure D.1 shows these layer weights for both Base and Large models. Overall, layer weights learned in the VIB setup closely follow the weights trained with original hidden states in the probing setup.\n\nIn stage 1 we observe that, in pre-trained models, the upper-middle layers are more capable of decoding transcription. In fine-tuned models, however, this information shifts to and becomes concentrated at the final layer. This pattern is consistent across both Base and Large model sizes as well as for different bottleneck dimensions.\n\nFor emotion recognition and speaker identity tasks in stage 2, however, there is a notable difference between Base and Large models. In Base models, the earlier layers contribute more in both pre-trained and fine-tuned setups. In contrast, layers in Large models contribute uniformly, suggesting that the acoustic information useful for these target tasks is distributed across layers in Large models.",
      "page_start": 15,
      "page_end": 15
    },
    {
      "section_name": "G Aligning Frame Representations With Audio Frames And Transcription",
      "text": "To map the frame representations to their original input frames and their corresponding word transcriptions, we use Montreal Forced Aligner  (McAuliffe et al., 2017, MFA) . Using this tool, we extract the start time (t s ) and the end time (t e ) of each word in an utterance, and map them to boundary frames f s and f e :\n\nwhere T and T denote the total time of a given audio and the total number of frames in the frame representation, respectively. and intensity), for two utterances from the IEMOCAP  (Busso et al., 2008)  test dataset labeled 'Happy'. Both figures are vertically segmented according to the word's time stamps in the utterances. In the left panel, the textual attention highlights part of the frames corresponding to the positive words \"THANKS\" and \"NICE\", while, the acoustic attention peaks at a frame where there is a drastic change for both pitch and intensity, at the beginning of the pronunciation of the word \"OH,\". In the right panel for the second utterance, we can see a uniform pattern for textual attention, which makes sense as there is no textual cue in the utterance for emotion recognition. However, the acoustic attention",
      "page_start": 15,
      "page_end": 16
    },
    {
      "section_name": "H Qualitative Examples For Feature Attribution",
      "text": "",
      "page_start": 16,
      "page_end": 16
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: An overview of our disentanglement framework: In stage 1, hidden states derived from a",
      "page": 3
    },
    {
      "caption": "Figure 1: Both stages use frozen speech",
      "page": 3
    },
    {
      "caption": "Figure 1: ). These weights",
      "page": 6
    },
    {
      "caption": "Figure 2: Probing performances of latent representations (d=128) learned at stages 1 and 2, along",
      "page": 7
    },
    {
      "caption": "Figure 2: illustrates the classification results for Large models. Dashed lines represent the random",
      "page": 7
    },
    {
      "caption": "Figure 3: t-SNE plots of the textual and acoustic latent representations for Wav2Vec2-Large, marked",
      "page": 8
    },
    {
      "caption": "Figure 3: shows a 2D projection of these latent representations with data points marked and colored",
      "page": 8
    },
    {
      "caption": "Figure 4: , with the black horizontal dashed line representing random performances.",
      "page": 8
    },
    {
      "caption": "Figure 4: (right) illustrates emotion probing performances. Comparing pre-trained and fine-tuned",
      "page": 8
    },
    {
      "caption": "Figure 4: Layerwise performance of VIB (d=128) for transcription at stage 1 (left) and emotion",
      "page": 9
    },
    {
      "caption": "Figure 1: ) can be used to identify those",
      "page": 9
    },
    {
      "caption": "Figure 5: presents these results, averaged across all examples in the IEMOCAP test",
      "page": 9
    },
    {
      "caption": "Figure 5: Dot product of attribution scores and different acoustic and textual cues.",
      "page": 10
    },
    {
      "caption": "Figure 1: ). Let us have a look at these trained layer weights to see the contribution of each layer",
      "page": 15
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": "to which such representations rely on textual and"
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        },
        {
          "ABSTRACT": ""
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "representations from both the textual and acoustic perspectives.": "1\nINTRODUCTION"
        },
        {
          "representations from both the textual and acoustic perspectives.": "The internal activation vectors of most modern deep learning systems,\nincluding Neural Speech"
        },
        {
          "representations from both the textual and acoustic perspectives.": "Models (NSM) such as Wav2Vec2 (Baevski et al., 2020), HuBERT (Hsu et al., 2021), and Whisper"
        },
        {
          "representations from both the textual and acoustic perspectives.": "(Radford et al., 2022), are highly entangled. This means that distinct input characteristics – such as"
        },
        {
          "representations from both the textual and acoustic perspectives.": "fundamental frequency, loudness, syntactic category, or semantic features of a spoken word—are not"
        },
        {
          "representations from both the textual and acoustic perspectives.": "separated into individual dimensions within the model’s latent space – but are instead intertwined"
        },
        {
          "representations from both the textual and acoustic perspectives.": "within the same ones. Entanglement is a major obstacle for our ability to interpret and to intervene;"
        },
        {
          "representations from both the textual and acoustic perspectives.": "disentanglement,\nto the extent\nthat\nit\nis possible and even if imperfect,\nis therefore often highly"
        },
        {
          "representations from both the textual and acoustic perspectives.": "desirable. For instance, when state-of-the-art NSMs are used in critical situations, we may want to be"
        },
        {
          "representations from both the textual and acoustic perspectives.": "able to guarantee that information about the speaker’s identity, gender, or health characteristics are not"
        },
        {
          "representations from both the textual and acoustic perspectives.": "used in downstream applications. However, the entangled nature of the NSM’s internal representation"
        },
        {
          "representations from both the textual and acoustic perspectives.": "makes it difficult to surgically suppress such acoustic information."
        },
        {
          "representations from both the textual and acoustic perspectives.": "In this paper, we investigate the extent to which we can learn to disentangle neural representations."
        },
        {
          "representations from both the textual and acoustic perspectives.": "Although our proposed disentanglement framework is not modality-specific, in this paper we focus"
        },
        {
          "representations from both the textual and acoustic perspectives.": "on two speech-related downstream tasks. As our first case study, we choose emotion recognition"
        },
        {
          "representations from both the textual and acoustic perspectives.": "as the target task: while the content of an utterance is a strong cue for detecting the emotion of the"
        },
        {
          "representations from both the textual and acoustic perspectives.": "speaker,\nthe additional acoustic signals offer significant advantages over text-based models. For"
        },
        {
          "representations from both the textual and acoustic perspectives.": "example, consider the utterances “I’m so happy” and “I’m fine,” where the former explicitly conveys"
        },
        {
          "representations from both the textual and acoustic perspectives.": "happiness through its content, whereas the latter can represent a wide range of emotions depending"
        },
        {
          "representations from both the textual and acoustic perspectives.": "on prosody and tone. Our second case study closely follows the setup of the first, but we choose"
        },
        {
          "representations from both the textual and acoustic perspectives.": "speaker identification as the target task. Here we hypothesize that text offers only limited information,"
        },
        {
          "representations from both the textual and acoustic perspectives.": "and the acoustic information will play an even greater role in revealing the identity of the speaker"
        },
        {
          "representations from both the textual and acoustic perspectives.": "than in the case of emotion recognition."
        },
        {
          "representations from both the textual and acoustic perspectives.": "In Section 2, we propose a two-stage disentanglement framework (sketched in Figure1) based on"
        },
        {
          "representations from both the textual and acoustic perspectives.": "the Information Bottleneck principle (Tishby et al., 2000; Alemi et al., 2016) to disentangle textual"
        },
        {
          "representations from both the textual and acoustic perspectives.": "and acoustic features encoded in NSMs.\nIn stage 1 of our\nframework, we train a decoder with"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "minimize the presence of irrelevant information in these representations. The goal is to ensure that"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "the latent representation preserves only the speech features necessary for accurate transcription, while"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "filtering out any extraneous characteristics. In stage 2, we train a second decoder on the same speech"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "representations. This decoder also has access to the latent ‘textual’ representation learned in stage 1,"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "and is again trained with 2 objectives:\nto predict our target labels (emotion or speaker IDs), and to"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "minimize the amount of information encoded in the vector. This objective should ensure that the"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "representation learned in stage 2 avoids encoding textual information – since the decoder already has"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "access to it and the information minimization term discourages redundancy. Instead, it is expected to"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "capture additional acoustic characteristics that are beneficial for the specific task."
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "In Section 3 we describe the models, dataset and training regime we use.\nIn Sections 4 and 5 we"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "evaluate our\nframework. We obtain highly compressed latent\nrepresentations,\nthat yield strong"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "performance on both the standard transcription and our two target\ntasks. Moreover, our probing"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "(Alain & Bengio, 2016) experiments show that the representations we obtain are almost perfectly"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "disentangled from each other: while textual\nlatent\nrepresentations can predict\ntranscriptions as"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "effectively as the original speech representations, they exhibit random performance when predicting"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "acoustic features (e.g., pitch or speaker\nidentity).\nIn contrast, acoustic representations excel at"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "predicting these features but show random performance for transcriptions, validating the effectiveness"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "of our disentanglement approach. The textual latent representations produced in the first stage are"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "independent of the target task and can be easily applied to new downstream tasks.1"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "Finally, we analyze the emergence of the two types of representations in the original speech model."
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "In Section 6, we trace back the contributions of the different\nlayers of pre-trained and fine-tuned"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "versions of the Wav2Vec2 (Baevski et al., 2020) to the representation of emotion. We find that"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "as we progress through the layers,\nthe acoustic contribution to emotion recognition significantly"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "decreases in models fine-tuned on ASR, while the textual contribution increases, as they benefit"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "from more accurate transcription and understanding of word polarity. Additionally,\nin Section 7,"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "we qualitatively demonstrate that our disentanglement framework can serve as a feature attribution"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "method to highlight the most significant frame representations for a given target task. Unlike existing"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "gradient-based methods (Sundararajan et al., 2017), our approach allows us to determine whether a"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "frame’s contribution is textual or acoustic. Such disentangled attribution techniques can have many"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "applications, e.g., in psychiatric research where the mismatch between textual and acoustic emotion"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "expression was shown predictive of mood and disorders (Niu et al., 2023) or in bias control within"
        },
        {
          "two objectives:\nto map the internal\nrepresentation of an existing speech model\nto text, but also": "speech agents."
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": "Stage 2"
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": "Target task"
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        },
        {
          "Stage 1": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1": ""
        },
        {
          "1": ""
        },
        {
          "1": ""
        },
        {
          "1": ""
        },
        {
          "1": ""
        },
        {
          "1": ""
        },
        {
          "1": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "assesses how accurately the model can transcribe spoken utterances based on their representations."
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "We extract all speech frame representations for a given audio from a pre-trained speech model"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "and compute a weighted average across model\nlayers, with the weights learned during training."
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "These frame representations are then given as input to a bottleneck encoder, which compresses the"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "information into low-dimensional latent frame representations. To decode transcriptions from the"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "latent frame representations, we employ the Connectionist Temporal Classification (CTC, Graves"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "et al., 2006) loss as the task loss term in Equation 2, thus minimizing the following loss function:"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "(3)\nLCTC − βLInformation"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "In this way, we force the bottleneck encoder p(ztextual|h) to retain only the information necessary"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "for transcription (as encouraged by the CTC loss), while discarding irrelevant features in the original"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "representation (h) (constrained by the information loss4). We refer to the latent frame representation"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "for frame t learned at\nthis stage as ztextual\n.\nIntuitively,\nthese latent representations capture the\nt"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "minimum statistics of speech representations needed for decoding transcriptions."
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "2.2.2\nSTAGE 2"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "Our goal\nin the second stage is to capture acoustic features that complement\nthe textual features"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "learned in stage 1 and contribute to the downstream task. To achieve this, we replace the task loss"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "in Equation 2 with the Cross-Entropy loss (CE) over the target class labels,\nthus, minimizing the"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "following loss function:"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "(4)\nLCE − βLInformation"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "Using labeled data for our target\ntask in this stage, we extract speech frame representations from"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "the same speech model and again learn a weighted average over layers. These representations are"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "then fed into a bottleneck encoder p(zacoustic|h) — with the same architecture as the encoder in"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "stage 1 — to form the complimentary latent representations (zacoustic\n). We apply the information\nt"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "loss to the frame-wise latent representations at\nthe output of the encoder. Subsequently, we pass"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "these latent representations through an attention layer5 to have latent representations at the utterance"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "level, since labels for our target tasks are assigned to the whole utterance. Finally, the pooled latent"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "representation zacoustic is concatenated with the frozen textual\nlatent\nrepresentations ztextual"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "(previously trained in Stage 1) to decode the target\ntask. This conditional setup encourages the"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "trainable latent representations to retain only non-textual features, particularly those absent in the"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "textual\nlatent representations. During the training process in this stage, no gradient updates are"
        },
        {
          "using automatic speech recognition (ASR), and measured by the word error rate (WER) metric, which": "learned in stage 1.\ndirected back to the ztextual\nt"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "in Appendix A."
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "3.2\nTARGET MODELS"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "To obtain speech representations, we use two prominent self-supervised speech models: Wav2Vec2"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "(Baevski et al., 2020) and HuBERT (Hsu et al., 2021) in their different sizes: Base (12-Transformer"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "layers, 768-hidden size) and Large (24, 1024), obtained from the HuggingFace (Wolf et al., 2020)"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "library. Our experiments include both pre-trained (on raw speech) and fine-tuned6 (on transcribed"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "speech) versions of\nthese models. Both models employ the Transformer\n(Vaswani et al., 2017)"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "architecture and learn speech representations through masked prediction in a self-supervised manner."
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "Wav2Vec2 employs a contrastive loss to identify the masked speech frame among distractors, while"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "HuBERT uses k-means clustering to create prediction targets. The models are further fine-tuned with"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "additional labeled speech data by optimizing a linear classifier using CTC loss to decode transcription."
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "3.3\nDATA"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "Transcription.\nFor training in stage 1, we use subsets of two widely used read speech corpora:"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "LibriSpeech (Panayotov et al., 2015) and Mozilla’s Common Voice 17.0 (Ardila et al., 2019). The"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "former is derived from audiobooks, while the latter is recorded by contributors reading sentences"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "displayed on a screen. We randomly select 5,000 examples from each corpus, ensuring an equal"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "representation of gender and speaker ID, with each sample having a maximum duration of 14 seconds."
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "Following (Baevski et al., 2020), we remove non-spoken special characters (e.g., commas and periods)"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "from transcriptions, as these are not included in our target vocabulary. We reserve 1000 examples"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "from each corpus as test data, leaving the remaining for training, which results in 17.4, and 3.2 hours"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "of transcribed speech data, in total, for training and test sets, respectively."
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "Target tasks.\nFor emotion data in stage 2, we utilize IEMOCAP (Busso et al., 2008) database,"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "which consists of five dyadic sessions involving ten speakers (5 male, 5 female). Following prior"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "research (Li et al., 2021; 2022), we exclude utterances without transcripts and combine Happy and"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "Excited labels to form a 4-way classification task. We then undersample the dataset\nto balance"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "emotion classes, resulting in 4,064 utterances (∼ 5 hours of audio, with an average duration of 4.4"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "seconds per segment). Each utterance is assigned one emotion from the label set: {Angry, Happy,"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "Neutral, Sad}. For Speaker Identity as our target task, we utilize a subset of Mozilla’s Common Voice"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "17.0 dataset (Ardila et al., 2019), consisting of 4,000 training and 1,000 test samples, stratified by"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "two genders (male and female) and 24 speaker identities. All audio files in this study are resampled"
        },
        {
          "emotion recognition, and C = 24 for speaker identification. The training hyperparameters are detailed": "to 16 kHz to match the sampling rate used for the pre-training data of the target models."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 1: VIB (d=128) and probing performance. Lower WER and higher Accuracy are better.",
      "data": [
        {
          "Stage 1": "Transcription (WER ↓)",
          "Stage 2": "Speaker Id. (Acc. ↑)"
        },
        {
          "Stage 1": "VIB",
          "Stage 2": "VIB"
        },
        {
          "Stage 1": "46.0",
          "Stage 2": "99.1"
        },
        {
          "Stage 1": "",
          "Stage 2": ""
        },
        {
          "Stage 1": "39.1",
          "Stage 2": "98.4"
        },
        {
          "Stage 1": "49.5",
          "Stage 2": "97.9"
        },
        {
          "Stage 1": "",
          "Stage 2": ""
        },
        {
          "Stage 1": "49.5",
          "Stage 2": "99.8"
        },
        {
          "Stage 1": "25.6",
          "Stage 2": "98.2"
        },
        {
          "Stage 1": "16.7",
          "Stage 2": "98.2"
        },
        {
          "Stage 1": "",
          "Stage 2": ""
        },
        {
          "Stage 1": "11.5",
          "Stage 2": "99.6"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table 1: VIB (d=128) and probing performance. Lower WER and higher Accuracy are better.",
      "data": [
        {
          "HuBERT-FT": "",
          "Large": "Base",
          "6.9": "10.3",
          "25.6": "16.7",
          "54.7": "63.5",
          "64.8": "56.6",
          "90.5": "99.6",
          "98.2": "98.2"
        },
        {
          "HuBERT-FT": "Wav2Vec2-FT",
          "Large": "",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "Large",
          "6.9": "7.7",
          "25.6": "11.5",
          "54.7": "62.1",
          "64.8": "63.4",
          "90.5": "98.5",
          "98.2": "99.6"
        },
        {
          "HuBERT-FT": "",
          "Large": "Table 1: VIB (d = 128) and probing performance. Lower WER and higher Accuracy are better.",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "Random baselines: WER = 100 for transcription, Accuracy = 25 for emotion, and Accuracy = 4.1 for",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "speaker identification. Scores are averaged over three runs with different random seeds.",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "representations derived from fine-tuned models show better transcription performance compared",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "to those from pre-trained models (for both VIB and probing) as they are specifically tuned for",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "transcription. The same success is evident in decoding our target tasks in stage 2, where we report the",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "accuracy of both VIB and probing classifiers for emotion recognition and speaker identification tasks",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "(the random baseline accuracy is 25 and 4.1, respectively). Overall, the classifier benefits more from",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "the specialized, compressed representations than from the original hidden states, as VIB encourages",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "retaining only task-relevant information, resulting in more robust representations.",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "We also found VIB performance consistent across various bottleneck dimensions; see the results in",
          "Large": "",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "Appendix B. Both VIB and probing learn weights for layer averaging (see Figure 1). These weights",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "are necessary because the information is not uniformly distributed across the NSM layers. As shown",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "in Appendix D, both approaches use these weights; the weights provide insights into the contribution",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        },
        {
          "HuBERT-FT": "",
          "Large": "of individual layers to the acoustic and textual features.",
          "6.9": "",
          "25.6": "",
          "54.7": "",
          "64.8": "",
          "90.5": "",
          "98.2": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CTC (WER)": "1.00 1.00\n1.00 1.00",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.80\n0.80\n0.79\n0.79\n0.78"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.72\n0.67"
        },
        {
          "CTC (WER)": "0.56",
          "Mean Intensity (Accuracy)": "0.56\n0.56\n0.55\n0.55\n0.55",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.53",
          "Mean Pitch (Accuracy)": "0.51"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.47\n0.43",
          "Mean Pitch (Accuracy)": "0.42"
        },
        {
          "CTC (WER)": "0.37",
          "Mean Intensity (Accuracy)": "0.38\n0.36",
          "Mean Pitch (Accuracy)": "0.38\n0.37"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.34\n0.33",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.26",
          "Mean Pitch (Accuracy)": "0.29\n0.27\n0.27\n0.27"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.26\n0.25",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "0.17",
          "Mean Intensity (Accuracy)": "0.24",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "0.14",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "Gender (Accuracy)",
          "Mean Intensity (Accuracy)": "Speaker Identity (Accuracy)",
          "Mean Pitch (Accuracy)": "Emotion (Accuracy)"
        },
        {
          "CTC (WER)": "1.00\n1.00\n1.00\n1.00\n0.99\n0.99",
          "Mean Intensity (Accuracy)": "1.00\n1.00\n0.99",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.98\n0.98\n0.98\n0.92",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.90",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "0.76",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "0.69",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.69\n0.67\n0.66"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.63\n0.63"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.62"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.57"
        },
        {
          "CTC (WER)": "0.52\n0.51",
          "Mean Intensity (Accuracy)": "0.52",
          "Mean Pitch (Accuracy)": "0.53\n0.51"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.50\n0.45"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.32",
          "Mean Pitch (Accuracy)": "0.34\n0.32"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.27",
          "Mean Pitch (Accuracy)": "0.30\n0.30"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.26",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.07\n0.06\n0.06\n0.05",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "2\n2\n2\nu\nu\nu": "v\nv\nv\nW\nW\nW\nH\nH\nH"
        },
        {
          "2\n2\n2\nu\nu\nu": "a\na\na"
        },
        {
          "2\n2\n2\nu\nu\nu": "W\nW\nW"
        },
        {
          "2\n2\n2\nu\nu\nu": "Model"
        },
        {
          "2\n2\n2\nu\nu\nu": "Figure 2: Probing performances of latent representations (d = 128) learned at stages 1 and 2, along"
        },
        {
          "2\n2\n2\nu\nu\nu": "with hidden states derived from Large models for transcription and a set of audio features. For the"
        },
        {
          "2\n2\n2\nu\nu\nu": "WER metric, the lower score is better, while for other metrics, the higher is better. The dashed line in"
        },
        {
          "2\n2\n2\nu\nu\nu": "each plot represents the random baseline."
        },
        {
          "2\n2\n2\nu\nu\nu": "5.1.2\nRESULTS"
        },
        {
          "2\n2\n2\nu\nu\nu": "Figure 2 illustrates the classification results for Large models. Dashed lines represent the random"
        },
        {
          "2\n2\n2\nu\nu\nu": "baseline performance. As we can see, acoustic latent representations (for both target tasks; zacoustic\nemotion"
        },
        {
          "2\n2\n2\nu\nu\nu": "and zacoustic\n) exhibit no awareness of the textual content of the audio as their performance for CTC"
        },
        {
          "2\n2\n2\nu\nu\nu": "matches the random baseline (WER = 1). Conversely, textual latent representations are as effective"
        },
        {
          "2\n2\n2\nu\nu\nu": "as the original hidden states and – for pre-trained models – even outperform them at decoding"
        },
        {
          "2\n2\n2\nu\nu\nu": "transcription."
        },
        {
          "2\n2\n2\nu\nu\nu": "Looking into predicting acoustic features, textual latent representations consistently show random"
        },
        {
          "2\n2\n2\nu\nu\nu": "performance, suggesting no acoustic features are encoded within them. Acoustic latent represen-"
        },
        {
          "2\n2\n2\nu\nu\nu": "tations, however, show substantial probing performance despite not having any explicit acoustic"
        },
        {
          "2\n2\n2\nu\nu\nu": "objective in their training at stage 2.\nInterestingly, acoustic latent representations for the task of"
        },
        {
          "2\n2\n2\nu\nu\nu": "speaker identification are better at encoding acoustic features than those of emotion recognition. It is"
        },
        {
          "2\n2\n2\nu\nu\nu": "likely due to acoustic information playing a greater role in revealing the speaker identity."
        },
        {
          "2\n2\n2\nu\nu\nu": "Additionally,\nin contrast\nto zacoustic\n, acoustic latent representations for emotion (zacoustic\n) do\nemotion"
        },
        {
          "2\n2\n2\nu\nu\nu": "not match the performance of the original hidden states. For example, for the Wav2Vec2 model,"
        },
        {
          "2\n2\n2\nu\nu\nu": "the probing performance for Speaker ID based on hidden states is 0.98, while it is 0.26 for acoustic"
        },
        {
          "2\n2\n2\nu\nu\nu": "latent representations (the random baseline is 1/24 ≈ 0.04). This disparity suggests that not all those"
        },
        {
          "2\n2\n2\nu\nu\nu": "acoustic features encoded in the hidden representations are crucial for recognizing emotion,\nthus,"
        },
        {
          "2\n2\n2\nu\nu\nu": "not all of those features were retained in stage 2. These findings could be important in real-world"
        },
        {
          "2\n2\n2\nu\nu\nu": "scenarios where, for privacy protection, encoding acoustic features without precisely identifying the"
        },
        {
          "2\n2\n2\nu\nu\nu": "speaker can be essential. The pattern of the probing results is also similar for Base models and for"
        },
        {
          "2\n2\n2\nu\nu\nu": "various bottleneck dimensions, as reported in Appendix E."
        },
        {
          "2\n2\n2\nu\nu\nu": "5.2\nQUALITATIVE EVALUATION"
        },
        {
          "2\n2\n2\nu\nu\nu": "Next, we visualize the latent representations to gain insight into how they have been encoded in the"
        },
        {
          "2\n2\n2\nu\nu\nu": "representation space for emotion recognition. We make use of RAVDESS (Livingstone & Russo,"
        },
        {
          "2\n2\n2\nu\nu\nu": "2018) dataset, which contains only two identical statements spoken by 24 actors (12 male, 12 female)"
        },
        {
          "2\n2\n2\nu\nu\nu": "with 4 different emotions. This makes it\nideal for this analysis, as the linguistic content remains"
        },
        {
          "2\n2\n2\nu\nu\nu": "constant across utterances with different emotions."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "20": "30\n20\n10\n0\n10\n20\n30\n30\n20\n10\n0\n10\n20\n30"
        },
        {
          "20": "t-SNE plots of the textual and acoustic latent representations for Wav2Vec2-Large, marked"
        },
        {
          "20": ""
        },
        {
          "20": ""
        },
        {
          "20": "ances (each averaging 3.7 seconds). We obtain the speech representations from the large Wav2Vec2"
        },
        {
          "20": "model and generate their corresponding latent representations using the bottleneck encoders trained"
        },
        {
          "20": "in stages 1 and 2 by doing only inference without any further training. Then, we compute the average"
        },
        {
          "20": ""
        },
        {
          "20": "Figure 3 shows a 2D projection of these latent representations with data points marked and colored"
        },
        {
          "20": "according to their transcription and emotion labels, respectively. The textual latent representations"
        },
        {
          "20": ""
        },
        {
          "20": "latent representations (learned in stage 2) are not separated by transcription, suggesting that no textual"
        },
        {
          "20": "information is retained there. Instead, these acoustic representations are roughly clustered by emotion,"
        },
        {
          "20": ""
        },
        {
          "20": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.25": "",
          "0.3": ""
        },
        {
          "0.25": "",
          "0.3": ""
        },
        {
          "0.25": "Figure 4: Layerwise performance of VIB (d = 128) for transcription at stage 1 (left) and emotion",
          "0.3": ""
        },
        {
          "0.25": "classification at stage 2 (middle), compared to layerwise performance using original hidden state",
          "0.3": ""
        },
        {
          "0.25": "activations. Right:",
          "0.3": ""
        },
        {
          "0.25": "layerwise performance using hidden states of Base models. Lower WER and higher accuracy are",
          "0.3": ""
        },
        {
          "0.25": "better. The black horizontal dashed line represents the random baseline.",
          "0.3": ""
        },
        {
          "0.25": "layers benefit more from textual features in predicting emotion, as their representations offer more",
          "0.3": ""
        },
        {
          "0.25": "accurate transcriptions. Also, Compared to Wav2Vec2, HuBERT shows a greater contribution to",
          "0.3": ""
        },
        {
          "0.25": "emotion recognition; acoustically in the middle layers, and textually in the last layer.",
          "0.3": ""
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8\nRELATED WORK": "Learning disentangled representations of complex data, such that distinct factors are separable and"
        },
        {
          "8\nRELATED WORK": "controllable has been the focus of many studies. Bengio et al. (2013) motivate this approach within"
        },
        {
          "8\nRELATED WORK": "the context of deep learning, while Wang et al. (2024) provide a general up-to-date overview."
        },
        {
          "8\nRELATED WORK": "For speech, disentanglement has been used for controllable style transfer in voice conversion, where"
        },
        {
          "8\nRELATED WORK": "the goal\nis\nto modify para-linguistic information while preserving linguistic content.\nSpeaker"
        },
        {
          "8\nRELATED WORK": "conversion is a canonical example of this line of work:\nfor example van den Oord et al. (2017)"
        },
        {
          "8\nRELATED WORK": "evaluate their modality-agnostic discrete representation learning framework on this task, while"
        },
        {
          "8\nRELATED WORK": "Polyak et al. (2021) apply discrete representation learning specifically for speech re-synthesis. In an"
        },
        {
          "8\nRELATED WORK": "alternative approach, Qian et al. (2019) propose AutoVC, as a style transfer autoencoder framework,"
        },
        {
          "8\nRELATED WORK": "that disentangles speaker identity information from content by training an autoencoder conditioned"
        },
        {
          "8\nRELATED WORK": "on speaker identity representations (encoded by another model). Nagrani et al. (2020) exploit videos"
        },
        {
          "8\nRELATED WORK": "containing speaker faces as a self-supervision signal for learning disentangled speech embeddings."
        },
        {
          "8\nRELATED WORK": "SpeechSplit (Qian et al., 2020) extends AutoVC by constraining latent representation dimensions and"
        },
        {
          "8\nRELATED WORK": "adding noise, splitting speech representations into four specific attributes (content, pitch, timbre, and"
        },
        {
          "8\nRELATED WORK": "rhythm)."
        },
        {
          "8\nRELATED WORK": "With the assumption that most voice synthesis tasks can be defined by controlling pitch, amplitude,"
        },
        {
          "8\nRELATED WORK": "linguistic content, and timbre, Choi et al. (2021; 2022) develop a framework for manipulating and"
        },
        {
          "8\nRELATED WORK": "synthesizing voice signals by autoencoding these four properties and reconstructing the waveform"
        },
        {
          "8\nRELATED WORK": "using a combination of information perturbation and bottleneck techniques. In a different approach"
        },
        {
          "8\nRELATED WORK": "Wang et al. (2021) trained an adversarial network to minimize correlations between the representations"
        },
        {
          "8\nRELATED WORK": "of content, pitch, timbre and rhythm. Wang et al. (2023) propose a multi-modal information bottleneck"
        },
        {
          "8\nRELATED WORK": "approach aimed at disentangling relevant visual and textual features to enhance the interpretability of"
        },
        {
          "8\nRELATED WORK": "vision-language models."
        },
        {
          "8\nRELATED WORK": "Qu et al. (2024) build upon the quantization module of HuBERT (Hsu et al., 2021) and propose"
        },
        {
          "8\nRELATED WORK": "Prosody2Vec, which consists of four modules: a unit encoder that employs a deep network to map the"
        },
        {
          "8\nRELATED WORK": "discrete speech representations produced by the HuBERT’s quantization module to continuous latent"
        },
        {
          "8\nRELATED WORK": "representations; a pre-trained deep network to encode speaker; a prosody encoder; and a decoder"
        },
        {
          "8\nRELATED WORK": "that reconstructs mel-spectrograms from the outputs of the three encoders. The trainable parameters"
        },
        {
          "8\nRELATED WORK": "are first pre-trained on spontaneous speech and then fine-tuned for emotion recognition.\nIn the"
        },
        {
          "8\nRELATED WORK": "current study, our goal is to propose a general method which accommodates many possible factors of"
        },
        {
          "8\nRELATED WORK": "variations and makes it possible to configure them. Unlike Prosody2Vec, our approach requires fewer"
        },
        {
          "8\nRELATED WORK": "trainable parameters, is directly trained on the target task, and is applicable to any speech model."
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "where privacy-preservation is a key concern for responsible deployment (and sometimes even a legal"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "requirement); separating sensitive speaker-specific information from task-relevant\ninformation is"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "essential in such systems. The framework is also proving useful, as we showed, in providing a route to"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "perform disentangled feature attribution, revealing the most significant speech frame representations"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "from both textual and acoustic perspectives. We hope our findings thus help advance neural speech"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "technology, by helping to improve transparency and control."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "A unique aspect of our framework is its modality-independent architecture:\nin principle, any type of"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "entangled multi-modal representation can be disentangled as long as the right pair of tasks are selected"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "for training in stage 1 and 2. This opens up the application of the framework beyond just neural"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "speech recognition, for instance to grounded language models, where we might want to disentangle"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "linguistic and visual information. We leave further exploration of these possibilities for future work."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "ACKNOWLEDGMENTS"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Support from the Netherlands Organization for Scientific Research (NWO), through the National"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Research Agenda project “InDeep: Interpreting Deep Learning Models for Text and Sound” (NWA-"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "ORC NWA.1292.19.399) and the VICI project “Language-Driven Modularity in Neural Networks”"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "(VI.C.212.053) is gratefully acknowledged. We also thank SURF (www.surf.nl) for the support"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "in using the National Supercomputer Snellius. The work was partially done while HM was visiting"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "the University of Edinburgh."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "REFERENCES"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Guillaume Alain and Yoshua Bengio. Understanding intermediate layers using linear classifier probes."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "ArXiv, abs/1610.01644, 2016. URL https://api.semanticscholar.org/CorpusID:"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "9794990."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Alexander A Alemi, Ian Fischer, Joshua V Dillon, and Kevin Murphy. Deep variational information"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "bottleneck.\nIn International Conference on Learning Representations, 2016."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Marco Ancona, Enea Ceolini, Cengiz ¨Oztireli, and Markus Gross. Towards better understanding of"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "gradient-based attribution methods for deep neural networks. In International Conference on Learn-"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "ing Representations, 2018. URL https://openreview.net/forum?id=Sy21R9JAW."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,\nJosh Meyer,"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. Common voice: A"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "massively-multilingual speech corpus.\nArXiv, abs/1912.06670, 2019. URL https://api."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "semanticscholar.org/CorpusID:209376338."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "for self-supervised learning of speech representations. Advances in neural information processing"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "systems, 33:12449–12460, 2020."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "perspectives.\nIEEE transactions on pattern analysis and machine intelligence, 35(8):1798–1828,"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "2013."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Carlos Busso, Murtaza Bulut, Chi-Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jean-"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "nette N Chang, Sungbok Lee, and Shrikanth S Narayanan.\nIemocap: Interactive emotional dyadic"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "motion capture database. Language resources and evaluation, 42:335–359, 2008."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Hyeong-Seok Choi,\nJuheon Lee, Wan Soo Kim,\nJie Hwan Lee, Hoon Heo,\nand Kyogu Lee."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Neural analysis and synthesis:\nReconstructing speech from self-supervised representations."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "ArXiv, abs/2110.14513, 2021. URL https://api.semanticscholar.org/CorpusID:"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "239998228."
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "Hyeong-Seok Choi, Jinhyeok Yang, Juheon Lee, and Hyeongju Kim. Nansy++: Unified voice"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "synthesis with neural analysis and synthesis.\nArXiv, abs/2211.09407, 2022.\nURL https:"
        },
        {
          "disentanglement\nis key. One important class of such applications are speech recognition systems": "//api.semanticscholar.org/CorpusID:253581364."
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "classification:\nlabelling unsegmented sequence data with recurrent neural networks. In Proceedings"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "of the 23rd International Conference on Machine Learning, ICML ’06, pp. 369–376, New York,"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "NY, USA, 2006. Association for Computing Machinery. ISBN 1595933832. doi: 10.1145/1143844."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "1143891. URL https://doi.org/10.1145/1143844.1143891."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016. URL"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "https://api.semanticscholar.org/CorpusID:125617073."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Matthew Honnibal and Ines Montani.\nspaCy 2: Natural\nlanguage understanding with Bloom"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "embeddings, convolutional neural networks and incremental parsing. To appear, 2017."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov,"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "and Abdel rahman Mohamed. Hubert: Self-supervised speech representation learning by masked"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "IEEE/ACM Transactions on Audio, Speech, and Language Pro-\nprediction of hidden units."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "cessing, 29:3451–3460, 2021. URL https://api.semanticscholar.org/CorpusID:"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "235421619."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Yannick Jadoul, Bill Thompson, and Bart de Boer.\nIntroducing parselmouth: A python interface"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "to praat.\nJournal of Phonetics, 71:1–15, 2018.\nISSN 0095-4470. doi: https://doi.org/10.1016/"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "j.wocn.2018.07.001. URL https://www.sciencedirect.com/science/article/"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "pii/S0095447017301389."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "arXiv\npreprint\nDiederik P Kingma\nand Max Welling.\nAuto-encoding\nvariational\nbayes."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "arXiv:1312.6114, 2013."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Yuanchao Li, Peter Bell, and Catherine Lai. Fusing asr outputs in joint training for speech emotion"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal\nrecognition."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Processing (ICASSP), pp. 7362–7366, 2021. URL https://api.semanticscholar.org/"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "CorpusID:240288514."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Yuanchao Li, Yumnah Mohamied, Peter Bell, and Catherine Lai. Exploration of a self-supervised"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "speech model: A study on emotional corpora. 2022 IEEE Spoken Language Technology Workshop"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "(SLT), pp. 868–875, 2022.\nURL https://api.semanticscholar.org/CorpusID:"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "252734977."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Steven R Livingstone and Frank A Russo. The ryerson audio-visual database of emotional speech"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "english. PloS one, 13(5):e0196391, 2018."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Michael McAuliffe, Michaela Socolof, Sarah Mihuc, Michael Wagner, and Morgan Sonderegger."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi.\nIn Proc. Interspeech"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "2017, pp. 498–502, 2017. doi: 10.21437/Interspeech.2017-1386."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Arsha Nagrani, Joon Son Chung, Samuel Albanie, and Andrew Zisserman. Disentangled speech"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "embeddings using cross-modal self-supervision.\nIn ICASSP 2020-2020 IEEE International Con-"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "ference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6829–6833. IEEE, 2020."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Minxue Niu, Amrit Romana, Mimansa Jaiswal, Melvin McInnis, and Emily Mower Provost. Captur-"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "ing mismatch between textual and acoustic emotion expressions for mood identification in bipolar"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "disorder.\nIn Interspeech, 2023."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "and Signal Processing (ICASSP), pp. 5206–5210, 2015. doi: 10.1109/ICASSP.2015.7178964."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Adam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Ab-"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "delrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "self-supervised representations.\nIn Interspeech, 2021."
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "Kaizhi Qian, Yang Zhang, Shiyu Chang, Xuesong Yang, and Mark A. Hasegawa-Johnson. Zero-shot"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "voice style transfer with only autoencoder loss. ArXiv, abs/1905.05879, 2019. URL https:"
        },
        {
          "Alex Graves, Santiago Fern´andez, Faustino Gomez, and J¨urgen Schmidhuber. Connectionist temporal": "//api.semanticscholar.org/CorpusID:155091770."
        }
      ],
      "page": 12
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "pervised speech decomposition via triple information bottleneck.\nIn International Conference"
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "on Machine Learning, 2020. URL https://api.semanticscholar.org/CorpusID:"
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "216080584."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "Leyuan Qu, Taihao Li, Cornelius Weber, Theresa Pekarek-Rosin, Fuji Ren, and Stefan Wermter."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "IEEE/ACM\nDisentangling prosody representations with unsupervised speech reconstruction."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "Transactions on Audio, Speech, and Language Processing, 32:39–54, 2024.\nISSN 2329-9304."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "doi: 10.1109/taslp.2023.3320864. URL http://dx.doi.org/10.1109/TASLP.2023."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "3320864."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "Robust speech recognition via large-scale weak supervision. ArXiv, abs/2212.04356, 2022."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "Wojciech Samek, Gr´egoire Montavon, Andrea Vedaldi, Lars Kai Hansen, and Klaus-Robert M¨uller."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "Explainable AI: interpreting, explaining and visualizing deep learning, volume 11700. Springer"
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "Nature, 2019."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks.\nIn"
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML’17, pp."
        },
        {
          "Kaizhi Qian, Yang Zhang, Shiyu Chang, David Cox, and Mark A. Hasegawa-Johnson.\nUnsu-": "3319–3328. JMLR.org, 2017."
        }
      ],
      "page": 13
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "In our experiments, all models were trained for 50 epochs using the AdamW optimizer with gradient": "norm clipping. The learning rate was set to 1e−3 and 1e−4 for Base and Large models, respectively,"
        },
        {
          "In our experiments, all models were trained for 50 epochs using the AdamW optimizer with gradient": "with a warmup ratio of 0.1 and a cosine decay scheduler, together with weight decay. Specifically for"
        },
        {
          "In our experiments, all models were trained for 50 epochs using the AdamW optimizer with gradient": "training transcription prediction using CTC loss, we use a batch size of 1; the effective batch size"
        },
        {
          "In our experiments, all models were trained for 50 epochs using the AdamW optimizer with gradient": "here is the number of frame representations since the CTC loss is computed across all frames. For"
        },
        {
          "In our experiments, all models were trained for 50 epochs using the AdamW optimizer with gradient": "other training objectives, we use a batch size of 8."
        }
      ],
      "page": 14
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "D\nLAYER WEIGHT AVERAGING": "In the input of both VIB and probing training setups in Section 4, we train weights for layer averaging"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "(see Figure 1). Let us have a look at these trained layer weights to see the contribution of each layer"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "at each stage. Figure D.1 shows these layer weights for both Base and Large models. Overall, layer"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "weights learned in the VIB setup closely follow the weights trained with original hidden states in the"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "probing setup."
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "In stage 1 we observe that,\nin pre-trained models,\nthe upper-middle layers are more capable of"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "decoding transcription.\nIn fine-tuned models, however,\nthis information shifts to and becomes"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "concentrated at the final layer. This pattern is consistent across both Base and Large model sizes as"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "well as for different bottleneck dimensions."
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "For emotion recognition and speaker identity tasks in stage 2, however, there is a notable difference"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "between Base and Large models. In Base models, the earlier layers contribute more in both pre-trained"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "and fine-tuned setups. In contrast, layers in Large models contribute uniformly, suggesting that the"
        },
        {
          "D\nLAYER WEIGHT AVERAGING": "acoustic information useful for these target tasks is distributed across layers in Large models."
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Task": "Transcription",
          "Setup": "VIB"
        },
        {
          "Task": "HuBERT",
          "Setup": "Wav2Vec2"
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "HuBERT-FT",
          "Setup": "Wav2Vec2-FT"
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        },
        {
          "Task": "",
          "Setup": ""
        }
      ],
      "page": 15
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "CTC (WER)": "1.00\n1.00",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "1.00\n1.00",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.80\n0.79\n0.79\n0.79\n0.79\n0.78"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "0.61",
          "Mean Intensity (Accuracy)": "0.63\n0.60",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.56\n0.55\n0.54",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.53",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "0.48",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.44\n0.42"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.39\n0.38",
          "Mean Pitch (Accuracy)": "0.36"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.30\n0.28",
          "Mean Pitch (Accuracy)": "0.28\n0.27\n0.27"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.25",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "0.20\n0.17",
          "Mean Intensity (Accuracy)": "0.25",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "Gender (Accuracy)",
          "Mean Intensity (Accuracy)": "Speaker Identity (Accuracy)",
          "Mean Pitch (Accuracy)": "Emotion (Accuracy)"
        },
        {
          "CTC (WER)": "1.00\n1.00\n1.00\n0.99",
          "Mean Intensity (Accuracy)": "1.00\n1.00\n0.99\n0.99",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.98\n0.98",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "0.77\n0.75",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.63\n0.62\n0.62"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.61\n0.59"
        },
        {
          "CTC (WER)": "0.56\n0.55",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.55"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.46"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.41\n0.40",
          "Mean Pitch (Accuracy)": "0.43\n0.41"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.33\n0.32"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": "0.28"
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.17",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "0.07\n0.07\n0.06",
          "Mean Pitch (Accuracy)": ""
        },
        {
          "CTC (WER)": "",
          "Mean Intensity (Accuracy)": "",
          "Mean Pitch (Accuracy)": ""
        }
      ],
      "page": 16
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "30": "20"
        },
        {
          "30": "10"
        },
        {
          "30": "0"
        },
        {
          "30": "10"
        },
        {
          "30": "20"
        }
      ],
      "page": 17
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Understanding intermediate layers using linear classifier probes",
      "authors": [
        "Guillaume Alain",
        "Yoshua Bengio"
      ],
      "year": "2016",
      "venue": "Understanding intermediate layers using linear classifier probes"
    },
    {
      "citation_id": "2",
      "title": "Deep variational information bottleneck",
      "authors": [
        "Ian Alexander A Alemi",
        "Joshua Fischer",
        "Kevin Dillon",
        "Murphy"
      ],
      "year": "2016",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "3",
      "title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
      "authors": [
        "Marco Ancona",
        "Enea Ceolini",
        "Cengiz Öztireli",
        "Markus Gross"
      ],
      "year": "2018",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "4",
      "title": "Common voice: A massively-multilingual speech corpus",
      "authors": [
        "Rosana Ardila",
        "Megan Branson",
        "Kelly Davis",
        "Michael Henretty",
        "Michael Kohler",
        "Josh Meyer",
        "Reuben Morais",
        "Lindsay Saunders",
        "Francis Tyers",
        "Gregor Weber"
      ],
      "year": "2019",
      "venue": "Common voice: A massively-multilingual speech corpus"
    },
    {
      "citation_id": "5",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "Alexei Baevski",
        "Yuhao Zhou",
        "Abdelrahman Mohamed",
        "Michael Auli"
      ],
      "year": "2020",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "6",
      "title": "Representation learning: A review and new perspectives",
      "authors": [
        "Yoshua Bengio",
        "Aaron Courville",
        "Pascal Vincent"
      ],
      "year": "2013",
      "venue": "IEEE transactions on pattern analysis and machine intelligence"
    },
    {
      "citation_id": "7",
      "title": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth S Narayanan"
      ],
      "year": "2008",
      "venue": "Iemocap: Interactive emotional dyadic motion capture database. Language resources and evaluation"
    },
    {
      "citation_id": "8",
      "title": "Neural analysis and synthesis: Reconstructing speech from self-supervised representations",
      "authors": [
        "Hyeong-Seok Choi",
        "Juheon Lee",
        "Wan Kim",
        "Jie Hwan Lee",
        "Hoon Heo",
        "Kyogu Lee"
      ],
      "year": "2021",
      "venue": "Neural analysis and synthesis: Reconstructing speech from self-supervised representations"
    },
    {
      "citation_id": "9",
      "title": "Nansy++: Unified voice synthesis with neural analysis and synthesis",
      "authors": [
        "Hyeong-Seok Choi",
        "Jinhyeok Yang",
        "Juheon Lee",
        "Hyeongju Kim"
      ],
      "year": "2022",
      "venue": "Nansy++: Unified voice synthesis with neural analysis and synthesis"
    },
    {
      "citation_id": "10",
      "title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks",
      "authors": [
        "Alex Graves",
        "Santiago Fernández",
        "Faustino Gomez",
        "Jürgen Schmidhuber"
      ],
      "year": "2006",
      "venue": "Proceedings of the 23rd International Conference on Machine Learning, ICML '06",
      "doi": "10.1145/1143844.1143891"
    },
    {
      "citation_id": "11",
      "title": "Gaussian error linear units (gelus). arXiv: Learning",
      "authors": [
        "Dan Hendrycks",
        "Kevin Gimpel"
      ],
      "year": "2016",
      "venue": "Gaussian error linear units (gelus). arXiv: Learning"
    },
    {
      "citation_id": "12",
      "title": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing",
      "authors": [
        "Matthew Honnibal",
        "Ines Montani"
      ],
      "year": "2017",
      "venue": "spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing"
    },
    {
      "citation_id": "13",
      "title": "Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "Wei-Ning Hsu",
        "Benjamin Bolte",
        "Hubert Yao-Hung",
        "Kushal Tsai",
        "Ruslan Lakhotia",
        "Abdel Salakhutdinov",
        "Mohamed Rahman",
        "Hubert"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Introducing parselmouth: A python interface to praat",
      "authors": [
        "Yannick Jadoul",
        "Bill Thompson",
        "Bart De Boer"
      ],
      "year": "2018",
      "venue": "Journal of Phonetics",
      "doi": "10.1016/j.wocn"
    },
    {
      "citation_id": "15",
      "title": "Auto-encoding variational bayes",
      "authors": [
        "P Diederik",
        "Max Kingma",
        "Welling"
      ],
      "year": "2013",
      "venue": "Auto-encoding variational bayes",
      "arxiv": "arXiv:1312.6114"
    },
    {
      "citation_id": "16",
      "title": "Fusing asr outputs in joint training for speech emotion recognition",
      "authors": [
        "Yuanchao Li",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2021",
      "venue": "ICASSP 2022 -2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "17",
      "title": "Exploration of a self-supervised speech model: A study on emotional corpora",
      "authors": [
        "Yuanchao Li",
        "Yumnah Mohamied",
        "Peter Bell",
        "Catherine Lai"
      ],
      "year": "2022",
      "venue": "IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "18",
      "title": "The ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english",
      "authors": [
        "R Steven",
        "Frank Livingstone",
        "Russo"
      ],
      "year": "2018",
      "venue": "PloS one"
    },
    {
      "citation_id": "19",
      "title": "Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi",
      "authors": [
        "Michael Mcauliffe",
        "Michaela Socolof",
        "Sarah Mihuc",
        "Michael Wagner",
        "Morgan Sonderegger"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2017-1386"
    },
    {
      "citation_id": "20",
      "title": "Disentangled speech embeddings using cross-modal self-supervision",
      "authors": [
        "Arsha Nagrani",
        "Son Joon",
        "Samuel Chung",
        "Andrew Albanie",
        "Zisserman"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "21",
      "title": "Capturing mismatch between textual and acoustic emotion expressions for mood identification in bipolar disorder",
      "authors": [
        "Minxue Niu",
        "Amrit Romana",
        "Mimansa Jaiswal",
        "Melvin Mcinnis",
        "Emily Provost"
      ],
      "year": "2023",
      "venue": "Capturing mismatch between textual and acoustic emotion expressions for mood identification in bipolar disorder"
    },
    {
      "citation_id": "22",
      "title": "Librispeech: An asr corpus based on public domain audio books",
      "authors": [
        "Vassil Panayotov",
        "Guoguo Chen",
        "Daniel Povey",
        "Sanjeev Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2015.7178964"
    },
    {
      "citation_id": "23",
      "title": "Abdelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled self-supervised representations",
      "authors": [
        "Adam Polyak",
        "Yossi Adi",
        "Jade Copet",
        "Eugene Kharitonov",
        "Kushal Lakhotia",
        "Wei-Ning Hsu"
      ],
      "year": "2021",
      "venue": "Abdelrahman Mohamed, and Emmanuel Dupoux. Speech resynthesis from discrete disentangled self-supervised representations"
    },
    {
      "citation_id": "24",
      "title": "Zero-shot voice style transfer with only autoencoder loss",
      "authors": [
        "Kaizhi Qian",
        "Yang Zhang",
        "Shiyu Chang",
        "Xuesong Yang",
        "Mark Hasegawa-Johnson"
      ],
      "year": "2019",
      "venue": "Zero-shot voice style transfer with only autoencoder loss"
    },
    {
      "citation_id": "25",
      "title": "Unsupervised speech decomposition via triple information bottleneck",
      "authors": [
        "Kaizhi Qian",
        "Yang Zhang",
        "Shiyu Chang",
        "David Cox",
        "Mark Hasegawa-Johnson"
      ],
      "year": "2020",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "26",
      "title": "Disentangling prosody representations with unsupervised speech reconstruction",
      "authors": [
        "Leyuan Qu",
        "Taihao Li",
        "Cornelius Weber",
        "Theresa Pekarek-Rosin",
        "Fuji Ren",
        "Stefan Wermter"
      ],
      "year": "2024",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing",
      "doi": "10.1109/TASLP.2023.3320864"
    },
    {
      "citation_id": "27",
      "title": "Robust speech recognition via large-scale weak supervision",
      "authors": [
        "Alec Radford",
        "Jong Kim",
        "Tao Xu",
        "Greg Brockman",
        "Christine Mcleavey",
        "Ilya Sutskever"
      ],
      "year": "2022",
      "venue": "Robust speech recognition via large-scale weak supervision"
    },
    {
      "citation_id": "28",
      "title": "Explainable AI: interpreting, explaining and visualizing deep learning",
      "authors": [
        "Wojciech Samek",
        "Grégoire Montavon",
        "Andrea Vedaldi",
        "Lars Hansen",
        "Klaus-Robert Müller"
      ],
      "year": "2019",
      "venue": "Explainable AI: interpreting, explaining and visualizing deep learning"
    },
    {
      "citation_id": "29",
      "title": "Axiomatic attribution for deep networks",
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan"
      ],
      "year": "2017",
      "venue": "Proceedings of the 34th International Conference on Machine Learning"
    },
    {
      "citation_id": "30",
      "title": "Bert rediscovers the classical nlp pipeline",
      "authors": [
        "Ian Tenney",
        "Dipanjan Das",
        "Ellie Pavlick"
      ],
      "year": "2019",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "doi": "10.18653/v1/P19-1452"
    },
    {
      "citation_id": "31",
      "title": "Aaron van den Oord, Oriol Vinyals, and koray kavukcuoglu",
      "authors": [
        "Naftali Tishby",
        "Fernando Pereira",
        "William Bialek"
      ],
      "year": "2000",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "32",
      "title": "Attention is all you need",
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan Gomez",
        "Łukasz Kaiser",
        "Illia Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "33",
      "title": "Adversarially learning disentangled speech representations for robust multi-factor voice conversion",
      "authors": [
        "Jie Wang",
        "Jingbei Li",
        "Xintao Zhao",
        "Zhiyong Wu",
        "Helen Meng"
      ],
      "year": "2021",
      "venue": "Adversarially learning disentangled speech representations for robust multi-factor voice conversion"
    },
    {
      "citation_id": "34",
      "title": "Disentangled representation learning",
      "authors": [
        "Xin Wang",
        "Hong Chen",
        "Si'ao Tang",
        "Zihao Wu",
        "Wenwu Zhu"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/TPAMI.2024.3420937"
    },
    {
      "citation_id": "35",
      "title": "Visual explanations of image-text representations via multi-modal information bottleneck attribution",
      "authors": [
        "Ying Wang",
        "Tim Rudner",
        "Andrew Gordon"
      ],
      "year": "2023",
      "venue": "Thirty-seventh Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "36",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "Remi Louf",
        "Morgan Funtowicz",
        "Joe Davison",
        "Sam Shleifer",
        "Clara Patrick Von Platen",
        "Yacine Ma",
        "Julien Jernite",
        "Canwen Plu",
        "Teven Xu",
        "Sylvain Le Scao",
        "Mariama Gugger",
        "Quentin Drame",
        "Alexander Lhoest",
        "Rush"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
      "doi": "10.18653/v1/2020.emnlp-demos.6"
    },
    {
      "citation_id": "37",
      "title": "Interpreting deep models for text analysis via optimization and regularization methods",
      "authors": [
        "Yongjun Hao Yuan",
        "Xia Chen",
        "Shuiwang Hu",
        "Ji"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    }
  ]
}