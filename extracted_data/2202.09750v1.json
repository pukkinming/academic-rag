{
  "paper_id": "2202.09750v1",
  "title": "Enhancing Affective Representations Of Music-Induced Eeg Through Multimodal Supervision And Latent Domain Adaptation",
  "published": "2022-02-20T07:32:12Z",
  "authors": [
    "Kleanthis Avramidis",
    "Christos Garoufis",
    "Athanasia Zlatintsi",
    "Petros Maragos"
  ],
  "keywords": [
    "Music Cognition",
    "Emotion Recognition",
    "Electroencephalography",
    "Cross-Modal Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "The study of Music Cognition and neural responses to music has been invaluable in understanding human emotions. Brain signals, though, manifest a highly complex structure that makes processing and retrieving meaningful features challenging, particularly of abstract constructs like affect. Moreover, the performance of learning models is undermined by the limited amount of available neuronal data and their severe inter-subject variability. In this paper we extract efficient, personalized affective representations from EEG signals during music listening. To this end, we employ music signals as a supervisory modality to EEG, aiming to project their semantic correspondence onto a common representation space. We utilize a bi-modal framework by combining an LSTM-based attention model to process EEG and a pre-trained model for music tagging, along with a reverse domain discriminator to align the distributions of the two modalities, further constraining the learning process with emotion tags. The resulting framework can be utilized for emotion recognition both directly, by performing supervised predictions from either modality, and indirectly, by providing relevant music samples to EEG input queries. The experimental findings show the potential of enhancing neuronal data through stimulus information for recognition purposes and yield insights into the distribution and temporal variance of music-induced affective features.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Music is an abstract, yet densely emotional form of art. It is universally enjoyed, due to its ability to induce powerful emotions irrespective of the underlying mood  [1]  and has been characterized to greatly affect the function of the human brain  [2, 3] . Hence it is widely used to study emotion recognition, both by analyzing the mood produced by several musical features  [4, 5]  and by studying its effects on human neural and physiological responses  [6] . However, the task of extracting emotional information from brain activity poses severe challenges, due to the inherently abstract nature of the induced emotions, the variability in emotional and physiological responses between different individuals and the lack of large-scale databases of emotionally coordinated neural activity. In this paper, we propose a deep multimodal approach  [7] , using musical stimuli.\n\nThe scope of this study lies at the intersection of Music Cognition, Emotion Recognition from neuronal signals and Multimodal Learning. We choose to study brain responses to music by employing a cross-modal system to identify the correspondence between these modalities. We use the electroencephalogram (EEG) to model brain responses for this task and we constrain the learning process with emotion labels. Therefore, we aim to derive important insights regarding the affective role that music can play on humans and the extent to which it can help us build richer neuronal representations of affect. To conduct the experiment, we exploit multimodal optimization and domain adaptation strategies to project EEG and music features onto a common latent space, from which we could assess their similarity. By conditioning the learning process with emotion tags, the constructed space represents affect, enabling thus emotion recognition both directly, by performing supervised predictions, and indirectly, by ranking music tracks to EEG inputs, based on their distance. To the best of our knowledge, this is the first study to propose such a framework, thus it could be utilized as a baseline reference. We also perform an extensive qualitative study across 32 subjects of the DEAP dataset  [8]  to derive inter-subject affective patterns.\n\nThe remainder of this paper is organized as follows: Section 2 reviews the related work in cross-modal learning and research on EEG processing and music cognition. In Section 3 we introduce our problem and present the proposed framework along with the optimization methods we utilized. Section 4 includes information about the data, their pre-processing and implementation details, while in Section 5 we provide the experimental results. In that Section and in Section 6 we provide extensive quantitative and qualitative analysis of the outcomes, while Section 7 concludes our study.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "Music Cognition: Studying the human brain's responses to music stimuli has always been a lively field of research in neuroscience and signal processing  [9]  aiming to answer fundamental questions regarding our enjoyment of music. The field has gained a lot of attention in recent years, with the upsurge in available neuronal data. Many studies in the field rely on EEG recordings as they provide better temporal resolution than other techniques, such as functional magnetic resonance imaging (fMRI). In addition to the traditional, well-controlled auditory experiments, modern approaches gather physiological data from music listeners as they enjoy or imagine naturalistic music  [10] , in order for instance to examine correlations in temporal structure  [11]  or the perceived tempo  [12] . One of the core findings on music cognition is the correlation between characteristics of the neural oscillation patterns and rhythmical patterns in music  [13] . Additionally, Event-Related Potentials (ERP) have been utilized to extract brain activity patterns that can relate to note onsets or pitch  [14, 15] . In parallel to the above, there has also been a shift towards deep learning approaches for information retrieval from music stimuli  [16] , in which we focus on the present study.\n\nEmotion Recognition: Undoubtedly, the most powerful impact of music on humans concerns the induced emotions. Emotion Recognition is a widely researched field of contemporary Machine Learning and Behavioral Signal Processing  [17]  and several studies have focused on the musical features  [5, 18]  that determine affective attributes of music listening in a wide range of emotions. Recently, several studies have examined physiological signals to analyze humans' felt emotions  [19] , with music emerging as an efficient method to elicit them. Due to its temporal resolution, the electroencephalogram is the most widely researched signal of this type and various statistical, spectral or time-frequency features have been proposed for Emotion Recognition  [20, 21] . Due to the noisy structure of EEG signals, many studies incorporate entropy  [22]  and fractal  [23]  algorithms to extract emotion-related features. Of course, variations of deep neural networks have been proposed and exceeded the performance of traditional feature extraction methods  [24, 25] , however the limited data availability and inter-subject variability present serious barriers for this kind of modeling.\n\nCross-Modal Learning: The task of learning a shared embedding space from different datasets or modalities is being studied through various approaches, which are predominantly applied to image and text modalities. A widely used baseline is Canonical Correlation Analysis (CCA). CCA is non-probabilistic and enables the extraction of linear components to optimize the correlation of pairs of vectors. One can find in the literature various non-linear CCA-based frameworks and architectures utilized to learn inter-modal similarities, such as Deep CCA  [26] . Besides CCA, other methods that have been used include an HGR-based maximal correlation metric  [27]  and adversarial training  [28] , focusing mainly on the optimization function of the respective model, and on adaptive hidden layers  [29] . Another study  [30]  incorporated music to co-train a shared space with images using a contrastive loss. Further, in  [31]  a stateof-the-art framework exploits label supervision to better manipulate the latent space, a key concept that we also follow in our study.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Methodology",
      "text": "In this study, we extract the semantic relationship between music tracks and corresponding EEG recordings, so that an EEG could be mapped to an efficient affective representation and retrieve emotionally consistent music samples. Let us assume a collection of n instances of EEG-music pairs, denoted as\n\nwhere x a i is the input EEG sample of the i th instance and x b i the input music stimulus corresponding to that sample. Each instance has been assigned an affective annotation yi ∈ R 2 for valence and arousal dimensions. For each instance i we aim to learn an EEG embedding ui\n\n, where d is the dimensionality of the common representation space and Y a , Y b the trainable parameters.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Proposed Framework",
      "text": "We use a bi-stream Neural Network with one branch for each modality. The EEG branch is a recurrent network, comprised of two LSTM modules and a softmax attention layer. The model takes as input an EEG trial of shape (channels, features) and attempts to capture its inter-channel correlations. Next, the output features of all channels are flattened and passed through an attention module to identify the most important components, that will lead the embedding vector in the common space. We utilize a lightweight network in order to avoid overfitting to the limited range of the available data, however any state-of-the-art model in the task could be applied. For the music branch we use the MusiCNN model  [32]  to extract high-level embeddings from the available audio stimuli. MusiCNN is a robust network, pre-trained on large audio databases, and produces highquality music embeddings that compensate for the limited size of our track set and further assist the learning process. The extracted embeddings are then fed into a feed-forward neural network.\n\nTo construct the final bi-modal framework, we connect the last layer of each of the previous networks to a dense layer (Fig.  1 ) constituting the common representation space, from which we output emotion predictions. Inspired by  [25] , we further apply a Gradient Reversal Layer (GRL)  [33] , aiming to reduce the distribution shift between EEG and music modalities. In specific, both 64D embeddings are fed to this layer, from where we output a prediction regarding the modality type. From each batch, we randomly permute half of the EEG samples and their respective music samples, forming a new equal-sized batch that we shuffle and input to the GRL module, along with a binary label vector to denote the modality. Subsequently, these embeddings are passed through dense layers to predict the modality of each sample. By reversing the gradients corresponding to these predictions during back-propagation, we help the feature extractor produce modality-invariant features.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Objective Function",
      "text": "Our goal is to learn a common space where the samples from the same semantic category should be similar, even though they come from different modalities. To learn discriminative features we want to minimize the discrimination loss in both the label space and the representation space, by reducing the cross-modal discrepancy. With regard to the label space, we use a linear classifier to predict the emotion labels of the samples projected in the common space. Outputs of each modality are passed through a sigmoid activation and a binary cross-entropy (BCE) loss is computed. For the crossmodal task we apply a weighted linear combination of those losses: J1 = λ11 a + λ12 b . To reduce the cross-modal discrepancy between EEG and music representations, we also compute the BCE loss of the modality prediction after GRL: J2 = dd . By combining terms J1, J2 we obtain the proposed objective, in which the hyperparameters λi control the contribution of each separate component and are determined through trial and error: J = λ1J1 + λ2J2.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Deap Dataset",
      "text": "DEAP  [8]  is a comprehensive dataset that includes EEG signals of music listening, collected from 32 subjects. Each subject watches forty 1-min long music videos while having their EEG recorded. After each video trial, the subject was instructed to rate the emotion that was elicited upon the entire trial in 5 dimensions: valence, arousal, dominance, liking and familiarity to the track. In this paper we solely experiment with the 2D emotion space, determined by valence and arousal, whose ratings range from 1 (weakest) to 9 (strongest). We use the EEG signals in their already preprocessed form: recorded at a sampling rate of 512 Hz and denoised by bandpass filtering, after downsampling to 128 Hz. Eye-related artefacts were removed whereas the 10-20 electrode placement system was followed.\n\nSpecifying Music Tracks: The 40 one-minute music stimuli of DEAP are not included in the dataset, so we located the video clips of the corresponding tracks and isolated the minute of interest for each one, according to the metadata provided. The task of deriving the common latent space poses a crucial challenge: the semantic gap between the \"subjective\" affective responses of participants and the emotion tags of the songs. Ideally, we need musical stimuli that are tagged in accordance with the participants' annotations. DEAP stimuli have been selected for this purpose and have been independently annotated by the experimenters at track level. Nearly all songs received average ratings from the participants that were in accordance with those annotations. We found that only 6/40 songs had such an inconsistency and discarded them. The resulting track set is used to extract MusiCNN embeddings which we make available 1 as well.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Input Feature Extraction",
      "text": "EEG and music signals are processed differently in order to produce an embedding form suitable for multimodal training. DEAP signals are first cut to 3-second segments, while having their preparatory phase removed. For feature extraction purposes we consider differential entropy features, reported to achieve superior performance in the task  [34] . Differential entropy (DE) h is defined as:\n\nwhere X is an EEG segment and f (x) its distribution. Assuming further that the utilized signals can be modeled as Gaussian distributions, i.e. f (x) = N (µ, σ), then h(X) can be determined by the logarithm energy spectrum of X as follows:\n\n(\n\nThus, for each EEG segment we use the Short-Time Fourier Transform with an 1-sec non-overlapping Hanning window to compute the variance σ 2 for each of the three windows in the frequency domain and subsequently we compute h in each channel of the four available EEG rhythms: θ (4-7Hz), α (7-13Hz), β (13-30Hz) and γ (31-50Hz). The features for all four bands are concatenated and the resulting feature vector is then used as channel-wise input. On the other hand, music tracks are cut in segments aligned with the EEG throughout their whole (3 sec) duration and fed directly into the pretrained model, from which we extract its \"pool5\" embeddings.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Evaluation Protocol",
      "text": "We evaluate our proposed method using accuracy to assess the supervised predictions for each modality and the Precision@10 (P@10) and mean Average Precision (mAP) metrics for the retrieval of music tracks given EEG queries. Those two metrics have been widely used to assess retrieval tasks in the literature  [35, 31]  as they evaluate the response's distance-based ranking to each query. In particular, P@10 considers the top 10 ranked tracks whereas mAP evaluates the whole ranking. Results are also presented after trial aggregation: For the accuracy, we simply denote a prediction as correct by majority voting on the segment-wise predictions. For the retrieval metrics, since no such voting can be made, we consider the median of the segment-wise distance scores as the overall query score.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "The training procedure considers personalized models, each one trained on data of a specific subject and the respective audio stimuli. To compensate for possible annotation noise, we binarize the emotion labels by setting the threshold to the median score 5, as in  [8] . Following the same paradigm, we consider separate experiments for valence and arousal dimensions. We apply 5-fold stratified cross-validation to train each network, where each fold holds 20% of the total trials (7 tracks). Additionally, we apply class weights to alleviate any subject-specific data imbalance. All networks are optimized using Adam at a 10 -4 learning rate and patience of 15 epochs of non-decreasing validation loss.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Predicting Emotion Tags",
      "text": "We first evaluate the models' performance on Emotion Recognition for both EEG and Music modalities (Table  1 ). EEG scores show high variance per subject, on average reaching up to 70.4% on valence and 68.9% on arousal after trial aggregation. The obtained scores are competitive for the specific dataset, despite the simple utilized architecture, something we attribute to the impact of music co-training and the adaptation of the common latent space. This contribution is further quantified in Section 5.3. Additionally, aggregating predictions on a per-track basis provides substantially enhanced results compared to non-aggregated ones, with the EEG accuracy increasing by above 5% in arousal recognition and about 8% in valence, implying that there is strong correlation (e.g., in the form of clusters) between same-track samples, especially in valence. On the other hand, despite the small number of tracks in our music set, the recognition performance is substantially high for the music branch, 78.8% average on valence and 91.9% on arousal, something that indicates the robustness of our transfer learning module.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Retrieving Tracks From Eeg Queries",
      "text": "Table  2  summarizes the retrieval scores from the personalized models, acquired by querying the common representation space of each  Here we solely consider mean aggregated scores over 32 subjects.\n\nnetwork with a test EEG sample and then evaluating the ranking of music samples based on their distance to the query. Retrieval metrics provide robust results in both cases, indicating that the EEG samples are well-situated in the common space and the majority of them are capable of retrieving tracks that are emotionally consistent. Specifically, in the case of induced valence, a P@10 value of 63.8% is achieved. We note that this percentage is higher than the reported mAP (59.1%), strongly implying that the learned valence space is fragmented into local subspaces of high similarity. Arousal on the other hand seems to be more consistently represented, as both mAP and P@10 median retrieval scores indicate that the majority of tested tracks can derive consistent music rankings, in contrast to valence where the emotional response similarity seems concentrated to the top-ranked elements. As a result, the correct retrieval percentage conditioned on arousal approaches 68% on average across subjects. We also note some preliminary results in approaching retrieval of the exact stimulus of an EEG sample. The derived scores, around 20%, are clearly above random selection, however we believe that further experimentation is required on the temporal resolution of the input samples, yielding an interesting direction of future study.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Ablation Study",
      "text": "In our study we incorporated a complex objective function, combining 3 BCE terms to minimize the discrimination loss in both the label space and the common latent space. To further investigate the impact of our proposals on the models' performance, we trained separate sessions, first by considering sole EEG samples without music supervision, and second by avoiding the domain discrimination module. From the results in Table  3  we deduce that our full objective J leads to higher overall performance, indicating that all utilized terms contribute to richer EEG affective representations. Specifically, we can see that the absence of multimodal training sharply impacts the validity of the common space and reduces the classification performance, 2.6% in valence and 0.9% in arousal. On the other side, the absence of domain adaptation causes slighter modifications to the correlation of samples and stimuli, as measured by precision metrics. Through this module we manage though to better distribute samples in the common space, break modality-specific clusters and reduce the overall sample distances (Section 6.1), which is reflected in the improved classification performance in both experiments.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Qualitative Analysis",
      "text": "Studying the Common Space: We visually inspect the produced latent space using t-SNE to reduce its 64D dimension to 2D. We select one of the 5 trained models for 2 subjects in Valence and Arousal to display their results in Fig.  2  (similar trends are observed for most subjects). It is evident that latent domain adaptation has alleviated the cross-modal discrepancy and the modalities homogenize their embeddings to a certain degree. Cohesive sub-clusters are though visible, especially in the case of valence. This provides an explanation towards the discrepancy we observed between P@10 and mAP metrics, since the top-ranked track retrievals originate from the corresponding local subspace, but there is no coarse bisection between high-and low-valence samples, in contrast to the case of arousal. Temporal Variation of Recognition: Since each track is segmented into 58 overlapping samples of 3 sec, it is expected that the emotion is not elicited at the same pace throughout its duration. Hence, the temporal variation of our scores could indicate important moments in the track. In Fig.  3 , we present the temporal evolution of the mAP scores for selected music tracks, averaged across all subjects. While the raw plots are noisy, each song individually exhibits a pattern of variation, which we depict by applying a 7-sample moving average filter. Scores typically reveal an oscillating pattern on the time axis and emotions are highly induced at certain peaks of the graph. These patterns reveal a characteristic picture of emotional induction in songs and could be subject of further experimentation.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Conclusion",
      "text": "In this paper we presented a novel approach to analyze emotion induction from EEG recordings of music listening. We proposed a cross-modal framework to learn rich affective representations for EEG data through music supervision and adaptation of a common latent space, from which one could retrieve consistent music rankings from EEG queries. Our approach indicates that distilling information from processed musical stimuli to the respective EEG signals can improve performance and provide insights on personalized emotion analysis. To the best of our knowledge, this is the first study to propose a complete framework for the specific task and dataset, thus our results can be viewed as a concrete baseline. This framework can be used to model the EEG-Music relationship by using different condition mechanisms, e.g., musical beat. Another interesting direction would be to explore improvements in exact stimulus retrieval.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).",
      "page": 2
    },
    {
      "caption": "Figure 2: t-SNE visualisation of the common space for subjects –from left to right– 8, 15 (Valence) and 18, 20 (Arousal). 0 →Low | 1 →High",
      "page": 4
    },
    {
      "caption": "Figure 3: Arousal mAP scores over the 58 time samples for the num-",
      "page": 4
    },
    {
      "caption": "Figure 2: (similar trends are observed for most",
      "page": 4
    },
    {
      "caption": "Figure 3: , we present the temporal evolution of",
      "page": 4
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "regarding the affective role that music can play on humans and the"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "extent\nto which it can help us build richer neuronal representations"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "of affect. To conduct\nthe experiment, we exploit multimodal opti-"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "mization and domain adaptation strategies to project EEG and music"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "features onto a common latent space,\nfrom which we could assess"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "their similarity. By conditioning the learning process with emotion"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "tags,\nthe constructed space represents affect, enabling thus emotion"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "recognition both directly, by performing supervised predictions, and"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "indirectly, by ranking music tracks to EEG inputs, based on their dis-"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "tance. To the best of our knowledge, this is the ﬁrst study to propose"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "such a framework,\nthus it could be utilized as a baseline reference."
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "We also perform an extensive qualitative study across 32 subjects of"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "the DEAP dataset [8] to derive inter-subject affective patterns."
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "The remainder of this paper is organized as follows: Section 2"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "reviews the related work in cross-modal\nlearning and research on"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "EEG processing and music cognition. In Section 3 we introduce our"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "problem and present\nthe proposed framework along with the opti-"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "mization methods we utilized. Section 4 includes information about"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "the data,\ntheir pre-processing and implementation details, while in"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "Section 5 we provide the experimental results. In that Section and in"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "Section 6 we provide extensive quantitative and qualitative analysis"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "of the outcomes, while Section 7 concludes our study."
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "2. RELATED WORK"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "Music Cognition: Studying the human brain’s responses to music"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "stimuli has always been a lively ﬁeld of\nresearch in neuroscience"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "and signal processing [9] aiming to answer\nfundamental questions"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "regarding our enjoyment of music. The ﬁeld has gained a lot of at-"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "tention in recent years, with the upsurge in available neuronal data."
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "Many studies in the ﬁeld rely on EEG recordings as they provide"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "better temporal resolution than other techniques, such as functional"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "magnetic resonance imaging (fMRI). In addition to the traditional,"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "well-controlled\nauditory\nexperiments, modern\napproaches\ngather"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "physiological data from music listeners as\nthey enjoy or\nimagine"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "naturalistic music [10], in order for instance to examine correlations"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "in temporal structure [11] or the perceived tempo [12]. One of the"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "core ﬁndings on music cognition is the correlation between char-"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "acteristics of the neural oscillation patterns and rhythmical patterns"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "in music [13]. Additionally, Event-Related Potentials (ERP) have"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "been utilized to extract brain activity patterns that can relate to note"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "onsets or pitch [14, 15]. In parallel to the above, there has also been"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "a shift\ntowards deep learning approaches for\ninformation retrieval"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "from music stimuli [16], in which we focus on the present study."
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "Emotion Recognition: Undoubtedly,\nthe most powerful\nim-"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "pact of music on humans concerns the induced emotions. Emotion"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "Recognition is a widely researched ﬁeld of contemporary Machine"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "Learning and Behavioral Signal Processing [17] and several studies"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": ""
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "have focused on the musical features [5, 18] that determine affective"
        },
        {
          "1 Signal Analysis and Interpretation Lab, University of Southern California, Los Angeles, CA 90089, USA": "attributes of music listening in a wide range of emotions."
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "Recently,\nseveral studies have examined physiological signals"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "to analyze humans’ felt emotions [19], with music emerging as an"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "efﬁcient method to elicit\nthem. Due to its temporal resolution,\nthe"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "electroencephalogram is the most widely researched signal of\nthis"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "type and various statistical, spectral or time-frequency features have"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "been proposed for Emotion Recognition [20, 21]. Due to the noisy"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "structure of EEG signals, many studies\nincorporate\nentropy [22]"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "and fractal\n[23] algorithms to extract emotion-related features. Of"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "course, variations of deep neural networks have been proposed and"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "exceeded the performance of\ntraditional\nfeature\nextraction meth-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "ods [24, 25], however the limited data availability and inter-subject"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "variability present serious barriers for this kind of modeling."
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "Cross-Modal Learning: The task of learning a shared embed-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "ding space from different datasets or modalities\nis being studied"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "through various approaches, which are predominantly applied to im-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "age and text modalities. A widely used baseline is Canonical Corre-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "lation Analysis (CCA). CCA is non-probabilistic and enables the ex-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "traction of linear components to optimize the correlation of pairs of"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "vectors. One can ﬁnd in the literature various non-linear CCA-based"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "frameworks and architectures utilized to learn inter-modal similar-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "ities,\nsuch as Deep CCA [26].\nBesides CCA, other methods that"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "have been used include an HGR-based maximal correlation metric"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "[27] and adversarial training [28], focusing mainly on the optimiza-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "tion function of\nthe respective model, and on adaptive hidden lay-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "ers [29]. Another study [30] incorporated music to co-train a shared"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "space with images using a contrastive loss. Further,\nin [31] a state-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "of-the-art framework exploits label supervision to better manipulate"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "the latent space, a key concept that we also follow in our study."
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": ""
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "3. METHODOLOGY"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": ""
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "In this study, we extract\nthe semantic relationship between music"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "tracks and corresponding EEG recordings,\nso that an EEG could"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "be mapped to an efﬁcient affective representation and retrieve emo-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "tionally consistent music samples.\nLet us assume a collection of"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": ""
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "i , xb\ni )}n\ni=1"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "where xa\nis the input EEG sample of\nthe ith instance and xb\nthe"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "input music stimulus corresponding to that sample. Each instance"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "has been assigned an affective annotation yi ∈ R2 for valence and"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "arousal dimensions.\nFor each instance i we aim to learn an EEG"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": ""
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "embedding ui = f (xa\ni , Y a) ∈ Rd and a musical audio embedding"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": ""
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "vi = g(xb\ni , Y b) ∈ Rd, where d is the dimensionality of the common"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "representation space and Y a, Y b the trainable parameters."
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": ""
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": ""
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "3.1. The Proposed Framework"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": ""
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "We use a bi-stream Neural Network with one branch for each modal-"
        },
        {
          "Fig. 1. The proposed bi-stream network. The output embedding layer of each stream is fed to the common 64D dense layer (common space).": "ity. The EEG branch is a recurrent network, comprised of two LSTM"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: ). EEG scores show",
      "data": [
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "Valence\n62.9% – 71.5%\n70.4% – 78.7%"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "Arousal\n63.3% – 88.0%\n68.9% – 91.9%"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "Table 1. Emotion Accuracy Scores for (EEG – Music) modalities,"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "reporting mean values over 32 subject-speciﬁc models."
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "Dimension\nPrecision@10\nmAvg. Precision"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "Valence\n19.4% – 63.8%\n18.8% – 59.1%"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "Arousal\n18.4% – 65.0%\n19.9% – 67.8%"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "Table 2.\n(Track – Emotion) Retrieval Scores on EEG input queries,"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "reporting mean aggregated scores over 32 subjects."
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "4.3. Evaluation Protocol"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "We evaluate our proposed method using accuracy to assess the super-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "vised predictions for each modality and the Precision@10 (P@10)"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "and mean Average Precision (mAP) metrics for the retrieval of mu-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "sic tracks given EEG queries. Those two metrics have been widely"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "used to assess retrieval tasks in the literature [35, 31] as they evaluate"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "the response’s distance-based ranking to each query.\nIn particular,"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "P@10 considers the top 10 ranked tracks whereas mAP evaluates"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "the whole ranking. Results are also presented after trial aggregation:"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "For the accuracy, we simply denote a prediction as correct by major-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "ity voting on the segment-wise predictions. For the retrieval metrics,"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "since no such voting can be made, we consider\nthe median of\nthe"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "segment-wise distance scores as the overall query score."
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "5. EXPERIMENTS"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "The\ntraining procedure\nconsiders personalized models,\neach one"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "trained on data of a speciﬁc subject and the respective audio stim-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "uli.\nTo compensate for possible annotation noise, we binarize the"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "emotion labels by setting the threshold to the median score 5, as"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "in [8]. Following the same paradigm, we consider separate experi-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "ments for valence and arousal dimensions. We apply 5-fold stratiﬁed"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "cross-validation to train each network, where each fold holds 20%"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "of\nthe total\ntrials (7 tracks). Additionally, we apply class weights"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "to alleviate any subject-speciﬁc data imbalance. All networks are"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "optimized using Adam at a 10−4\nlearning rate and patience of 15"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "epochs of non-decreasing validation loss."
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "5.1. Predicting Emotion Tags"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "We ﬁrst evaluate the models’ performance on Emotion Recognition"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "for both EEG and Music modalities (Table 1).\nEEG scores show"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "high variance per subject, on average reaching up to 70.4% on va-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "lence and 68.9% on arousal after\ntrial aggregation.\nThe obtained"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "scores are competitive for\nthe speciﬁc dataset, despite the simple"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "utilized architecture, something we attribute to the impact of music"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "co-training and the adaptation of the common latent space. This con-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "tribution is further quantiﬁed in Section 5.3. Additionally, aggregat-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "ing predictions on a per-track basis provides substantially enhanced"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "results compared to non-aggregated ones, with the EEG accuracy"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "increasing by above 5% in arousal recognition and about 8% in va-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "lence,\nimplying that\nthere is strong correlation (e.g.,\nin the form of"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "clusters) between same-track samples, especially in valence. On the"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "other hand, despite the small number of tracks in our music set,\nthe"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "recognition performance is substantially high for the music branch,"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "78.8% average on valence and 91.9% on arousal, something that in-"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "dicates the robustness of our transfer learning module."
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "5.2. Retrieving Tracks from EEG Queries"
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": ""
        },
        {
          "Dimension\nNon-Aggregated\nAggregated": "Table 2 summarizes the retrieval scores from the personalized mod-"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "70.4% – 68.9%\n67.8% – 68.0%\n67.9% – 63.4%\nAccEEG"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "P@10\n63.8% – 65.0%\n57.3% – 53.1%\n63.4% – 66.7%"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "mAP\n59.1% – 67.8%\n51.9% – 55.8%\n59.8% – 68.1%"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "Table 3. Ablation on the Objective Function for (Valence – Arousal)."
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "Here we solely consider mean aggregated scores over 32 subjects."
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "network with a test EEG sample and then evaluating the ranking of"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "music samples based on their distance to the query. Retrieval met-"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "rics provide robust\nresults\nin both cases,\nindicating that\nthe EEG"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "samples are well-situated in the common space and the majority of"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "them are capable of\nretrieving tracks\nthat are emotionally consis-"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "tent. Speciﬁcally,\nin the case of induced valence, a P@10 value of"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "63.8% is achieved. We note that\nthis percentage is higher than the"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "reported mAP (59.1%), strongly implying that\nthe learned valence"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "space is fragmented into local subspaces of high similarity. Arousal"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "on the other hand seems to be more consistently represented, as both"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "mAP and P@10 median retrieval scores indicate that the majority of"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "tested tracks can derive consistent music rankings, in contrast to va-"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "lence where the emotional response similarity seems concentrated to"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "the top-ranked elements. As a result, the correct retrieval percentage"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "conditioned on arousal approaches 68% on average across subjects."
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "We also note some preliminary results in approaching retrieval of the"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "exact stimulus of an EEG sample. The derived scores, around 20%,"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "are clearly above random selection, however we believe that further"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "experimentation is required on the temporal resolution of the input"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "samples, yielding an interesting direction of future study."
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "5.3. Ablation Study"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "In our study we incorporated a complex objective function, com-"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "bining 3 BCE terms to minimize the discrimination loss in both the"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "label space and the common latent space. To further investigate the"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "impact of our proposals on the models’ performance, we trained sep-"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "arate sessions, ﬁrst by considering sole EEG samples without music"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "supervision, and second by avoiding the domain discrimination mod-"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "ule. From the results in Table 3 we deduce that our full objective J"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "leads to higher overall performance, indicating that all utilized terms"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "contribute to richer EEG affective representations. Speciﬁcally, we"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "can see that\nthe absence of multimodal\ntraining sharply impacts the"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "validity of the common space and reduces the classiﬁcation perfor-"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "mance, 2.6% in valence and 0.9% in arousal. On the other side,\nthe"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "absence of domain adaptation causes slighter modiﬁcations to the"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "correlation of samples and stimuli, as measured by precision met-"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "rics.\nThrough this module we manage though to better distribute"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "samples in the common space, break modality-speciﬁc clusters and"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "reduce the overall sample distances (Section 6.1), which is reﬂected"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "in the improved classiﬁcation performance in both experiments."
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "6. QUALITATIVE ANALYSIS"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": ""
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "Studying the Common Space: We visually inspect the produced la-"
        },
        {
          "J\nMetric\n(cid:96)a only\n¬ (cid:96)dd": "tent space using t-SNE to reduce its 64D dimension to 2D. We select"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "peripheral physiological signals enhanced by EEG,”\nin Proc."
        },
        {
          "8. REFERENCES": "[1] K. Jakubowski and A. Ghosh,\n“Music-evoked autobiographi-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "ICASSP 2016, New Orleans, LA, USA, 2016."
        },
        {
          "8. REFERENCES": "cal memories in everyday life,” Psychology of music, vol. 49,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[20] X. Wang, D. Nie, and B. Lu,\n“EEG-Based Emotion Recog-"
        },
        {
          "8. REFERENCES": "no. 3, pp. 649–666, 2021.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "nition Using Frequency Domain Features and Support Vector"
        },
        {
          "8. REFERENCES": "[2] O. W. Sacks, Musicophilia : Tales of Music and the Brain,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Machines,” in Proc. ICONIP 2011, Shanghai, China, 2011."
        },
        {
          "8. REFERENCES": "Alfred A. Knopf, New York, 1st edition, 2007.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[21]\nP. C. Petrantonakis and L. J. Hadjileontiadis, “Emotion Recog-"
        },
        {
          "8. REFERENCES": "[3] D. Bashwiner,\n“Brain and Music. By Stefan Koelsch,” Music",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "nition from Brain Signals Using Hybrid Adaptive Filtering and"
        },
        {
          "8. REFERENCES": "Theory Spectrum, vol. 39, no. 2, pp. 269–274, 2017.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Higher Order Crossings Analysis,” IEEE Trans. Affective Com-"
        },
        {
          "8. REFERENCES": "[4] R. Panda, R. Malheiro, and R. P. Paiva,\n“Novel Audio Fea-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "puting, vol. 1, no. 2, pp. 81–97, 2010."
        },
        {
          "8. REFERENCES": "IEEE Trans. Affective\ntures for Music Emotion Recognition,”",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[22] R. Duan, J. Zhu, and B. Lu,\n“Differential Entropy Feature for"
        },
        {
          "8. REFERENCES": "Computing, 2020.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "EEG-based Emotion Classiﬁcation,” in Proc. Int’l IEEE/EMBS"
        },
        {
          "8. REFERENCES": "[5] Y. Song, S. Dixon, and M. Pearce,\n“Evaluation of Musical",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Conf. on Neural Engineering (NER), San Diego, CA, USA,"
        },
        {
          "8. REFERENCES": "Features for Emotion Classiﬁcation,”\nin Proc.\nISMIR 2012,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "2013."
        },
        {
          "8. REFERENCES": "Porto, Portugal, 2012.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[23] K. Avramidis, A. Zlatintsi, C. Garouﬁs, and P. Maragos, “Mul-"
        },
        {
          "8. REFERENCES": "[6] T. Greer, B. Ma, M. Sachs, A. Habibi, and S. Narayanan,\n“A",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "tiscale Fractal Analysis on EEG Signals\nfor Music-Induced"
        },
        {
          "8. REFERENCES": "Multimodal View into Music’s Effect on Human Neural, Phys-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Emotion Recognition,” in Proc. EUSIPCO 2021, Amsterdam,"
        },
        {
          "8. REFERENCES": "iological, and Emotional Experience,” in Proc. ACM Int’l Mul-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "the Netherlands, 2021."
        },
        {
          "8. REFERENCES": "timedia Conf. 2019, Nice, France, 2019.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[24] Y. Wang, Z. Huang, B. McCane, and P. Neo,\n“EmotioNet:"
        },
        {
          "8. REFERENCES": "[7]\nJ. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "A 3-D Convolutional Neural Network for EEG-based Emotion"
        },
        {
          "8. REFERENCES": "“Multimodal deep learning,”\nin Proc.\nICML 2011, Bellevue,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Recognition,”\nin Proc.\nIJCNN 2018, Rio de Janeiro, Brazil,"
        },
        {
          "8. REFERENCES": "WA, USA, 2011.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "2018."
        },
        {
          "8. REFERENCES": "[8]\nS. Koelstra et al.,\n“DEAP: A Database for Emotion Analysis",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[25] X. Du et al., “An Efﬁcient LSTM Network for Emotion Recog-"
        },
        {
          "8. REFERENCES": "IEEE Trans. Affective Comput-\nUsing Physiological Signals,”",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "nition from Multichannel EEG Signals,” IEEE Trans. Affective"
        },
        {
          "8. REFERENCES": "ing, vol. 3, 2011.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Computing, pp. 1–1, 2020."
        },
        {
          "8. REFERENCES": "[9] T. Sch¨afer, P. Sedlmeier, C. St¨adtler, and D. Huron, “The psy-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[26] G. Andrew, R. Arora, J. Bilmes, and K. Livescu, “Deep Canon-"
        },
        {
          "8. REFERENCES": "chological functions of music listening,” Frontiers in Psychol-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "ical Correlation Analysis,”\nin Proc. of Machine Learning Re-"
        },
        {
          "8. REFERENCES": "ogy, vol. 4, pp. 511, 2013.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "search, Atlanta, GA, USA, 2013, pp. 1247–1255."
        },
        {
          "8. REFERENCES": "[10]\nS. Losorelli, D. T. Nguyen, J. Dmochowski, and B. Kaneshiro,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[27] M. Li, Y. Li, S.-L. Huang, and L. Zhang,\n“Semantically Su-"
        },
        {
          "8. REFERENCES": "“NMED-T: A Tempo-Focused Dataset of Cortical and Behav-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "pervised Maximal Correlation For Cross-Modal Retrieval,” in"
        },
        {
          "8. REFERENCES": "ioral Responses to Naturalistic Music,”\nin Proc. ISMIR 2017,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Proc. ICIP 2020, Abu Dhabi, UAE, 2020."
        },
        {
          "8. REFERENCES": "Suzhou, China, 2017.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[28] B. Wang, Y. Yang, X. Xu, A. Hanjalic, and H. T. Shen, “Adver-"
        },
        {
          "8. REFERENCES": "[11] A. Vinay, A. Lerch, and G. Leslie,\n“Mind the Beat: Detect-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "sarial Cross-Modal Retrieval,” in Proc. ACM Int’l Multimedia"
        },
        {
          "8. REFERENCES": "ing Audio Onsets from EEG Recordings of Music Listening,”",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Conf. 2017, Mountain View, CA, USA, 2017."
        },
        {
          "8. REFERENCES": "ArXiv, vol. abs/2102.06393, 2021.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[29] T.-K. Hu, Y.-Y. Lin, and P.-C. Hsiu,\n“Learning Adaptive Hid-"
        },
        {
          "8. REFERENCES": "[12]\nS. Stober, T. Pr¨atzlich, and M. M¨uller,\n“Brain Beats: Tempo",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "den Layers for Mobile Gesture Recognition,”\nin Proc. AAAI"
        },
        {
          "8. REFERENCES": "Extraction from EEG Data,” in Proc. ISMIR 2016, New York,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "2018, New Orleans, LA, USA, 2018."
        },
        {
          "8. REFERENCES": "NY, USA, 2016.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[30] B. Li and A. Kumar,\n“Query by Video: Cross-modal Music"
        },
        {
          "8. REFERENCES": "[13]\nS. Nozaradan,\nI. Peretz, M. Missal, and A. Mouraux,\n“Tag-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Retrieval,” in Proc. ISMIR 2019, Delft, the Netherlands, 2019."
        },
        {
          "8. REFERENCES": "ging the Neuronal Entrainment to Beat and Meter,” Journal of",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "Neuroscience, pp. 10234–10240, 2011.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[31] L. Zhen, P. Hu, X. Wang, and D. Peng,\n“Deep Supervised"
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Cross-Modal Retrieval,”\nin Proc. CVPR 2019, Salt Lake City,"
        },
        {
          "8. REFERENCES": "[14] R. S. Schaefer, P. Desain, and P. Suppes, “Structural Decompo-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "UT, USA, 2019."
        },
        {
          "8. REFERENCES": "sition of EEG Signatures of Melodic Processing,” Biological",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "Psychology, pp. 253–259, 2009.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[32]\nJ. Pons, O. Nieto, M. Prockup, E. M. Schmidt, A. F. Ehmann,"
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "and X. Serra, “End-to-End Learning for Music Audio Tagging"
        },
        {
          "8. REFERENCES": "[15] H. Poikonen, et al., “Event-related Brain Responses while Lis-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "at Scale,” in Proc. ISMIR 2018, Paris, France, 2018."
        },
        {
          "8. REFERENCES": "tening to Entire Pieces of Music,” Neuroscience, vol. 312, pp.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "58–73, 2016.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[33] Y. Ganin and V. Lempitsky,\n“Unsupervised Domain Adapta-"
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "tion by Backpropagation,” in Proc. ICML 2015, Lille, France,"
        },
        {
          "8. REFERENCES": "[16] E. J. Humphrey, J. P. Bello, and Y. LeCun,\n“Moving Beyond",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "2015."
        },
        {
          "8. REFERENCES": "Feature Design: Deep Architectures and Automatic Feature",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "Learning in Music Informatics.,”\nin Proc. ISMIR 2012, Porto,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[34] W.-L. Zheng and B.-L. Lu,\n“Investigating Critical Frequency"
        },
        {
          "8. REFERENCES": "Portugal, 2012.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Bands\nand Channels\nfor EEG-Based Emotion Recognition"
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "IEEE Trans. on Autonomous\nwith Deep Neural Networks,”"
        },
        {
          "8. REFERENCES": "Int’l\n[17] M. Sarprasatham,\n“Emotion Recognition: A Survey,”",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "Mental Development, vol. 7, pp. 162–175, 2015."
        },
        {
          "8. REFERENCES": "Journal of Advanced Research in Computer Science, vol. 3,",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "pp. 14–19, 01 2015.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "[35] M. Won, S. Oramas, O. Nieto, F. Gouyon, and X. Serra, “Mul-"
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "timodal Metric Learning for Tag-based Music Retrieval,” arXiv"
        },
        {
          "8. REFERENCES": "[18] R. Panda, B. Rocha, and R. P. Paiva, “Music Emotion Recog-",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": "preprint:2010.16030, 2020."
        },
        {
          "8. REFERENCES": "Applied\nnition with Standard and Melodic Audio Features,”",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        },
        {
          "8. REFERENCES": "Artiﬁcial Intelligence, pp. 313–334, 2015.",
          "[19]\nS. Chen, Z. Gao, and S. Wang,\n“Emotion recognition from": ""
        }
      ],
      "page": 5
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Music-evoked autobiographical memories in everyday life",
      "authors": [
        "K Jakubowski",
        "A Ghosh"
      ],
      "year": "2021",
      "venue": "Psychology of music"
    },
    {
      "citation_id": "3",
      "title": "Tales of Music and the Brain",
      "authors": [
        "O Sacks",
        "Musicophilia"
      ],
      "year": "2007",
      "venue": "Tales of Music and the Brain"
    },
    {
      "citation_id": "4",
      "title": "Brain and Music. By Stefan Koelsch",
      "authors": [
        "D Bashwiner"
      ],
      "year": "2017",
      "venue": "Music Theory Spectrum"
    },
    {
      "citation_id": "5",
      "title": "Novel Audio Features for Music Emotion Recognition",
      "authors": [
        "R Panda",
        "R Malheiro",
        "R Paiva"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "6",
      "title": "Evaluation of Musical Features for Emotion Classification",
      "authors": [
        "Y Song",
        "S Dixon",
        "M Pearce"
      ],
      "year": "2012",
      "venue": "Proc. ISMIR 2012"
    },
    {
      "citation_id": "7",
      "title": "A Multimodal View into Music's Effect on Human Neural, Physiological, and Emotional Experience",
      "authors": [
        "T Greer",
        "B Ma",
        "M Sachs",
        "A Habibi",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Proc. ACM Int'l Multimedia Conf"
    },
    {
      "citation_id": "8",
      "title": "Multimodal deep learning",
      "authors": [
        "J Ngiam",
        "A Khosla",
        "M Kim",
        "J Nam",
        "H Lee",
        "A Ng"
      ],
      "year": "2011",
      "venue": "Proc. ICML"
    },
    {
      "citation_id": "9",
      "title": "DEAP: A Database for Emotion Analysis Using Physiological Signals",
      "authors": [
        "S Koelstra"
      ],
      "year": "2011",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "The psychological functions of music listening",
      "authors": [
        "T Schäfer",
        "P Sedlmeier",
        "C Städtler",
        "D Huron"
      ],
      "year": "2013",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "11",
      "title": "NMED-T: A Tempo-Focused Dataset of Cortical and Behavioral Responses to Naturalistic Music",
      "authors": [
        "S Losorelli",
        "D Nguyen",
        "J Dmochowski",
        "B Kaneshiro"
      ],
      "year": "2017",
      "venue": "Proc. ISMIR 2017"
    },
    {
      "citation_id": "12",
      "title": "Mind the Beat: Detecting Audio Onsets from EEG Recordings of Music Listening",
      "authors": [
        "A Vinay",
        "A Lerch",
        "G Leslie"
      ],
      "year": "2021",
      "venue": "ArXiv"
    },
    {
      "citation_id": "13",
      "title": "Brain Beats: Tempo Extraction from EEG Data",
      "authors": [
        "S Stober",
        "T Prätzlich",
        "M Müller"
      ],
      "year": "2016",
      "venue": "Proc. ISMIR 2016"
    },
    {
      "citation_id": "14",
      "title": "Tagging the Neuronal Entrainment to Beat and Meter",
      "authors": [
        "S Nozaradan",
        "I Peretz",
        "M Missal",
        "A Mouraux"
      ],
      "year": "2011",
      "venue": "Journal of Neuroscience"
    },
    {
      "citation_id": "15",
      "title": "Structural Decomposition of EEG Signatures of Melodic Processing",
      "authors": [
        "R Schaefer",
        "P Desain",
        "P Suppes"
      ],
      "year": "2009",
      "venue": "Biological Psychology"
    },
    {
      "citation_id": "16",
      "title": "Event-related Brain Responses while Listening to Entire Pieces of Music",
      "authors": [
        "H Poikonen"
      ],
      "year": "2016",
      "venue": "Neuroscience"
    },
    {
      "citation_id": "17",
      "title": "Moving Beyond Feature Design: Deep Architectures and Automatic Feature Learning in Music Informatics",
      "authors": [
        "E Humphrey",
        "J Bello",
        "Y Lecun"
      ],
      "year": "2012",
      "venue": "Proc. ISMIR 2012"
    },
    {
      "citation_id": "18",
      "title": "Emotion Recognition: A Survey",
      "authors": [
        "M Sarprasatham"
      ],
      "venue": "Int'l Journal of Advanced Research in Computer Science"
    },
    {
      "citation_id": "19",
      "title": "Music Emotion Recognition with Standard and Melodic Audio Features",
      "authors": [
        "R Panda",
        "B Rocha",
        "R Paiva"
      ],
      "year": "2015",
      "venue": "Applied Artificial Intelligence"
    },
    {
      "citation_id": "20",
      "title": "Emotion recognition from peripheral physiological signals enhanced by EEG",
      "authors": [
        "S Chen",
        "Z Gao",
        "S Wang"
      ],
      "year": "2016",
      "venue": "Proc. ICASSP 2016"
    },
    {
      "citation_id": "21",
      "title": "EEG-Based Emotion Recognition Using Frequency Domain Features and Support Vector Machines",
      "authors": [
        "X Wang",
        "D Nie",
        "B Lu"
      ],
      "year": "2011",
      "venue": "Proc. ICONIP 2011"
    },
    {
      "citation_id": "22",
      "title": "Emotion Recognition from Brain Signals Using Hybrid Adaptive Filtering and Higher Order Crossings Analysis",
      "authors": [
        "P Petrantonakis",
        "L Hadjileontiadis"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "Differential Entropy Feature for EEG-based Emotion Classification",
      "authors": [
        "R Duan",
        "J Zhu",
        "B Lu"
      ],
      "year": "2013",
      "venue": "Proc. Int'l IEEE/EMBS Conf. on Neural Engineering (NER)"
    },
    {
      "citation_id": "24",
      "title": "Multiscale Fractal Analysis on EEG Signals for Music-Induced Emotion Recognition",
      "authors": [
        "K Avramidis",
        "A Zlatintsi",
        "C Garoufis",
        "P Maragos"
      ],
      "year": "2021",
      "venue": "Proc. EUSIPCO 2021"
    },
    {
      "citation_id": "25",
      "title": "EmotioNet: A 3-D Convolutional Neural Network for EEG-based Emotion Recognition",
      "authors": [
        "Y Wang",
        "Z Huang",
        "B Mccane",
        "P Neo"
      ],
      "year": "2018",
      "venue": "Proc. IJCNN"
    },
    {
      "citation_id": "26",
      "title": "An Efficient LSTM Network for Emotion Recognition from Multichannel EEG Signals",
      "authors": [
        "X Du"
      ],
      "year": "2020",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "27",
      "title": "Deep Canonical Correlation Analysis",
      "authors": [
        "G Andrew",
        "R Arora",
        "J Bilmes",
        "K Livescu"
      ],
      "year": "2013",
      "venue": "Proc. of Machine Learning Research"
    },
    {
      "citation_id": "28",
      "title": "Semantically Supervised Maximal Correlation For Cross-Modal Retrieval",
      "authors": [
        "M Li",
        "Y Li",
        "S.-L Huang",
        "L Zhang"
      ],
      "year": "2020",
      "venue": "Proc. ICIP 2020"
    },
    {
      "citation_id": "29",
      "title": "Adversarial Cross-Modal Retrieval",
      "authors": [
        "B Wang",
        "Y Yang",
        "X Xu",
        "A Hanjalic",
        "H Shen"
      ],
      "year": "2017",
      "venue": "Proc. ACM Int'l Multimedia Conf"
    },
    {
      "citation_id": "30",
      "title": "Learning Adaptive Hidden Layers for Mobile Gesture Recognition",
      "authors": [
        "T.-K Hu",
        "Y.-Y Lin",
        "P.-C Hsiu"
      ],
      "year": "2018",
      "venue": "Proc. AAAI 2018"
    },
    {
      "citation_id": "31",
      "title": "Query by Video: Cross-modal Music Retrieval",
      "authors": [
        "B Li",
        "A Kumar"
      ],
      "year": "2019",
      "venue": "Proc. ISMIR 2019"
    },
    {
      "citation_id": "32",
      "title": "Deep Supervised Cross-Modal Retrieval",
      "authors": [
        "L Zhen",
        "P Hu",
        "X Wang",
        "D Peng"
      ],
      "year": "2019",
      "venue": "Proc. CVPR 2019"
    },
    {
      "citation_id": "33",
      "title": "End-to-End Learning for Music Audio Tagging at Scale",
      "authors": [
        "J Pons",
        "O Nieto",
        "M Prockup",
        "E Schmidt",
        "A Ehmann",
        "X Serra"
      ],
      "year": "2018",
      "venue": "Proc. ISMIR 2018"
    },
    {
      "citation_id": "34",
      "title": "Unsupervised Domain Adaptation by Backpropagation",
      "authors": [
        "Y Ganin",
        "V Lempitsky"
      ],
      "year": "2015",
      "venue": "Proc. ICML 2015"
    },
    {
      "citation_id": "35",
      "title": "Investigating Critical Frequency Bands and Channels for EEG-Based Emotion Recognition with Deep Neural Networks",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2015",
      "venue": "IEEE Trans. on Autonomous Mental Development"
    },
    {
      "citation_id": "36",
      "title": "Multimodal Metric Learning for Tag-based Music Retrieval",
      "authors": [
        "M Won",
        "S Oramas",
        "O Nieto",
        "F Gouyon",
        "X Serra"
      ],
      "year": "2020",
      "venue": "Multimodal Metric Learning for Tag-based Music Retrieval"
    }
  ]
}