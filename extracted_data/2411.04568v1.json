{
  "paper_id": "2411.04568v1",
  "title": "Dynamic-Attention-Based Eeg State Transition Modeling For Emotion Recognition",
  "published": "2024-11-07T09:45:54Z",
  "authors": [
    "Xinke Shen",
    "Runmin Gan",
    "Kaixuan Wang",
    "Shuyi Yang",
    "Qingzhu Zhang",
    "Quanying Liu",
    "Dan Zhang",
    "Sen Song"
  ],
  "keywords": [
    "Dynamic attention",
    "state transition",
    "EEG",
    "emotion recognition"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Electroencephalogram (EEG)-based emotion decoding can objectively quantify people's emotional state and has broad application prospects in human-computer interaction and early detection of emotional disorders. Recently emerging deep learning architectures have significantly improved the performance of EEG emotion decoding. However, existing methods still fall short of fully capturing the complex spatiotemporal dynamics of neural signals, which are crucial for representing emotion processing. This study proposes a Dynamic-Attention-based EEG State Transition (DAEST) modeling method to characterize EEG spatiotemporal dynamics. The model extracts spatiotemporal components of EEG that represent multiple parallel neural processes and estimates dynamic attention weights on these components to capture transitions in brain states. The model is optimized within a contrastive learning framework for cross-subject emotion recognition. The proposed method achieved state-of-the-art performance on three publicly available datasets: FACED, SEED, and SEED-V. It achieved 75.4 ± 5.5% accuracy in the binary classification of positive and negative emotions and 59.3 ± 7.7% in nine-class discrete emotion classification on the FACED dataset, 88.1 ± 3.6% in the three-class classification of positive, negative, and neutral emotions on the SEED dataset, and 73.6 ± 12.7% in five-class discrete emotion classification on the SEED-V dataset. The learned EEG spatiotemporal patterns and dynamic transition properties offer valuable insights into neural dynamics underlying emotion processing.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "E MOTIONS play a crucial role in human social life.\n\nQuantifying human emotions and enabling intelligent devices to interpret them can significantly enhance human-centric intelligent interaction systems. This also facilitates real-time monitoring and early detection of emotional disorders  [1] . Electroencephalography (EEG) measures multi-channel electrical activities of the neural system, providing rich information and high objectivity of emotion recognition, which has increasingly attracted researchers' attention  [2] .\n\nMany studies have explored effective EEG features and deep learning architectures for emotion decoding. Commonly used features include power spectral density (PSD)  [3]  and differential entropy (DE, equivalent to the logarithmic energy spectrum)  [4] , both of which capture the power or energy of local electrode signals in specific frequency bands. Various deep learning methods, such as recurrent neural networks (RNNs)  [5] , convolutional neural networks (CNNs)  [6] , and graph neural networks (GNNs)  [7] , have been applied to learn emotion-related representations from DE features. Attention mechanisms have also been adopted recently to emphasize critical feature dimensions for emotion recognition  [8] . Many of these methods focus on identifying interaction between features from different brain regions. However, the rapid dynamics of EEG signals on finer time scales have been largely ignored.\n\nNeural activities exhibit complex and intriguing spatiotemporal dynamics  [9] . From the neuron population to the whole-brain level, the spatial and temporal dynamic analyses are key to understanding the brain's organization and functions  [10] ,  [11] . EEG signals encompass orchestration dynamics of neuronal activities at the macroand millisecond-level  [12] . State transition modeling is a major approach to studying EEG spatiotemporal dynamics, which reveals meta-stable states of spatial, spectral, or network activities in EEG  [13] ,  [14] ,  [15] . Temporal dynamics of these transient states serve as functional or behavioral signatures and changes in neurological or mental disorders  [16] ,  [17] ,  [18] ,  [19] ,  [20] . Different emotion states modulate temporal features and transition properties of these EEG states  [21] ,  [22] ,  [23] ,  [24] ,  [25] . It is promising to investigate EEG spatiotemporal dynamics to understand how the brain produces different emotion states and develop biophysically interpretable models for emotion recognition. Previous EEG state transition modeling mainly relies on simple clustering methods like microstate  [13]  or statistical methods like hidden Markov models  [14] . These methods suffer from weak representative power and do not account for the inter-subject differences of EEG dynamics. Deep learning provides flexible model architectural design and self-supervised learning strategies to extract target-related and generalizable representations  [26] . However, current deep-learning-based emotion recognition methods largely overlook EEG state transition properties. A deep learning architecture designed for EEG state transition modeling can potentially boost emotion recognition performance.\n\nIn this paper, we propose a Dynamic-Attention-based EEG State Transition (DAEST) model for emotion recognition. With the assumption that dominant EEG spatiotemporal components vary in different brain states, we estimate dynamic attention weights on these spatiotemporal components at each time point. The spatiotemporal components are extracted using a combination of temporal convolution for spectral decomposition and spatial transition convolution for spatial variation pattern estimation. A depthwise convolution with temporal pooling is employed to estimate the dynamic attention weights on these components. We utilize a contrastive learning method for inter-subject alignment in cross-subject emotion recognition. It identifies interpretable emotion-related EEG spatiotemporal dynamics shared across subjects and facilitates the generalization of emotion recognition across different individuals. The code is made publicly available at https://github.com/RunminGan1218/DAEST.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Related Work",
      "text": "",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Modeling Of Eeg Spatiotemporal Dynamics",
      "text": "The spatiotemporal dynamics of EEG capture large-scale activities, reflecting fundamental operating patterns of the human brain. Variations in these EEG dynamic features are indicative of different mental states and disorders  [13] ,  [16] ,  [17] . Distinct emotion states also exhibit characteristic EEG spatiotemporal dynamic features  [27] ,  [28] . Several spatiotemporal dynamic modeling methods have been employed in emotion recognition studies.\n\nMicrostate analysis identifies several typical EEG activation patterns through spatial clustering. Each pattern remains stable for tens of milliseconds before transitioning to another state  [13] . The temporal characteristics of EEG microstates reflect the underlying processes of various emotional states. Microstate C and D are associated with arousal and valence processing in music video watching, respectively  [24] . Microstates C and B are most relevant in distinguishing discrete emotions  [25] . For emotion regression, microstates in finer frequency bands predict emotions better than those in broadband  [21] . Beyond simple spatial pattern clustering, Hsu et al. proposed the adaptive mixture independent component analysis (AMICA) model, assuming different combinations of independent components under various states  [29] . The EEG states identified by the model in an unsupervised manner correspond to different imagined emotional states  [28] . Dynamic brain network methods have also been applied to emotion recognition by modeling the transitions of brain network activations. Yahya et al. enhanced emotion recognition accuracy and sensitivity by integrating local cortical activation and dynamic functional network connectivity  [30] . These studies underscore the significance of EEG's spatiotemporal dynamics in emotional processing. However, most methods still rely on clustering or statistical learning to model brain state transitions. Leveraging the powerful representation capabilities of neural networks to extract EEG state transition properties can potentially identify more effective emotion representations and improve decoding performance.\n\nBesides, EEG spatiotemporal dynamics have been shown to vary significantly across individuals and are associated with factors such as personality traits and intelligence scores  [31][32] . This variability poses a challenge in crosssubject emotion recognition, making it crucial to address and mitigate these differences in spatiotemporal dynamic features. While statistical methods have been proposed to align hidden states across subjects in fMRI studies  [33]   [34] , there remains a lack of effective approaches for inter-subject spatiotemporal dynamics alignment for EEG data.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Deep Learning Architectures For Eeg Emotion Recognition",
      "text": "To enhance the performance of emotion decoding, various deep learning architectures have been explored for EEGbased emotion recognition to capture spatial and/or temporal information from EEG data.\n\nOne prominent approach in modeling the spatial relationships across EEG channels is the use of Graph Neural Networks (GNNs). Song et al. treated each EEG channel as a node in a graph and applied graph convolution to features extracted from each channel, effectively integrating information across channels  [7] . Building on this, subsequent studies introduced sparse constraints to GNNs to better reflect the sparsity of inter-channel relationships  [35] ,  [36] . Recently, more graph network variants were proposed to further improve the emotion recognition performance  [37] ,  [38] ,  [39] ,  [40] . For instance, Ye et al. proposed a model that combines GNN and fully connected networks to extract both spatially structured and unstructured information from differential entropy features and employed self-attention mechanisms for multi-branch feature fusion  [40] .\n\nRecurrent Neural Networks (RNNs)  [5] ,  [41] ,  [42]  and Convolutional Neural Networks (CNNs)  [43] ,  [44] ,  [6] ,  [45] ,  [46]  are widely used for extracting both spatial and temporal information from EEG data. Zhang et al. proposed a two-layer RNN to handle both spatial and temporal sequences within EEG  [5] . They transformed EEG channel features into spatial sequences and modeled their dynamics within a time window. Li et al. further expanded on this by introducing hierarchical spatial-temporal RNNs to process local relationships and global relationships progressively  [41] . Considering the asymmetry in brain hemisphere activities related to emotions  [47] , Li et al. suggested using four parallel RNNs to extract spatial sequence information from both brain hemispheres in different directions  [48] , performing interhemispheric operations such as subtraction and division to highlight inter-hemispheric differences. For CNNs, several studies interpolated EEG electrode distributions into twodimensional maps, using convolution operations to extract spatial and temporal patterns from EEG features  [6] ,  [45] ,  [46] . Cui et al. suggested using two-dimensional convolution to extract local spatial features of EEG and an asymmetry difference layer to capture asymmetry information between the left and right hemispheres  [6] .\n\nRecently, attention mechanisms have been introduced to further enhance the modeling of crucial spatial and temporal features in EEG data.  Tao     [50] .\n\nDespite these advances, current deep learning methods still have limitations in capturing the full complexity of EEG dynamics. Most models focus on static spatial or temporal features and do not fully explore the dynamic transitions between different brain states. There is a need for a deep learning architecture that can effectively model the spatiotemporal dynamics of EEG signals to further improve the performance of emotion recognition.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Cross-Subject Eeg Emotion Recognition Methods",
      "text": "Domain adaptation methods have been widely adopted to tackle individual differences in EEG signals and improve cross-subject EEG emotion recognition performance  [51]   [52] . These methods minimize data distribution discrepancies between individuals in the test set (target domain) and those in the training set (source domain). Zheng et al. demonstrated that the transductive parameter transfer (TPT) method could enhance cross-subject emotion recognition accuracy by 19.6% over the baseline  [53] . Li et al. proposed a multisource domain transfer learning method using style transfer mapping to reduce discrepancies between individuals in training and test sets and selecting individuals similar to the test set for model training  [54] . Domain adversarial neural networks (DANN)  [55]  use a domain discriminator to classify which domain the sample comes from. A gradient reversal layer is inserted between the backbone and the domain discriminator to make the backbone outputs similar across different domains. Combining DANN with fully connected networks, LSTM, or GNN has yielded good performance in cross-subject emotion recognition  [36] [42]  [48]   [56] .\n\nHowever, domain adaptation methods still require testset data to train the domain discriminator, preventing direct generalization of models to test-set individuals. Domain generalization methods  [57]  have been proposed to address this by using domain adversarial training to learn frequency domain representations shared among training-set individuals, enabling direct application of trained models to test set individuals. Furthermore, researchers have suggested finetuning the model with minimal test-set data to enhance performance  [58] . More recently, we propose a contrastive learning method for inter-subject alignment, treating sample pairs from different individuals under similar states as positive samples to extract generalized representations  [59] . Additionally, Wang et al. introduced the denoising mixed mutual reconstruction method for pretraining models  [60] , allowing generalization to new individuals.\n\nThese approaches mitigate individual differences in EEG signals and facilitate cross-subject emotion recognition.\n\nHowever, further exploration into the shared spatiotemporal dynamic patterns of EEG across subjects could potentially enhance performance and deepen our understanding of the mechanisms underlying emotion processing.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Methods",
      "text": "The DAEST model is designed to extract the spatiotemporal dynamics of EEG effectively. It comprises two main modules: a temporal and spatial transition convolution (TSTC) module and a dynamic attention (DyA) module. To capture temporal-frequency information and rapid spatial variations within EEG, the TSTC module consists of a temporal convolution layer and a spatial transition convolution layer. To select activated dimensions dynamically across different states, the DyA module estimates dynamic attention weights to the spatiotemporal components extracted by the TSTC module. The complete encoder architecture is illustrated in Fig.  1 . To facilitate cross-subject emotion recognition, a contrastive learning framework is implemented to learn common representations across individuals  [59] . The DAEST model serves as a base encoder in contrastive learning and is trained alongside a subsequent projector. Then the latent representations extracted by the base encoder are used for emotion classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Temporal And Spatial Transition Convolution (Tstc)",
      "text": "For EEG inputs X ∈ R M ×T with M channels and T time points, the encoder first applies one-dimensional temporal convolutional kernels of size 1 × L 1 to extract temporal patterns, learning temporal filters to decompose the EEG into different frequency bands. The weights in temporal convolution is denoted as W temp1 ∈ R K1×1×L1 . The output of the temporal convolution is H (1) ∈ R K1×M ×T . K 1 is the number of temporal convolutional kernels. The input is padded on the time dimension to make the time points equal to T in the output.\n\nFollowing this, a spatial transition convolution is employed, utilizing convolutional kernels of size M × L 2 to capture spatial variation patterns across multiple time steps L 2 . The convolutional kernels use dilations with various temporal intervals, enabling the extraction of EEG variation patterns across different time scales. Unlike the EEGNet  [61]  and the CLISA  [59]  architectures, which use a onedimensional spatial convolution, the proposed spatial transition convolution can capture rapid changes in EEG over multiple time points. The weights in the spatial transition convolution is denoted as W spat ∈ R K1K2×M ×L2 . The output of the spatial transition convolution is H (2) ∈ R K1K2×1×T . Here, we used group convolution with K 2 spatial transition convolutional kernels for each output dimension of the temporal convolution, resulting in K 1 K 2 dimensions in total. We implemented spatial transition convolution with four different dilations, with K 1 K 2 /4 kernels for each dilation. The hyperparameters are detailed in Section 4.3. Each output dimension of the spatial transition convolution can be regarded as the activation of a specific spatial transition pattern in a specific frequency band. H (2)  can be squeezed into the shape K 1 K 2 × T and we denote it as X latent .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Dynamic Attention",
      "text": "The DyA module is applied to the output of the TSTC to estimate time-varying weights for each dimension. Initially, a one-dimensional temporal convolution followed by an average pooling layer along the temporal dimension extracts variation patterns of each dimension over time:\n\nwhere K = K 1 K 2 and W temp2 ∈ R K×L3 are the temporal convolution weights, each row representing a convolution kernel of size 1 × L 3 for each dimension of X latent . \" * \" denotes the convolution operation. MovingAverage() denotes the operation of one-dimensional average pooling. A pointwise linear mixing layer then performs a linear combination of the dimensions:\n\nwhere S ∈ R K×T . The pointwise linear mixing is also called pointwise convolution, with a convolution kernel of size 1 × 1. These two convolution layers form a depthwise convolution, but with an average pooling layer inserted between them, distinguishing it from a standard depthwise convolution. S was submitted to an activation function ϕ to obtain the attention weights for each dimension at each time point. ϕ can be a function with saturation and non-negative characteristics, such as Sigmoid or Softmax:\n\nFor the Sigmoid function, the attention weights were rescaled to between 0 and 1. For the Softmax function, the attention weights competed against each other. We compared the performance of Sigmoid function and Softmax function in the experiments. The attention weights are then multiplied with the original output of the TSTC to obtain the weighted activation Xlatent ∈ R K×T :\n\nXlatent is the output of the base encoder and is submitted to the projector in the contrastive learning procedure. After contrastive learning, Xlatent was submitted to the classifier for emotion recognition.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "The Contrastive Learning Framework",
      "text": "To address EEG individual differences and improve crosssubject decoding performance, a contrastive learning framework is adopted here  [59] . This framework includes two procedures: a contrastive learning procedure and an emotion classification procedure. The contrastive learning procedure aims to minimize individual differences by maximizing the similarity of EEG representations in similar states across subjects. In the emotion classification procedure, the base encoder trained during the contrastive learning procedure extracts latent representations Xlatent , and then an emotion classifier is trained for classification.\n\nDuring the contrastive learning procedure, one EEG sample is selected from each trial of two different individuals per batch, resulting in a batch size of 2N , where N is the number of trials. Samples corresponding to the same video segment are treated as positive pairs and those from different segments are considered negative pairs. Each sample is sequentially processed by a base encoder and a projector. The projector further extracts temporal variation patterns of EEG using average pooling over the temporal dimension and two one-dimensional temporal convolutional layers. Group convolution with ReLU activations is utilized, with the number of convolution kernels doubled after each layer. The output of the projector is used to compute the normalized temperature-scaled cross-entropy loss, which aims to maximize the cosine similarity of positive pairs relative to negative pairs.\n\nIn the emotion classification procedure, the parameters of the base encoder are fixed, and the latent representations Xlatent within each second are averaged over time, resulting in K dimensional features per second. These features are submitted to adaptive feature normalization across time for each individual and smoothed within each trial using a linear dynamical system. The smoothed features are then input into a three-layer Multilayer Perceptron, which outputs the corresponding emotion labels. The readers can refer to our previous study for more details  [59] .",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Experiments",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Datasets",
      "text": "We validated our methods on three datasets, FACED, SEED, and SEED-V. SEED is a widely adopted benchmark for emotion recognition. FACED and SEED-V are more recently published datasets containing more emotion categories, with FACED featuring a much larger number of subjects than other publicly available datasets.\n\nFACED is an affective computing EEG dataset with the largest number of subjects (N = 123), which provides a good benchmark for cross-subject emotion recognition methods. The dataset includes nine emotion categories elicited through film clips: anger, fear, disgust, sadness, amusement, inspiration, joy, tenderness, and neutral. Each participant underwent 28 trials-four for the neutral emotion and three for each of the other emotions. Each trial consisted of four stages: fixation, film clip presentation, selfreport ratings, and rest. The film clips have a duration of 34 to 129 seconds. During emotion elicitation, 32-channel EEG signals were recorded using the NeuSen.W32 system (Neuracle, China). We conducted two types of classifications on the FACED dataset: a nine-class classification of fine-grained discrete emotions and a binary classification distinguishing positive emotions (including amusement, inspiration, joy, and tenderness) from negative emotions (including anger, fear, disgust, and sadness).\n\nSEED provides 62-channel EEG recordings (ESI Neu-roScan System) from 15 subjects during emotion elicitation. Positive, neutral, and negative emotions were induced by film clips. The experiment included 15 trials per session, with three sessions conducted in total, spaced one-week or longer apart. Each trial consisted of a start cue, movie clip presentation, self-assessment, and rest. There were five trials for each emotion. The movie clips have a duration ranging from 185 to 265 seconds. We performed a three-class classification on the SEED dataset.\n\nSEED-V is an extension of the SEED dataset, with five emotion categories elicited: happiness, sadness, disgust, fear, and neutral. It includes 62-channel EEG recordings (ESI NeuroScan System) from 20 subjects during emotion elicitation. In each session, 15 film clips were used, with three clips for each emotion. Each participant completed three sessions. The experimental procedure closely mirror that of the SEED dataset, consisting of a start cue, movie clip presentation, self-assessment, and rest for each trial. We performed a five-class classification on the SEED-V dataset.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Data Preprocessing",
      "text": "To ensure consistency and comparability across the three datasets, we implemented a standardized automatic preprocessing pipeline using Matlab. First, EEG signals were downsampled to 125 Hz and filtered with a 0.5-47 Hz bandpass filter. The signals were then segmented into trials based on the onset and offset of emotional video stimuli. For each channel, if the proportion of data exceeding a specified multiple (m) of the median value was greater than a certain percentage (n) of the trial duration, the channel was classified noisy. We used two sets of thresholds: m=3, n=0.4 to identify long-lasting artifacts, and m=30, n=0.01 to detect short-term large artifacts. Noisy channels identified in this manner were interpolated using their three nearest neighboring channels. Next, independent Component Analysis (ICA) was applied to remove artifacts caused by eye movements or muscle activity. We utilized automatic component labeling tools in EEGLab. Components labeled as eye-or muscle-related with a confidence level exceeding 0.8, the default thresholds in EEGLab, were removed. On average, 5.4 out of 30 components were labeled as noise for the FACED dataset, 8.9 out of 60 for SEED, and 12.4 out of 60 for SEED-V.\n\nDuring the initial noisy channel detection, we observed that frontal channels (such as Fp1 and Fp2) were frequently identified as noisy, which interfered with the subsequent ICA-based detection of eye movement components. To address this issue, we excluded channels Fp1, Fp2, F7, and F8 from the initial noisy channel interpolation to preserve eye movement features. After applying ICA, the noisy channel interpolation again, this time including all channels. Finally, the data were re-referenced to the common average.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Implementation Details",
      "text": "This section provides details on model hyperparameter settings and training procedures. Unless otherwise stated, all settings refer to those used on the FACED dataset. Any different settings for SEED and SEED-V will be explicitly noted.\n\nIn the TSTC module, the temporal convolution utilizes K 1 = 16 one-dimensional filters, each with a length of L 1 = 30, translating to an actual time duration of 240 ms (1000 • L 1 /f s , where f s = 125). There are K 2 = 16 spatial transition convolution kernels for each temporal convolution kernel, resulting in K 1 * K 2 = 256 spatial transition convolution kernels in total. The spatial transition convolution covers L 2 = 3 steps in the temporal dimension. There are four sets of two-dimensional dilated convolution kernels, with dilations of 1, 3, 6, and 12 on the temporal dimension, respectively. Each of the four sets contains K 1 K 2 /4 = 16 * 16/4 = 64 convolution kernels. In the DyA module, the temporal convolution also employs group convolution with a kernel length of L 3 = 7. The subsequent average pooling has a window length of L 3 = 7 and a stride of 1. In The projector, the average pooling window length and stride are all set to 15 and the kernel sizes of the two convolutional layers are both set to 3. The emotion classifier is a multilayer perceptron with two hidden layers of 128 neurons and 64 neurons, respectively, with ReLU activation between layers.\n\nFor model training in the contrastive learning procedure, we set the learning rate as 0.0007 and the weight decay as 0.00015. The model was trained for 30 epochs with an early stopping patience of 10 epochs. The time length of the samples in contrastive learning was determined by a tradeoff between the training samples' number and adequate sample length. We set the average sample number for each video to 13 in the three datasets. The sample number is formulated as N sample = ⟨⌊(timeLen vi -timeLen sample )/stepLen⌋⟩ i + 1, where timeLen vi is time length of video i, timeLen sample and stepLen are individual sample length and sampling interval, where stepLen = timeLen sample //2. The sample lengths for FACED, SEED, and SEED-V datasets are 5, 30, and 22 seconds, respectively. In the emotion classification procedure, we set the learning rate as 0.0005. The weight decay was selected from 0.001, 0.0022, 0.005, 0.011, and 0.025 by cross-validation. The classifier was trained for 100 epochs with an early stopping patience of 30 epochs.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "Performance Comparison",
      "text": "We compared the performance of the proposed DAEST model with state-of-the-art domain generalization models on the three datasets, including CLISA  [59] , GCPL  [62] , DResNet  [63] , PPDA  [64] , LDG+Resnet101  [65] , PCDG  [66] , and DMMR  [60] . Additionally, we evaluated the proposed model against variations in network architecture to assess the effectiveness of dynamic attention mechanisms. These variations included a model without attention, one with global channel attention, and another using Transformer layers. In the model without attention, the DyA module was removed, while all other settings remained identical to the complete model. For the global channel attention model, the average pooling size at the DyA module's output was equal to the number of time steps in the sample, thus the attention weights were constant for each channel within an EEG sample. Other settings were the same as the proposed model. For the Transformer-based models, we replaced the DyA module with two types of Transformer layers, one applying self-attention across the feature dimension to learn interactions among different latent dimensions, and another applying self-attention across the temporal dimension to capture potential variation patterns in time sequences. Besides, a baseline model (DE+MLP) without contrastive learning was implemented, in which the DE features were directly extracted from the EEG signals and submitted to adaptive normalization, smoothing, and emotion classification. We also compared the dynamic attention module's performance using different activation functions, including Sigmoid, Softmax, and ReLU. To validate the TSTC module's design, we evaluated lesioned models with no temporal convolution, no spatial transition convolution, and no dilations in spatial transition convolution.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Results",
      "text": "",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Comparison With State-Of-The-Art Methods On Three Datasets",
      "text": "The comparison of cross-subject emotion recognition performance between the DAEST model and other state-of-the-art methods on the FACED, SEED, and SEED-V datasets are presented in Tables  1  and 2 .   [63]  85.3±8.0 / PPDA  [64]  86.7±7.1 / CLISA  [59]  86.4±6.4 67.3±13.0 LDG+Resnet101  [65]  82.0±5.9 / PCDG  [66]  87.3±2.1 / GCPL  [62]  80.7±6.0 / DMMR  [60]  88.3±5.6 / DAEST (ours) 88.1±3.6 73.6±12.7\n\nOn the FACED dataset, the DAEST model achieved an accuracy of 75.4±5.5% for binary classification (FACED-2) and 59.3±7.7% for nine-class classification (FACED-9)(Table  1 ). In comparison, existing domain generalization methods yielded much lower accuracies: the DE+MLP model achieved 60.9±3.2% and 35.0±4.6% for the binary and nine-class tasks, respectively; the CLISA model reached 67.8±4.1% and 43.2±5.9%; and the GCPL model achieved an accuracy of 36.9±3.3% for the nine-class task. These results indicate that the DAEST model dramatically outperforms other methods on the FACED dataset, particularly in the nine-class task, where the accuracy increases by 22.4% over GCPL and 16.1% over CLISA, underscoring its superiority in handling more fine-grained emotion classification tasks.\n\nOn the SEED and SEED-V datasets, the DAEST model also demonstrated exceptional cross-subject emotion recognition capability (Table  2 ). It achieved an accuracy of 88.1±3.6% on the SEED dataset, comparable to the stateof-the-art methods. On the SEED-V dataset, the DAEST model achieved an accuracy of 73.6±12.7%, outperforming other domain generalization methods. In comparison, the DE+MLP model achieved accuracies of 79.9±8.7% and 59.3±17.2% on the SEED and SEED-V datasets, respectively, while the CLISA model achieved accuracies of 86.4±6.4% and 67.3±13.0%. Consistent with the results on the FACED dataset, the DAEST model exhibited a larger performance improvement on the more fine-grained emotion classification task on SEED-V, with a 6.3% increase in accuracy.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results Of Model Variants And Ablation Studies",
      "text": "To evaluate the effectiveness of each module in the DAEST model, we tested various model variants and performed ablation studies. The results, summarized in Table  3 , show the impact of each module on emotion recognition performance.\n\nIn examining the impact of the DyA module on model performance, we found that the dynamic attention mod- ). Among the different activation functions used in the dynamic attention module, the Sigmoid function produced the best results, followed by Softmax, which achieved 74.6±4.3% and 56.2±7.2% on FACED-2 and FACED-9, respectively. ReLU performed the worst. These findings highlight the importance of scaling the dynamic attention weights within the 0-1 range for optimal model performance.\n\nIn evaluating the TSTC module's design on the model's performance, we found that removing the temporal convolution led to a decrease in accuracy by 5.1% and 11.0% on the FACED-2 and FACED-9 datasets, respectively. The model without spatial transition convolution exhibited a decrease in accuracy by 5.4% and 10.0%, indicating that both temporal and spatial transition convolutions have a significant impact on emotion recognition performance. The removal of dilation in spatial transition convolution resulted in a 3.5% and 5.7% decrease in accuracy on the FACED-2 and FACED-9 datasets, respectively, demonstrating the importance of covering multiple time scales with dilations for spatial transition convolution.\n\nTo further investigate the effects of the dynamic attention mechanism on recognition of each emotion category, we generated confusion matrices for the FACED-9, SEED, and SEED-V datasets (Fig.  2 ), comparing models with and with-out the DyA module. For the FACED-9 dataset, the DyA module led to the greatest improvements in recognizing the positive emotion categories of amusement, inspiration, and joy, with accuracy increases of 15%, 17%, and 6%, respectively. The classification accuracies of negative emotions, such as anger, disgust, and fear were also enhanced by 9%, 7%, and 10%, respectively. This indicates that the dynamic attention mechanism enhances the model's ability to distinguish between nuanced emotions. For the SEED dataset, the DyA module led to improvements in recognizing positive and neutral emotions, with accuracy increases of 3% and 1%, respectively. In the SEED-V dataset, the DyA module enhanced the model's ability to recognize nuanced negative emotions, with notable accuracy gains of 6% for both disgust and sad and 5% for neutral. Additionally, there was a reduction in the misclassification rates of these negative emotions as positive (i.e., happy), contributing to a more refined emotional recognition overall.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Visualization Of The Learned Eeg Spatiotemporal Dynamics",
      "text": "To investigate the important EEG spatiotemporal components and their dynamics underlying emotion processing, we used the integrated gradient method to identify feature contributions to each emotion in the classifier of the FACED-9 task. A feature input to the classifier was extracted from a latent dimension in the encoder. A latent dimension was produced by a temporal convolution, a spatial transition convolution, and multiplied by attention weights in the encoder (Fig.  3 ). Thus, the temporal-frequency characteristics, spatial transition patterns, and the dynamic activations of a dimension can be directly reflected in the temporal convolution kernels (or temporal filters), spatial transition convolution kernels (or spatial filters), and the dynamic attention weights, respectively. We visualized the temporal filters and their frequency response, the spatial filters and spatial activations, and an example segment of attention weights for the most important dimension of each emotion in (Fig.  4 ). The spatial activation is calculated by  [67] . It transforms the parameters W spat of a backward projection model to those (A spat ) of a forward model. Σ is the average covariance matrix across the input dimensions of the convolution kernel W spat k•• . Three emotions -anger, joy, and tenderness -share the same most important feature, with an increase of this feature leading to a higher probability of anger and a decrease corresponding to a higher probability of joy and tenderness. This feature has the highest frequency response in 4-10 Hz and a spatial activation transitioned from the occipital region to bilateral parietal regions and bilateral frontal regions. This dimension was sparsely activated as shown by the attention weights (the first row in Fig.  4 ). Disgust and sadness shared the same frequency response with a peak of 4 Hz, but differed in their spatial activation transitions and attention weight patterns, with the important dimension for sadness activated more frequently. Fear had a peak frequency response at 8-12 Hz, with a spatial activation transitioned from anterior-posterior activation to middle occipital activation and then to bilateral occipital activation. The other two positive emotions, amusement and inspiration had a higher frequency response of 12 to 30 Hz, overlapping with the beta band. These two dimensions have much higher attention weights than other dimensions.\n\nTo better understand the relationship of feature contributions to different emotions, we calculated the correlation of feature contributions between every two emotions (Fig.  5 ). A higher correlation coefficient between two emotions means more overlapping features contribute to both these emotions. Negative correlations indicate features tend to contribute reversely to the two emotions. and tenderness have the most similar feature contributions, with a correlation of 0.724. Among negative emotions, anger and fear have the highest correlation of 0.410. Interestingly, notable correlations were observed between positive emotions and negative emotions. For example, the correlations between amusement and fear, amusement and disgust, and tenderness and sadness all exceeded 0.3. The strongest negative correlations were observed between joy and anger (-0.704), tenderness and anger (-0.660), tenderness and amusement (-0.624), and inspiration and sadness (-0.611). These findings suggest that the underlying neural activities associated with different emotions share common features to some extent and that the grouping of emotions may not be strictly based on negative and positive valence.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "The Effects Of Hyperparameters And Data Noise",
      "text": "We investigated the effects of two important hyperparameters on model performance, namely the time window of dynamic attention and the time steps of spatial transition convolution, on the FACED dataset.\n\nWe evaluated the effect of attention window length (L 3 ) on model performance by varying its values from 1 to 30 while keeping other parameters constant. The results showed that performance generally peaked within the ranges of 1-10 and 10-20 (Fig.  6a, b ). For the FACED-9 task, the sigmoid model achieved optimal performance at L 3 = 7 with an accuracy of 59.3 ± 7.7, while the softmax model peaked at L 3 = 1 with an accuracy of 56.2 ± 7.2. In the FACED-2 task, the sigmoid model performed best at L 3 = 5 with an accuracy of 75.4 ± 5.5, and the softmax model peaked at L 3 = 15 with an accuracy of 74.6 ± 4.3. Considering the performance of both FACED-2 and FACED-9 tasks, L 3 = 15 is identified as a generally effective parameter setting.\n\nWe also examined how varying the time steps of the spatial transition convolution (L 2 ) from 1 to 8 affected model performance while keeping other parameters constant (Fig.  6c, d ). Increasing L 2 to 3 resulted in a dramatic improvement in performance. The performance continued Fig.  3 : The pipeline of interpretability analysis. The integrated gradient method is used to identify the most important dimension or feature for each emotion in the Multilayer Perceptron. The temporal convolution kernel (or temporal filter), the spatial transition convolution kernel (or spatial filters), and the attention weights that produce this feature are visualized. Fourier transform of the temporal filter is identified as its frequency responses. Spatial activations are defined as multiplications of spatial filters with their corresponding inputs' covariance.\n\nto improve until L 2 reached 6, after which no further gains were observed. Thus, L 2 = 3 is identified as the optimal setting, balancing model efficiency and performance. The peak performance varied slightly with different activation functions and classification tasks, with sigmoid generally outperforming softmax.\n\nWe also experimented to see the effect of data noise on model performance. The complete preprocessing used the default threshold in EEGLab to remove eye and muscle components in ICA. Here, we test the effect of less noise removal on the emotion recognition performance. Specifically, we only remove the eye-blink components in ICA. Surprisingly, the accuracy increased dramatically by 4.8% for the binary classification of the FACED dataset and by 8.0% for the nine-class classification of the FACED dataset. The results of less noise removal are comparable to those obtained using the officially preprocessed data, where primarily eye-blink components were removed. The results suggest that activities typically considered as EEG noise might contain useful information for emotion recognition. Since this study focuses on the emotion recognition capability of brain components and aims to provide insights into the brain mechanisms underlying emotion processing, we used data from the complete preprocessing pipeline throughout our analysis. Future studies could explore the effects of isolating eye-related and muscle-related noise on emotion recognition.",
      "page_start": 8,
      "page_end": 9
    },
    {
      "section_name": "Discussion",
      "text": "This study proposes a biologically interpretable dynamic attention mechanism to model the state transitions in EEG activities. The dynamic attention mechanism has been shown to significantly enhance the performance of emotion recognition across three datasets, underscoring the critical role of neural state transitions underlying emotion processing. In addition, we extend the spatial convolution to cover more time steps with varying dilation sizes, which effectively captures spatial transition patterns at multiple time scales. These architectural designs enable the network to compre-Fig.  4 : Visualization of the spatiotemporal dynamics for the most important dimension of each emotion on the FACED-9 task. The temporal filters (the first column) and their frequency response (the second column), spatial filters and spatial activations (the third column), and example segments of attention weights (the fourth column) for the most important dimensions are shown. The \"+\"/\"-\" symbols in the parentheses following the emotion category name indicate that an increase in that feature corresponds to a high/lower probability of the corresponding emotion. The \"interval\" noted below spatial activations refers to the time interval of dilations for the corresponding spatial transition convolution kernel. hensively learn the spatiotemporal dynamics within EEG signals.\n\nIn model comparisons, the performance enhancement of the DAEST model in more fine-grained discrete emotion classification is notable, with an accuracy increase of 16.1% on the FACED-9 task and an accuracy increase of 6.3% on the SEED-V dataset in comparison to the stateof-the-art method. The higher performance of DAEST over CLISA (contrastive learning with only temporal and spatial convolutions in the encoder) indicates the effectiveness of spatial transition convolution and dynamic attention designs for discrete emotion classification. From the confusion matrices, we can see that the more challenging discrimination of distinct emotions with the same valence is enhanced by dynamic attention mechanisms, especially amusement, inspiration, and joy. The dynamic attention mechanisms also significantly outperformed global channel attention, which estimates a constant attention weight for each dimension. Similar designs have been adopted in previous studies  [8] . This indicates the importance of considering dynamic weights on the latent dimensions, which model the change of neural states. In summary, DAEST is a lightweight interpretable model for EEG-based emotion recognition with an extraordinary ability in fine-grained discrete emotion recognition.\n\nThe spatiotemporal dynamic patterns learned by the model shed light on neural mechanisms of discrete emotion processing. Anger, joy, and, tenderness share the most important feature and lie on two poles of this attribute, i.e., the increase of this feature is associated with more anger, and the decrease of this feature is associated with more joy or tenderness. Other discrete emotions all have distinct important spatiotemporal dynamic patterns. The positive emotions, amusement, and inspiration, have a higher frequency response, and negative emotions, disgust and sadness, have a lower frequency response. Some of the spatial activation patterns resemble microstates C and D  [13]  and new activation patterns not presented in microstate analysis were also learned (Fig.  4 ). The building blocks of spatial transition patterns in EEG can be further investigated in future studies.\n\nBy employing contrastive learning for inter-subject alignment, the model successfully learns representations shared across subjects, ensuring robust generalization to new subjects. Other self-supervised learning techniques, like reconstruction-based pertaining  [60] , have also been proven effective in mitigating cross-subject discrepancy. Future studies can further explore these techniques comprehensively.",
      "page_start": 9,
      "page_end": 9
    },
    {
      "section_name": "Conclusion",
      "text": "In conclusion, we proposed a dynamic attention mechanism for EEG state transition modeling. The model achieved state-of-the-art cross-subject emotion recognition performance on three publicly available datasets. It obtained an accuracy of 75.",
      "page_start": 11,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: To facilitate cross-subject emotion recog-",
      "page": 3
    },
    {
      "caption": "Figure 1: The architecture of the dynamic-attention-based EEG state transition model.",
      "page": 4
    },
    {
      "caption": "Figure 2: ), comparing models with and with-",
      "page": 7
    },
    {
      "caption": "Figure 3: ). Thus, the temporal-frequency character-",
      "page": 7
    },
    {
      "caption": "Figure 4: ). The spatial activation is calculated by",
      "page": 7
    },
    {
      "caption": "Figure 4: ). Disgust and sadness shared",
      "page": 7
    },
    {
      "caption": "Figure 2: Confusion matrices on the FACED-9, SEED, and SEED-V classification tasks (without DyA module and with DyA",
      "page": 8
    },
    {
      "caption": "Figure 6: a,b). For the FACED-9",
      "page": 8
    },
    {
      "caption": "Figure 6: c,d). Increasing L2 to 3 resulted in a dramatic",
      "page": 8
    },
    {
      "caption": "Figure 3: The pipeline of interpretability analysis. The integrated gradient method is used to identify the most important",
      "page": 9
    },
    {
      "caption": "Figure 4: Visualization of the spatiotemporal dynamics for the most important dimension of each emotion on the FACED-9",
      "page": 10
    },
    {
      "caption": "Figure 5: Correlation of feature contributions to each emotion",
      "page": 11
    },
    {
      "caption": "Figure 4: ). The building blocks of spatial",
      "page": 11
    },
    {
      "caption": "Figure 6: The performance with varied hyperparameters (on the FACED task).",
      "page": 12
    }
  ],
  "tables": [
    {
      "caption": "Table 1: Cross-subject emotion recognition performance on",
      "page": 6
    },
    {
      "caption": "Table 2: Cross-subject emotion recognition performance on",
      "page": 6
    },
    {
      "caption": "Table 1: ). In comparison, existing domain generaliza-",
      "page": 6
    },
    {
      "caption": "Table 2: ). It achieved an accuracy of",
      "page": 6
    },
    {
      "caption": "Table 3: , show the",
      "page": 6
    },
    {
      "caption": "Table 3: Performance comparison of different model archi-",
      "page": 7
    },
    {
      "caption": "Table 4: The effect of denoising on the performance",
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Affective computing",
      "authors": [
        "R Picard"
      ],
      "year": "2000",
      "venue": "Affective computing"
    },
    {
      "citation_id": "2",
      "title": "A review of emotion recognition using physiological signals",
      "authors": [
        "L Shu",
        "J Xie",
        "M Yang",
        "Z Li",
        "Z Li",
        "D Liao",
        "X Xu",
        "X Yang"
      ],
      "year": "2018",
      "venue": "Sensors"
    },
    {
      "citation_id": "3",
      "title": "Deap: A database for emotion analysis; using physiological signals",
      "authors": [
        "S Koelstra",
        "C Muhl",
        "M Soleymani",
        "J.-S Lee",
        "A Yazdani",
        "T Ebrahimi",
        "T Pun",
        "A Nijholt",
        "I Patras"
      ],
      "year": "2011",
      "venue": "IEEE transactions on affective computing"
    },
    {
      "citation_id": "4",
      "title": "Differential entropy feature for eeg-based emotion classification",
      "authors": [
        "R.-N Duan",
        "J.-Y Zhu",
        "B.-L Lu"
      ],
      "year": "2013",
      "venue": "Differential entropy feature for eeg-based emotion classification"
    },
    {
      "citation_id": "5",
      "title": "Spatial-temporal recurrent neural network for emotion recognition",
      "authors": [
        "T Zhang",
        "W Zheng",
        "Z Cui",
        "Y Zong",
        "Y Li"
      ],
      "year": "2018",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "6",
      "title": "Eeg-based emotion recognition using an end-to-end regionalasymmetric convolutional neural network",
      "authors": [
        "H Cui",
        "A Liu",
        "X Zhang",
        "X Chen",
        "K Wang",
        "X Chen"
      ],
      "year": "2020",
      "venue": "Knowledge-Based Systems"
    },
    {
      "citation_id": "7",
      "title": "Eeg emotion recognition using dynamical graph convolutional neural networks",
      "authors": [
        "T Song",
        "W Zheng",
        "P Song",
        "Z Cui"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Eeg-based emotion recognition via channel-wise attention and self attention",
      "authors": [
        "W Tao",
        "C Li",
        "R Song",
        "J Cheng",
        "Y Liu",
        "F Wan",
        "X Chen"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Is temporo-spatial dynamics the \"common currency\" of brain and mind? in quest of \"spatiotemporal neuroscience",
      "authors": [
        "G Northoff",
        "S Wainio-Theberge",
        "K Evers"
      ],
      "year": "2020",
      "venue": "Physics of Life Reviews"
    },
    {
      "citation_id": "10",
      "title": "Interacting spiral wave patterns underlie complex brain dynamics and are related to cognitive processing",
      "authors": [
        "Y Xu",
        "X Long",
        "J Feng",
        "P Gong"
      ],
      "year": "2023",
      "venue": "Nature human behaviour"
    },
    {
      "citation_id": "11",
      "title": "Spatiotemporal dynamics of neuronal population response in the primary visual cortex",
      "authors": [
        "D Zhou",
        "A Rangan",
        "D Mclaughlin",
        "D Cai"
      ],
      "year": "2013",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "12",
      "title": "Where does eeg come from and what does it mean?",
      "authors": [
        "M Cohen"
      ],
      "year": "2017",
      "venue": "Trends in neurosciences"
    },
    {
      "citation_id": "13",
      "title": "Eeg microstates as a tool for studying the temporal dynamics of whole-brain neuronal networks: a review",
      "authors": [
        "C Michel",
        "T Koenig"
      ],
      "year": "2018",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "14",
      "title": "Spectrally resolved fast transient brain states in electrophysiological data",
      "authors": [
        "D Vidaurre",
        "A Quinn",
        "A Baker",
        "D Dupret",
        "A Tejero-Cantero",
        "M Woolrich"
      ],
      "year": "2016",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "15",
      "title": "Microstate functional connectivity in eeg cognitive tasks revealed by a multivariate gaussian hidden markov model with phase locking value",
      "authors": [
        "N Duc",
        "B Lee"
      ],
      "year": "2019",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "16",
      "title": "Assessing the depth of language processing in patients with disorders of consciousness",
      "authors": [
        "P Gui",
        "Y Jiang",
        "D Zang",
        "Z Qi",
        "J Tan",
        "H Tanigawa",
        "J Jiang",
        "Y Wen",
        "L Xu",
        "J Zhao"
      ],
      "year": "2020",
      "venue": "Nature neuroscience"
    },
    {
      "citation_id": "17",
      "title": "The functional significance of eeg mi-crostates-associations with modalities of thinking",
      "authors": [
        "P Milz",
        "P Faber",
        "D Lehmann",
        "T Koenig",
        "K Kochi",
        "R Pascual-Marqui"
      ],
      "year": "2016",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "18",
      "title": "Eeg microstates distinguish between cognitive components of fluid reasoning",
      "authors": [
        "F Zappasodi",
        "M Perrucci",
        "A Saggino",
        "P Croce",
        "P Mercuri",
        "R Romanelli",
        "R Colom",
        "S Ebisch"
      ],
      "year": "2019",
      "venue": "Neuroimage"
    },
    {
      "citation_id": "19",
      "title": "Eeg based dynamic functional connectivity analysis in mental workload tasks with different types of information",
      "authors": [
        "K Guan",
        "Z Zhang",
        "X Chai",
        "Z Tian",
        "T Liu",
        "H Niu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Neural Systems and Rehabilitation Engineering"
    },
    {
      "citation_id": "20",
      "title": "Spontaneous transient brain states in eeg source space in disorders of consciousness",
      "authors": [
        "Y Bai",
        "J He",
        "X Xia",
        "Y Wang",
        "Y Yang",
        "H Di",
        "X Li",
        "U Ziemann"
      ],
      "year": "2021",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "21",
      "title": "Exploring eeg microstates for affective computing: decoding valence and arousal experiences during video watching",
      "authors": [
        "X Shen",
        "X Hu",
        "S Liu",
        "S Song",
        "D Zhang"
      ],
      "year": "2020",
      "venue": "2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "22",
      "title": "Dual-thresholdbased microstate analysis on characterizing temporal dynamics of affective process and emotion recognition from eeg signals",
      "authors": [
        "J Chen",
        "H Li",
        "L Ma",
        "H Bo",
        "F Soong",
        "Y Shi"
      ],
      "year": "2021",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "23",
      "title": "Exploring brain activity for positive and negative emotions by means of eeg microstates",
      "authors": [
        "G Prete",
        "P Croce",
        "F Zappasodi",
        "L Tommasi",
        "P Capotosto"
      ],
      "year": "2022",
      "venue": "Scientific Reports"
    },
    {
      "citation_id": "24",
      "title": "Eeg microstate correlates of emotion dynamics and stimulation content during video watching",
      "authors": [
        "W Hu",
        "Z Zhang",
        "H Zhao",
        "L Zhang",
        "L Li",
        "G Huang",
        "Z Liang"
      ],
      "year": "2023",
      "venue": "Cerebral Cortex"
    },
    {
      "citation_id": "25",
      "title": "The eeg microstate representation of discrete emotions",
      "authors": [
        "J Liu",
        "X Hu",
        "X Shen",
        "Z Lv",
        "S Song",
        "D Zhang"
      ],
      "year": "2023",
      "venue": "International Journal of Psychophysiology"
    },
    {
      "citation_id": "26",
      "title": "Self-supervised learning: Generative or contrastive",
      "authors": [
        "X Liu",
        "F Zhang",
        "Z Hou",
        "L Mian",
        "Z Wang",
        "J Zhang",
        "J Tang"
      ],
      "year": "2021",
      "venue": "IEEE transactions on knowledge and data engineering"
    },
    {
      "citation_id": "27",
      "title": "Eeg microstates in social and affective neuroscience",
      "authors": [
        "B Schiller",
        "M Sperl",
        "T Kleinert",
        "K Nash",
        "L Gianotti"
      ],
      "year": "2024",
      "venue": "Brain topography"
    },
    {
      "citation_id": "28",
      "title": "Unsupervised learning of brain state dynamics during emotion imagination using high-density eeg",
      "authors": [
        "S.-H Hsu",
        "Y Lin",
        "J Onton",
        "T.-P Jung",
        "S Makeig"
      ],
      "year": "2022",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "29",
      "title": "Modeling brain dynamic state changes with adaptive mixture independent component analysis",
      "authors": [
        "S.-H Hsu",
        "L Pion-Tonachini",
        "J Palmer",
        "M Miyakoshi",
        "S Makeig",
        "T.-P Jung"
      ],
      "year": "2018",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "30",
      "title": "Emotion recognition based on fusion of local cortical activations and dynamic functional networks connectivity: An eeg study",
      "authors": [
        "F Al-Shargie",
        "U Tariq",
        "M Alex",
        "H Mir",
        "H Al-Nashash"
      ],
      "year": "2019",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "31",
      "title": "Within and between-person correlates of the temporal dynamics of resting eeg microstates",
      "authors": [
        "A Zanesco",
        "B King",
        "A Skwara",
        "C Saron"
      ],
      "year": "2020",
      "venue": "NeuroImage"
    },
    {
      "citation_id": "32",
      "title": "Reliability and individual specificity of eeg microstate characteristics",
      "authors": [
        "J Liu",
        "J Xu",
        "G Zou",
        "Y He",
        "Q Zou",
        "J.-H Gao"
      ],
      "year": "2020",
      "venue": "Brain Topography"
    },
    {
      "citation_id": "33",
      "title": "Hyper-hmm: aligning human brains and semantic features in a common latent event space",
      "authors": [
        "C Lee",
        "J Han",
        "M Feilong",
        "G Jiahui",
        "J Haxby",
        "C Baldassano"
      ],
      "year": "2023",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "34",
      "title": "Uncovering hidden brain state dynamics that regulate performance and decision-making during cognition",
      "authors": [
        "J Taghia",
        "W Cai",
        "S Ryali",
        "J Kochalka",
        "J Nicholas",
        "T Chen",
        "V Menon"
      ],
      "year": "2018",
      "venue": "Nature communications"
    },
    {
      "citation_id": "35",
      "title": "Sparsedgcnn: Recognizing emotion from multichannel eeg signals",
      "authors": [
        "G Zhang",
        "M Yu",
        "Y.-J Liu",
        "G Zhao",
        "D Zhang",
        "W Zheng"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "36",
      "title": "Eeg-based emotion recognition using regularized graph neural networks",
      "authors": [
        "C Peng Zhong",
        "Di Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "A multi-dimensional graph convolution network for eeg emotion recognition",
      "authors": [
        "G Du",
        "J Su",
        "L Zhang",
        "K Su",
        "X Wang",
        "S Teng",
        "P Liu"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "38",
      "title": "A dual-branch dynamic graph convolution based adaptive transformer feature fusion network for eeg emotion recognition",
      "authors": [
        "M Sun",
        "W Cui",
        "S Yu",
        "H Han",
        "B Hu",
        "Y Li"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "39",
      "title": "Gcb-net: Graph convolutional broad network and its application in emotion recognition",
      "authors": [
        "T Zhang",
        "X Wang",
        "X Xu",
        "C Chen"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Semi-supervised dual-stream self-attentive adversarial graph contrastive learning for cross-subject eeg-based emotion recognition",
      "authors": [
        "W Ye",
        "Z Zhang",
        "F Teng",
        "M Zhang",
        "J Wang",
        "D Ni",
        "F Li",
        "P Xu",
        "Z Liang"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "41",
      "title": "From regional to global brain: A novel hierarchical spatial-temporal neural network model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "W Zheng",
        "L Wang",
        "Y Zong",
        "Z Cui"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "42",
      "title": "An efficient lstm network for emotion recognition from multichannel eeg signals",
      "authors": [
        "X Du",
        "C Ma",
        "G Zhang",
        "J Li",
        "Y.-K Lai",
        "G Zhao",
        "X Deng",
        "Y.-J Liu",
        "H Wang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition based on eeg using lstm recurrent neural network",
      "authors": [
        "S Alhagry",
        "A Fahmy",
        "R El-Khoribi"
      ],
      "year": "2017",
      "venue": "International Journal of Advanced Computer Science and Applications"
    },
    {
      "citation_id": "44",
      "title": "Sae+ lstm: A new framework for emotion recognition from multi-channel eeg",
      "authors": [
        "X Xing",
        "Z Li",
        "T Xu",
        "L Shu",
        "B Hu",
        "X Xu"
      ],
      "year": "2019",
      "venue": "Frontiers in neurorobotics"
    },
    {
      "citation_id": "45",
      "title": "Utilizing deep learning towards multi-modal bio-sensing and vision-based affective computing",
      "authors": [
        "T.-P Jung",
        "T Sejnowski"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "46",
      "title": "Spatio-temporal representation of an electroencephalogram for emotion recognition using a threedimensional convolutional neural network",
      "authors": [
        "H.-Y Cho"
      ],
      "year": "2020",
      "venue": "Sensors"
    },
    {
      "citation_id": "47",
      "title": "Electroencephalographic spectral asymmetry index for detection of depression",
      "authors": [
        "H Hinrikus",
        "A Suhhova",
        "M Bachmann",
        "K Aadamsoo",
        "Ü Õhma",
        "J Lass",
        "V Tuulik"
      ],
      "year": "2009",
      "venue": "Medical & biological engineering & computing"
    },
    {
      "citation_id": "48",
      "title": "A novel bi-hemispheric discrepancy model for eeg emotion recognition",
      "authors": [
        "Y Li",
        "L Wang",
        "W Zheng",
        "Y Zong",
        "L Qi",
        "Z Cui",
        "T Zhang",
        "T Song"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "49",
      "title": "Temporal aware mixed attention-based convolution and transformer network (mactn) for eeg emotion recognition",
      "authors": [
        "X Si",
        "D Huang",
        "Y Sun",
        "D Ming"
      ],
      "year": "2024",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "50",
      "title": "Spatial-temporal transformers for eeg emotion recognition",
      "authors": [
        "J Liu",
        "H Wu",
        "L Zhang",
        "Y Zhao"
      ],
      "year": "2022",
      "venue": "Proceedings of the 6th International Conference on Advances in Artificial Intelligence"
    },
    {
      "citation_id": "51",
      "title": "Domain adaptation techniques for eeg-based emotion recognition: a comparative study on two public datasets",
      "authors": [
        "Z Lan",
        "O Sourina",
        "L Wang",
        "R Scherer",
        "G Üller-Putz"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "52",
      "title": "Transfer learning for eeg-based braincomputer interfaces: A review of progress made since 2016",
      "authors": [
        "D Wu",
        "Y Xu",
        "B.-L Lu"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "53",
      "title": "Personalizing eeg-based affective models with transfer learning",
      "authors": [
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2016",
      "venue": "Proceedings of the twenty-fifth international joint conference on artificial intelligence"
    },
    {
      "citation_id": "54",
      "title": "Multisource transfer learning for cross-subject eeg emotion recognition",
      "authors": [
        "J Li",
        "S Qiu",
        "Y.-Y Shen",
        "C.-L Liu",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE transactions on cybernetics"
    },
    {
      "citation_id": "55",
      "title": "Domain-adversarial training of neural networks",
      "authors": [
        "H Yaroslav Ganin",
        "Evgeniya Ustinova"
      ],
      "year": "2016",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "56",
      "title": "Domain adaptation for eeg emotion recognition based on latent representation similarity",
      "authors": [
        "J Li",
        "S Qiu",
        "C Du",
        "Y Wang",
        "H He"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Cognitive and Developmental Systems"
    },
    {
      "citation_id": "57",
      "title": "Reducing the subject variability of eeg signals with adversarial domain generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "W.-L Zheng",
        "B.-L Lu"
      ],
      "year": "2019",
      "venue": "Neural Information Processing: 26th International Conference"
    },
    {
      "citation_id": "58",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "L.-M Zhao",
        "X Yan",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "59",
      "title": "Contrastive learning of subject-invariant eeg representations for cross-subject emotion recognition",
      "authors": [
        "X Shen",
        "X Liu",
        "X Hu",
        "D Zhang",
        "S Song"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "60",
      "title": "Dmmr: Cross-subject domain generalization for eeg-based emotion recognition via denoising mixed mutual reconstruction",
      "authors": [
        "Y Wang",
        "B Zhang",
        "Y Tang"
      ],
      "year": "2024",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "61",
      "title": "Eegnet: a compact convolutional neural network for eeg-based brain-computer interfaces",
      "authors": [
        "V Lawhern",
        "A Solon",
        "N Waytowich",
        "S Gordon",
        "C Hung",
        "B Lance"
      ],
      "year": "2018",
      "venue": "Journal of neural engineering"
    },
    {
      "citation_id": "62",
      "title": "Generalized contrastive partial label learning for cross-subject eeg-based emotion recognition",
      "authors": [
        "W Li",
        "L Fan",
        "S Shao",
        "A Song"
      ],
      "year": "2024",
      "venue": "IEEE Transactions on Instrumentation and Measurement"
    },
    {
      "citation_id": "63",
      "title": "Reducing the subject variability of eeg signals with adversarial domain generalization",
      "authors": [
        "B.-Q Ma",
        "H Li",
        "W.-L Zheng"
      ],
      "year": "2019",
      "venue": "International Conference on Neural Information Processing"
    },
    {
      "citation_id": "64",
      "title": "Plug-and-play domain adaptation for cross-subject eeg-based emotion recognition",
      "authors": [
        "L Zhao",
        "X Yan",
        "B.-L Lu"
      ],
      "year": "2021",
      "venue": "Proceedings of the 35th AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "65",
      "title": "Local domain generalization with low-rank constraint for eeg-based emotion recognition",
      "authors": [
        "J Tao",
        "Y Dan",
        "D Zhou"
      ],
      "year": "2023",
      "venue": "Frontiers in Neuroscience"
    },
    {
      "citation_id": "66",
      "title": "Two-phase prototypical contrastive domain generalization for cross-subject eeg-based emotion recognition",
      "authors": [
        "H Cai",
        "J Pan"
      ],
      "year": "2023",
      "venue": "ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "67",
      "title": "On the interpretation of weight vectors of linear models in multivariate neuroimaging",
      "authors": [
        "S Haufe",
        "F Meinecke",
        "K Örgen",
        "S Dähne",
        "J.-D Haynes",
        "B Blankertz",
        "F Bießmann"
      ],
      "year": "2014",
      "venue": "Neuroimage"
    }
  ]
}