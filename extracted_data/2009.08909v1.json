{
  "paper_id": "2009.08909v1",
  "title": "Optimizing Speech Emotion Recognition Using Manta-Ray Based Feature Selection",
  "published": "2020-09-18T16:09:34Z",
  "authors": [
    "Soham Chattopadhyay",
    "Arijit Dey",
    "Hritam Basak"
  ],
  "keywords": [
    "Audio signal processing",
    "Emotion recognition",
    "LPC",
    "MFCC",
    "Optimization algorithm"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Emotion recognition from audio signals has been regarded as a challenging task in signal processing as it can be considered as a collection of static and dynamic classification tasks. Recognition of emotions from speech data has been heavily relied upon end-to-end feature extraction and classification using machine learning models, though the absence of feature selection and optimization have restrained the performance of these methods. Recent studies have shown that Mel Frequency Cepstral Coefficients (MFCC) have been emerged as one of the most relied feature extraction methods, though it circumscribes the accuracy of classification with a very small feature dimension. In this paper, we propose that the concatenation of features, extracted by using different existing feature extraction methods can not only boost the classification accuracy but also expands the possibility of efficient feature selection. We have used Linear Predictive Coding (LPC) apart from the MFCC feature extraction method, before feature merging. Besides, we have performed a novel application of Manta Ray optimization in speech emotion recognition tasks that resulted in a state-of-the-art result in this field. We have evaluated the performance of our model using SAVEE and Emo-DB, two publicly available datasets. Our proposed method outperformed all the existing methods in speech emotion analysis and resulted in a decent result in these two datasets with a classification accuracy of 97.49% and 97.68% respectively.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "Expressing emotions is an important part of communication among human and non-human primates and it is the most common way to express love, sorrow, anger, hatred, or any other state of mind  [1] ,  [6] . Even, non-speaking living beings also find their ways to express their emotions. We find the verbal communication and associated emotions so important that we often miss the scarcity of those in text messages or emails and hence switch to the usage of emojis. Since emotions help us to understand each other better, we have tried to implement emotion recognition using computers as well. With the recent advancements in natural language processing and speech to text conversions, scientists have often looked upon the automation of information parsing from speech audio and artificial intelligence has been successfully deployed for generating auto-replies; the chat-bots and recent speakinghumanoid robots are exemplary evidence of these advance- * Corresponding author ments. However, the emotion analysis of auditory signals has been studied lately by researchers and several improvements have been made in this domain for the last two decades. However, in real life it is a difficult work to predict human emotion from a conversation. The most important work in the acoustic signal processing field is to extract features properly. Now a days different machine learning and deep learning models are made to deal with signal processing task. In this literature we have used different methods for feature extraction task. However, every extracted features from the audio is not accurate and can be redundant. That is the main reason researchers find it difficult to remove redundancy from the feature vector. There are different features of a audio signal such that, 1. Temporal features and spectral features. We are mainly focusing on temporal features and there are some standard techniques, Mel-frequency cepstral coefficient(MFCC)  [39] , Linear prediction coefficients (LPC)  [40] , Linear prediction cepstral coefficients (LPCC)  [41] , Perceptual Linear Prediction (PLP)  [42]  etc. Combination of these features give the accurate result and can explain the nature of the audio signal properly.\n\nAlthough the Speech Emotion Recognition (SER) has versatile applications  [11] , there exists no generalized or common consensus on categorization and classification of emotions from speech signals as the emotions are subjective property of human. Besides, the emotions may vary in intensity, mode of expression from person to person, and may vary or frequently misinterpreted by people. Therefore, the automatic AI-based classification of emotion from auditory signals is the testbed of assessing the performance of several existing feature extraction and classification methods  [14] . Though the discrete and dimensional models have shown improved performance in recent times, there is a huge scope of improvements as it remains an open problem in a birds eye view.\n\nThe human voice can have features from different modalities, though the most predominant ones are: (1) voice quality, (2) Teager energy operator, (3) prosodic, and (4) spectral features,  [8]  though, the classifier performance can be improved by incorporating features from other different modalities too. As supervised learning is based on the feature quality and accurately labeled dataset, the performance of these classification problems has highly relied upon the efficacy and experience of the feature engineer performing the feature Contribution of this paper: The contribution of this paper can, therefore, be considered as a two-fold contribution: First, we explore the feature concatenation in auditory signal processing. Merging of features from different sources has been successfully utilized in the image processing task before  [1] [2] [3] [4] . Being inspired by these, we have successfully evolved our proposed approach based on feature concatenation. Secondly, we have proposed a bio-inspired meta-heuristic Manta-Ray foraging Optimization algorithm for feature selection and removal of redundant features. This algorithm was evolved in 2020 by Zhao et al.  [5] , being inspired by the behavioral study of an aquatic species named Manta Ray.\n\nOur proposed method outperformed all the existing methods in speech emotion analysis and resulted in a state-of-the-art result in the SAVEE dataset with a classification accuracy of 97.49% on average. We also validated our method on the Emo-DB dataset that also produced a state-of-the-art result with a classification accuracy of 97.68%. Fig 1 . shows the workflow digram of our proposed method.\n\nThe results of our experiment are described in Section 4.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Ii. Related Works",
      "text": "Speech emotion recognition deals with the basic task of identification of emotions and expressions from audible sounds and an intensive research in this domain started in early 90s. However, recent advancements in technology and computa-tional resources have made this study diverse and compelling. Following are the three major approaches made by the scientists recently to address this problem.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Machine Learning Algorithms",
      "text": "The Research in this field started in the early 1990's, whereas the first significant result was obtained by Nakatsu et al.  [7]  in 1999 where they had used speech power and basic Linear Predictive Coding (LPC) features from 100 human specimens, equally distributed among males and females. The simplest neural network model they used, produced a result of 50% recognition accuracy. Later, Schuller et al.  [9]  proposed a hidden-Markov model, and validation was done on the Emo-DB and VAM dataset that produced correct classification accuracy of 76.1% and 71.8% respectively, using raw contours of ZCR, energy and pitch features. Later, the improved version of their proposed Markov model produced a state-of-the-art result on German and English speech signals containing 5250 samples with an average accuracy of 86.8% with global prosodic features using continuous HMM classifier and pitch and energy-based features  [28] . Rao et al.  [11]  used the Support Vector Machine classifier using RBF kernel with approximately 67% classification accuracy using prosodic features. LFPC features were extracted and the HMM classifier was used for classification of Mandarin language by  [12]  with an average precision of 78.5%. Another important result was produced by Wu et al.  [13]  by using an SVM classifier that reached a classification accuracy of 91.3% on the Emo-DB dataset and 86% accuracy on the VAM dataset by using prosodic, ZCR and TEO features. Deng et al.  [6]  validated their proposed method on four different datasets: Emo-DB, VAM, AVIC, and SUSAS by using autoencoder classifiers. The features used were MFCC and other LLD features (example: ZCR and pitch frequency etc.) and denoising was performed before the classification task and presented classification accuracy of 58.3%, 60.2%, 63.1%, and 58.9% respectively.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Deep Learning Methods",
      "text": "With the recent advancement in deep learning in the last few years, scientists have tried to exploit the ability of Deep Neural Networks models to learn high-quality semantic information and invariant features from different types of datasets  [14, 15] . A few recent studies provided results that supported this conclusion that DNNs are equally efficient and useful for speech-emotion classification. Rong et al.  [10]  had used KNN classifiers for classifying the Mandarin dataset using ZCR, spectral, and energy features. Stuhlsatz et al.  [16]  and Kim et al.  [17]  used the utterance level features to train the DNN models. Rozgic et al. trained the DNNs using the combination of lexical and acoustic features. Unlike these existing DNN models, that directly used acoustic features learned from sources, our proposed method uses optimization in between for the improvement in performance.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Feature Selection And Optimization",
      "text": "Research works have also been made on selection of optimal features from the huge feature set, extracted from speech data using various feature extractors. Feature selection is a crucial step in system development to identify emotions in speech. Recently, the interaction between features collected from the auditory signals was rarely considered, which may result to redundant features and sub-optimal classification results. Gharavian et al.  [37]  used FCBF feature selection methods and Genetic algorithm based ARTMAP neural networks for optimization and classification. Particle Swarm Optimization (PSO) is another popular optimization algorithm, used for feature selection in SER by Muthusamy et al.  [38] . Both of them reported a high accuracy as compared to the previous works on various datasets. These laid the groundwork for the analysis of different feature selection algorithm empirically which was further taken forward by others. Liu et al.  [33]  used optimal features before feeding them to the extreme learning machine (ELM) for classification whereas Ververidis et al.  [34]  suggested sequential floating forward feature selection (SFFS) so that the features obey the multivariate Gaussian distribution before using Bayesian classifier for classification. Sheikhan et al.  [36]  proposed ANOVA feature selection method in SER task and used machine learning based modular neural-SVM classifier. zseven  [35]  suggested a novel feature selection algorithm and suggested the superiority of that by validating the results on some standard datasets. These works helped us to further investigate in the SER feature selection task prior to classification.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Iii. Materials And Methods",
      "text": "This section describes the workflow of this experiment which consists of the following steps: (1) Data acquisition and preprocessing, (2) Feature Extraction, (3) Feature selection using optimization algorithm, and (4) Classification and analysis of results.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Data Acquisition",
      "text": "The initial step of the experiment is dataset collection and preprocessing. We used two publicly available datasets for this experimentation purpose. These are:\n\na) SAVEE dataset: The Surrey Audio-Visual Expressed Emotion (SAVEE) Database consists of speech recordings from four British actors consisting of 480 samples in total, collected, and labeled with extreme precision and by using high-quality equipment. The dataset is classified into seven different emotional categories: happiness, sadness, disgust/fear, neutral, common, anger, and surprise. The sentences were chosen by experts carefully from TIMIT Corpus and were balanced phenotypically in every category.\n\nb) Emo-DB dataset: The Berlin Emo-DB dataset contains emotion speech data from 10 different speakers and contains 500 labeled samples in total. It also contains 7 categories of emotion-speeches: normal, anger, sadness, happiness, disgust, anxiety, and fear.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "B. Preprocessing",
      "text": "For every kind of signal processing task, the pre-processing of sample data plays a vital role in determining the performance of a model. Some simple audio pre-processing techniques which have improved the exploitation of our model are discussed below: a) Pre-emphaize: Pre-emphasis step is mainly carried out as it synthesizes the normal form of any amplitude signal. The main idea behind this is to flatten the speech spectrum, which can be done by implementing a high-pass Finite Impulse Response (FIR) filter. The expression of the filter in a discrete frequency domain is given by:\n\nTo normalize the signal, firstly the maximum value of the signal has to be taken as the nominator and dividing the signal with it. Thus the entire signal is normalized between -1 and 1. For smooth transactions between frames 50% overlaps of consecutive frames are accepted. The mechanical acoustic signal can be stable in the range of 50ms to 200ms and so we have selected a short window for better feature extraction.\n\nb) Framing: For further processing, the signal is divided into small frames such that to get a sequence of frames forming the entire original signal. This is done so that a long signal can be analyzed independently in small frames and can be expressed through a single feature vector. Some aspects of framing like frameshift are the time difference between two starting points of two consecutive frames and frame length is the duration of time for each frame. c) Windowing: For audio signals it is quite common of having discontinuities at the frame edges of the signal. This phenomenon often causes bad performance in the audio processing task. To get rid of this some trapped windows such as the Hamming window is applied at each frame. The general expression of the Hamming window is:\n\nwhere a=0.54, b=0.46, and N is the number of samples in a partition of the data into some random complementary subsets.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Feature Extraction",
      "text": "Features play an important role in any classification task as all the information of speech data is embedded in these features. Therefore, high-quality and accurately extracted features may contribute immensely towards the improved performance of a classifier. In this experiment, we have extracted two short-time features from the dataset: Mel Frequency Cepstral Coefficient (MFCC) and the Linear Predictive Coding (LPC). The brief description of the feature-extraction is described below.\n\na) MFCC features: MFCC features are based on the human auditory sensation characteristics. The mathematical simulation of hearing is done in MFCC by using some nonlinear frequency units. We normally use the Fast Fourier Transform or Discrete Fourier transform for the conversion of acoustic signals from the time domain to the frequency domain for each sample frame. The Fast Fourier Transform (FFT) is described by the following equation:\n\nwhere y[n] represents the signal in the time domain and the domain-converted signal y(k) is in the frequency domain and N is the number of samples in every frame. Next, we calculate the Disperse Power Spectrum by using the following equation:\n\nAfter this, we have the MEL spectrum by the PS p in the triangular filter bank. The filter bank consists of the series of triangular filters with the cutoff frequency determined by the center-frequencies of the two presently adjacent filters. These filters are linear in MEL frequency coordinates. The scale is equivalent to the span of every filter and the value for the span is set to 167.859 in this project. The frequency response of the triangular filter can be calculated as:\n\n(5) where p=1,2,.....,12, g(p) is the centre frequency of the filter, n=1,2,....,(N/2-1) where N being number of samples per frame. To improve the quality of the features we used logarithmic spectrum of power spectrum on the signal and which is represented by the following equation.\n\nwhere L(p) is the logarithmic spectrum, F[n] is the series of filters, PS p is defined earlier, N is the number of samples per frame.\n\nFinally, the Discrete Cosine Transform (DCT) of the logarithmic spectrum of the filter banks are calculated that gave the MFCC feature which is described in the following equation.\n\nb) LPC features: The speech signal is sequential data and so lets assume the voice acoustics of n th speech sample is P[n] which can be shown as the combination of previous k samples. The n th speech sample can be mathematically represented as:\n\nwhere S[k-j], where j = 1, 2, 3 . . . k, is the j th test sample, H stands for gain factor, E[k] denotes the excitation of the k th sample, a j is the vocal tract vector coefficient. LPC is also known as inverse filtering because it determines all zero filters which are also inverse of the vocal tract model.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "D. Manta Ray Optimization",
      "text": "In this work we have used the Manta-Ray Foraging Optimization (MRFO) algorithm for feature selection. It is a bioinspired optimization algorithm that works on three unique foraging strategies of Manta Ray.\n\na) Chain foraging: Manta Rays have their unique approach towards their prey, which is imitated in our project. They tend to swim towards the highest concentration of planktons, i.e. the best possible position of food. In our algorithm, we set some random initial values and ask them to move towards the optimal solution in every iteration. Though the actual optimal solution is not known, this algorithm assumes that the best solution is the one where the plankton concentration is highest. All the members of the group proceed towards the plankton concentration, by following the previous member of the group, except the first one. This is known as forming a foraging chain, which is imitated in our experimentation. Fig.  2  is a simulation diagram of chain foraging in 2-D space. Mathematically, the chain foraging is represented by: where P j k (n) is the position of a j th agent in k th dimension, a is the random vector in closed range in 0 to 1, b is known as weight coefficient, P best k (n) is the best plankton concentration position. The position of the (j+1) th is determined by the previous j agents.\n\nb) Cyclone foraging: When a group of manta rays gets to know about a high concentration of plankton in the water, they form a chain-like structure that looks like a cyclone and headed towards the plankton. Each of the agents follows the previous agent towards the prey and forms cyclone foraging. Every manta ray doesnt only form the spiral but also follows the same path and moving towards one step to the plankton following the one in front of it. The mathematical expression for two-dimensional cyclone foraging is given below.\n\nwhere z is a random value in closed range between 0 and 1. This kind of spiral foraging algorithm was also explained by Mirajmili et al. in 2016 for Grey Wolf Optimization, however, this is different from that. Fig.  3  is a simulation diagram of cyclone foraging in 2-D space. The N-dimensional form of the equation is as followed:\n\nwhere γ is the coefficient of weight, I being the maximum iteration and c being a random parameter having value in [0,1]. Each of the search agent perform independent exploration between its current position and the position of the prey. Therefore, this algorithm can efficiently find a best solution in this range. However, we can force any agent to take a new position which is far from its current position by the following equation:\n\nwhere, P r k is the newly specified random position of the k th in the N-dimensional space. B Low k and B Upper k are the lower and upper bounds respectively of the N-dimensional space.Thus this algorithm is suitable for finding any best solution in this N-dimensional space.\n\nc) Somersault foraging: In this type of foraging the best concentration plankton is considered as a pivot point and all manta rays are moving to the point and eventually update their position around the position of the high plankton concentration(i.e. best solution). Somersault foraging is represented by the equation below:\n\nwhere F is known as Factor of somersault, d and g are random parameters in the closed range between 0 and 1.\n\nAs the equation suggests, this algorithm allows the search agents to update their position at any possible position in the range of its current position and the position of the highest concentration of planktons. With the reduction in the distance between the agents and the optimal solution, the perturbation of the current search agent position also gets decreased. Fig.  4  is a representative diagram of somersault foraging of Manta Ray in 2-D space.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "E. Classification",
      "text": "In this current work we have used two different classifiers, one is K-Nearest Neighbourhood classifier and the other one is Multilayer Perceptron classifier commonly known as KNN and MLP classifiers respectively. a) KNN Classifier: KNN is a type of supervised Machine Learning model, which finds the feature similarity between different data points to prognosticate new data point with a value based on how closely it matches with the other data points in the training dataset. The K-Nearest Neighbour algorithm has some inflexible properties like it is very lazy to learn in type. The fact that it has no proper phase of training and classifies with the entire training dataset as a whole, justifies its lazy learning very clearly.\n\nTo implement this algorithm we have to firstly choose a certain value of K that is the number of nearest neighbor points. Thereafter using anyone of the Euclidean or Hamming Distance or Manhattan methods the distance between the test data and each row of training is calculated. Then the points are sorted based on their distance values and top K values are chosen amongst them.\n\nb) MLP Classifier: Multilayer Perceptron (MLP), colloquially known as Vanilla Network is a type of feed-forward Artificial Neural Network. MLP uses back-propagation (a supervised learning technique) to train its weights and biases of different layers. In recent past frequently used activation functions of MLP were sigmoid or tanh, but now as ReLU or Leaky ReLU has proved their better performance so they are used mostly now but specifically in the classification layer still sigmoid or softmax activation functions are used. In this algorithm, changes of each weight in each layer are done by a certain technique called Gradient Descent, given by the following formula:\n\nWhere Ω(n) = 0.5 × j l j 2 (n) is the correction that minimizes the prediction error which is calculated by l j (n)=p j (n)-y j (n). Here l j (n) is the loss, p j (n) and y j (n) are the prediction made by the classifier respectively of the actual class of n th data point of j th node and α is the learning rate. Now say σ' is the derivative of the activation function. Then for the output node, the derivative would be:\n\nFor the hidden nodes, the derivative of change follows comparatively complex mathematics given by\n\nIV. RESULTS AND DISCUSSIONS\n\nThe classification of different audio signals is mainly done by the feed-forward neural classifier, specifically the Multilayer Perceptron (MLP).The quantitative measurements are done by using the following equations:\n\nF 1score i = 2\n\nWhere M ij = the weighted element of the confusion matrix at i th row and j th column. M ii = the weighted diagonal element of the confusion matrix.\n\nIt is mentioned above that we have particularly extracted MFCC and LPC features separately and concatenated them to get the final feature set. From MFCC and LPC methods we got 216 and 743 features respectively. Therefore, after concatenation, we had total features of 959. Usually using LPC feature extraction methods around 13 features are extracted, as we are implementing optimization techniques to get the best feature set, and it is quite evident that if the feature space is pretty vast then the performance of the entire model is improved by many folds. After concatenation the entire feature space becomes of size 480×959, as we had 480 sample data and 959 features were extracted from each sample data. Manta Ray Optimization technique is used for optimized feature selection. It is obvious that the optimized feature space contains lesser features than that of the entire feature space, therefore the problem of overfitting is already being taken care of by the optimization  algorithm. Fig.  7  and Fig.  8  give the obtained ROC curves on both datasets with MLP and KNN calssifiers. With our proposed framework it is observed that the performance of MLP is better than that of the KNN.  To classify the dataset into given classes we have used KNN classifier with number of nearest neighborhood equals to 5 and MLP classifier consisting of two hidden layers, each having 5 neurons. Fig.  6  represents the comparison of the evaluation parameters using MLP and KNN classifiers on Emo-DB and SAVEE datasets. The bar graph shows that the MLP classifier performed better in both the cases. From Table  II  and Table  IV  we can say that our proposed model with MLP classifier achieves State-Of-The-Art accuracies of 97.06% and 97.68% on SAVEE and EmoDB datasets respectively. Apart from that KNN classifier also gives promising accuracies of 96.55% and 96.006% 7 class classification accuracies on aforementioned datasets. In addition, to standardize our model performance we perform a comparative study of MantaRay with other popular optimization algorithms such as Genetic Algorithm  [43] , Particle Swarm  [44]  and Gray-Wolf Optimization algorithm  [45] . The results of the optimization algorithms along with MantaRay is given by Table  V . For SAVEE and EmoDB both datsets MantaRay achieves very good accuracies of 97.06% and 97.68% with significantly smaller feature space having only 43 and 61 features. This fact clearly supports the higher efficiency of our model.\n\nThe performances of the proposed model with KNN and MLP classifier on both SAVEE and EmoDB datasets are shown using bar-plots in Fig.  6 .\n\nTable  VI  and VII display comparative analysis of the obtained result with the existing results so far. The results are selected for comparison in such a way that the experimentation parameters (e.g. train-test splitting ratio on a particular dataset) are more or less similar to maintain uniformity. It is observed that, in both the cases, our method outperforms the existing approaches by a good margin.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Conclusion And Future Work",
      "text": "This paper aims to contribute to the improvement of the speech emotion recognition task using a meta-heuristic Manta-Ray optimization algorithm for discarding redundant features and selecting the most accurate ones for classification. It implements two emotion recognition databases, two feature extraction methods, and achieves the best performance on both datasets. The neural network-based MLP classifier performed better in both the cases as compared to the KNN classifier. This is also justified from Fig.  5  and 6  where the area under ROC curves show better results in MLP classifier in both the datasets. However, we aim for validating our model on some other datasets, and also we would like to add some experimentation with different optimization algorithms. Further, we would like to assess the model performance with variations in sentence length as it has been found difficult to classify long sentences than small syllables earlier. Finally, we would like to perform experiments on some other languages too and try to evaluate the performance of native languages as there exists some relationship between accents and ease of classification.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: Workﬂow diagram of our proposed method.",
      "page": 2
    },
    {
      "caption": "Figure 1: shows the workﬂow",
      "page": 2
    },
    {
      "caption": "Figure 2: is a simulation diagram of chain foraging in 2-D space.",
      "page": 4
    },
    {
      "caption": "Figure 2: Simulation model of chain-foraging of Manta Ray in two-dimensional",
      "page": 5
    },
    {
      "caption": "Figure 3: is a simulation diagram of",
      "page": 5
    },
    {
      "caption": "Figure 3: Simulation model of cyclone-foraging of Manta Ray in two dimen-",
      "page": 5
    },
    {
      "caption": "Figure 4: Simulation diagram of somersault-foraging of Manta Ray in two",
      "page": 5
    },
    {
      "caption": "Figure 4: is a representative diagram of somersault foraging of Manta",
      "page": 6
    },
    {
      "caption": "Figure 5: Combparative study of performance of classiﬁers (i.e. MLP and KNN)",
      "page": 7
    },
    {
      "caption": "Figure 7: and Fig. 8 give the obtained ROC curves",
      "page": 7
    },
    {
      "caption": "Figure 6: represents the comparison of the evaluation",
      "page": 7
    },
    {
      "caption": "Figure 6: Table VI and VII display comparative analysis of the",
      "page": 7
    },
    {
      "caption": "Figure 5: and 6 where the area under",
      "page": 7
    },
    {
      "caption": "Figure 6: Combparative study of performance of classiﬁers (i.e. MLP and KNN) with same features on two diferrent datasets (A) Emo-DB dataset and (B)",
      "page": 8
    },
    {
      "caption": "Figure 7: Reciever Operating Characteristics of KNN classiﬁer on (A) Emo-DB",
      "page": 9
    },
    {
      "caption": "Figure 8: Reciever Operating Characteristics of MLP classiﬁer on (A) Emo-DB",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fold:": "Fold 1",
          "Accuracy": "96.56",
          "Precision": "97",
          "Recall": "97",
          "F1 Score": "97"
        },
        {
          "Fold:": "Fold 2",
          "Accuracy": "95.26",
          "Precision": "96",
          "Recall": "96",
          "F1 Score": "95"
        },
        {
          "Fold:": "Fold 3",
          "Accuracy": "97.81",
          "Precision": "98",
          "Recall": "98",
          "F1 Score": "98"
        },
        {
          "Fold:": "Fold 4",
          "Accuracy": "96.56",
          "Precision": "97",
          "Recall": "97",
          "F1 Score": "97"
        },
        {
          "Fold:": "Fold 5",
          "Accuracy": "96.56",
          "Precision": "97",
          "Recall": "97",
          "F1 Score": "97"
        },
        {
          "Fold:": "Mean & STD:",
          "Accuracy": "96.55%\n0.80",
          "Precision": "97.00%\n0.63",
          "Recall": "97.00%\n0.63",
          "F1 Score": "96.8%\n0.98"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fold:": "Fold 1",
          "Accuracy": "97.91",
          "Precision": "98",
          "Recall": "98",
          "F1 Score": "98"
        },
        {
          "Fold:": "Fold 2",
          "Accuracy": "97.91",
          "Precision": "98",
          "Recall": "98",
          "F1 Score": "98"
        },
        {
          "Fold:": "Fold 3",
          "Accuracy": "96.88",
          "Precision": "97",
          "Recall": "97",
          "F1 Score": "97"
        },
        {
          "Fold:": "Fold 4",
          "Accuracy": "96.88",
          "Precision": "97",
          "Recall": "97",
          "F1 Score": "97"
        },
        {
          "Fold:": "Fold 5",
          "Accuracy": "97.91",
          "Precision": "98",
          "Recall": "98",
          "F1 Score": "98"
        },
        {
          "Fold:": "Mean & STD:",
          "Accuracy": "97.49%\n0.50",
          "Precision": "97.6%\n0.48",
          "Recall": "97.6%\n0.48",
          "F1 Score": "97.6%\n0.48"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fold:": "Fold 1",
          "Accuracy%": "96.00",
          "Precision%": "97",
          "Recall%": "96",
          "F1 Score%": "96"
        },
        {
          "Fold:": "Fold 2",
          "Accuracy%": "95.43",
          "Precision%": "96",
          "Recall%": "96",
          "F1 Score%": "96"
        },
        {
          "Fold:": "Fold 3",
          "Accuracy%": "95.96",
          "Precision%": "96",
          "Recall%": "96",
          "F1 Score%": "96"
        },
        {
          "Fold:": "Fold 4",
          "Accuracy%": "97.21",
          "Precision%": "98",
          "Recall%": "98",
          "F1 Score%": "97"
        },
        {
          "Fold:": "Fold 5",
          "Accuracy%": "95.43",
          "Precision%": "96",
          "Recall%": "96",
          "F1 Score%": "96"
        },
        {
          "Fold:": "Mean & STD:",
          "Accuracy%": "96.006%\n0.65",
          "Precision%": "96.60%\n0.80",
          "Recall%": "96.40%\n0.80",
          "F1 Score%": "96.2%\n0.40"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Fold:": "Fold 1",
          "Accuracy%": "97.25",
          "Precision%": "98",
          "Recall%": "97",
          "F1 Score%": "98"
        },
        {
          "Fold:": "Fold 2",
          "Accuracy%": "98.31",
          "Precision%": "99",
          "Recall%": "99",
          "F1 Score%": "98"
        },
        {
          "Fold:": "Fold 3",
          "Accuracy%": "98.31",
          "Precision%": "99",
          "Recall%": "99",
          "F1 Score%": "99"
        },
        {
          "Fold:": "Fold 4",
          "Accuracy%": "98.31",
          "Precision%": "99",
          "Recall%": "99",
          "F1 Score%": "99"
        },
        {
          "Fold:": "Fold 5",
          "Accuracy%": "96.22",
          "Precision%": "97",
          "Recall%": "97",
          "F1 Score%": "96"
        },
        {
          "Fold:": "Mean & STD:",
          "Accuracy%": "97.68%\n0.83",
          "Precision%": "98.40%\n0.80",
          "Recall%": "98.20%\n0.97",
          "F1 Score%": "98.00%\n1.09"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Optimization\nNumber of selected\nAccuracy\nPrecision\nRecall\nF1 score\nDataset\nAlgorithm\nfeatures\n(rounded off)\n(rounded off)\n(rounded off)\n(rounded off)": "Genetic .\n325\n72.23%\n73%\n73%\n74%\nAlgorithm\nGrey Wolf\nSAVEE\n89\n84.31%\n85%\n87%\n82%\nOptimization Algorithm\nParticle Swarm\n35\n81.32%\n81%\n82%\n83%\nOptimization\nManta Ray\n43\n97.06%\n97%\n98%\n99%\nOptimization"
        },
        {
          "Optimization\nNumber of selected\nAccuracy\nPrecision\nRecall\nF1 score\nDataset\nAlgorithm\nfeatures\n(rounded off)\n(rounded off)\n(rounded off)\n(rounded off)": "Genetic .\n230\n85.91%\n87%\n86%\n87%\nAlgorithm\nGrey Wolf\nEmo-DB\n76\n82.21%\n83%\n82%\n83%\nOptimization Algorithm\nParticle Swarm\n26\n87.29%\n88%\n87%\n89%\nOptimization\nManta Ray\n61\n97.68%\n99%\n98%\n98%\nOptimization"
        }
      ],
      "page": 8
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Improving Classification Performance by Merging Distinct Feature Sets of Similar Quality Generated by Multiple Initializations of mRMR",
      "authors": [
        "T Bottesch",
        "G Palm"
      ],
      "year": "2015",
      "venue": "IEEE Symposium Series on Computational Intelligence, Cape Town",
      "doi": "10.1109/SSCI.2015.56"
    },
    {
      "citation_id": "2",
      "title": "Classification of Content based Medical Image Retrieval Using Texture and Shape feature with Neural Network",
      "authors": [
        "S Maniar",
        "J Shah"
      ],
      "year": "2017",
      "venue": "International Journal of Advances in Applied Sciences",
      "doi": "10.11591/ijaas.v6.i4.pp368-374"
    },
    {
      "citation_id": "3",
      "title": "Feature selection for text classification based on part of speech filter and synonym merge",
      "authors": [
        "Sijun Qin",
        "Jia Song",
        "P Zhang",
        "Yue Tan"
      ],
      "year": "2015",
      "venue": "12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)",
      "doi": "10.1109/FSKD.2015.7382024"
    },
    {
      "citation_id": "4",
      "title": "Merging recovery feature network to faster RCNN for low-resolution images detection",
      "authors": [
        "R Zhang",
        "Y Yang"
      ],
      "year": "2017",
      "venue": "2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP), Montreal, QC",
      "doi": "10.1109/GlobalSIP.2017.8309157"
    },
    {
      "citation_id": "5",
      "title": "Manta ray foraging optimization: An effective bio-inspired optimizer for engineering applications",
      "authors": [
        "W Zhao",
        "Z Zhang",
        "L Wang"
      ],
      "year": "2020",
      "venue": "Engineering Applications of Artificial Intelligence",
      "doi": "10.1016/j.engappai.2019.103300"
    },
    {
      "citation_id": "6",
      "title": "Sparse Autoencoder-Based Feature Transfer Learning for Speech Emotion Recognition",
      "authors": [
        "J Deng",
        "Z Zhang",
        "E Marchi",
        "B Schuller"
      ],
      "year": "2013",
      "venue": "2013 Humaine Association Conference on Affective Computing and Intelligent Interaction",
      "doi": "10.1109/ACII.2013.90"
    },
    {
      "citation_id": "7",
      "title": "Emotion recognition and its application to computer agents with spontaneous interactive capabilities",
      "authors": [
        "R Nakatsu",
        "J Nicholson",
        "N Tosa"
      ],
      "year": "1999",
      "venue": "Reciever Operating Characteristics of KNN classifier on (A) Emo-DB dataset and (B) SAVEE dataset"
    },
    {
      "citation_id": "8",
      "title": "Reciever Operating Characteristics of MLP classifier on (A) Emo-DB dataset and (B) SAVEE dataset",
      "venue": "Reciever Operating Characteristics of MLP classifier on (A) Emo-DB dataset and (B) SAVEE dataset",
      "doi": "10.1109/MMSP.1999.793887"
    },
    {
      "citation_id": "9",
      "title": "Speech emotion recognition",
      "authors": [
        "S Lalitha",
        "A Madhavan",
        "B Bhushan",
        "S Saketh"
      ],
      "year": "2014",
      "venue": "2014 International Conference on Advances in Electronics Computers and Communications",
      "doi": "10.1109/ICAECC.2014.7002390"
    },
    {
      "citation_id": "10",
      "title": "Speaker Independent Speech Emotion Recognition by Ensemble Classification",
      "authors": [
        "B Schuller",
        "S Reiter",
        "R Muller",
        "M Al-Hames",
        "M Lang",
        "G Rigoll"
      ],
      "year": "2005",
      "venue": "2005 IEEE International Conference on Multimedia and Expo",
      "doi": "10.1109/ICME.2005.1521560"
    },
    {
      "citation_id": "11",
      "title": "Acoustic feature selection for automatic emotion recognition from speech",
      "authors": [
        "J Rong",
        "G Li",
        "Y Chen"
      ],
      "year": "2009",
      "venue": "Information Processing & Management",
      "doi": "10.1016/j.ipm.2008.09.003"
    },
    {
      "citation_id": "12",
      "title": "Emotion recognition from speech using global and local prosodic features",
      "authors": [
        "K Rao",
        "S Koolagudi",
        "R Vempada"
      ],
      "year": "2012",
      "venue": "International Journal of Speech Technology",
      "doi": "10.1007/s10772-012-9172-2"
    },
    {
      "citation_id": "13",
      "title": "Speech emotion recognition using hidden Markov models",
      "authors": [
        "T Nwe",
        "S Foo",
        "L Silva"
      ],
      "year": "2003",
      "venue": "Speech Communication",
      "doi": "10.1016/s0167-6393(03)00099-2"
    },
    {
      "citation_id": "14",
      "title": "Automatic speech emotion recognition using modulation spectral features",
      "authors": [
        "S Wu",
        "T Falk",
        "W Chan"
      ],
      "year": "2011",
      "venue": "Speech Communication",
      "doi": "10.1016/j.specom.2010.08.013"
    },
    {
      "citation_id": "15",
      "title": "Evaluating deep learning architectures for Speech Emotion Recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks",
      "doi": "10.1016/j.neunet.2017.02.013"
    },
    {
      "citation_id": "16",
      "title": "Representation Learning: A Review and New Perspectives",
      "authors": [
        "Y Bengio",
        "A Courville",
        "P Vincent"
      ],
      "year": "2013",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
      "doi": "10.1109/tpami.2013.50"
    },
    {
      "citation_id": "17",
      "title": "Deep neural networks for acoustic emotion recognition: Raising the benchmarks",
      "authors": [
        "A Stuhlsatz",
        "C Meyer",
        "F Eyben",
        "T Zielke",
        "G Meier",
        "B Schuller"
      ],
      "year": "2011",
      "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing",
      "doi": "10.1109/ICASSP.2011.5947651"
    },
    {
      "citation_id": "18",
      "title": "Bimodal Emotion Recognition using Speech and Physiological Changes",
      "authors": [
        "J Kim"
      ],
      "year": "2007",
      "venue": "Robust Speech Recognition and Understanding",
      "doi": "10.5772/4754"
    },
    {
      "citation_id": "19",
      "title": "Learning Salient Features for Speech Emotion Recognition Using Convolutional Neural Networks",
      "authors": [
        "Q Mao",
        "M Dong",
        "Z Huang",
        "Y Zhan"
      ],
      "year": "2014",
      "venue": "IEEE Transactions on Multimedia",
      "doi": "10.1109/TMM.2014.2360798"
    },
    {
      "citation_id": "20",
      "title": "Speech emotion recognition based on an improved brain emotion learning model",
      "authors": [
        "Z Liu",
        "Q Xie",
        "M Wu",
        "W Cao",
        "Y Mei",
        "J Mao"
      ],
      "year": "2018",
      "venue": "Neurocomputing",
      "doi": "Available:10.1016/j.neucom.2018.05.005"
    },
    {
      "citation_id": "21",
      "title": "Meta Transfer Learning for Facial Emotion Recognition",
      "authors": [
        "D Nguyen",
        "K Nguyen",
        "S Sridharan",
        "I Abbasnejad",
        "D Dean",
        "C Fookes"
      ],
      "year": "2018",
      "venue": "24th International Conference on Pattern Recognition (ICPR)",
      "doi": "Available:10.1109/icpr.2018.8545411"
    },
    {
      "citation_id": "22",
      "title": "3D CNN-Based Speech Emotion Recognition Using K-Means Clustering and Spectrograms",
      "authors": [
        "N Hajarolasvadi",
        "H Demirel"
      ],
      "year": "2019",
      "venue": "Entropy",
      "doi": "10.3390/e21050479"
    },
    {
      "citation_id": "23",
      "title": "Semi-supervised Model for Emotion Recognition in Speech",
      "authors": [
        "I Pereira",
        "D Santos",
        "A Maciel",
        "P Barros"
      ],
      "year": "2018",
      "venue": "Artificial Neural Networks and Machine Learning ICANN 2018",
      "doi": "Available:10.1007/978-3-030-01418-677"
    },
    {
      "citation_id": "24",
      "title": "Emotion Classification of Audio Signals Using Ensemble of Support Vector Machines",
      "authors": [
        "T Danisman",
        "A Alpkocak"
      ],
      "year": "2020",
      "venue": "Lecture Notes in Computer Science",
      "doi": "Available:10.1007/978-3-540-69369-723"
    },
    {
      "citation_id": "25",
      "title": "Spoken emotion recognition using hierarchical classifiers",
      "authors": [
        "E Albornoz",
        "D Milone",
        "H Rufiner"
      ],
      "year": "2011",
      "venue": "Computer Speech & Language",
      "doi": "10.1016/j.csl.2010.10.001"
    },
    {
      "citation_id": "26",
      "title": "Automatic Speech Emotion Recognition using Support Vector Machine",
      "authors": [
        "P Shen",
        "Z Changjun",
        "X Chen"
      ],
      "year": "2011",
      "venue": "Proceedings of 2011 International Conference on Electronic & Mechanical Engineering and Information Technology",
      "doi": "10.1109/EMEIT.2011.6023178"
    },
    {
      "citation_id": "27",
      "title": "Speech Emotion Recognition Using Fourier Parameters",
      "authors": [
        "K Wang",
        "N An",
        "B Li",
        "Y Zhang",
        "L Li"
      ],
      "year": "2015",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/TAFFC.2015.2392101"
    },
    {
      "citation_id": "28",
      "title": "Emotion Recognition of Affective Speech Based on Multiple Classifiers Using Acoustic-Prosodic Information and Semantic Labels",
      "authors": [
        "C Wu",
        "W Liang"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Affective Computing",
      "doi": "10.1109/T-AFFC.2010.16"
    },
    {
      "citation_id": "29",
      "title": "Hidden Markov model-based speech emotion recognition",
      "authors": [
        "B Schuller",
        "G Rigoll",
        "M Lang"
      ],
      "year": "2003",
      "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing",
      "doi": "10.1109/ICASSP.2003.1202279"
    },
    {
      "citation_id": "30",
      "title": "Application of Speech Emotion Recognition in Intelligent Household Robot",
      "authors": [
        "X Huahu",
        "G Jue",
        "Y Jian"
      ],
      "year": "2010",
      "venue": "2010 International Conference on Artificial Intelligence and Computational Intelligence",
      "doi": "10.1109/AICI.2010.118"
    },
    {
      "citation_id": "31",
      "title": "Speech emotion recognition",
      "authors": [
        "B Schuller"
      ],
      "year": "2018",
      "venue": "Communications of the ACM"
    },
    {
      "citation_id": "32",
      "title": "Emotion recognition techniques using physiological signals and video games Systematic review",
      "authors": [
        "M Callejas-Cuervo",
        "L Martnez-Tejada",
        "A Alarcn-Aldana"
      ],
      "year": "2017",
      "venue": "Revista Facultad de Ingeniera",
      "doi": "10.19053/01211129.v26.n46.2017.7310"
    },
    {
      "citation_id": "33",
      "title": "Detection of Clinical Depression in Adolescents Speech During Family Interactions",
      "authors": [
        "L Low",
        "M Maddage",
        "M Lech",
        "L Sheeber",
        "N Allen"
      ],
      "year": "2011",
      "venue": "IEEE Transactions on Biomedical Engineering",
      "doi": "10.1109/tbme.2010.2091640"
    },
    {
      "citation_id": "34",
      "title": "Speech emotion recognition based on feature selection and extreme learning machine decision tree",
      "authors": [
        "Z Liu",
        "M Wu",
        "W Cao",
        "J Mao",
        "J Xu",
        "G Tan"
      ],
      "year": "2018",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "35",
      "title": "Fast and accurate sequential floating forward feature selection with the Bayes classifier applied to speech emotion recognition. Signal processing",
      "authors": [
        "D Ververidis",
        "C Kotropoulos"
      ],
      "year": "2008",
      "venue": "Fast and accurate sequential floating forward feature selection with the Bayes classifier applied to speech emotion recognition. Signal processing"
    },
    {
      "citation_id": "36",
      "title": "A novel feature selection method for speech emotion recognition. Applied Acoustics",
      "year": "2019",
      "venue": "A novel feature selection method for speech emotion recognition. Applied Acoustics"
    },
    {
      "citation_id": "37",
      "title": "Modular neural-SVM scheme for speech emotion recognition using ANOVA feature selection method. Neural Computing and Applications",
      "authors": [
        "M Sheikhan",
        "M Bejani",
        "D Gharavian"
      ],
      "year": "2013",
      "venue": "Jul"
    },
    {
      "citation_id": "38",
      "title": "Speech emotion recognition using FCBF feature selection method and GA-optimized fuzzy ARTMAP neural network. Neural Computing and Applications",
      "authors": [
        "D Gharavian",
        "M Sheikhan",
        "A Nazerieh",
        "S Garoucy"
      ],
      "year": "2012",
      "venue": "Speech emotion recognition using FCBF feature selection method and GA-optimized fuzzy ARTMAP neural network. Neural Computing and Applications"
    },
    {
      "citation_id": "39",
      "title": "Particle swarm optimization based feature enhancement and feature selection for improved emotion recognition in speech and glottal signals",
      "authors": [
        "H Muthusamy",
        "K Polat",
        "S Yaacob"
      ],
      "year": "2015",
      "venue": "PloS one"
    },
    {
      "citation_id": "40",
      "title": "Mel frequency cepstral coefficients for music modeling",
      "authors": [
        "B Logan"
      ],
      "year": "2000",
      "venue": "InIsmir"
    },
    {
      "citation_id": "41",
      "title": "Speech analysis and synthesis by linear prediction of the speech wave",
      "authors": [
        "B Atal",
        "S Hanauer"
      ],
      "year": "1971",
      "venue": "The journal of the acoustical society of America"
    },
    {
      "citation_id": "42",
      "title": "Effectiveness of linear prediction characteristics of the speech wave for automatic speaker identification and verification",
      "authors": [
        "B Atal"
      ],
      "year": "1974",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "43",
      "title": "Perceptual linear predictive (PLP) analysis of speech",
      "authors": [
        "H Hermansky"
      ],
      "year": "1990",
      "venue": "Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "44",
      "title": "Genetic algorithms. Scientific american",
      "authors": [
        "J Holland"
      ],
      "year": "1992",
      "venue": "Genetic algorithms. Scientific american"
    },
    {
      "citation_id": "45",
      "title": "Particle swarm optimization",
      "authors": [
        "J Kennedy",
        "R Eberhart"
      ],
      "year": "1995",
      "venue": "InProceedings of ICNN'95-International Conference on Neural Networks"
    },
    {
      "citation_id": "46",
      "title": "Binary grey wolf optimization approaches for feature selection",
      "authors": [
        "E Emary",
        "H Zawbaa",
        "A Hassanien"
      ],
      "year": "2016",
      "venue": "Neurocomputing"
    }
  ]
}