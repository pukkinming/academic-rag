{
  "paper_id": "2407.19809v1",
  "title": "Twins-Painvit: Towards A Modality-Agnostic Vision Transformer Framework For Multimodal Automatic Pain Assessment Using Facial Videos And Fnirs",
  "published": "2024-07-29T09:02:43Z",
  "authors": [
    "Stefanos Gkikas",
    "Manolis Tsiknakis"
  ],
  "keywords": [
    "Pain recognition",
    "deep learning",
    "transformers",
    "multi-task learning",
    "data fusion",
    "biosignals",
    "waveforms"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Automatic pain assessment plays a critical role for advancing healthcare and optimizing pain management strategies. This study has been submitted to the First Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN). The proposed multimodal framework utilizes facial videos and fNIRS and presents a modality-agnostic approach, alleviating the need for domain-specific models. Employing a dual ViT configuration and adopting waveform representations for the fNIRS, as well as for the extracted embeddings from the two modalities, demonstrate the efficacy of the proposed method, achieving an accuracy of 46.76% in the multilevel pain assessment task.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "The International Association for the Study of Pain (IASP) defines pain as \"an unpleasant sensory and emotional experience associated with actual or potential tissue damage, or described in terms of such damage\"  [1] , marking a recent update to the definition. Pain significantly affects individuals and societal structures, with people of all ages experiencing it due to accidents, diseases, or medical treatments-making it the primary reason for medical consultations. Acute and chronic pain pose clinical, economic, and social difficulties. Beyond its direct effects on a person's daily life, pain is associated with various negative consequences, such as increased opioid use, substance abuse, addiction, declining social interactions, and mental health problems  [2] . Effective pain assessment is essential for early diagnosis, disease progression monitoring, and evaluation of treatment efficacy, especially in managing chronic pain  [3] . Additionally, adjusting pain intensity is crucial in therapy approaches like myofascial therapy, where a practitioner, such as a physiotherapist, externally induces the pain, and understanding the patient's pain level is vital  [4] . Pain evaluation is crucial yet challenging for healthcare professionals  [5] , especially when dealing with patients who cannot communicate verbally. This challenge is further amplified in elderly patients who may be less expressive or hesitant to discuss their pain  [6] . Moreover, comprehensive research  [7] -  [9]  highlights significant differences in pain expression across different genders and age groups, adding complexity to the pain assessment process. Pain assessment encompasses a variety of approaches, from self-reporting using detailed rating scales and questionnaires, considered the gold standard, to observing behavioral indicators like facial expressions, vocalizations, and bodily movements  [10] . It also includes analyzing physiological responses such as electrocardiography and skin conductance, which offer essential insights into the physical manifestations of pain  [3] . Moreover, functional near-infrared spectroscopy (fNIRS) is a promising method for measuring pain-related physiological responses. This non-invasive neuroimaging technique evaluates brain activity by tracking cerebral hemodynamics and oxygenation changes. Specifically, fNIRS simultaneously records changes in the cortical concentrations of oxygenated hemoglobin (HbO) and deoxygenated hemoglobin (HbR), offering critical insights into brain function  [11] . Furthermore, fNIRS studies have demonstrated that noxious stimuli initiate changes in oxygenation levels across various cortical regions in healthy and diseased subjects  [12] .\n\nThis study introduces a modality-agnostic multimodal framework that utilizes videos and fNIRS. The proposed pipeline is based on a dual Vision Transformer (ViT) configuration, eliminating the need for domain-specific architectures or extensive feature engineering for each modality by interpreting the inputs as unified images through 2D waveform representation.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "Recent developments have introduced various innovative methods for assessing pain levels from video data. The authors in  [13]  developed a temporal convolutional network (TCN) and utilized the HSV color model, arguing that it offers more advantages for tasks related to human visual perception, such as skin pixel detection and multi-face detection. The authors in  [14]  combined the VGG-Face CNN with a 3-layer LSTM to extract spatio-temporal features from grayscale images, applying zero-phase component analysis for enhancement. Conversely, in  [15] , principal component analysis was employed to reduce dimensionality. Finally, in  [16] , the authors introduced a hybrid approach that combines a vision transformer for spatial feature extraction with a standard transformer for temporal analysis, achieving high accuracy. Several studies in pain research have employed fNIRS in conjunction with machine learning techniques to extract relevant features and evaluate pain conditions effectively. In  [17] , combining a bag-of-words (BoW) approach with a K-NN classifier to analyze timefrequency features yields better results than analyzing time or frequency features in isolation. Conversely, the study  [18]  demonstrated that the best results were achieved by combining time and frequency domain features with a Gaussian SVM, while Rojas et al.  [19]  utilized the raw fNIRS with a twolayer BiLSTM achieving 90.60% accuracy in a multi-class classification task. Finally, the authors in  [20]  developed a hybrid architecture of CNN and LSTM model to capture spatio-temporal features from the fNIRS, achieving high performances. Regarding the multimodal approaches, Gkikas et al.  [21]  introduced an efficient transformer-based multimodal framework that leverages facial videos and heart rate signals, demonstrating that integrating behavioral and physiological modalities enhances pain estimation performance. In  [22] , statistical features were extracted from electrodermal activity, respiration rate, and photoplethysmography, and a joint mutual information process was implemented to assess the intensity and locate the origin of pain.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Iii. Methodology",
      "text": "This section describes the pipeline of the proposed multimodal automatic pain assessment framework, the architecture of the models, the pre-processing methods, the pre-training strategy, and the augmentation techniques.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "A. Framework Architecture",
      "text": "The proposed framework, Twins-PainViT, consists of two models: PainViT-1 and PainViT-2. Both models are identical in architecture and parameters and follow the same pre-training procedure, which will be detailed in Section III-D. PainViT-1 is provided with the corresponding video frames and the visualized fNIRS channels and functions as an embedding extractor. PainViT-2 receives the visual representation of the embeddings and completes the final pain assessment.\n\n1) PainViT: Vision Transformers (ViTs)  [23]  have emerged as a new paradigm in computer vision tasks due to their performance. However, despite their impressive efficacy, transformerbased models face challenges in scaling with larger input sizes, leading to substantial computational costs. This inefficiency primarily derives from the element-wise operations in the multihead self-attention mechanism. Numerous efforts have been made to enhance the efficiency and reduce the complexity of transformer-based architectures by modifying the self-attention module or the model's overall structure  [24]    [25] . Our approach is founded on the principles of  [26]  introducing the hierarchical architectures into the vision transformers and  [27]  proposing mechanisms that increase efficiency and speed.\n\n2) PainViT-block: Each block features two components: the Token-Mixer and the Cascaded-Attention. It is structured with an Cascaded-Attention module at the core and a Token-Mixer module positioned preceding and following it. For every input image I, overlapping patch embedding is applied, producing 16 ˆ16 patches, each projected into a token with a dimension of d.\n\na) Token-Mixer: To enhance the incorporation of local structural information, the token T is processed through a depthwise convolution layer:\n\nwhere Y c is the output of the depthwise convolution for channel c of the token T c . K c is the convolutional kernel specifically for channel c, T c is the c-th channel of the token T , and b c is the bias term added to the convolution output of channel c.\n\nThe symbol ˚denotes the convolution operation. Following the depthwise convolution, batch normalization is applied to the output:\n\nwhere Z c is the batch-normalized output for channel c of the token T . γ c and β c are learnable parameters specific to channel c that scale and shift the normalized data. µ B is the batch mean of Y c , σ 2 B is the batch variance of Y c , and ϵ is a small constant added for numerical stability to avoid division by zero. Next, a feed-forward network (FFN) facilitates more efficient communication between different feature channels:\n\nwhere Φ F pZ c q is the output of the feed-forward network for the input Z c . W 1 and W 2 are the weight matrices of the first and second linear layers; b 1 and b 2 are the bias terms for the first and second linear layers, respectively, and ReLU is the activation function. b) Cascaded-Attention: Regarding the attention mechanism, there is a single self-attention layer. For every input embedding:\n\nwhere X i is the full input embedding for the i-th PainViT-block.\n\nMore specifically, the Cascaded-Attention module employs a cascaded mechanism that partitions the full input embedding into smaller segments, each directed to a distinct attention head. This approach allows the computation to be distributed across the heads, enhancing efficiency by avoiding long input embeddings. The attention is described as:\n\nwhere each j-th head calculates the self-attention for X i,j , which represents the j-th segment of the full input embedding X i , structured as rX i1 , X i2 , . . . , X ih s where 1 ď j ď h and h denotes the total number of heads. The projection layers    and W  V ij map each segment input embedding into distinct subspaces. Finally, W P i is a linear layer that reassembles the concatenated output embeddings from all heads back to a dimensionality that aligns with the original input. Furthermore, the cascaded architecture enhances the learning of richer embedding representations for Q, K, and V layers. This is achieved by adding the output from each head to the input of the subsequent head, enabling the accumulation of information throughout the process. Specifically:\n\nHere, X 1 ij represents the addition of the j-th input segment X ij and the output Xipj´1q from the pj ´1q-th head. The summation replaces X ij as the new input embedding for the jth head in the self-attention computation. Finally, it is noted that depthwise convolution is applied to each Q in every attention head. This enables the subsequent self-attention process to capture global representations and local information.\n\nThe framework comprises three PainViT-blocks, each with 1, 3, and 4 depths, respectively. This hierarchical structure features a progressive reduction in the number of tokens by subsampling the resolution by a factor of 2ˆat each stage. Correspondingly, the architecture facilitates the extraction of embeddings with dimensions d across the blocks, specifically 192, 288, and 500. Additionally, the multihead self-attention mechanism within each block employs 3, 3, and 4 heads, respectively. Fig.  1(a-d ) illustrates the PainViT architecture and its fundamental building blocks, while Table  I  presents the number of parameters and the computational cost in terms of floating-point operations (FLOPS).",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Embedding Extraction & Fusion",
      "text": "For each frame of a video, V \" rv 1 , v 2 , . . . , v n s, PainViT-1 extracts a corresponding embedding. These embeddings are aggregated to form a unified feature representation of the video. Similarly, for each channel of an fNIRS signal, C \" rc 1 , c 2 , . . . , c m s, PainViT-1 extracts embeddings, which are subsequently aggregated to create a representation of the fNIRS signal. This process can be described as:\n\nwhere E V and E C are the corresponding embedding representations for the video and fNIRs. Following the extraction of embeddings, E V and E C are visualized as waveform diagrams.\n\nThe waveform from each modality-video and fNIRS-is merged into a single image with a resolution of 224 ˆ224. This unified visual representation is fed into PainViT-2 for the final pain assessment. (Fig.  1e ) presents a high-level overview of the multimodal proposed pipeline.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "C. Pre-Processing",
      "text": "The pre-processing involves face detection for the corresponding frames of the videos and the generation of waveform diagrams from the original fNIRS. The MTCNN face detector  [28]  was utilized, employing a series of cascaded convolutional neural networks to predict both faces and facial landmarks. The resolution of the detected faces was set at 224 ˆ224 pixels. All fNIRS channels are used to generate waveform diagrams. A waveform diagram visually represents the shape and form of a signal wave as it progresses over time, illustrating the signal's amplitude, frequency, and phase. This method provides the simplest and most direct way to visualize a signal, as it does not necessitate any transformations or additional computations such as those involved in creating spectrograms, scalograms, or recurrence plots. Similarly, the embeddings extracted from PainViT-1 are visualized using the same method. Although these embeddings are not signals, the 1D vectors can still be plotted in a 2D space for analysis or utilization from the deep-learning vision models. All waveform diagrams generated from the fNIRS data and embeddings are formatted as images with a 224 ˆ224 pixels resolution. Fig.  2  depicts waveform representations of channel-specific fNIRS signal, an embedding extracted from a video, and an embedding derived from a channel-specific fNIRS sample.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Pre-Training",
      "text": "Before the automatic pain assessment training process, the Twins-PainViT models were pre-trained using a multi-task learning strategy. Four datasets, which include images for emotion assessment tasks, were employed. The AffectNet  [29]  and RAF-DB basic  [30]  datasets provided facial images for recognizing basic emotions, while the Compound FEE-DB  [31]  and RAF-DB compound  [30]  datasets were used for identifying complex emotions. Additionally, five datasets containing biosignals were also utilized. EEG-BST-SZ  [32]  comprises electroencephalograms used for schizophrenia recognition, and Silent-EMG  [33]  includes electromyograms aimed at identifying the location of origin of the EMGs (such as throat and midjaw). Furthermore, electrocardiogram, electromyogram, and galvanic skin response samples from the BioVid  [34]  dataset were employed for the pain assessment task. All the biosignals were utilized in the form of waveform representations, as described in III-C. The multi-task learning process is described as: where L Si is the loss associated with each specific task corresponding to different datasets, and w i represents the learned weights that guide the learning process in minimizing the overall loss L total , taking into account all the individual losses. Table  II  details the datasets used in the pre-training process.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "E. Augmentation Methods & Regularization",
      "text": "Several augmentation methods have been utilized for training the proposed framework. Regarding the pre-training process, RandAugment  [35]  and TrivialAugment  [36]  were adopted.  Additionally, auxiliary noise from a uniform distribution was employed, along with MaskOut, a custom-implemented technique masking out random square sections of input images. For the automatic pain assessment task, AugMix  [37]  is employed in addition to RandAugment, TrivialAugment, and MaskOut as augmentation methods. Furthermore, Label Smoothing  [38]  and DropOut  [39]  were employed as regularization techniques.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iv. Experimental Evaluation & Results",
      "text": "This study utilizes the dataset provided by the challenge organizers  [22] ,  [40] , comprising facial videos and fNIRS data from 65 participants. The dataset includes 41 training, 12 validation, and 12 testing subjects recorded at the Human-Machine Interface Laboratory, University of Canberra, Australia. Electrodes for transcutaneous electrical nerve stimulation, serving as pain stimuli, were placed on the inner forearm and the back of the right hand. Pain threshold, defined as the lowest stimulus intensity at which stimulation becomes painful (low pain), and pain tolerance, defined as the highest intensity of pain a person can endure before it becomes intolerable (high pain), were measured. For the fNIRS, 24 channels each for HbO and HbR were utilized, alongside all 30 frames per video available. The results presented in this study focus on the validation part of the dataset, structured in a multi-level classification setting (No Pain, Low Pain, and High Pain). Table  III  outlines the training framework details for the automatic pain assessment. We note that numerous experiments were conducted across each modality and their fusion; however, only the most successful results are presented in the subsequent sections and corresponding tables.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "A. Facial Videos",
      "text": "In the context of facial videos, two fusion embedding techniques were applied: the Addition method aggregating the 30 embeddings into a single fused vector with dimension d \" 500 and the Concatenation method combining the embeddings to form a vector with d \" 30 ˆ500 \" 15, 000. Utilizing the Addition method, we observed an initial accuracy of 41.90% for the multi-class classification task with augmentation and regularization levels (0.1 for AugMix, Rand, Trivial, and 0.1|3 for MaskOut, and 0.5 for DropOut). Increasing the augmentation intensities to 0.5 and MaskOut to 0.7|3 raised the accuracy to 44.91%. Applying MaskOut to 0.7|3 and raising DropOut from 0.5 to 0.6 achieved 42.36%. Increasing DropOut to 0.7 and AugMix, Rand, and Trivial to 0.9 improved accuracy to 43.52%. Table  IV  presents the results. Utilizing the Concatenation method, and initial settings with a uniform augmentation probability of 0.3 across AugMix, Rand, Trivial, and 0.3|3 for MaskOut and 0.1 LS and 0.5 DropOut yielded a 40.28% accuracy. Increasing MaskOut to 0.8|5 while maintaining other augmentations at 0.5 improved accuracy to 41.44%. The highest accuracy of 43.75% was achieved with 0.9 across all augmentations except MaskOut, which was adjusted to 0.6|3, and high regularization (LS 0.4, DropOut 0.5). The corresponding results are summarized in Table  V .",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Fnirs",
      "text": "Similar to facial videos, the methods of Addition and Concatenation were applied. From the original 24 channels 2   VIII ). In the HbO & Concatenation method, the augmentation methods with a probability of 0.1 started with an accuracy of 42.13%. The peak accuracy of 44.44% was achieved with a balanced augmentation at 0.9 and MaskOut at 0.7|3, indicating effectiveness of increased overall applied augmentation combined with high regularization. Subsequent adjustments slightly lowered accuracy, underscoring the importance of optimal augmentation settings (refer to Table  IX ).Generally, enhanced performance is observed with HbO compared to HbR, as also noted in other studies  [41] , due to its superior signal-to-noise ratio. The combined HbR and HbO using the Addition method initially showed an accuracy of 42.82% with all augmentations at zero except for MaskOut at 0.7|3. Increasing AugMix, Rand, and Trivial to 0.5 while elevating MaskOut to 0.7|7 marginally improved accuracy to 43.29%. Maintaining augmentations but adjusting MaskOut back to 0.7|3 with a slight increase in LS resulted in a slight decrease in accuracy to 42.59%. However, further increasing all augmentations to 0.9 and LS to 0.3 while maintaining MaskOut at 0.7|3 maximized the accuracy to 43.75%. Reducing DropOut to 0.1 in the final configuration slightly reduced accuracy to 43.06%, emphasizing the importance of optimizing regularization alongside augmentation strategies for achieving the best possible results (refer to Table  X ).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Fusion",
      "text": "In this section, we describe the fusion of facial videos and fNIRS. The HbO was utilized solely for the experiments since      it demonstrated superior performance to the HbR. Two methods were developed for data fusion: the previously described Addition method, aggregating embeddings from video frames and fNIRS channels and then combines them, and Single-Diagram method, where aggregated embeddings from both modalities are concurrently visualized in the same image. For the Addition method and initial configurations with moderate augmentation levels (0.5 for AugMix, Rand, Trivial) and MaskOut at 0.4|5 achieved a 42.36% accuracy. Increasing augmentation levels to 0.9 and adjusting regularization parameters (LS up to 0.4 and DropOut up to 0.9) improved the accuracy, peaking at 43.75% (refer to Table  XI ). For the Single Diagram method, accuracy improvements were observed, as shown in Table  XII . Starting with lower MaskOut levels at 0.3|5 and standard augmentation probabilities (0.5), the accuracy was 45.83%. Utilizing augmentation probabilities to 0.9 and MaskOut adjustments to 0.7|3 significantly improved performance, achieving a high of 46.76%.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "V. Interpretation & Comparison",
      "text": "Regarding the framework's interpretation, attention maps were generated from the last layer of PainViT-2, which processed the unified image visualizing both the video and HbO embedding waveforms. This layer contains 500 neurons, each contributing uniquely to and attending to the input. Fig.  3  illustrates four examples where certain neurons focus on the video embedding waveform, others on the HbO, and some attend to both waveforms, emphasizing different parts and details. Table XIV compares the proposed pipeline and the baseline results provided by the challenge organizers. The videobased approach using the Addition method outperformed the  baseline by 4.91%. Using the HbO with the Addition method, the improvement was lesser, at 1.48%. Finally, the modality fusion using the Single Diagram approach resulted in a more significant improvement of 6.56%.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "This study outlines our contribution to the First Multimodal Sensing Grand Challenge for Next-Gen Pain Assessment (AI4PAIN), employing facial videos and fNIRS through a modality-agnostic approach. Twins-PainViT was introduced, a framework founded on a dual configuration of vision transformers, pre-trained on the plethora of datasets in a multitask learning setting. Furthermore, a fundamental component of the proposed pipeline was the waveform representation, which was applied to the original fNIRS data and the learned embeddings from both modalities. This approach of extracting embeddings and integrating them into a single image diagram effectively and efficiently eliminated the need for dedicated domain-specific models for each modality. The conducted experiments showcased high performances for unimodal and multimodal settings, surpassing the provided baseline results. Additionally, the interpretation of Pain-ViT-2 through the creation of attention maps for the image diagrams showed that specific neurons target particular modalities or distinct aspects of them, indicating a holistic consideration in the analysis process. We suggest that future research employ multimodal approaches, which have proven to be the most effective method for assessing pain in real-world settings. It is also essential to develop methods for interpreting data, particularly to facilitate the integration of these frameworks into clinical practice.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Ethical Impact Statement",
      "text": "This research employed the AI4PAIN dataset  [22] ,  [40]  provided by the challenge organizers to evaluate the proposed methods. The participants did not report any prior history of neurological or psychiatric disorders, current unstable medical conditions, chronic pain, or regular medication use at the time of testing. Upon arrival, they received a detailed explanation of the experimental procedures. Written informed consent was obtained before the experiment began. The experimental procedures involving human subjects described in the original paper were approved by the University of Canberra's Human Ethics Committee (approval number: 11837). This study presents a pain assessment framework for continuous patient monitoring and minimizing human biases. However, it is crucial to acknowledge that deploying this framework in real-world clinical environments may pose challenges, requiring additional experiments and thorough validation through clinical trials prior to final deployment. Furthermore, the sole facial image featured in this study is a designed illustration and does not represent an actual person.\n\nIn addition, several datasets were utilized to pretrain the proposed pain assessment framework. The AffectNet  [29]  dataset is compiled using search engine queries. The original paper does not explicitly detail ethical compliance measures. The RAF-DB  [30]  dataset was compiled using the Flickr image hosting service. Although Flickr hosts both public and privately shared images, the authors do not explicitly mention the type of the downloaded images. The original paper of Compound FEE-DB  [31]  does not mention ethical compliance measures, but only that the subjects were recruited from the Ohio State University area and received a monetary reward for participating. The EEG-BST-SZ  [32]  dataset was recorded with assistance from trained research assistants, psychiatrists, or clinical psychologists who conducted all interviews. The study received approval from the University of California at San Francisco Institutional Review Board and the San Francisco Veterans Affairs Medical Center. The original paper on the Silent-EMG  [33]  dataset does not explicitly mention adherence to ethical compliance measures. However, it is noted that the data recorded came solely from one individual, also one of the authors and creators of the dataset. The data from the BioVid Heat Pain Database  [34]  were recorded according to the ethical guidelines of Helsinki (ethics committee: 196/10-UBB/bal).",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: e) presents a high-level overview",
      "page": 3
    },
    {
      "caption": "Figure 2: depicts waveform",
      "page": 3
    },
    {
      "caption": "Figure 1: PainViT: (a) Hierarchical organization of the PainViT blocks, each with different depths, illustrating the reduction",
      "page": 4
    },
    {
      "caption": "Figure 2: Waveform diagrams representing different data modali-",
      "page": 4
    },
    {
      "caption": "Figure 3: illustrates four examples where certain neurons focus on the",
      "page": 7
    },
    {
      "caption": "Figure 3: Attention maps from the PainViT–2.",
      "page": 7
    },
    {
      "caption": "Figure 4: Additional attention maps from the PainViT–2.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "gkikas@ics.forth.gr": "Abstract—Automatic pain assessment plays a critical role for",
          "tsiknaki@ics.forth.gr": "to discuss their pain [6]. Moreover, comprehensive research [7]–"
        },
        {
          "gkikas@ics.forth.gr": "advancing healthcare and optimizing pain management strategies.",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "[9] highlights significant differences in pain expression across"
        },
        {
          "gkikas@ics.forth.gr": "This study has been submitted to the First Multimodal Sensing",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "different genders and age groups, adding complexity to the pain"
        },
        {
          "gkikas@ics.forth.gr": "Grand Challenge for Next-Gen Pain Assessment\n(AI4PAIN). The",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "assessment process. Pain assessment encompasses a variety"
        },
        {
          "gkikas@ics.forth.gr": "proposed multimodal\nframework utilizes facial videos and fNIRS",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "and presents a modality-agnostic approach, alleviating the need for",
          "tsiknaki@ics.forth.gr": "of approaches,\nfrom self-reporting using detailed rating scales"
        },
        {
          "gkikas@ics.forth.gr": "domain-specific models. Employing a dual ViT configuration and",
          "tsiknaki@ics.forth.gr": "and questionnaires, considered the gold standard,\nto observing"
        },
        {
          "gkikas@ics.forth.gr": "adopting waveform representations for the fNIRS, as well as for",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "behavioral\nindicators like facial expressions, vocalizations, and"
        },
        {
          "gkikas@ics.forth.gr": "the extracted embeddings from the two modalities, demonstrate",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "bodily movements [10]. It also includes analyzing physiological"
        },
        {
          "gkikas@ics.forth.gr": "the efficacy of\nthe proposed method, achieving an accuracy of",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "responses such as electrocardiography and skin conductance,"
        },
        {
          "gkikas@ics.forth.gr": "46.76% in the multilevel pain assessment\ntask.",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "Index Terms—Pain recognition, deep learning,\ntransformers,",
          "tsiknaki@ics.forth.gr": "which offer essential\ninsights into the physical manifestations"
        },
        {
          "gkikas@ics.forth.gr": "multi-task learning, data fusion, biosignals, waveforms",
          "tsiknaki@ics.forth.gr": "of pain [3]. Moreover,\nfunctional near-infrared spectroscopy"
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "(fNIRS) is a promising method for measuring pain-related phys-"
        },
        {
          "gkikas@ics.forth.gr": "I.\nINTRODUCTION",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "iological\nresponses. This non-invasive neuroimaging technique"
        },
        {
          "gkikas@ics.forth.gr": "The International Association for\nthe Study of Pain (IASP)",
          "tsiknaki@ics.forth.gr": "evaluates brain activity by tracking cerebral hemodynamics"
        },
        {
          "gkikas@ics.forth.gr": "defines pain as “an unpleasant sensory and emotional expe-",
          "tsiknaki@ics.forth.gr": "and oxygenation changes. Specifically,\nfNIRS simultaneously"
        },
        {
          "gkikas@ics.forth.gr": "rience associated with actual or potential\ntissue damage, or",
          "tsiknaki@ics.forth.gr": "records changes in the cortical concentrations of oxygenated"
        },
        {
          "gkikas@ics.forth.gr": "described in terms of\nsuch damage” [1], marking a recent",
          "tsiknaki@ics.forth.gr": "hemoglobin\n(HbO)\nand\ndeoxygenated\nhemoglobin\n(HbR),"
        },
        {
          "gkikas@ics.forth.gr": "update to the definition. Pain significantly affects individuals",
          "tsiknaki@ics.forth.gr": "offering critical\ninsights into brain function [11]. Furthermore,"
        },
        {
          "gkikas@ics.forth.gr": "and societal structures, with people of all ages experiencing it",
          "tsiknaki@ics.forth.gr": "fNIRS studies have demonstrated that noxious stimuli\ninitiate"
        },
        {
          "gkikas@ics.forth.gr": "due to accidents, diseases, or medical treatments—making it the",
          "tsiknaki@ics.forth.gr": "changes in oxygenation levels across various cortical\nregions"
        },
        {
          "gkikas@ics.forth.gr": "primary reason for medical consultations. Acute and chronic",
          "tsiknaki@ics.forth.gr": "in healthy and diseased subjects [12]."
        },
        {
          "gkikas@ics.forth.gr": "pain pose clinical, economic, and social difficulties. Beyond",
          "tsiknaki@ics.forth.gr": "This study introduces a modality-agnostic multimodal frame-"
        },
        {
          "gkikas@ics.forth.gr": "its direct effects on a person’s daily life, pain is associated",
          "tsiknaki@ics.forth.gr": "work that utilizes videos and fNIRS. The proposed pipeline"
        },
        {
          "gkikas@ics.forth.gr": "with various negative consequences, such as increased opioid",
          "tsiknaki@ics.forth.gr": "is based on a dual Vision Transformer\n(ViT) configuration,"
        },
        {
          "gkikas@ics.forth.gr": "use, substance abuse, addiction, declining social\ninteractions,",
          "tsiknaki@ics.forth.gr": "eliminating the need for domain-specific architectures or exten-"
        },
        {
          "gkikas@ics.forth.gr": "and mental health problems [2]. Effective pain assessment\nis",
          "tsiknaki@ics.forth.gr": "sive feature engineering for each modality by interpreting the"
        },
        {
          "gkikas@ics.forth.gr": "essential\nfor early diagnosis, disease progression monitoring,",
          "tsiknaki@ics.forth.gr": "inputs as unified images through 2D waveform representation."
        },
        {
          "gkikas@ics.forth.gr": "and evaluation of\ntreatment efficacy, especially in managing",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "II. RELATED WORK"
        },
        {
          "gkikas@ics.forth.gr": "chronic\npain\n[3]. Additionally,\nadjusting\npain\nintensity\nis",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "crucial\nin therapy approaches like myofascial\ntherapy, where",
          "tsiknaki@ics.forth.gr": "Recent developments have\nintroduced various\ninnovative"
        },
        {
          "gkikas@ics.forth.gr": "a practitioner,\nsuch as a physiotherapist, externally induces",
          "tsiknaki@ics.forth.gr": "methods for assessing pain levels from video data. The authors"
        },
        {
          "gkikas@ics.forth.gr": "the pain, and understanding the patient’s pain level\nis vital",
          "tsiknaki@ics.forth.gr": "in [13] developed a temporal convolutional network (TCN)"
        },
        {
          "gkikas@ics.forth.gr": "[4]. Pain evaluation is crucial yet challenging for healthcare",
          "tsiknaki@ics.forth.gr": "and utilized the HSV color model, arguing that\nit offers more"
        },
        {
          "gkikas@ics.forth.gr": "professionals [5], especially when dealing with patients who",
          "tsiknaki@ics.forth.gr": "advantages for\ntasks related to human visual perception, such"
        },
        {
          "gkikas@ics.forth.gr": "cannot communicate verbally. This challenge is further ampli-",
          "tsiknaki@ics.forth.gr": "as skin pixel detection and multi-face detection. The authors in"
        },
        {
          "gkikas@ics.forth.gr": "fied in elderly patients who may be less expressive or hesitant",
          "tsiknaki@ics.forth.gr": "[14]\ncombined the VGG-Face CNN with a 3-layer LSTM"
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "to\nextract\nspatio-temporal\nfeatures\nfrom grayscale\nimages,"
        },
        {
          "gkikas@ics.forth.gr": "the Computational\n:: Corresponding Author, ;: Affiliated Researcher at",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "",
          "tsiknaki@ics.forth.gr": "applying zero-phase component analysis for enhancement. Con-"
        },
        {
          "gkikas@ics.forth.gr": "Biomedicine Laboratory of\nthe Foundation for Research and Technology",
          "tsiknaki@ics.forth.gr": ""
        },
        {
          "gkikas@ics.forth.gr": "(FORTH), Heraklion, Greece",
          "tsiknaki@ics.forth.gr": "versely,\nin [15], principal component analysis was employed to"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "hybrid approach that combines a vision transformer\nfor spatial",
          "2) PainViT–block: Each block features two components:\nthe": "Token-Mixer and the Cascaded-Attention.\nIt\nis structured with"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "feature extraction with a standard transformer\nfor\ntemporal",
          "2) PainViT–block: Each block features two components:\nthe": "an Cascaded-Attention module at\nthe core and a Token-Mixer"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "analysis,\nachieving\nhigh\naccuracy. Several\nstudies\nin\npain",
          "2) PainViT–block: Each block features two components:\nthe": "module positioned preceding and following it. For every input"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "research have employed fNIRS in conjunction with machine",
          "2) PainViT–block: Each block features two components:\nthe": "image I, overlapping patch embedding is applied, producing"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "learning techniques to extract\nrelevant\nfeatures and evaluate",
          "2) PainViT–block: Each block features two components:\nthe": "16 ˆ 16 patches, each projected into a token with a dimension"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "pain conditions effectively.\nIn [17], combining a bag-of-words",
          "2) PainViT–block: Each block features two components:\nthe": "of d."
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "(BoW)\napproach with\na K-NN classifier\nto\nanalyze\ntime-",
          "2) PainViT–block: Each block features two components:\nthe": "a) Token-Mixer: To enhance the incorporation of\nlocal"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "frequency features yields better\nresults\nthan analyzing time",
          "2) PainViT–block: Each block features two components:\nthe": "structural\ninformation,\nthe\ntoken T\nis processed through a"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "or\nfrequency features in isolation. Conversely,\nthe study [18]",
          "2) PainViT–block: Each block features two components:\nthe": "depthwise convolution layer:"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "demonstrated that\nthe best results were achieved by combining",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "(1)\nYc “ Kc ˚ Tc ` bc,"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "time and frequency domain features with a Gaussian SVM,",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "while Rojas et al.\n[19] utilized the raw fNIRS with a two-",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "where Yc is the output of the depthwise convolution for channel"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "layer BiLSTM achieving 90.60% accuracy in a multi-class",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "c of\nis the convolutional kernel specifically\nthe token Tc. Kc"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "classification task. Finally,\nthe authors\nin [20] developed a",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "is the c-th channel of\nfor channel c, Tc\nthe token T , and bc"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "hybrid\narchitecture\nof CNN and LSTM model\nto\ncapture",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "is the bias term added to the convolution output of channel c."
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "spatio-temporal\nfeatures\nfrom the\nfNIRS,\nachieving\nhigh",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "The symbol ˚ denotes the convolution operation. Following"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "performances. Regarding the multimodal approaches, Gkikas et",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "the depthwise convolution, batch normalization is applied to"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "al.\n[21]\nintroduced an efficient\ntransformer-based multimodal",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "the output:\n˜\n¸"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "framework that\nleverages facial videos and heart\nrate signals,",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "Yc ´ µB"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "a"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "demonstrating that\nintegrating behavioral and physiological",
          "2) PainViT–block: Each block features two components:\nthe": "(2)\nZc “ γc\n` βc,"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "σ2"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "B ` ϵ"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "modalities\nenhances\npain\nestimation\nperformance.\nIn\n[22],",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "statistical\nfeatures were extracted from electrodermal activity,",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "is\nthe batch-normalized output\nfor\nchannel\nwhere Zc"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "respiration rate, and photoplethysmography, and a joint mutual",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "specific to\nthe token T . γc and βc are learnable parameters"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "information process was implemented to assess the intensity",
          "2) PainViT–block: Each block features two components:\nthe": ""
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "the normalized data. µB is the"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "and locate the origin of pain.",
          "2) PainViT–block: Each block features two components:\nthe": "batch mean of Yc, σ2\nB is the batch variance of Yc, and ϵ is a"
        },
        {
          "reduce dimensionality. Finally,\nin [16],\nthe authors introduced a": "",
          "2) PainViT–block: Each block features two components:\nthe": "small constant added for numerical stability to avoid division"
        }
      ],
      "page": 2
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "nents of\nthe proposed Twins-PainViT."
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "Module\nParams (M)\nFLOPS (G)"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "PainViT–1\n16.46\n0.59"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "PainViT–2\n16.46\n0.59"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "Total\n32.92\n1.18"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "W Q"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "and W V\neach\nsegment\ninput\nembedding\nij ,\nij map\nij , W K"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "into distinct\nsubspaces. Finally, W P\nis\na\nlinear\nlayer\nthat\ni"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "reassembles the concatenated output embeddings from all heads"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "back to a dimensionality that aligns with the original\ninput."
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "Furthermore,\nthe cascaded architecture enhances the learning"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "of\nricher embedding representations for Q, K, and V layers."
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "This is achieved by adding the output\nfrom each head to the"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "input of\nthe subsequent head, enabling the accumulation of"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "information throughout\nthe process. Specifically:"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "r"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "1 i\nX\n(7)\nXipj´1q.\nj “ Xij `"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "Here, X 1\nrepresents the addition of\nthe j-th input segment\nij"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "˜"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "and the output\nfrom the pj ´ 1q-th head. The\nXij\nXipj´1q"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "summation replaces Xij as the new input embedding for the j-"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "th head in the self-attention computation. Finally, it is noted that"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "depthwise convolution is applied to each Q in every attention"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "head. This enables\nthe subsequent\nself-attention process\nto"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "capture global\nrepresentations and local\ninformation."
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "The framework comprises three PainViT–blocks, each with 1,"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "3, and 4 depths, respectively. This hierarchical structure features"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "a progressive reduction in the number of tokens by subsampling"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "the resolution by a factor of 2ˆ at each stage. Correspondingly,"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "the architecture facilitates the extraction of embeddings with"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "dimensions d across the blocks, specifically 192, 288, and 500."
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "Additionally,\nthe multihead self-attention mechanism within"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "each block employs 3, 3, and 4 heads,\nrespectively. Fig. 1(a-d)"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "illustrates the PainViT architecture and its fundamental building"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "blocks, while Table I presents the number of parameters and"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "the computational cost\nin terms of floating-point operations"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "(FLOPS)."
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "B. Embedding extraction & Fusion"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "For each frame of a video, V “ rv1, v2, . . . , vns, PainViT–1"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "extracts a corresponding embedding. These embeddings are"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "aggregated to form a unified feature\nrepresentation of\nthe"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "video. Similarly,\nfor each channel of an fNIRS signal, C “"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "embeddings, which are\nrc1, c2, . . . , cms, PainViT–1 extracts"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "subsequently aggregated to create a representation of the fNIRS"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "signal. This process can be described as:"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "ÿ"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "(8)\nEV Ð\nPainViT–1pviq,"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "i“1"
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": ""
        },
        {
          "TABLE I: Number of parameters and FLOPS for\nthe compo-": "ÿ"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "PainViT-2\n…": "Assessment"
        },
        {
          "PainViT-2\n…": "fNIRS"
        },
        {
          "PainViT-2\n…": "Fig. 1: PainViT:\n(a) Hierarchical organization of\nthe PainViT blocks, each with different depths,\nillustrating the reduction"
        },
        {
          "PainViT-2\n…": "in token resolution at each stage;\n(b) Detail of\nthe Token-Mixer module, showcasing its components including a depthwise"
        },
        {
          "PainViT-2\n…": "(d)\nconvolution (DWConv) and batch normalization;\n(c) The Feed-Forward Network (FFN) structure within the Token-Mixer;"
        },
        {
          "PainViT-2\n…": "The Cascaded Attention mechanism across multiple heads, depicting the process of adding outputs from previous heads to"
        },
        {
          "PainViT-2\n…": "enhance the self-attention computation, and the final output projection;\n(e) Overview of\nthe proposed multimodal pipeline,"
        },
        {
          "PainViT-2\n…": "utilizing videos and fNIRS. The extracted embeddings from PainViT–1 are visualized as waveform diagrams, which are then"
        },
        {
          "PainViT-2\n…": "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "is\nthe\nloss\nassociated with\neach\nspecific\ntask\nwhere LSi"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "a\nc\nb"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "corresponding\nto\ndifferent\ndatasets,\nrepresents\nthe\nand wi"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "learned weights that guide the learning process in minimizing"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "the overall\ntaking into account all\nthe individual\nloss Ltotal,"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "losses. Table II details\nthe datasets used in the pre-training"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "process."
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "Fig. 2: Waveform diagrams representing different data modali-"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "E. Augmentation Methods & Regularization"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "ties: (a) original fNIRS signal waveform, (b) video embedding"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "Several augmentation methods have been utilized for training"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "extracted from PainViT–1, and (c)\nfNIRS embedding extracted"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "the proposed framework. Regarding the pre-training process,"
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "from PainViT–1."
        },
        {
          "combined into a single diagram depicting both modalities before being entered into PainViT–2 for\nthe final pain assessment.": "RandAugment\n[35]\nand TrivialAugment\n[36] were\nadopted."
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 0: 6|3, and high regularization (LS 0.4, DropOut 0.5). The",
      "data": [
        {
          "TABLE II: Datasets utilized for": "framework.",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "modality & Addition method,",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "Dataset",
          "the pretraining process of": "# classes",
          "the": "",
          "TABLE IV: Classification results utilizing the": "Augmentation",
          "facial video": "Task"
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "Rand\nTrivial",
          "facial video": "MC"
        },
        {
          "TABLE II: Datasets utilized for": "AffectNet",
          "the pretraining process of": "8",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "7",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "0.1\n0.1",
          "facial video": "41.90"
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "11",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "0.5\n0.5",
          "facial video": "44.91"
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "26",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "0.5\n0.5",
          "facial video": "42.13"
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "2",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "0.5\n0.5",
          "facial video": "42.36"
        },
        {
          "TABLE II: Datasets utilized for": "Silent-EMG [33]",
          "the pretraining process of": "8",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "0.9\n0.9",
          "facial video": "43.52"
        },
        {
          "TABLE II: Datasets utilized for": "BioVid [34]",
          "the pretraining process of": "5",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "BioVid [34]",
          "the pretraining process of": "5",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "pain assessment. For Augmentation & Regularization the first number",
          "facial video": "represents\nthe"
        },
        {
          "TABLE II: Datasets utilized for": "BioVid [34]",
          "the pretraining process of": "5",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "probability of application, while in MaskOut",
          "facial video": "followed | indicates the number"
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        },
        {
          "TABLE II: Datasets utilized for": "",
          "the pretraining process of": "",
          "the": "",
          "TABLE IV: Classification results utilizing the": "",
          "facial video": ""
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 0: 1 0.1 0.1 0.1|3 0.1 0.5 42.13",
      "data": [
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": "method,"
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": ""
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": ""
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": ""
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": "AugMix"
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": ""
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": "0.5"
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": "0.5"
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": "0.9"
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": "0.9"
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": "0.5"
        },
        {
          "TABLE VI: Classification results utilizing the HbR & Addition": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Augmentation": "Trivial",
          "Regularization": "DropOut",
          "Task": "MC"
        },
        {
          "Augmentation": "0.5",
          "Regularization": "0.5",
          "Task": "42.36"
        },
        {
          "Augmentation": "0.5",
          "Regularization": "0.5",
          "Task": "41.67"
        },
        {
          "Augmentation": "09.",
          "Regularization": "0.6",
          "Task": "42.59"
        },
        {
          "Augmentation": "0.9",
          "Regularization": "0.9",
          "Task": "43.06"
        },
        {
          "Augmentation": "0.9",
          "Regularization": "0.5",
          "Task": "43.75"
        },
        {
          "Augmentation": "",
          "Regularization": "",
          "Task": ""
        },
        {
          "Augmentation": "",
          "Regularization": "",
          "Task": ""
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "0.5\n0.5\n0.5\n0.7|3\n0.0\n0.7\n45.14"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "it demonstrated superior performance to the HbR. Two methods"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "were\ndeveloped\nfor\ndata\nfusion:\nthe\npreviously\ndescribed"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "Addition method, aggregating embeddings from video frames"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "and fNIRS channels and then combines them, and the Single-"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "Diagram method, where aggregated embeddings\nfrom both"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "modalities are concurrently visualized in the same image. For"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "the Addition method and initial configurations with moderate"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "augmentation\nlevels\n(0.5\nfor AugMix, Rand,\nTrivial)\nand"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "MaskOut\nIncreasing\nat 0.4|5 achieved a 42.36% accuracy."
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "augmentation levels\nto 0.9 and adjusting regularization pa-"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "rameters\n(LS up to 0.4 and DropOut up to 0.9)\nimproved"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "the accuracy, peaking at 43.75% (refer\nto Table XI). For\nthe"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "Single Diagram method, accuracy improvements were observed,"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "as\nshown in Table XII. Starting with lower MaskOut\nlevels"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "(0.5),\nthe\nat 0.3|5 and standard augmentation probabilities"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "accuracy was 45.83%. Utilizing augmentation probabilities to"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "0.9 and MaskOut adjustments to 0.7|3 significantly improved"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "performance, achieving a high of 46.76%."
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "V.\nINTERPRETATION & COMPARISON"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": ""
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "Regarding the framework’s\ninterpretation, attention maps"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "were\ngenerated\nfrom the\nlast\nlayer\nof PainViT–2, which"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "processed the unified image visualizing both the video and"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "HbO embedding waveforms. This layer contains 500 neurons,"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "each contributing uniquely to and attending to the input. Fig."
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "3 illustrates four examples where certain neurons focus on the"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "video embedding waveform, others on the HbO, and some"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "attend to both waveforms,\nemphasizing different parts\nand"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "details. Table XIV compares\nthe proposed pipeline and the"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "baseline results provided by the challenge organizers. The video-"
        },
        {
          "0.9\n0.9\n0.9\n0.9|3\n0.4\n0.5\n45.83": "based approach using the Addition method outperformed the"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "deep learning methods: A systematic review,” Computer Methods and"
        },
        {
          "ETHICAL IMPACT STATEMENT": "This\nresearch employed the AI4PAIN dataset\n[22],\n[40]",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Programs in Biomedicine, vol. 231, p. 107365, 2023."
        },
        {
          "ETHICAL IMPACT STATEMENT": "provided by the challenge organizers to evaluate the proposed",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[4] A. Badura, A. Masłowska, A. My´sliwiec, and E. Pietka, “Multimodal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "signal\nanalysis\nfor pain recognition in physiotherapy using wavelet"
        },
        {
          "ETHICAL IMPACT STATEMENT": "methods. The participants did not\nreport any prior history of",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "scattering transform,” Sensors, vol. 21, no. 4, 2021."
        },
        {
          "ETHICAL IMPACT STATEMENT": "neurological or psychiatric disorders, current unstable medical",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[5]\nS. A. H. Aqajari, R. Cao, E. Kasaeyan Naeini, M.-D. Calderon, K. Zheng,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "conditions, chronic pain, or\nregular medication use at\nthe time",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "N. Dutt, P. Liljeberg, S. Salanter¨a, A. M. Nelson, and A. M. Rahmani,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "“Pain\nassessment\ntool with\nelectrodermal\nactivity\nfor\npostoperative"
        },
        {
          "ETHICAL IMPACT STATEMENT": "of\ntesting. Upon arrival,\nthey received a detailed explanation",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "patients: method validation study,” JMIR mHealth and uHealth, vol. 9,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "of\nthe\nexperimental\nprocedures. Written\ninformed\nconsent",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "no. 5, p. e25258, 2021."
        },
        {
          "ETHICAL IMPACT STATEMENT": "was obtained before the experiment began. The experimental",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[6] H. H. Yong, S. J. Gibson, D. J. Horne, and R. D. Helme, “Development"
        },
        {
          "ETHICAL IMPACT STATEMENT": "procedures involving human subjects described in the original",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "of a pain attitudes questionnaire to assess\nstoicism and cautiousness"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "for possible age differences.” The journals of gerontology. Series B,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "paper were approved by the University of Canberra’s Human",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Psychological sciences and social sciences, vol. 56, no. 5, pp. P279–84,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "number:\nEthics Committee\n(approval\n11837). This\nstudy",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "sep 2001."
        },
        {
          "ETHICAL IMPACT STATEMENT": "presents a pain assessment\nframework for continuous patient",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[7]\nE. J. Bartley and R. B. Fillingim, “Sex differences in pain: a brief review"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "of clinical and experimental findings,” British journal of anaesthesia,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "monitoring and minimizing human biases. However, it is crucial",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "vol. 111, no. 1, pp. 52–58,\njul 2013."
        },
        {
          "ETHICAL IMPACT STATEMENT": "to acknowledge that deploying this framework in real-world",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[8]\nS. Gkikas., C. Chatzaki., E.\nPavlidou.,\nF. Verigou., K. Kalkanis.,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "clinical environments may pose challenges, requiring additional",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "and M. Tsiknakis.,\n“Automatic\npain\nintensity\nestimation\nbased\non"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "electrocardiogram and demographic factors.”\nSciTePress, 2022, pp."
        },
        {
          "ETHICAL IMPACT STATEMENT": "experiments and thorough validation through clinical trials prior",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "155–162."
        },
        {
          "ETHICAL IMPACT STATEMENT": "to final deployment. Furthermore, the sole facial image featured",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[9]\nS. Gkikas, C. Chatzaki, and M. Tsiknakis, “Multi-task neural networks"
        },
        {
          "ETHICAL IMPACT STATEMENT": "in this study is a designed illustration and does not\nrepresent",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "for pain intensity estimation using electrocardiogram and demographic"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "factors,” in Information and Communication Technologies for Ageing"
        },
        {
          "ETHICAL IMPACT STATEMENT": "an actual person.",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Well and e-Health.\nSpringer Nature Switzerland, 2023, pp. 324–337."
        },
        {
          "ETHICAL IMPACT STATEMENT": "In addition,\nseveral datasets were utilized to pretrain the",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[10] R. Fernandez Rojas, N. Brown, G. Waddington, and R. Goecke, “A"
        },
        {
          "ETHICAL IMPACT STATEMENT": "proposed\npain\nassessment\nframework. The AffectNet\n[29]",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "systematic review of neurophysiological sensing for\nthe assessment of"
        },
        {
          "ETHICAL IMPACT STATEMENT": "dataset\nis compiled using search engine queries. The original",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "acute pain,” NPJ Digital Medicine, vol. 6, no. 1, p. 76, 2023."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[11] R. F. Rojas, X. Huang, and K.-L. Ou, “Region of\ninterest detection and"
        },
        {
          "ETHICAL IMPACT STATEMENT": "paper does not explicitly detail ethical compliance measures.",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "evaluation in functional near\ninfrared spectroscopy,” Journal of Near"
        },
        {
          "ETHICAL IMPACT STATEMENT": "The RAF-DB [30] dataset was compiled using the Flickr image",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Infrared Spectroscopy, vol. 24, no. 4, pp. 317–326, 2016."
        },
        {
          "ETHICAL IMPACT STATEMENT": "hosting service. Although Flickr hosts both public and privately",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[12] R. Fernandez Rojas, M. Liao,\nJ. Romero, X. Huang, and K.-L. Ou,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "“Cortical network response to acupuncture and the effect of\nthe hegu"
        },
        {
          "ETHICAL IMPACT STATEMENT": "shared images, the authors do not explicitly mention the type of",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "point: An fnirs study,” Sensors, vol. 19, no. 2, 2019."
        },
        {
          "ETHICAL IMPACT STATEMENT": "the downloaded images. The original paper of Compound FEE-",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[13] G. Bargshady, X. Zhou, R. C. Deo, J. Soar, F. Whittaker, and H. Wang,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "DB [31] does not mention ethical compliance measures, but only",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "“The modeling\nof\nhuman\nfacial\npain\nintensity\nbased\non\ntemporal"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "convolutional networks trained with video frames in hsv color space,”"
        },
        {
          "ETHICAL IMPACT STATEMENT": "that\nthe subjects were recruited from the Ohio State University",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Applied Soft Computing, vol. 97, p. 106805, 2020."
        },
        {
          "ETHICAL IMPACT STATEMENT": "area and received a monetary reward for participating. The EEG-",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[14] G. Bargshady, J. Soar, X. Zhou, R. C. Deo, F. Whittaker, and H. Wang, “A"
        },
        {
          "ETHICAL IMPACT STATEMENT": "BST-SZ [32] dataset was recorded with assistance from trained",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "joint deep neural network model for pain recognition from face,” in 2019"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "IEEE 4th International Conference on Computer and Communication"
        },
        {
          "ETHICAL IMPACT STATEMENT": "research assistants, psychiatrists, or clinical psychologists who",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Systems (ICCCS), 2019, pp. 52–56."
        },
        {
          "ETHICAL IMPACT STATEMENT": "conducted all\ninterviews. The study received approval from the",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[15] G. Bargshady, X. Zhou, R. C. Deo, J. Soar, F. Whittaker, and H. Wang,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "University of California at San Francisco Institutional Review",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "“Enhanced deep learning algorithm development\nto detect pain intensity"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Board and the San Francisco Veterans Affairs Medical Center.",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "from facial expression images,” Expert Systems with Applications, vol."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "149, p. 113305, 2020."
        },
        {
          "ETHICAL IMPACT STATEMENT": "The original paper on the Silent-EMG [33] dataset does not",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[16]\nS. Gkikas\nand M. Tsiknakis,\n“A full\ntransformer-based\nframework"
        },
        {
          "ETHICAL IMPACT STATEMENT": "explicitly mention adherence to ethical compliance measures.",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "2023\n45th Annual\nfor\nautomatic\npain\nestimation\nusing\nvideos,”\nin"
        },
        {
          "ETHICAL IMPACT STATEMENT": "However,\nit\nis noted that\nthe data recorded came solely from",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "International Conference of the IEEE Engineering in Medicine & Biology"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Society (EMBC), 2023, pp. 1–6."
        },
        {
          "ETHICAL IMPACT STATEMENT": "one individual, also one of\nthe authors and creators of\nthe",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[17] R. F. Rojas, X. Huang, J. Romero, and K.-L. Ou, “fnirs approach to pain"
        },
        {
          "ETHICAL IMPACT STATEMENT": "dataset. The data from the BioVid Heat Pain Database [34]",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "assessment\nfor non-verbal patients,” in Neural\nInformation Processing."
        },
        {
          "ETHICAL IMPACT STATEMENT": "were recorded according to the ethical guidelines of Helsinki",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Cham: Springer\nInternational Publishing, 2017, pp. 778–787."
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[18] R. Fernandez Rojas, X. Huang, and K.-L. Ou, “A machine learning"
        },
        {
          "ETHICAL IMPACT STATEMENT": "(ethics committee: 196/10-UBB/bal).",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "approach for the identification of a biomarker of human pain using fnirs,”"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Scientific reports, vol. 9, no. 1, p. 5645, 2019."
        },
        {
          "ETHICAL IMPACT STATEMENT": "ACKNOWLEDGEMENT",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[19] R.\nF. Rojas,\nJ. Romero,\nJ. Lopez-Aparicio,\nand K.-L. Ou,\n“Pain"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Research supported by the ODIN project\nthat has received",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "assessment based on fnirs using bi-lstm rnns,” in 2021 10th International"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "IEEE/EMBS Conference on Neural Engineering (NER), 2021, pp. 399–"
        },
        {
          "ETHICAL IMPACT STATEMENT": "funding from the European Union’s Horizon 2020 research and",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "402."
        },
        {
          "ETHICAL IMPACT STATEMENT": "innovation program under grant agreement No 101017331.",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[20] R. Fernandez Rojas, C. Joseph, G. Bargshady, and K.-L. Ou, “Empirical"
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "comparison of deep learning models for\nfnirs pain decoding,” Frontiers"
        },
        {
          "ETHICAL IMPACT STATEMENT": "REFERENCES",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": ""
        },
        {
          "ETHICAL IMPACT STATEMENT": "",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "in Neuroinformatics, vol. 18, 2024."
        },
        {
          "ETHICAL IMPACT STATEMENT": "[1]\nS. N. Raja, D. B. Carr, M. Cohen, N. B. Finnerup, H. Flor, S. Gibson, F. J.",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "S. Gkikas, N. S. Tachos, S. Andreadis, V. C. Pezoulas, D. Zaridis,\n[21]"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Keefe, J. S. Mogil, M. Ringkamp, K. A. Sluka, X.-J. Song, B. Stevens,",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "G. Gkois, A. Matonaki, T. G. Stavropoulos, and D.\nI. Fotiadis, “Mul-"
        },
        {
          "ETHICAL IMPACT STATEMENT": "M. D. Sullivan, P. R. Tutelman, T. Ushida, and K. Vader, “The revised",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "timodal automatic assessment of acute pain through facial videos and"
        },
        {
          "ETHICAL IMPACT STATEMENT": "international association for the study of pain definition of pain: concepts,",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "heart\nrate signals utilizing transformer-based architectures,” Frontiers in"
        },
        {
          "ETHICAL IMPACT STATEMENT": "challenges, and compromises,” Pain, vol. 161, no. 9, p. 1976—1982,",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "Pain Research, vol. 5, 2024."
        },
        {
          "ETHICAL IMPACT STATEMENT": "September 2020.",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "[22] R. Fernandez Rojas, N. Hirachan, N. Brown, G. Waddington, L. Murtagh,"
        },
        {
          "ETHICAL IMPACT STATEMENT": "in\n[2]\nP. Dinakar and A. M. Stillman, “Pathogenesis of pain,” Seminars",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "B. Seymour, and R. Goecke, “Multimodal physiological sensing for\nthe"
        },
        {
          "ETHICAL IMPACT STATEMENT": "Pediatric Neurology, vol. 23, no. 3, pp. 201–208, aug 2016.",
          "S. Gkikas and M. Tsiknakis, “Automatic assessment of pain based on\n[3]": "assessment of acute pain,” Frontiers in Pain Research, vol. 4, 2023."
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "",
          "APPENDIX": "Supplementary Metrics"
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "“An image is worth 16x16 words: Transformers for\nimage recognition at",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "scale,” arXiv preprint arXiv:2010.11929, 2020.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[24] Y. Chen, X. Dai, D. Chen, M. Liu, X. Dong, L. Yuan,\nand Z. Liu,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "“Mobile-former: Bridging mobilenet and transformer,” in 2022 IEEE/CVF",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "",
          "APPENDIX": "reported on macro-averaged precision,"
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "Conference on Computer Vision and Pattern Recognition (CVPR), 2022,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "pp. 5260–5269.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[25]\nS. Wu, T. Wu, H. Tan, and G. Guo, “Pale transformer: A general vision",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "transformer backbone with pale-shaped attention,” in Proceedings of",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "",
          "APPENDIX": "Modality\nApproach"
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "the AAAI Conference on Artificial\nIntelligence, vol. 36, no. 3, 2022, pp.",
          "APPENDIX": "Precision"
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "2731–2739.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[26]\nZ. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,",
          "APPENDIX": "Addition\nVideo\n44.91"
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "“Swin transformer: Hierarchical vision transformer using shifted windows,”",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "",
          "APPENDIX": "fNIRS\nHbO & Addition\n44.68"
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "in 2021 IEEE/CVF International Conference on Computer Vision (ICCV),",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "",
          "APPENDIX": "Single Diagram\nFusion\n46.76"
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "2021, pp. 9992–10 002.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[27] X. Liu, H. Peng, N. Zheng, Y. Yang, H. Hu, and Y. Yuan, “Efficientvit:",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "Memory efficient vision transformer with cascaded group attention,” in",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "(CVPR), 2023, pp. 14 420–14 430.",
          "APPENDIX": "Supplementary Attention Maps"
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "and Y. Qiao,\n“Joint\nface detection and\n[28] K. Zhang, Z. Zhang, Z. Li,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "alignment using multitask cascaded convolutional networks,” IEEE signal",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "processing letters, vol. 23, no. 10, pp. 1499–1503, 2016.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[29] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "for\nfacial expression, valence, and arousal computing in the wild,” IEEE",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "Transactions on Affective Computing, vol. 10, no. 1, pp. 18–31, 2019.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[30]\nS. Li, W. Deng, and J. Du, “Reliable crowdsourcing and deep locality-",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "preserving learning for expression recognition in the wild,” in Proceedings",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "of\nthe IEEE Conference on Computer Vision and Pattern Recognition",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "(CVPR), July 2017.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "S. Du, Y. Tao, and A. M. Martinez, “Compound facial expressions of\n[31]",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "emotion,” Proceedings of\nthe National Academy of Sciences, vol. 111,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "no. 15, pp. E1454–E1462, 2014.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[32]\nJ. M. Ford, V. A. Palzes, B. J. Roach, and D. H. Mathalon, “Did I Do",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "That? Abnormal Predictive Processes\nin Schizophrenia When Button",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "Pressing to Deliver a Tone,” Schizophrenia Bulletin, vol. 40, no. 4, pp.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "804–812, 07 2013.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[33] D. Gaddy and D. Klein, “Digital voicing of silent speech,” in Proceedings",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "of\nthe 2020 Conference on Empirical Methods\nin Natural Language",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "Processing (EMNLP). Online: Association for Computational Linguistics,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "2020, pp. 5521–5530.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[34]\nS. Walter, S. Gruss, H. Ehleiter,\nJ. Tan, H. C. Traue, S. Crawcour,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "P. Werner, A. Al-Hamadi, A. O. Andrade, and G. M. D. Silva, “The",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "biovid heat pain database: Data for\nthe advancement and systematic",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "validation of an automated pain recognition,” 2013, pp. 128–131.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[35]\nE. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment: Practical",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "automated data augmentation with a reduced search space,” in 2020",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "Workshops (CVPRW), 2020, pp. 3008–3017.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "S. G. M¨uller and F. Hutter, “Trivialaugment: Tuning-free yet state-of-\n[36]",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "the-art data augmentation,” in 2021 IEEE/CVF International Conference",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "on Computer Vision (ICCV), 2021, pp. 754–762.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[37] D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Laksh-",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "minarayanan, “Augmix: A simple data processing method to improve",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "robustness and uncertainty,” arXiv preprint arXiv:1912.02781, 2019.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[38] C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, “Rethinking",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "the\nthe inception architecture for computer vision,” in Proceedings of",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "IEEE conference on computer vision and pattern recognition, 2016, pp.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "2818–2826.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "I. Sutskever,\nand R. R.\n[39] G. E. Hinton, N. Srivastava, A. Krizhevsky,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "Salakhutdinov, “Improving neural networks by preventing co-adaptation",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "of\nfeature detectors,” arXiv preprint arXiv:1207.0580, 2012.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "[40] R. Fernandez Rojas, N. Hirachan, C. Joseph, B. Seymour, and R. Goecke,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "“The ai4pain grand challenge 2024: Advancing pain assessment with",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "multimodal\nfnirs and facial video analysis,” in 2024 12th International",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "Conference on Affective Computing and Intelligent\nInteraction.\nIEEE,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "2024.",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "J. Hernandez-Juarez,\nand K.-L. Ou,\n[41] R. Fernandez Rojas, X. Huang,",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "“Physiological fluctuations show frequency-specific networks in fnirs sig-",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "International Conference\nnals during resting state,” in 2017 39th Annual",
          "APPENDIX": ""
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "of\nthe IEEE Engineering in Medicine and Biology Society (EMBC), 2017,",
          "APPENDIX": "Fig. 4: Additional attention maps from the PainViT–2."
        },
        {
          "[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,": "pp. 2550–2553.",
          "APPENDIX": ""
        }
      ],
      "page": 9
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "The revised international association for the study of pain definition of pain: concepts, challenges, and compromises",
      "authors": [
        "S Raja",
        "D Carr",
        "M Cohen",
        "N Finnerup",
        "H Flor",
        "S Gibson",
        "F Keefe",
        "J Mogil",
        "M Ringkamp",
        "K Sluka",
        "X.-J Song",
        "B Stevens",
        "M Sullivan",
        "P Tutelman",
        "T Ushida",
        "K Vader"
      ],
      "year": "2020",
      "venue": "Pain"
    },
    {
      "citation_id": "2",
      "title": "Pathogenesis of pain",
      "authors": [
        "P Dinakar",
        "A Stillman"
      ],
      "year": "2016",
      "venue": "Seminars in Pediatric Neurology"
    },
    {
      "citation_id": "3",
      "title": "Automatic assessment of pain based on deep learning methods: A systematic review",
      "authors": [
        "S Gkikas",
        "M Tsiknakis"
      ],
      "year": "2023",
      "venue": "Computer Methods and Programs in Biomedicine"
    },
    {
      "citation_id": "4",
      "title": "Multimodal signal analysis for pain recognition in physiotherapy using wavelet scattering transform",
      "authors": [
        "A Badura",
        "A Masłowska",
        "A Myśliwiec",
        "E Pietka"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "5",
      "title": "Pain assessment tool with electrodermal activity for postoperative patients: method validation study",
      "authors": [
        "S Aqajari",
        "R Cao",
        "E Kasaeyan",
        "M.-D Naeini",
        "K Calderon",
        "N Zheng",
        "P Dutt",
        "S Liljeberg",
        "A Salanterä",
        "A Nelson",
        "Rahmani"
      ],
      "year": "2021",
      "venue": "JMIR mHealth and uHealth"
    },
    {
      "citation_id": "6",
      "title": "Development of a pain attitudes questionnaire to assess stoicism and cautiousness for possible age differences",
      "authors": [
        "H Yong",
        "S Gibson",
        "D Horne",
        "R Helme"
      ],
      "year": "2001",
      "venue": "The journals of gerontology. Series B, Psychological sciences and social sciences"
    },
    {
      "citation_id": "7",
      "title": "Sex differences in pain: a brief review of clinical and experimental findings",
      "authors": [
        "E Bartley",
        "R Fillingim"
      ],
      "year": "2013",
      "venue": "British journal of anaesthesia"
    },
    {
      "citation_id": "8",
      "title": "Automatic pain intensity estimation based on electrocardiogram and demographic factors",
      "authors": [
        "S Gkikas",
        "C Chatzaki",
        "E Pavlidou",
        "F Verigou",
        "K Kalkanis",
        "M Tsiknakis"
      ],
      "year": "2022",
      "venue": "Automatic pain intensity estimation based on electrocardiogram and demographic factors"
    },
    {
      "citation_id": "9",
      "title": "Multi-task neural networks for pain intensity estimation using electrocardiogram and demographic factors,\" in Information and Communication Technologies for Ageing Well and e-Health",
      "authors": [
        "S Gkikas",
        "C Chatzaki",
        "M Tsiknakis"
      ],
      "year": "2023",
      "venue": "Multi-task neural networks for pain intensity estimation using electrocardiogram and demographic factors,\" in Information and Communication Technologies for Ageing Well and e-Health"
    },
    {
      "citation_id": "10",
      "title": "A systematic review of neurophysiological sensing for the assessment of acute pain",
      "authors": [
        "R Rojas",
        "N Brown",
        "G Waddington",
        "R Goecke"
      ],
      "year": "2023",
      "venue": "NPJ Digital Medicine"
    },
    {
      "citation_id": "11",
      "title": "Region of interest detection and evaluation in functional near infrared spectroscopy",
      "authors": [
        "R Rojas",
        "X Huang",
        "K.-L Ou"
      ],
      "year": "2016",
      "venue": "Journal of Near Infrared Spectroscopy"
    },
    {
      "citation_id": "12",
      "title": "Cortical network response to acupuncture and the effect of the hegu point: An fnirs study",
      "authors": [
        "R Rojas",
        "M Liao",
        "J Romero",
        "X Huang",
        "K.-L Ou"
      ],
      "year": "2019",
      "venue": "Sensors"
    },
    {
      "citation_id": "13",
      "title": "The modeling of human facial pain intensity based on temporal convolutional networks trained with video frames in hsv color space",
      "authors": [
        "G Bargshady",
        "X Zhou",
        "R Deo",
        "J Soar",
        "F Whittaker",
        "H Wang"
      ],
      "year": "2020",
      "venue": "Applied Soft Computing"
    },
    {
      "citation_id": "14",
      "title": "A joint deep neural network model for pain recognition from face",
      "authors": [
        "G Bargshady",
        "J Soar",
        "X Zhou",
        "R Deo",
        "F Whittaker",
        "H Wang"
      ],
      "year": "2019",
      "venue": "2019 IEEE 4th International Conference on Computer and Communication Systems (ICCCS)"
    },
    {
      "citation_id": "15",
      "title": "Enhanced deep learning algorithm development to detect pain intensity from facial expression images",
      "authors": [
        "G Bargshady",
        "X Zhou",
        "R Deo",
        "J Soar",
        "F Whittaker",
        "H Wang"
      ],
      "year": "2020",
      "venue": "Expert Systems with Applications"
    },
    {
      "citation_id": "16",
      "title": "A full transformer-based framework for automatic pain estimation using videos",
      "authors": [
        "S Gkikas",
        "M Tsiknakis"
      ],
      "year": "2023",
      "venue": "2023 45th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)"
    },
    {
      "citation_id": "17",
      "title": "fnirs approach to pain assessment for non-verbal patients",
      "authors": [
        "R Rojas",
        "X Huang",
        "J Romero",
        "K.-L Ou"
      ],
      "year": "2017",
      "venue": "Neural Information Processing"
    },
    {
      "citation_id": "18",
      "title": "A machine learning approach for the identification of a biomarker of human pain using fnirs",
      "authors": [
        "R Rojas",
        "X Huang",
        "K.-L Ou"
      ],
      "year": "2019",
      "venue": "Scientific reports"
    },
    {
      "citation_id": "19",
      "title": "Pain assessment based on fnirs using bi-lstm rnns",
      "authors": [
        "R Rojas",
        "J Romero",
        "J Lopez-Aparicio",
        "K.-L Ou"
      ],
      "year": "2021",
      "venue": "2021 10th International IEEE/EMBS Conference on Neural Engineering (NER)"
    },
    {
      "citation_id": "20",
      "title": "Empirical comparison of deep learning models for fnirs pain decoding",
      "authors": [
        "R Rojas",
        "C Joseph",
        "G Bargshady",
        "K.-L Ou"
      ],
      "year": "2024",
      "venue": "Frontiers in Neuroinformatics"
    },
    {
      "citation_id": "21",
      "title": "Multimodal automatic assessment of acute pain through facial videos and heart rate signals utilizing transformer-based architectures",
      "authors": [
        "S Gkikas",
        "N Tachos",
        "S Andreadis",
        "V Pezoulas",
        "D Zaridis",
        "G Gkois",
        "A Matonaki",
        "T Stavropoulos",
        "D Fotiadis"
      ],
      "year": "2024",
      "venue": "Frontiers in Pain Research"
    },
    {
      "citation_id": "22",
      "title": "Multimodal physiological sensing for the assessment of acute pain",
      "authors": [
        "R Rojas",
        "N Hirachan",
        "N Brown",
        "G Waddington",
        "L Murtagh",
        "B Seymour",
        "R Goecke"
      ],
      "year": "2023",
      "venue": "Frontiers in Pain Research"
    },
    {
      "citation_id": "23",
      "title": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "authors": [
        "A Dosovitskiy",
        "L Beyer",
        "A Kolesnikov",
        "D Weissenborn",
        "X Zhai",
        "T Unterthiner",
        "M Dehghani",
        "M Minderer",
        "G Heigold",
        "S Gelly"
      ],
      "year": "2020",
      "venue": "An image is worth 16x16 words: Transformers for image recognition at scale",
      "arxiv": "arXiv:2010.11929"
    },
    {
      "citation_id": "24",
      "title": "Mobile-former: Bridging mobilenet and transformer",
      "authors": [
        "Y Chen",
        "X Dai",
        "D Chen",
        "M Liu",
        "X Dong",
        "L Yuan",
        "Z Liu"
      ],
      "year": "2022",
      "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "25",
      "title": "Pale transformer: A general vision transformer backbone with pale-shaped attention",
      "authors": [
        "S Wu",
        "T Wu",
        "H Tan",
        "G Guo"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Swin transformer: Hierarchical vision transformer using shifted windows",
      "authors": [
        "Z Liu",
        "Y Lin",
        "Y Cao",
        "H Hu",
        "Y Wei",
        "Z Zhang",
        "S Lin",
        "B Guo"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "27",
      "title": "Efficientvit: Memory efficient vision transformer with cascaded group attention",
      "authors": [
        "X Liu",
        "H Peng",
        "N Zheng",
        "Y Yang",
        "H Hu",
        "Y Yuan"
      ],
      "year": "2023",
      "venue": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "28",
      "title": "Joint face detection and alignment using multitask cascaded convolutional networks",
      "authors": [
        "K Zhang",
        "Z Zhang",
        "Z Li",
        "Y Qiao"
      ],
      "year": "2016",
      "venue": "IEEE signal processing letters"
    },
    {
      "citation_id": "29",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2019",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "30",
      "title": "Reliable crowdsourcing and deep localitypreserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    {
      "citation_id": "31",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "32",
      "title": "Did I Do That? Abnormal Predictive Processes in Schizophrenia When Button Pressing to Deliver a Tone",
      "authors": [
        "J Ford",
        "V Palzes",
        "B Roach",
        "D Mathalon"
      ],
      "venue": "Did I Do That? Abnormal Predictive Processes in Schizophrenia When Button Pressing to Deliver a Tone"
    },
    {
      "citation_id": "33",
      "title": "Digital voicing of silent speech",
      "authors": [
        "D Gaddy",
        "D Klein"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)"
    },
    {
      "citation_id": "34",
      "title": "The biovid heat pain database: Data for the advancement and systematic validation of an automated pain recognition",
      "authors": [
        "S Walter",
        "S Gruss",
        "H Ehleiter",
        "J Tan",
        "H Traue",
        "S Crawcour",
        "P Werner",
        "A Al-Hamadi",
        "A Andrade",
        "G Silva"
      ],
      "year": "2013",
      "venue": "The biovid heat pain database: Data for the advancement and systematic validation of an automated pain recognition"
    },
    {
      "citation_id": "35",
      "title": "Randaugment: Practical automated data augmentation with a reduced search space",
      "authors": [
        "E Cubuk",
        "B Zoph",
        "J Shlens",
        "Q Le"
      ],
      "year": "2020",
      "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "36",
      "title": "Trivialaugment: Tuning-free yet state-ofthe-art data augmentation",
      "authors": [
        "S Müller",
        "F Hutter"
      ],
      "year": "2021",
      "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "37",
      "title": "Augmix: A simple data processing method to improve robustness and uncertainty",
      "authors": [
        "D Hendrycks",
        "N Mu",
        "E Cubuk",
        "B Zoph",
        "J Gilmer",
        "B Lakshminarayanan"
      ],
      "year": "2019",
      "venue": "Augmix: A simple data processing method to improve robustness and uncertainty",
      "arxiv": "arXiv:1912.02781"
    },
    {
      "citation_id": "38",
      "title": "Rethinking the inception architecture for computer vision",
      "authors": [
        "C Szegedy",
        "V Vanhoucke",
        "S Ioffe",
        "J Shlens",
        "Z Wojna"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition"
    },
    {
      "citation_id": "39",
      "title": "Improving neural networks by preventing co-adaptation of feature detectors",
      "authors": [
        "G Hinton",
        "N Srivastava",
        "A Krizhevsky",
        "I Sutskever",
        "R Salakhutdinov"
      ],
      "year": "2012",
      "venue": "Improving neural networks by preventing co-adaptation of feature detectors",
      "arxiv": "arXiv:1207.0580"
    },
    {
      "citation_id": "40",
      "title": "The ai4pain grand challenge 2024: Advancing pain assessment with multimodal fnirs and facial video analysis",
      "authors": [
        "R Rojas",
        "N Hirachan",
        "C Joseph",
        "B Seymour",
        "R Goecke"
      ],
      "year": "2024",
      "venue": "2024 12th International Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "41",
      "title": "Physiological fluctuations show frequency-specific networks in fnirs signals during resting state",
      "authors": [
        "R Rojas",
        "X Huang",
        "J Hernandez-Juarez",
        "K.-L Ou"
      ],
      "year": "2017",
      "venue": "2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society"
    }
  ]
}