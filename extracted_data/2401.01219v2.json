{
  "paper_id": "2401.01219v2",
  "title": "Distribution Matching For Multi-Task Learning Of Classification Tasks: A Large-Scale Study On Faces & Beyond",
  "published": "2024-01-02T14:18:11Z",
  "authors": [
    "Dimitrios Kollias",
    "Viktoriia Sharmanska",
    "Stefanos Zafeiriou"
  ],
  "keywords": [],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Multi-Task Learning (MTL) is a framework, where multiple related tasks are learned jointly and benefit from a shared representation space, or parameter transfer. To provide sufficient learning support, modern MTL uses annotated data with full, or sufficiently large overlap across tasks, i.e., each input sample is annotated for all, or most of the tasks. However, collecting such annotations is prohibitive in many real applications, and cannot benefit from datasets available for individual tasks. In this work, we challenge this setup and show that MTL can be successful with classification tasks with little, or non-overlapping annotations, or when there is big discrepancy in the size of labeled data per task. We explore task-relatedness for co-annotation and co-training, and propose a novel approach, where knowledge exchange is enabled between the tasks via distribution matching. To demonstrate the general applicability of our method, we conducted diverse case studies in the domains of affective computing, face recognition, species recognition, and shopping item classification using nine datasets. Our large-scale study of affective tasks for basic expression recognition and facial action unit detection illustrates that our approach is network agnostic and brings large performance improvements compared to the state-of-the-art in both tasks and across all studied databases. In all case studies, we show that co-training via task-relatedness is advantageous and prevents negative transfer (which occurs when MT model's performance is worse than that of at least one single-task model).",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Introduction",
      "text": "Holistic frameworks, where several learning tasks are interconnected and explicable by the reference to the whole, are common in computer vision. A diverse set of examples includes a scene understanding framework that reasons about 3D object detection, semantic segmentation and depth reconstruction  (Wang, Fidler, and Urtasun 2015) , a face analysis framework that addresses face detection, landmark localization, gender recognition, age estimation  (Ranjan et al. 2017) , a universal network for low-, mid-, high-level vision  (Kokkinos 2017) , a large-scale framework of visual tasks for indoor scenes  (Zamir et al. 2018) . Most if not all prior works rely on building a multi-task framework where learning is done based on the ground truth annotations with full or partial overlap across tasks. During training, all the tasks are optimised simultaneously aiming at representation learning that supports a holistic view of the framework.\n\nWhat differentiates our work from these holistic approaches is exploring the idea of task-relatedness as means for co-training different tasks. In our work, relatedness between tasks is either provided explicitly in a form of expert knowledge, or is inferred based on empirical studies. Importantly, in co-training, the related tasks exchange their predictions and iteratively teach each other so that predictors of all tasks can excel even if we have limited or no data for some of them. We propose an effective distribution matching and co-labeling approach based on distillation  (Hinton, Vinyals, and Dean 2015) , where knowledge exchange between tasks is enabled via distribution matching over their predictions.\n\nUp until now training holistic models has been primarily addressed by combining multiple datasets to solve individual tasks  (Ranjan et al. 2017) , or by collecting the annotations in terms of all tasks  (Zamir et al. 2018; Kokkinos 2017) . For example, in affective computing, two most common tasks are predicting categorical expressions (e.g., happy, sad) and activations of binary action units  (Ekman 1997 ) (activation of facial muscles) to explain the affective state. Collecting annotations of AUs is particularly costly, as it requires skilled annotators. The datasets collected so far  (Mollahosseini, Hasani, and Mahoor 2017; Benitez-Quiroz, Srinivasan, and Martinez 2016)  have annotations for training only one task and, despite significant effort, there is no dataset that for has complete annotations of both tasks. Cotraining via task relatedness is an effective way of aggregating knowledge across datasets and transferring it across tasks, especially with little or non-overlapping annotations, or when not many training data are available, or when there is a big discrepancy in the size of labeled data per task.\n\nIn this work we discuss two strategies to infer taskrelatedness, via domain knowledge and dataset annotation. For example, the two aforementioned tasks of facial behavior analysis are interconnected with known strengths of relatedness in literature. In  (Ekman 1997) , the facial action coding system (FACS) was built to indicate for each of the basic expressions its prototypical AUs. In  (Du, Tao, and Martinez 2014) , a dedicated user study has been conducted to study the relationship between AUs and expressions. In  (Khorrami, Paine, and Huang 2015) , the authors show that DNNs trained for expression recognition implicitly learn AUs. In our case study on face recognition, we have a dataset (CelebA  (Liu et al. 2015) ), where annotations for both tasks, identification and attribute prediction, are available. We can infer task relatedness empirically using its annotations.\n\nOne of the important challenges in MTL is how to avoid negative transfer, defined as when the performance of the multi-task model is worse than that of at least one singletask model  (Wang et al. 2019; Liu, Liang, and Gitter 2019) . Negative transfer occurs naturally in MTL scenarios when: i) source data are heterogeneous or less related (since tasks are diverse to each other, there is no suitable common latent representation and thus MTL produces poor representations); ii) one task or group of related tasks dominates the training process (negative transfer may occur simultaneously on tasks outside the dominant group).\n\nTo overcome negative transfer one can, in the loss function, change the lambdas that control the importance of some tasks. However: this could severely affect the performance on other tasks; it is a computationally expensive procedure, which lasts many days for each trial; it is an ad-hoc method that does not guarantee to work on other tasks or databases. To balance the performance on many tasks,  (Liu, Liang, and Gitter 2019)  proposed a method that uses each task's training loss to indicate whether it is well trained, and then decreases the relative weights of the well trained tasks. The evaluation of performance indicators during each training iteration is costly. Negative transfer may be induced by conflicting gradients among the different tasks  (Yu et al. 2020) .  (Lin et al. 2019 ) tackled this through multi-objective optimization, with decomposition of the problem into a set of constrained sub-problems with different trade-off preferences (among different tasks). However, this approach is rather complex, providing a finite set of solutions that do not always satisfy the MTL requirements and finally needs to perform trade-offs among tasks.\n\nWe demonstrate empirically that the proposed distribution matching and co-labeling approach based on task relatedness can prevent negative transfer in all our case studies. Via the proposed approach, knowledge of task relationship is infused in network training, providing it, in a simple manner, with higher level representation of the relationship between the tasks; it is not based on performance indicators and it does not perform any trade-offs between the different tasks. The main contributions of this paper are as follows:\n\n• We propose a flexible framework that can accommodate different classification tasks by encoding prior knowledge of tasks relatedness. In our experiments we evaluate two effective strategies of task relatedness: a) obtained from domain knowledge, e.g., based on cognitive studies, and b) inferred empirically from dataset annotations (when no domain knowledge is available). • We propose an effective weakly-supervised learning approach that couples, via distribution matching and label co-annotation, tasks with little, or even non-overlapping annotations, or with big discrepancy in their labeled data sizes; we consider a plethora of application scenarios, split in two case studies: i) affective computing; ii) beyond affective computing, including face recognition, fine-grained species categorization, shoe type classification, clothing categories recognition. • We conduct an extensive experimental study utilizing 9 databases; we show that the proposed method is network agnostic (i.e., it can be incorporated and used in MTL networks) as it brings similar level of performance improvement in all utilized networks, for all tasks and databases. We also show that our method outperforms the state-ofthe-art in all tasks and databases. Finally we show that our method successfully prevents negative transfer in MTL.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Related Work",
      "text": "Works exist in literature that use expression labels to complement missing AU annotations or increase generalization of AU classifiers  (Yang et al. 2016; Wang, Gan, and Ji 2017) .\n\nOur work deviates from such methods, as we target joint learning of both facial behavior tasks via a single framework, whilst these works perform only AU detection. In the face analysis domain, the use of MTL is somewhat limited.\n\nIn  (Wang et al. 2017) , MTL was tackled through a network that jointly handled face recognition and facial attribute prediction tasks. At first a network was trained for facial attribute detection; then it was used to generate attribute labels for another database that only contained face identification labels. Then, a MT network was trained using that database. The network's loss function was the sum of the independent task losses. In  (Deng, Chen, and Shi 2020) , a unified model performing facial AU detection, expression recognition, and valence-arousal estimation was proposed; the utilized database contained images not annotated for all tasks.\n\nTo tackle this, authors trained a teacher MT model using the complete labels, then generated pseudo-annotations and finally trained a student MT model on the union of original and pseudo labels; that network outperformed the teacher one. The teacher model did not exploit the tasks being interconnected -the overall loss was equal to the sum of the independent task losses. Thus, the student model did not learn this relatedness. This work utilized only one database.\n\nIn terms of MTL,  (Sener and Koltun 2018)  proposed MGDA-UB that casts MTL as multi-objective optimization, with the aim of finding a Pareto optimal solution, and proposed an upper bound for the multi-objective loss.  (Chen et al. 2018)  proposed GradNorm, a gradient optimization algorithm that automatically balances training in MTL by dynamically tuning gradient magnitudes.  (Sun et al. 2020 ) proposed AdaShare, an adaptive sharing approach that decides what to share across which tasks by using a task-specific policy optimized jointly with the network weights.  (Strezoski, Noord, and Worring 2019)  proposed TR (Task Routing) that applies a conditional feature-wise transformation over the conv activations and is encapsulated in the conv layers.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "The Proposed Approach",
      "text": "Let us consider a set of m classification tasks {T i } m i=1 . In task T i , the observations are generated by the underlying distribution D i over inputs X and their labels Y associated with the task. For the i-th task T i , the training set D i consists of Cognitive-Psychological Study  (Du, Tao, and     (Du, Tao, and Martinez 2014)  (the weights denote fraction of annotators that observed the AU activation) or from Aff-Wild2 (the weights denote percentage of images that the AU was activated)\n\nThe goal of MTL is to find m hypothesis: h 1 , ..., h m over the hypothesis space H to control the average expected error over all tasks: 1 m m i=1 E (x,y)∼Di L(h i (x), y) with L being the loss function. We can also define a weight w i ∈ ∆ m , {w i } m i=1 > 0 to govern each task's contribution. The overall loss is:\n\nIn the following, we present the proposed framework via a plethora of case studies, mainly focusing on affective computing. The framework includes inferring the tasks' relationship and using it for coupling them during MTL. The coupling is achieved via the proposed co-annotation and distribution matching losses, which can be incorporated and used in any network that performs MTL, regardless of the input modality  (visual, audio, text) . The advantages of using these losses include: i) flexibility: no changes to network structure are made and no additional burden on inference is placed; ii) effectiveness: performance of various networks on multiple databases (small-or large-scale, image or video) is boosted and negative transfer is alleviated; iii) efficiency: negligible computational complexity is added during training; iv) easiness: a few lines of code are needed to implement.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Case Study I: Affective Computing",
      "text": "We start with the multi-task formulation of the behavior model. In this model we have two objectives: (1) learning 7 basic expressions, (2) detecting activations of 17 binary AUs. We train a multi-task model to jointly perform (1)-(  2 ). For a given image x ∈ X , we can have label annotations of either one of 7 basic expressions y exp ∈ {1, 2, . . . , 7}, or M AU activations y au ∈ {0, 1} M . For simplicity of presentation, we use the same notation x for all images leaving the context to be explained by the label notations. We train the multi-task model by minimizing the following objective:\n\nwhere: , δ i ∈ {0, 1} indicates if the image contains AU i annotation; L DM and L SCA are the distribution matching and soft co-annotation losses (i.e., the proposed coupling losses) based on the relatedness between expressions and AUs; the losses' derivation is explained in the following.\n\nTask-Relatedness 1) Obtained from Domain Knowledge: In the seminal work of  (Du, Tao, and Martinez 2014) , a cognitive and psychological study on the relationship between expressions and facial AU activations is conducted. The summary of the study is a Table of the relatedness between expressions and their prototypical and observational AUs, that we include in Table  1  for completeness. Prototypical AUs are ones that are labelled as activated across all annotators' responses; observational are AUs that are labelled as activated by a fraction of annotators.\n\n2) Inferred Empirically from Dataset Annotations: If the above cognitive study is not available, we can infer task relatedness from external dataset annotations. In particular, we use the training set of Aff-Wild2 database  (Kollias and Zafeiriou 2019, 2021a,b; Kollias et al. 2017 Kollias et al. , 2019 Kollias et al. , 2020b Kollias et al. , 2023;; Zafeiriou et al. 2017 ) to infer task relatedness, since this dataset is the first in-the-wild one that is fully annotated with basic expressions and AUs; this is shown in Table  1 . In the following, we use as domain knowledge the cognitive and psychological study  (Du, Tao, and Martinez 2014) , to encode task relatedness and introduce the proposed approach for coupling the tasks.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "Coupling Of Basic Expressions And Aus",
      "text": "Via Distribution Matching Here, we propose the distribution matching loss for coupling the expression and AU tasks. The aim is to align the predictions of expression and AU tasks during training by making them consistent. From expression predictions we create new soft AU predictions and then match these with the network's actual AU predictions. For instance, if the network predicts happy with probability 1 and also predicts that AUs 4, 15 and 1 are activated, this is a mistake as these AUs are associated with the expression sad according to the prior knowledge. With this loss we infuse the prior knowledge into the network to guide the generation of better and consistent predictions.\n\nFor each sample x we have the predictions of expressions p(y exp |x) as the softmax scores over seven basic expressions and we have the prediction of AUs activations p(y i au |x), i = 1, . . . , M as the sigmoid scores over M AUs. We match the distribution over AU predictions p(y i au |x) with the distribution q(y i au |x), where the AUs are modeled as a mixture over the basic expression categories:\n\nwhere p(y i au |y exp ) is defined deterministically from Table  1  and is 1 for prototypical/observational action units, or 0 otherwise. For example, AU2 is prototypical for expression surprise and observational for expression fear and thus q(y AU2 |x) = p(y surprise |x)+p(y fear |x). So with this matching if, e.g., the network predicts the expression happy with probability 1, i.e., p(y happy |x) = 1, then the prototypical and observational AUs of happy -AUs 12, 25 and 6-need to be activated in the distribution q: q(y AU12 |x) = 1; q(y AU25 |x) = 1; q(y AU6 |x) = 1; q(y i au |x) = 0, i ∈ {1, .., 14}. In spirit of the distillation approach, we match the distributions p(y i au |x) and q(y i au |x) by minimizing the cross entropy with the soft targets loss term:\n\nwhere all available train samples are used to match the predictions.\n\nVia soft co-annotation Here, we propose the soft coannotation loss for coupling the expression and AU tasks. At first we create soft expression labels (that are guided by AU labels) by infusing prior knowledge of their relationship.\n\nThen we match these labels with the expression predictions.\n\nThe new expression labels will help in cases of images with partial or no annotation overlap, especially if there are not many training data. We use the AU labels (instead of predictions) as they provide more confidence (the AU predictions -especially at the beginning of training-will be quite wrong; if we utilized this loss with the wrong AU predictions, it would also affect negatively the expression predictions).\n\nGiven an image x with ground truth AU annotations, y au , we first co-annotate it with a soft label in form of the distribution over expressions and then match it with the predictions of expressions p(y exp |x). Thus, at first we compute, for each basic expression, an indicator score, I(y exp |x) over its prototypical and observational AUs being present:\n\nwhere: w i au is 1 if AU i is prototypical for y exp (from Table  1 ); is w if AU i is observational for y exp ; is 0 otherwise. For example, for expression happy, the indicator score\n\nThen, we convert the indicator scores to probability scores over expression categories; this soft expression label, q(y exp |x), is computed as following:\n\nIn this variant, every single image that has ground truth annotation of AUs will have a soft expression label assigned. Finally we match the predictions p(y exp |x) and the soft expression label q(y exp |x) by minimizing the cross entropy with the soft targets loss term:\n\nCase Study II: Beyond Affective Computing\n\nHere, we show that our approach can also be used in other application scenarios: i) face recognition (facial attribute detection and face identification); ii) fine-grained species categorization (species classification and attribute detection); iii) shoe type recognition (shoe type classification and attribute detection); iv) clothing categories recognition (classification of clothing categories and attributes).\n\nIn the model's multi-task (MT) formulation, we have two objectives: (1) to detect M binary attributes, (2) to classify N classes. The aim of a MT model is to jointly perform (1) and (  2 ). For a given image x ∈ X , we can have labels of one of N classes y cls ∈ {1, . . . , N }, and M binary attributes y att ∈ {0, 1} M . We train the MT model by minimizing the objective: L MT = L Clc + L Att + L DM + L SCA , where: L att is the binary cross entropy loss for the detection task; L DM is the distribution matching loss for matching the distributions p(y i att |x) and the one where the attributes are modeled as a mixture over the classes; L SCA is the soft co-annotation loss for matching predictions p(y cls |x) and soft class labels (i.e., probability of each class indicator score, I(y cls |x), over its detected attributes); p(y i att |y cls ) = total number of images with both y i att and y cls total number of images with y cls , is inferred empirically from dataset annotations.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Experimental Study",
      "text": "Databases In this work we utilized: AffectNet (Mollahosseini, Hasani, and Mahoor 2017) with around 350K inthe-wild images annotated for 7 basic expressions; RAF-DB  (Li, Deng, and Du 2017)  with around 15K in-the-wild images annotated for 7 basic expressions; ABAW4 LSD (Kollias 2022a) -utilized in 4th Affective Behavior Analysis in-the-wild (ABAW) Competition at ECCV 2022-with around 280K in-the-wild synthetic images and 100K inthe-wild real images (which constitute the test set) annotated for 6 basic expressions; Aff-Wild2 (Kollias 2022b)as utilized in 3rd ABAW Competition at CVPR 2022-with 564 in-the-wild videos (A/V) of around 2.8M frames annotated for 7 basic expressions (plus 'other'), 12 AUs and Performance Measures We use: i) average accuracy (AA) for RAF-DB; ii) accuracy for AffectNet; iii) the average between F1 and mean accuracy for EmotioNet (AFA); iv) F1 for ABAW4 LSD and Aff-Wild2; vii) total accuracy and F1 for CelebA, CUB, S-ADD, CAD.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Pre-Processing & Training Implementation Details",
      "text": "Case Study I: We used RetinaFace  (Deng et al. 2020)   The result of using each state-of-the-art (sota) in singletask manner is shown in the row 'ST' (i.e., Single Task) of Table  2 . The result of using each sota in MTL manner (e.g., EmoAffectNet trained on both AffectNet and EmotioNet; PSR trained on both RAF-DB and EmotioNet) is shown in row 'NC MT' (i.e., Multi-Task without coupling) of Table  2 . It might be argued that since more data are used for network training (i.e., the additional data coming from multiple tasks, even if they contain partial or non-overlapping annotations), the MTL performance will be better for all tasks. However, as shown and explained next, this is not the case as negative transfer can occur, or sub-optimal models can be produced for some, or even all tasks  (Wu, Zhang, and Ré 2019) .\n\nIt can be seen in Table  2  (rows 'ST' and 'NC MT'), for all databases, that the sota, when trained in a MTL manner (without coupling), display a better performance for AU detection, but an inferior one for expression recognitionwhen compared to the corresponding performance of the single-task sota. This indicates that negative transfer occurs in the case of basic expressions. This negative transfer effect was due to the fact that the AU detection task dominated the training process. In fact, the EmotioNet database has a larger size than the RAF-DB, AffectNet and ABAW4 LSD. Negative transfer largely depends on the size of labeled data per task  (Wang et al. 2019 ), which has a direct effect on the feasibility and reliability of discovering shared regularities between the joint distributions of the tasks in MTL.\n\nFinally, we trained each of the sota networks in a MTL manner with the proposed coupling, under two relatedness scenarios; when the relatedness between the expressions and AUs was derived from the cognitive and psychological study of  (Du, Tao, and Martinez 2014) , or from dataset annotations (from Aff-Wild2 database). The former case is shown in row 'C MT (DM))' of Table  2  and the latter case in row 'C MT (Aff-Wild2)'. From Table  2  two observations can be made.\n\nFirstly, when the proposed coupling is conducted, in each sota multi-task network, negative transfer is alleviated; the performance of all multi-task networks is better than the corresponding one of the single-task counterparts for both tasks. This is consistently observed in all utilized databases and experiments. Secondly, the use of the proposed coupling brings similar levels of performance improvement in all sota multi-task networks across the databases. In more detail, when coupling is conducted, networks outperform their counterparts without coupling by approximately: i) 5% on Affectnet and 5% on EmotioNet (both EmoAffectNet and EffNet-B2); ii) 6% on RAF-DB and 5% on EmotioNet (both PSR and VGGFACE); iii) 5% on ABAW4 LSD and 6% on EmotioNet (both MTER-KDTD and HSE-NN); iv) 5.5% on Aff-Wild2 (MTER-KDTD).\n\nTo sum up, the use of coupling makes the MT networks greatly outperform their MT (without coupling) and singletask counterparts. This proves that the proposed coupling losses are network and modality agnostic as they can be applied and be effective in different networks and different modalities (visual, audio, A/V and text; e.g. TMIF-FEA is a multi-modal approach). This stands no matter which task relatedness scenario has been used for coupling the two tasks.\n\nFinally, for comparison purposes we also used the Student-Teacher (S-T) knowledge distillation approach. We used one, or multiple teacher networks to create soft-labels for the databases that contain annotations only for one task, so that they contain complete, overlapping annotations for both tasks; we then trained a multi-task network on them. To illustrate this via an example: we use the single-task EmoAf-fectNet trained on AffectNet for expression recognition and test it on EmotioNet to create soft-expression labels; thus EmotioNet contains its AU labels and soft expression labels, i.e., the predictions of EmoAffectNet. Then we use the single-task EmoAffectNet trained on EmotioNet for AU detection and test it on AffectNet to create soft-AU labels; thus AffectNet contains its expression labels and soft AU labels. Then we train EmoAffectNet for MTL using both databases. We compare its performance to EmoAffectNet trained for MTL with the proposed coupling losses on both databases with their original non-overlapping annotations.\n\nThe results of the S-T approach are denoted in row 'S-T NC MT' (denoting Student-Teacher Multi-Task with no coupling) of Table  2 . It can be observed that this approach shows a slightly better performance in both tasks compared to the multi-task counterparts that have been trained with the original non-overlapping annotations -without coupling-. This is expected as these networks have been trained with more annotations for both tasks. Nevertheless, negative transfer for the basic expressions still occurs. Moreover, our proposed approach greatly outperforms the S-T one. So overall, our proposed approach alleviates negative transfer and also brings bigger performance gain than that of S-T approach.\n\nAblation Study Here we perform an ablation study, utilizing the Aff-Wild2, on the effect of each proposed coupling loss on the performance of TMIF-FEA. Table  3  shows the results when task relatedness was drawn from domain knowledge, or from the training set of Aff-Wild2. It can be seen that when TMIF-FEA was trained with either or both coupling losses under any relatedness scenario, its performance was superior to the case when no coupling loss has    4  shows that when the MTL baselines were trained without coupling, they displayed a better performance than the 2 single-task networks; this occurred in all studied cases, tasks, metrics and baseline models. This shows that the studied tasks were coherently correlated; training the multi-task architecture therefore, led to improved performance and no negative transfer occurred. Table  4  further shows that when the baselines were trained in the multi-task setting with coupling, they greatly outperformed its counterpart trained without coupling, in all studied tasks and metrics and for all baseline models. More precisely, when training with coupling, performance increased by 4.35% and 6% in Accuracy and F1 Score for identity classification, by 5.1% and 5% in Accuracy and F1 Score for species categorization; and 1.25% and 2.3% in Accuracy and F1 Score for facial attribute detection and 2.2% and 10.8% in Accuracy and F1 Score for species attribute detection. Shoe Type Recognition Table  4  shows that negative transfer occurs in the case of attribute detection. Each singletask baseline for attribute detection displayed a better performance than its multi-task counterpart without coupling, whereas the latter displayed a better performance for shoe     4  presents the outcomes of the 2-fold cross validation experiments (performed 6 times) in which the results are averaged and their spread is also shown (in the form: mean ± spread). From Table  4 , it can be seen that the selected tasks are very heterogeneous and less correlated as all multi-task baselines without coupling performed significantly worse than single-task counterparts in all utilized metrics. Such severe negative transfers occurs as there is a big discrepancy in the size of labeled data per task in CAD dataset (the missing values for each attribute range from 12% to 84%) and its size is very small (it contains only 1856 images). When the multi-task baselines were trained with coupling, negative transfer was prevented and the models significantly outperformed their single-task counterparts (10-14% difference in Total Accuracy and 13-15% in F1 Score for classification; 4-5% in Total Accuracy and 5-7% in F1 Score for attributes). Finally, a smaller spread of the results can be observed in the case when the models were trained with coupling.\n\nEffectiveness of proposed coupling losses across the state-of-the-art At first, we show that the proposed coupling losses can also be incorporated in sota networks and thus we implement NTS-Net  (Yang et al. 2018 ) in single task setting (denoted NTS-ST), in MTL setting without coupling (NTS-MT NC) and in MTL setting by adding our proposed coupling losses (NTS-MT C). Results are shown on Table  5  and are in accordance with the previous presented results (similar performance gain and alleviation of negative transfer). We then compare our method against MTL ones -presented in related work section-and thus we implement (ResNet50): MGDA-UB, GradNorm, AdaShare and TR. Table 5 presents their results. Comparing these with ResNet50 C MT of Table  4 , it is evident that our method significantly outperforms all of them. Also, when comparing them to ST ResNet of Table  4 : i) MGDA-UB, GradNorm and AdaShare cannot alleviate negative transfer in CAD for both tasks; ii) TR cannot alleviate negative transfer in CUB and CAD for attribute detection and in S-ADD for both tasks.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "Conclusion & Limitation",
      "text": "We proposed a method for accommodating classification tasks by encoding prior knowledge of their relatedness. Our method is important as deep neural networks cannot necessarily capture tasks' relationship, especially in cases where: i) there is no or partial annotation overlap between tasks; ii) not many training data exist; iii) one task dominates the training process; iv) sub-optimal models for some tasks are produced; vi) there is big discrepancy in the size of labeled data per task. We considered a plethora of application scenarios and conducted extensive experimental studies. In all experiments our method helped the MT models greatly improve their performance compared to ST and MT models without coupling. Our method further helped alleviate mild or significant negative transfer that occurred when MT models displayed worse performance in some or all studied tasks than ST models. Our approach is general and flexible as long as there is a direct relationship between the studied tasks; the latter is our method's requirement and thus its limitation.",
      "page_start": 8,
      "page_end": 8
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "happiness": "sadness",
          "12, 25": "4, 15",
          "6 (0.51)": "1 (0.6),\n6 (0.5),\n11 (0.26),\n17 (0.67)",
          "12 (0.82),\n25 (0.7),\n6 (0.57),\n7 (0.83),\n10 (0.63)": "4 (0.53),\n15 (0.42),\n1 (0.31),\n7 (0.13)"
        },
        {
          "happiness": "fear",
          "12, 25": "1, 4, 20, 25",
          "6 (0.51)": "2 (0.57),\n5 (0.63),\n26 (0.33)",
          "12 (0.82),\n25 (0.7),\n6 (0.57),\n7 (0.83),\n10 (0.63)": "1 (0.52),\n4 (0.4),\n25 (0.85),\n7 (0.57),\n10 (0.57)"
        },
        {
          "happiness": "anger",
          "12, 25": "4, 7, 24",
          "6 (0.51)": "10 (0.26),\n17 (0.52),\n23 (0.29)",
          "12 (0.82),\n25 (0.7),\n6 (0.57),\n7 (0.83),\n10 (0.63)": "4 (0.65),\n7 (0.45),\n25 (0.4),\n10 (0.33)"
        },
        {
          "happiness": "surprise",
          "12, 25": "1, 2, 25, 26",
          "6 (0.51)": "5 (0.66)",
          "12 (0.82),\n25 (0.7),\n6 (0.57),\n7 (0.83),\n10 (0.63)": "1 (0.38),\n2 (0.37),\n25 (0.85),\n26 (0.3),\n5 (0.5),\n7 (0.2)"
        },
        {
          "happiness": "disgust",
          "12, 25": "9, 10, 17",
          "6 (0.51)": "4 (0.31),\n24 (0.26)",
          "12 (0.82),\n25 (0.7),\n6 (0.57),\n7 (0.83),\n10 (0.63)": "10 (0.85),\n4 (0.6),\n7 (0.75),\n25 (0.8)"
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 5: and are in accordance with the previous presented",
      "data": [
        {
          "VGG": "ResNet",
          "CelebA\nCUB\nS-ADD\nCAD": "CelebA\nCUB\nS-ADD\nCAD",
          "78.11\n78.23\n71.87\n52±5": "80.83\n82.77\n74.7\n55±5",
          "70.02\n78.44\n71.24\n38±7": "72.9\n82.84\n74.59\n41±7",
          "87.57\n85.18\n91.05\n80±2": "90.11\n89.52\n92.64\n84±2",
          "67.88\n27.01\n89.23\n40±2": "71.38\n30.83\n90.6\n44±2",
          "80.75\n80.02\n72.21\n41±7": "84.01\n84.25\n75.04\n44±7",
          "71.98\n80.18\n72.01\n32±9": "75.1\n84.25\n75.12\n35±9",
          "89.39\n85.54\n90.44\n75±3": "92.03\n89.9\n91.98\n79±3",
          "68.65\n28.59\n88.51\n33±4": "72.31\n32.47\n90.1\n38±4",
          "84.98\n85.12\n76.37\n64±3": "88.63\n89.32\n79.27\n67±3",
          "77.98\n85.13\n76.48\n52±6": "81.1\n89.45\n79.39\n55±6",
          "90.61\n87.98\n93.44\n85±1": "93.33\n92.05\n94.66\n89±1",
          "71.01\n39.31\n91.33\n46±1": "74.68\n43.33\n93.19\n50±1"
        },
        {
          "VGG": "DenseNet",
          "CelebA\nCUB\nS-ADD\nCAD": "CelebA\nCUB\nS-ADD\nCAD",
          "78.11\n78.23\n71.87\n52±5": "80.07\n80.62\n73.25\n53±5",
          "70.02\n78.44\n71.24\n38±7": "72.2\n80.66\n73.12\n39±7",
          "87.57\n85.18\n91.05\n80±2": "89.87\n87.57\n91.26\n82±2",
          "67.88\n27.01\n89.23\n40±2": "70.24\n29.13\n89.6\n42±2",
          "80.75\n80.02\n72.21\n41±7": "82.75\n82.02\n73.66\n42±7",
          "71.98\n80.18\n72.01\n32±9": "74.1\n82.46\n73.74\n33±9",
          "89.39\n85.54\n90.44\n75±3": "91.7\n87.99\n90.66\n77±3",
          "68.65\n28.59\n88.51\n33±4": "71.04\n30.87\n88.93\n36±4",
          "84.98\n85.12\n76.37\n64±3": "87.13\n87.24\n77.89\n65±3",
          "77.98\n85.13\n76.48\n52±6": "80.01\n87.25\n78.01\n53±6",
          "90.61\n87.98\n93.44\n85±1": "92.99\n90.01\n93.77\n87±1",
          "71.01\n39.31\n91.33\n46±1": "73.31\n41.53\n92.44\n48±1"
        }
      ],
      "page": 7
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "EmotioNet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "C Benitez-Quiroz",
        "R Srinivasan",
        "A Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of IEEE International Conference on Computer Vision & Pattern Recognition (CVPR'16)"
    },
    {
      "citation_id": "2",
      "title": "Describing clothing by semantic attributes",
      "authors": [
        "H Chen",
        "A Gallagher",
        "B Girod"
      ],
      "year": "2012",
      "venue": "European conference on computer vision"
    },
    {
      "citation_id": "3",
      "title": "Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks",
      "authors": [
        "Z Chen",
        "V Badrinarayanan",
        "C.-Y Lee",
        "A Rabinovich"
      ],
      "year": "2018",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "4",
      "title": "Multitask emotion recognition with incomplete labels",
      "authors": [
        "D Deng",
        "Z Chen",
        "B Shi"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition (FG 2020)(FG)"
    },
    {
      "citation_id": "5",
      "title": "Retinaface: Single-shot multi-level face localisation in the wild",
      "authors": [
        "J Deng",
        "J Guo",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Compound facial expressions of emotion",
      "authors": [
        "S Du",
        "Y Tao",
        "A Martinez"
      ],
      "year": "2014",
      "venue": "Proceedings of the National Academy of Sciences"
    },
    {
      "citation_id": "7",
      "title": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)",
      "authors": [
        "R Ekman"
      ],
      "year": "1997",
      "venue": "What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS)"
    },
    {
      "citation_id": "8",
      "title": "Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild",
      "authors": [
        "Fabian Benitez-Quiroz",
        "C Srinivasan",
        "R Martinez"
      ],
      "year": "2016",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "9",
      "title": "Distilling the knowledge in a neural network",
      "authors": [
        "G Hinton",
        "O Vinyals",
        "J Dean"
      ],
      "year": "2015",
      "venue": "Distilling the knowledge in a neural network",
      "arxiv": "arXiv:1503.02531"
    },
    {
      "citation_id": "10",
      "title": "Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks",
      "authors": [
        "J.-Y Jeong",
        "Y.-G Hong",
        "J Oh",
        "S Hong",
        "J.-W Jeong",
        "Y Jung"
      ],
      "year": "2022",
      "venue": "Learning from Synthetic Data: Facial Expression Classification based on Ensemble of Multi-task Networks",
      "arxiv": "arXiv:2207.10025"
    },
    {
      "citation_id": "11",
      "title": "Do deep neural networks learn facial action units when doing expression recognition?",
      "authors": [
        "P Khorrami",
        "T Paine",
        "T Huang"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops"
    },
    {
      "citation_id": "12",
      "title": "Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory",
      "authors": [
        "I Kokkinos"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "13",
      "title": "ABAW: learning from synthetic data & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "14",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & multi-task learning challenges",
      "authors": [
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "15",
      "title": "Deep neural network augmentation: Generating faces for affect analysis",
      "authors": [
        "D Kollias",
        "S Cheng",
        "E Ververas",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "16",
      "title": "Recognition of affect in the wild using deep neural networks",
      "authors": [
        "D Kollias",
        "M Nicolaou",
        "I Kotsia",
        "G Zhao",
        "S Zafeiriou"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "17",
      "title": "Analysing affective behavior in the first abaw 2020 competition",
      "authors": [
        "D Kollias",
        "A Schulc",
        "E Hajiyev",
        "S Zafeiriou"
      ],
      "year": "2020",
      "venue": "2020 15th IEEE International Conference on Automatic Face and Gesture Recognition"
    },
    {
      "citation_id": "18",
      "title": "Abaw: Valence-arousal estimation, expression recognition, action unit detection & emotional reaction intensity estimation challenges",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "A Baird",
        "A Cowen",
        "S Zafeiriou"
      ],
      "year": "2023",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "19",
      "title": "Deep affect prediction in-the-wild: Aff-wild database and challenge, deep architectures, and beyond",
      "authors": [
        "D Kollias",
        "P Tzirakis",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "B Schuller",
        "I Kotsia",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "International Journal of Computer Vision"
    },
    {
      "citation_id": "20",
      "title": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2019",
      "venue": "Expression, Affect, Action Unit Recognition: Aff-Wild2, Multi-Task Learning and ArcFace",
      "arxiv": "arXiv:1910.04855"
    },
    {
      "citation_id": "21",
      "title": "Analysing affective behavior in the second abaw2 competition",
      "authors": [
        "D Kollias",
        "S Zafeiriou"
      ],
      "year": "2021",
      "venue": "Affect Analysis inthe-wild: Valence-Arousal, Expressions, Action Units and a Unified Framework",
      "arxiv": "arXiv:2103.15792"
    },
    {
      "citation_id": "22",
      "title": "Whittlesearch: Image search with relative attribute feedback",
      "authors": [
        "A Kovashka",
        "D Parikh",
        "K Grauman"
      ],
      "year": "2012",
      "venue": "2012 IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "23",
      "title": "Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild",
      "authors": [
        "S Li",
        "W Deng",
        "J Du"
      ],
      "year": "2017",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "24",
      "title": "Pareto Multi-Task Learning",
      "authors": [
        "X Lin",
        "H.-L Zhen",
        "Z Li",
        "Q Zhang",
        "S Kwong"
      ],
      "year": "2019",
      "venue": "Thirty-third Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "25",
      "title": "Loss-balanced task weighting to reduce negative transfer in multi-task learning",
      "authors": [
        "S Liu",
        "Y Liang",
        "A Gitter"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "26",
      "title": "Deep Learning Face Attributes in the Wild",
      "authors": [
        "Z Liu",
        "P Luo",
        "X Wang",
        "X Tang"
      ],
      "year": "2015",
      "venue": "Proceedings of International Conference on Computer Vision (ICCV)"
    },
    {
      "citation_id": "27",
      "title": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "authors": [
        "A Mollahosseini",
        "B Hasani",
        "M Mahoor"
      ],
      "year": "2017",
      "venue": "Affectnet: A database for facial expression, valence, and arousal computing in the wild",
      "arxiv": "arXiv:1708.03985"
    },
    {
      "citation_id": "28",
      "title": "Mixaugment & mixup: Augmentation methods for facial expression recognition",
      "authors": [
        "A Psaroudakis",
        "D Kollias"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "An all-in-one convolutional neural network for face analysis",
      "authors": [
        "R Ranjan",
        "S Sankaranarayanan",
        "C Castillo",
        "R Chellappa"
      ],
      "year": "2017",
      "venue": "2017 12th IEEE International Conference on Automatic Face & Gesture Recognition"
    },
    {
      "citation_id": "30",
      "title": "In search of a robust facial expressions recognition model: A large-scale visual cross-corpus study",
      "authors": [
        "E Ryumina",
        "D Dresvyanskiy",
        "A Karpov"
      ],
      "year": "2022",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "31",
      "title": "Facial expression and attributes recognition based on multi-task learning of lightweight neural networks",
      "authors": [
        "A Savchenko"
      ],
      "year": "2021",
      "venue": "2021 IEEE 19th International Symposium on Intelligent Systems and Informatics (SISY)"
    },
    {
      "citation_id": "32",
      "title": "HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images",
      "authors": [
        "A Savchenko"
      ],
      "year": "2022",
      "venue": "HSE-NN Team at the 4th ABAW Competition: Multi-task Emotion Recognition and Learning from Synthetic Images",
      "arxiv": "arXiv:2207.09508"
    },
    {
      "citation_id": "33",
      "title": "Many task learning with task routing",
      "authors": [
        "O Sener",
        "V Koltun",
        "G Strezoski",
        "N Noord",
        "M Worring"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE/CVF International Conference on Computer Vision",
      "arxiv": "arXiv:1810.04650"
    },
    {
      "citation_id": "34",
      "title": "Adashare: Learning what to share for efficient deep multitask learning",
      "authors": [
        "X Sun",
        "R Panda",
        "R Feris",
        "K Saenko"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "35",
      "title": "Pyramid with super resolution for in-the-wild facial expression recognition",
      "authors": [
        "T.-H Vo",
        "G.-S Lee",
        "H.-J Yang",
        "S.-H Kim"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "36",
      "title": "The Caltech-UCSD Birds-200-2011 Dataset",
      "authors": [
        "C Wah",
        "S Branson",
        "P Welinder",
        "P Perona",
        "S Belongie"
      ],
      "year": "2011",
      "venue": "The Caltech-UCSD Birds-200-2011 Dataset"
    },
    {
      "citation_id": "37",
      "title": "Holistic 3d scene understanding from a single geo-tagged image",
      "authors": [
        "S Wang",
        "S Fidler",
        "R Urtasun"
      ],
      "year": "2015",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "38",
      "title": "Expression-assisted facial action unit recognition under incomplete AU annotation",
      "authors": [
        "S Wang",
        "Q Gan",
        "Q Ji"
      ],
      "year": "2017",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "39",
      "title": "Characterizing and avoiding negative transfer",
      "authors": [
        "Z Wang",
        "Z Dai",
        "B Póczos",
        "J Carbonell"
      ],
      "year": "2019",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "40",
      "title": "Multi-task deep neural network for joint face recognition and facial attribute prediction",
      "authors": [
        "Z Wang",
        "K He",
        "Y Fu",
        "R Feng",
        "Y.-G Jiang",
        "X Xue"
      ],
      "year": "2017",
      "venue": "Proceedings of the 2017 ACM on International Conference on Multimedia Retrieval"
    },
    {
      "citation_id": "41",
      "title": "Understanding and Improving Information Transfer in Multi-Task Learning",
      "authors": [
        "S Wu",
        "H Zhang",
        "C Ré",
        "J Yang",
        "S Wu",
        "S Wang",
        "Q Ji"
      ],
      "year": "2016",
      "venue": "2016 23rd International Conference on Pattern Recognition (ICPR)"
    },
    {
      "citation_id": "42",
      "title": "Learning to navigate for fine-grained classification",
      "authors": [
        "Z Yang",
        "T Luo",
        "D Wang",
        "Z Hu",
        "J Gao",
        "L Wang"
      ],
      "year": "2018",
      "venue": "Proceedings of the European conference on computer vision (ECCV)"
    },
    {
      "citation_id": "43",
      "title": "Aff-wild: Valence and arousal 'in-the-wild'challenge",
      "authors": [
        "T Yu",
        "S Kumar",
        "A Gupta",
        "S Levine",
        "K Hausman",
        "C Finn",
        "S Zafeiriou",
        "D Kollias",
        "M Nicolaou",
        "A Papaioannou",
        "G Zhao",
        "I Kotsia"
      ],
      "year": "2017",
      "venue": "Computer Vision and Pattern Recognition Workshops (CVPRW)"
    },
    {
      "citation_id": "44",
      "title": "Taskonomy: Disentangling task transfer learning",
      "authors": [
        "A Zamir",
        "A Sax",
        "W Shen",
        "L Guibas",
        "J Malik",
        "S Savarese"
      ],
      "year": "2018",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "45",
      "title": "Transformer-based Multimodal Information Fusion for Facial Expression Analysis",
      "authors": [
        "W Zhang",
        "F Qiu",
        "S Wang",
        "H Zeng",
        "Z Zhang",
        "R An",
        "B Ma",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    }
  ]
}