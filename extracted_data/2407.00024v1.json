{
  "paper_id": "2407.00024v1",
  "title": "Lmvd: A Large-Scale Multimodal Vlog Dataset For Depression Detection In The Wild",
  "published": "2024-05-09T01:27:10Z",
  "authors": [
    "Lang He",
    "Kai Chen",
    "Junnan Zhao",
    "Yimeng Wang",
    "Ercheng Pei",
    "Haifeng Chen",
    "Jiewei Jiang",
    "Shiqing Zhang",
    "Jie Zhang",
    "Zhongmin Wang",
    "Tao He",
    "Prayag Tiwari"
  ],
  "keywords": [
    "Depression Detection",
    "Transformer",
    "Vlog",
    "Multimodal",
    "Deep Learning"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Depression can significantly impact many aspects of an individual's life, including their personal and social functioning, academic and work performance, and overall quality of life. Many researchers within the field of affective computing are adopting deep learning technology to explore potential patterns related to the detection of depression. However, because of subjects' privacy protection concerns, that data in this area is still scarce, presenting a challenge for the deep discriminative models used in detecting depression. To navigate these obstacles, a large-scale multimodal vlog dataset (LMVD), for depression recognition in the wild is built. In LMVD, which has 1823 samples with 214 hours of the 1475 participants captured from four multimedia platforms (Sina Weibo, Bilibili, Tiktok, and YouTube). A novel architecture termed MDDformer to learn the non-verbal behaviors of individuals is proposed. Extensive validations are performed on the LMVD dataset, demonstrating superior performance for depression detection. We anticipate that the LMVD will contribute a valuable function to the depression detection community. The data and code will released at the link: https://github.com/helang818/LMVD/.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "M AJOR depression disorder (MDD) has been projected to a primary mental illness by 2030. A comprehensive survey and meta-analysis unveiled that from a study of 41,531 individuals, 33.7% of them experienced depression during the COVID-19 pandemic  [1] . Normally, depression can affect various aspects of daily activities, including the progress of one's careers, studies, and families, etc  [2] . In some severe cases, individuals suffering from depression may contemplate or attempt suicide  [3] . In spite of the many attempts to identify depression, recent findings suggest that the prevalence of depression may be increasing in the younger individuals group.\n\nDepressed subjects often express several different nonverbal behaviors, e.g., facial expressions, body gestures, and smile. Diagnostic and Statistical Manual of Mental Disorders (DSM-V) outlines symptoms such as agitation (e.g., the inability to sit still, pacing, and hand-wringing) or retardation (e.g., slowed speech and body movements and increased pauses before answering) that may be displayed by depressed subjects. Current approaches of treating depression are mainly based on assessment by clinicians or the description of depressed subjects, both of which may be subjective. With the fast advancement of computer vision methods, a list of methods are explored to study the depression severity. Present methods of recognising depression can be considered as hand-crafted  [4] -  [7]  and deep learning-based  [8] -  [12]  approaches. From the modality perspective, methods for depression recognition can be classified into audiovisual cues, social media data (Weibo, Twitter), and physiological and non-physiological signals  [13]  such as skin conductance, electroencephalography (EEG), magnetoencephalography (MEG), electrocardiography (ECG), heart sounds, respiration, and pulse signals. Despite promising performances achieved by current depression recognition methods, several challenges remain in effectively recognizing depression via multimodal signals. This is especially prevalent in the field of deep learning community, where a substantial quantity of data samples is necessary to train the available models. By reviewing the depression databases from recent decades  [2] , one observes small samples of publicly accessible data because of privacy protection. Furthermore, most of the available databases are collected in controlled laboratory environments via doctor-patient interactions; therefore behavioral patterns of the depressed subjects outside of the laboratory environment are missed  [14] . Although, Yoon et al.  [15]  introduced a D-vlog depression dataset containing 961 samples and providing facial landmarks and low level descriptors (LLDs).\n\nHence, to mitigate the above-mentioned major issues, a large-scale multimodal vlog database (LMVD) is built. In general, these vlogs are recorded and uploaded spontaneously by users, which motivate us to release this novel dataset to capture the potential patterns embedded in the vlogs of individuals navigating their daily lives. First, we obtain the arXiv:2407.00024v1 [cs.CV] 9 May 2024 vlogs from three Chinese multimedia platforms (Sina Weibo, Bilibili, and Tiktok) based on the following keywords related to depression (\"depressed category\": depression, my depressed life, and depressed vlog) in the depressed category and; \"health category:\" daily life, daily vlog, my vlog in the Non-depressed category. In addition, for each word, the following labels are also considered such as lower mood, loss of interest in many things, insomnia, and persistent thoughts of death or suicide. Secondly, we ask the volunteers to clean and count the unusable video clips, e.g., gender, duration time, etc. Thirdly, we ask the volunteers and clinicians to annotate the videos. In this stage, the volunteers first checked and annotated the quality of video vlogs, while clinicians verified the labels (right or not) and made the final decisions. After this, we decode the audio and Chinese text from the vlogs, and extract the audio features by using the pre-trained VGGish model  [16] , and extract the visual features, i.e., facial action units (FAUs), landmarks, eye gaze, and head pose. Finally, we introduced a multimodal depression recognition architecture to fuse the non-verbal behavioral features of the audio and video. In this architecture, a Transformer module to learn the potential characteristics, and an attentional multimodal feature fusion mechanism is proposed to capture the non-verbal patterns from the audiovisual features. To establish a baseline and open the dataset to researchers in the community of affective computing, we adopt both machine learning and deep learning methods; a great number of validations were performed on the collected dataset, obtaining excellent performances.\n\nThe novelties of this study are highlighted as follows:\n\n1) To encourage collaboration and to assist with future studies, the LMVD is publicly available 1 . Due to privacy protection, the raw vlogs and detailed information are not accessible. However, to assist the researchers, we provide the following data and features: raw audio signals, the learned features of VGGish, FAUs, landmarks, head pose, and eye gaze features. The proposed dataset has 1823 samples (214 hours) from 1475 participants. After reviewing the current studies on depression detection, we can confirm that our collected dataset is the first study in the field that can be used for depression detection. 2) To provide a benchmark dataset and generate a baseline in the affective computing community, machine learning and deep learning methods are adopted to perform the experimental validations. In addition, we also leverage the transformer and cross attention mechanism to learn the complementary non-verbal behaviors from the audiovisual features. Using LMVD, we obtain the values of 76.85%, 76.88%, 77.02%, 76.88% for the F1-score, accuracy, precision, recall, respectively. Moreover, the performances are illustrated to further showcase the effectiveness of MDDformer.\n\nThe remained of the present paper is structured as follows.\n\nSection II details previous study on COVID-19 detection. Our method is introduced in Section III. Section V discusses the experimental performance. Conclusions and future studies are planned in Section VI.\n\n1 https://github.com/helang818/LMVD/ II. RELATED WORKS\n\nIn this section, we offer a detailed explanation of the collected dataset and its relevance to multimodal depression detection by reviewing related works.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "A. The Depression Dataset",
      "text": "Because of the privacy protection related to studies on depression, data collection is very complicated. Consequently, various research teams have endeavored to record their own databases for depression estimation. This section examines a total of 21 databases, with only nine being accessible to the public. Table  I  shows the available audiovisual depression databases over the past 30 years. Since 1994, depression recognition gained attention from researchers, resulting in the release of a dataset by Becker et al  [17] .\n\nBased on Table  I",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "B. Multimodal Depression Detection",
      "text": "A series of studies, which utilize cues from audio, video, and text modalities, have been suggested to accurately evaluate the status of depression. Lam et al.  [35]  leverage topic modelling strategy to augment the size of the data and combine the power of a transformer mechanism with a 1D-CNN (1 dimension convolutional neural networks) to capture the patterns from acoustic features. Niu et al.  [36]  adopt combined spatiotemporal attention (STA) and multimodal attention feature fusion (MAFF) network for modeling the multimodal features.\n\nIn a later study  [37] , Ni et al. designed a hierarchical contextaware graph (HCAG) attention model that reflects layered information for the assessment of depression and employed a graph attention network (GAT) to discern contextual connections within the text/audio modalities. In the work of  [38] , a graph neural network-based semi-supervised domain adaptation (GNN-SDA) technique is presented to address the challenges associated with limited sample sizes and isolated data clusters. Pan et al.  [39]  present an audiovisual attention architecture named AVA-DepressNet, which focuses on privacy protection concern and an embedded attention-driven module for identifying depression. Moreover, an adversarial multistage (AMS) approach is formulated for refining the encoderdecoder framework, integrating knowledge of facial structures.\n\nIn 2024, a transformer-based structure was introduced from the video, audio, and remote photoplethysmographic (rPPG) cues for multimodal prediction of depression  [40] .",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Iii. Lmvd Depression Dataset",
      "text": "This section introduces the LMVD dataset, a large-scale multimodal dataset built \"in the wild\" for depression detection. We elaborate on the following aspects: (1) the motivation of this study, (2) the procedure of collecting the Chinese vlogs from the four media platforms, (3) the step for annotating the vlogs, (4) the step of prepossessing the details, and (  5 ) the extraction of audio and visual features.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "A. Motivation",
      "text": "As reported in Table  I , the limitations of the available datasets motivated us to build a large-scale dataset for depression detection in individuals navigating their daily lives. This large-scale dataset offers several advantages:\n\n1) Promotes Research and Applications: the collection of LMVD will boost both research endeavors and clinical scenarios in the community of automatic depression detection. 2) Benefits Various Stakeholders: the developed prototype system offers an efficient solution applicable to various sectors, including government agencies, hospitals, and universities.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "B. Data Collection",
      "text": "Our goal is to build a large-scale dataset for multimodal depression detection in individuals navigating their daily lives. To achieve this, we aim to collect depression and non-depression vlogs from different platforms with similar content distribution. Therefore, we collect the vlog videos from three Chinese multimedia platforms (Sina Weibo, Bilibili, and Tiktok). Data was collected from 1st Jan 2019 to 30th October 2023 using the following keywords (in Chinese):\n\n1) Depressed category: depression, my depressed life, and depressed vlog. 2) Non-depressed category: daily Life, daily vlog, my vlog. In addition to the Chinese platforms, we collect vlogs from YouTube using the same keywords in the same period. The collection process followed a similar approach. By collecting data from both Chinese and English platforms, we aimed to enhance the diversity and generalized ability of the LMVD dataset.\n\nAs shown in Table  II , the two platforms have balanced samples except for Sina Weibo. This is because many users adopt Sina Weibo to post text messages about their feelings,  encompassing their lives, careers, and emotions. This resulted in 65 and 214 samples for the Depressed and Non-depressed. In total, there are 2303 and 2357 samples for the Depressed and Non-depressed categories, respectively. Bilibili, TikTok, and Sina Weibo have 2184, 2197, and 279 samples, respectively. In total, 4660 vlogs from the three Chinese multimedia platforms are used. In addition, the vlogs from YouTube are of good quality and are used directly in this work.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Data Annotation",
      "text": "To perform the annotation for the depression and health vlogs, four master and ten undergraduate students are recruited. First, we ask the ten undergraduate students to check whether the vlogs have faces and if the audio is synthesized. As shown in Table  III , we obtained 1823 data samples for the LMVD. For the Depressed and Non-depressed categories, the number of vlogs is 908 and 915, respectively. Secondly, we ask the four master's students to recheck the vlogs to ensure the quality is sufficient for training the deep models for multimodal depression detection. Then we assign the 1823 vlogs to the ten undergraduate students who assigned them to either the Depressed or Non-depressed categories. Finally, the master students check the labels assigned by the undergraduate students.\n\nTable  IV  shows the duration of vlogs from different platforms. Bilibili exhibits the most significantly higher time spent between the Depressed and Non-depressed category. As a matter of fact, it shows the highest disparity between the two categories among all listed platforms. TikTok shows less time spent in both categories compared to Bilibili, but still has a considerable amount of time spent by users on the platform. The ratio of time spent between Depressed and Non-depressed is less than Bilibili, indicating a more balanced usage among the two categories. Sina Weibo displays a trend wherein users in the Non-depressed category spend more than twice the amount of time compared to those in the Depressed category. This implies that Sina Weibo is predominantly utilized by users who are not classified as depressed. YouTube has the closest time spent between the two categories, which might suggest a more uniform distribution of usage across users categorized as Depressed and Non-depressed.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "D. Multimodal Feature Extraction",
      "text": "To establish a foundational benchmark for the field of depression recognition, we employ the primary features commonly used.\n\nFor audio features, the pre-trained VGGish  [41]  model is adopted. This is because the traditional hand-crafted features have the following limitations: (1) professional knowledge is often needed to design the discriminative features, and (2) additional valuable patterns may be lost in developing the deep features.\n\nFor visual features, FAU, facial landmarks, eye gaze, and head pose features are adopted.\n\n1) Facial Action Units (FAUs): We focus on a subset of 17 AUs (AU01, AU02, AU04, ..., AU45) that have been linked to emotional expression. These features represent specific muscle movements in the face that can provide insights into a person's emotional state. 2) Facial Landmarks: We extract facial landmarks (see Fig.  1 ) to represent the key points of the participant's facial structure. Facial landmarks are robust for capturing facial muscle movements, making them valuable features for tasks like emotion analysis, depression detection, and facial action unit detection. 3) Eye Gaze: We extract eye gaze features to describe the direction of a participant's gaze. This information is represented by four sets of eye movement feature vectors with 12 dimensions each. The first two sets define the direction of eye movements in the coordinate space, while the latter two sets define the direction based on the head coordinate space. The values \"0\" and \"1\" represent the left and right eyes, respectively. 4) Head Pose: We extract head pose features to capture the position and rotational direction of the participant's head. These features are represented by a 6-dimensional vector.",
      "page_start": 4,
      "page_end": 5
    },
    {
      "section_name": "Iv. Methods",
      "text": "In this section, we describe the pipeline for multimodal depression detection using the LMVD. We first provide a brief overview of the pipeline, followed by a detailed description of the multimodal depression detection method.\n\nA. Architecture Overview Fig.  2  illustrates the proposed MDDformer. First, the audio features are extracted by the VGGish architecture. For the visual cue, AUs, head pose, landmarks, and eye gaze features are extracted by TCN architecture. Then, the CFformer can adopt the advantages of cross fusion for learning behaviors from the audiovisual cues. Finally, two fully connected layers and the softmax function are performed to predicting the depression.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Multimodal Depression Detection",
      "text": "1) Baseline Model: Over the past decades, deep learning methods have gathered significant attention across various tasks. Consequently, to establish a benchmark in the field of depression detection, we adopt both traditional machine learning methods and several deep learning techniques. Specifically, K-nearest neighbor (KNN)  [42]  is employed to address two category problems, serving as the baseline model for multimodal depression detection. More information on KNN can be found in  [42] .\n\n2) Structure of MDDformer: To effectively detect the discriminative patterns within audiovisual cues, MDDformer is proposed. Let's define the audio feature as X a ∈ R N ×Da , and the video feature as X v ∈ R N ×Dv before inputting into the MDDformer. Here, D a represents the dimension of audio, D v represents the dimension of video, and N denotes the length of sequences.\n\nInitially, the transformer architecture  [43]  is proposed to model therelationships in natural language processing (NLP) tasks, which consists of an encoder-decoder structure. Both the encoder and decoder are composed of multiple identical layers, each containing two main sub-modules,i.e., multi-head self-attention and position-wise feed-forward networks. In our task, to fuse the patterns from audio and video branch, an MDDformer is proposed.\n\nThe input audio feature X a ∈ R N ×Da is mapped to three matrices by three linear transformations, i.e., key K a , query Q a , and value V a .\n\nwhere W Qa , W Ka , and W Va denotes the weights of linear transformation. The video feature X v ∈ R N ×Dv can be mapped to three matrices by three linear transformations, i.e., key\n\nwhere W Qv , W Kv , and W Vv denotes the weights of linear transformation.\n\nNext, Q a is multiplied with K T a to generate the feature\n\nThen we concatenate F v and F a to generate the feature map, i.e., F av :\n\nThe self attention of the audio branch can be expressed as:\n\nwhere d k is the dimension of the F av matrix. The self attention of the visual branch can be expressed as:\n\nwhere d k is the dimension of the F av matrices. Then, we concatenate the outputs of each head and reshape them to add with the feature X a ∈ R N ×Da ,generating the fusion feature.\n\nwhere h is the heads of the multi-head self-attention and W a and W v are weight matrices.\n\nFollowing the concatenate operation, F f is then input to the feed-forward network, adopting the add/norm operation to generate the feature F n :\n\nwhere N orm is the normalization operation. Following the add and layer normalization, two fully connected layers with ELU activation, and dropout operation are performed on F n , represented by F p :\n\nwhere Dropout, ELU , F C, and Sof tmax represent dropout, activation, fully connected layer, and softmax function, respectively.\n\nFinally, cross entropy mechanism is adopted as the loss function:\n\nwhere N denotes the list of samples, y i is the label (depression and non-depression), and p i is the predicted value (between 0 and 1).",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "V. Results",
      "text": "We elaborate on the details the experimental setup and describe the MDDformer performances in this section.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "A. Experimental Setup",
      "text": "We implement and trained the MDDformer model using the PyTorch deep learning toolkit. A 10-fold cross-validation is performed for validating the efficiency of MDDformer. The Adam optimizer is set to β = (0.9, 0.999) and ϵ = 1e8 with the batch size of 4. The initial training learning rate size is 0.00001 and then updated with CosineAnnealingLR decay. To overcome the overfitting problem, a dropout of 0.2 is adopted in the linear layers and the total epochs is set to 300. Our architecture is evaluated on four NVIDIA Tesla V100-DGX with 32GB.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "B. Evaluation Metrics",
      "text": "In general, the classification performance is mainly evaluated using five metrics for binary classification problems: accuracy, precision, recall (also known as sensitivity), specificity, and F1-score. Here, true positive (TP) indicates samples with positive labels that are correctly predicted as positive. Similarly, true negative (TN), false positive (FP), and false negative (FN) respectively represent samples correctly predicted as negative, incorrectly predicted as positive, and incorrectly predicted as negative, respectively. The formula can be expressed as:\n\nP recision = T P T P + F P (11)\n\nF 1 -score = 2 × P recision × Recall P recision + Recall (13)\n\n1) Performances of Baseline Methods: To further validate the performances of the MDDformer, several machine learning and deep learning architectures are adopted i.e., KNN, SVM, LR, RF, Xception, ViT, BiLSTM, and SEResnet. To make a fair comparison, the weighted accuracy, precision, recall, and F1-score are adopted. As shown in Table V, the MDDformer obtains the best performance in term of the evaluation metrics. The terms \"add\" and \"concat\" represent the addition and concatenate operation, respectively. In our task, we list the models in ascending order according of accuracy. One can   V ). Fig.  3  provides the confusion matrix for the MDDformer with other baseline architectures. The MDDformer obtains the best performances regarding the classification of depression and non-depression. Note that each row represents the true labels, and each column represents the predicted values.\n\nFrom Fig.  4 , it is evident that the classification results of the MDDformer stand out compared to those of other models. The MDDformer yields clearer and more uniform results, indicating its superior performance in classification effectiveness over other methods. Fig.  5  presents the classification performances using a bar chart. Each model is depicted by a group of four bars, each corresponding to one of the evaluation metrics. From left to right, the models are KNN, SVM, LR, RF, Xception (add), ViT (add), Xception (concat), BiLSTM (add), SEResnet (concat), BiLSTM (concat), SEResnet (add), ViT (concat), and the MDDformer. The MDDformer is represented by the last group of bars, displaying performance measures consistently around the 76-77% mark for all four metrics. Notably, the performance of the MDDformer surpasses that of the other baseline models depicted in the chart.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "Vi. Conclusion",
      "text": "In this article, we collect a large-scale vlog dataset, i.e., the LMVD, for depression recognition among individuals navigating their daily lives based on audiovisual cues. The proposed dataset has 1823 samples (214 hours) from 1475 participants. To performance with the MDDformer, i.e., KNN, SVM, LR, RF, Xception, BiLSTM, SEResnet, and ViT. More importantly, our LMVD is the largest dataset for audiovisual depression recognition in individuals navigating their daily lives, which is a positive contribution to the affective computing field. In the future, we will augment the dataset and explore non-verbal behaviors for multimodal depression recognition.",
      "page_start": 7,
      "page_end": 7
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: ) to represent the key points of the participant’s facial",
      "page": 4
    },
    {
      "caption": "Figure 1: Illustration of the 68 key points for facial landmarks feature.",
      "page": 5
    },
    {
      "caption": "Figure 2: illustrates the proposed MDDformer. First, the audio",
      "page": 5
    },
    {
      "caption": "Figure 2: The MDDformer model comprises three main steps: (a) Multimodal Feature Extraction: For the audio cue, the deep features are extracted by",
      "page": 6
    },
    {
      "caption": "Figure 3: provides the confusion matrix for the MDDformer",
      "page": 7
    },
    {
      "caption": "Figure 4: , it is evident that the classification results",
      "page": 7
    },
    {
      "caption": "Figure 5: presents the classification performances using a bar",
      "page": 7
    },
    {
      "caption": "Figure 3: Confusion matrix of the MDDformer and other baseline methods. Each row represents the true labels, and each column represents the predicted",
      "page": 8
    },
    {
      "caption": "Figure 4: Visualisation of the multimodal features using 3D t-SNE. The red dots represent data from depressed subjects, while the gray dots represent data",
      "page": 9
    },
    {
      "caption": "Figure 5: The grouped bar chart for the different baseline methods and",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Linear V V\nLineaLrinear K\nLineaLrinear Q Q\nConcat V\nLineaLrinear Q\nLinear K": "t\nLineaLrinear Q\nLinear K",
          "Column_2": "",
          "Column_3": ""
        }
      ],
      "page": 6
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Prevalence of stress, anxiety, depression among the general population during the covid-19 pandemic: a systematic review and meta-analysis",
      "authors": [
        "N Salari",
        "A Hosseinian-Far",
        "R Jalali",
        "A Vaisi-Raygani",
        "S Rasoulpoor",
        "M Mohammadi",
        "S Rasoulpoor",
        "B Khaledi-Paveh"
      ],
      "year": "2020",
      "venue": "Globalization and health"
    },
    {
      "citation_id": "2",
      "title": "Deep learning for depression recognition with audiovisual cues: A review",
      "authors": [
        "L He",
        "M Niu",
        "P Tiwari",
        "P Marttinen",
        "R Su",
        "J Jiang",
        "C Guo",
        "H Wang",
        "S Ding",
        "Z Wang"
      ],
      "year": "2022",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "3",
      "title": "Risk factors for suicide in individuals with depression: a systematic review",
      "authors": [
        "K Hawton",
        "C Comabella",
        "C Haw",
        "K Saunders"
      ],
      "year": "2013",
      "venue": "Journal of affective disorders"
    },
    {
      "citation_id": "4",
      "title": "AVEC2013: the continuous audio/visual emotion and depression recognition challenge",
      "authors": [
        "M Valstar",
        "B Schuller",
        "K Smith",
        "F Eyben",
        "B Jiang",
        "S Bilakhia",
        "S Schnieder",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2013",
      "venue": "Emotion Challenge"
    },
    {
      "citation_id": "5",
      "title": "AVEC 2014: 3D dimensional affect and depression recognition challenge",
      "authors": [
        "M Valstar",
        "B Schuller",
        "K Smith",
        "T Almaev",
        "F Eyben",
        "J Krajewski",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2014",
      "venue": "Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge"
    },
    {
      "citation_id": "6",
      "title": "AVEC 2016: Depression, mood, and emotion recognition workshop and challenge",
      "authors": [
        "M Valstar",
        "J Gratch",
        "B Schuller",
        "F Ringeval",
        "D Lalanne",
        "M Torres",
        "S Scherer",
        "G Stratou",
        "R Cowie",
        "M Pantic"
      ],
      "year": "2016",
      "venue": "AVEC 2016: Depression, mood, and emotion recognition workshop and challenge"
    },
    {
      "citation_id": "7",
      "title": "Automatic depression analysis using dynamic facial appearance descriptor and dirichlet process fisher encoding",
      "authors": [
        "L He",
        "D Jiang",
        "H Sahli"
      ],
      "year": "2018",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "8",
      "title": "Automated depression diagnosis based on deep networks to encode facial appearance and dynamics",
      "authors": [
        "Y Zhu",
        "Y Shang",
        "Z Shao",
        "G Guo"
      ],
      "year": "2017",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "9",
      "title": "Visually interpretable representation learning for depression recognition from facial images",
      "authors": [
        "X Zhou",
        "K Jin",
        "Y Shang",
        "G Guo"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "10",
      "title": "Automatic depression recognition using CNN with attention mechanism from videos",
      "authors": [
        "L He",
        "J -W. Chan",
        "Z Wang"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "11",
      "title": "Reducing noisy annotations for depression estimation from facial images",
      "authors": [
        "L He",
        "P Tiwari",
        "C Lv",
        "W Wu",
        "L Guo"
      ],
      "year": "2022",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "12",
      "title": "An improved globallocal fusion network for depression detection telemedicine framework",
      "authors": [
        "L Zhang",
        "J Zhao",
        "L He",
        "J Jia",
        "X Meng"
      ],
      "year": "2023",
      "venue": "IEEE Internet of Things Journal"
    },
    {
      "citation_id": "13",
      "title": "An insight into diagnosis of depression using machine learning techniques: a systematic review",
      "authors": [
        "S Bhadra",
        "C Kumar"
      ],
      "year": "2022",
      "venue": "Current Medical Research and Opinion"
    },
    {
      "citation_id": "14",
      "title": "Domain adaptation for enhancing speech-based depression detection in natural environmental conditions using dilated cnns",
      "authors": [
        "Z Huang",
        "J Epps",
        "D Joachim",
        "B Stasak",
        "J Williamson",
        "T Quatieri"
      ],
      "year": "2020",
      "venue": "Domain adaptation for enhancing speech-based depression detection in natural environmental conditions using dilated cnns"
    },
    {
      "citation_id": "15",
      "title": "D-vlog: Multimodal vlog dataset for depression detection",
      "authors": [
        "J Yoon",
        "C Kang",
        "S Kim",
        "J Han"
      ],
      "year": "2022",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "16",
      "title": "Youtube-8m: A large-scale video classification benchmark",
      "authors": [
        "S Abu-El-Haija",
        "N Kothari",
        "J Lee",
        "P Natsev",
        "G Toderici",
        "B Varadarajan",
        "S Vijayanarasimhan"
      ],
      "year": "2016",
      "venue": "Youtube-8m: A large-scale video classification benchmark",
      "arxiv": "arXiv:1609.08675"
    },
    {
      "citation_id": "17",
      "title": "The natural history of cognitive decline in Alzheimer's disease",
      "authors": [
        "J Becker",
        "F Boiler",
        "O Lopez",
        "J Saxton",
        "K Mcgonigle"
      ],
      "year": "1994",
      "venue": "Archives of Neurology"
    },
    {
      "citation_id": "18",
      "title": "The speech analysis approach to determining onset of improvement under antidepressants",
      "authors": [
        "H Stassen",
        "S Kuny",
        "D Hell"
      ],
      "year": "1998",
      "venue": "European Neuropsychopharmacology"
    },
    {
      "citation_id": "19",
      "title": "Acoustical properties of speech as indicators of depression and suicidal risk",
      "authors": [
        "D France",
        "R Shiavi",
        "S Silverman",
        "M Silverman",
        "M Wilkes"
      ],
      "year": "2000",
      "venue": "IEEE Transactions on Biomedical Engineering"
    },
    {
      "citation_id": "20",
      "title": "Reflections of depression in acoustic measures of the patient's speech",
      "authors": [
        "M Alpert",
        "E Pouget",
        "R Silva"
      ],
      "year": "2001",
      "venue": "Journal of Affective Disorders"
    },
    {
      "citation_id": "21",
      "title": "Comparing objective feature statistics of speech for classifying clinical depression",
      "authors": [
        "E Moore",
        "M Clements",
        "J Peifer",
        "L Weisser"
      ],
      "year": "2004",
      "venue": "Engineering in Medicine and Biology Society, 2004. IEMBS'04. 26th Annual International Conference of the IEEE"
    },
    {
      "citation_id": "22",
      "title": "Objective estimation of suicidal risk using vocal output characteristics",
      "authors": [
        "T Yingthawornsuk",
        "H Keskinpala",
        "D France",
        "D Wilkes",
        "R Shiavi",
        "R Salomon"
      ],
      "year": "2006",
      "venue": "Ninth International Conference on Spoken Language Processing"
    },
    {
      "citation_id": "23",
      "title": "Detecting depression from facial actions and vocal prosody",
      "authors": [
        "J Cohn",
        "T Kruez",
        "I Matthews",
        "Y Yang",
        "M Nguyen",
        "M Padilla",
        "F Zhou",
        "F De"
      ],
      "year": "2009",
      "venue": "2009 3rd International Conference on Affective Computing and Intelligent Interaction and Workshops"
    },
    {
      "citation_id": "24",
      "title": "Video-based detection of the clinical depression in adolescents",
      "authors": [
        "N Maddage",
        "R Senaratne",
        "L.-S Low",
        "M Lech",
        "N Allen"
      ],
      "year": "2009",
      "venue": "EMBC 2009. Annual International Conference of the IEEE"
    },
    {
      "citation_id": "25",
      "title": "From joyous to clinically depressed: Mood detection using spontaneous speech",
      "authors": [
        "S Alghowinem",
        "R Goecke",
        "M Wagner",
        "J Epps",
        "M Breakspear",
        "G Parker"
      ],
      "year": "2012",
      "venue": "Proceedings of the Twenty-Fifth International Florida Artificial Intelligence Research Society Conference"
    },
    {
      "citation_id": "26",
      "title": "Prediction of clinical depres-sion in adolescents using facial image analysis",
      "authors": [
        "K Ooi",
        "L Low",
        "M Lech",
        "N Allen"
      ],
      "year": "2011",
      "venue": "WIAMIS 2011: 12th International Workshop on Image Analysis for Multimedia Interactive Services"
    },
    {
      "citation_id": "27",
      "title": "Vocal acoustic biomarkers of depression severity and treatment response",
      "authors": [
        "J Mundt",
        "A Vogel",
        "D Feltner",
        "W Lenderking"
      ],
      "year": "2012",
      "venue": "Biological Psychiatry"
    },
    {
      "citation_id": "28",
      "title": "Visualizations for Mental Health Topic Models",
      "authors": [
        "G Chen"
      ],
      "year": "2014",
      "venue": "Visualizations for Mental Health Topic Models"
    },
    {
      "citation_id": "29",
      "title": "The distress analysis interview corpus of human and computer interviews",
      "authors": [
        "J Gratch",
        "R Artstein",
        "G Lucas",
        "G Stratou",
        "S Scherer",
        "A Nazarian",
        "R Wood",
        "J Boberg",
        "D Devault",
        "S Marsella"
      ],
      "year": "2014",
      "venue": "LREC"
    },
    {
      "citation_id": "30",
      "title": "Tackling mental health by integrating unobtrusive multimodal sensing",
      "authors": [
        "D Zhou",
        "J Luo",
        "V Silenzio",
        "Y Zhou",
        "J Hu",
        "G Currier",
        "H Kautz"
      ],
      "year": "2015",
      "venue": "AAAI"
    },
    {
      "citation_id": "31",
      "title": "Unipolar depres-sion vs. bipolar disorder: An elicitation-based approach to short-term detection of mood disorder",
      "authors": [
        "K.-Y Huang",
        "C.-H Wu",
        "Y.-T Kuo",
        "F.-L Jang"
      ],
      "year": "2016",
      "venue": "Unipolar depres-sion vs. bipolar disorder: An elicitation-based approach to short-term detection of mood disorder"
    },
    {
      "citation_id": "32",
      "title": "Dynamic multimodal measurement of depression severity using deep autoencoding",
      "authors": [
        "H Dibeklioglu",
        "Z Hammal",
        "J Cohn"
      ],
      "year": "2018",
      "venue": "IEEE Journal of Biomedical and Health Informatics"
    },
    {
      "citation_id": "33",
      "title": "MODMA dataset: a multi-model open dataset for mental-disorder analysis",
      "authors": [
        "H Cai",
        "Y Gao",
        "S Sun",
        "N Li",
        "F Tian",
        "H Xiao",
        "J Li",
        "Z Yang",
        "X Li",
        "Q Zhao"
      ],
      "year": "2020",
      "venue": "MODMA dataset: a multi-model open dataset for mental-disorder analysis",
      "arxiv": "arXiv:2002.09283"
    },
    {
      "citation_id": "34",
      "title": "Semi-structural interview-based chinese multimodal depression corpus towards automatic preliminary screening of depressive disorders",
      "authors": [
        "B Zou",
        "J Han",
        "Y Wang",
        "R Liu",
        "S Zhao",
        "L Feng",
        "X Lyu",
        "H Ma"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "35",
      "title": "Context-aware deep learning for multi-modal depression detection",
      "authors": [
        "G Lam",
        "H Dongyan",
        "W Lin"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "36",
      "title": "Multimodal spatiotemporal representation for automatic depression level detection",
      "authors": [
        "M Niu",
        "J Tao",
        "B Liu",
        "J Huang",
        "Z Lian"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "37",
      "title": "HCAG: A hierarchical contextaware graph attention model for depression detection",
      "authors": [
        "M Niu",
        "K Chen",
        "Q Chen",
        "L Yang"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "38",
      "title": "Semi-supervised domain adaptation for major depressive disorder detection",
      "authors": [
        "T Chen",
        "Y Guo",
        "S Hao",
        "R Hong"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Multimedia"
    },
    {
      "citation_id": "39",
      "title": "Integrating deep facial priors into landmarks for privacy preserving multimodal depression recognition",
      "authors": [
        "Y Pan",
        "Y Shang",
        "Z Shao",
        "T Liu",
        "G Guo",
        "H Ding"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Transformer-based multimodal feature enhancement networks for multimodal depression detection integrating video, audio and remote photoplethysmograph signals",
      "authors": [
        "H Fan",
        "X Zhang",
        "Y Xu",
        "J Fang",
        "S Zhang",
        "X Zhao",
        "J Yu"
      ],
      "year": "2024",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "41",
      "title": "Cnn architectures for large-scale audio classification",
      "authors": [
        "S Hershey",
        "S Chaudhuri",
        "D Ellis",
        "J Gemmeke",
        "A Jansen",
        "R Moore",
        "M Plakal",
        "D Platt",
        "R Saurous",
        "B Seybold"
      ],
      "year": "2017",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "42",
      "title": "K-nearest neighbor",
      "authors": [
        "L Peterson"
      ],
      "year": "2009",
      "venue": "Scholarpedia"
    },
    {
      "citation_id": "43",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Advances in neural information processing systems"
    }
  ]
}