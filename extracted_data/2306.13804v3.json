{
  "paper_id": "2306.13804v3",
  "title": "Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers",
  "published": "2023-06-23T22:38:32Z",
  "authors": [
    "Syed Aun Muhammad Zaidi",
    "Siddique Latif",
    "Junaid Qadir"
  ],
  "keywords": [
    "Speech emotion recognition",
    "multi-modal learning",
    "co-attention networks",
    "graph attention networks"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pretrained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross-language SER results using minimal target language data. In addition, our model also exploits a transformer encoder layer for high-level feature representation to improve emotion classification accuracy. In this way, MDAT performs refinement of feature representation at various stages and provides emotional salient features to the classification layer. This novel approach also ensures the preservation of modality-specific emotional information while enhancing cross-modality and cross-language interactions. We assess our model's performance on four publicly available SER datasets and establish its superior effectiveness compared to recent approaches and baseline models.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "I. Introduction",
      "text": "S PEECH Emotion Recognition (SER), involves the ap- plication of machine learning (ML) systems to identify and classify emotions expressed in human speech, has recently garnered significant attention  [1] . SER demonstrates a wide range of potential applications across various domains, presenting opportunities in healthcare, transportation services, forensic investigations, education, and media industries  [2] -  [6] . Given its complex nature, modelling human emotions in speech relies on multiple factors such as the speaker, gender, age, culture, and dialect  [7] -  [11] . To tackle this complexity, researchers have explored diverse ML techniques, including deep neural networks (DNNs) like Deep Belief Networks (DBN), Convolutional Neural Networks (CNNs), and Recurrent Neural Networks (RNNs) for SER  [12] -  [15] . Among these, RNN architectures such as Long Short-Term Memory (LSTM) networks or Bidirectional LSTM (BiLSTM) combined with CNNs have been widely explored and found to be effective in modelling emotional context in speech  [16] ,  [17] . However, RNN-based models have some limitations in capturing long-term dependencies and modelling context in speech  [18] -  [20] .\n\nTransformers have been a major breakthrough in the field of natural language processing (NLP) and speech processing. They have revolutionized the way intricate relationships among different elements of input sequences are learned by leveraging self-attention mechanisms. Their impact has been remarkable, leading to transformative outcomes in machine translation, natural language understanding, and speech recognition tasks  [21] . This paper introduces a transformerbased multimodal technique that uses speech and text data for cross-language SER using a dual-attention mechanism. By leveraging state-of-the-art pre-trained models for speech and text modalities, our technique enhances cross-lingual SER performance models through the use of a dual attention mechanism to capture complex multimodal representations across different languages.\n\nExisting SER models mostly rely on within-corpus analysis, where they are trained and tested on the same dataset or language  [22] . Consequently, their generalisation and applicability to different datasets or languages are hindered. This limitation is particularly evident in the case of low-resource languages like Urdu, which lack sufficient labeled data for pre-training or fine-tuning. There is a pressing need for crosslingual models capable of performing effectively across multiple languages, leveraging the abundant data available from diverse sources. Our approach aims to fulfil this demand and contribute to the advancement of cross-lingual SER research.\n\nA promising methodology for tackling the challenging problem of SER for low-resource languages is to employ multimodal data that capitalises on the abundant and varied cues for emotion recognition present in the complementary modalities. In this context, speech encompasses prosodic and acoustic features of emotion (e.g., pitch, intensity, and tone), while text encompasses lexical and semantic features of emotion (e.g., words, phrases, and sentiments). The inclusion of the text modality in SER is crucial as it can offer supplementary or alternative information to the speech modality in certain scenarios. For instance, when the speech signal is contaminated with noise or distortion, the text modality can provide a clearer and more dependable source of information. Additionally, the text modality can capture emotional expressions that may not be conveyed through speech, such as sarcasm, irony, or humor. The complementary nature of text and speech modalities can help enhance the accuracy of SER models, particularly for low-resource languages.\n\nDespite the promise of combining audio and text modalities for SER, their integration into multimodal models remains largely unexplored with very few existing works  [23] ,  [24] . Various text-based models, such as transformer-based ones like BERT, RoBERTa, and T5  [25] -  [27] , can be used to learn powerful semantic representations from large-scale text corpora and transfer them to SER tasks. However, the utilization of pre-trained models for both speech and text modalities remains underexplored in SER literature, with current works  [23] ,  [24]  mostly relying on hand-crafted or shallow features for speech and text modalities. Our hypothesis is that by leveraging the complementary nature of pre-trained models for speech and text modalities, we can greatly enhance the generalisation of SER models for cross-language settings, paving the way for promising avenues of further investigation and improvement in this field. This paper presents a novel multimodal model that combines RoBERTa embeddings for text and wav2vec 2.0 for speech. We utilize XLS-R  [28] , a multilingual extension of wav2vec 2.0 pretrained on speech data from 128 languages, enabling cross-lingual speech representation learning. Additionally, we employ RoBERTa  [26] , a multilingual variant of BERT  [25] , for text analysis. Our model incorporates dual attention mechanism leverages (1) graph attention to capture the complex dependencies between different parts of the modalities, such as words, phrases, or speech segments  [29] ; as well as (2) coattention to capture the cross-modality interactions between speech and text modalities  [30] . By combining graph attention and co-attention, our model can effectively preserve modalityspecific information while enhancing the integration of crossmodality information.\n\nThe major contributions of this work are outlined next. Organisation of this paper. Section II provides an overview of the related work. Section III describes our methodology and provides details of our proposed model architecture and the baseline model architecture. Section IV provides details about the experimental setup and the datasets used. Section V provides details of our experiments and results. Finally, Section VI concludes the paper.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Ii. Related Work",
      "text": "In this paper, we present a novel framework that combines transformer-based multimodal learning with cross-language SER. This section reviews the related work on various aspects of our technique, such as cross-lingual SER, multimodal SER, and transformer-based SER approaches. We also highlight the differences and contributions of our proposed framework compared to the existing methods.\n\nA. Cross-Language SER SER addresses the task of recognizing emotions from speech signals in diverse languages, with practical applications in healthcare, education, and social media  [2] ,  [5] ,  [6] . The main challenges include limited labelled data for low-resource languages like Urdu  [10] , Persian  [31] , or Marathi  [32] , and domain mismatch between different speech emotion corpora  [33] -  [35] . In addition, many existing SER models are trained on a single language or corpus, hence they suffer from limited applicability and generalisation. Cross-language SER presents a more realistic and challenging scenario, expanding the potential of SER for broader languages.\n\nRecent works on cross-language SER attempt to address the challenges of data scarcity and domain mismatch by using various methods including feature selection  [36] , domain adaptation  [37] , data augmentation  [38] ,  [39] , and multimodal fusion  [40] . Feature selection methods aim to select the most relevant and discriminative features for emotion recognition from different modalities, such as acoustic, linguistic, or prosodic features  [41] . Domain adaptation methods aim to reduce the distribution mismatch between the source and target domains by using techniques such as adversarial learning  [42] , transfer learning  [10] , or meta-learning  [43] . Data augmentation methods aim to increase the diversity and size of the training data by applying various transformations, such as speech synthesis  [38] , noise injection  [44] , or pitch shifting  [45] . Multimodal fusion methods aim to combine speech and text data to enhance emotion recognition accuracy by using techniques such as attention mechanisms  [46] , tensor fusion  [47] , or graph neural networks  [48] . These methods have shown promising results on various benchmark datasets, such as IEMOCAP  [49] , MELD  [50] , or MOSI  [51] .\n\nSome studies also utilise unsupervised pre-training  [52]  or self-supervised pre-training  [53] ,  [54]  to learn general representations that can transfer across languages or domains. Additionally, meta-learning has been proposed as a promising approach to train models that can quickly adapt to new languages or tasks with few labelled examples  [31] . Moreover, synthetic data and data augmentation techniques, such as speech synthesis  [55] -  [57]  or noise injection  [58] , can help alleviate the scarcity of labelled data for low-resource languages.\n\nMost of the above mentioned studies also have some limitations, such as relying on hand-crafted features, requiring large amounts of labelled data, or ignoring the complex dependencies between different modalities. Therefore, there is still room for improvement in cross-language SER by developing more effective methods that can leverage the stateof-the-art pre-trained models, learn general and comprehensive representations, and capture the cross-modality interactions.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "B. Transformers In Ser",
      "text": "Transformers are a type of neural network architecture that has revolutionised NLP  [59] . Transformers rely on selfattention mechanisms to compute representations of the input and output data without using sequence-aligned RNNs or CNNs  [60] . Transformers can handle long-range dependencies and parallelise the computation better than RNNs or CNNs, which makes them more efficient and accurate for various NLP tasks, such as machine translation, text summarisation, question answering, and natural language understanding  [61] ,  [62] . Transformers are playing an increasingly prominent role in speech technology, particularly in the field of speech emotion recognition  [21] . They excel at harnessing the abundant semantic and acoustic information inherent in speech data to capture intricate interactions between various modalities, such as audio and text, and fully leverage the capabilities of these modalities when they are available  [63] ,  [64] .\n\nSeveral recent works use transformers for SER. For example, Chen et al.  [63]  propose a key-sparse transformer for multimodal SER that focuses more on emotion-related information and reduces the noise from redundant information. Wagner et al.  [65]  conduct a thorough analysis of the influence of model size and pre-training data on the downstream performance of transformers for SER. They found that larger models and more diverse pre-training data lead to better results, especially for valence prediction. They also showed that transformers can learn implicit linguistic information from speech signals that can improve their valence predictions. Zenkov et al.  [66]  build two parallel CNNs in parallel with a transformer encoder network to classify emotions from speech data. They used the RAVDESS dataset to classify emotions from one of eight classes. Li et al.  [67]  propose a speech emotion recognition transformer (SERT) that uses a multi-head self-attention mechanism to model the temporal dynamics of speech signals. They achieved state-of-the-art results on three Engligh language benchmark datasets including IEMOCAP  [49] , MSP-Podcast  [68] , and MOSI  [51] . Triantafyllopoulos et al.  [69]  probed speech emotion recognition transformers for linguistic knowledge and found that they are very reactive to positive and negative sentiment content, as well as negations.\n\nIn contrast to previous works, we employ a transformer layer as part of our multimodal model for cross-language SER. A transformer encoder layer consists of two sub-layers: a multi-head self-attention layer and a feed-forward network layer  [60] . We use the transformer layer after dual attention mechanisms to learn high-level feature extraction to improve the classification accuracy. The transformer encoder layer can enhance the representation learning by capturing the longrange dependencies and non-linear transformations within and across the modalities.",
      "page_start": 3,
      "page_end": 4
    },
    {
      "section_name": "C. Multimodal Ser",
      "text": "Multimodal SER involves the identification of human affective states from speech data using multiple sources of information, including audio, text, visual, and paralinguistic features. This approach has demonstrated superior accuracy and robustness compared to single-modality approaches  [82] -  [85] . Nevertheless, there are several challenges associated with multimodal emotion recognition, such as feature extraction difficulties  [86] , feature alignment complexities  [87] , fusion techniques  [88] , dealing with missing or noisy data  [86] . Therefore, we need more advanced methods that can exploit the information from multimodal data in an effective way. This can enhance multimodal SER by integrating multiple modalities, and provide us with a richer understanding of human emotions.\n\nSeveral methods are proposed for multimodal SER to improve performance by using pre-trained models for feature extraction. Makiuchi et al.  [89]  propose a cross-representation speech model that combines self-supervised high-level features extracted from raw audio waveforms with text-based features extracted with Transformer-based models. They achieve stateof-the-art results on the IEMOCAP dataset using a score fusion approach, but their method may require fine-tuning for different languages or domains. Tang et al.  [90]  propose a feature fusion method based on facial expression and speech using attention mechanisms, which showed improved accuracy on the RAVDESS dataset, but may not generalise well to other datasets or modalities. Yoon et al.  [91]  propose a deep dual recurrent encoder model that uses text data and audio signals simultaneously to better understand speech data. Their model outperforms previous state-of-the-art methods in emotion classification on the IEMOCAP dataset but may not capture long-term dependencies or context information and depend on the quality of the text transcripts.\n\nDifferent recent works have proposed a novel fusion techniques for multimodal SER that use hybrid transformer models  [63] ,  [65] ,  [92] . Hybrid transformer models are models that combine different types of transformer architectures, such as encoder-decoder, encoder-only, or decoder-only, to achieve better performance on multimodal tasks  [63] . For example, Chen et al.  [63]  propose a key-sparse attention model that uses an encoder-decoder transformer to fuse multimodal data in a more efficient and adaptive manner. They used a key-sparse attention mechanism to reduce the computational complexity and memory consumption of the self-attention mechanism by selecting a subset of keys for each query. They show that their model can achieve state-of-the-art results on various multimodal tasks, including multimodal SER  [63] . Wagner et al.  [65]  propose a progressive fusion model that uses an encoder-only transformer to fuse multimodal data in a more refined and iterative manner. They use a progressive fusion technique that introduces backward connections between different stages of fusion to preserve modality-specific information while enhancing cross-modality interactions. Jin et al.  [92]  propose a hybrid transformer model that uses a decoder-only transformer to fuse multimodal data in a more flexible and scalable manner. They use a hybrid transformer architecture that combines the advantages of autoregressive and non-autoregressive transformers to generate multimodal outputs with high quality and efficiency. They evaluate their model on various multimodal tasks, including multimodal SER",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "D. Comparing Proposed Model With Related Works",
      "text": "We will now compare our proposed MDAT model with previous methods. Table  I  summarises the existing works on multimodal techniques and their evaluations. Upon reviewing the previous studies, it becomes evident that most previous multimodal methods solely evaluated their models on the same corpus setting. Only a few of them utilised transformer-based multimodal models for cross-language SER. One of the major challenges in cross-language SER is the scarcely of labelled datasets in different languages. Here we address this issue by proposing a transformer-based multimodal model that utilises dual attention to learn generalised multimodal representations and can improve the performance of few shots target samples adaptation.\n\nTo summarise, the key aspects that distinguish our proposed model are as follows:\n\n• In contrast to previous works, MDAT uses multilingual pre-trained models to get feature embeddings on each of our modalities. Using these models can improve the performance and generalizability of our model compared to the previous methods  [48] ,  [70] ,  [71] . • MDAT leverages graph attention and co-attention networks to capture the fine-grained dependencies between modalities and preserve the modality-specific information in contrast to the single-type attention layer used in previous studies  [70] ,  [71] ,  [81] . • MDAT uses a transformer encoder layer to learn highlevel feature representation to improve the multimodal cross-language SER. The transformer encoder layer help improves the performance of our model compared to the previous methods that use recurrent or graph-based layers, such as BiLSTM  [70] , BiGRU  [48] , or HCAM  [81] .",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Iii. Proposed Model Architecture",
      "text": "The overall architecture of our model is shown in Figure  1 . Our proposed Multimodal Dual Attention Transformer (MDAT) leverages pre-trained models, RoBERTa and wav2vec 2.0 based XLS-R, for feature embedding creation from each modality. The model architecture exploits the dual attention mechanisms to capture the complex dependencies between different parts of each modality. We also use a BiLSTMbased multimodal model as a baseline to compare with our proposed model and show the effectiveness of our proposed framework. This section describes the details of our proposed model architecture for multimodal SER.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "A. Input Feature Layers",
      "text": "The input layer takes speech and text features from pretrained models XLS-R and RoBERTa, respectively. Let T s and T t be the sequence lengths and D s and D t be the feature dimensions for speech and text modalities. The speech input layer has a shape of (T s , D s ), while the text input layer has a shape of (T t , D t ). Since D s ̸ = D t , we use a convolutional layer with kernel size 1 to transform the text features to have the same dimensionality as the speech features, i.e., D s . The convolutional layer is defined as:\n\nwhere e t ∈ R Tt✗Dt is the input text feature matrix, W c ∈ R Ds×Dt and b c ∈ R Ds are the convolutional weights and bias, and x t ∈ R Tt×Ds is the output text feature matrix.\n\nTo ensure that the speech and text features have the same length, we pad the shorter sequence and crop the longer ones. This is necessary for applying the subsequent dual attention and transformer layers that require aligned inputs. We denote the padded or cropped speech and text features as xs and xt , respectively. We pass these features to the graph attention",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Co-Attention Module",
      "text": "Co  layers to learn the dependencies between audio and text. We describe the function of the graph attention layer next.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "B. Graph Attention Layer",
      "text": "The graph attention layer is used to capture the complex dependencies between different parts of the audio and text modalities and generate updated features for each modality.\n\nThe motivation behind using graph attention is to model the input data as a graph where the nodes represent the features and the edges represent the similarity between features. By applying a graph attention operation on the input features, we can aggregate the information from the neighbouring nodes using attention weights and update the features accordingly. This technique is useful for capturing complex dependencies between different parts of the audio and text modalities that may not be captured by other methods.\n\nWe implement the graph attention layer based on the original paper by Velivckovic et al. (  2017 )  [29] . The graph attention layer consists of two steps:\n\n1) Computing the attention weights between each pair of features in the same modality. 2) Multiplying the attention weights with the original features to generate updated features.\n\nWe apply a graph attention layer separately for each modality.\n\nThe graph attention layer is defined as:\n\nwhere Z ∈ R (Ts+Tt)×Ds is the input feature matrix for both modalities, W ∈ R Ds×U is the weight matrix, a ∈ R U is the attention vector, α ∈ R (Ts+Tt)×(Ts+Tt) is the graph attention matrix, and xg ∈ R (Ts+Tt)×U is the updated feature matrix for both modalities. The graph attention layer updates the features for each modality by aggregating the information from both modalities. This allows our model to capture the complex dependencies between different parts of the audio and text modalities and generate a more emotionally salient multimodal representation.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "C. Co-Attention Layer",
      "text": "The co-attention layer is used to align the audio and text modalities and generate attended features for each modality. The motivation behind using co-attention is to compute the semantic similarity between each pair of features from both modalities and produce an attention matrix. This attention matrix is then used to generate context vectors for each modality by multiplying it with the original features. This technique is useful for capturing cross-modality interactions and generating a comprehensive representation of the input data.\n\nWe implement the co-attention layer based on the original paper by  Lu et al. (2016)    [30] , but with some modifications to adapt it to our task and data. The co-attention layer consists of two steps:\n\n1) the attention weights between each pair of features from both modalities. 2) Multiplying the attention weights with the original features to generate context vectors.\n\nWe apply a co-attention layer to align the audio and text modalities. The co-attention layer is defined as:\n\nwhere H s ∈ R Ts×Ds and H t ∈ R Tt×Ds are the transformed features for each modality using dense layers with weights W s , W t and bias b s , b t , C ∈ R Ts×Tt is the co-attention matrix between audio and text modalities, α s ∈ R Ts×Tt and α t ∈ R Tt×Ts are the attention matrices for each modality, x ′ s ∈ R Ts×Ds and x ′ t ∈ R Tt×Ds are the attended features for each modality. To further enhance the representation, we concatenate the input features to the co-attention layer and attended features to form a richer representation for each modality. This enables our model to capture both the modality-specific features and the cross-modality interactions in a comprehensive manner. The concatenated features are defined as:\n\nwhere xs ∈ R Ts×2Ds and xt ∈ R Tt×2Ds are the concatenated features for each modality. The graph attention features for both audio and text modalities are directly connected to the co-attention layer, where they are transformed by dense layers and used to compute the co-attention weights. The coattention weights are then multiplied element-wise with the graph attention features to generate the co-attended features for each modality. The main differences between our implementation and the original paper  [30]  are that we use a dense layer to transform the features before computing the co-attention matrix, and we use a softmax function instead of a sigmoid function to normalise the attention weights. These modifications help improve the performance of our model and make it more suitable for multimodal cross-language SER.",
      "page_start": 5,
      "page_end": 6
    },
    {
      "section_name": "D. Transformer Encoder Layer And Classification Layer",
      "text": "The transformer encoder layer is used to process both audio and text contexts and generate transformer outputs for each modality. The motivation behind using a transformer encoder is to use self-attention and feed-forward networks to capture the long-range dependencies and semantic information within each modality. This technique is useful for enhancing the representation of each modality and making it more suitable for classification. The transformer encoder layer consists of the following two sub-layers. First is the multi-head self-attention sub-layer, which computes the attention weights between each pair of features in the same modality and generates context vectors. Second is the feed-forward sub-layer, which applies a linear transformation and an activation function to the context vectors. We apply a transformer encoder layer separately for each modality.\n\nThe output classification layer concatenates the audio and text transformer outputs and applies a dense layer with softmax activation to predict the emotion class of the input. This layer computes the final output probabilities based on both modalities.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "E. Baseline Model Architecture",
      "text": "The baseline model architecture is a simple bidirectional LSTM (BiLSTM) model that concatenates the audio and text features after applying BiLSTM layers to each modality. We choose this model as a baseline because it is widely used for sequence modeling tasks and it does not use any sophisticated fusion technique. The input layer takes speech and text features as inputs. The speech and text features have different lengths and dimensions, so we need to align them before concatenating them. The BiLSTM layer is applied to each modality separately to capture the sequential information. The BiLSTM layer consists of two LSTM sub-layers that process the input sequence from both forward and backward directions and concatenate their outputs. This allows the model to capture both the past and future context of each feature. The concatenation layer concatenates the audio and text features along the feature dimension to combine both modalities into a single feature vector. This feature vector contains both the modality-specific and the cross-modality information. The final layers consist of a dense layer with regularization and dropout, and an output layer with softmax activation to predict the emotion class of the input.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Iv. Experimental Setup A. Datasets",
      "text": "In order to assess the effectiveness of our multimodal framework, we have employed four commonly used datasets including IEMOCAP, EMODB, EMOVO, and URDU to cover a variety of languages for evaluations. The details of these corpora is presented below.\n\n1) IEMOCAP: IEMOCAP  [49]  is a widely used and publicly available multimodal dataset of emotional speech and text in English. This corpus was collected by researchers from the University of Southern California and consists of 12 hours of audiovisual data from 10 actors (5 male and 5 female) in scripted and improvised scenarios. The actors performed a range of emotions such as anger, happiness, sadness, frustration, and neutrality. Each utterance has been labelled by multiple human raters using both categorical and dimensional labels. Specifically, we employ a subset of the dataset, both the audio and text transcripts of 800 utterances, encompassing four emotions: angry, happy, neutral, and sad, although the exact number of utterances used may vary across different experiments.\n\n2) EMODB: EMODB  [93]  is a widely recognised and publicly available emotional dataset in German. The dataset was recorded by researchers at the Institute of Communication Science, Technical University Berlin, and consists of audio recordings of ten professional speakers expressing seven different emotions in 10 German sentences. For our study, we have selected a total of 420 utterances, including 127 angry, 143 sad, 79 neutral, and 71 happy expressions, for categorical cross-language SER analysis.\n\n3) EMOVO: EMOVO  [94]  is an Italian emotional speech corpus that was developed by researchers at the Laboratory of Language Technologies at Roma Tre University. The dataset consists of 14 sentences delivered by six actors, with an equal representation of three male and three female voices. These sentences encompass seven distinct emotional states: anger, disgust, fear, joy, sadness, surprise, and neutral. These emotions represent the widely recognised \"Big Six\" emotions that are commonly used in emotional speech research. For our study, we have selected 336 utterances, with 84 utterances each for angry, happy, neutral, and sad emotions, for categorical cross-language SER analysis.\n\n4) URDU: URDU  [10]  is an emotional speech dataset in Urdu which contains 400 utterances representing four fundamental emotions: angry, happy, neutral, and sad. There are 38 speakers (27 male and 11 female) who were selected randomly from Urdu talk shows on YouTube. In this work, we use both audio and text transcripts of the utterances for multimodal SER as done in  [95] . We use EmulationAI API 1 for textual transcript generation from the given Urdu audio. We use all 400 utterances for angry  (100) , happy  (100) , neutral  (100) , and sad (100) emotions.",
      "page_start": 6,
      "page_end": 7
    },
    {
      "section_name": "V. Experiments And Results",
      "text": "In this section, we present the results of our proposed MDAT model on various SER tasks. We compare our model with baselines and different state-of-the-art methods on different. We conduct two types of experiments: (1) within corpus experiments, where we evaluate our model on the same corpus as the training data, using standard train-test splits; and (2) cross language experiments, where we evaluate our model on a different language than the training data. We report the unweighted accuracy (UA) as the performance metric for all the models.",
      "page_start": 7,
      "page_end": 7
    },
    {
      "section_name": "A. Within Corpus Experiments",
      "text": "This section evaluates the results of our proposed model on the same corpus as the training data, using standard train-test  splits. We use four datasets with different languages and emotion classes: IEMOCAP (English), URDU (Urdu), EMOVO (Italian), and EMODB (German). We compare our model with three baseline models: a simple bidirectional LSTMbased baseline model that we implemented using a simple fusion of audio and text features (see III-E), SAFRLM  [79]  that uses a self-adjusting fusion representation learning model for unaligned text-audio sequences, and HCAM  [81]  that uses a hierarchical cross attention model for multimodal emotion recognition. We report the accuracy as the performance metric for all the models. Table  II  shows the results of the same corpus experiments.\n\nOur proposed model outperformed the baseline models, SAFRLM, and HCAM models on the IEMOCAP dataset, effectively capturing multimodal information and emotion dynamics. In contrast to other approaches, our proposed model also achieve improved classification accuracy on all the datasets, which demonstrating its generalisation to different languages. The SAFRLM model also performs well, handling unaligned text-audio sequences. The HCAM model shows slight improvement over the baseline model, leveraging crossattention for inter-modal interactions. However, both HCAM and the baseline model fall short compared to our proposed model and SAFRLM, indicating limited exploitation of multimodal features and temporal dependencies. The baseline model performs the poorest, lacking cross-modality interaction consideration in its simple fusion of audio and text features.\n\nResults reported in Table  III  demonstrate the effectiveness and robustness of our proposed model for speech emotion recognition in different languages and emotion classes. Our model consistently achieves the highest accuracy on all the datasets, surpassing the baseline and the state-of-the-art models by a large margin. Our model leverages the power of wav2vec 2.0's XLS-R and RoBERTa for feature extraction, as well as the dual attention module, that comprehends the intricate pattern from each modality using graph attention and fuses the cross-modal information from the subsequent coattention layer.",
      "page_start": 7,
      "page_end": 8
    },
    {
      "section_name": "B. Cross-Language Evaluations",
      "text": "This section evaluates the models in the cross-language setting, where the audio and text inputs are from different languages. This setting is realistic and challenging for SER applications. We use four datasets with different languages and emotion classes: IEMOCAP (English), EMODB (German), URDU (Urdu), and EMOVO (Italian). We compare our model with the same the baseline, SAFRLM  [79] , and HCAM  [81]  We perform evaluations in the categorical emotions classification, where we use the four basic emotions (happy, sad, angry, and neutral) as the target labels. We train our model on one language and test it on another language. Table V-A shows the results of the models on different language pairs. Our model achieves the considerably improved performance on most of the language pairs and outperforms the baseline, SAFRLM, and HCAM by a large margin.\n\nFrom Table  V -A, we observe some general trends and patterns among the different language pairs and models.\n\nFirst, IEMOCAP has the lowest accuracy among the four datasets, indicating that it is a relatively difficult dataset for speech emotion recognition. This may be due to the fact that IEMOCAP contains spontaneous and natural speech with more variations and noise than the other datasets, which are mostly acted and recorded in controlled settings  [49] . IEMOCAP also has more speakers and longer utterances than the other datasets, making it harder to learn the consistent and discriminative features for emotion recognition.\n\nSecond, URDU has the highest accuracy among the four datasets, suggesting that it is a relatively easy dataset for SER. This may be due to the fact that URDU has fewer speakers and less diversity than the other languages  [103] , making it easier to learn the acoustic and linguistic features for emotion recognition. URDU also has fewer emotion classes and simpler emotions than the other datasets  [103] , making it easier to classify the emotions correctly.\n\nThird, MDAT performs better when trained on English and tested on other languages than vice versa. This may imply that English has more information and richness than the other languages for speech emotion recognition  [104] , and that a model trained on English can generalise better to other languages than a model trained on other languages. Moreover, our model performs better when trained on German and tested on Italian than vice versa. This may indicate that German and Italian have some similarities in their acoustic and linguistic features for emotion recognition  [104] , and that German has more information and richness than Italian for speech emotion recognition. In contrast, our model performs worse when trained on Urdu and tested on other languages than vice versa. This may suggest that Urdu has less information and richness than the other languages for speech emotion recognition  [103] , and that a model trained on Urdu cannot generalise well to other languages than a model trained on other languages.\n\nFourth, MDAT outperforms the baseline models by a large margin on most of the language pairs. This may demonstrate that our model can effectively fuse multimodal data in a progressive and interpretable way using co-attention and graph attention networks. Our model can also leverage the powerful pre-trained models for speech and text modalities: wav2vec 2.0's XLS-R and RoBERTa. Our model can capture complex dependencies between modalities, preserve modalityspecific information, and enhance cross-modality and crosslanguage interactions. The baseline models, on the other hand, may suffer from some limitations, such as losing modalityspecific information, ignoring complex dependencies between modalities, or requiring large computation and parameters. For example, the BLSTM model with simple fusion may not be able to capture the long-term dependencies and the interactions between modalities. The SAFRLM model may struggle with aligned text-audio sequences and require large computation and parameters  [79] . The HCAM model may ignore the global dependencies among the features and require large computation and parameters  [81] .",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "C. K-Shot Adaptation",
      "text": "In this experiment, we evaluate our MDAT model on the kshot adaptation experiments, where we use varying numbers of shots from 0 to 15. The shots are the number of labelled samples from the target language that are used to fine-tune the models. We compare our model with the baseline model on different language pairs and measure the accuracy as the performance metric. Table  V  shows the results of the proposed model and the baseline model respectively. We first analyse the results of our Multimodal Dual Attention Transformer (MDAT) model in Table  V .\n\nOur proposed model achives considerable improvements on all the language pairs when increasing the number of shots. For example, when the model is trained using English and tested on German, our model increases its accuracy from 42.48 % with 0 shots to 91.48 % with 15 shots. Similarly, when trained on English and tested on Italian, our model increases its accuracy from 82.51 & with 0 shots to 92.05 % with 15 shots. This shows that our model can effectively adapt to a new low-resource language with a small amount of labelled data. We then compare the results of our proposed MDAT model with the baseline model in Table  V . We can observe that our proposed model outperforms the baseline model on all language pairs and shots, demonstrating its superiority and generalisation for cross-language emotion recognition. We can also observe that our proposed model surpasses the baseline model by a large margin on some language pairs and shots. For example, when trained using German and tested on Italian with 15 shots, our proposed model outperforms the baseline model by more than 9 %. Similarly, when the models are trained using Urdu and tested on English with 15 shots, our proposed model outperforms the baseline model by more than 12 % points.\n\nWe attribute these results to the advantages of our proposed model over the baseline model in terms of feature extraction and dual attention mechanisms.\n\nFirst, our proposed model leverages multilingual pre-trained models for each modality, namely wav2vec 2.0  [105]  for the audio modality and RoBERTa  [26]  for the text modality. These models are pre-trained on large-scale multilingual corpora and can capture rich and universal acoustic and linguistic features for different languages and domains  [106] . These features help our model to generalise better to new languages with a small amount of labelled data. In contrast, the baseline model encodes the audio and text inputs separately using a bidirectional LSTM  [107] , which may not be able to capture the nuances and variations of different languages and domains. These features limit the generalisation ability of the baseline model to new languages with a small amount of labelled data.\n\nSecond, our proposed model utilise the co-attention and graph attention layers to learn emotionally salient features for cross-language SER. The co-attention network aligns the audio and text features at the local level and generates a multimodal context vector for each feature. The graph attention network aggregates the multimodal context vectors at the global level and generates a multimodal representation vector for each input. These networks capture complex dependencies between modalities and effectively preserve modality-specific information. These information help our model to exploit the complementary and supplementary aspects of different modalities for emotion recognition. In contrast, the baseline model fuses the audio and text features at the end of the encoding process using a simple concatenation technique  [70] . This technique may not be able to capture the complex dependencies between modalities and may lose modality-specific information. These information limit the exploitation ability of the baseline model to use different modalities for emotion recognition.\n\nThird, in contrast to the baseline, our proposed model enhances the multimodal SER using a transformer encoder layer  [60] . The transformer encoder layer consists of a multihead self-attention sublayer and a feed-forward sublayer. This layer enhances cross-modality and cross-language interactions and generates more expressive and robust multimodal features for emotion recognition  [71] . These features help our model to handle different emotions and scenarios in different languages and domains.",
      "page_start": 8,
      "page_end": 8
    },
    {
      "section_name": "D. Ablation Experiments",
      "text": "The MDAT model has three main modules: graph attention, which applies attention mechanisms to graph-structured data; co-attention, which computes the attention weights for each modality based on the other modality; and transformer encoder, which uses self-attention and feed-forward layers to encode one modality. In this section, we present the results of an ablation study to test the contribution and impact of each module in our proposed MDAT model for crosslanguage SER with different combinations of these modules and measured their performance using unweighted accuracy (Table  VI ). We used three cross-language scenarios: IEMO-CAP (English) to EMODB (German); IEMOCAP (English) to EMOVO (Italian); and IEMOCAP (English) to URDU (Urdu). Our results in Table VI indicate that graph attention and coattention are essential for cross-language emotion recognition, and that transformer encoder can enhance the performance when combined with them.\n\nThe complete MDAT model (Model 1) achieves the highest unweighted accuracy in all three scenarios, demonstrating the benefits of incorporating all three modules. This is because each module leverages complementary information: graph attention captures consistent syntactic dependencies, coattention aligns different modalities, and transformer encoder encodes self-attention and feed-forward features. Removing any of the modules from the complete proposed model (Model 2 and Model 7) leads to a decrease in unweighted accuracy across all scenarios. This indicates that each module contributes significantly to the model's performance. Graph attention provides structural information for modeling long-range dependencies between words, co-attention reduces noise and redundancy in modalities, and transformer encoder enhances the encoding of one modality.\n\nAmong the modules, graph attention emerges as the most crucial for the task (Model 3 vs. Model 4, Model 5 vs. Model 6, and Model 7 vs. Model 6). Its addition to any configuration consistently improves unweighted accuracy, highlighting its ability to leverage universal dependency structure of languages, which is more consistent and transferable across languages than other features. Graph attention also helps to model long-range dependencies between words that are not directly connected in the dependency tree, which may be crucial for emotion recognition.\n\nCo-attention ranks as the second most important module, improving unweighted accuracy in all configurations except for one specific case (Model 3 vs. Model 2). Co-attention's alignment of modalities and reduction of noise and redundancy are valuable, although it may not be sufficient without the structural information provided by graph attention. Similar to other modules, transformer encoder layer is playing its role towards achieving improved results for cross-language SER. It can be noted that transformer encoder layer is when it is combined with co-attention (Model 6 vs. Model 4). This shows all the MDAT's modules are chosen carefully to achieve improve results for cross-language SER.",
      "page_start": 10,
      "page_end": 11
    },
    {
      "section_name": "Vi. Conclusions And Outlook",
      "text": "In this paper, we proposed a novel multimodal framework to improve cross-language speech emotion recognition (SER) using dual attention layers including co-attention and graph attention. Our technique can effectively leverage the powerful pre-trained models including wav2vec 2.0 and RoBERTa for extracting multi-language speech and text embedding. We evaluated our technique on four datasets with different languages and emotion classes: IEMOCAP (English), EMODB (German), URDU (Urdu), and EMOVO (Italian). We compared our proposed model, Multimodal Dual Attention Transformer (MDAT) with baseline and recent multimodal frameworks including SAFRLM with self-adjusting fusion, and HCAM with hierarchical cross-attention on categorical cross-language SER. The results showed that our proposed framework achieved considerably improved results by capturing complex dependencies between modalities and preserving modality-specific information Our work leads to the following overarching conclusions in the realm of cross-language SER: (1) Cross-language SER is a challenging yet highly significant task with diverse practical applications, such as multilingual virtual assistants, crosscultural communication, and emotion-aware machine translation.\n\n(2) The incorporation of multimodal learning serves as a crucial technique for cross-language SER, effectively harnessing the complementary and redundant information present in different modalities and languages. Additionally, (3) leveraging pre-trained models proves to be instrumental in extracting multi-language embeddings from speech and text modalities, enabling the encoding of multimodal input data into gener-alized representations that enhance the overall performance of multi-modal cross-language SER. Notably, (4) co-attention and graph attention networks emerge as effective mechanisms for facilitating multimodal learning by effectively capturing complex dependencies among speech samples from different languages, thereby improving the accuracy of cross-language SER. Finally, (5) k-shot adaptation techniques demonstrate their usefulness in addressing the scarcity of labeled data in low-resource languages, as they successfully enhance model performance even with a limited number of labeled samples.\n\nThese findings collectively contribute to the advancement of cross-language SER, paving the way for future research and practical applications. This work opens several opportunities for future work in the field of cross-language SER. These include the exploration of the visual modality to further enhance the performance of MDAT. Furthermore, it is important to evaluate the effectiveness of multimodal frameworks in more realistic and diverse scenarios, such as noisy environments, in the face of adversarial attacks, and with spontaneous speech and mixed emotions.",
      "page_start": 10,
      "page_end": 11
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: In this work, we propose a Multimodal Dual Attention Trans-",
      "page": 5
    },
    {
      "caption": "Figure 2: Cross-language K-shot results: IEMOCAP to EMODB.",
      "page": 9
    },
    {
      "caption": "Figure 3: Cross-language K-shot results: IEMOCAP to EMOVO.",
      "page": 9
    }
  ],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Work (Year)": "",
          "Evaluations": "Same Corpus",
          "Transformer Based": "",
          "Co-Attention": "",
          "Graph Attention": ""
        },
        {
          "Work (Year)": "Chen et al.\n(2019)\n[70]",
          "Evaluations": "✓",
          "Transformer Based": "✗",
          "Co-Attention": "✗",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Huang et al.\n(2020)\n[71]",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✗",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Siriwardhana et al.\n(2020)\n[72]",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✗",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Su et al.\n(2020)\n[73]",
          "Evaluations": "✓",
          "Transformer Based": "✗",
          "Co-Attention": "✗",
          "Graph Attention": "✓"
        },
        {
          "Work (Year)": "Zhou et al.\n(2020)\n[48]",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✗",
          "Graph Attention": "✓"
        },
        {
          "Work (Year)": "Wang et al.\n(2021)\n[74]",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✗",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Zheng et al.\n(2021)\n[75]",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✗",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Chen et al.\n(2022)\n[63]",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✓",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Kim et al.\n(2022)\n[76]",
          "Evaluations": "✓",
          "Transformer Based": "✗",
          "Co-Attention": "✗",
          "Graph Attention": "✓"
        },
        {
          "Work (Year)": "Zhang et al.\n(2022)\n[77]",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✓",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Guo et al.\n(2022)\n[78]",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✓",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Yang et al.\n(2022)\n[79]",
          "Evaluations": "✓",
          "Transformer Based": "✗",
          "Co-Attention": "✗",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Wang et al.\n(2023)\n[80]",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✗",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "Dutta et al.\n(2023)\n[81]",
          "Evaluations": "✓",
          "Transformer Based": "✗",
          "Co-Attention": "✗",
          "Graph Attention": "✗"
        },
        {
          "Work (Year)": "This work (2023)",
          "Evaluations": "✓",
          "Transformer Based": "✓",
          "Co-Attention": "✓",
          "Graph Attention": "✓"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Datasets": "",
          "Models performance UA (%)": "Baseline"
        },
        {
          "Datasets": "IEMOCAP\n(4 Classes)",
          "Models performance UA (%)": "63.33"
        },
        {
          "Datasets": "EMODB\n(7 Classes)",
          "Models performance UA (%)": "81.00"
        },
        {
          "Datasets": "URDU\n(4 Classes)",
          "Models performance UA (%)": "91.13"
        },
        {
          "Datasets": "EMOVO\n(6 Classes)",
          "Models performance UA (%)": "72.25"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Models": "",
          "Feature Extraction": "Audio",
          "UA (%)": ""
        },
        {
          "Models": "Baseline",
          "Feature Extraction": "XLS-R",
          "UA (%)": "63.33"
        },
        {
          "Models": "Yoon et al.\n(2018)\n[91]",
          "Feature Extraction": "openSMILE",
          "UA (%)": "48.70"
        },
        {
          "Models": "Xu et al.\n(2019)\n[96]",
          "Feature Extraction": "MFCC",
          "UA (%)": "69.50"
        },
        {
          "Models": "Sebastian et al.\n(2019)\n[97]",
          "Feature Extraction": "openSMILE",
          "UA (%)": "59.30"
        },
        {
          "Models": "Chen et al.\n(2020)\n[98]",
          "Feature Extraction": "log-mel spectrograms",
          "UA (%)": "72.82"
        },
        {
          "Models": "Krishna et al.\n(2020)\n[99]",
          "Feature Extraction": "CNN",
          "UA (%)": "72.82"
        },
        {
          "Models": "Sun et al.\n(2021)\n[100]",
          "Feature Extraction": "CNN + LSTM",
          "UA (%)": "56.00"
        },
        {
          "Models": "Lian et al.\n(2021)\n[101]",
          "Feature Extraction": "openSMILE",
          "UA (%)": "67.60"
        },
        {
          "Models": "Kumar et al.\n(2021)\n[102]",
          "Feature Extraction": "MFCC + chroma +\nmel",
          "UA (%)": "75.00"
        },
        {
          "Models": "Yang et al.\n(2022)\n[79]",
          "Feature Extraction": "COVAREP",
          "UA (%)": "72.14"
        },
        {
          "Models": "Wang et al.\n(2023)\n[80]",
          "Feature Extraction": "Librosa",
          "UA (%)": "75.08"
        },
        {
          "Models": "Dutta et al.\n(2023)\n[81]",
          "Feature Extraction": "wav2vec 2.0",
          "UA (%)": "73.67"
        },
        {
          "Models": "MDAT",
          "Feature Extraction": "XLS-R",
          "UA (%)": "75.58"
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source": "",
          "Target": "",
          "Models performance UA (%)": "Baseline\nmultimodal Model"
        },
        {
          "Source": "IEMOCAP (English)",
          "Target": "EMODB (German)",
          "Models performance UA (%)": "40.78"
        },
        {
          "Source": "",
          "Target": "EMOVO (Italian)",
          "Models performance UA (%)": "73.46"
        },
        {
          "Source": "",
          "Target": "URDU (Urdu)",
          "Models performance UA (%)": "63.18"
        },
        {
          "Source": "EMODB (German)",
          "Target": "IEMPCAP (English)",
          "Models performance UA (%)": "49.23"
        },
        {
          "Source": "",
          "Target": "EMOVO (Italian)",
          "Models performance UA (%)": "50.54"
        },
        {
          "Source": "",
          "Target": "URDU (Urdu)",
          "Models performance UA (%)": "72.46"
        },
        {
          "Source": "URDU (Urdu)",
          "Target": "IEMPCAP (English)",
          "Models performance UA (%)": "47.96"
        },
        {
          "Source": "",
          "Target": "EMODB (German)",
          "Models performance UA (%)": "65.35"
        },
        {
          "Source": "",
          "Target": "EMOVO (Italian)",
          "Models performance UA (%)": "63.55"
        },
        {
          "Source": "EMOVO (Italian)",
          "Target": "IEMPCAP (English)",
          "Models performance UA (%)": "57.25"
        },
        {
          "Source": "",
          "Target": "EMODB (German)",
          "Models performance UA (%)": "64.60"
        },
        {
          "Source": "",
          "Target": "URDU (Urdu)",
          "Models performance UA (%)": "72.46"
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Source": "",
          "Target": "",
          "K-Shot UA (%)": "0 Shot"
        },
        {
          "Source": "",
          "Target": "",
          "K-Shot UA (%)": "Baseline"
        },
        {
          "Source": "IEMOCAP",
          "Target": "EMODB",
          "K-Shot UA (%)": "40.78"
        },
        {
          "Source": "",
          "Target": "URDU",
          "K-Shot UA (%)": "61.18"
        },
        {
          "Source": "",
          "Target": "EMOVO",
          "K-Shot UA (%)": "73.46"
        },
        {
          "Source": "EMODB",
          "Target": "IEMOCAP",
          "K-Shot UA (%)": "49.23"
        },
        {
          "Source": "",
          "Target": "URDU",
          "K-Shot UA (%)": "72.46"
        },
        {
          "Source": "",
          "Target": "EMOVO",
          "K-Shot UA (%)": "50.54"
        },
        {
          "Source": "URDU",
          "Target": "IEMOCAP",
          "K-Shot UA (%)": "47.96"
        },
        {
          "Source": "",
          "Target": "EMODB",
          "K-Shot UA (%)": "65.35"
        },
        {
          "Source": "",
          "Target": "EMOVO",
          "K-Shot UA (%)": "65.35"
        },
        {
          "Source": "EMOVO",
          "Target": "IEMOCAP",
          "K-Shot UA (%)": "57.25"
        },
        {
          "Source": "",
          "Target": "EMODB",
          "K-Shot UA (%)": "65.60"
        },
        {
          "Source": "",
          "Target": "URDU",
          "K-Shot UA (%)": "72.46"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Model": "",
          "Configuration": "",
          "Graph\nAttention": "",
          "Co-\nAttention": "",
          "Transformer\nEncoder": "",
          "Cross-Language UA (%)": "IEMOCAP (English)\nto\nEMODB\nEMOVO\nURDU\n(German)\n(Italian)\n(Urdu)"
        },
        {
          "Model": "1",
          "Configuration": "",
          "Graph\nAttention": "✓",
          "Co-\nAttention": "✓",
          "Transformer\nEncoder": "✓",
          "Cross-Language UA (%)": "42.48\n85.51\n64.43"
        },
        {
          "Model": "2",
          "Configuration": "",
          "Graph\nAttention": "✓",
          "Co-\nAttention": "✓",
          "Transformer\nEncoder": "✗",
          "Cross-Language UA (%)": "39.21\n80.68\n56.52"
        },
        {
          "Model": "3",
          "Configuration": "",
          "Graph\nAttention": "✓",
          "Co-\nAttention": "✗",
          "Transformer\nEncoder": "✗",
          "Cross-Language UA (%)": "41.58\n76.13\n45.41"
        },
        {
          "Model": "4",
          "Configuration": "",
          "Graph\nAttention": "✗",
          "Co-\nAttention": "✓",
          "Transformer\nEncoder": "✗",
          "Cross-Language UA (%)": "39.06\n41.82\n53.14"
        },
        {
          "Model": "5",
          "Configuration": "",
          "Graph\nAttention": "✗",
          "Co-\nAttention": "✗",
          "Transformer\nEncoder": "✓",
          "Cross-Language UA (%)": "34.80\n46.59\n36.23"
        },
        {
          "Model": "6",
          "Configuration": "",
          "Graph\nAttention": "✗",
          "Co-\nAttention": "✓",
          "Transformer\nEncoder": "✓",
          "Cross-Language UA (%)": "38.72\n57.95\n36.71"
        },
        {
          "Model": "7",
          "Configuration": "",
          "Graph\nAttention": "✓",
          "Co-\nAttention": "✗",
          "Transformer\nEncoder": "✓",
          "Cross-Language UA (%)": "39.70\n30.91\n42.99"
        }
      ],
      "page": 11
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "Speech technology for healthcare: Opportunities, challenges, and state of the art",
      "authors": [
        "S Latif",
        "J Qadir",
        "A Qayyum",
        "M Usama",
        "S Younis"
      ],
      "year": "2020",
      "venue": "IEEE Reviews in Biomedical Engineering"
    },
    {
      "citation_id": "2",
      "title": "Automated screening for distress: A perspective for the future",
      "authors": [
        "R Rana",
        "S Latif",
        "R Gururajan",
        "A Gray",
        "G Mackenzie",
        "G Humphris",
        "J Dunn"
      ],
      "year": "2019",
      "venue": "European journal of cancer care"
    },
    {
      "citation_id": "3",
      "title": "Driver emotion recognition for intelligent vehicles: A survey",
      "authors": [
        "S Zepf",
        "J Hernandez",
        "A Schmitt",
        "W Minker",
        "R Picard"
      ],
      "year": "2020",
      "venue": "ACM Computing Surveys (CSUR)"
    },
    {
      "citation_id": "4",
      "title": "Prosodic cues of speech under stress: phonetic exploration of finnish emergency calls",
      "authors": [
        "L Tavi"
      ],
      "year": "2020",
      "venue": "Itä-Suomen yliopisto"
    },
    {
      "citation_id": "5",
      "title": "Affective computing in education: A systematic review and future research",
      "authors": [
        "E Yadegaridehkordi",
        "N Noor",
        "M Ayub",
        "H Affal",
        "N Hussin"
      ],
      "year": "2019",
      "venue": "Computers & Education"
    },
    {
      "citation_id": "6",
      "title": "Improved soccer action spotting using both audio and video streams",
      "authors": [
        "B Vanderplaetse",
        "S Dupont"
      ],
      "year": "2020",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops"
    },
    {
      "citation_id": "7",
      "title": "You're not you when you're angry: Robust emotion features emerge by recognizing speakers",
      "authors": [
        "Z Aldeneh",
        "E Provost"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "8",
      "title": "Multi-head attention for speech emotion recognition with auxiliary learning of gender recognition",
      "authors": [
        "A Nediyanchath",
        "P Paramasivam",
        "P Yenigalla"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "9",
      "title": "Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks",
      "authors": [
        "Z.-Q Wang",
        "I Tashev"
      ],
      "year": "2017",
      "venue": "2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)"
    },
    {
      "citation_id": "10",
      "title": "Cross lingual speech emotion recognition: Urdu vs. western languages",
      "authors": [
        "S Latif",
        "A Qayyum",
        "M Usman",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "2018 International Conference on Frontiers of Information Technology (FIT)"
    },
    {
      "citation_id": "11",
      "title": "Evidence for cultural dialects in vocal emotion expression: acoustic classification within and across five nations",
      "authors": [
        "P Laukka",
        "D Neiberg",
        "H Elfenbein"
      ],
      "year": "2014",
      "venue": "Emotion"
    },
    {
      "citation_id": "12",
      "title": "A fast learning algorithm for deep belief nets",
      "authors": [
        "G Hinton",
        "S Osindero",
        "Y.-W Teh"
      ],
      "year": "2006",
      "venue": "Neural computation"
    },
    {
      "citation_id": "13",
      "title": "Handwritten digit recognition with a back-propagation network",
      "authors": [
        "Y Lecun",
        "B Boser",
        "J Denker",
        "D Henderson",
        "R Howard",
        "W Hubbard",
        "L Jackel"
      ],
      "year": "1989",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "14",
      "title": "Deep architecture enhancing robustness to noise, adversarial attacks, and cross-corpus setting for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech 2020"
    },
    {
      "citation_id": "15",
      "title": "Attentive convolutional neural network based speech emotion recognition: A study on the impact of input features, signal length, and acted speech",
      "authors": [
        "M Neumann",
        "N Vu"
      ],
      "year": "2017",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "16",
      "title": "Attention guided 3d cnn-lstm model for accurate speech based emotion recognition",
      "authors": [
        "O Atila",
        "A ¸engür"
      ],
      "year": "2021",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "17",
      "title": "Direct Modelling of Speech Emotion from Raw Speech",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Epps"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech",
      "doi": "10.21437/Interspeech.2019-3252"
    },
    {
      "citation_id": "18",
      "title": "Learning longterm dependencies with recurrent neural networks",
      "authors": [
        "A Schaefer",
        "S Udluft",
        "H.-G Zimmermann"
      ],
      "year": "2008",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "19",
      "title": "Recurrent neural networks with long term temporal dependencies in machine tool wear diagnosis and prognosis",
      "authors": [
        "J Zhang",
        "Y Zeng",
        "B Starly"
      ],
      "year": "2021",
      "venue": "SN Applied Sciences"
    },
    {
      "citation_id": "20",
      "title": "Subtraction gates: Another way to learn long-term dependencies in recurrent neural networks",
      "authors": [
        "Y Liu",
        "X Li",
        "S Li",
        "S Zhang",
        "Z Liu",
        "X Zhang"
      ],
      "year": "2020",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems"
    },
    {
      "citation_id": "21",
      "title": "Transformers in speech processing: A survey",
      "authors": [
        "S Latif",
        "A Zaidi",
        "H Cuayahuitl",
        "F Shamshad",
        "M Shoukat",
        "J Qadir"
      ],
      "year": "2023",
      "venue": "Transformers in speech processing: A survey",
      "arxiv": "arXiv:2303.11607"
    },
    {
      "citation_id": "22",
      "title": "Survey of deep representation learning for speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "J Qadir",
        "B Schuller"
      ],
      "year": "2021",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "23",
      "title": "A review of affective computing: From unimodal analysis to multimodal fusion",
      "authors": [
        "S Poria",
        "E Cambria",
        "R Bajpai",
        "A Hussain"
      ],
      "year": "2017",
      "venue": "Information Fusion"
    },
    {
      "citation_id": "24",
      "title": "Multimodal language analysis in the wild: Cmu-mosei dataset and interpretable dynamic fusion graph",
      "authors": [
        "A Zadeh",
        "P Liang",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2018",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics"
    },
    {
      "citation_id": "25",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)"
    },
    {
      "citation_id": "26",
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "authors": [
        "Y Liu",
        "M Ott",
        "N Goyal",
        "J Du",
        "M Joshi",
        "D Chen",
        "O Levy",
        "M Lewis",
        "L Zettlemoyer",
        "V Stoyanov"
      ],
      "year": "2019",
      "venue": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
      "arxiv": "arXiv:1907.11692"
    },
    {
      "citation_id": "27",
      "title": "Exploring the limits of transfer learning with a unified text-to-text transformer",
      "authors": [
        "C Raffel",
        "N Shazeer",
        "A Roberts",
        "K Lee",
        "S Narang",
        "M Matena",
        "Y Zhou",
        "W Li",
        "P Liu"
      ],
      "year": "2020",
      "venue": "The Journal of Machine Learning Research"
    },
    {
      "citation_id": "28",
      "title": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "authors": [
        "A Babu",
        "C Wang",
        "A Tjandra",
        "K Lakhotia",
        "Q Xu",
        "N Goyal",
        "K Singh",
        "P Von Platen",
        "Y Saraf",
        "J Pino"
      ],
      "year": "2021",
      "venue": "Xls-r: Self-supervised cross-lingual speech representation learning at scale",
      "arxiv": "arXiv:2111.09296"
    },
    {
      "citation_id": "29",
      "title": "Graph attention networks",
      "authors": [
        "P Veličković",
        "G Cucurull",
        "A Casanova",
        "A Romero",
        "P Lio",
        "Y Bengio"
      ],
      "year": "2017",
      "venue": "Graph attention networks",
      "arxiv": "arXiv:1710.10903"
    },
    {
      "citation_id": "30",
      "title": "Hierarchical questionimage co-attention for visual question answering",
      "authors": [
        "J Lu",
        "J Yang",
        "D Batra",
        "D Parikh"
      ],
      "year": "2016",
      "venue": "Advances in neural information processing systems"
    },
    {
      "citation_id": "31",
      "title": "When low resource nlp meets unsupervised language model: Meta-pretraining then meta-learning for few-shot text classification (student abstract)",
      "authors": [
        "S Deng",
        "N Zhang",
        "Z Sun",
        "J Chen",
        "H Chen"
      ],
      "year": "2020",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "32",
      "title": "A survey on nlp resources, tools, and techniques for marathi language processing",
      "authors": [
        "P Lahoti",
        "N Mittal",
        "G Singh"
      ],
      "year": "2022",
      "venue": "ACM Transactions on Asian and Low-Resource Language Information Processing"
    },
    {
      "citation_id": "33",
      "title": "A very low resource language speech corpus for computational language documentation experiments",
      "authors": [
        "P Godard",
        "G Adda",
        "M Adda-Decker",
        "J Benjumea",
        "L Besacier",
        "J Cooper-Leavitt",
        "G.-N Kouarata",
        "L Lamel",
        "H Maynard",
        "M Müller"
      ],
      "year": "2017",
      "venue": "A very low resource language speech corpus for computational language documentation experiments",
      "arxiv": "arXiv:1710.03501"
    },
    {
      "citation_id": "34",
      "title": "Interspeech 2010: 11th annual conference of the international speech communication association",
      "authors": [
        "F Schuller",
        "S Steidl",
        "A Batliner",
        "G Rigoll",
        "K Lang"
      ],
      "year": "2010",
      "venue": "Interspeech 2010: 11th annual conference of the international speech communication association"
    },
    {
      "citation_id": "35",
      "title": "The interspeech 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "A Vinciarelli",
        "K Scherer",
        "F Ringeval",
        "M Chetouani",
        "F Weninger",
        "F Eyben",
        "E Marchi"
      ],
      "year": "2013",
      "venue": "Proceedings INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association"
    },
    {
      "citation_id": "36",
      "title": "A novel feature selection method for speech emotion recognition",
      "authors": [
        "T Özseven"
      ],
      "year": "2019",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "37",
      "title": "Cross-corpus speech emotion recognition based on few-shot learning and domain adaptation",
      "authors": [
        "Y Ahn",
        "S Lee",
        "J Shin"
      ],
      "year": "2021",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "38",
      "title": "Cross-language speech emotion recognition using bag-of-word representations, domain adaptation, and data augmentation",
      "authors": [
        "S Kshirsagar",
        "T Falk"
      ],
      "year": "2022",
      "venue": "Sensors"
    },
    {
      "citation_id": "39",
      "title": "Multitask learning from augmented auxiliary data for improving speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "40",
      "title": "Wise: Word-level interaction-based multimodal fusion for speech emotion recognition",
      "authors": [
        "G Shen",
        "R Lai",
        "R Chen",
        "Y Zhang",
        "K Zhang",
        "Q Han",
        "H Song"
      ],
      "year": "2020",
      "venue": "Interspeech"
    },
    {
      "citation_id": "41",
      "title": "Survey on bimodal speech emotion recognition from acoustic and linguistic information fusion",
      "authors": [
        "B Atmaja",
        "A Sasou",
        "M Akagi"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "42",
      "title": "Contrastive adversarial learning for person independent facial emotion recognition",
      "authors": [
        "D Kim",
        "B Song"
      ],
      "year": "2021",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence"
    },
    {
      "citation_id": "43",
      "title": "Emotion recognition from facial images with simultaneous occlusion, pose and illumination variations using meta-learning",
      "authors": [
        "S Kuruvayil",
        "S Palaniswamy"
      ],
      "year": "2022",
      "venue": "Journal of King Saud University-Computer and Information Sciences"
    },
    {
      "citation_id": "44",
      "title": "Emotion classification from noisy speech-a deep learning approach",
      "authors": [
        "R Rana",
        "R Jurdak",
        "X Li",
        "J Soar",
        "R Goecke",
        "J Epps",
        "M Breakspear"
      ],
      "year": "2016",
      "venue": "Emotion classification from noisy speech-a deep learning approach",
      "arxiv": "arXiv:1603.05901"
    },
    {
      "citation_id": "45",
      "title": "Data augmentation using gans for speech emotion recognition",
      "authors": [
        "A Chatziagapi",
        "G Paraskevopoulos",
        "D Sgouropoulos",
        "G Pantazopoulos",
        "M Nikandrou",
        "T Giannakopoulos",
        "A Katsamanis",
        "A Potamianos",
        "S Narayanan"
      ],
      "year": "2019",
      "venue": "Data augmentation using gans for speech emotion recognition"
    },
    {
      "citation_id": "46",
      "title": "Cross-corpus speech emotion recognition based on deep domain-adaptive convolutional neural network",
      "authors": [
        "J Liu",
        "W Zheng",
        "Y Zong",
        "C Lu",
        "C Tang"
      ],
      "year": "2020",
      "venue": "IEICE TRANSACTIONS on Information and Systems"
    },
    {
      "citation_id": "47",
      "title": "Tensor fusion network for multimodal sentiment analysis",
      "authors": [
        "A Zadeh",
        "M Chen",
        "S Poria",
        "E Cambria",
        "L.-P Morency"
      ],
      "year": "2017",
      "venue": "Tensor fusion network for multimodal sentiment analysis",
      "arxiv": "arXiv:1707.07250"
    },
    {
      "citation_id": "48",
      "title": "Graph neural networks: A review of methods and applications",
      "authors": [
        "J Zhou",
        "G Cui",
        "S Hu",
        "Z Zhang",
        "C Yang",
        "Z Liu",
        "L Wang",
        "C Li",
        "M Sun"
      ],
      "year": "2020",
      "venue": "AI open"
    },
    {
      "citation_id": "49",
      "title": "Iemocap: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "50",
      "title": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "authors": [
        "S Poria",
        "D Hazarika",
        "N Majumder",
        "G Naik",
        "E Cambria",
        "R Mihalcea"
      ],
      "year": "2018",
      "venue": "Meld: A multimodal multi-party dataset for emotion recognition in conversations",
      "arxiv": "arXiv:1810.02508"
    },
    {
      "citation_id": "51",
      "title": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "authors": [
        "A Zadeh",
        "R Zellers",
        "E Pincus",
        "L.-P Morency"
      ],
      "year": "2016",
      "venue": "Mosi: multimodal corpus of sentiment intensity and subjectivity analysis in online opinion videos",
      "arxiv": "arXiv:1606.06259"
    },
    {
      "citation_id": "52",
      "title": "Unsupervised transfer learning for spoken language understanding in intelligent agents",
      "authors": [
        "A Siddhant",
        "A Goyal",
        "A Metallinou"
      ],
      "year": "2019",
      "venue": "Proceedings of the AAAI conference on artificial intelligence"
    },
    {
      "citation_id": "53",
      "title": "Self-supervised prototypical transfer learning for few-shot classification",
      "authors": [
        "C Medina",
        "A Devos",
        "M Grossglauser"
      ],
      "year": "2020",
      "venue": "Self-supervised prototypical transfer learning for few-shot classification",
      "arxiv": "arXiv:2006.11325"
    },
    {
      "citation_id": "54",
      "title": "A survey on self-supervised pre-training for sequential transfer learning in neural networks",
      "authors": [
        "H Mao"
      ],
      "year": "2020",
      "venue": "A survey on self-supervised pre-training for sequential transfer learning in neural networks",
      "arxiv": "arXiv:2007.00800"
    },
    {
      "citation_id": "55",
      "title": "Multi-speaker sequence-to-sequence speech synthesis for data augmentation in acoustic-to-word speech recognition",
      "authors": [
        "S Ueno",
        "M Mimura",
        "S Sakai",
        "T Kawahara"
      ],
      "year": "2019",
      "venue": "ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "56",
      "title": "A preliminary study on augmenting speech emotion recognition using a diffusion model",
      "authors": [
        "I Malik",
        "S Latif",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "A preliminary study on augmenting speech emotion recognition using a diffusion model",
      "arxiv": "arXiv:2305.11413"
    },
    {
      "citation_id": "57",
      "title": "Generative emotional ai for speech emotion recognition: The case for synthetic emotional speech augmentation",
      "authors": [
        "S Latif",
        "A Shahid",
        "J Qadir"
      ],
      "year": "2023",
      "venue": "Applied Acoustics"
    },
    {
      "citation_id": "58",
      "title": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir"
      ],
      "year": "2018",
      "venue": "Adversarial machine learning and speech emotion recognition: Utilizing generative adversarial networks for robustness",
      "arxiv": "arXiv:1811.11402"
    },
    {
      "citation_id": "59",
      "title": "Transformers: State-of-the-art natural language processing",
      "authors": [
        "T Wolf",
        "L Debut",
        "V Sanh",
        "J Chaumond",
        "C Delangue",
        "A Moi",
        "P Cistac",
        "T Rault",
        "R Louf",
        "M Funtowicz"
      ],
      "year": "2020",
      "venue": "Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations"
    },
    {
      "citation_id": "60",
      "title": "Attention is all you need",
      "authors": [
        "A Vaswani",
        "N Shazeer",
        "N Parmar",
        "J Uszkoreit",
        "L Jones",
        "A Gomez",
        "Ł Kaiser",
        "I Polosukhin"
      ],
      "year": "2017",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems"
    },
    {
      "citation_id": "61",
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2018",
      "venue": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "arxiv": "arXiv:1810.04805"
    },
    {
      "citation_id": "62",
      "title": "Language models are unsupervised multitask learners",
      "authors": [
        "A Radford",
        "J Wu",
        "R Child",
        "D Luan",
        "D Amodei",
        "I Sutskever"
      ],
      "year": "2019",
      "venue": "OpenAI, Tech. Rep"
    },
    {
      "citation_id": "63",
      "title": "Key-sparse transformer for multimodal speech emotion recognition",
      "authors": [
        "W Chen",
        "X Xing",
        "X Xu",
        "J Yang",
        "J Pang"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "64",
      "title": "Paralinguistic speech processing: An overview",
      "authors": [
        "R Abdullah",
        "S Ameen",
        "D Ahmed",
        "S Kak",
        "H Yasin",
        "I Ibrahim",
        "A Ahmed",
        "Z Rashid",
        "N Omar",
        "A Salih"
      ],
      "year": "2021",
      "venue": "Asian Journal of Research in Computer Science"
    },
    {
      "citation_id": "65",
      "title": "Dawn of the transformer era in speech emotion recognition: closing the valence gap",
      "authors": [
        "J Wagner",
        "A Triantafyllopoulos",
        "H Wierstorf",
        "M Schmitt",
        "F Burkhardt",
        "F Eyben",
        "B Schuller"
      ],
      "year": "2023",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
    },
    {
      "citation_id": "66",
      "title": "Transformer-cnn emotion recognition",
      "authors": [
        "I Zenkov"
      ],
      "year": "2021",
      "venue": "Transformer-cnn emotion recognition"
    },
    {
      "citation_id": "67",
      "title": "Speech emotion recognition transformer: A novel end-to-end model for ser",
      "authors": [
        "Y Li",
        "Z Li",
        "Z Zhang",
        "X Li",
        "J Li"
      ],
      "year": "2021",
      "venue": "Neurocomputing"
    },
    {
      "citation_id": "68",
      "title": "Msp-podcast: A large-scale dataset of natural and emotionally evocative speech",
      "authors": [
        "J Park",
        "C Busso"
      ],
      "year": "2019",
      "venue": "2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)"
    },
    {
      "citation_id": "69",
      "title": "Probing speech emotion recognition transformers for linguistic knowledge",
      "authors": [
        "A Triantafyllopoulos",
        "J Wagner",
        "H Wierstorf",
        "M Schmitt",
        "U Reichel",
        "F Eyben",
        "F Burkhardt",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "70",
      "title": "Complementary fusion of multifeatures and multi-modalities in sentiment analysis",
      "authors": [
        "F Chen",
        "Z Luo",
        "Y Xu",
        "D Ke"
      ],
      "year": "2019",
      "venue": "Complementary fusion of multifeatures and multi-modalities in sentiment analysis",
      "arxiv": "arXiv:1904.08138"
    },
    {
      "citation_id": "71",
      "title": "Multimodal transformer fusion for continuous emotion recognition",
      "authors": [
        "J Huang",
        "J Tao",
        "B Liu",
        "Z Lian",
        "M Niu"
      ],
      "year": "2020",
      "venue": "ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "72",
      "title": "Multimodal emotion recognition with transformer-based self supervised feature fusion",
      "authors": [
        "S Siriwardhana",
        "T Kaluarachchi",
        "M Billinghurst",
        "S Nanayakkara"
      ],
      "year": "2020",
      "venue": "IEEE Access"
    },
    {
      "citation_id": "73",
      "title": "Improving speech emotion recognition using graph attentive bi-directional gated recurrent unit network",
      "authors": [
        "Z Su",
        "Z Zhang",
        "X Li",
        "J Li"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "74",
      "title": "Learning mutual correlation in multimodal transformer for speech emotion recognition",
      "authors": [
        "Y Wang",
        "G Shen",
        "Y Xu",
        "J Li",
        "Z Zhao"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "75",
      "title": "Fused acoustic and text encoding for multimodal bilingual pretraining and speech translation",
      "authors": [
        "R Zheng",
        "J Chen",
        "M Ma",
        "L Huang"
      ],
      "year": "2021",
      "venue": "International Conference on Machine Learning"
    },
    {
      "citation_id": "76",
      "title": "Representation learning with graph neural networks for speech emotion recognition",
      "authors": [
        "J Kim",
        "J Kim"
      ],
      "year": "2022",
      "venue": "Representation learning with graph neural networks for speech emotion recognition",
      "arxiv": "arXiv:2208.09830"
    },
    {
      "citation_id": "77",
      "title": "Transformer-based multimodal information fusion for facial expression analysis",
      "authors": [
        "W Zhang",
        "F Qiu",
        "S Wang",
        "H Zeng",
        "Z Zhang",
        "R An",
        "B Ma",
        "Y Ding"
      ],
      "year": "2022",
      "venue": "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "78",
      "title": "Emotion recognition with multimodal transformer fusion framework based on acoustic and lexical information",
      "authors": [
        "L Guo",
        "L Wang",
        "J Dang",
        "Y Fu",
        "J Liu",
        "S Ding"
      ],
      "year": "2022",
      "venue": "IEEE MultiMedia"
    },
    {
      "citation_id": "79",
      "title": "A self-adjusting fusion representation learning model for unaligned text-audio sequences",
      "authors": [
        "K Yang",
        "R Zhang",
        "H Xu",
        "K Gao"
      ],
      "year": "2022",
      "venue": "A self-adjusting fusion representation learning model for unaligned text-audio sequences",
      "arxiv": "arXiv:2212.11772"
    },
    {
      "citation_id": "80",
      "title": "Multimodal transformer augmented fusion for speech emotion recognition",
      "authors": [
        "Y Wang",
        "Y Gu",
        "Y Yin",
        "Y Han",
        "H Zhang",
        "S Wang",
        "C Li",
        "D Quan"
      ],
      "year": "2023",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "81",
      "title": "Hcam-hierarchical cross attention model for multi-modal emotion recognition",
      "authors": [
        "S Dutta",
        "S Ganapathy"
      ],
      "year": "2023",
      "venue": "Hcam-hierarchical cross attention model for multi-modal emotion recognition",
      "arxiv": "arXiv:2304.06910"
    },
    {
      "citation_id": "82",
      "title": "Robust multimodal emotion recognition from conversation with transformer-based crossmodality fusion",
      "authors": [
        "B Xie",
        "M Sidulova",
        "C Park"
      ],
      "year": "2021",
      "venue": "Sensors"
    },
    {
      "citation_id": "83",
      "title": "Training emotion recognition accuracy: Results for multimodal expressions and micro expressions",
      "authors": [
        "K Schlegel",
        "K Scherer",
        "M Mortillaro"
      ],
      "year": "2021",
      "venue": "Frontiers in Psychology"
    },
    {
      "citation_id": "84",
      "title": "Multi-modal emotion recognition using eeg and speech signals",
      "authors": [
        "Q Wang",
        "M Wang",
        "Y Yang",
        "X Zhang"
      ],
      "year": "2022",
      "venue": "Computers in Biology and Medicine"
    },
    {
      "citation_id": "85",
      "title": "Multimodal transformer for unaligned multimodal language sequences",
      "authors": [
        "Y.-H Tsai",
        "S Bai",
        "P Liang",
        "J Kolter",
        "L.-P Morency",
        "R Salakhutdinov"
      ],
      "year": "2019",
      "venue": "Proceedings of the conference. Association for Computational Linguistics. Meeting"
    },
    {
      "citation_id": "86",
      "title": "Multi-modal fusion emotion recognition method of speech expression based on deep learning",
      "authors": [
        "D Liu",
        "Z Wang",
        "L Wang",
        "L Chen"
      ],
      "year": "2021",
      "venue": "Frontiers in Neurorobotics"
    },
    {
      "citation_id": "87",
      "title": "Multi-modal speech emotion recognition using self-attention mechanism and multi-scale fusion framework",
      "authors": [
        "Y Liu",
        "H Sun",
        "W Guan",
        "Y Xia",
        "Z Zhao"
      ],
      "year": "2022",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "88",
      "title": "Multi-head attention fusion networks for multi-modal speech emotion recognition",
      "authors": [
        "C.-P Ho",
        "C.-C Yang",
        "S Kim",
        "Y.-N Lee"
      ],
      "year": "2020",
      "venue": "Computer Speech & Language"
    },
    {
      "citation_id": "89",
      "title": "Speech emotion recognition using self-supervised features",
      "authors": [
        "E Morais",
        "R Hoory",
        "W Zhu",
        "I Gat",
        "M Damasceno",
        "H Aronowitz"
      ],
      "year": "2022",
      "venue": "ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "90",
      "title": "Matr: Multimodal medical image fusion via multiscale adaptive transformer",
      "authors": [
        "W Tang",
        "F He",
        "Y Liu",
        "Y Duan"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Image Processing"
    },
    {
      "citation_id": "91",
      "title": "Multimodal speech emotion recognition using audio and text",
      "authors": [
        "D Yoon",
        "S Lee",
        "H Lee"
      ],
      "year": "2018",
      "venue": "2018 IEEE Spoken Language Technology Workshop (SLT)"
    },
    {
      "citation_id": "92",
      "title": "Hybrid transformer: a flexible and efficient neural network for multimodal applications",
      "authors": [
        "Z Jin",
        "J Li",
        "J Tang"
      ],
      "year": "2021",
      "venue": "Proceedings of the 2021 Conference of the North American Chapter"
    },
    {
      "citation_id": "93",
      "title": "A database of german emotional speech",
      "authors": [
        "F Burkhardt",
        "A Paeschke",
        "M Rolfes",
        "W Sendlmeier",
        "B Weiss"
      ],
      "year": "2005",
      "venue": "Interspeech"
    },
    {
      "citation_id": "94",
      "title": "Emovo corpus: an italian emotional speech database",
      "authors": [
        "G Costantini",
        "I Iaderola",
        "A Paoloni",
        "M Todisco"
      ],
      "year": "2014",
      "venue": "Proceedings of the ninth international conference on language resources and evaluation (LREC'14)"
    },
    {
      "citation_id": "95",
      "title": "Self supervised adversarial domain adaptation for cross-corpus and crosslanguage speech emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "S Khalifa",
        "R Jurdak",
        "B Schuller"
      ],
      "year": "2022",
      "venue": "IEEE Transactions on Affective Computing"
    },
    {
      "citation_id": "96",
      "title": "Learning alignment for multimodal emotion recognition from speech",
      "authors": [
        "H Xu",
        "H Zhang",
        "K Han",
        "Y Wang",
        "Y Peng",
        "X Li"
      ],
      "year": "2019",
      "venue": "Learning alignment for multimodal emotion recognition from speech",
      "arxiv": "arXiv:1909.05645"
    },
    {
      "citation_id": "97",
      "title": "Fusion techniques for utterance-level emotion recognition combining speech and transcripts",
      "authors": [
        "J Sebastian",
        "P Pierucci"
      ],
      "year": "2019",
      "venue": "Interspeech"
    },
    {
      "citation_id": "98",
      "title": "A multi-scale fusion framework for bimodal speech emotion recognition",
      "authors": [
        "M Chen",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "Interspeech"
    },
    {
      "citation_id": "99",
      "title": "Multimodal emotion recognition using crossmodal attention and 1d convolutional neural networks",
      "authors": [
        "D Krishna",
        "A Patil"
      ],
      "year": "2020",
      "venue": "Interspeech"
    },
    {
      "citation_id": "100",
      "title": "Multimodal cross-and selfattention network for speech emotion recognition",
      "authors": [
        "L Sun",
        "B Liu",
        "J Tao",
        "Z Lian"
      ],
      "year": "2021",
      "venue": "ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "101",
      "title": "Ctnet: Conversational transformer network for emotion recognition",
      "authors": [
        "Z Lian",
        "B Liu",
        "J Tao"
      ],
      "year": "2021",
      "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
    },
    {
      "citation_id": "102",
      "title": "Towards the explainability of multimodal speech emotion recognition",
      "authors": [
        "P Kumar",
        "V Kaushik",
        "B Raman"
      ],
      "year": "2021",
      "venue": "Interspeech"
    },
    {
      "citation_id": "103",
      "title": "Urdu speech corpus for emotion recognition",
      "authors": [
        "S Latif",
        "R Rana",
        "J Qadir",
        "J Epps"
      ],
      "year": "2018",
      "venue": "Data in brief"
    },
    {
      "citation_id": "104",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "B Schuller",
        "S Steidl",
        "A Batliner",
        "F Burkhardt",
        "L Devillers",
        "C Müller",
        "S Narayanan"
      ],
      "year": "2010",
      "venue": "2010 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "105",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "106",
      "title": "Using large pre-trained models with cross-modal attention for multi-modal emotion recognition",
      "year": "2021",
      "venue": "Using large pre-trained models with cross-modal attention for multi-modal emotion recognition"
    },
    {
      "citation_id": "107",
      "title": "Long short-term memory",
      "authors": [
        "S Hochreiter",
        "J Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural computation"
    }
  ]
}