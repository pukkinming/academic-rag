{
  "paper_id": "2110.06309v3",
  "title": "Exploring Wav2Vec 2.0 Fine Tuning For Improved Speech Emotion Recognition",
  "published": "2021-10-12T19:55:55Z",
  "authors": [
    "Li-Wei Chen",
    "Alexander Rudnicky"
  ],
  "keywords": [
    "Speech emotion recognition",
    "deep neural networks",
    "wav2vec 2.0",
    "fine-tuning",
    "pretrained models"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "While Wav2Vec 2.0 has been proposed for speech recognition (ASR), it can also be used for speech emotion recognition (SER); its performance can be significantly improved using different fine-tuning strategies. Two baseline methods, vanilla fine-tuning (V-FT) and task adaptive pretraining (TAPT) are first presented. We show that V-FT is able to outperform state-of-the-art models on the IEMOCAP dataset. TAPT, an existing NLP fine-tuning strategy, further improves the performance on SER. We also introduce a novel fine-tuning method termed P-TAPT, which modifies the TAPT objective to learn contextualized emotion representations. Experiments show that P-TAPT performs better than TAPT, especially under low-resource settings. Compared to prior works in this literature, our top-line system achieved a 7.4% absolute improvement in unweighted accuracy (UA) over the stateof-the-art performance on IEMOCAP. Our code is publicly available. 1",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) remains one of the key components in human-machine interaction and in human communication systems. With the development of deep learning, several attempts  [1, 2, 3]  have been made to automatically learn emotion representations from audio signals using neural nets. However, the improvement of deep learning based systems is often limited by the lack of annotated data. Commonly used SER datasets  [4, 5, 6]  are relatively small in size in comparison to automatic speech recognition (ASR) datasets. Moreover, systems trained on these datasets may not generalize well to other domains such as call centers.\n\nSelf-supervised pretrained models  [7, 8]  provide a solution by first learning from a large scale speech corpus without explicit labeling. The knowledge learned from pretraining can be transferred to downstream tasks by either using the model as a feature extractor or directly fine-tuning the whole model. While first introduced for the purpose of natural language processing (NLP), several pretrained models  [9, 10, 11]  1 https://github.com/b04901014/FT-w2v2-ser have been developed for speech processing. Wav2vec  [9]  is a multi-layer convolutional neural network (CNN) trained to predict future frames conditioned on past frames by minimizing a contrastive loss. On the other hand, wav2vec 2.0  [10]  is a transformer-based model that adopts a masked learning objective to predict missing frames from the remaining context.\n\nDespite the success of these methods in ASR, speaker verification, and mispronunciation detection  [10, 12, 13] , only a few attempts  [14, 15, 16]  have been made to apply them on SER. Boigne et al.  [14]  find that wav2vec features are superior to traditional spectral-based features on SER. Xia et al.  [15]  compare features extracted with different time spans and conclude that features with longer temporal context such as wav2vec perform better on SER. Pepino et al.  [16]  show that features extracted from a linear combination of layers outperform singe layer representations in wav2vec 2.0 on SER. While these studies demonstrated the usefulness of the pretrained models as feature extractors, little research has been conducted on fine-tuning them for SER.\n\nOne persistent issue on fine-tuning pretrained models is the mismatch between pretraining and target domain  [17, 18] . Task adaptive pretraining (TAPT)  [17]  is proposed to resolve the domain shift by continuing the pretraining process on the target dataset. Hsu et al.  [18]  show that TAPT greatly improves generalization and robustness on ASR when the pretraining and fine-tuning data are dissimilar. Since the speech in the pretraining ASR corpus differs from emotive speech in multiple regards  [19] , we consider TAPT a compelling method for fine-tuning on SER.\n\nIn this paper, we explore methods for fine-tuning wav2vec 2.0 on SER. We show that by adding a simple neural network on top of wav2vec 2.0, vanilla fine-tuning (V-FT) outperforms state-of-the-art (SOTA) methods on the IEMOCAP  [4]  dataset. In addition, with V-FT as a baseline, TAPT significantly boosts the performance of fine-tuning wav2vec 2.0 on SER. Furthermore, motivated by previous works on the benefits of segment-based emotion features  [1, 20, 15]  and selfsupervised representation learning  [11, 21] , we developed a novel fine-tuning procedure for SER which yields even better performance, especially in low-resource conditions. Finally, we achieve a 7.4% absolute increase on unweighted accuracy (UA) over the SOTA performance on IEMOCAP.\n\nFig.  1 : System overview of our methods. (a) Emotion state estimation phase of P-TAPT. An additional CNN with stride 2 is used to align the time steps between wav2vec and wav2vec 2.0. The output of cluster assignments will be used as pseudo-labels for the P-TAPT objective. (b) Model architecture and pretraining objective of wav2vec 2.0 along with our P-TAPT objective.",
      "page_start": 1,
      "page_end": 2
    },
    {
      "section_name": "Method",
      "text": "We first review wav2vec 2.0, which serves as the backbone model for the methods we examine. We then present the two baseline methods we established. Finally, we introduce pseudo-label task adaptive pretraining (P-TAPT), a novel method we designed to fine-tune wav2vec 2.0 on SER. Feature encoder is a multi-layer CNN that processes the input signal into low-level features. Based on this representation, the transformer module is further applied to produce contextualized representation. The quantization module discretizes the low-level features into a trainable codebook. To train the model, part of the low-level features are masked from the transformer module, and the objective is to identify the quantized version of the masked features based on its context.  2",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Comparing Methods",
      "text": "As there is no existing baseline system fine-tuning wav2vec 2.0 on SER, we created two baselines. One is the conventional fine-tuning method, and the other is task adaptive pretraining which is first introduced in NLP.\n\nVanilla fine-tuning. Wav2vec 2.0 differs from its NLP counterparts  [7]  in that there is no utterance-level pretraining task to naturally form a sentence representation. As a consequence, aggregation across time steps is required to fine-tune on utterance level classification tasks. We experimented with different configurations and found that using average pooling on the final layer is simple yet effective for SER. Specifically, the final contextualized representation extracted by wav2vec 2.0 is first processed by a global average pooling across the time dimension, then followed by the ReLU activation and a single linear layer to predict the emotion categories. In addition, a modified version of SpecAugment  [22]  proposed in wav2vec 2.0 is applied during training for better generalization. We will use this architecture for the fine-tuning stage of all three methods. We abbreviate the vanilla fine-tuning method as V-FT.\n\nTask adaptive pretraining. Task adaptive pretraining (TAPT)  [17]  is a simple but effective method to fine-tune pretrained language models  [7]  on domain-specific tasks. It bridges the difference between the pretraining and target domain by continuing to pretrain on the target dataset. In this paper, we examine TAPT as one of the methods of fine-tuning wav2vec 2.0 on SER. To distinguish from the original pretraining and fine-tuning stage, we define an intermediate task adaptation stage for the continual pretraining process.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Pseudo-Label Task Adaptive Pretraining",
      "text": "While TAPT adapts to emotive speech by continual training with the pretraining objective, it does not make use of emotion labels. Essentially, the contextualized representations obtained will be general features suitable for various downstream tasks. As we only focus on SER, we propose to adapt this objective to generate emotion-specific features. Instead of identifying the missing low-level features, we focus on predicting the emotion state of the masked sequence. One advantage it brings is better data efficiency. Reconstruction of missing audio parts is a more complicated task, which makes the model vulnerable to over-fitting. Additionally, it simplifies the fine-tuning stage as it already filters out information unrelated to emotion recognition from the contextualized representation.\n\nHowever, frame-level emotion states need to be recog-nized to realize our method. While only utterance-level emotion labels are given for most of the SER dataset, several studies  [15, 1, 20]  indicate that frame-level emotion information can still be inferred by training with a segment-based classification objective. Particularly, as shown in Figure  1 .a, we finetune wav2vec to extract frame-level emotion representation that is useful for predicting an utterance-level emotion label. We find that using CNN architectures such as wav2vec is important since the locality of CNN preserves sequential structure. After training, we run k-means clustering algorithm  [23]  on all of the extracted representations from the target dataset.\n\nAs Mathilde et al.  [21]  have shown, the k-means cluster assignments on intermediate layers of CNN classifiers can capture information related to the target labels. Therefore, we interpret this cluster assignment as a pseudo-label that represents the local emotion state.\n\nWe then replace the TAPT objective with our new P-TAPT objective. We add a position-wise linear head composed of two linear layers to predict the k-means cluster assignments of the masked frames. In practice, we run multiple k-means clustering with a different number of clusters, and our model needs to predict an ensemble of cluster assignments with multiple linear heads. This cluster ensemble technique is shown to facilitate representation learning in HuBERT  [11] , a recent self-supervised speech representation learning model.",
      "page_start": 2,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Dataset",
      "text": "We use two datasets for evaluation, IEMOCAP  [4]  and SAVEE  [5] . We only use the speech modality.\n\nIEMOCAP. Interactive Emotional Dyadic Motion Capture (IEMOCAP) is a popular dataset for evaluating SER systems. It contains five recording sessions, each with one male speaker and one female speaker. To compare with previous works, we use the default labels provided by IEMO-CAP. However, only four emotion categories are considered: neutral, sad, angry, and happy. In particular, the \"excited\" category is merged with \"happy\" due to its sparsity in the dataset. The total amount of speech is about 7 hours.\n\nSAVEE. The Surrey Audio-Visual Expressed Emotion (SAVEE) dataset contains four male speakers: DC, JE, JK, and KL. Each speaker reads out the same set of 120 sentences labeled with one of the 7 emotion categories: angry, disgust, sad, fear, happy, surprise, and neutral. We use all of the emotion categories, which results in 480 utterances with a total of 30 minutes of speech.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Training And Evaluation Procedure",
      "text": "All experiments use the same learning rate 1 × 10 - 4  with Adam optimizer  [24] . For the wav2vec model, we use a pretrained model developed by Facebook AI 3 . We build our 3 https://github.com/pytorch/fairseq/tree/master/examples/wav2vec wav2vec 2.0 implementation on top of the huggingface implementation and adopt a pretrained model checkpoint from Facebook AI 4 . Both models are pretrained on the unsupervised speech of LibriSpeech 960h  [25]  without transcriptions. We evaluate our systems using unweighted accuracy (UA)  [2]  under a speaker-independent setting; the speakers in the test set are excluded from the training data. Additional implementation details are provided in our github repository. We run each experiment 5 times for the full IEMOCAP dataset and 20 times for SAVEE and on sub-sampled versions of IEMOCAP. Additionally, we observe that wav2vec 2.0 fails to converge with some of the random seeds. Therefore we discard and rerun outlier runs where the performance is outside two standard deviations from the mean.\n\nIEMOCAP. To have a fair comparison with the majority of previous works, we split the dataset by leaving one session out as test set, the remaining four sessions are used for training. Note that most of the papers using IEMOCAP do not explicitly define their validation set  [26] . We therefore train with all four sessions for a fixed 15 epochs without validation using a batch size of 64. The number of epochs is chosen so that each of our competing methods converges in terms of training loss.\n\nSAVEE. A similar evaluation procedure is used for SAVEE. In each fold, one speaker is left out for testing and the remaining three are used for training. We increase the number of training epochs to 30 and the batch size is halved to 32 for the smaller training set.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results And Discussion",
      "text": "",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison Of Fine-Tuning Methods",
      "text": "Table  1  compares performance for the fine-tuning methods on IEMOCAP. For all sessions except the first, TAPT yields a noticeable improvement over V-FT, and P-TAPT performs better than TAPT for all sessions. On the other hand, Table  2  shows that on SAVEE, both TAPT and P-TAPT outperform V-FT by a large margin. However, the performance of P-TAPT is very close to that of TAPT on SAVEE. We analyze these results by considering the characteristics of SAVEE and IEMOCAP.\n\nDomain shift and linguistic content. We first quantify the domain shift between both datasets and the pretraining dataset. We take the wav2vec 2.0 model pretrained on Lib-riSpeech and calculate the pretraining loss on both datasets along with the test set of LibriSpeech. Table  3  verifies the presence of domain shift on both datasets providing room for TAPT to improve. A smaller loss indicates that SAVEE is closer to LibriSpeech as the model can already generalize well to SAVEE. However, this improvement is larger on SAVEE than IEMOCAP despite the smaller domain shift. We observe a strong correlation between linguistic content and emotion labels in SAVEE.  5  We conjecture that this correlation is captured by our model and surpasses human evaluators who annotate emotion from only para-linguistic information. This also explains why P-TAPT does not further improve TAPT, as the TAPT objective is already suitable for modeling linguistic information. Nonetheless, in more naturally elicited emotional conversations (IEMOCAP), P-TAPT performs better than TAPT. Data efficiency. We also investigated the behavior of our methods when presented with different amounts of training data. Specifically, We fix session 5 of IEMOCAP as the heldout test set, and gradually halve the number of training examples in the remaining four sessions by random selection. We compare TAPT and P-TAPT using the ratio of their corresponding improvements over V-FT. A lower ratio indicates that the improvement from P-TAPT is more significant than TAPT. As shown in Table  4 , this ratio is lower under lowresource settings with one hour or less of training data. Thus P-TAPT is more data-efficient than TAPT. As mentioned in Section 2.3, we attribute this to the change of objective from the reconstruction of audio frames to the prediction of emotion states which is less data-intensive though it requires labeled data.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Comparison With Prior Works",
      "text": "Table  5  compares our performance on IEMOCAP to that of existing SOTA models. We only include methods that evaluate under speaker-independent settings. Simply fine-tuning the wav2vec 2.0 model (using V-FT) outperforms wav2vec 2.0 without fine-tuning  [16]  by 3.6% absolute UA. The P-TAPT method provides 7.4% absolute improvement over SOTA models on IEMOCAP. We also show performance for methods that use both speech and text  [27, 28] ; our audioonly method appears comparable.   [15]  Waveform 66.9 Wav2vec 2.0 w/o. FT  [16]  Wav2vec 2.0 66.3 Wav2vec 2.0 w. V-FT Waveform 69.9 Wav2vec 2.0 w. TAPT Waveform 73.5 Wav2vec 2.0 w. P-TAPT Waveform",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "74.3",
      "text": "Audio + Text  [27]  MFCC+ALBERT  7 72.1 Audio + ASR  [28]  MFCC+BERT 75.9",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Conclusion",
      "text": "We describe different fine-tuning strategies for wav2vec 2.0 on SER. These strategies produce SOTA performance on IEMOCAP, a well-studied corpus. We verify the presence of domain shift in SER and demonstrate that addressing it improves performance. We describe an algorithm for learning contextualized emotion representation and show its advantage in fine-tuning a wav2vec 2.0 model for SER. We believe that these techniques can be generalized to other tasks and can provide a basis for research on the utility of contextualized emotion representation. We intend to continue exploring the usefulness of this approach, in a multi-modal setting.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Acknowledgements",
      "text": "We are grateful to PwC USA as well as to The Digital Transformation and Innovation Center at Carnegie Mellon University for supporting our research. We thank Yangyang Xia and Richard M. Stern for the discussions and feedback.",
      "page_start": 4,
      "page_end": 4
    }
  ],
  "figures": [
    {
      "caption": "Figure 1: System overview of our methods. (a) Emotion state estimation phase of P-TAPT. An additional CNN with stride 2 is",
      "page": 2
    },
    {
      "caption": "Figure 1: a, we ﬁne-",
      "page": 3
    }
  ],
  "tables": [
    {
      "caption": "Table 1: compares performance for the ﬁne-tuning methods on",
      "page": 3
    },
    {
      "caption": "Table 3: veriﬁes the",
      "page": 3
    },
    {
      "caption": "Table 1: Comparison of methods on IEMOCAP in UA(%)",
      "page": 4
    },
    {
      "caption": "Table 2: Comparison of methods on SAVEE in UA(%)",
      "page": 4
    },
    {
      "caption": "Table 4: , this ratio is lower under low-",
      "page": 4
    },
    {
      "caption": "Table 3: Wav2vec 2.0 pretraining loss on different datasets",
      "page": 4
    },
    {
      "caption": "Table 5: compares our performance on IEMOCAP to that of",
      "page": 4
    },
    {
      "caption": "Table 4: Comparison of methods on data efﬁciency in UA(%)",
      "page": 4
    },
    {
      "caption": "Table 5: Comparison with prior works on IEMOCAP",
      "page": 4
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Evaluating deep learning architectures for speech emotion recognition",
      "authors": [
        "H Fayek",
        "M Lech",
        "L Cavedon"
      ],
      "year": "2017",
      "venue": "Neural Networks"
    },
    {
      "citation_id": "3",
      "title": "Speech emotion recognition using deep neural network and extreme learning machine",
      "authors": [
        "K Han",
        "D Yu",
        "I Tashev"
      ],
      "year": "2014",
      "venue": "Fifteenth annual conference of the international speech communication association"
    },
    {
      "citation_id": "4",
      "title": "Attention based fully convolutional network for speech emotion recognition",
      "authors": [
        "Y Zhang",
        "J Du",
        "Z Wang",
        "J Zhang",
        "Y Tu"
      ],
      "year": "2018",
      "venue": "2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference"
    },
    {
      "citation_id": "5",
      "title": "IEMO-CAP: Interactive emotional dyadic motion capture database",
      "authors": [
        "C Busso",
        "M Bulut",
        "C.-C Lee",
        "A Kazemzadeh",
        "E Mower",
        "S Kim",
        "J Chang",
        "S Lee",
        "S Narayanan"
      ],
      "year": "2008",
      "venue": "Language resources and evaluation"
    },
    {
      "citation_id": "6",
      "title": "Surrey audio-visual expressed emotion (savee) database",
      "authors": [
        "P Jackson",
        "S Haq"
      ],
      "year": "2011",
      "venue": "Surrey audio-visual expressed emotion (savee) database"
    },
    {
      "citation_id": "7",
      "title": "The Ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English",
      "authors": [
        "S Livingstone",
        "F Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "8",
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "authors": [
        "J Devlin",
        "M.-W Chang",
        "K Lee",
        "K Toutanova"
      ],
      "year": "2019",
      "venue": "NAACL-HLT"
    },
    {
      "citation_id": "9",
      "title": "Albert: A lite bert for self-supervised learning of language representations",
      "authors": [
        "Z Lan",
        "M Chen",
        "S Goodman",
        "K Gimpel",
        "P Sharma",
        "R Soricut"
      ],
      "year": "2020",
      "venue": "International Conference on Learning Representations"
    },
    {
      "citation_id": "10",
      "title": "wav2vec: Unsupervised Pre-Training for Speech Recognition",
      "authors": [
        "S Schneider",
        "A Baevski",
        "R Collobert",
        "M Auli"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "11",
      "title": "wav2vec 2.0: A framework for self-supervised learning of speech representations",
      "authors": [
        "A Baevski",
        "Y Zhou",
        "A Mohamed",
        "M Auli"
      ],
      "year": "2020",
      "venue": "Advances in Neural Information Processing Systems"
    },
    {
      "citation_id": "12",
      "title": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units",
      "authors": [
        "W.-N Hsu",
        "B Bolte",
        "Y.-H Tsai",
        "K Lakhotia",
        "R Salakhutdinov",
        "A Mohamed"
      ],
      "year": "2021",
      "venue": "Hubert: Self-supervised speech representation learning by masked prediction of hidden units"
    },
    {
      "citation_id": "13",
      "title": "Exploring wav2vec 2.0 on Speaker Verification and Language Identification",
      "authors": [
        "Z Fan",
        "M Li",
        "S Zhou",
        "B Xu"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "14",
      "title": "A Study on Fine-Tuning wav2vec2.0 Model for the Task of Mispronunciation Detection and Diagnosis",
      "authors": [
        "L Peng",
        "K Fu",
        "B Lin",
        "D Ke",
        "J Zhan"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "15",
      "title": "Recognizing more emotions with less data using self-supervised transfer learning",
      "authors": [
        "J Boigne",
        "B Liyanage",
        "T Östrem"
      ],
      "year": "2020",
      "venue": "ArXiv"
    },
    {
      "citation_id": "16",
      "title": "Temporal Context in Speech Emotion Recognition",
      "authors": [
        "Y Xia",
        "L.-W Chen",
        "A Rudnicky",
        "R Stern"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "17",
      "title": "Emotion Recognition from Speech Using wav2vec 2.0 Embeddings",
      "authors": [
        "L Pepino",
        "P Riera",
        "L Ferrer"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "18",
      "title": "Don't stop pretraining: Adapt language models to domains and tasks",
      "authors": [
        "S Gururangan",
        "A Marasović",
        "S Swayamdipta",
        "K Lo",
        "I Beltagy",
        "D Downey",
        "N Smith"
      ],
      "year": "2020",
      "venue": "Proceedings of ACL"
    },
    {
      "citation_id": "19",
      "title": "Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training",
      "authors": [
        "W.-N Hsu",
        "A Sriram",
        "A Baevski",
        "T Likhomanenko",
        "Q Xu",
        "V Pratap",
        "J Kahn",
        "A Lee",
        "R Collobert",
        "G Synnaeve",
        "M Auli"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    },
    {
      "citation_id": "20",
      "title": "Influence of emotion and focus location on prosody in matched statements and questions",
      "authors": [
        "M Pell"
      ],
      "year": "2001",
      "venue": "The Journal of the Acoustical Society of America"
    },
    {
      "citation_id": "21",
      "title": "Advancing Multiple Instance Learning with Attention Modeling for Categorical Speech Emotion Recognition",
      "authors": [
        "S Mao",
        "P Ching",
        "C.-C Kuo",
        "T Lee"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "22",
      "title": "Deep clustering for unsupervised learning of visual features",
      "authors": [
        "M Caron",
        "P Bojanowski",
        "A Joulin",
        "M Douze"
      ],
      "year": "2018",
      "venue": "European Conference on Computer Vision"
    },
    {
      "citation_id": "23",
      "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
      "authors": [
        "D Park",
        "W Chan",
        "Y Zhang",
        "C.-C Chiu",
        "B Zoph",
        "E Cubuk",
        "Q Le"
      ],
      "year": "2019",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "24",
      "title": "Least squares quantization in pcm",
      "authors": [
        "S Lloyd"
      ],
      "year": "1982",
      "venue": "IEEE Transactions on Information Theory"
    },
    {
      "citation_id": "25",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "D Kingma",
        "J Ba"
      ],
      "year": "2015",
      "venue": "3rd International Conference on Learning Representations, ICLR 2015"
    },
    {
      "citation_id": "26",
      "title": "Librispeech: An ASR corpus based on public domain audio books",
      "authors": [
        "V Panayotov",
        "G Chen",
        "D Povey",
        "S Khudanpur"
      ],
      "year": "2015",
      "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing"
    },
    {
      "citation_id": "27",
      "title": "CNN+LSTM Architecture for Speech Emotion Recognition with Data Augmentation",
      "authors": [
        "C Etienne",
        "G Fidanza",
        "A Petrovskii",
        "L Devillers",
        "B Schmauch"
      ],
      "year": "2018",
      "venue": "Proc. Workshop on Speech, Music and Mind"
    },
    {
      "citation_id": "28",
      "title": "A Multi-Scale Fusion Framework for Bimodal Speech Emotion Recognition",
      "authors": [
        "M Chen",
        "X Zhao"
      ],
      "year": "2020",
      "venue": "Proc. Interspeech"
    },
    {
      "citation_id": "29",
      "title": "Speech Emotion Recognition Based on Attention Weight Correction Using Word-Level Confidence Measure",
      "authors": [
        "J Santoso",
        "T Yamada",
        "S Makino",
        "K Ishizuka",
        "T Hiramura"
      ],
      "year": "2021",
      "venue": "Proc. Interspeech 2021"
    }
  ]
}