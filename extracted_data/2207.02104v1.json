{
  "paper_id": "2207.02104v1",
  "title": "A Cross-Corpus Study On Speech Emotion Recognition",
  "published": "2022-07-05T15:15:22Z",
  "authors": [
    "Rosanna Milner",
    "Md Asif Jalal",
    "Raymond W. M. Ng",
    "Thomas Hain"
  ],
  "keywords": [
    "speech emotion recognition",
    "cross-corpus",
    "bidirectional LSTM",
    "attention",
    "domain adversarial training"
  ],
  "sections": [
    {
      "section_name": "Abstract",
      "text": "For speech emotion datasets, it has been difficult to acquire large quantities of reliable data and acted emotions may be over the top compared to less expressive emotions displayed in everyday life. Lately, larger datasets with natural emotions have been created. Instead of ignoring smaller, acted datasets, this study investigates whether information learnt from acted emotions is useful for detecting natural emotions. Cross-corpus research has mostly considered cross-lingual and even cross-age datasets, and difficulties arise from different methods of annotating emotions causing a drop in performance. To be consistent, four adult English datasets covering acted, elicited and natural emotions are considered. A state-of-the-art model is proposed to accurately investigate the degradation of performance. The system involves a bi-directional LSTM with an attention mechanism to classify emotions across datasets. Experiments study the effects of training models in a cross-corpus and multi-domain fashion and results show the transfer of information is not successful. Out-of-domain models, followed by adapting to the missing dataset, and domain adversarial training (DAT) are shown to be more suitable to generalising to emotions across datasets. This shows positive information transfer from acted datasets to those with more natural emotions and the benefits from training on different corpora.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Introduction",
      "text": "Speech emotion recognition (SER) aims to automatically detect human emotional states from audio  [1, 2]  and many reviews into methods, features and datasets exist  [3, 4, 5, 6] . The problem is complex because emotion cannot be clearly defined, let alone accurately detected. This was initially analysed in the field of psychology where various tools to represent human emotion were studied, such as Plutchik's wheel of emotion  [7]  and the hourglass of emotions  [8] . In SER, it is common to adopt classification schema similar to Ekmans's 'big-six' emotions  [9] , which are happy, sad, anger, surprise, disgust and fear.\n\nMany SER studies carry out investigations on specially designed datasets in which human voice is recorded and emotions are annotated. This is challenging as many datasets vary in terms of language, age, labelling scheme (categorical or dimensional), emotions annotated and how the emotion was produced (acted, elicited or simulated, or natural). A lot of research within SER has focussed on a single dataset avoiding different annotation problems  [10] . Cross-corpus has been investigated in the past with initial work by Schuller et al  [11]  using various normalising schemes across six datasets. In  [12] , to relieve the feature distribution mismatch between training and test speech, importance weights are learnt for a support vector machine (SVM) which applies three domain adaption methods for children's speech corpora. Alternatively,  [13, 14, 15]  learnt new representations for training and test speech via an autoencorder based domain adaptation framework. The domain-adaptive least squares regression model (DALSR)  [16, 17]  calculates the maximum mean discrepancy (MMD) to balance the feature distribution difference using transfer non-negative matrix factorisation (TNNMF). Kim et al.  [18]  used auxiliary gender and naturalness recognition tasks in a multi-task learning setting for emotion recognition. Most recently, to deal with the unsupervised problem, the domain-adaptive subspace learning (DoSL) approach was proposed where an SVM is trained based on labelled training set speech signals  [19] .\n\nHowever, the majority of this cross-corpus work has been cross-lingual and occasionally both cross-lingual and crossage. In the past this has been arguably necessary given the sparsity of emotional databases. But now with the introduction of larger datasets, such as the MOSEI database which has around 65 hours of natural emotion data  [20] , more effort can be put on cross-corpus research focussing on a single language and a single age group. Additionally, more natural datasets allow research to move away from unrealistic acted emotions. However, given the number of emotional datasets that have already been created, acted or not, it is interesting to investigate whether information can be learnt from these datasets to improve performance on others.\n\nDeep neural networks, such as long short-term memory (LSTM) networks  [21]  and attention mechanism  [22] , were employed in various emotion recognition studies  [23, 17, 18, 24] . Recently, in speech recognition, domain adversarial training (DAT) was proposed as a neural network adaptation approach to combat the undesirable variability across different data distributions  [25] . DAT is applicable to the SER task  [26] , and achieves representation learning even if the training and test domains are unknown. Specifically, a gradient reversal layer is applied in the domain classifier in an attempt to bring the representations closer. Training models using DAT not only saves time, by learning two representations simultaneously, it also aims to improve the representations learnt by using the transfer of information between the two tasks.\n\nThis work considers cross-corpus SER within the same language (English) and same age (adult) and investigates what can be learnt between acted and natural datasets, the first study of this kind. A bidirectional-LSTM with an attention mechanism approach is considered for training the crosscorpus models. The information transfer to other datasets is investigated and further experiments looking into domain adversarial training (DAT) and out-of-domain (OOD) models, then adapting these models, are considered. The approach is described in Section 2, the datasets studied are detailed in Section 3, the experimental setup and results are shown in Section 4, Section 4.9 contains the discussion and lastly, the conclusions are stated in Section 5.",
      "page_start": 1,
      "page_end": 1
    },
    {
      "section_name": "Approach",
      "text": "The proposed system is derived from the triple attention network described in  [24]  which is based on work by Nam et al  [27] . Both these systems use more than one modality (audio, visual and textual), whereas this work focusses on audio only. The network consists of an encoder which contains a bidirectional LSTM (BLSTM) followed by an attention mechanism and an emotion classifier. For DAT experiments, a domain classifier for corpus ID is included as well.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Bi-Directional Lstm (Blstm)",
      "text": "LSTM networks rely on the temporal order of the sequence, ignoring the future context. BLSTMs  [28]  introduce a second layer of hidden connections flowing in the opposite temporal order as a method to exploit the contextual information from the past and the future  [29] . The output vector from the BLSTM for a sequence of length T contains the temporal representation of the sequence from time step 1 to T -1. When considering the final time step, the overall prediction across the sequence is acquired. However, considering all the time steps, a temporal feature distribution over the sequence is obtained which is useful for SER tasks.",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Attention Mechanism",
      "text": "Attention mechanism has the flexibility of computing longterm inter-sequence dependencies. In this work, the attention value on the overall temporal feature space from the BLSTM is computed. The attention mechanism focuses the network onto specific parts of itself by computing the global mean which captures global information. The global mean is multiplied over the whole temporal vector to compute the posi-tional dependency of each element with tanh non-linearity. The resulting vector is used to compute the attention weights using sof tmax scoring. The soft attention mechanism is adopted for this work and the multiplicative method is applied as in  [24] , similar to  [27] , as the authors found their results similar to the standard concatenated approach  [22] . The main challenge for cross corpus SER is to minimise the longterm spatio-temporal variation between different corpora for a given categorical distribution and one solution is to model global spatio-temporal dependencies between corpora. The proposed framework with attention modelling is an efficient way to approach the problem.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Emotion Classifier",
      "text": "The predictor stage of the network contains a fully connected linear layer which projects the attention output down to the number of emotions present. This is then passed through a sof tmax layer before computing the loss.  [25] . Let i denote the index of the training samples, y denotes the label and d denotes the domain, the SER task is formulated as multitask learning with the primary task being emotion classification, with loss function L i y , and domain prediction being the auxiliary task, with loss function L i d . DAT models are trained in parallel with the aim to minimise the loss of the target label predictor (emotion category) and the loss of the domain label predictor (corpus ID). Following other DAT studies, it is assumed that the source of the data is an important indication of domain and so corpus ID is used as the domain label indicator. The network contains an arbitrary number of shared layers where their parameters contribute to both target and domain prediction. These shared layers provide high-level, regularised representation. Like most settings in multi-task learning, the network then branches off into the target and domain classifiers. The objective of the network is:",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Domain Classifier: Dat",
      "text": "",
      "page_start": 2,
      "page_end": 2
    },
    {
      "section_name": "Domain Adversarial Training (Dat) Is A Neural Network Training Technique Which Incorporates Domain Adaptation Into The Process Of Learning Representation",
      "text": "(1) which is minimised w.r.t. f and y but maximised w.r.t. d. The parameters of the shared layers are represented by θ f , and θ y and θ d represent the parameters of the target and domain classifier layer respectively. The number of training samples is referred to as n. To implement Equation 1, the first domain classifier layer is implemented as a gradient reversal layer. The gradients being back-propagated from the domain layer are reversed multiplying by a negative factor. The λ controls how optimisation is biased to the target or the domain.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Datasets",
      "text": "This work only considers English speaking adult datasets across three emotion production types: two acted datasets, eNTERFACE  [30]  and RAVDESS  [31] , one elicited dataset, IEMOCAP  [32] , and one natural dataset, MOSEI  [20] . Table  1  gives an overview of the emotion classes covered in each dataset.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Enterface",
      "text": "For eNTERFACE (ENT), Project 2 from 2005 has the bigsix emotion classes and contains about 1 hour of acted utterrances  [30] . There are 5 recordings of each emotion for every speaker. There are 44 speakers (8 female), however due to further inspection of the data this was reduced to 43 (Spkr6's recordings have not been segmented) and 1287 segments (Spkr23 is missing three happy segments). The speakers are from 14 nations, not all native speakers, resulting in different English accents. A training set is created with 38 speakers (Spkr1 to Spkr39 without Spkr6) and the test set contains the last 5 speakers (Spkr40 to Spkr44).",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Ravdess",
      "text": "RAVDESS (RAV) is another acted dataset containing 24 speakers (12 male and 12 female) with 1.5 hours of speech  [31] . The dataset contains the big-six emotions and also calm and neutral. It contains North American English accented speech. The data is split into the first 19 speakers for training and the last 5 for testing. The reference labels the emotions as 0 for non-existing emotion, 1 as exists and 2 as strong emotion. All 2 values have been changed to 1 for classification.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Iemocap",
      "text": "IEMOCAP (IEM) contains utterances from 10 speakers (5 male and 5 female) over 12 hours  [32] . There are five dyadic sessions (between two speakers) which are either scripted or improvised to elicit emotions. As well as the big-six emotions, it also contains other categories: excitement, frustration, neutral and other. The English spoken has a North American accent. The first four sessions, containing 8 speakers, are used as training data and the last session, containing 2 speakers, is used for testing. In the literature it is common for IEMOCAP to be evaluated as four classes: happy, sad, anger and neutral (where excitement is combined with happy to give 1636 happy segments across all the sessions)  [33] . This test set will be referred to as IEM4.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Mosei",
      "text": "MOSEI (MOS) is currently the largest sentiment and emotion dataset at around 65 hours of data and more than 1000 speakers  [20] . Utterances have been segmented from YouTube videos and annotated for the big-six emotions using Amazon Mechanical Turk. As data is collected from YouTube and the videos are not specifically designed as an emotion dataset, the emotional speech is seen as natural. The videos contain English speech but it is unclear which accents are covered. The official training, validation and test splits 1  for the ACL 2018 conference have been used, where the training and validation sets are combined for training. The emotion labels for classification, which are in the range 0 to 3, have been changed to binary values, whether the emotion exists or not.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experiments",
      "text": "Several different experiments are performed to study the effects of cross-corpus SER. Firstly, a comparison of the efficacy of different features in SER is carried out. Secondly, cross-corpus experiments are performed to see the effectiveness of SER using matched and unmatched data. This is followed by training a model on all the domains for comparison, referred to as multidomain, MD. Next, out-of-domain, OOD, and adaptation is investigated as well as a system with domain adversarial training, DAT.",
      "page_start": 3,
      "page_end": 3
    },
    {
      "section_name": "Experimental Setup",
      "text": "The system is implemented in PyTorch  [34] . The BLSTM contains two hidden layers of 512 nodes each. The output layer of size 1024 feeds in to the attention mechanism computing a context vector of size 128 which is projected to 1024 nodes. This is then passed to the predictor stage which linearly projects to the number of classes. in the PyTorch implementation. The Adam optimiser  [35]  is used with the initial learning rate of 0.0001. As Adam adaptively optimises the learning rate but does not change it, the PyTorch approach of ReduceLROnPlateau was investigated. The optimum patience setting was found to be 4 epochs with a multiplicative factor of 0.8. The models were trained to 200 epochs and the best model chosen by averaging the results across the datasets as adding a stopping criterion could influence the effect on the different datasets. Further work is necessary to find the optimum stopping criterion which does not give bias or priority towards one particular dataset. The training batch size was set to 1 as various batch sizes were tried fixing the segment lengths but no significant difference of performance was found. For the DAT experiment, the optimum negative variable λ was found to be 0.007. Table  2  shows the training and testing data splits for each dataset where only the big-six emotions are considered. For eNTERFACE and RAVDESS, the SHoUT speech activity detector  [36]  has been applied to remove the start and end silences around the speech as exact timing was not provided.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Evaluation",
      "text": "The metrics used to evaluate the approach are the unweighted accuracy (UA) and the weighted accuracy (WA). The UA calculates accuracy in terms of the total correct predictions divided by total samples, which gives the same weight to each class:\n\nwhere P is the number of correct positive instances (equivalent to T P + F N ) and N is the number of correct negative instances (equivalent to T N + F P ). As some of the datasets are imbalanced across the emotion classes, see Table  1 , the WA is calculated which weighs each class according to the number of samples in that class:",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Baseline Systems",
      "text": "Baseline system results from the literature are difficult to find due to the fact that this work requires the big-six emotions and many datasets contain more, or less, and therefore evaluation results are not equivalent.   [33]  presenting an attention pooling based representation learning method. A cross-corpus UA of 64.0% is shown in  [38]  where a twochannel system approach is adopted combining high-level statistic features with a CRNN. For WA, 56.1% is shown in  [39]  which also presents a cross-corpus WA of 48.4% applying factor analysis to find emotion factors to classify. For MOS, a WA of 52.5% is presented in  [24]  which this work is based on.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results: Acoustic Features",
      "text": "For deep learning, it has been shown that log-Mel filterbanks (LMFB) yield better performance over Mel frequency cepstral coefficients (MFCCs)  [40] . However, SER has seen better improvements when considering other acoustic features so different types are investigated, such as MFCC, PLP (perceptual linear prediction), and COVAREP  [41]  features. COVAREP features, which include pitch, were extracted applying the COVAREP feature extraction.m script and the rest were extracted using the HTK toolkit  [42] . The eNTERFACE dataset is used for investigating the training setup as it is the smallest dataset in terms of time and has a high number of speakers with almost completely balanced emotions. Table  3  displays the UA and WA results across the different features extracted. LMFBs with 23 dimensions outperform the rest in terms of UA and WA, therefore the remaining experiments are performed with 23 dimensional LMFBs.",
      "page_start": 4,
      "page_end": 4
    },
    {
      "section_name": "Results: Cross-Corpus",
      "text": "The cross-corpus (CC) results are seen in Table  4  where the performances in the bold diagonal refer to the matched con- Table  5 . Out-of-domain (OOD) and adaptation (+adaptation) results across the five testsets. There are two models for each dataset, OOD and OOD with adaptation.\n\ndition (trained and tested on the same dataset) and the nonbold performances are the mismatched condition (trained and tested on different datasets). Each row in the table refers to a single model. In terms of the baseline systems described in Section 4.3, asterisks are displayed next to comparable results. The cross-corpus and matched results achieve better results than the baselines. This shows the proposed system is set up well for recognising emotions from speech and acceptable to use for this study. The best performance is found in the matched condition as would be expected. The model trained on IEM achieves the best mismatched results for RAV and MOS testsets but not for ENT. As an elicited dataset, it can be argued to be close to both the acted and more natural scenarios. Comparing the models trained on the acted datasets, it can be seen that the RAV model performs better for IEM, IEM4 and MOS than the ENT model. This shows that despite both being similar acted datasets, there seems to be more useful information to learn from a model trained on RAV than on ENT. This reinforces the challenges from datasets with different annotations  [10] . Performance reduces when moving to the natural dataset MOS, showing the difficulty of classifying more naturally produced emotions.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results: Multi-Domain",
      "text": "The multi-domain (MD) model is trained on the four datasets and results are shown in Table  4 . The model does not outperform the cross-corpus matched results, except in one case. The 2.2% improvement in the WA performance for MOS begins to show how training on acted datasets could be beneficial for some natural datasets. However, the multi-domain model does consistently outperform the best cross-corpus mismatched results. For UA, the average improvement between the best mismatched model and the multi-domain model is 7.1%, whereas for WA this is larger at 11.8%. The gains are larger for ENT and RAV, the smaller acted datasets, than the IEM, IEM4 and MOS testsets. It is known that neural networks can improve performance with more data, so this shows the benefit of including different corpora, particularly for smaller datasets, despite the annotation challenges. Information about the emotions in different settings is shown to be learnt and beneficial across datasets.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results: Dat",
      "text": "The DAT model is trained on the four datasets and performance is displayed in Table  4 . Comparing to the multidomain model, the DAT model performs better in all testsets except for ENT. This shows, in most cases, how the domain information can help models to generalise for recognising emotions across datasets. In the ENT dataset, each emotion has five defined sentences which are spoken by every speaker. This high degree of matching between the training and test sets contributes to the good performance when training and testing on ENT. However, this matching property cannot be picked up effectively by the domain classifier. There are some small gains in comparison to the cross-corpus matched condition results but it is not consistent across the datasets or the two accuracies. Nevertheless, the DAT method outperforms the best mismatched results, as the multi-domain model does, reinforcing that data from different datasets can be useful for SER.\n\nExp.",
      "page_start": 5,
      "page_end": 5
    },
    {
      "section_name": "Results: Out-Of-Domain And Adaptation",
      "text": "The OOD results with the corresponding adaptation performance are displayed in Table  5 . There are four OOD models, each trained on three datasets. The performance for each testset shown comes from the mismatched trained model. Results for IEM and IEM4 are from the same model. The OOD models are then adapted to the dataset previously not included and this matched result is shown. The OOD results do not reach the accuracies of the multi-domain model, but they do outperform the worst cross-corpus mismatched result. This shows that learning on additional data is useful even if it's from different, mismatched datasets. When adapting the OOD models, large gains are seen reaching similar results as the multidomain and DAT models. The average UA improvement is 8.9% and 14.8% for WA. This shows how important it is to train on matched data.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Results: Emotion Classes",
      "text": "The experiments have shown how acted datasets can benefit more natural datasets with the multi-domain, DAT and OOD with adaptation models all achieving WA results outperforming the cross-corpus matched model for MOS. Complimentary to looking at the performances across datasets, comparing the performances between the emotion classes can give insight into how the models are behaving. The cross-corpus and DAT models are considered, and the results are displayed in Table  6  where the neutral performance comes from only one testset, IEM4. For the cross-corpus results, the bold numbers refer to the best performance for that emotion. For the UA results, it is fairly inconsistent which model is better, however the models trained on IEM and MOS give the better performances for all the emotions except for happy. For the WA results, it shows the opposite, that training on ENT is better across five of the seven emotions. This seems to suggest that acted datasets are good at distinguishing emotions, even across datasets with different emotion production types. The DAT model gives better performance across all emotions in terms of UA and WA, except for the weighted accuracy of fear and neutral emotions.",
      "page_start": 6,
      "page_end": 6
    },
    {
      "section_name": "Conclusions",
      "text": "This study has carried out an investigation into cross-corpus speech emotion recognition. It is the first study of its kind investigating the degradation of performance when moving from acted to more natural datasets. Initial experiments into the optimum acoustic features for this set-up showed that LMFB features outperform MFCCs, PLPs and COVAREP features. Cross-corpus experiments show the matched results outperform mismatched and that the model trained on the elicited dataset achieves best mismatched performance in most cases. In the multi-domain setting, the model outperforms the best mismatched results showing more data is beneficial, even if the emotions are in a different setting or production type. The DAT experiment shows how including the domain information does help the model to generalise and outperform the multi-domain model for all but one dataset.\n\nThe OOD models show that learning from multiple mismatched datasets is more useful than training on a single mismatched dataset, and adapting gives large improvements showing the importance of training on matched data. When looking across the emotion class performance, ENT data shows to be best in terms of WA for most of the emotions showing acted datasets may have their uses for the more natural datasets.",
      "page_start": 6,
      "page_end": 6
    }
  ],
  "figures": [],
  "tables": [
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "ABSTRACT"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "For speech emotion datasets,\nit has been difﬁcult\nto ac-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "quire large quantities of reliable data and acted emotions may"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "be over\nthe top compared to less expressive emotions dis-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "played in everyday life. Lately,\nlarger datasets with natural"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "emotions have been created.\nInstead of\nignoring smaller,"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "acted datasets,\nthis\nstudy investigates whether\ninformation"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "learnt\nfrom acted emotions\nis useful\nfor detecting natural"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "emotions.\nCross-corpus\nresearch\nhas mostly\nconsidered"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "cross-lingual\nand even cross-age datasets,\nand difﬁculties"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "arise from different methods of annotating emotions causing"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "a drop in performance. To be consistent,\nfour adult English"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "datasets covering acted, elicited and natural emotions are con-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "sidered. A state-of-the-art model\nis proposed to accurately"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "investigate the degradation of performance. The system in-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "volves a bi-directional LSTM with an attention mechanism"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "to classify emotions across datasets. Experiments study the"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "effects of training models in a cross-corpus and multi-domain"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "fashion and results\nshow the transfer of\ninformation is not"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "successful. Out-of-domain models, followed by adapting to"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "the missing dataset, and domain adversarial\ntraining (DAT)"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "are shown to be more suitable to generalising to emotions"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "across datasets.\nThis\nshows positive\ninformation transfer"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "from acted datasets to those with more natural emotions and"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "the beneﬁts from training on different corpora."
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "Index Terms:\nspeech emotion recognition, cross-corpus, bi-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "directional LSTM, attention, domain adversarial training"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "1.\nINTRODUCTION"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "Speech emotion recognition (SER) aims to automatically de-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "tect human emotional states from audio [1, 2] and many re-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "views into methods,\nfeatures and datasets exist\n[3, 4, 5, 6]."
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "The problem is complex because emotion cannot be clearly"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "deﬁned, let alone accurately detected. This was initially anal-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": ""
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "ysed in the ﬁeld of psychology where various tools to repre-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "sent human emotion were studied, such as Plutchik’s wheel of"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "emotion [7] and the hourglass of emotions [8].\nIn SER,\nit\nis"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "common to adopt classiﬁcation schema similar to Ekmans’s"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "‘big-six’ emotions [9], which are happy, sad, anger, surprise,"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "disgust and fear."
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "Many SER studies carry out\ninvestigations on specially"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "designed datasets in which human voice is recorded and emo-"
        },
        {
          "1University of Shefﬁeld, UK and 2Emotech Labs, UK": "tions are annotated. This is challenging as many datasets vary"
        }
      ],
      "page": 1
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "sal\nlayer is applied in the domain classiﬁer\nin an attempt\nto",
          "tional dependency of each element with tanh non-linearity.": "The resulting vector is used to compute the attention weights"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "bring the representations closer. Training models using DAT",
          "tional dependency of each element with tanh non-linearity.": "using sof tmax scoring.\nThe soft attention mechanism is"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "not only saves time, by learning two representations simulta-",
          "tional dependency of each element with tanh non-linearity.": "adopted for\nthis work and the multiplicative method is ap-"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "neously, it also aims to improve the representations learnt by",
          "tional dependency of each element with tanh non-linearity.": "plied as in [24], similar to [27], as the authors found their re-"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "using the transfer of information between the two tasks.",
          "tional dependency of each element with tanh non-linearity.": "sults similar to the standard concatenated approach [22]. The"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "This work considers cross-corpus SER within the same",
          "tional dependency of each element with tanh non-linearity.": "main challenge for cross corpus SER is to minimise the long-"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "language\n(English)\nand same\nage\n(adult)\nand investigates",
          "tional dependency of each element with tanh non-linearity.": "term spatio-temporal variation between different corpora for"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "what can be learnt between acted and natural datasets,\nthe",
          "tional dependency of each element with tanh non-linearity.": "a given categorical distribution and one solution is to model"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "ﬁrst study of this kind. A bidirectional-LSTM with an atten-",
          "tional dependency of each element with tanh non-linearity.": "global spatio-temporal dependencies between corpora.\nThe"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "tion mechanism approach is considered for training the cross-",
          "tional dependency of each element with tanh non-linearity.": "proposed framework with attention modelling is an efﬁcient"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "corpus models. The information transfer to other datasets is",
          "tional dependency of each element with tanh non-linearity.": "way to approach the problem."
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "investigated and further experiments looking into domain ad-",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "versarial\ntraining (DAT) and out-of-domain (OOD) models,",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "2.3. Emotion Classiﬁer"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "then adapting these models, are considered.\nThe approach",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "is described in Section 2,\nthe datasets studied are detailed in",
          "tional dependency of each element with tanh non-linearity.": "The predictor stage of the network contains a fully connected"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "Section 3,\nthe experimental setup and results are shown in",
          "tional dependency of each element with tanh non-linearity.": "linear layer which projects the attention output down to the"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "Section 4, Section 4.9 contains the discussion and lastly,\nthe",
          "tional dependency of each element with tanh non-linearity.": "number of emotions present. This is then passed through a"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "conclusions are stated in Section 5.",
          "tional dependency of each element with tanh non-linearity.": "sof tmax layer before computing the loss."
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "2. APPROACH",
          "tional dependency of each element with tanh non-linearity.": "2.4. Domain Classiﬁer: DAT"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "Domain adversarial training (DAT) is a neural network train-"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "The proposed system is derived from the triple attention net-",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "ing technique which incorporates domain adaptation into the"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "work described in [24] which is based on work by Nam et",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "process of learning representation [25]. Let i denote the in-"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "al [27]. Both these systems use more than one modality (au-",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "dex of the training samples, y denotes the label and d denotes"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "dio, visual and textual), whereas this work focusses on audio",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "the domain, the SER task is formulated as multitask learning"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "only. The network consists of an encoder which contains a bi-",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "with the primary task being emotion classiﬁcation, with loss"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "directional LSTM (BLSTM) followed by an attention mech-",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "function Li\ny, and domain prediction being the auxiliary task,"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "anism and an emotion classiﬁer. For DAT experiments, a do-",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "with loss function Li"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "main classiﬁer for corpus ID is included as well.",
          "tional dependency of each element with tanh non-linearity.": "d."
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "with the aim to minimise the loss of the target label predictor"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "(emotion category) and the loss of the domain label predictor"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "2.1. Bi-directional LSTM (BLSTM)",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "(corpus ID). Following other DAT studies,\nit\nis assumed that"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "the source of\nthe data is an important\nindication of domain"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "LSTM networks rely on the temporal order of the sequence,",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "and so corpus ID is used as the domain label\nindicator. The"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "ignoring the future context. BLSTMs [28] introduce a second",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "network contains an arbitrary number of shared layers where"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "layer of hidden connections ﬂowing in the opposite tempo-",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "their parameters contribute to both target and domain predic-"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "ral order as a method to exploit\nthe contextual\ninformation",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "tion. These shared layers provide high-level, regularised rep-"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "from the past and the future [29].\nThe output vector\nfrom",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "resentation. Like most settings in multi-task learning, the net-"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "the BLSTM for a sequence of length T contains the tempo-",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "work then branches off into the target and domain classiﬁers."
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "ral representation of the sequence from time step 1 to T − 1.",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "",
          "tional dependency of each element with tanh non-linearity.": "The objective of the network is:"
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "When considering the ﬁnal\ntime step,\nthe overall prediction",
          "tional dependency of each element with tanh non-linearity.": ""
        },
        {
          "and test domains are unknown. Speciﬁcally, a gradient rever-": "across the sequence is acquired. However, considering all the",
          "tional dependency of each element with tanh non-linearity.": ""
        }
      ],
      "page": 2
    },
    {
      "caption": "Table 1: Number of segments across the emotions in each",
      "data": [
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "IEMOCAP (IEM) contains utterances from 10 speakers\n(5"
        },
        {
          "3.3.\nIEMOCAP": "male and 5 female) over 12 hours [32]. There are ﬁve dyadic"
        },
        {
          "3.3.\nIEMOCAP": "sessions (between two speakers) which are either scripted or"
        },
        {
          "3.3.\nIEMOCAP": "improvised to elicit emotions. As well as the big-six emo-"
        },
        {
          "3.3.\nIEMOCAP": "frustra-\ntions,\nit also contains other categories:\nexcitement,"
        },
        {
          "3.3.\nIEMOCAP": "tion, neutral and other.\nThe English spoken has a North"
        },
        {
          "3.3.\nIEMOCAP": "American accent. The ﬁrst four sessions, containing 8 speak-"
        },
        {
          "3.3.\nIEMOCAP": "ers, are used as training data and the last session, containing 2"
        },
        {
          "3.3.\nIEMOCAP": "speakers, is used for testing. In the literature it is common for"
        },
        {
          "3.3.\nIEMOCAP": "IEMOCAP to be evaluated as four classes: happy, sad, anger"
        },
        {
          "3.3.\nIEMOCAP": "and neutral (where excitement is combined with happy to give"
        },
        {
          "3.3.\nIEMOCAP": "1636 happy segments across all\nthe sessions) [33]. This test"
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "set will be referred to as IEM4."
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "3.4. MOSEI"
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "MOSEI (MOS) is currently the largest sentiment and emotion"
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "dataset at around 65 hours of data and more than 1000 speak-"
        },
        {
          "3.3.\nIEMOCAP": "ers\n[20].\nUtterances have been segmented from YouTube"
        },
        {
          "3.3.\nIEMOCAP": "videos and annotated for the big-six emotions using Amazon"
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "Mechanical Turk. As data is collected from YouTube and the"
        },
        {
          "3.3.\nIEMOCAP": "videos are not speciﬁcally designed as an emotion dataset,"
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "the emotional speech is seen as natural. The videos contain"
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "English speech but\nit\nis unclear which accents are covered."
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "The ofﬁcial\ntraining, validation and test splits1\nfor the ACL"
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "2018 conference have been used, where the training and val-"
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "idation sets are combined for\ntraining.\nThe emotion labels"
        },
        {
          "3.3.\nIEMOCAP": ""
        },
        {
          "3.3.\nIEMOCAP": "for classiﬁcation, which are in the range 0 to 3, have been"
        },
        {
          "3.3.\nIEMOCAP": "changed to binary values, whether the emotion exists or not."
        },
        {
          "3.3.\nIEMOCAP": ""
        }
      ],
      "page": 3
    },
    {
      "caption": "Table 1: , the datasetisusedforinvestigatingthetrainingsetupasitisthe",
      "data": [
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "Train\n1137\n912\n2345\n-\n7193\n11587",
          "Features\n#dimensions\nUA%\nWA%": "LMFB\n20\n94.4\n90.0"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "hours\n0.6\n0.9\n3.1\n-\n13.7\n18.3",
          "Features\n#dimensions\nUA%\nWA%": "95.6\n92.0\nLMFB\n23"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "LMFB\n40\n92.7\n86.8"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "Test\n150\n240\n586\n1241\n2009\n4226",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "LMFB\n60\n94.4\n90.0"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "LMFB\n80\n94.9\n90.8"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "Table 2. For each of the four datasets, the number of training",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "PLP\n14\n90.7\n83.2"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "and testing segments are shown, where IEM4 is testing only.",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "MFCC\n13\n87.8\n78.0"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "COVAREP\n74\n84.4\n72.0"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "in the PyTorch implementation. The Adam optimiser [35] is",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "used with the initial learning rate of 0.0001. As Adam adap-",
          "Features\n#dimensions\nUA%\nWA%": "Table 3. Performance for ENT data where models have been"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "tively optimises the learning rate but does not change it,\nthe",
          "Features\n#dimensions\nUA%\nWA%": "trained using different acoustic features where bold shows the"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "PyTorch approach of ReduceLROnPlateau was investigated.",
          "Features\n#dimensions\nUA%\nWA%": "best performance."
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "The optimum patience setting was found to be 4 epochs with",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "a multiplicative factor of 0.8.\nThe models were trained to",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "states both unweighted accuracy and weighted accuracy. For"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "200 epochs and the best model chosen by averaging the re-",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "ENT, a system which applies a convolutional recurrent neural"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "sults across the datasets as adding a stopping criterion could",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "network (CRNN) with an attention mechanism [37] achieves"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "inﬂuence the effect on the different datasets. Further work is",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "a UA of 91.7%.\nFor 6-class classiﬁcation, RAV and IEM"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "necessary to ﬁnd the optimum stopping criterion which does",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "are not\ntypically evaluated this way,\nIEM results\nfocus on"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "not give bias or priority towards one particular dataset. The",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "the four classes (IEM4) mentioned in Section 3.3. The best"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "training batch size was set\nto 1 as various batch sizes were",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "UA found for the IEM4 test set\nis 71.8% [33] presenting an"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "tried ﬁxing the segment\nlengths but no signiﬁcant difference",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "attention pooling based representation learning method. A"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "of performance was found. For the DAT experiment, the op-",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "cross-corpus UA of 64.0% is\nshown in [38] where a two-"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "timum negative variable λ was found to be 0.007.",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "channel\nsystem approach is\nadopted combining high-level"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "Table 2 shows the training and testing data splits for each",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "statistic features with a CRNN. For WA, 56.1% is shown in"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "dataset where only the big-six emotions are considered. For",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "[39] which also presents a cross-corpus WA of 48.4% apply-"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "eNTERFACE and RAVDESS, the SHoUT speech activity de-",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "ing factor analysis to ﬁnd emotion factors to classify.\nFor"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "tector [36] has been applied to remove the start and end si-",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "MOS, a WA of 52.5% is presented in [24] which this work is"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "lences around the speech as exact timing was not provided.",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "based on."
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "4.2. Evaluation",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "4.4. Results: Acoustic features"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "The metrics used to evaluate the approach are the unweighted",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "accuracy (UA) and the weighted accuracy (WA). The UA cal-",
          "Features\n#dimensions\nUA%\nWA%": "For deep learning, it has been shown that log-Mel ﬁlterbanks"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "culates accuracy in terms of the total correct predictions di-",
          "Features\n#dimensions\nUA%\nWA%": "(LMFB) yield better performance over Mel\nfrequency cep-"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "vided by total samples, which gives the same weight to each",
          "Features\n#dimensions\nUA%\nWA%": "stral\ncoefﬁcients\n(MFCCs)\n[40].\nHowever, SER has\nseen"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "class:",
          "Features\n#dimensions\nUA%\nWA%": "better\nimprovements when considering other\nacoustic\nfea-"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "T P + T N",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "tures so different types are investigated, such as MFCC, PLP"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "(2)\nU A =",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "P + N",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "(perceptual\nlinear prediction), and COVAREP [41]\nfeatures."
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "where P is the number of correct positive instances (equiva-",
          "Features\n#dimensions\nUA%\nWA%": "COVAREP features, which include pitch, were extracted ap-"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "lent\nto T P + F N ) and N is the number of correct negative",
          "Features\n#dimensions\nUA%\nWA%": "plying the COVAREP feature extraction.m script and the rest"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "instances (equivalent to T N + F P ). As some of the datasets",
          "Features\n#dimensions\nUA%\nWA%": "were extracted using the HTK toolkit [42]. The eNTERFACE"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "are imbalanced across the emotion classes, see Table 1,\nthe",
          "Features\n#dimensions\nUA%\nWA%": "dataset is used for investigating the training setup as it\nis the"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "WA is calculated which weighs each class according to the",
          "Features\n#dimensions\nUA%\nWA%": "smallest dataset\nin terms of\ntime and has a high number of"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "number of samples in that class:",
          "Features\n#dimensions\nUA%\nWA%": "speakers with almost completely balanced emotions. Table 3"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "displays the UA and WA results across the different features"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "T P\nT N",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "1 2\n(3)\n(\n+\n)\nW A =",
          "Features\n#dimensions\nUA%\nWA%": "extracted. LMFBs with 23 dimensions outperform the rest in"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "P\nN",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "terms of UA and WA,\ntherefore the remaining experiments"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "are performed with 23 dimensional LMFBs."
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "4.3. Baseline systems",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "Baseline system results from the literature are difﬁcult to ﬁnd",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "",
          "Features\n#dimensions\nUA%\nWA%": "4.5. Results: Cross-corpus"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "due to the fact\nthat\nthis work requires the big-six emotions",
          "Features\n#dimensions\nUA%\nWA%": ""
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "and many datasets contain more, or less, and therefore evalu-",
          "Features\n#dimensions\nUA%\nWA%": "The cross-corpus (CC) results are seen in Table 4 where the"
        },
        {
          "Split\nENT\nRAV\nIEM\nIEM4\nMOS\nTotal": "ation results are not equivalent. Additionally, not all research",
          "Features\n#dimensions\nUA%\nWA%": "performances in the bold diagonal refer to the matched con-"
        }
      ],
      "page": 4
    },
    {
      "caption": "Table 4: Cross-corpus(CC), multi-domain(MD) and DAT results across the five testsets, where each row refersto a single",
      "data": [
        {
          "Training": "",
          "Unweighted Accuracy, UA%": "",
          "Weighted Accuracy, WA%": ""
        },
        {
          "Training": "Data",
          "Unweighted Accuracy, UA%": "RAV",
          "Weighted Accuracy, WA%": "RAV"
        },
        {
          "Training": "ENT",
          "Unweighted Accuracy, UA%": "74.7",
          "Weighted Accuracy, WA%": "54.5"
        },
        {
          "Training": "RAV",
          "Unweighted Accuracy, UA%": "86.1",
          "Weighted Accuracy, WA%": "75.0"
        },
        {
          "Training": "",
          "Unweighted Accuracy, UA%": "",
          "Weighted Accuracy, WA%": ""
        },
        {
          "Training": "IEM",
          "Unweighted Accuracy, UA%": "75.6",
          "Weighted Accuracy, WA%": "56.2"
        },
        {
          "Training": "MOS",
          "Unweighted Accuracy, UA%": "73.3",
          "Weighted Accuracy, WA%": "52.0"
        },
        {
          "Training": "all",
          "Unweighted Accuracy, UA%": "86.1",
          "Weighted Accuracy, WA%": "75.0"
        },
        {
          "Training": "all",
          "Unweighted Accuracy, UA%": "87.1",
          "Weighted Accuracy, WA%": "76.8"
        }
      ],
      "page": 5
    },
    {
      "caption": "Table 4: Cross-corpus(CC), multi-domain(MD) and DAT results across the five testsets, where each row refersto a single",
      "data": [
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "dataset, OOD and OOD with adaptation."
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "dition (trained and tested on the same dataset) and the non-"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "bold performances are the mismatched condition (trained and"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "tested on different datasets). Each row in the table refers to"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "a single model.\nIn terms of\nthe baseline systems described"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "in Section 4.3, asterisks are displayed next to comparable re-"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "sults.\nThe cross-corpus and matched results achieve better"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "results than the baselines. This shows the proposed system is"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "set up well for recognising emotions from speech and accept-"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "able to use for this study. The best performance is found in the"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "matched condition as would be expected. The model trained"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "on IEM achieves the best mismatched results for RAV and"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "MOS testsets but not for ENT. As an elicited dataset,\nit can"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "be argued to be close to both the acted and more natural sce-"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": ""
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "narios. Comparing the models trained on the acted datasets,"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "it can be seen that\nthe RAV model performs better for IEM,"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": ""
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "IEM4 and MOS than the ENT model. This shows that despite"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": ""
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "both being similar acted datasets, there seems to be more use-"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": ""
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "ful information to learn from a model trained on RAV than on"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": ""
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "ENT. This reinforces the challenges from datasets with dif-"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": ""
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "ferent annotations [10].\nPerformance reduces when moving"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": ""
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "to the natural dataset MOS, showing the difﬁculty of classify-"
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": ""
        },
        {
          "Table 5. Out-of-domain (OOD) and adaptation (+adaptation) results across the ﬁve testsets. There are two models for each": "ing more naturally produced emotions."
        }
      ],
      "page": 5
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "Training": "",
          "Unweighted Accuracy, UA%": "",
          "Weighted Accuracy, WA%": ""
        },
        {
          "Training": "Data",
          "Unweighted Accuracy, UA%": "ang.",
          "Weighted Accuracy, WA%": "ang."
        },
        {
          "Training": "ENT",
          "Unweighted Accuracy, UA%": "70.8",
          "Weighted Accuracy, WA%": "69.0"
        },
        {
          "Training": "RAV",
          "Unweighted Accuracy, UA%": "71.3",
          "Weighted Accuracy, WA%": "61.3"
        },
        {
          "Training": "",
          "Unweighted Accuracy, UA%": "",
          "Weighted Accuracy, WA%": ""
        },
        {
          "Training": "IEM",
          "Unweighted Accuracy, UA%": "56.8",
          "Weighted Accuracy, WA%": "67.8"
        },
        {
          "Training": "MOS",
          "Unweighted Accuracy, UA%": "75.8",
          "Weighted Accuracy, WA%": "53.6"
        },
        {
          "Training": "all",
          "Unweighted Accuracy, UA%": "81.4",
          "Weighted Accuracy, WA%": "78.0"
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "75.8\n88.1\n88.4\nMOS\n60.7\n67.3\n84.7",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "58.9\n66.3\n66.6\n53.6\n49.9\n51.6\n50.0\n55.4"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "DAT\nall\n75.0\n81.5\n81.4\n89.2\n86.5\n90.4",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "69.0\n67.4\n76.0\n78.0\n64.5\n70.3\n57.5\n53.1"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "Table 6. Performances across the emotions for the cross-corpus (CC) and DAT models, where each row refers to a single model.",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "Results in bold refer to the best performances for each emotion across the CC models.",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "4.8. Results: Out-of-domain and adaptation",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "5. CONCLUSIONS"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "The OOD results with the corresponding adaptation perfor-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "This study has carried out an investigation into cross-corpus"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "mance are displayed in Table 5. There are four OOD models,",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "speech emotion recognition.\nIt\nis the ﬁrst study of\nits kind"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "each trained on three datasets. The performance for each test-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "investigating the degradation of performance when moving"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "set shown comes from the mismatched trained model. Results",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "from acted to more natural datasets.\nInitial experiments into"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "for IEM and IEM4 are from the same model. The OOD mod-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "the optimum acoustic\nfeatures\nfor\nthis\nset-up showed that"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "els are then adapted to the dataset previously not included and",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "LMFB features outperform MFCCs, PLPs and COVAREP"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "this matched result\nis shown. The OOD results do not reach",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "features.\nCross-corpus experiments\nshow the matched re-"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "the accuracies of the multi-domain model, but they do outper-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "sults outperform mismatched and that\nthe model\ntrained on"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "form the worst cross-corpus mismatched result. This shows",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "the elicited dataset achieves best mismatched performance"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "that learning on additional data is useful even if it’s from dif-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "in most cases.\nIn the multi-domain setting,\nthe model out-"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "ferent, mismatched datasets. When adapting the OOD mod-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "performs the best mismatched results showing more data is"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "els, large gains are seen reaching similar results as the multi-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "beneﬁcial, even if\nthe emotions are in a different setting or"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "domain and DAT models. The average UA improvement\nis",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "production type. The DAT experiment shows how including"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "8.9% and 14.8% for WA. This shows how important\nit\nis to",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "the domain information does help the model to generalise and"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "train on matched data.",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "outperform the multi-domain model\nfor all but one dataset."
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "The OOD models\nshow that\nlearning from multiple mis-"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "matched datasets\nis more useful\nthan training on a\nsingle"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "4.9. Results: Emotion classes",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "mismatched dataset, and adapting gives large improvements"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "showing the importance of training on matched data. When"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "The experiments have shown how acted datasets can beneﬁt",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "looking across\nthe\nemotion class\nperformance, ENT data"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "more natural datasets with the multi-domain, DAT and OOD",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "shows to be best\nin terms of WA for most of\nthe emotions"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "with adaptation models all achieving WA results outperform-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "showing acted datasets may have\ntheir uses\nfor\nthe more"
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "ing the cross-corpus matched model for MOS. Complimen-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": "natural datasets."
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "tary to looking at\nthe performances across datasets, compar-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "ing the performances between the emotion classes can give in-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "sight into how the models are behaving. The cross-corpus and",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "DAT models are considered, and the results are displayed in",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "Table 6 where the neutral performance comes from only one",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "testset, IEM4. For the cross-corpus results, the bold numbers",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "refer to the best performance for that emotion. For the UA re-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "sults,\nit\nis fairly inconsistent which model is better, however",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "the models trained on IEM and MOS give the better perfor-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "mances for all\nthe emotions except for happy.\nFor\nthe WA",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "results,\nit shows the opposite,\nthat\ntraining on ENT is bet-",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "ter across ﬁve of the seven emotions. This seems to suggest",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "that acted datasets are good at distinguishing emotions, even",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "across datasets with different emotion production types. The",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "DAT model gives better performance across all emotions in",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "terms of UA and WA, except\nfor\nthe weighted accuracy of",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        },
        {
          "77.4\n85.6\n88.4\nIEM\n66.4\n56.8\n87.8": "fear and neutral emotions.",
          "69.1\n68.1\n53.3\n67.8\n50.0\n50.0\n50.0\n50.0": ""
        }
      ],
      "page": 6
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "Schuller,\n“Sparse autoencoder-based feature transfer"
        },
        {
          "6. REFERENCES": "[1] Frank Dellaert, Thomas Polzin, and Alex Waibel, “Rec-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "learning for speech emotion recognition,”\nin Humaine"
        },
        {
          "6. REFERENCES": "ognizing emotion in speech,” in Spoken Language Pro-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "Association Conference on Affective Computing and In-"
        },
        {
          "6. REFERENCES": "cessing, Philadelphia, PA, USA, Oct 3-6, 1996.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "telligent Interaction, ACII, Geneva, Switzerland, Sep 2-"
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "5, 2013, pp. 511–516."
        },
        {
          "6. REFERENCES": "[2] Rosalind W. Picard, Affective Computing, MIT Press,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "Cambridge, MA, USA, 1997.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "[14]\nJun Deng, Xinzhou Xu, Zixing Zhang, Sascha Fr¨uhholz,"
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "and Bj¨orn W. Schuller,\n“Universum autoencoder-based"
        },
        {
          "6. REFERENCES": "[3] Shashidhar G. Koolagudi and K. Sreenivasa Rao, “Emo-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "domain\nadaptation\nfor\nspeech\nemotion\nrecognition,”"
        },
        {
          "6. REFERENCES": "International\ntion recognition from speech: a review,”",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "IEEE Signal Processing Letters, vol. 24, no. 4, pp. 500–"
        },
        {
          "6. REFERENCES": "Journal of Speech Technology, vol. 15, no. 2, pp. 99–",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "504, 2017."
        },
        {
          "6. REFERENCES": "117, 2012.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "[15]\nJun Deng, Xinzhou Xu, Zixing Zhang, Sascha Fr¨uhholz,"
        },
        {
          "6. REFERENCES": "[4] Moataz M. H. El Ayadi, Mohamed S. Kamel, and Fakhri",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "and Bj¨orn W. Schuller,\n“Universum autoencoder-based"
        },
        {
          "6. REFERENCES": "Karray,\n“Survey on speech emotion recognition: Fea-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "domain\nadaptation\nfor\nspeech\nemotion\nrecognition,”"
        },
        {
          "6. REFERENCES": "Pattern\ntures, classiﬁcation schemes, and databases,”",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "IEEE Signal Processing Letters, 2017."
        },
        {
          "6. REFERENCES": "Recognition, vol. 44, no. 3, pp. 572–587, 2011.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "[16] Yuan Zong, Wenming Zheng, Tong Zhang,\nand Xi-"
        },
        {
          "6. REFERENCES": "[5] Christos-Nikolaos Anagnostopoulos, Theodoros\nIliou,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "aohua Huang,\n“Cross-corpus\nspeech emotion recog-"
        },
        {
          "6. REFERENCES": "and Ioannis Giannoukos,\n“Features and classiﬁers for",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "nition based on domain-adaptive least-squares\nregres-"
        },
        {
          "6. REFERENCES": "emotion recognition from speech:\na survey from 2000",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "sion,”\nIEEE Signal Processing Letters, vol. 23, no. 5,"
        },
        {
          "6. REFERENCES": "to 2011,” Artiﬁcial Intelligence Review, vol. 43, no. 2,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "pp. 585–589, 2016."
        },
        {
          "6. REFERENCES": "pp. 155–177, 2015.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "[17] Yuan Zong, Wenming Zheng, Xiaohua Huang, Keyu"
        },
        {
          "6. REFERENCES": "[6] Dimitrios\nVerveridis\nand\nConstantine\nKotropoulos,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "Yan, Jingwei Yan, and Tong Zhang,\n“Emotion recog-"
        },
        {
          "6. REFERENCES": "“Emotional\nspeech recognition:\nResources,\nfeatures,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "nition in the wild via sparse transductive transfer linear"
        },
        {
          "6. REFERENCES": "and methods,”\nSpeech Communication, vol. 48, no. 9,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "Journal of Multimodal User In-\ndiscriminant analysis,”"
        },
        {
          "6. REFERENCES": "pp. 1162–1181, 2006.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "terfaces, vol. 10, no. 2, pp. 163–172, 2016."
        },
        {
          "6. REFERENCES": "[7] Robert Plutchik,\n“Emotion: Theory, research, and ex-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "[18]\nJaebok Kim, Gwenn Englebienne, Khiet P. Truong, and"
        },
        {
          "6. REFERENCES": "perience: Vol. 1. theories of emotion,” New York: Aca-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "Vanessa Evers,\n“Towards speech emotion recognition"
        },
        {
          "6. REFERENCES": "demic, 1997.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "”in the wild” using aggregated corpora and deep multi-"
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "task learning,” in Interspeech, Stockholm, Sweden, Aug"
        },
        {
          "6. REFERENCES": "[8] Erik Cambria, Andrew Livingstone, and Amir Hussain,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "20-24, 2017, pp. 1113–1117."
        },
        {
          "6. REFERENCES": "“The hourglass of emotions,”\nin COST 2102 Train-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "ing School. 2011, Lecture Notes in Computer Science,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "[19] Na Liu, Yuan Zong, Baofeng Zhang, Li Liu, Jie Chen,"
        },
        {
          "6. REFERENCES": "Springer.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "Guoying Zhao,\nand\nJunchao Zhu,\n“Unsupervised"
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "cross-corpus speech emotion recognition using domain-"
        },
        {
          "6. REFERENCES": "[9] Paul Ekman, “An argument for basic emotions,” Cogni-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "adaptive subspace learning,”\nin ICASSP, Calgary, AB,"
        },
        {
          "6. REFERENCES": "tion and Emotion, pp. 169–200, 1992.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "Canada, Apr 15-20, 2018, pp. 5144–5148."
        },
        {
          "6. REFERENCES": "[10] Mia Atcheson, Vidhyasaharan Sethu, and Julien Epps,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "[20] Amir Zadeh, Paul Pu Liang, Soujanya Poria, Erik Cam-"
        },
        {
          "6. REFERENCES": "“Demonstrating and modelling systematic time-varying",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "bria, and Louis-Philippe Morency,\n“Multimodal\nlan-"
        },
        {
          "6. REFERENCES": "annotator disagreement\nin continuous emotion annota-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "guage analysis in the wild: CMU-MOSEI dataset and"
        },
        {
          "6. REFERENCES": "tion,” in Interspeech, Hyderabad, India, 2-6 Sep, 2018,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "interpretable dynamic fusion graph,” in Annual Meeting"
        },
        {
          "6. REFERENCES": "pp. 3668–3672.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "of\nthe Association for Computational Linguistics, ACL,"
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "Melbourne, Australia, Jul 15-20, 2018, pp. 2236–2246."
        },
        {
          "6. REFERENCES": "[11] Bj¨orn W. Schuller, Bogdan Vlasenko, Florian Eyben,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "Martin W¨ollmer, Andr´e Stuhlsatz, Andreas Wendemuth,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "[21] Sepp Hochreiter and J¨urgen Schmidhuber, “Long short-"
        },
        {
          "6. REFERENCES": "and Gerhard Rigoll,\n“Cross-corpus acoustic emotion",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "term memory,” Neural Computation, vol. 9, no. 8, pp."
        },
        {
          "6. REFERENCES": "recognition: Variances and strategies,” IEEE Trans. Af-",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "1735–1780, 1997."
        },
        {
          "6. REFERENCES": "fective Computing, vol. 1, no. 2, pp. 119–131, 2010.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": ""
        },
        {
          "6. REFERENCES": "",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "[22] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-"
        },
        {
          "6. REFERENCES": "[12] Ali Hassan, Robert I. Damper, and Mahesan Niranjan,",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "gio,\n“Neural machine translation by jointly learning"
        },
        {
          "6. REFERENCES": "“On acoustic emotion recognition: Compensating for",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "to align and translate,”\nin International Conference on"
        },
        {
          "6. REFERENCES": "covariate shift,” IEEE Trans. Audio, Speech & Language",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "Learning Representations, ICLR, San Diego, CA, USA,"
        },
        {
          "6. REFERENCES": "Processing, vol. 21, no. 7, pp. 1458–1468, 2013.",
          "[13]\nJun Deng, Zixing Zhang, Erik Marchi, and Bj¨orn W.": "May 7-9,, 2015."
        }
      ],
      "page": 7
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "Bouthillier,\nPierre Froumenty, C¸ aglar G¨ulc¸ehre,\nand",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "ture database,”\nLanguage Resources and Evaluation,"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "Roland Memisevic et al, “Combining modality speciﬁc",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "vol. 42, no. 4, pp. 335–359, 2008."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "deep neural networks for emotion recognition in video,”",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "[33] Pengcheng Li,\nYan\nSong,\nIan Vince McLoughlin,"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "in International Conference on Multimodal Interaction,",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "Wu Guo,\nand Lirong Dai,\n“An\nattention\npooling"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "ICMI, Sydney, NSW, Australia, Dec 9-13, 2013, pp. 543–",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "based representation learning method for speech emo-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "550.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "tion recognition,” in Interspeech, Hyderabad, India, 2-6"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "[24] Rory\nBeard,\nRitwik\nDas,\nRaymond W. M.\nNg,",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "Sep,, 2018, pp. 3087–3091."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "P. G. Keerthana Gopalakrishnan, Luka Eerens, Pawel",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "[34] Adam Paszke et al.,\n“Automatic differentiation in py-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "Swietojanski,\nand Ondrej Miksik,\n“Multi-modal\nse-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "torch,” in NIPS-W, 2017."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "quence fusion via recursive attention for emotion recog-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "nition,”\nin Computational Natural Language Learning,",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "[35] Diederik P. Kingma and Jimmy Ba,\n“Adam: A method"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "CoNLL, Brussels, Belgium, Oct 31 - Nov 1, 2018, pp.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "for stochastic optimization,”\nin International Confer-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "251–259.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "ence on Learning Representations,\nICLR, San Diego,"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "CA, USA, May 7-9,, 2015."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "[25] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "cal Germain, Hugo Larochelle,\nFranc¸ois Laviolette,",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "Segmentation,\ndiarization\nand\n[36] Marijn Huijbregts,"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "Mario Marchand, and Victor S. Lempitsky,\n“Domain-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "speech transcription :\nsurprise data unraveled,\nPh.D."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "adversarial training of neural networks,” Journal of Ma-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "thesis, University of Twente, Enschede, Netherlands,"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "chine Learning Research, vol. 17, pp. 59:1–59:35, 2016.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "2008."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "[26] Mohammed Abdel-Wahab and Carlos Busso,\n“Do-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "[37] Che-Wei Huang and Shrikanth S. Narayanan,\n“Deep"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "main\nadversarial\nfor\nacoustic\nemotion\nrecognition,”",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "convolutional\nrecurrent neural network with attention"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "IEEE/ACM Trans. Audio, Speech & Language Process-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "mechanism for\nrobust\nspeech\nemotion\nrecognition,”"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "ing, vol. 26, no. 12, pp. 2423–2435, 2018.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "in IEEE International Conference on Multimedia and"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "Expo, ICME, Hong Kong, China, Jul 10-14,, 2017, pp."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "[27] Hyeonseob Nam,\nJung-Woo Ha,\nand Jeonghee Kim,",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "583–588."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "“Dual attention networks for multimodal reasoning and",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "[38] Danqing Luo, Yuexian Zou, and Dongyan Huang,\n“In-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "matching,” in IEEE Conference on Computer Vision and",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "vestigation on joint\nrepresentation learning for\nrobust"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "Pattern Recognition, CVPR, Honolulu, HI, USA, Jul 21-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "feature extraction in speech emotion recognition,”\nin"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "26, 2017, pp. 2156–2164.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "Interspeech, Hyderabad, India, 2-6 Sep, 2018, pp. 152–"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "[28] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "156."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "hamed,\n“Hybrid speech recognition with deep bidirec-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "[39] Brecht Desplanques\nand Kris Demuynck,\n“Cross-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "tional LSTM,” in IEEE Workshop on Automatic Speech",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "lingual speech emotion recognition through factor anal-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "Recognition and Understanding, Olomouc, Czech Re-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "ysis,” in Interspeech, Hyderabad, India, 2-6 Sep, 2018,"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "public, Dec 8-12,, 2013, pp. 273–278.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "pp. 3648–3652."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "[29] Mike Schuster and Kuldip K. Paliwal, “Bidirectional re-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "[40] Hynek Hermansky and Sangita Sharma, “TRAPS - clas-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "IEEE Trans. Signal Process-\ncurrent neural networks,”",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "siﬁers of\ntemporal patterns,”\nin International Confer-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "ing, vol. 45, no. 11, pp. 2673–2681, 1997.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "ence on Spoken Language Processing, Australian In-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "ternational Speech Science and Technology Conference,"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "[30] Olivier Martin,\nIrene Kotsia, Benoˆıt Macq, and Ioan-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "Sydney Convention Centre, Sydney, Australia, Nov 30 -"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "nis\nPitas,\n“The\nenterface’05\naudio-visual\nemotion",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "Dec 4,, 1998."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "database,”\nin International Conference on Data Engi-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "neering Workshops, ICDE, Atlanta, GA, USA, Apr 3-7,",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "[41] Gilles Degottex, John Kane, Thomas Drugman, Tuomo"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "2006, p. 8.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "Raitio, and Stefan Scherer,\n“COVAREP - A collabora-"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "tive voice analysis repository for speech technologies,”"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "[31] Steven R. Livingstone and Frank A. Russo,\n“The Ry-",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "in ICASSP, Florence, Italy, May 4-9, 2014, pp. 960–964."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "erson Audio-Visual Database of Emotional Speech and",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "Song (RAVDESS),” PLoS ONE, 2018.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "[42] Steve\nJ. Young, D. Kershaw,\nJ. Odell, D. Ollason,"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "The HTK Book Version\nV\n. Valtchev, and P. Woodland,"
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "[32] Carlos Busso, Murtaza Bulut,\nChi-Chun Lee, Abe",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": "3.4, Cambridge University Press, 2006."
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N.",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        },
        {
          "[23] Samira Ebrahimi Kahou, Christopher\nJ. Pal, Xavier": "Chang,\nSungbok\nLee,\nand\nShrikanth\nNarayanan,",
          "“IEMOCAP:\ninteractive emotional dyadic motion cap-": ""
        }
      ],
      "page": 8
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This figure \"all4ua.png\" is available in \"png\"(cid:10) format from:": "http://arxiv.org/ps/2207.02104v1"
        }
      ],
      "page": 9
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This figure \"all4wa.png\" is available in \"png\"(cid:10) format from:": "http://arxiv.org/ps/2207.02104v1"
        }
      ],
      "page": 10
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This figure \"dat007ua.png\" is available in \"png\"(cid:10) format from:": "http://arxiv.org/ps/2207.02104v1"
        }
      ],
      "page": 11
    },
    {
      "caption": "Table: No caption found",
      "data": [
        {
          "This figure \"dat007wa.png\" is available in \"png\"(cid:10) format from:": "http://arxiv.org/ps/2207.02104v1"
        }
      ],
      "page": 12
    }
  ],
  "citations": [
    {
      "citation_id": "1",
      "title": "",
      "authors": [
        "References"
      ],
      "venue": ""
    },
    {
      "citation_id": "2",
      "title": "Recognizing emotion in speech",
      "authors": [
        "Frank Dellaert",
        "Thomas Polzin",
        "Alex Waibel"
      ],
      "year": "1996",
      "venue": "Spoken Language Processing"
    },
    {
      "citation_id": "3",
      "title": "Affective Computing",
      "authors": [
        "Rosalind Picard"
      ],
      "year": "1997",
      "venue": "Affective Computing"
    },
    {
      "citation_id": "4",
      "title": "Emotion recognition from speech: a review",
      "authors": [
        "G Shashidhar",
        "K Koolagudi",
        "Rao"
      ],
      "year": "2012",
      "venue": "International Journal of Speech Technology"
    },
    {
      "citation_id": "5",
      "title": "Survey on speech emotion recognition: Features, classification schemes, and databases",
      "authors": [
        "M Moataz",
        "Mohamed Ayadi",
        "Fakhri Kamel",
        "Karray"
      ],
      "year": "2011",
      "venue": "Pattern Recognition"
    },
    {
      "citation_id": "6",
      "title": "Features and classifiers for emotion recognition from speech: a survey from 2000 to 2011",
      "authors": [
        "Christos-Nikolaos Anagnostopoulos",
        "Theodoros Iliou",
        "Ioannis Giannoukos"
      ],
      "year": "2015",
      "venue": "Artificial Intelligence Review"
    },
    {
      "citation_id": "7",
      "title": "Emotional speech recognition: Resources, features, and methods",
      "authors": [
        "Dimitrios Ververidis",
        "Constantine Kotropoulos"
      ],
      "year": "2006",
      "venue": "Speech Communication"
    },
    {
      "citation_id": "8",
      "title": "Emotion: Theory, research, and experience: Vol. 1. theories of emotion",
      "authors": [
        "Robert Plutchik"
      ],
      "year": "1997",
      "venue": "Emotion: Theory, research, and experience: Vol. 1. theories of emotion"
    },
    {
      "citation_id": "9",
      "title": "The hourglass of emotions",
      "authors": [
        "Erik Cambria",
        "Andrew Livingstone",
        "Amir Hussain"
      ],
      "year": "2011",
      "venue": "COST 2102 Training School"
    },
    {
      "citation_id": "10",
      "title": "An argument for basic emotions",
      "authors": [
        "Paul Ekman"
      ],
      "year": "1992",
      "venue": "Cognition and Emotion"
    },
    {
      "citation_id": "11",
      "title": "Demonstrating and modelling systematic time-varying annotator disagreement in continuous emotion annotation",
      "authors": [
        "Mia Atcheson",
        "Vidhyasaharan Sethu",
        "Julien Epps"
      ],
      "year": "2018",
      "venue": "Demonstrating and modelling systematic time-varying annotator disagreement in continuous emotion annotation"
    },
    {
      "citation_id": "12",
      "title": "Cross-corpus acoustic emotion recognition: Variances and strategies",
      "authors": [
        "W Björn",
        "Bogdan Schuller",
        "Florian Vlasenko",
        "Martin Eyben",
        "André Wöllmer",
        "Andreas Stuhlsatz",
        "Gerhard Wendemuth",
        "Rigoll"
      ],
      "year": "2010",
      "venue": "IEEE Trans. Affective Computing"
    },
    {
      "citation_id": "13",
      "title": "On acoustic emotion recognition: Compensating for covariate shift",
      "authors": [
        "Ali Hassan",
        "Robert Damper",
        "Mahesan Niranjan"
      ],
      "year": "2013",
      "venue": "IEEE Trans. Audio, Speech & Language Processing"
    },
    {
      "citation_id": "14",
      "title": "Sparse autoencoder-based feature transfer learning for speech emotion recognition",
      "authors": [
        "Jun Deng",
        "Zixing Zhang",
        "Erik Marchi",
        "Björn Schuller"
      ],
      "year": "2013",
      "venue": "Humaine Association Conference on Affective Computing and Intelligent Interaction"
    },
    {
      "citation_id": "15",
      "title": "Universum autoencoder-based domain adaptation for speech emotion recognition",
      "authors": [
        "Jun Deng",
        "Xinzhou Xu",
        "Zixing Zhang",
        "Sascha Frühholz",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "16",
      "title": "Universum autoencoder-based domain adaptation for speech emotion recognition",
      "authors": [
        "Jun Deng",
        "Xinzhou Xu",
        "Zixing Zhang",
        "Sascha Frühholz",
        "Björn Schuller"
      ],
      "year": "2017",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "17",
      "title": "Cross-corpus speech emotion recognition based on domain-adaptive least-squares regression",
      "authors": [
        "Yuan Zong",
        "Wenming Zheng",
        "Tong Zhang",
        "Xiaohua Huang"
      ],
      "year": "2016",
      "venue": "IEEE Signal Processing Letters"
    },
    {
      "citation_id": "18",
      "title": "Emotion recognition in the wild via sparse transductive transfer linear discriminant analysis",
      "authors": [
        "Yuan Zong",
        "Wenming Zheng",
        "Xiaohua Huang",
        "Keyu Yan",
        "Jingwei Yan",
        "Tong Zhang"
      ],
      "year": "2016",
      "venue": "Journal of Multimodal User Interfaces"
    },
    {
      "citation_id": "19",
      "title": "Towards speech emotion recognition \"in the wild\" using aggregated corpora and deep multitask learning",
      "authors": [
        "Jaebok Kim",
        "Gwenn Englebienne",
        "P Khiet",
        "Vanessa Truong",
        "Evers"
      ],
      "year": "2017",
      "venue": "Towards speech emotion recognition \"in the wild\" using aggregated corpora and deep multitask learning"
    },
    {
      "citation_id": "20",
      "title": "Unsupervised cross-corpus speech emotion recognition using domainadaptive subspace learning",
      "authors": [
        "Na Liu",
        "Yuan Zong",
        "Baofeng Zhang",
        "Li Liu",
        "Jie Chen",
        "Guoying Zhao",
        "Junchao Zhu"
      ],
      "year": "2018",
      "venue": "ICASSP"
    },
    {
      "citation_id": "21",
      "title": "Multimodal language analysis in the wild: CMU-MOSEI dataset and interpretable dynamic fusion graph",
      "authors": [
        "Amir Zadeh",
        "Paul Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency"
      ],
      "year": "2018",
      "venue": "Annual Meeting of the Association for Computational Linguistics, ACL"
    },
    {
      "citation_id": "22",
      "title": "Long shortterm memory",
      "authors": [
        "Sepp Hochreiter",
        "Jürgen Schmidhuber"
      ],
      "year": "1997",
      "venue": "Neural Computation"
    },
    {
      "citation_id": "23",
      "title": "Neural machine translation by jointly learning to align and translate",
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations, ICLR"
    },
    {
      "citation_id": "24",
      "title": "Combining modality specific deep neural networks for emotion recognition in video",
      "authors": [
        "Samira Kahou",
        "Christopher Pal",
        "Xavier Bouthillier",
        "Pierre Froumenty",
        "C ¸aglar Gülc ¸ehre",
        "Roland Memisevic"
      ],
      "year": "2013",
      "venue": "International Conference on Multimodal Interaction, ICMI"
    },
    {
      "citation_id": "25",
      "title": "Multi-modal sequence fusion via recursive attention for emotion recognition",
      "authors": [
        "Rory Beard",
        "Ritwik Das",
        "Raymond Ng",
        "P Keerthana Gopalakrishnan",
        "Luka Eerens",
        "Pawel Swietojanski",
        "Ondrej Miksik"
      ],
      "year": "2018",
      "venue": "Computational Natural Language Learning"
    },
    {
      "citation_id": "26",
      "title": "Domainadversarial training of neural networks",
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "Mario Franc ¸ois Laviolette",
        "Victor Marchand",
        "Lempitsky"
      ],
      "year": "2016",
      "venue": "Journal of Machine Learning Research"
    },
    {
      "citation_id": "27",
      "title": "Domain adversarial for acoustic emotion recognition",
      "authors": [
        "Mohammed Abdel",
        "- Wahab",
        "Carlos Busso"
      ],
      "year": "2018",
      "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing"
    },
    {
      "citation_id": "28",
      "title": "Dual attention networks for multimodal reasoning and matching",
      "authors": [
        "Hyeonseob Nam",
        "Jung-Woo Ha",
        "Jeonghee Kim"
      ],
      "year": "2017",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition"
    },
    {
      "citation_id": "29",
      "title": "Hybrid speech recognition with deep bidirectional LSTM",
      "authors": [
        "Alex Graves",
        "Navdeep Jaitly",
        "Abdel-Rahman Mohamed"
      ],
      "year": "2013",
      "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding"
    },
    {
      "citation_id": "30",
      "title": "Bidirectional recurrent neural networks",
      "authors": [
        "Mike Schuster",
        "Kuldip Paliwal"
      ],
      "year": "1997",
      "venue": "IEEE Trans. Signal Processing"
    },
    {
      "citation_id": "31",
      "title": "The enterface'05 audio-visual emotion database",
      "authors": [
        "Olivier Martin",
        "Irene Kotsia",
        "Benoît Macq",
        "Ioannis Pitas"
      ],
      "year": "2006",
      "venue": "International Conference on Data Engineering Workshops, ICDE"
    },
    {
      "citation_id": "32",
      "title": "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)",
      "authors": [
        "Steven Livingstone",
        "Frank Russo"
      ],
      "year": "2018",
      "venue": "PLoS ONE"
    },
    {
      "citation_id": "33",
      "title": "IEMOCAP: interactive emotional dyadic motion capture database",
      "authors": [
        "Carlos Busso",
        "Murtaza Bulut",
        "Chi-Chun Lee",
        "Abe Kazemzadeh",
        "Emily Mower",
        "Samuel Kim",
        "Jeannette Chang",
        "Sungbok Lee",
        "Shrikanth Narayanan"
      ],
      "year": "2008",
      "venue": "Language Resources and Evaluation"
    },
    {
      "citation_id": "34",
      "title": "An attention pooling based representation learning method for speech emotion recognition",
      "authors": [
        "Pengcheng Li",
        "Yan Song",
        "Ian Vince Mcloughlin",
        "Wu Guo",
        "Lirong Dai"
      ],
      "year": "2018",
      "venue": "An attention pooling based representation learning method for speech emotion recognition"
    },
    {
      "citation_id": "35",
      "title": "Automatic differentiation in pytorch",
      "authors": [
        "Adam Paszke"
      ],
      "year": "2017",
      "venue": "NIPS-W"
    },
    {
      "citation_id": "36",
      "title": "Adam: A method for stochastic optimization",
      "authors": [
        "P Diederik",
        "Jimmy Kingma",
        "Ba"
      ],
      "year": "2015",
      "venue": "International Conference on Learning Representations, ICLR"
    },
    {
      "citation_id": "37",
      "title": "Segmentation, diarization and speech transcription : surprise data unraveled",
      "authors": [
        "Marijn Huijbregts"
      ],
      "year": "2008",
      "venue": "Segmentation, diarization and speech transcription : surprise data unraveled"
    },
    {
      "citation_id": "38",
      "title": "Deep convolutional recurrent neural network with attention mechanism for robust speech emotion recognition",
      "authors": [
        "Che-Wei Huang",
        "Shrikanth Narayanan"
      ],
      "year": "2017",
      "venue": "IEEE International Conference on Multimedia and Expo, ICME"
    },
    {
      "citation_id": "39",
      "title": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition",
      "authors": [
        "Danqing Luo",
        "Yuexian Zou",
        "Dongyan Huang"
      ],
      "year": "2018",
      "venue": "Investigation on joint representation learning for robust feature extraction in speech emotion recognition"
    },
    {
      "citation_id": "40",
      "title": "Crosslingual speech emotion recognition through factor analysis",
      "authors": [
        "Brecht Desplanques",
        "Kris Demuynck"
      ],
      "year": "2018",
      "venue": "Crosslingual speech emotion recognition through factor analysis"
    },
    {
      "citation_id": "41",
      "title": "TRAPS -classifiers of temporal patterns",
      "authors": [
        "Hynek Hermansky",
        "Sangita Sharma"
      ],
      "year": "1998",
      "venue": "International Conference on Spoken Language Processing, Australian International Speech Science and Technology Conference"
    },
    {
      "citation_id": "42",
      "title": "COVAREP -A collaborative voice analysis repository for speech technologies",
      "authors": [
        "Gilles Degottex",
        "John Kane",
        "Thomas Drugman",
        "Tuomo Raitio",
        "Stefan Scherer"
      ],
      "year": "2014",
      "venue": "ICASSP"
    },
    {
      "citation_id": "43",
      "title": "",
      "authors": [
        "Steve Young",
        "D Kershaw",
        "J Odell",
        "D Ollason",
        "V Valtchev",
        "P Woodland"
      ],
      "venue": ""
    }
  ]
}